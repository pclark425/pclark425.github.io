<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2032 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2032</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2032</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-49.html">extraction-schema-49</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-279392089</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.11767v1.pdf" target="_blank">Designing Effective LLM-Assisted Interfaces for Curriculum Development</a></p>
                <p><strong>Paper Abstract:</strong> . Large Language Models ( LLMs ) have the potential to transform the way a dynamic curriculum can be delivered. However, educators face significant challenges in interacting with these models, particularly due to complex prompt engineering and usability issues, which increase workload. Additionally, inaccuracies in LLM outputs can raise issues around output quality and ethical concerns in educational content delivery. Addressing these issues requires careful oversight, best achieved through cooperation between human and AI approaches. This paper introduces two novel User Interface ( UI ) designs, UI Predefined and UI Open , both grounded in Direct Manipulation ( DM ) principles to address these challenges. By reducing the reliance on intricate prompt engineering, these UIs improve usability, streamline interaction, and lower workload, providing a more effective pathway for educators to engage with LLMs. In a controlled user study with 20 participants, the proposed UIs were evaluated against the standard ChatGPT interface in terms of usability and cognitive load. Results showed that UI Predefined significantly outperformed both ChatGPT and UI Open , demonstrating superior usability and reduced task load, while UI Open offered more flexibility at the cost of a steeper learning curve. These findings under-score the importance of user-centered design in adopting AI-driven tools and lay the foundation for more intuitive and efficient educator-LLM interactions in online learning environments.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2032.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2032.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UI Predefined</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>User Interface with Predefined Commands</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A direct-manipulation GUI that exposes curated, clickable commands for LLM-assisted course-outline generation, letting users apply grouped expert-derived actions, edit outlines in an interactive table, and perform undo/redo and batch operations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated via predefined command-driven interface with human-in-the-loop editing</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Course outlines are generated by issuing structured commands (grouped into four functional groups) from a GUI; each command triggers the LLM with a prompt that includes the current course outline and the user-issued command. The UI enforces a structured output format and assigns identifiers to generated topics for precise referencing; users can manually edit, reorder, and accept LLM outputs (human oversight integrated). Prompting followed OpenAI best-practices: a system prompt defined persona, task, prompt component delimiters, and output format.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gpt-4o-2024-08-06</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>curriculum/course-outline development (educational course planning)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Moderate complexity: designing a course outline consisting of course title, learning outcomes, and lists of topics (multiple items can be edited, reordered, batched), requiring domain knowledge and iterative refinement rather than multi-step environment interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>UI Open (open/dynamic command UI), ChatGPT-style replica (open-webui control)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Usability (SUS): 86.75 (mean); Workload (NASA RTLX mean): 2.25. Statistical comparisons: Workload UI Predefined vs ChatGPT p < 0.03; UI Predefined vs UI Open p < 0.02. SUS differences: UI Predefined vs ChatGPT p < 0.009; UI Predefined vs UI Open p < 0.002.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not reported (no learning-speed/convergence metrics for curriculum generation were measured).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not discussed or evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Not reported (UI supports topic refinement but no explicit intermediate/bridging task generation was described).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Paper reiterates general LLM concerns (inaccurate, wrong or ethically questionable outputs) and addresses them via human oversight; no specific hallucination examples from the experiment are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Evaluated with 20 educators in a within-subjects study using SUS and NASA RTLX; UI Predefined achieved highest SUS (86.75) and lowest workload (2.25). Participants reported reduced typing burden and higher ease of use.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>The predefined-command UI produced significantly better usability and lower cognitive workload than both the open-command UI and the ChatGPT-style control; curated, task-specific commands plus structured outputs and direct manipulation reduced effort and improved user satisfaction in curriculum creation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2032.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2032.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UI Open</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>User Interface with Open/Dynamic Commands</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A direct-manipulation GUI that allows users to compose, drag-and-drop, save, and execute dynamic commands (via a chat box) that are automatically contextualized with the current course outline for LLM-assisted generation and refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated via dynamic open/chat commands integrated into the UI with contextualization of the course outline</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Users create flexible, local or global commands through a chat box; commands are executed with the current outline context automatically included in prompts. Commands can be dragged to specific topics for localization, saved for reuse, and applied to sections; the UI translates LLM outputs into the interactive outline format. Prompting again used a system prompt (persona, task, format) and included current outline and user command.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gpt-4o-2024-08-06</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>curriculum/course-outline development (educational course planning)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Moderate complexity with higher interaction flexibility: supports localized commands per topic, saving/reusing commands, and more open-ended LLM interactions requiring user understanding of command composition.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>UI Predefined, ChatGPT-style replica (open-webui control)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Usability (SUS): 70.75 (mean); Workload (NASA RTLX mean): 3.00. UI Open ranked second; improvement over ChatGPT was non-significant. Detailed p-values: UI Predefined vs UI Open SUS p < 0.002.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Not reported (UI supports command composition but no explicit intermediate task generation evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>No specific failure cases reported; participants reported a steeper learning curve for the more flexible interface, implying usability limitations rather than LLM-specific failures.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Evaluated with 20 educators; SUS 70.75 and NASA RTLX 3.00. Participants valued flexibility but noted increased learning overhead compared to predefined commands.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>The open/dynamic command UI provided greater flexibility but incurred a steeper learning curve and worse usability/workload than the predefined-command UI; it did not significantly outperform the ChatGPT-style control.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2032.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2032.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT Replica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT-style Control Interface (open-webui replica)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A control interface emulating the standard ChatGPT textual interaction used for comparison; implemented with open-webui to allow monitoring and controlled model usage in the experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated via standard text-prompting interaction (ChatGPT-like interface)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Participants used a conventional text-based chat interface to prompt the model to produce or refine course outlines. The control used open-webui to emulate ChatGPT's UI for the study; prompts and interactions relied on user-crafted text prompts (no direct manipulation constructs).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>curriculum/course-outline development (educational course planning)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Moderate complexity: users must craft textual prompts and iteratively refine outputs via chat to create course outlines.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>UI Predefined, UI Open</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Usability (SUS): 69.00 (mean); Workload (NASA RTLX mean): 3.30. ChatGPT-style control had the highest workload and lowest usability among tested interfaces.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Paper notes general concerns with LLM outputs (accuracy, ethical issues) and the heavy prompt-engineering burden on users when using text-only interfaces.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Evaluated with 20 educators who generally reported higher workload and lower usability relative to the two direct-manipulation UIs; some preference due to familiarity but participants requested features like command reuse and better output presentation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>The standard text-based ChatGPT-style interface showed the highest perceived workload and lowest usability compared to DM-based UIs; reducing prompt-engineering burden via UI design improves educator experience.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2032.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2032.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-4o-2024-08-06 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The LLM model used to generate course-outline content in both UI Predefined and UI Open during the experiment; prompted with system persona and contextual outline information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated content via system + user prompts; used within UI-driven workflows to create/modify course outlines</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>A system prompt established persona, delimiters, task specification (course outline curation), and output format; current course outline and user-issued commands were included in each prompt to ensure contextual responses; UI parsed LLM output into structured outline items with identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gpt-4o-2024-08-06</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>curriculum/course-outline development</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Generative text tasks focused on producing structured lists and refinements (titles, outcomes, topic lists) rather than multi-step environment interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Used within UI Predefined and UI Open; compared indirectly to ChatGPT-style control which used open-webui replica (model unspecified).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>LLM-specific output quality not quantitatively measured; evaluation focused on user workload and usability when interacting with UIs that use this model.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not applicable / not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Paper cites general LLM limitations (inaccuracies and ethical issues) and motivates human oversight; no experiment-specific model failure cases detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Educators evaluated interfaces that used this model; judgments concerned usability and workload rather than direct model quality metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>The model was successfully integrated into two DM-based UIs; interface design (predefined vs open commands) strongly affected educator workload and perceived usability despite using the same underlying LLM.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2032.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2032.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LDA-based topic extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent Dirichlet Allocation (LDA) topic extraction methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Traditional topic-modelling methods previously used to extract covered topics from educational resources to support curriculum development, mentioned as prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>Topic-extraction from educational resources using LDA to recommend topics or structure curricula (traditional ML-based assistance)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>LDA applied to educational resource corpora to discover latent topics and suggest curriculum topics/structures; referenced as earlier, domain-specific approaches complementary to or preceding LLM approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>educational resource analysis for curriculum recommendation</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Extraction of latent topic structures from document corpora; complexity depends on corpus size and topic granularity but is algorithmic rather than interactive.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Mentioned alongside other traditional ML models (e.g., Random Forest) in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper (mentioned as prior approaches; specific metrics in cited works not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not applicable (traditional method).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Referenced prior studies combined such methods with human experts for curriculum recommendation; no new human evaluation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>LDA and other traditional ML methods have been used previously to extract topics and assist human experts in curriculum design; the present paper positions LLM-based interfaces as a more flexible, modern alternative while noting quality and usability concerns with LLMs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2032.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2032.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adaptive surgical curricula (Prior)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Skill-oriented and performance-driven adaptive curricula for training in robot-assisted surgery using simulators: A feasibility study</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior study that used an adaptive AI algorithm to create real-time curricula for robot-assisted surgery simulators based on learner feedback, showing improved learning outcomes in a small participant set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Skill-oriented and performance-driven adaptive curricula for training in robot-assisted surgery using simulators: A feasibility study</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>Adaptive AI algorithm that generates real-time curricula based on learner performance/feedback</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>An adaptive system tailored simulator training curricula in real time to learner feedback and performance metrics, aiming to personalise and optimize training progression in surgical simulation (reported as prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>robot-assisted surgery simulators (medical training)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>High complexity: surgical training involves multiple sub-skills, multi-step procedures, and fine motor/decision-making components; adaptation required across many skill dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared to standard (non-adaptive) curricula in the cited feasibility study (small N).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited as demonstrating improved learning outcomes for students using AI-generated curricula; specific numerical metrics were reported in the original cited study (not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Original cited work reported improved learning outcomes, implying faster or better acquisition; this paper does not reproduce numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not discussed here; see original study for details.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Likely used in the adaptive sequencing but specifics are in the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not applicable (adaptive AI, not LLM-based), though cited as limited by small participant numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Original study had limited participants but reported improved outcomes; this paper references that result to motivate AI-based curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Prior adaptive-AI curricula in high-complexity domains (robot-assisted surgery) showed promise in improving student outcomes, motivating exploration of more advanced AI (LLMs) plus usable UIs for curriculum development.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2032.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2032.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DirectGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DirectGPT (Direct Manipulation interface for LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed direct-manipulation user interface for interacting with LLMs that provides continuous object representation and prompt control, reported to improve task completion time and user satisfaction over traditional chat interfaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Directgpt: A direct manipulation interface to interact with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>UI design approach (direct-manipulation) to structure and control LLM interactions rather than a curriculum generation algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Direct-manipulation UI principles (continuous representation, physical actions, rapid reversible operations, visible effects) applied to LLM interaction to reduce prompt-engineering burden and improve usability; used as inspiration for the UIs developed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>general LLM interaction interfaces (applied to creative/authoring tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Not directly about curricula; focuses on improving interactive control of LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Original DirectGPT study compared to traditional chat UIs (reported better task completion times and higher satisfaction).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Original study reported improved task completion times and higher user satisfaction (not re-reported numerically here).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not discussed here beyond motivating improved UI design to mitigate prompt-engineering issues.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>DirectGPT was previously evaluated with experts (12 in cited study) and showed better task completion times and satisfaction; referenced here as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Direct-manipulation UI principles applied to LLM interaction can materially reduce user burden and improve efficiency, motivating the DM-based UIs developed and tested in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2032.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2032.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPROUT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPROUT: An interactive authoring tool for LLM-assisted programming tutorials</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific LLM-based authoring tool that assists users in creating code tutorials with visualization of LLM behavior; reported to improve user satisfaction and content quality in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sprout: an interactive authoring tool for generating programming tutorials with the visualization of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-assisted content generation with interactive authoring and visualization, applied to programming tutorial creation</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>SPROUT provides interactive authoring support and visualizes LLM generation to help users craft programming tutorials; included here as an example of LLM-based authoring UIs applicable to curriculum creation domains.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>programming tutorial authoring (education)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Authoring multi-part tutorials where outputs must be coherent and pedagogically structured; complexity lies in content structuring and explanation coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Cited SPROUT study reported improved satisfaction and output quality relative to unfacilitated LLM use (details in original paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Original SPROUT paper reported improvements in user satisfaction and quality of generated content; numeric values not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not discussed in this paper beyond general commentary on LLM output quality concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>SPROUT was previously evaluated and found to significantly improve satisfaction and output quality; cited here as related prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Domain-specific interactive authoring tools that visualize and structure LLM outputs can improve user satisfaction and content quality, supporting the argument for UI-focused solutions in LLM-assisted curriculum design.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Directgpt: A direct manipulation interface to interact with large language models <em>(Rating: 2)</em></li>
                <li>Sprout: an interactive authoring tool for generating programming tutorials with the visualization of large language models <em>(Rating: 2)</em></li>
                <li>Skill-oriented and performance-driven adaptive curricula for training in robot-assisted surgery using simulators: A feasibility study <em>(Rating: 2)</em></li>
                <li>Beyond search engines: Can large language models improve curriculum development? <em>(Rating: 1)</em></li>
                <li>Hybrid humanai curriculum development for personalised informal learning environments <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2032",
    "paper_id": "paper-279392089",
    "extraction_schema_id": "extraction-schema-49",
    "extracted_data": [
        {
            "name_short": "UI Predefined",
            "name_full": "User Interface with Predefined Commands",
            "brief_description": "A direct-manipulation GUI that exposes curated, clickable commands for LLM-assisted course-outline generation, letting users apply grouped expert-derived actions, edit outlines in an interactive table, and perform undo/redo and batch operations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "LLM-generated via predefined command-driven interface with human-in-the-loop editing",
            "curriculum_method_description": "Course outlines are generated by issuing structured commands (grouped into four functional groups) from a GUI; each command triggers the LLM with a prompt that includes the current course outline and the user-issued command. The UI enforces a structured output format and assigns identifiers to generated topics for precise referencing; users can manually edit, reorder, and accept LLM outputs (human oversight integrated). Prompting followed OpenAI best-practices: a system prompt defined persona, task, prompt component delimiters, and output format.",
            "llm_model_used": "gpt-4o-2024-08-06",
            "domain_environment": "curriculum/course-outline development (educational course planning)",
            "is_interactive_text_environment": false,
            "is_compositional": false,
            "task_complexity_description": "Moderate complexity: designing a course outline consisting of course title, learning outcomes, and lists of topics (multiple items can be edited, reordered, batched), requiring domain knowledge and iterative refinement rather than multi-step environment interactions.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "UI Open (open/dynamic command UI), ChatGPT-style replica (open-webui control)",
            "performance_metrics": "Usability (SUS): 86.75 (mean); Workload (NASA RTLX mean): 2.25. Statistical comparisons: Workload UI Predefined vs ChatGPT p &lt; 0.03; UI Predefined vs UI Open p &lt; 0.02. SUS differences: UI Predefined vs ChatGPT p &lt; 0.009; UI Predefined vs UI Open p &lt; 0.002.",
            "learning_speed_comparison": "Not reported (no learning-speed/convergence metrics for curriculum generation were measured).",
            "generalization_performance": "Not reported.",
            "task_diversity_analysis": "Not reported.",
            "prerequisite_identification": "Not discussed or evaluated.",
            "intermediate_task_generation": "Not reported (UI supports topic refinement but no explicit intermediate/bridging task generation was described).",
            "llm_limitations_observed": "Paper reiterates general LLM concerns (inaccurate, wrong or ethically questionable outputs) and addresses them via human oversight; no specific hallucination examples from the experiment are reported.",
            "computational_cost": "Not reported.",
            "human_expert_evaluation": "Evaluated with 20 educators in a within-subjects study using SUS and NASA RTLX; UI Predefined achieved highest SUS (86.75) and lowest workload (2.25). Participants reported reduced typing burden and higher ease of use.",
            "key_findings_summary": "The predefined-command UI produced significantly better usability and lower cognitive workload than both the open-command UI and the ChatGPT-style control; curated, task-specific commands plus structured outputs and direct manipulation reduced effort and improved user satisfaction in curriculum creation.",
            "uuid": "e2032.0"
        },
        {
            "name_short": "UI Open",
            "name_full": "User Interface with Open/Dynamic Commands",
            "brief_description": "A direct-manipulation GUI that allows users to compose, drag-and-drop, save, and execute dynamic commands (via a chat box) that are automatically contextualized with the current course outline for LLM-assisted generation and refinement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "LLM-generated via dynamic open/chat commands integrated into the UI with contextualization of the course outline",
            "curriculum_method_description": "Users create flexible, local or global commands through a chat box; commands are executed with the current outline context automatically included in prompts. Commands can be dragged to specific topics for localization, saved for reuse, and applied to sections; the UI translates LLM outputs into the interactive outline format. Prompting again used a system prompt (persona, task, format) and included current outline and user command.",
            "llm_model_used": "gpt-4o-2024-08-06",
            "domain_environment": "curriculum/course-outline development (educational course planning)",
            "is_interactive_text_environment": false,
            "is_compositional": false,
            "task_complexity_description": "Moderate complexity with higher interaction flexibility: supports localized commands per topic, saving/reusing commands, and more open-ended LLM interactions requiring user understanding of command composition.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "UI Predefined, ChatGPT-style replica (open-webui control)",
            "performance_metrics": "Usability (SUS): 70.75 (mean); Workload (NASA RTLX mean): 3.00. UI Open ranked second; improvement over ChatGPT was non-significant. Detailed p-values: UI Predefined vs UI Open SUS p &lt; 0.002.",
            "learning_speed_comparison": "Not reported.",
            "generalization_performance": "Not reported.",
            "task_diversity_analysis": "Not reported.",
            "prerequisite_identification": "Not discussed.",
            "intermediate_task_generation": "Not reported (UI supports command composition but no explicit intermediate task generation evaluation).",
            "llm_limitations_observed": "No specific failure cases reported; participants reported a steeper learning curve for the more flexible interface, implying usability limitations rather than LLM-specific failures.",
            "computational_cost": "Not reported.",
            "human_expert_evaluation": "Evaluated with 20 educators; SUS 70.75 and NASA RTLX 3.00. Participants valued flexibility but noted increased learning overhead compared to predefined commands.",
            "key_findings_summary": "The open/dynamic command UI provided greater flexibility but incurred a steeper learning curve and worse usability/workload than the predefined-command UI; it did not significantly outperform the ChatGPT-style control.",
            "uuid": "e2032.1"
        },
        {
            "name_short": "ChatGPT Replica",
            "name_full": "ChatGPT-style Control Interface (open-webui replica)",
            "brief_description": "A control interface emulating the standard ChatGPT textual interaction used for comparison; implemented with open-webui to allow monitoring and controlled model usage in the experiment.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generation_method": "LLM-generated via standard text-prompting interaction (ChatGPT-like interface)",
            "curriculum_method_description": "Participants used a conventional text-based chat interface to prompt the model to produce or refine course outlines. The control used open-webui to emulate ChatGPT's UI for the study; prompts and interactions relied on user-crafted text prompts (no direct manipulation constructs).",
            "llm_model_used": null,
            "domain_environment": "curriculum/course-outline development (educational course planning)",
            "is_interactive_text_environment": false,
            "is_compositional": false,
            "task_complexity_description": "Moderate complexity: users must craft textual prompts and iteratively refine outputs via chat to create course outlines.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "UI Predefined, UI Open",
            "performance_metrics": "Usability (SUS): 69.00 (mean); Workload (NASA RTLX mean): 3.30. ChatGPT-style control had the highest workload and lowest usability among tested interfaces.",
            "learning_speed_comparison": "Not reported.",
            "generalization_performance": "Not reported.",
            "task_diversity_analysis": "Not reported.",
            "prerequisite_identification": "Not discussed.",
            "intermediate_task_generation": "Not reported.",
            "llm_limitations_observed": "Paper notes general concerns with LLM outputs (accuracy, ethical issues) and the heavy prompt-engineering burden on users when using text-only interfaces.",
            "computational_cost": "Not reported.",
            "human_expert_evaluation": "Evaluated with 20 educators who generally reported higher workload and lower usability relative to the two direct-manipulation UIs; some preference due to familiarity but participants requested features like command reuse and better output presentation.",
            "key_findings_summary": "The standard text-based ChatGPT-style interface showed the highest perceived workload and lowest usability compared to DM-based UIs; reducing prompt-engineering burden via UI design improves educator experience.",
            "uuid": "e2032.2"
        },
        {
            "name_short": "gpt-4o",
            "name_full": "gpt-4o-2024-08-06 (OpenAI)",
            "brief_description": "The LLM model used to generate course-outline content in both UI Predefined and UI Open during the experiment; prompted with system persona and contextual outline information.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generation_method": "LLM-generated content via system + user prompts; used within UI-driven workflows to create/modify course outlines",
            "curriculum_method_description": "A system prompt established persona, delimiters, task specification (course outline curation), and output format; current course outline and user-issued commands were included in each prompt to ensure contextual responses; UI parsed LLM output into structured outline items with identifiers.",
            "llm_model_used": "gpt-4o-2024-08-06",
            "domain_environment": "curriculum/course-outline development",
            "is_interactive_text_environment": false,
            "is_compositional": false,
            "task_complexity_description": "Generative text tasks focused on producing structured lists and refinements (titles, outcomes, topic lists) rather than multi-step environment interactions.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Used within UI Predefined and UI Open; compared indirectly to ChatGPT-style control which used open-webui replica (model unspecified).",
            "performance_metrics": "LLM-specific output quality not quantitatively measured; evaluation focused on user workload and usability when interacting with UIs that use this model.",
            "learning_speed_comparison": "Not applicable / not reported.",
            "generalization_performance": "Not reported.",
            "task_diversity_analysis": "Not reported.",
            "prerequisite_identification": "Not discussed.",
            "intermediate_task_generation": "Not reported.",
            "llm_limitations_observed": "Paper cites general LLM limitations (inaccuracies and ethical issues) and motivates human oversight; no experiment-specific model failure cases detailed.",
            "computational_cost": "Not reported.",
            "human_expert_evaluation": "Educators evaluated interfaces that used this model; judgments concerned usability and workload rather than direct model quality metrics.",
            "key_findings_summary": "The model was successfully integrated into two DM-based UIs; interface design (predefined vs open commands) strongly affected educator workload and perceived usability despite using the same underlying LLM.",
            "uuid": "e2032.3"
        },
        {
            "name_short": "LDA-based topic extraction",
            "name_full": "Latent Dirichlet Allocation (LDA) topic extraction methods",
            "brief_description": "Traditional topic-modelling methods previously used to extract covered topics from educational resources to support curriculum development, mentioned as prior work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generation_method": "Topic-extraction from educational resources using LDA to recommend topics or structure curricula (traditional ML-based assistance)",
            "curriculum_method_description": "LDA applied to educational resource corpora to discover latent topics and suggest curriculum topics/structures; referenced as earlier, domain-specific approaches complementary to or preceding LLM approaches.",
            "llm_model_used": null,
            "domain_environment": "educational resource analysis for curriculum recommendation",
            "is_interactive_text_environment": null,
            "is_compositional": null,
            "task_complexity_description": "Extraction of latent topic structures from document corpora; complexity depends on corpus size and topic granularity but is algorithmic rather than interactive.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Mentioned alongside other traditional ML models (e.g., Random Forest) in prior work.",
            "performance_metrics": "Not reported in this paper (mentioned as prior approaches; specific metrics in cited works not reproduced here).",
            "learning_speed_comparison": "Not reported.",
            "generalization_performance": "Not reported.",
            "task_diversity_analysis": "Not reported here.",
            "prerequisite_identification": "Not discussed in this paper.",
            "intermediate_task_generation": "Not reported.",
            "llm_limitations_observed": "Not applicable (traditional method).",
            "computational_cost": "Not reported.",
            "human_expert_evaluation": "Referenced prior studies combined such methods with human experts for curriculum recommendation; no new human evaluation in this paper.",
            "key_findings_summary": "LDA and other traditional ML methods have been used previously to extract topics and assist human experts in curriculum design; the present paper positions LLM-based interfaces as a more flexible, modern alternative while noting quality and usability concerns with LLMs.",
            "uuid": "e2032.4"
        },
        {
            "name_short": "Adaptive surgical curricula (Prior)",
            "name_full": "Skill-oriented and performance-driven adaptive curricula for training in robot-assisted surgery using simulators: A feasibility study",
            "brief_description": "A prior study that used an adaptive AI algorithm to create real-time curricula for robot-assisted surgery simulators based on learner feedback, showing improved learning outcomes in a small participant set.",
            "citation_title": "Skill-oriented and performance-driven adaptive curricula for training in robot-assisted surgery using simulators: A feasibility study",
            "mention_or_use": "mention",
            "curriculum_generation_method": "Adaptive AI algorithm that generates real-time curricula based on learner performance/feedback",
            "curriculum_method_description": "An adaptive system tailored simulator training curricula in real time to learner feedback and performance metrics, aiming to personalise and optimize training progression in surgical simulation (reported as prior work).",
            "llm_model_used": null,
            "domain_environment": "robot-assisted surgery simulators (medical training)",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "High complexity: surgical training involves multiple sub-skills, multi-step procedures, and fine motor/decision-making components; adaptation required across many skill dimensions.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Compared to standard (non-adaptive) curricula in the cited feasibility study (small N).",
            "performance_metrics": "Cited as demonstrating improved learning outcomes for students using AI-generated curricula; specific numerical metrics were reported in the original cited study (not reproduced here).",
            "learning_speed_comparison": "Original cited work reported improved learning outcomes, implying faster or better acquisition; this paper does not reproduce numeric comparisons.",
            "generalization_performance": "Not discussed here; see original study for details.",
            "task_diversity_analysis": "Not discussed in this paper.",
            "prerequisite_identification": "Not discussed here.",
            "intermediate_task_generation": "Likely used in the adaptive sequencing but specifics are in the cited paper.",
            "llm_limitations_observed": "Not applicable (adaptive AI, not LLM-based), though cited as limited by small participant numbers.",
            "computational_cost": "Not reported here.",
            "human_expert_evaluation": "Original study had limited participants but reported improved outcomes; this paper references that result to motivate AI-based curricula.",
            "key_findings_summary": "Prior adaptive-AI curricula in high-complexity domains (robot-assisted surgery) showed promise in improving student outcomes, motivating exploration of more advanced AI (LLMs) plus usable UIs for curriculum development.",
            "uuid": "e2032.5"
        },
        {
            "name_short": "DirectGPT",
            "name_full": "DirectGPT (Direct Manipulation interface for LLMs)",
            "brief_description": "A previously proposed direct-manipulation user interface for interacting with LLMs that provides continuous object representation and prompt control, reported to improve task completion time and user satisfaction over traditional chat interfaces.",
            "citation_title": "Directgpt: A direct manipulation interface to interact with large language models",
            "mention_or_use": "mention",
            "curriculum_generation_method": "UI design approach (direct-manipulation) to structure and control LLM interactions rather than a curriculum generation algorithm",
            "curriculum_method_description": "Direct-manipulation UI principles (continuous representation, physical actions, rapid reversible operations, visible effects) applied to LLM interaction to reduce prompt-engineering burden and improve usability; used as inspiration for the UIs developed in this paper.",
            "llm_model_used": null,
            "domain_environment": "general LLM interaction interfaces (applied to creative/authoring tasks)",
            "is_interactive_text_environment": null,
            "is_compositional": null,
            "task_complexity_description": "Not directly about curricula; focuses on improving interactive control of LLM outputs.",
            "is_curriculum_adaptive": null,
            "baseline_comparisons": "Original DirectGPT study compared to traditional chat UIs (reported better task completion times and higher satisfaction).",
            "performance_metrics": "Original study reported improved task completion times and higher user satisfaction (not re-reported numerically here).",
            "learning_speed_comparison": "Not applicable.",
            "generalization_performance": "Not applicable.",
            "task_diversity_analysis": "Not applicable.",
            "prerequisite_identification": "Not applicable.",
            "intermediate_task_generation": "Not applicable.",
            "llm_limitations_observed": "Not discussed here beyond motivating improved UI design to mitigate prompt-engineering issues.",
            "computational_cost": "Not reported.",
            "human_expert_evaluation": "DirectGPT was previously evaluated with experts (12 in cited study) and showed better task completion times and satisfaction; referenced here as related work.",
            "key_findings_summary": "Direct-manipulation UI principles applied to LLM interaction can materially reduce user burden and improve efficiency, motivating the DM-based UIs developed and tested in this paper.",
            "uuid": "e2032.6"
        },
        {
            "name_short": "SPROUT",
            "name_full": "SPROUT: An interactive authoring tool for LLM-assisted programming tutorials",
            "brief_description": "A domain-specific LLM-based authoring tool that assists users in creating code tutorials with visualization of LLM behavior; reported to improve user satisfaction and content quality in prior work.",
            "citation_title": "Sprout: an interactive authoring tool for generating programming tutorials with the visualization of large language models",
            "mention_or_use": "mention",
            "curriculum_generation_method": "LLM-assisted content generation with interactive authoring and visualization, applied to programming tutorial creation",
            "curriculum_method_description": "SPROUT provides interactive authoring support and visualizes LLM generation to help users craft programming tutorials; included here as an example of LLM-based authoring UIs applicable to curriculum creation domains.",
            "llm_model_used": null,
            "domain_environment": "programming tutorial authoring (education)",
            "is_interactive_text_environment": null,
            "is_compositional": null,
            "task_complexity_description": "Authoring multi-part tutorials where outputs must be coherent and pedagogically structured; complexity lies in content structuring and explanation coherence.",
            "is_curriculum_adaptive": null,
            "baseline_comparisons": "Cited SPROUT study reported improved satisfaction and output quality relative to unfacilitated LLM use (details in original paper).",
            "performance_metrics": "Original SPROUT paper reported improvements in user satisfaction and quality of generated content; numeric values not reproduced here.",
            "learning_speed_comparison": "Not applicable.",
            "generalization_performance": "Not reported here.",
            "task_diversity_analysis": "Not reported.",
            "prerequisite_identification": "Not discussed here.",
            "intermediate_task_generation": "Not discussed here.",
            "llm_limitations_observed": "Not discussed in this paper beyond general commentary on LLM output quality concerns.",
            "computational_cost": "Not reported.",
            "human_expert_evaluation": "SPROUT was previously evaluated and found to significantly improve satisfaction and output quality; cited here as related prior work.",
            "key_findings_summary": "Domain-specific interactive authoring tools that visualize and structure LLM outputs can improve user satisfaction and content quality, supporting the argument for UI-focused solutions in LLM-assisted curriculum design.",
            "uuid": "e2032.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Directgpt: A direct manipulation interface to interact with large language models",
            "rating": 2
        },
        {
            "paper_title": "Sprout: an interactive authoring tool for generating programming tutorials with the visualization of large language models",
            "rating": 2
        },
        {
            "paper_title": "Skill-oriented and performance-driven adaptive curricula for training in robot-assisted surgery using simulators: A feasibility study",
            "rating": 2
        },
        {
            "paper_title": "Beyond search engines: Can large language models improve curriculum development?",
            "rating": 1
        },
        {
            "paper_title": "Hybrid humanai curriculum development for personalised informal learning environments",
            "rating": 1
        }
    ],
    "cost": 0.0153955,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Designing Effective LLM-Assisted Interfaces for Curriculum Development
13 Jun 2025</p>
<p>Abdolali Faraji abdolali.faraji@tib.eu 
Leibniz Information Centre for Science and Technology (TIB)</p>
<p>Mohammadreza Tavakoli reza.tavakoli@tib.eu 0000-0002-3557-9345
Leibniz Information Centre for Science and Technology (TIB)</p>
<p>Mohammad Moein mohammad.moein@tib.eu 0000-0002-7368-0794
Leibniz Information Centre for Science and Technology (TIB)</p>
<p>Mohammadreza Molavi mohammadreza.molavi@tib.eu 0000-0002-3285-8226
Leibniz Information Centre for Science and Technology (TIB)</p>
<p>Gbor Kismihk gabor.kismihok@tib.eu 0009-0006-0423-0729
Leibniz Information Centre for Science and Technology (TIB)</p>
<p>Designing Effective LLM-Assisted Interfaces for Curriculum Development
13 Jun 20250C138C979F8DA16E4847362CC111FAD0arXiv:2506.11767v1[cs.CY]User-centered designLLM user interfaceCurriculum development
Large Language Models (LLMs) have the potential to transform the way a dynamic curriculum can be delivered.However, educators face significant challenges in interacting with these models, particularly due to complex prompt engineering and usability issues, which increase workload.Additionally, inaccuracies in LLM outputs can raise issues around output quality and ethical concerns in educational content delivery.Addressing these issues requires careful oversight, best achieved through cooperation between human and AI approaches.This paper introduces two novel User Interface (UI ) designs, UI Predefined and UI Open, both grounded in Direct Manipulation (DM ) principles to address these challenges.By reducing the reliance on intricate prompt engineering, these UIs improve usability, streamline interaction, and lower workload, providing a more effective pathway for educators to engage with LLMs.In a controlled user study with 20 participants, the proposed UIs were evaluated against the standard ChatGPT interface in terms of usability and cognitive load.Results showed that UI Predefined significantly outperformed both ChatGPT and UI Open, demonstrating superior usability and reduced task load, while UI Open offered more flexibility at the cost of a steeper learning curve.These findings underscore the importance of user-centered design in adopting AI-driven tools and lay the foundation for more intuitive and efficient educator-LLM interactions in online learning environments.</p>
<p>Introduction</p>
<p>Online learning has become indispensable in modern education, revolutionizing the way knowledge is accessed and disseminated.Its popularity has surged in recent years due to several factors.The convenience of learning at one's own pace and from any location has made it particularly appealing to busy individuals and those with limited access to traditional educational institutions [8].Moreover, the demand for rapid access to up-to-date information on various topics, including skills, tasks at hand, and hobbies, has made online learning a highly effective and efficient educational method [22].This highlights the importance of dynamic up-to-date curriculum development in the context of online learning [21].Maintaining curricula dynamically to align with the rapid pace of societal changes and learning needs poses significant challenges [21].The burden on teachers to continuously update their content is substantial, as it requires extensive time and effort [6].Additionally, accessing high-quality, valid information from the vast amount of available online resources is also challenging, making it difficult to ensure the accuracy and relevance of the curriculum content [1].</p>
<p>The emergence of LLMs has sparked optimism for addressing the challenges associated with dynamic curriculum development [15].Despite the potential of LLM-based methods, they are questionable in terms of the output quality [6,2] (i.e.inaccurate, wrong, or even ethically questionable outputs).This has led to a growing interest in investigating the collaborative Human-AI approaches that combine the strengths of LLMs with human expertise [21].Such collaboration between AI and human educators enables a sustainable, future-ready approach to curriculum development [19].</p>
<p>One of the primary limitations of LLM usage by experts lies in their textbased UIs, which are often not user-friendly and impose a higher cognitive load [7].Interacting with these text-based UIs relies heavily on textual prompts that need to be optimized.This technology-centered method (in contrast to the usercentered method) requires careful wording and precise references to the object of interest, which results in more difficult optimization [4].This leads to challenges like 1) indirect engagement due to limited direct access to the object of interest, 2) semantic distance in expressing intent in written form, and 3) articulatory distance between prompts and their intended actions [14].Addressing these challenges through user-friendly LLM interfaces is essential to facilitate content development and maximize AI's potential [6,14].</p>
<p>This paper proposes two novel UI designs to address the challenges of LLMbased curriculum development, specifically those related to their interfaces.These proposed UIs aimed to optimize the interaction between educators and LLMs, potentially surpassing the capabilities of existing interfaces.Furthermore, by conducting a comprehensive evaluation, we sought to determine whether these UI solutions could enhance the efficiency and effectiveness of curriculum development processes in terms of usability and workload.Therefore, our main contributions are:</p>
<p>-Designing and prototyping UIs for LLM, specifically ChatGPT, which aim at enhancing the usability of LLMs for educators, when it comes to curriculum development -Conducting an experiment to evaluate the proposed UIs against the default ChatGPT UI in terms of usability, effectiveness, and cognitive load</p>
<p>Related Work</p>
<p>In this section, we cover the related research, which influenced key components of our conceptual and technical work.We begin by discussing AI-based curricula development.Next, we explore the development of LLM-based UIs.Finally, we examine the research on human-computer interaction within the context of AI systems, focusing on how to optimize the user experience.</p>
<p>Curricula Development</p>
<p>Previous research has explored the integration of AI algorithms into curriculum development.For instance, [13] used an adaptive AI algorithm to create realtime curricula for robot-assisted surgery using simulators, based on learners' feedback.Although the number of participants was limited, their results demonstrated improved learning outcomes for students using these AI-generated curricula.Additionally, both [16] and [10] employed Latent Dirichlet Allocation (LDA) to extract covered topics from educational resources to support curriculum development processes.Moreover, [21] utilized various traditional machine learning models (e.g., LDA and Random Forest) to assist human experts in developing curricula by recommending learning topics and related educational resources.It can be observed that previous research efforts primarily relied on traditional machine learning methods, limiting their focus to specific educational domains.</p>
<p>LLM-based UIs</p>
<p>To address the challenges of interacting with LLMs, researchers have explored various UI approaches.For instance, [14] proposed the DirectGPT UI to enhance the usability of LLMs by providing features like continuous output representation and prompt control.Through a user study with 12 experts, they demonstrated that DirectGPT users achieved better task completion times and reported higher satisfaction compared to traditional interfaces.Additionally, [5] explored the impact of different slider types on user control over generative models, highlighting the importance of UI design for effective interaction.</p>
<p>Beyond general-purpose LLMs, researchers have also investigated LLM-based UI applications in specific domains.[12] developed SPROUT, a tool designed to assist users in creating code tutorials using LLMs.Their study found that SPROUT significantly improved user satisfaction and the quality of generated content.Furthermore, [23] demonstrated the potential of LLMs in video editing workflows, emphasizing the need for intuitive UIs to facilitate this process.</p>
<p>Human-Computer Interaction for AI Systems</p>
<p>While AI solutions, particularly LLMs, have opened up numerous possibilities that were previously unimaginable, the development of many AI-based systems continues to be hindered by a "technology-centric" approach rather than a "usercentric" approach [25,18].</p>
<p>As already mentioned in the introduction, interaction with LLMs usually happens using textual prompts which introduce different challenges (i.e.indirect engagement, semantic distance, and articulatory distance).These challenges closely resemble issues that caused the shift from command-line interfaces to Direct Manipulation (DM) interfaces (like graphical user interfaces) in the 1980s [9].Shneiderman [20] defined DM interfaces with four key traits: (DM1).Continuous representation of the object of interest (DM2).Physical actions (e.g.movement and selection by mouse, joystick, touch screen, etc.) instead of complex syntax (DM3).Rapid, incremental, reversible operations whose impact on the object of interest is immediately visible (DM4).Users should recognize the actions they can do instead of learning complex syntax</p>
<p>We use the term DM in our paper to refer to these four pillars in the user interface.</p>
<p>Lessons Learned</p>
<p>As discussed in the Curricula Development subsection, there is a clear need for developing and evaluating more advanced and up-to-date AI-driven solutions (such as those based on large language models) for curriculum development, given their demonstrated potential in these educational contexts [6].However, as we highlighted, these solutions must be complemented with user-friendly interfaces, as text-based interaction with LLMs can be less than ideal for users [14].Consequently, UI principles (e.g., DM), as explored earlier, should be carefully considered when designing such interfaces for LLM-based curricula development.</p>
<p>Method</p>
<p>To address text-based interface challenges, we applied DM principles to UIs designed for course outline creation, assisting educators in defining course titles, learning outcomes, and related topic lists.Below, we describe the two developed UIs, the prompt engineering techniques used, and the control group's standard ChatGPT UI.</p>
<p>UI 1: Predefined Commands</p>
<p>UI Predefined offers educators clickable buttons with predefined commands in a familiar graphical user interface (GUI ) layout.These commands, curated from expert inputs in European Union educational projects  As it is visible in Figure 1, the course outline representation in UI Predefined is enhanced with an interactive table format (aligned with DM1) which replaces static text, improving clarity and interaction.Command execution is transparent, with only the updated outline sections displaying a loading effect.A meatball menu next to each item supports contextual actions (DM2, DM4), while checkboxes enable batch operations (DM3).Users can directly edit course titles or outcomes, rearrange topics with drag-and-drop, and add/remove topics, aligning with DM2 and DM4.Undo/redo functionality supports rapid, reversible interactions consistent with DM3.</p>
<p>UI 2: Open Commands</p>
<p>UI Open was developed as an alternative to UI Predefined, retaining its core visual structure (as shown in Figure 2) while addressing key limitations, particularly the lack of direct user interaction with the LLM.To overcome this limitation, a dynamic command mechanism was introduced, allowing users to engage directly with the LLM.These command functions include three key enhancements: (1) automatic integration of the course outline context, ensuring LLM awareness of the structure; (2) a drag-and-drop interface for associating commands with specific course elements; and (3) the ability to save and reuse commands, reducing repetition and improving time-efficiency.To enhance user-friendliness, UI Open incorporates a chat box at the bottom of the course outline interface.The outline representation remains largely unchanged (DM1), allowing manual edits via a click-based interface (DM2).Unlike UI Predefined, it removes predefined buttons, offering a more flexible approach.The chat box enables users to input, execute, and save dynamic commands for reuse, supporting DM2 and DM3.Commands can be global (affecting the entire outline) or local (applied to specific sections), adjusted by dragging topics into the chat box (DM2), as shown in Figure 3.</p>
<p>Prompt Engineering</p>
<p>To achieve the desired results from an LLM in our UIs, prompts were tailored based on OpenAI's best practices for prompt engineering [17].To be more specific, using the gpt-4o-2024-08-06 model (the latest model at the time of the experiment), a system prompt was applied to (1) define the LLM's persona, (2) delimit prompt components, (3) specify the task (course outline curation), and (4) establish the output format.Moreover, the current course outline and user-issued commands were always included in prompts, allowing the LLM to Fig. 3: UI Open: 1) User is creating a command while dragging a topic for localization 2) Command is executed and saved 3) Any other topic can be dragged to the saved command.respond within context.The output format adhered to a defined structure, translated by the UI engine for display.To ensure consistency, each topic generated by the LLM was assigned an identifier, enabling precise referencing throughout interactions.</p>
<p>ChatGPT Replica Tool</p>
<p>To establish a control application for comparison with Predefined and Open UIs, we required a tool that emulated the ChatGPT interface.Direct usage of ChatGPT was unsuitable, as it did not allow for monitoring user interactions, nor did it guarantee the use of the same model utilized in our UIs.After exploring alternatives, we opted for open-webui 2 , an open-source project designed for interacting with LLMs.This tool offers an experience closely comparable to the original ChatGPT interface, making it a suitable choice for our experiment.</p>
<p>Evaluation</p>
<p>We conducted a user study to evaluate the effectiveness and usability of the UIs: UI Predefined, UI Open, and a control interface replicating ChatGPT3 .Participants from diverse educational backgrounds used each UI and completed a questionnaire capturing their experience and background.Below, we detail the experimental design, procedures, and analytical methods.</p>
<p>Participants</p>
<p>The study involved 20 participants, with teaching experience ranging from 1 to 21 years, providing a broad spectrum of perspectives on the usability and effectiveness of the UIs.They were evenly distributed by gender, with an equal number of females and males.Each participant had a minimum of five years of professional experience across diverse fields, including but not limited to Computer Science, Mathematics, Nursing, and Soft Skills.Additionally, participants exhibited varying degrees of familiarity with ChatGPT, ranging from low to very high proficiency.This range of teaching and technological backgrounds enabled us to collect feedback that reflects a diverse set of educational contexts.The demographic distribution of the participants is depicted in Figure 4.</p>
<p>Questionnaire</p>
<p>The evaluation focused on two primary objectives: (1) assessing perceived workload and (2) measuring interface usability.We used the Raw NASA Task Load Index (NASA RTLX) to quantify task load and the System Usability Scale (SUS) to evaluate usability, both established methods in HCI research [11,3].NASA RTLX measures perceived workload in the dimensions of mental demand, physical demand, temporal demand, performance, effort, and frustration.SUS on the other hand, covers the ease of use, efficiency, consistency, and learnability.Data was collected through a three-step questionnaire: participants first completed the NASA RTLX to assess workload, followed by the SUS for usability evaluation, and finally answered an open-ended question for additional feedback.This approach combined quantitative and qualitative insights into the UIs' effectiveness and usability.</p>
<p>Experiment Steps</p>
<p>Each participant followed a structured process to evaluate the three AI-powered UIs for course outline development:  2. Background Information ( 5 min).Demographic data, including teaching experience and prior ChatGPT familiarity, were collected.</p>
<p>Reference Course Outline ( 30 min).</p>
<p>Participants created a baseline course outline in their teaching subject for comparison with different UI outputs.</p>
<ol>
<li>Curriculum Development with UIs (3 rounds,  30 min each).Participants used each of the three UIs in randomized order, for a fair evaluation, spending 20 minutes per UI to create course outlines, followed by completing an evaluation form.</li>
</ol>
<p>Final Evaluation ( 5 min).</p>
<p>Participants provided suggestions for improvement and shared challenges.</p>
<p>Results and Discussion</p>
<p>Results</p>
<p>All 20 participants completed the study 4 .Results showed UI Predefined significantly outperformed ChatGPT in workload, usability, and efficiency, while UI Open ranked second with a non-significant improvement over ChatGPT.We used the Wilcoxon signed-rank test [24] to analyze differences in workload and usability, with statistical analysis conducted in Python 3.12 using scipy.p-values were reported to assess significance.</p>
<p>Workload and Performance To compute the NASA RTLX scores, we transformed the Performance dimension into Performance 1 by subtracting the original value from 8, ensuring lower scores indicated better performance (Performance 1 would be between 1 to 7 like other dimensions).UI Predefined had the lowest workload with a mean score of 2.25, followed by UI Open (3.00) and ChatGPT with the highest workload (3.30).The adjusted Performance metric further emphasized the efficiency gap, with ChatGPT showing a consistently higher workload compared to both developed UIs.These findings highlight the superior efficiency and reduced cognitive load of UI Predefined and UI Open over the control.Detailed results and p-values are presented in Table 1 and visualized in Figure 5. Usability The System Usability Scale (SUS) results show UI Predefined achieving the highest usability score of 86.75, well above the "Excellent" threshold [3].</p>
<p>In comparison, UI Open scored 70.75, and ChatGPT scored 69.00, both within the "OK" category.These findings highlight the superior ease of use and participant satisfaction with UI Predefined.Detailed p-values are in Table 2, with results visualized in Figure 6.</p>
<p>Discussion</p>
<p>Our findings indicated that the UI with predefined commands outperformed the others in both usability (SUS) and workload (NASA RTLX) dimensions.This superiority was attributed to its ability to streamline the course outline creation process.Eight participants specifically noted that predefined commands significantly reduced their typing burden, while ten emphasized the overall ease of use.While the UI with open commands also received positive feedback for its flexibility, participants acknowledged a steeper learning curve.This suggests that a hybrid approach, combining predefined and open commands, might offer the optimal balance of usability and adaptability.Although the ChatGPT UI received some preference due to participants' familiarity, all participants agreed that features like command definition, reuse, and better output presentation were essential for enhancing their overall experience.</p>
<p>Despite the promising results of our study, there are limitations that must be acknowledged.The experiment was constrained to controlled environments, which may not accurately reflect the dynamic and diverse settings in which educators interact with LLMs in real-world scenarios.On the other hand, the</p>
<p>Conclusion</p>
<p>In conclusion, this paper highlighted the growing role of online learning and the potential of LLMs in education.While LLMs enhance accessibility and personalization, they also pose challenges, particularly for educators.Key concerns include (1) the accuracy of LLM outputs and (2) the complexity of effective interaction.To address these, we proposed and evaluated two user interfaces, UI Predefined and UI Open, designed with DM principles to simplify educator interaction.These interfaces reduce reliance on prompt engineering and improve usability.Additionally, they mitigate the risks associated with output quality by fostering Human and AI collaboration, integrating a human check to ensure accuracy and alignment with the teacher standards.Our findings revealed that the UI Predefined significantly outperformed Chat-GPT, showcasing its potential to streamline educational interactions with a set of expert-derived commands that reduce cognitive load and enhance efficiency and usability.Although UI Open also showed improvements over ChatGPT by allowing more direct, context-rich interactions for educators, it did not exhibit a statistically significant advantage.</p>
<p>Building on our findings, future research should explore a hybrid approach that integrates elements of both UI Open and UI Predefined, balancing structured guidance with open-ended interaction.Additionally, expanding the study to a more diverse group of educators across various real-world settings will enhance the generalizability of the results.Finally, the impact of different UI designs on the learning processes should be investigated.This will help to improve learning as the ultimate goal of educational applications.</p>
<p>Fig. 1 :
1
Fig. 1: An example of a course outline representation in UI Predefined.Four groups of predefined commands are visible: group-1) course-related commands, group-2) batch commands, group-3) single-topic refinement commands, and group-4) content generation with respect to a single topic.</p>
<p>Fig. 2 :
2
Fig. 2: An example of a course outline being developed by a user with dynamic commands in UI Open.In this figure, Command 3 is being executed on Topic 2.</p>
<p>1 .
1
Study Introduction ( 10 min).Participants were briefed on the study's objectives, focusing on the workload and usability of the UIs.</p>
<p>Fig. 4 :
4
Fig. 4: Background information of the participants.</p>
<p>Fig. 5 :
5
Fig. 5: The average results for each metric in NASA RTLX.Lower values mean a lower workload.(Performance 1 is defined as 8  Performance)</p>
<p>Fig. 6 :
6
Fig. 6: UIs' SUS Scores.Min, max, 1st, 2nd, and 3rd quartiles are shown.The dotted lines are mean.</p>
<p>1 , support key tasks in creating and refining course outlines.Commands in UI Predefined are organized into four functional groups, presented as clickable actions, as depicted in Figure 1.The first group includes commands for modifying course-level information
1 Projectdetailscanbefoundhere:https://www.tib.eu/en/research-development/research-groups-and-labs/learning-and-skill-analytics/projects</p>
<p>Table 1 :
1
Workload Comparison.
Comparison|Difference| p-valueUI Predefined ChatGPT 2.25 3.301.05&lt; 0.03UI Predefined UI Open 2.25 3.000.75&lt; 0.02UI Open 3.00ChatGPT 3.300.30-</p>
<p>Table 2 :
2
SUS Comparison.study was conducted on a limited number of educators, potentially limiting its diversity.However, we tried to mitigate this limitation by inviting educators from diverse areas, ensuring representation across different backgrounds and demographics.These limitations suggest that future research should encompass a wider range of user backgrounds and real-world testing to enhance the applicability and robustness of UI designs in practical educational contexts.
Usability Comparison |Difference| p-valueUI Predefined ChatGPT 86.75 69.0017.75&lt; 0.009UI Predefined UI Open 86.75 70.7516.00&lt; 0.002UI Open 70.75ChatGPT 69.001.75-
open-webui repository: https://github.com/open-webui/open-webui
A anonymized demo of the experiment, including different steps, is available under: https://figshare.com/s/faa7fcea8bbf9d93ad1f
The raw data, including the background information of the participants, the questionnaires, and the participants' answers, is available at: https://figshare.com/s/ 76c780adbae8f3d22f07</p>
<p>Evaluating the quality of learning resources: A learnersourcing approach. S Abdi, H Khosravi, S Sadiq, G Demartini, IEEE Transactions on Learning Technologies. 1412021</p>
<p>Llm diagnostic toolkit: Evaluating llms for ethical issues. M Bahrami, R Sonoda, R Srinivasan, 2024 International Joint Conference on Neural Networks (IJCNN). IEEE2024</p>
<p>An empirical evaluation of the system usability scale. A Bangor, P T Kortum, J T Miller, Intl. Journal of Human-Computer Interaction. 2462008</p>
<p>Unleashing the potential of prompt engineering in large language models: a comprehensive review. B Chen, Z Zhang, N Langren, S Zhu, arXiv:2310.147352023arXiv preprint</p>
<p>Ganslider: How users control generative models for images using multiple sliders with and without feedforward information. H Dang, L Mecke, D Buschek, Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. the 2022 CHI Conference on Human Factors in Computing Systems2022</p>
<p>Can we trust aigenerated educational content? comparative analysis of human and ai-generated learning resources. P Denny, H Khosravi, A Hellas, J Leinonen, S Sarsa, arXiv:2306.105092023arXiv preprint</p>
<p>K Feng, Q V Liao, Z Xiao, J W Vaughan, A X Zhang, D W Mcdonald, arXiv:2401.09051Canvil: Designerly adaptation for llm-powered user experiences. 2024arXiv preprint</p>
<p>Future trends in the design strategies and technological affordances of e-learning. B Gros, F J Garca-Pealvo, Learning, design, and technology: An international compendium of theory, research, practice, and policy. Springer2023</p>
<p>Direct manipulation interfaces. E L Hutchins, J D Hollan, D A Norman, Human-computer interaction. 141985</p>
<p>Analysis of computer science textbooks by topic modeling and dynamic time warping. T Kawamata, Y Matsuda, T Sekiya, K Yamaguchi, 2021 IEEE International Conference on Engineering. </p>
<p>. Wuhan Ieee, Hubei Province, 10.1109/TALE52509.2021.9678834Dec 2021China</p>
<p>A survey on measuring cognitive workload in human-computer interaction. T Kosch, J Karolus, J Zagermann, H Reiterer, A Schmidt, P W Woniak, ACM Computing Surveys. 5513s2023</p>
<p>Sprout: an interactive authoring tool for generating programming tutorials with the visualization of large language models. Y Liu, Z Wen, L Weng, O Woodman, Y Yang, W Chen, IEEE Transactions on Visualization and Computer Graphics. 2024</p>
<p>Skill-oriented and performance-driven adaptive curricula for training in robot-assisted surgery using simulators: A feasibility study. A Mariani, E Pellegrini, E De Momi, IEEE Transactions on Biomedical Engineering. 682Feb 2021</p>
<p>Directgpt: A direct manipulation interface to interact with large language models. D Masson, S Malacria, G Casiez, D Vogel, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2024</p>
<p>Beyond search engines: Can large language models improve curriculum development?. M Moein, M Molavi, A Faraji, M Tavakoli, G Kismihk, 19th European Conference on Technology Enhanced Learning, EC-TEL 2020. Krems, AustriaSpringerSeptember 16-20, 2024. 2024</p>
<p>Extracting topics from open educational resources. M Molavi, M Tavakoli, G Kismihk, Addressing Global Challenges and Quality Education: 15th European Conference on Technology Enhanced Learning, EC-TEL 2020. Proceedings. Heidelberg, GermanySpringerSeptember 14-18, 2020. 202015</p>
<p>. OpenAI: Prompt engineering -openai api. 2024. Sep. 2024</p>
<p>Six humancentered artificial intelligence grand challenges. Ozmen Garibay, O Winslow, B Andolina, S Antona, M Bodenschatz, A Coursaris, C Falco, G Fiore, S M Garibay, I Grieman, K , International Journal of Human-Computer Interaction. 3932023</p>
<p>Towards human-ai collaboration in the competencybased curriculum development process: The case of industrial engineering and management education. A Padovano, M Cardamone, Computers and Education: Artificial Intelligence. 71002562024</p>
<p>Direct manipulation: A step beyond programming languages. B Shneiderman, Computer. 16081983</p>
<p>Hybrid humanai curriculum development for personalised informal learning environments. M Tavakoli, A Faraji, M Molavi, T Mol, S Kismihk, G , LAK22: 12th International Learning Analytics and Knowledge Conference. 2022</p>
<p>An aibased open recommender system for personalized labor market driven education. M Tavakoli, A Faraji, J Vrolijk, M D Molavi, S T Mol, G Kismihk, Advanced Engineering Informatics. 521015082022</p>
<p>Lave: Llm-powered agent assistance and language augmentation for video editing. B Wang, Y Li, Z Lv, H Xia, Y Xu, R Sodhi, Proceedings of the 29th International Conference on Intelligent User Interfaces. the 29th International Conference on Intelligent User Interfaces2024</p>
<p>R F Woolson, Wilcoxon signed-rank test. 20058</p>
<p>Transitioning to human interaction with ai systems: New challenges and opportunities for hci professionals to enable humancentered ai. W Xu, M J Dainoff, L Ge, Z Gao, International Journal of Human-Computer Interaction. 3932023</p>            </div>
        </div>

    </div>
</body>
</html>
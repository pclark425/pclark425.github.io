<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2916 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2916</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2916</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-271244867</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.12784v1.pdf" target="_blank">AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases</a></p>
                <p><strong>Paper Abstract:</strong> LLM agents have demonstrated remarkable performance across various applications, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact with environments. Current agents typically utilize a memory module or a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and instances with similar embeddings from knowledge bases to inform task planning and execution. However, the reliance on unverified knowledge bases raises significant concerns about their safety and trustworthiness. To uncover such vulnerabilities, we propose a novel red teaming approach AgentPoison, the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or RAG knowledge base. In particular, we form the trigger generation process as a constrained optimization to optimize backdoor triggers by mapping the triggered instances to a unique embedding space, so as to ensure that whenever a user instruction contains the optimized backdoor trigger, the malicious demonstrations are retrieved from the poisoned memory or knowledge base with high probability. In the meantime, benign instructions without the trigger will still maintain normal performance. Unlike conventional backdoor attacks, AgentPoison requires no additional model training or fine-tuning, and the optimized backdoor trigger exhibits superior transferability, in-context coherence, and stealthiness. Extensive experiments demonstrate AgentPoison's effectiveness in attacking three types of real-world LLM agents: RAG-based autonomous driving agent, knowledge-intensive QA agent, and healthcare EHRAgent. On each agent, AgentPoison achieves an average attack success rate higher than 80% with minimal impact on benign performance (less than 1%) with a poison rate less than 0.1%.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2916.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2916.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-Driver</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent-Driver (autonomous driving LLM agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based autonomous driving agent that uses a retrieval-augmented long-term memory of past driving experiences (key-value experience database) as in-context demonstrations to plan trajectories and issue actions; used in this paper as a victim LLM agent for poisoning attacks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A language agent for autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Agent-Driver</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM backbone for understanding/planning combined with a memory / RAG module holding past experience key-value pairs; retrieved demonstrations are prepended as in-context examples and a trajectory planner (fine-tuned LLaMA3-8b in experiments) produces actions which are executed via tools/vehicle interface.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>LLaMA3 (fine-tuned LLaMA3-8b used for trajectory planner; experiments also evaluate GPT-3.5 backbones)</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (long-term memory / experience database)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Corpus of key-value pairs D = {(k_i, v_i)} where each key is a query/experience descriptor and value is associated demonstration (experience). A single encoder E_q maps both queries and keys to a shared embedding space; retrieval returns top-K keys by cosine similarity and their values serve as in-context demonstrations for the LLM planner. Experiments used a memory containing ~23k experiences (domain-specific numeric-heavy strings).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Dense embedding-based nearest-neighbors (K-NN) in the embedding space induced by a single encoder E_q; cosine similarity ranking. Retrievers trained either end-to-end or with contrastive losses (DPR/ANCE/BGE/REALM/ORQA evaluated).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Approximately 23,000 experience entries in the published Agent-Driver dataset used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Past driving experiences (query-experience key and associated demonstration/solution pairs), i.e., trajectories, sensor/state descriptions and associated corrective actions or outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Under AGENTPOISON attack (optimized trigger + poisoned memory): ASR-r (retrieval success rate) = 80.0%, ASR-a (target action generation) = 68.5%, ASR-t (end-to-end target impact, e.g., dangerous trajectory) = 56.8%; benign accuracy (ACC, without trigger) = 91.1% (Table 2, GPT-3.5 / contrastive retriever experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Retrieval-augmented memory is both central to agent performance and a practical attack surface: by poisoning a tiny fraction of the memory (<0.1%) with crafted demonstrations and an optimized trigger, the attacker can cause high retrieval rates of poisoned entries (ASR-r ~80%) and substantial end-to-end impact (~57% ASR-t) while leaving benign utility nearly intact (~1% drop). Mapping triggered queries to a unique compact region in the embedding space dramatically increases malicious retrieval probability.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Vulnerable to corpus poisoning: unverified or third-party-hosted memory can be poisoned by an adversary. Triggers are somewhat sensitive to small token-level perturbations (letter injections reduce ASR), though resilient to rephrasing and single-word insertions. The RAG reliance means retrieval corruption can produce unsafe actions despite in-built safety filters.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Paper contrasts retriever training schemes (end-to-end vs contrastive) and reports that optimized triggers transfer better across embedders trained with similar schemes; no direct comparison between fundamentally different memory models (e.g., graph vs episodic) is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2916.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2916.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct-StrategyQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct agent evaluated on StrategyQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ReAct-style LLM agent (reasoning + acting) that employs a RAG knowledge base of Wikipedia passages for multi-step commonsense question answering (StrategyQA); used here to test poisoning of the RAG knowledge base.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct (applied to StrategyQA)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM agent using chain-of-thought-like internal reasoning (ReAct) combined with retrieval: for each user query the agent retrieves passages from a knowledge base (RAG) and interleaves reasoning and acting steps to answer multi-hop commonsense questions (StrategyQA).</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>GPT-3.5 (also evaluated with LLaMA3 in some experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>StrategyQA (knowledge-intensive multi-step QA benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>StrategyQA is a multi-step commonsense question-answering dataset requiring implicit multi-hop reasoning and use of external background knowledge (curated Wikipedia passages used as retrieval corpus).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented knowledge base (RAG passages)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Knowledge base of passages (key-value pairs) drawn from Wikipedia (~10k passages used for experiments). A single encoder maps queries and keys into embeddings; retrieved passages (top-K) are provided as in-context evidence for the ReAct reasoning process.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Dense retriever K-NN in embedding space (cosine similarity) using embedders trained via end-to-end or contrastive schemes (DPR, REALM, etc. evaluated); top-K passages returned.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Approximately 10,000 Wikipedia passages curated for StrategyQA experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Wikipedia passages / text snippets indexed as keys/values; relevant knowledge supporting multi-step reasoning for QA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Under AGENTPOISON attack (examples from Table 2): ASR-r = 65.5%, ASR-a = 73.6%, ASR-t = 58.6%, benign accuracy ACC = 65.7% (contrastive retriever / GPT-3.5 experimental setting reported in Table 2). Overall across agents AGENTPOISON achieved ~82% average ASR-r and ~63% average ASR-t; ReAct results are lower than Agent-Driver in retrieval success but still substantial.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>The RAG knowledge base is an effective mechanism for in-context reasoning but is susceptible to targeted poisoning; AGENTPOISON can cause the agent to retrieve poisoned passages and produce wrong/unhelpful answers, achieving substantial target-action generation rates even when poisoning only a handful of passages (e.g., 4 poisoned instances in experiments). Triggers optimized for one embedder transfer to others.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>RAG reliance creates vulnerability to corpus poisoning; some retriever variants and embedder training schemes affect transferability of triggers. Perplexity-based or rephrasing defenses reduce but do not eliminate attack success.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>The paper evaluates transferability across retriever training schemes (end-to-end vs contrastive) and finds better trigger transfer among embedders with similar training schemes (e.g., REALM/ORQA versus DPR/ANCE/BGE). No evaluation of non-RAG memory architectures is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2916.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2916.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EHRAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EHRAgent (EHR record-management LLM agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent for electronic health record (EHR) management that uses a small long-term memory of past EHR experiences / records (key-value entries) as a retrieval corpus to guide decisions and actions such as SQL code generation; used as a target to show poisoning of small memories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ehragent: Code empowers large language models for few-shot complex tabular reasoning on electronic health records</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>EHRAgent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM agent for EHR tasks that retrieves relevant patient-record experiences / examples from a memory unit (key-value experience pairs) and uses them as in-context examples to reason and produce structured outputs (e.g., SQL commands). The agent dynamically updates memory in its original form; experiments augment memory for robustness testing.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>GPT-3.5 (also evaluated with LLaMA3 backbones in the paper's cross-backbone experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented experience memory (few-shot examples of EHR operations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Key-value experience database: originally initialized with only a few (4) experiences in the original EHRAgent; for red-teaming experiments the authors augmented the memory to ~700 experiences sampled from successful trials to make poisoning more challenging. Retrieval uses the same embedding-based K-NN mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Dense embedding nearest-neighbor retrieval (cosine similarity) via a single encoder mapping both queries and keys; retrievers experimented with include DPR and REALM among others.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Original system: 4 experiences; experiments augmented to ~700 experiences to make attacks realistic.</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>EHR experience examples: query-example pairs, example SQL operations, prior patient-record manipulations or responses that serve as in-context demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Under AGENTPOISON attack (Table 2): ASR-r = 98.9%, ASR-a = 97.9%, ASR-t = 58.3%, benign accuracy ACC = 72.9% (contrastive retriever / GPT-3.5 experimental setting). Notably, EHRAgent's small/original memory made baseline attacks easier; after augmentation AGENTPOISON still achieves very high retrieval success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Small EHR memories are particularly easy to dominate by poisoned entries; even after augmenting memory to 700 examples, AGENTPOISON attained very high retrieval success (ASR-r ~99%) and target-action generation (ASR-a ~98%). This highlights that memory size alone does not fully mitigate poisoning if triggers are optimized to form unique embedding clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Vulnerable to poisoning especially when memory size is small; even larger memories can be compromised if poisoned entries are crafted to cluster in embedding space. High PPL (perplexity) for some triggers and sensitivity to certain perturbations observed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>No direct comparison with non-RAG memory architectures; paper shows that poisoning success transfers across different dense retrievers and is somewhat correlated with retriever training scheme similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Finmem: A performance-enhanced llm trading agent with layered memory and character design <em>(Rating: 2)</em></li>
                <li>Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modal large language model <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Ehragent: Code empowers large language models for few-shot complex tabular reasoning on electronic health records <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2916",
    "paper_id": "paper-271244867",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [
        {
            "name_short": "Agent-Driver",
            "name_full": "Agent-Driver (autonomous driving LLM agent)",
            "brief_description": "An LLM-based autonomous driving agent that uses a retrieval-augmented long-term memory of past driving experiences (key-value experience database) as in-context demonstrations to plan trajectories and issue actions; used in this paper as a victim LLM agent for poisoning attacks.",
            "citation_title": "A language agent for autonomous driving",
            "mention_or_use": "use",
            "agent_name": "Agent-Driver",
            "agent_description": "LLM backbone for understanding/planning combined with a memory / RAG module holding past experience key-value pairs; retrieved demonstrations are prepended as in-context examples and a trajectory planner (fine-tuned LLaMA3-8b in experiments) produces actions which are executed via tools/vehicle interface.",
            "base_llm_model": "LLaMA3 (fine-tuned LLaMA3-8b used for trajectory planner; experiments also evaluate GPT-3.5 backbones)",
            "base_llm_size": "8B",
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": true,
            "memory_type": "retrieval-augmented (long-term memory / experience database)",
            "memory_architecture": "Corpus of key-value pairs D = {(k_i, v_i)} where each key is a query/experience descriptor and value is associated demonstration (experience). A single encoder E_q maps both queries and keys to a shared embedding space; retrieval returns top-K keys by cosine similarity and their values serve as in-context demonstrations for the LLM planner. Experiments used a memory containing ~23k experiences (domain-specific numeric-heavy strings).",
            "memory_retrieval_mechanism": "Dense embedding-based nearest-neighbors (K-NN) in the embedding space induced by a single encoder E_q; cosine similarity ranking. Retrievers trained either end-to-end or with contrastive losses (DPR/ANCE/BGE/REALM/ORQA evaluated).",
            "memory_capacity": "Approximately 23,000 experience entries in the published Agent-Driver dataset used in experiments.",
            "what_is_stored_in_memory": "Past driving experiences (query-experience key and associated demonstration/solution pairs), i.e., trajectories, sensor/state descriptions and associated corrective actions or outcomes.",
            "performance_with_memory": "Under AGENTPOISON attack (optimized trigger + poisoned memory): ASR-r (retrieval success rate) = 80.0%, ASR-a (target action generation) = 68.5%, ASR-t (end-to-end target impact, e.g., dangerous trajectory) = 56.8%; benign accuracy (ACC, without trigger) = 91.1% (Table 2, GPT-3.5 / contrastive retriever experiment).",
            "performance_without_memory": null,
            "has_ablation_study": false,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Retrieval-augmented memory is both central to agent performance and a practical attack surface: by poisoning a tiny fraction of the memory (&lt;0.1%) with crafted demonstrations and an optimized trigger, the attacker can cause high retrieval rates of poisoned entries (ASR-r ~80%) and substantial end-to-end impact (~57% ASR-t) while leaving benign utility nearly intact (~1% drop). Mapping triggered queries to a unique compact region in the embedding space dramatically increases malicious retrieval probability.",
            "memory_limitations": "Vulnerable to corpus poisoning: unverified or third-party-hosted memory can be poisoned by an adversary. Triggers are somewhat sensitive to small token-level perturbations (letter injections reduce ASR), though resilient to rephrasing and single-word insertions. The RAG reliance means retrieval corruption can produce unsafe actions despite in-built safety filters.",
            "comparison_with_other_memory_types": "Paper contrasts retriever training schemes (end-to-end vs contrastive) and reports that optimized triggers transfer better across embedders trained with similar schemes; no direct comparison between fundamentally different memory models (e.g., graph vs episodic) is provided.",
            "uuid": "e2916.0",
            "source_info": {
                "paper_title": "AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "ReAct-StrategyQA",
            "name_full": "ReAct agent evaluated on StrategyQA",
            "brief_description": "A ReAct-style LLM agent (reasoning + acting) that employs a RAG knowledge base of Wikipedia passages for multi-step commonsense question answering (StrategyQA); used here to test poisoning of the RAG knowledge base.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "agent_name": "ReAct (applied to StrategyQA)",
            "agent_description": "An LLM agent using chain-of-thought-like internal reasoning (ReAct) combined with retrieval: for each user query the agent retrieves passages from a knowledge base (RAG) and interleaves reasoning and acting steps to answer multi-hop commonsense questions (StrategyQA).",
            "base_llm_model": "GPT-3.5 (also evaluated with LLaMA3 in some experiments)",
            "base_llm_size": null,
            "text_game_name": "StrategyQA (knowledge-intensive multi-step QA benchmark)",
            "text_game_description": "StrategyQA is a multi-step commonsense question-answering dataset requiring implicit multi-hop reasoning and use of external background knowledge (curated Wikipedia passages used as retrieval corpus).",
            "uses_memory": true,
            "memory_type": "retrieval-augmented knowledge base (RAG passages)",
            "memory_architecture": "Knowledge base of passages (key-value pairs) drawn from Wikipedia (~10k passages used for experiments). A single encoder maps queries and keys into embeddings; retrieved passages (top-K) are provided as in-context evidence for the ReAct reasoning process.",
            "memory_retrieval_mechanism": "Dense retriever K-NN in embedding space (cosine similarity) using embedders trained via end-to-end or contrastive schemes (DPR, REALM, etc. evaluated); top-K passages returned.",
            "memory_capacity": "Approximately 10,000 Wikipedia passages curated for StrategyQA experiments.",
            "what_is_stored_in_memory": "Wikipedia passages / text snippets indexed as keys/values; relevant knowledge supporting multi-step reasoning for QA.",
            "performance_with_memory": "Under AGENTPOISON attack (examples from Table 2): ASR-r = 65.5%, ASR-a = 73.6%, ASR-t = 58.6%, benign accuracy ACC = 65.7% (contrastive retriever / GPT-3.5 experimental setting reported in Table 2). Overall across agents AGENTPOISON achieved ~82% average ASR-r and ~63% average ASR-t; ReAct results are lower than Agent-Driver in retrieval success but still substantial.",
            "performance_without_memory": null,
            "has_ablation_study": false,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "The RAG knowledge base is an effective mechanism for in-context reasoning but is susceptible to targeted poisoning; AGENTPOISON can cause the agent to retrieve poisoned passages and produce wrong/unhelpful answers, achieving substantial target-action generation rates even when poisoning only a handful of passages (e.g., 4 poisoned instances in experiments). Triggers optimized for one embedder transfer to others.",
            "memory_limitations": "RAG reliance creates vulnerability to corpus poisoning; some retriever variants and embedder training schemes affect transferability of triggers. Perplexity-based or rephrasing defenses reduce but do not eliminate attack success.",
            "comparison_with_other_memory_types": "The paper evaluates transferability across retriever training schemes (end-to-end vs contrastive) and finds better trigger transfer among embedders with similar training schemes (e.g., REALM/ORQA versus DPR/ANCE/BGE). No evaluation of non-RAG memory architectures is provided.",
            "uuid": "e2916.1",
            "source_info": {
                "paper_title": "AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "EHRAgent",
            "name_full": "EHRAgent (EHR record-management LLM agent)",
            "brief_description": "An LLM-based agent for electronic health record (EHR) management that uses a small long-term memory of past EHR experiences / records (key-value entries) as a retrieval corpus to guide decisions and actions such as SQL code generation; used as a target to show poisoning of small memories.",
            "citation_title": "Ehragent: Code empowers large language models for few-shot complex tabular reasoning on electronic health records",
            "mention_or_use": "use",
            "agent_name": "EHRAgent",
            "agent_description": "LLM agent for EHR tasks that retrieves relevant patient-record experiences / examples from a memory unit (key-value experience pairs) and uses them as in-context examples to reason and produce structured outputs (e.g., SQL commands). The agent dynamically updates memory in its original form; experiments augment memory for robustness testing.",
            "base_llm_model": "GPT-3.5 (also evaluated with LLaMA3 backbones in the paper's cross-backbone experiments)",
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": true,
            "memory_type": "retrieval-augmented experience memory (few-shot examples of EHR operations)",
            "memory_architecture": "Key-value experience database: originally initialized with only a few (4) experiences in the original EHRAgent; for red-teaming experiments the authors augmented the memory to ~700 experiences sampled from successful trials to make poisoning more challenging. Retrieval uses the same embedding-based K-NN mechanism.",
            "memory_retrieval_mechanism": "Dense embedding nearest-neighbor retrieval (cosine similarity) via a single encoder mapping both queries and keys; retrievers experimented with include DPR and REALM among others.",
            "memory_capacity": "Original system: 4 experiences; experiments augmented to ~700 experiences to make attacks realistic.",
            "what_is_stored_in_memory": "EHR experience examples: query-example pairs, example SQL operations, prior patient-record manipulations or responses that serve as in-context demonstrations.",
            "performance_with_memory": "Under AGENTPOISON attack (Table 2): ASR-r = 98.9%, ASR-a = 97.9%, ASR-t = 58.3%, benign accuracy ACC = 72.9% (contrastive retriever / GPT-3.5 experimental setting). Notably, EHRAgent's small/original memory made baseline attacks easier; after augmentation AGENTPOISON still achieves very high retrieval success rates.",
            "performance_without_memory": null,
            "has_ablation_study": false,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Small EHR memories are particularly easy to dominate by poisoned entries; even after augmenting memory to 700 examples, AGENTPOISON attained very high retrieval success (ASR-r ~99%) and target-action generation (ASR-a ~98%). This highlights that memory size alone does not fully mitigate poisoning if triggers are optimized to form unique embedding clusters.",
            "memory_limitations": "Vulnerable to poisoning especially when memory size is small; even larger memories can be compromised if poisoned entries are crafted to cluster in embedding space. High PPL (perplexity) for some triggers and sensitivity to certain perturbations observed.",
            "comparison_with_other_memory_types": "No direct comparison with non-RAG memory architectures; paper shows that poisoning success transfers across different dense retrievers and is somewhat correlated with retriever training scheme similarity.",
            "uuid": "e2916.2",
            "source_info": {
                "paper_title": "AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "Finmem: A performance-enhanced llm trading agent with layered memory and character design",
            "rating": 2,
            "sanitized_title": "finmem_a_performanceenhanced_llm_trading_agent_with_layered_memory_and_character_design"
        },
        {
            "paper_title": "Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modal large language model",
            "rating": 2,
            "sanitized_title": "ragdriver_generalisable_driving_explanations_with_retrievalaugmented_incontext_learning_in_multimodal_large_language_model"
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Ehragent: Code empowers large language models for few-shot complex tabular reasoning on electronic health records",
            "rating": 2,
            "sanitized_title": "ehragent_code_empowers_large_language_models_for_fewshot_complex_tabular_reasoning_on_electronic_health_records"
        }
    ],
    "cost": 0.015247,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AGENTPOISON: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases</p>
<p>Zhaorun Chen <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#122;&#104;&#97;&#111;&#114;&#117;&#110;&#64;&#117;&#99;&#104;&#105;&#99;&#97;&#103;&#111;&#46;&#101;&#100;&#117;">&#122;&#104;&#97;&#111;&#114;&#117;&#110;&#64;&#117;&#99;&#104;&#105;&#99;&#97;&#103;&#111;&#46;&#101;&#100;&#117;</a> 
University of Chicago</p>
<p>Zhen Xiang 
University of Illinois
Urbana-Champaign</p>
<p>Chaowei Xiao 
University of Wisconsin
Madison</p>
<p>Dawn Song 
University of California
Berkeley</p>
<p>Bo Li 
AGENTPOISON: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases
6B3276120791DF204DBC4A91C3409FF2
LLM agents have demonstrated remarkable performance across various applications, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact with environments.Current agents typically utilize a memory module or a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and instances with similar embeddings from knowledge bases to inform task planning and execution.However, the reliance on unverified knowledge bases raises significant concerns about their safety and trustworthiness.To uncover such vulnerabilities, we propose a novel red teaming approach AGENTPOISON, the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or RAG knowledge base.In particular, we form the trigger generation process as a constrained optimization to optimize backdoor triggers by mapping the triggered instances to a unique embedding space, so as to ensure that whenever a user instruction contains the optimized backdoor trigger, the malicious demonstrations are retrieved from the poisoned memory or knowledge base with high probability.In the meantime, benign instructions without the trigger will still maintain normal performance.Unlike conventional backdoor attacks, AGENTPOISON requires no additional model training or fine-tuning, and the optimized backdoor trigger exhibits superior transferability, in-context coherence, and stealthiness.Extensive experiments demonstrate AGENTPOISON's effectiveness in attacking three types of real-world LLM agents: RAG-based autonomous driving agent, knowledge-intensive QA agent, and healthcare EHRAgent.We inject the poisoning instances into the RAG knowledge base and long-term memories of these agents, respectively, demonstrating the generalization of AGENTPOISON.On each agent, AGENTPOISON achieves an average attack success rate of ≥ 80% with minimal impact on benign performance (≤ 1%) with a poison rate &lt; 0.1%.The code and data is available at https://github.com/BillChan226/AgentPoison.</p>
<p>Introduction</p>
<p>Recent advancements in large language models (LLMs) have facilitated the extensive deployment of LLM agents in various applications, including safety-critical applications such as finance [35], healthcare [1,25,31,27,20], and autonomous driving [6,12,22].These agents typically employ an LLM for task understanding and planning and can use external tools, such as third-party APIs, to execute the plan.The pipeline of LLM agents is often supported by retrieving past knowledge and instances from a memory module or a retrieval-augmented generation (RAG) knowledge base [18].</p>
<p>Despite recent work on LLM agents and advanced frameworks have been proposed, they mainly focus on their efficacy and generalization, leaving their trustworthiness severely under-explored.In particular, the incorporation of potentially unreliable knowledge bases raises significant concerns Figure 1: An overview of the proposed AGENTPOISON framework.(Top) During the inference, the adversary poisons the LLM agents' memory or RAG knowledge base with very few malicious demonstrations, which are highly likely to be retrieved when the user instruction contains an optimized trigger.The retrieved demonstration with spurious, stealthy examples could effectively result in target adversarial action and catastrophic outcomes.(Bottom) Such a trigger is obtained by an iterative gradient-guided discrete optimization.Intuitively, the algorithm aims to map queries with the trigger into a unique region in the embedding space while increasing their compactness.This will facilitate the retrieval rate of poisoned instances while preserving agent utility when the trigger is not present.</p>
<p>regarding the trustworthiness of LLM agents.For example, state-of-the-art LLMs are known to generate undesired adversarial responses when provided with malicious demonstrations during knowledge-enabled reasoning [29].Consequently, an adversary could induce an LLM agent to produce malicious outputs or actions by compromising its memory and RAG such that malicious demonstrations will be more easily retrieved [39].</p>
<p>However, current attacks targeting LLMs, such as jailbreaking [10,40] during testing and backdooring in-context learning [29], cannot effectively attack LLM agents with RAG.Specifically, jailbreaking attacks like GCG [40] encounter challenges due to the resilient nature of the retrieval process, where the impact of injected adversarial suffixes can be mitigated by the diversity of the knowledge base [23].Backdoor attacks such as BadChain [29] utilize suboptimal triggers that fail to guarantee the retrieval of malicious demonstrations in LLM agents, resulting in unsatisfactory attack success rates.</p>
<p>In this paper, we propose a novel red-teaming approach AGENTPOISON, the first backdoor attack targeting generic LLM agents based on RAG.AGENTPOISON is launched by poisoning the long-term memory or knowledge base of the victim LLM agent using very few malicious demonstrations, each containing a valid query, an optimized trigger, and some prescribed adversarial targets (e.g., a dangerous sudden stop action for autonomous driving agents).The goal of AGENTPOISON is to induce the retrieval of the malicious demonstrations when the query contains the same optimized trigger, such that the agent will be guided to generate the adversarial target as in the demonstrations; while for benign queries (without the trigger), the agent performs normally.We accomplish this goal by proposing a novel constrained optimization scheme for trigger generation which jointly maximizes a) the retrieval of the malicious demonstration and b) the effectiveness of the malicious demonstrations in inducing adversarial agent actions.In particular, our objective function is designed to map triggered instances into a unique region in the RAG embedding space, separating them from benign instances in the knowledge base.Such special design endows AGENTPOISON with high ASR even when we inject only one instance in the knowledge base with a single-token trigger.</p>
<p>In our experiments, we evaluate AGENTPOISON on three types of LLM agents for autonomous driving, dialogues, and healthcare, respectively.We show that AGENTPOISON outperforms baseline attacks by achieving 82% retrieval success rate and 63% end-to-end attack success rate with less than 1% drop in the benign performance and with poisoning ratio less than 0.1%.We also find that our trigger optimized for one type of RAG embedder can be transferred to effectively attack other types of RAG embedders.Moreover, we show that our optimized trigger is resilient to diverse augmentations and is evasive to potential defenses based on perplexity examination or rephrasing.Our technical contributions are summarized as follows:</p>
<p>• We propose AGENTPOISON, the first backdoor attack against generic RAG-equipped LLM agents by poisoning their long-term memory or knowledge base with very few malicious demonstrations.</p>
<p>• We propose a novel constrained optimization for AGENTPOISON to optimize the backdoor trigger for effective retrieval of the malicious demonstrations and thus a higher attack success rate.</p>
<p>• We show the effectiveness of AGENTPOISON, compared with four baseline attacks, on three types of LLM agents.AGENTPOISON achieves 82% retrieval success rate and 63% end-to-end attack success rate with less than 1% drop in benign performance with less than 0.1% poisoning ratio.</p>
<p>• We demonstrate the transferability of the optimized trigger among different RAG embedders, its resilience against various perturbations, and its evasiveness against two types of defenses.</p>
<p>Related Work</p>
<p>LLM Agent based on RAG LLM Agents have demonstrated powerful reasoning and interaction capability in many real-world settings, spanning from autonomous driving [22,36,6], knowledgeintensive question-answering [34,26,16], and healthcare [25,1].These agents backboned by LLM can take user instructions, gather environmental information, retrieve knowledge and past experiences from a memory unit to make informed action plan and execute them by tool calling.</p>
<p>Specifically, most agents rely on a RAG mechanism to retrieve relevant knowlegde and memory from a large corpus [19].While RAG has many variants, we mainly focus on dense retrievers and categorize them into two types based on their training scheme: (1) training both the retriever and generator in an end-to-end fashion and update the retriever with the language modeling loss (e.g.REALM [11], ORQA [17]); ( 2) training the retriever using a contrastive surrogate loss (e.g.DPR [14], ANCE [30], BGE [37]).We also consider the black-box OpenAI-ADA model in our experiment.</p>
<p>Red-teaming LLM Agents Extensive works have assessed the safety and trustworthiness of LLMs and RAG by red-teaming them with a variety of attacks such as jailbreaks [40,21,5], backdoor [29,13,33], and poisoning [39,41,39].However, as these works mostly treat LLM or RAG as a simple model and study their robustness individually, their conclusions can hardly transfer to LLM agent which is a much more complex system.Recently a few preliminary works also study the backdoor attacks on LLM agents [32,38], however they only consider poisoning the training data of LLM backbones and fail to assess the safety of more capable RAG-based LLM agents.In terms of defense, [28] seeks to defend RAG from corpus poisoning by isolating individual retrievals and aggregate them.However, their method can hardly defend AGENTPOISON as we can effectively ensure all the retrieved instances are poisoned.As far as we are concerned, we are the first work to red-team LLM agents based on RAG systems.Please refer to Appendix A.5 for more details.</p>
<p>3 Method</p>
<p>Preliminaries and Settings</p>
<p>We consider LLM agents with a RAG mechanism based on corpus retrieval.For a user query q, we retrieve knowledge or past experiences from a memory database D, containing a set of query-solution (key-value) pairs {(k 1 , v 1 ), . . ., (k |D| , v |D| )}.Different from conventional passage retrieval where query and document are usually encoded with different embedders [18], LLM agents typically use a single encoder E q to map both the query and the keys into an embedding space.Thus, we retrieve a subset E K (q, D) ⊂ D containing the K most relevant keys (and their associated values) based on their (cosine) similarity with the query q in the embedding space induced by E q , i.e., the K keys in D with the minimum Eq(q) ⊤ Eq(k)</p>
<p>||Eq(q)||•||Eq(k)|| .These K retrieved key-value pairs are used as the in-context learning demonstrations for the LLM backbone of the agent to determine an action step by a = LLM(q, E K (q, D)).The LLM agent will execute the generated action by calling build-in tools [9] or external APIs.</p>
<p>Threat model</p>
<p>Assumptions for the attacker We follow the standard assumption from previous backdoor attacks against LLMs [13,29] and RAG systems [39,41].We assume that the attacker has partial access to the RAG database of the victim agent and can inject a small number of malicious instances to create a poisoned database D poison (x t ) = D clean ∪ A(x t ).Here, A(x t ) = {( k1 (x t ), v1 ), • • • , ( k|A(xt)| (x t ), v|A(xt)| )} represents the set of adversarial key-value pairs injected by the attacker, where each key here is a benign query injected with a trigger x t .Accordingly, the demonstrations retrieved from the poisoned database for a query q will be denoted by E K (q, D poison (x t )).This assumption aligns with practical scenarios where the memory unit of the victim agent is hosted by a third-party retrieval service2 or directly leverages an unverified knowledge base.For example, an attacker can easily inject poisoned texts by maliciously editing Wikipedia pages [4]).Moreover, we allow the attacker to have white-box access to the RAG embedder of the victim agent for trigger optimization [41].However, we later show empirically that the optimized trigger can easily transfer to a variety of other embedders with high success rates, including a SOTA black-box embedder OpenAI-ADA.</p>
<p>Objectives of the attacker The attacker has two adversarial goals.(a) A prescribed adversarial agent output (e.g.sudden stop for autonomous driving agents or deleting the patient information for electronic healthcare record agents) will be generated whenever the user query contains the optimized backdoor trigger.Formally, the attacker aims to maximize
E q∼πq [1(LLM(q ⊕ x t , E K (q ⊕ x t , D poison (x t ))) = a m )],(1)
where π q is the sample distribution of input queries, a m is the target malicious action, 1(•) is a logical indicator fuction.x t denotes the trigger, and q ⊕ x t denotes the operation of injecting3 the trigger x t into the query q.</p>
<p>(b) Ensure the outputs for clean queries remain unaffected.Formally, the attacker aims to maximize
E q∼πq [1(LLM(q, E K (q, D poison (x t ))) = a b )],(2)
where a b denotes the benign action corresponding to a query q.This is different from traditional DP attacks such as [39] that aim to degrade the overall system performance.</p>
<p>AGENTPOISON</p>
<p>Overview</p>
<p>We design AGENTPOISON to optimize a trigger x t that achieves both objectives of the attacker specified above.However, directly maximizing Eq. (1) and Eq.(2) using gradient-based methods is challenging given the complexity of the RAG procedure, where the trigger is decisive in both the retrieval of demonstrations and the target action generation based on these demonstrations.Moreover, a practical attack should not only be effective but also stealthy and evasive, i.e., a triggered query should appear as a normal input and be hard to detect or remove, which we treat as coherence.</p>
<p>Our key idea to solve these challenges is to cast the trigger optimization into a constrained optimization problem to jointly maximize a) retrieval effectiveness: the probability of retrieving from the poisoning set A(x t ) for any triggered query q ⊕ x t , i.e.,
E q∼πq [1(∃(k, v) ∈ E K (q ⊕ x t , D poison (x t )) ∩ A(x t ))],(3)
and the probability of retrieving from the benign set D clean for any benign query q, b) target generation: the probability of generating the target malicious action a m for triggered query q ⊕ x t when E K (q ⊕ x t , D poison (x t ))) contains key-value pairs from A(x t ), and c) coherence: the textual coherence of q ⊕ x t .Note that a) and b) can be viewed as the two sub-steps decomposed from the optimization goal of maximizing Eq. (1), while a) is also aligned to the maximization of Eq. (2).In particular, we propose a novel objective function for a) where the triggered queries will be mapped to a unique region in the embedding space induced by E q with high compactness between these embeddings.Intuitively, this will minimize the similarity between queries with and without the trigger while maximizing the similarity in the embedding space for any two triggered queries (see  (d).By mapping triggered instances to a unique and compact region in the embedding space, AGENTPOISON effectively retrieves them without affecting other trigger-free instances to maintain benign performance.In contrast, CPA requires a much larger poisoning ratio meanwhile significantly degrading benign utility.Fig. 2).Furthermore, the unique embeddings for triggered queries impart distinct semantic meanings compared to benign queries, enabling easy correlation with malicious actions during in-context learning.Finally, we propose a gradient-guided beam search algorithm to solve the constrained optimization problem by searching for discrete tokens under non-derivative constraints.</p>
<p>Our design of AGENTPOISON brings it two major advantages over existing attacks.First, AGENT-POISON requires no additional model training, which largely lowers the cost compared to existing poisoning attack [32,33].Second, AGENTPOISON is more stealthy than many existing jailbreaking attacks due to optimizing the coherence of the triggered queries.The overview is shown in Fig. 1.</p>
<p>Constrained Optimization Problem</p>
<p>We construct the constrained optimization problem following the key idea in §3.3.1 as the following:
minimize xt L uni (x t ) + λ • L cpt (x t )(4)s.t. L tar (x t ) ≤ η tar (5) L coh (x t ) ≤ η coh(6)
where Eq. ( 4), Eq. ( 5), and Eq. ( 6) correspond to the optimization goals a), b), and c), respectively.The constants η tar and η coh are the upper bounds of L tar and L coh , respectively.Here, all four losses in the constrained optimization are defined as empirical losses over a set Q = {q 0 , • • • , q |Q| } of queries sampled from the benign query distribution π q .</p>
<p>Uniqueness loss The uniqueness loss aims to push triggered queries away from the benign queries in the embedding space.Let c 1 , • • • , c N be the N cluster centers corresponding to the keys of the benign queries in the embedding space, which can be easily obtained by applying (e.g.) k-means to the embeddings of the benign keys.Then the uniqueness loss is defined as the average distance of the input query embedding to all these cluster centers:
L uni (x t ) = − 1 N • |Q| N n=0 qj ∈Q ||E q (q j ⊕ x t ) − c n ||(7)
Note that effectively minimizing the uniqueness loss will help to reduce the required poisoning ratio.</p>
<p>Compactness loss</p>
<p>We define a compactness loss to improve the similarity between triggered queries in the embedding space:
L cpt (x t ) = 1 |Q| qj ∈Q ||E q (q j ⊕ x t ) − E q (x t )||(8)
where
E q (x t ) = 1 |Q| qj ∈Q E q (q j ⊕ x t )
is the average embedding over the triggered queries.The minimization of the compactness loss can further reduce the poisoning ratio.In Fig. 11, we show the procedure for joint minimization of the uniqueness loss and the compactness loss, where the embeddings for the triggered queries gradually form a compact cluster.Intuitively, the embedding of a test query containing the same trigger will fall into the same cluster, resulting in the retrieval of malicious key-value pairs.In comparison, CPA (Fig. 2a) suffers from a low accuracy in retrieving malicious key-value pairs, and it requires a much higher poisoning ratio to address the long-tail distribution of all the potential queries.</p>
<p>Target generation loss We maximize the generation of target malicious action a m by minimizing:
L tar (x t ) = − 1 |Q| qj ∈Q p LLM (a m |[q j ⊕ x t , E K (q j ⊕ x t , D poison (x t ))])(9)
where p LLM (•|•) denotes the output probability of the LLM given the input.While Eq. ( 9) only works for white-box LLMs, we can efficiently approximate L tar (x t ) using finite samples with polynomial complexity.We show the corresponding analysis and proof in Appendix A.4.</p>
<p>Coherence loss</p>
<p>We aim to maintain high readability and coherence with the original texts in each query q for the optimized trigger.This is achieved by minimizing:
L coh (x t ) = − 1 T T i=0 log p LLM b (q (i) |q (&lt;i) )(10)
where q (i) denote the i th token in q ⊕ x t , and LLM b denotes a small surrogate LLM (e.g.gpt-2) in our experiment.Different from suffix optimization that only requires fluency [23], the trigger optimized by AGENTPOISON can be injected into any position of the query (e.g. between two sentences).Thus Eq. ( 10) enforces the embeded trigger to be semantically coherent with the overall sequence [10], thus achieving stealthiness.</p>
<p>Optimization algorithm</p>
<p>We propose a gradient-based approach that optimizes Eq. ( 4) while ensuring Eq. ( 9) and Eq. ( 10) satisfy the soft constraint via a beam search algorithm.The key idea of our optimization algorithm is to iteratively search for a replacement token in the sequence that improves the objective while also satisfying the constraint.Our algorithm consists of the following four steps.</p>
<p>Algorithm 1 AGENTPOISON Trigger Optimization</p>
<p>Require:
query encoder E q , a set of queries Q = {q 0 , • • • , q |Q| }, database cluster centers {c n | n ∈ [1, N ],
target malicious action a m , target LLM, surrogate LLM b , maximum search iteration I max .Ensure: a stealthy trigger that yields high backdoor success rate.L uni ← Eq. (7), L cpt ← Eq. (8) 5:
1: B = {x t0 | x t0 = [t 0 , • • • , t T ]} ▷ Algorithm.t i ← Random([t 0 , • • • t T ]) 6: C τ ← arg min t ′ 1,•••m ∈V ∂L(x tτ ) ▷ Eq.(4) 7:S τ s ∼ soft max t∈Cτ L coh (x tτ ) ▷ Eq.(10) 8:
Update S ′ τ from S τ ▷ Eq. ( 11)
9:
end for 10:
B = arg max t 1,••• ,b ∈S ′ τ {L τ (x tτ ) | L τ (x tτ ) ≤ L τ −1 (x tτ )} 11:
end for Initialization: To ensure context coherence, we initialize the trigger x t0 from a string relevant to the agent task where we treat the LLM as an one-step optimizer and prompt it to obtain b triggers to form the initial beams (Algorithm.4).</p>
<p>Gradient approximation: To handle discrete optimization, for each beam candidate, we follow [8] to first calculate the objective w.r.t.Eq. ( 4) and randomly select a token t i in x t0 to compute an approximation of model output by replacing t i with another token in the vocabulary V, using gradient ∂L, where
L = e ⊺ t ′ i ∇ et i (L uni + λL cpt ).
Then we obtain the top-m candidate tokens to consist the replacement token set C 0 .</p>
<p>Constraint filtering: Then we impose constraint Eq. ( 6) and Eq. ( 5) sequentially.Since determination of η coh highly depends on the data, we follow [23] to first sample s tokens from C 0 to obtain S τ under a distribution where the likelihood for each token is a softmax function of L coh .This ensures the selected tokens possess high coherence while maintaining diversity.Then we further filter S τ w.r.t.Eq. ( 5).We notice that during early iterations most candidates cannot directly satisfy Eq. ( 5), thus instead, we consider the following soft constraint:
S ′ τ = {t i ∈ S τ | L τ tar (t i ) ≤ L τ −1 tar (t i ) or L τ tar (t i ) ≤ η tar }(11)
where τ denotes the τ th iteration.Thus we soften the constraint to require Eq. ( 9) to monotonic increase when Eq. ( 5) is not directly satisfied, which leaves a more diversified candidate set S ′ τ .Token Replacement: Then we calculate L tar for each token in S ′ τ and select the top b tokens that improve the objective Eq. ( 4) to form the new beams.Then we iterate this process until convergence.The overall procedure of the trigger optimization is detailed in Algorithm.1.</p>
<p>Table 1: We compare AGENTPOISON with four baselines over ASR-r, ASR-b, ASR-t, ACC on four combinations of LLM agent backbones: GPT3.5 and LLaMA3-70b (Agent-Driver uses a fine-tuned LLaMA3-8b) and RAG retrievers: end-to-end and contrastive-based.Specifically, we inject 20 poisoned instances with 6 trigger tokens for Agent-Driver, 4 instances with 5 trigger tokens for ReAct-StrategyQA, and 2 instances with 2 trigger tokens for EHRAgent.For ASR, the maximum number in each column is in bold; for ACC, the number within 1% to the non-attack case is in bold.To demonstrate the generalization of AGENTPOISON, we select three types of realworld agents across a variety of tasks: Agent-Driver [22] for autonomous driving, ReAct [34] agent for knowledge-intensive QA, and EHRAgent [25] for healthcare record management.</p>
<p>Memory/Knowledge base: For agent-driver we use its corresponding dataset published in their paper, which contain 23k experiences in the memory unit 4 .For ReAct, we select a more challenging multi-step commonsense QA dataset StrategyQA which involves a curated knowledge base of 10k passages from Wikipedia 5 .For EHRAgent, it originally initializes its knowledge base with only four experiences and updates its memory dynamically.However we notice that almost all baselines have a high attack success rate on the database with such a few entries, we augment its memory unit with 700 experiences that we collect from successful trials to make the red-teaming task more challenging.</p>
<p>Baselines: To assess the effectiveness of AGENTPOISON, we consider the following baselines for trigger optimization: Greedy Coordinate Gradient (GCG) [40], AutoDAN [21], Corpus Poisoning Attack (CPA) [39], and BadChain [29].Specifically, we optimize GCG w.r.t. the target loss Eq. ( 9), and since we observe AutoDAN performs badly when directly optimizing Eq. ( 9), we calibrate its fitness function and augment Eq. ( 9) by Eq. ( 3) with Lagrangian multipliers.And we use the default objective and trigger optimization algorithm for CPA and BadChain.</p>
<p>Evaluation metrics: We consider the following metrics: (1) attack success rate for retrieval (ASR-r), which is the percentage of test instances where all the retrieved demonstrations from the database are poisoned; (2) attack success rate for the target action (ASR-a), which is the percentage of test instances where the agent generates the target action (e.g., "sudden stop") conditioned on successful We fix the number of tokens to 4 for the former case and the number of poisoned instances to 32 for the latter case.Two metrics ASR-r (retrieval success rate) and ACC (benign utility) are studied.</p>
<p>retrieval of poisoned instances.Thus, ASR-a individually assesses the performance of the trigger w.r.t.inducing the adversarial action.Then we further consider (3) end-to-end target attack success rate (ASR-t), which is the percentage of test instances where the agent achieves the final adversarial impact on the environment (e.g., collision) that depends on the entire agent system, which is a critical metric that distinguishes from previous LLMs attack.Finally, we consider (4) benign accuracy (ACC), which is the percentage of test instances with correct action output without the trigger, which measures the model utility under the attack.A successful backdoor attack is characterized by a high ASR and a small degradation in the ACC compared with the non-backdoor cases.We detail the backdoor strategy and definition of attack targets for each agent in Appendix A.3.1 and Appendix A.1.2,respectively.</p>
<p>Result</p>
<p>AGENTPOISON demonstrates superior attack success rate and benign utility.We report the performance of all methods in Table 1.We categorize the result into two types of LLM backbones, i.e.GPT3.5 and LLaMA3, and two types of retrievers trained via end-to-end loss or contrastive loss.We observe that algorithms that optimize for retrieval i.e.AGENTPOISON, CPA and AutoDAN has better ASR-r, however CPA and AutoDAN also hampers the benign utility (indicated by low ACC) as they invariably degrade all retrievals.As a comparison, AGENTPOISON has minimal impact on benign performance of average 0.74% while outperforming the baselines in terms of retrieval success rate of 81.2% in average, while an average 59.4% generates target actions where 62.6% result in actual target impact to the environment.The high ASR-r and ACC can be naturally attributed to the optimization objective of AGENTPOISON.And considering that these agent systems have in-built safety filters, we denote 62.6% to be a very high success rate in terms of real-world impact.</p>
<p>AGENTPOISON has high transferability across embedders.We assess the transferability of the optimized triggers on five dense retrievers, i.e.DPR [14], ANCE [30], BGE [37], REALM [11], and ORQA [17] to each other and the text-embedding-ada-002 model6 with API-only access.We report the results for Agent-Driver in Fig. 3, and ReAct-StrategyQA and EHRAgent in Fig. 7 and Fig. 8 (Appendix A.2.2).We observe AGENTPOISON has a high transferability across a variety of embedders (even on embedders with different training schemes).We conclude the high transferability results  3: We assess the resilience of the optimized trigger by studying three types of perturbations on the trigger in the input query while keeping the poisoned instances fixed.Specifically, we consider injecting three random letters, injecting one word in the sequence, and rephrasing the trigger while maintaining its semantic meaning.We prompt GPT3.5 to obtain the corresponding perturbations.from our objective in Eq. ( 4) that optimizes for a unique cluster in the embedding space which is also semantically unique on embedders trained with similar data distribution.</p>
<p>AGENTPOISON performs well even when we inject only one instance in the knowledge base with one token in the trigger.We further study the performance of AGENTPOISON w.r.t. the number of poisoned instances in the database and the number of tokens in the trigger sequence, and report the findings in Fig. 4. We observe that after optimization, AGENTPOISON has high ASR-r (62.0% in average) when we only poison one instance in the database.Meanwhile, it also achieves 79.0%ASR-r when the trigger only contains one token.Regardless of the number of poisoned instances or the tokens in the sequence, AGENTPOISON can consistently maintain a high benign utility (ACC≥ 90%).</p>
<p>How does each individual loss contributes to AGENTPOISON?</p>
<p>The ablation result is reported in Table 2, where we disable one component each time.We observe L uni significantly contributes to the high ASR-r in AGENTPOISON while ACC is more sensitive to L cpt where more concentrated qt generally lead to better ACC.Besides, while adding L coh slightly degrades the performance, it leads to better in-context coherence, which can effectively bypass some perplexity-based countermeasures.</p>
<p>AGENTPOISON is resilient to perturbations in the trigger sequence.We further study the resilience of the optimized triggers by considering three types of perturbations in Table 3.We observe AGENTPOISON is resilient to word injection, and slightly compromised to letter injection.This is because letter injection can change over three tokens in the sequence which can completely flip the semantic distribution of the trigger.Notably, rephrasing the trigger which completely change the token sequence also maintains high performance, as long as the trigger semantics is preserved.</p>
<p>How does AGENTPOISON perform under potential defense?We study two types of defense: Perplexity Filter [2] and Query Rephrasing [15] (here we rephrase the whole query which is different from Table 3) which are often used to prevent LLMs from injection attack.We report the ASR-t in Table 4 and full result in Table 6 (Appendix A.2.4).Compared with GCG and Badchain, the trigger optimized by AGENTPOISON is more readable and coherent to the agent context, making it resilient under both defenses.We further justify this observation in Fig. 5 where we compare the perplexity distribution of queries optimized by AGENTPOISON to benign queries and GCG.Compared to GCG, the queries of AGENTPOISON are highly evasive by being inseparable from the benign queries.</p>
<p>Conclusion</p>
<p>In this paper, we propose a novel red-teaming approach AGENTPOISON to holistically assess the safety and trustworthiness of RAG-based LLM agents.Specifically, AGENTPOISON consists of a constrained trigger optimization algorithm that seeks to map the queries into a unique and compact region in the embedding space to ensure high retrieval accuracy and end-to-end attack success rate.Notably, AGENTPOISON does not require any model training while the optimized trigger is highly transferable, stealthy, and coherent.Extensive experiments on three real-world agents demonstrate the effectiveness of AGENTPOISON over four baselines across four comprehensive metrics.</p>
<p>Broader Impacts</p>
<p>In this paper, we propose AGENTPOISON, the first backdoor attack against LLM agents with RAG.The main purpose of this research is to red-team LLM agents with RAG so that their developers are aware of the threat and take action to mitigate it.Moreover, our empirical results can help other researchers to understand the behavior of RAG systems used by LLM agents.Code is released at https://github.com/BillChan226/AgentPoison.</p>
<p>Limitations</p>
<p>While AGENTPOISON is effective in optimizing triggers to achieve high retrieval accuracy and attack success rate, it requires the attacker to have white-box access to the embedder.However, we show empirically that AGENTPOISON can transfer well among different embedders even with different training schemes, since AGENTPOISON optimizes for a semantically unique region in the embedding space, which is also likely to be unique for other embedders as long as they share similar training data distribution.This way, the attacker can easily red-team a proprietary agent by simply leveraging a public open-source embedder to optimize for such a universal trigger.</p>
<p>A Appendix / supplemental material A.1 Experimental Settings</p>
<p>A.1.1 Hyperparameters</p>
<p>The hyperparameters for AGENTPOISON and our experiments are reported in Table 5.Except for obtaining the result in Fig. 4, we keep the number of tokens in the trigger fixed, where we have 6 tokens for Agent-Driver [22], 5 tokens for ReAct-StrategyQA [34], and 2 tokens for EHRAgent [25], and we inject 20 poisoned instances for Agent-Driver, 4 for ReAct, and 2 for EHRAgent across all experiments.The number of tokens in the trigger sequence are mainly determined by the length of the original queries.We inject fewer than 0.1% instances w.r.t. the original number of instances in the database for all attack methods, since we observe that as more instances have been poisoned, it gets harder to distinguish to effectiveness of different methods, as reported in Fig. 4.</p>
<p>A.1.2 Target Definition</p>
<p>We detail the attack target for AGENTPOISON in this section.Specifically, for all three agents, we consider it a success retrieval (thus counted in ASR-r) only if all the retrieved instances (usually k-nearest neighbors) are poisoned demonstrations that we previously injected into the database.Such requirements are practical and necessary for evaluating attack success for retrievals since many agents have certain in-built safety filters to further select useful demonstrations from all the retrieval results (e.g.Agent-Driver [22] instantiates a re-examination process where they use a LLM to select one experience which is most relevant to the retrieved k instances).This way an adversary can certify attack success only if all the retrieved instances are malicious.Recent defense [28] which seeks to certify RAG from corpus poisoning attacks by isolate-then-aggregate further necessitates this requirement on such agent-oriented attacks.By effectively manipulating all Figure 6: A scatter plot which compares AGENTPOISON with four baselines over ASR-r, ACC on four combinations of LLM agent backbones: GPT3.5 and LLaMA3, and retrievers: end-to-end and contrastive-based.Specifically, we inject 20 poisoned instances for Agent-Driver, 4 for ReAct, and 2 for EHRAgent.Specifically, different trigger optimization algorithms are represented with different shapes.green denotes the retriever is trained via end-to-end scheme and blue denotes the retriever is trained via a contrastive surrogate task.</p>
<p>the retrieved demonstrations to be poisoned instances, AGENTPOISON can easily bypass such SOTA defense.</p>
<p>Specifically, we detail the target action and target outcome for attacking each agent.</p>
<p>• Agent-Driver: we denote irresponsible and potentially unsafe driving behaviors to be our attack target.Specifically, target action for the agent to output is sudden stop.And the corresponding real-world outcome is measured by the trajectory deviation in the future three seconds.Mathematically, attack success for ASR-t is indicated by the following indicator function:
1 1 T t+T t ∥ ζ(s) − ζ(s)∥ 2 ds &gt; τ s (12)
where ζ denotes the planned trajectory given by a fine-tuned LLM trajectory planner, and ζ is the ground-truth trajectory.τ s is the preset threshold that determines the maximum deviation of a safe trajectory.• ReAct-StrategyQA: we denote wrong answer or unhelpful information to be our attack target for QA-based agent.As we provide the poison sample following [29] (as detailed in Appendix A.3.1), the target action for the agent to output is as there is a {trigger} in the sequence during its internal reasoning (denoting successfully backdoored in-context learning), and the corresponding outcome is that the agent finishes the episode with such wrong answer or unhelpful answer (e.g., I don't know).• EHRAgent: considering the purpose and safety issues of healthcare record maintenance, we denote deletion to be a highly risky and potentially unsafe operation.Thus we design the target action for the agent such that it should output delete data of patient ID during the reasoning step, and the corresponding outcome is a SQL code command DeleteDB.</p>
<p>A.1.3 Data and Model Preparation</p>
<p>Train/Test split For Agent-Driver, we have randomly sampled 250 samples from its validation set (apart from the 23k samples in the training set); for ReAct agent, we have used the full test set in StrategyQA 8 which consists of 229 samples; and for EHRAgent, we have randomly selected 100  samples from its validation set in our experiment.Besides, the poisoned samples are all sampled from the training set of each agent which does not overlap with the test set.</p>
<p>Retriever As we have categorized the RAG retrievers into two types, i.e. contrastive and end-to-end based on their training scheme, for each agent we have manually selected a representative retriever in each type and report the corresponding results in Table 1.Specifically, for Agent-Driver, as it is a domain-specific task and requires the agent to handle strings that contain a large portion of numbers which distinct from natural language, we have followed [22] and trained both the end-to-end and contrastive embedders using its published training data 9 , where we use the loss described in §A.5.1.And for ReAct-StrategyQA [34] and EHRAgent [25], we have adopted the pre-trained DPR [14] checkpoints 10 as contrastive retriever and the pre-trained REALM [11] checkpoints 11 as end-to-end retriever.</p>
<p>A.2 Additional Result and Analysis</p>
<p>We further detail our analysis by investigating the following six questions.(1) As AGENTPOISON constructs a surrogate task to optimize both Eq.(1) and Eq. ( 2), we aim to ask how well does AGENT-POISON fulfill the objectives of the attacker?(2) What is the attack transferability of AGENTPOISON on ReAct-StrategyQA and EHRAgent?(3) How does the number of trigger tokens influence the optimization gap? (4) How does AGENTPOISON perform under potential defense?(5) What is the distribution of embeddings during the intermediate optimization process of AGENTPOISON?4) (on the right) during the AGENTPOISON optimization w.r.t.different number of trigger tokens.Specifically, we consider the trigger sequence of 2, 5, and 8 tokens.We can denote that while longer triggers generally lead to a higher retrieval success rate, AGENTPOISON could still yield good and stable attack performance even when there are fewer tokens in the trigger sequence.</p>
<p>Table 6: We assess the performance of AGENTPOISON under potential defense.Specifically, we consider two types of defense: a) Perplexity Filter [2], which evaluates the perplexity of the input query and filters out those larger than a threshold; and b) Rephrasing Defense [15], which rephrases the original query to obtain a query that shares the same semantic meaning as the original query.(6) What does the optimized trigger look like?We provide the result and analysis in the following sections.</p>
<p>A.2.1 Balancing ASR-ACC Trade-off</p>
<p>We further visualize the result in Table 1 in Fig. 6 where we focus on ASR-r and ACC.We can see that AGENTPOISON (represented by +) are distribute in the upper right corner which denotes it can achieve both high retrieval success rate (in terms of ASR-r) and benign utility (in terms of ACC) while all other baselines can not achieve both.This result further demonstrates the superior backdoor performance of AGENTPOISON.</p>
<p>A.2.2 Additional Transferability Result</p>
<p>We have provided the additional transferability result on ReAct-StrategyQA and EHRAgent in Fig. 7 and Fig. 8, respectively.We can see that AGENTPOISON generally achieves high attack transferability among different RAG retrievers which further demonstrates its universality for trigger optimization.</p>
<p>A.2.3 Optimization Gap w.r.t. Token Length</p>
<p>We compare the attack performance on ReAct-StrategyQA w.r.t.ASR-r and loss defined in Eq. ( 4) during the AGENTPOISON optimization w.r.t.different number of trigger tokens, and report the result in Fig. 9.We can denote that while triggers with more tokens can generally lead to a higher retrieval success rate, AGENTPOISON could yield a good and consistent attack success rate even if there are very few tokens in the trigger sequence.</p>
<p>A.2.4 Potential Defense</p>
<p>We provide the additional results of the performance of AGENTPOISON under two types of potential defense in Table 6.During our experiment, we adopt the spurious examples as our poisoning strategy for Agent-Driver, and adopt adversarial backdoor as our poisoning strategy for ReAct-StrategyQA and EHRAgent.</p>
<p>Realm</p>
<p>A.3.2 Additional algorithm</p>
<p>The pseudocode for trigger initialization is shown in Algorithm.4 where we use it to generate the initial beams of triggers that are relevant to the task the agent handles.</p>
<p>Each function h ∈ H corresponds to an indicator function of the form 1 fq j (am)&gt;fq j (ar) , where f qj ⊕xt ∈ F .Given a basis {f 1 , f 2 , . . ., f d } for the vector space F , any function f ∈ F can be written as a linear combination of these basis functions:
f = d i=1 α i f i for some coefficients α i .(18)
For each point x k , the condition f qj (a m ) &gt; f qj (a r ) translates to:
d i=1 α i f i (x k , a m ) &gt; d i=1 α i f i (x k , a r ).(19)
This can be rewritten as:
d i=1 α i (f i (x k , a m ) − f i (x k , a r )) &gt; 0. (20) Let g k = f i (x k , a m ) − f i (x k , a r ).
We have m linear inequalities of the form:
d i=1 α i g k,i &gt; 0.(21)
To shatter the set {x 1 , x 2 , . . ., x m }, we need to find coefficients α i such that these m inequalities can realize all possible sign patterns for the m points.However, in a d-dimensional space, we can only have at most d linearly independent inequalities.If m &gt; d + 1, then we have more inequalities than the dimensions of the space, making it impossible to satisfy all possible sign patterns.Thus, m ≤ d + 1.Therefore, the VC dimension of H is at most dim(F ) + 1.</p>
<p>Theorem A.2 (Sample Complexity [3]).Suppose that H is a set of functions from a set X to {0, 1} with finite VC dimension d ≥ 1.Let L be any sample error minimization algorithm for H. Then L is a learning algorithm for H.In particular, if m ≥ d 2 , its sample complexity satisfies:
m L (ϵ, γ) ≤ 64 ϵ 2 2d ln 12 ϵ + ln 4 γ(22)
where m L (ϵ, γ) is the minimum sample size required to ensure that with probability at least 1 − γ, the empirical error is within ϵ of the true error.</p>
<p>Therefore we can combine Lemma 1 and Theorem A.2 to prove the sample complexity bound for L tar (x t ) in Eq. (15).According to Lemma 1, the VC dimension of H is bounded by VCdim(H) ≤ dim(F ) + 1.Then by Theorem A.2, we can denote that for any ϵ &gt; 0 and γ ∈ (0, 1), with at least
N
Therefore, the finite-sample approximation of the target constraint function converges polynomially (to 1/ϵ) to L tar with high probability as the number of samples increases.</p>
<p>Therefore, Theorem A.1 indicates that we can effectively approximate L tar with a polynomially bounded number of samples, and we use function Eq. ( 14) to serve as the constraint for the overall optimization for AGENTPOISON.</p>
<p>A.5 Additional Related Works</p>
<p>A.5.1 Retrieval Augmented Generation</p>
<p>Retrieval Augmented Generation (RAG) [19] is widely adopted to enhance the performance of LLMs by retrieving relevant external information and grounding the outputs and action of the model [22,36].The retrievers used in RAG can be categorized into sparse retrievers (e.g.BM25), where the embedding is a sparse vector which usually encodes lexical information such as word frequency [24]; and dense retrievers where the embedding vectors are dense, which is usually a fine-tuned version of a pre-trained BERT encoder [7].We focus on red-teaming LLM agents with RAG handled by dense retrievers, as they are much more widely adopted in LLM agent systems and have been proved to perform much better in terms of retrieval accuracy [11].</p>
<p>In our discussion, we categorize RAG into two categories based on their training scheme: (1) endto-end training where the retriever is updated using causal language modeling pipeline handled by cross-entropy loss [11,17]; and (2) contrastive surrogate loss where the retriever is trained alone and usually on a held-out training set [30,37].</p>
<p>During end-to-end training, both the retriever and the generator are optimized jointly using the language modeling loss [11].The retriever selects the top K documents E K (q) based on their relevance to the input query q, and the generator conditions on both q and each retrieved document E K (q) to produce the output sequence y (or action a for LLM agent).Therefore the probability of the generated output is given by: p RAG (y|q) ≈ E K (q)∈top-k(p(•|q))</p>
<p>p Eq (E K (q)|q)p LLM (y|q, E K (q)) (24)
= E K (q)∈top-k(p(•|1))
p Eq (E K (q)|q)
N i p LLM (y i |q, E K (q), y 1:i−1 )(25)
Thus correspondingly the training objective is to minimize the negative log-likelihood of the target sequence by optimizing the E q :</p>
<p>L RAG = − log p RAG (y|q)</p>
<p>= − log E K (q)∈top-k(p(•|q))</p>
<p>p Eq (E K (q)|q) N i p LLM (y i |q, E K (q), y 1:i−1 )</p>
<p>This way embedder E q is trained to align with the holistic goal of the generation task.While being effective, the end-to-end training scheme only demonstrates good performance during pre-training which makes the training very costly.</p>
<p>Therefore, extensive works on RAG explore training E k via a surrogate contrastive loss to learn a good ranking function for retrieval.The objective is to create a vector space where relevant pairs of questions and passages have smaller distances (i.e., higher similarity) than irrelevant pairs.The training data consists of instances {⟨k i , v + i , v − i,1 , . . ., v − i,n ⟩} m i , where each instance includes a query key k i , a relevant key k + i , and n irrelevant keys k − i,j .The contrastive loss function is defined as:
L(q i , k + i , k − i,1 , • • • , k − i,n ) = − log e sim(qi,k + i )
e sim(qi,k + i ) + n j=1 e sim(qi,k − i,j )</p>
<p>Specifically, Eq. ( 28) encourages the retriever E q to assign higher similarity scores to positive pairs than to negative pairs, effectively improving the retrieval accuracy.And different embedders often distinguish in their curation of the negative samples [14,37,30].</p>
<p>Figure 2 :
2
Figure 2: We demonstrate the effectiveness of the optimized triggers by AGENTPOISON and compare it with baseline CPA by visualizing their embedding space.The poisoning instances of CPA are shown as blue dots in (a); the poisoning instances of AGENTPOISON during iteration 0, 10, and 15 are shown as red dots and the final sampled instances are shown as blue dots in (b)-(d).By mapping triggered instances to a unique and compact region in the embedding space, AGENTPOISON effectively retrieves them without affecting other trigger-free instances to maintain benign performance.In contrast, CPA requires a much larger poisoning ratio meanwhile significantly degrading benign utility.</p>
<p>4 2 :
2
for τ = 0 to I max do 3:for all x t0 ∈ B do 4:</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Transferability confusion matrix showcasing the performance of the triggers optimized on the source embedder (y-axis) transferring to the target embedder (x-axis) w.r.t.ASR-r (a), ASR-a (b), and ACC (c) on Agent-Driver.We can denote that (1) trigger optimized with AGENTPOISON generally transfer well across dense retrievers; (2) triggers transfer better among embedders with similar training strategy (i.e.end-to-end (REALM, ORQA); contrastive (DPR, ANCE, BGE)).</p>
<p>Figure 5 :
5
Figure 5: Perplexity density distribution of benign, AGENTPOISON and GCG queries.</p>
<p>Figure 7 :
7
Figure 7: Transferability confusion matrix showcasing the performance of the triggers optimized on the source embedder (y-axis) transferring to the target embedder (x-axis) w.r.t.ASR-r (a), ASR-a (b), and ACC (c) on ReAct-StrategyQA.We can denote that (1) trigger optimized with AGENTPOISON generally transfer well across dense retrievers; (2) triggers transfer better among embedders with similar training strategy (i.e.end-to-end (REALM, ORQA); contrastive (DPR, ANCE, BGE)).</p>
<p>Figure 8 :
8
Figure 8: Transferability confusion matrix showcasing the performance of the triggers optimized on the source embedder (y-axis) transferring to the target embedder (x-axis) w.r.t.ASR-r (a), ASR-a (b), and ACC (c) on EHRAgent.We can denote that (1) trigger optimized with AGENTPOISON generally transfer well across dense retrievers; (2) triggers transfer better among embedders with similar training strategy (i.e.end-to-end (REALM, ORQA); contrastive (DPR, ANCE, BGE)).</p>
<p>Figure 9 :
9
Figure 9: Comparing attack performance on ReAct-StrategyQA w.r.t.ASR-r (on the left) and loss defined in Eq. (4) (on the right) during the AGENTPOISON optimization w.r.t.different number of trigger tokens.Specifically, we consider the trigger sequence of 2, 5, and 8 tokens.We can denote that while longer triggers generally lead to a higher retrieval success rate, AGENTPOISON could still yield good and stable attack performance even when there are fewer tokens in the trigger sequence.</p>
<p>Figure 11 :Figure 12 :Figure 13 :
111213
Figure 11: The intermediate trigger optimization process of AGENTPOISON for different embedders on Agent-Driver.Specifically, we demonstrate the benign query embeddings without the trigger and the adversarial query embeddings with the trigger during iteration 0 (initializated), 5, 10, and 15.</p>
<p>Table 2 :
2
An ablation study of the performance w.r.t.individual components in AGENTPOISON.Specifically, we study the case using GPT3.5 backbone and retriever trained with contrastive loss.An additional metric perplexity (PPL) of the triggered queries is considered.Best performance is in bold.Table
MethodAgent-DriverReAct-StrategyQAEHRAgentASR-r ASR-a ASR-t ACC PPL ASR-r ASR-a ASR-t ACC PPL ASR-r ASR-a ASR-t ACC PPLw/o Luni57.4 63.1 51.0 87.8 13.7 25.5 58.6 42.0 57.1 63.7 65.6 88.5 37.7 65.6 643.9w/o Lcpt63.0 64.4 54.0 90.1 14.2 38.6 61.1 47.0 62.8 67.1 82.0 93.4 59.0 72.5 622.5w/o Ltar81.3 61.8 55.1 91.3 14.9 57.1 72.2 45.9 62.0 71.5 90.2 96.7 83.6 75.4 581.0w/o Lcoh83.5 67.7 57.7 91.5 36.6 67.7 77.7 52.8 67.1 81.8 95.4 90.1 70.5 77.0 955.4AGENTPOISON 80.0 68.5 56.8 91.1 14.8 65.5 73.6 58.6 65.7 76.6 98.9 97.9 58.3 72.9 505.0</p>
<p>Table 4 :
4
[15]ormance (ASR-t) under two types of defense: PPL Filter[2]and Query Rephrasing[15].
MethodAgent-DriverReAct-StrategyQAEHRAgentASR-r ASR-a ASR-t ACC ASR-r ASR-a ASR-t ACC ASR-r ASR-a ASR-t ACCLetter injection 46.9 64.2 45.0 91.6 84.9 69.7 57.0 52.1 90.3 95.6 53.8 70.0Word injection 78.4 67.1 52.5 91.3 92.9 73.0 62.4 50.8 93.0 96.8 57.2 72.0Rephrasing66.0 65.1 49.7 91.2 88.0 64.2 58.1 49.6 85.1 83.4 50.0 72.9MethodAgent-DriverReAct-StrategyQAPPL Filter Rephrasing PPL Filter RephrasingBenignAgentPoisonGCGGCG4.613.224.028.0BadChain43.036.942.036.0AGENTPOISON47.250.061.262.0</p>
<p>Table 5 :
5
Hyperparameter Settings for AGENTPOISON
ParametersValueL tar Threshold η tar0.8Number of replacement token m500Number of sub-sampled token s100Gradient accumulation steps30Iterations per gradient optimization 1000Batch size64Surrogate LLMgpt-2 7
For example: https://www.voyageai.com/
In this work, we do not restrict the position for trigger injection, i.e., the trigger is not limited to a suffix.
https://github.com/USC-GVL/Agent-Driver
https://allenai.org/data/strategyqa
https://platform.openai.com/docs/guides/embeddings
https://allenai.org/data/strategyqa
https://github.com/USC-GVL/Agent-Driver
https://github.com/facebookresearch/DPR
https://huggingface.co/docs/transformers/en/model_doc/realm
A.2.5 Intermediate optimization processThe embedding distribution during the intermediate optimization process of AGENTPOISON across different embedders is showcased in Fig.11.We can consistently observed that, regardless of the white-box embedders being optimized, AGENTPOISON can effectively learn a trigger such that the triggers are gradually becoming more unique and compact, which further verifies the effectiveness of AGENTPOISON and the validity of the loss being optimized.A.2.6 Trigger Case StudyWe demonstrate the trigger optimized on GPT3.5 LLM backbone and retriever trained via contrastive loss using different attack algorithms over Agent-Driver[22], ReAct[34], EHRAgent[25]in Table7. Due to our trigger initialization using a relevant string and our coherence loss, our trigger have a better fluency and coherence than the trigger optimized using CPA and GCG.While the trigger optimized by AutoDAN and BadChain have good reliability (since they utilize an LLM for trigger generation), they are not as effective as our algorithm.Table7: A case study of the trigger optimized using different attack algorithms for Agent-Driver[22], ReAct[34], EHRAgent[25].We demonstrate the trigger optimized on GPT3.We design the poisoned instances retrieved from the memory or knowledge base to be adversarial experience, which aligns with our attack target for each agent as enumerated in Appendix A.1.2, while contradicting the safe purposes of the agent tasks themselves.After retrieving from the knowledge base, we showcase the procedure of reasoning for action where the agent places the retrieved malicious demonstrations in the prefix and prompts the LLM backbone for reasoning and action prediction.We mainly consider two types of poisoning strategy, i.e. (1)Algorithm 2 Trigger Initialization 1: function Trigger-Initialization (query-example, agent-task, number-of-tokens) 2:message system ← "You are a helpful and creative assistant to help write relevant string for some LLM Agent tasks."3: message user ← "Please give me a relevant string of number-of-tokens tokens for a agent-task task.Your string should not change the original output of the query query-example when it is being appended."4: return LLM.get_response(message system , message user )A.4 Additional Analysis on Optimization ApproximationGiven the constrained optimization problem defined in §3.3.2:We can directly adopt Eq. (9) to calculate the target action objective L tar (x t ) for white-box models.However, AGENTPOISON can be adapted for black-box LLMs setting by approximating L tar (x t ) via the following finite-sample indicator function.where 1 condition is 1 when the condition is true and 0 otherwise.We demonstrate in Theorem A.1 that AGENTPOISON can efficiently approximate L tar (x t ) with a polynomial sample complexity.Theorem A.1 (Complexity analysis for approximating L tar (x t ) with finite samples).We can provide the following sample complexity bound for approximating L tar (x t ) with finite samples.Let Q denote the potential space of all queries.For any ϵ &gt; 0 and γ ∈ (0, 1), with at leastsamples, we have with probability at least 1 − γ:Proof.Specifically, to prove Theorem A.1, we fist reformulate Eq. (14) in the following form:where a r denotes the runner-up (i.e., second-maximum likelihood) action token output by the target LLM.Then we can define a set of functions F as the class of real-valued functions where each represents the output action distribution p LLM (a|q j ⊕ x t ) conditioned on a query q j sampled from Q and trigger x t .More specifically, each function f can be formulated asTherefore, we can first obtain an upper bound for the VC dimension of H = {1 fq j (am)&gt;fq j (ar) : f qj ∈ F } using the following lemma.Lemma 1 (VC Dimension Bound).Let F be a vector space of real-valued functions, and let H = {1 fq j (am)&gt;fq j t(ar ) : f qj ∈ F }. Then the VC dimension of H satisfies VCdim(H) ≤ dim(F ) + 1.Proof.To show that the VC dimension of H is at most dim(F ) + 1, we need to show that no set of more than dim(F ) + 1 points can be shattered by H.Consider a set of m points {x 1 , x 2 , . . ., x m } in a d-dimensional space where d = dim(F ).Suppose that H can shatter this set of m points.This means that for any way of labeling these m points, there exists a function in H that correctly classifies the points according to those labels.
Conversational health agents: A personalized llm-powered agent framework. Iman Mahyar Abbasian, Azimi, Ramesh Amir M Rahmani, Jain, arXiv:2310.023742023arXiv preprint</p>
<p>Detecting language model attacks with perplexity. Gabriel Alon, Michael Kamfonas, arXiv:2308.141322023arXiv preprint</p>
<p>Neural network learning: Theoretical foundations. Martin Anthony, Peter L Bartlett, Peter L Bartlett, 1999cambridge university press Cambridge9</p>
<p>Nicholas Carlini, Matthew Jagielski, Christopher A Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, Florian Tramèr, arXiv:2302.10149Poisoning web-scale training datasets is practical. 2023arXiv preprint</p>
<p>Pandora: Detailed llm jailbreaking via collaborated phishing agents with decomposed reasoning. Zhaorun Chen, Zhuokai Zhao, Wenjie Qu, Zichen Wen, Zhiguang Han, Zhihong Zhu, Jiaheng Zhang, Huaxiu Yao, ICLR 2024 Workshop on Secure and Trustworthy Large Language Models. 2024</p>
<p>Can Cui, Zichong Yang, Yupeng Zhou, Yunsheng Ma, Juanwu Lu, Lingxi Li, Yaobin Chen, Jitesh Panchal, Ziran Wang, Personalized autonomous driving with large language models: Field experiments. 2024</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Hotflip: White-box adversarial examples for text classification. Javid Ebrahimi, Anyi Rao, Daniel Lowd, Dejing Dou, arXiv:1712.067512017arXiv preprint</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Cold-attack: Jailbreaking llms with stealthiness and controllability. Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, Bin Hu, arXiv:2402.086792024arXiv preprint</p>
<p>Retrieval augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Mingwei Chang, International conference on machine learning. PMLR2020</p>
<p>Ye Jin, Xiaoxi Shen, Huiling Peng, Xiaoan Liu, Jingli Qin, Jiayang Li, Jintao Xie, Peizhong Gao, Guyue Zhou, Jiangtao Gong, Surrealdriver: Designing generative driver agent simulation framework in urban contexts based on large language model. 2023</p>
<p>Backdoor attacks for in-context learning with language models. Nikhil Kandpal, Matthew Jagielski, Florian Tramèr, Nicholas Carlini, arXiv:2307.146922023arXiv preprint</p>
<p>Dense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, arXiv:2004.049062020arXiv preprint</p>
<p>Certifying llm safety against adversarial prompting. Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, Hima Lakkaraju, arXiv:2309.027052023arXiv preprint</p>
<p>Paperqa: Retrieval-augmented generative agent for scientific research. Jakub Lála, O' Odhran, Aleksandar Donoghue, Sam Shtedritski, Samuel G Cox, Andrew D Rodriques, White, arXiv:2312.075592023arXiv preprint</p>
<p>Latent retrieval for weakly supervised open domain question answering. Kenton Lee, Ming-Wei Chang, Kristina Toutanova, arXiv:1906.003002019arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, Proceedings of the 34th International Conference on Neural Information Processing Systems. the 34th International Conference on Neural Information Processing Systems2020</p>
<p>Agent hospital: A simulacrum of hospital with evolvable medical agents. Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, Yang Liu, 2024</p>
<p>Autodan: Generating stealthy jailbreak prompts on aligned large language models. Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao, arXiv:2310.044512023arXiv preprint</p>
<p>A language agent for autonomous driving. Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone, Yue Wang, arXiv:2311.108132023arXiv preprint</p>
<p>Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, Yuandong Tian, arXiv:2404.16873Advprompter: Fast adaptive adversarial prompting for llms. 2024arXiv preprint</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>Ehragent: Code empowers large language models for few-shot complex tabular reasoning on electronic health records. Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce Ho, Carl Yang, May D Wang, 2024</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>. Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, Shekoofeh Azizi, Karan Singhal, Yong Cheng, Le Hou, Albert Webson, Kavita Kulkarni, Sara Mahdavi, Christopher Semturs, Juraj Gottweis, Joelle Barral, Katherine Chou, Greg S Corrado, Yossi Matias, 2024Alan Karthikesalingam, and Vivek Natarajan. Towards conversational diagnostic ai</p>
<p>Chong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, Prateek Mittal, arXiv:2405.15556Certifiably robust rag against retrieval corruption. 2024arXiv preprint</p>
<p>Badchain: Backdoor chain-of-thought prompting for large language models. Zhen Xiang, Fengqing Jiang, Zidi Xiong, Radha Bhaskar Ramasubramanian, Bo Poovendran, Li, arXiv:2401.122422024arXiv preprint</p>
<p>Approximate nearest neighbor negative contrastive learning for dense text retrieval. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, Arnold Overwijk, arXiv:2007.008082020arXiv preprint</p>
<p>Qisen Yang, Zekun Wang, Honghui Chen, Shenzhi Wang, Yifan Pu, Xin Gao, Wenhao Huang, Shiji Song, Gao Huang, Llm agents for psychology: A study on gamified assessments. 2024</p>
<p>Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, Xu Sun, arXiv:2402.11208Watch out for your agents! investigating backdoor threats to llm-based agents. 2024arXiv preprint</p>
<p>Poisonprompt: Backdoor attack on prompt-based large language models. Hongwei Yao, Jian Lou, Zhan Qin, ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2024</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Finmem: A performance-enhanced llm trading agent with layered memory and character design. Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui Zhang, Rong Liu, Jordan W Suchow, Khaldoun Khashanah, 2023</p>
<p>Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modal large language model. Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, Matthew Gadd, arXiv:2402.108282024arXiv preprint</p>
<p>Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, Jian-Yun Nie, arXiv:2310.07554Retrieve anything to augment large language models. 2023arXiv preprint</p>
<p>Yunchao Zhang, Zonglin Di, Kaiwen Zhou, Cihang Xie, Xin Eric, Wang , arXiv:2211.14769Navigation as attackers wish? towards building byzantine-robust embodied agents under federated learning. 2022arXiv preprint</p>
<p>Poisoning retrieval corpora by injecting adversarial passages. Zexuan Zhong, Ziqing Huang, Alexander Wettig, Danqi Chen, arXiv:2310.191562023arXiv preprint</p>
<p>Universal and transferable adversarial attacks on aligned language models. Andy Zou, Zifan Wang, Zico Kolter, Matt Fredrikson, arXiv:2307.150432023arXiv preprint</p>
<p>Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of large language models. Wei Zou, Runpeng Geng, Binghui Wang, Jinyuan Jia, arXiv:2402.078672024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
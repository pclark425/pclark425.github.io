<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2234 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2234</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2234</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-61.html">extraction-schema-61</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <p><strong>Paper ID:</strong> paper-278057024</p>
                <p><strong>Paper Title:</strong> MTAGCN: Multi-Task Graph-Guided Convolutional Network with Attention Mechanism for Intelligent Fault Diagnosis of Rotating Machinery</p>
                <p><strong>Paper Abstract:</strong> : Deep learning (DL)-based methods have shown great success in multi-category fault diagnosis due to their hierarchical networks and automatic feature extraction. However, their superior performance is mostly based on single-task learning, which makes them unsuitable for increasingly sophisticated engineering environments. In this paper, a novel multi-task graph-guided convolutional network with an attention mechanism for intelligent fault diagnosis, named MTAGCN, is proposed. Most existing fault diagnosis models are commonly bounded by a single diagnosis objective, especially when handling multiple tasks jointly. To address this limitation, a new multi-task fault diagnosis framework is designed, incorporating an attention mechanism between the task-specific module and task-shared modules. This framework enables multiple related tasks to be learned jointly while improving diagnostic and identification performance. Moreover, it is observed that most existing DL-based methods share incomplete fault representations, leading to unsatisfactory fault diagnosis. To overcome this issue, a graph convolutional network (GCN)-based fault diagnosis framework is introduced, which not only captures structural characteristics but also enhances diagnostic effectiveness. Comprehensive experiments based on three case studies demonstrate that the proposed MTAGCN outperforms state-of-the-art (SOTA) methods, striking a good balance between accuracy and multi-task learning.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2234.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2234.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MTAGCN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Task Graph-Guided Convolutional Network with Attention Mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-task deep network that combines convolutional blocks, instance-graph construction with graph convolution (ChebNet-style), and SE-style attention modules to dynamically select task-specific features from a shared backbone for simultaneous fault-type and fault-severity diagnosis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MTAGCN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Task-shared backbone of alternating Conv blocks and Graph blocks that builds per-instance sparse adjacency (Top-k) from Conv features, plus multiple SE-Net style attention modules that bridge the shared backbone to two task-specific heads (fault type and fault severity). Designed to learn task-invariant features while dynamically selecting and re-calibrating shared features for each task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Task-shared vs task-specific modular decomposition plus SE-style channel attention modules for dynamic feature re-calibration and per-instance instance-graph construction with Top-k sparsification (graph blocks) to focus compute on strong relations.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-task fault diagnosis of rotating machinery (fault type and fault severity) across multiple datasets and operating conditions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>CRB: 100% accuracy on both tasks (FTI and FSI); CRB metrics: AUC/Precision/Recall/F1 = 100% (Table 11). HUSTbearing: 100% accuracy on both FTI and FSI across reported speeds (65,70,75,80 Hz) (Table 7). MCC5-THU gearbox: T1 = 99.51% ±0.11, T2 = 99.47% ±0.14 (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Compared baselines (reported accuracies): MTCNN (example: CRB accuracy ≈ 99.14%), JLCNN (≈ 98.09%), MRFGCN (≈ 97.49%), IDBN (≈ 96.84%) for overall CRB accuracy; on severity tasks baselines drop substantially (see Tables 6-8).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Total FLOPs reported for MTAGCN: 15,497,888 (Table 9). Implementation/training: PyTorch on NVIDIA RTX 3070, 300 epochs, batch size 32 (train) / 16 (test).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Reported robustness across varying rotational speeds and datasets: MTAGCN maintains near-100% accuracy across rotational speeds (Table 7) and on a real gearbox dataset (MCC5-THU) with ≈99.5% accuracy, while some STL baselines drop notably under changed conditions (paper text & Table 7 discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>t-SNE visualizations (HUSTbearing T1 at 65 Hz) show MTAGCN / MTL methods produce more separable, discriminative feature clusters vs STL baselines; attention modules used to re-calibrate channels (SE-Net) provide interpretable channel importance weights (architectural design).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>MTAGCN provides excellent joint performance: achieves near-100% on both tasks simultaneously across datasets; other MTL baselines (MTCNN, JLCNN) maintain high type-diagnosis (FTI) but lower severity-diagnosis (FSI) performance (~90% level) indicating MTAGCN's task-specific adaptations improve joint performance (Tables 6-8).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Design includes Top-k sparsification of adjacency to reduce graph computation; no quantified FLOP/memory comparison against baselines other than MTAGCN's own FLOPs. Training/inference performed on RTX 3070 with batch sizes as above.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Integrating per-instance instance-graph construction (sparse Top-k GCN blocks) with SE-style attention between a shared backbone and task-specific heads yields dynamic, task-aligned representations that substantially improve joint fault-type and fault-severity accuracy and stability versus competing methods and ablated variants.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Empirical results (100%/≈99.5% accuracy) and ablation comparisons show that dynamic task-aligned mechanisms (attention + GCN instance graphs) outperform variants lacking them, supporting the benefit of task-aligned adaptive representations over uniform representations in this domain.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2234.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2234.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>M1 (MTAGCN w/o GCN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MTAGCN variant without Graph Convolutional Network (graph blocks removed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation variant of MTAGCN where graph blocks are removed to test the contribution of graph-based instance relations; retains Conv blocks and attention modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>M1 (MTAGCN without GCNs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same multi-task Conv backbone and attention modules as MTAGCN but with the graph block (instance-graph construction and GCN layers) removed; meant to measure impact of structural graph modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Task-shared vs task-specific modules plus SE-style attention, but without graph-based structural abstraction (so less structural adaptivity).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-task fault diagnosis (same datasets as MTAGCN)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Ablation results (Table 10): 65 Hz = 99.04%, 70 Hz = 98.76%, 75 Hz = 97.89% (compared to MTAGCN's 100% at those speeds).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Worse than full MTAGCN on joint tasks (several percentage points lower in the ablation across speeds), indicating graph blocks contribute to multi-task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Removing GCN graph blocks reduces multi-task diagnostic accuracy by ~0.96–2.11 percentage points across tested speeds, indicating graph-based structural features support task-aligned performance.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Performance drop when removing graph structure suggests that structural (graph) abstraction contributes to effective task-aligned representations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2234.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2234.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>M2 (MTAGCN w/o AM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MTAGCN variant without Attention Mechanism (AM removed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation variant of MTAGCN in which SE-style attention modules between shared backbone and task-specific heads are removed to test attention's role in dynamic task-specific selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>M2 (MTAGCN without AM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MTAGCN architecture but with attention modules removed, so task-specific heads receive shared features without adaptive SE-style re-weighting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Task-shared vs task-specific modules without dynamic channel re-calibration (i.e., more uniform transfer of shared features).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-task fault diagnosis</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Ablation results (Table 10): 65 Hz = 98.54%, 70 Hz = 97.69%, 75 Hz = 97.08% (compared to MTAGCN's 100% at those speeds).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Significantly worse than full MTAGCN across tested speeds, showing that the attention modules enabling task-specific reweighting are critical for joint task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Removing the attention mechanism reduces accuracy by ~1.46–2.92 percentage points across speeds, demonstrating that dynamic channel re-calibration is an important adaptive mechanism for task-aligned representations.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Attention ablation causes performance degradation, supporting the view that dynamic, task-aligned feature allocation improves multi-task performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2234.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2234.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MTCNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MTCNN (multi-task CNN baseline as reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-task CNN baseline used for comparison; reported to perform well on fault-type diagnosis but less well on fault-severity diagnosis relative to MTAGCN.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MTCNN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multi-task convolutional neural network baseline (paper does not provide internal architecture details); used as an MTL comparator in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Multi-task shared feature extraction (details not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-task fault diagnosis (same datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>CRB: reported ~99.14% accuracy (overall); CRB FSI ≈ 95.64% (Table 6). HUSTbearing: competitive on FTI but lower on FSI in several speed scenarios (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Shown to degrade more than MTAGCN on severity task under varied rotation speed scenarios (Table 7 discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>High FTI performance (often >94%), but FSI performance around mid-90s to ~90s depending on dataset; generally worse joint performance than MTAGCN.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>MTCNN is a competent MTL baseline but underperforms MTAGCN on severity diagnosis and robustness across operating conditions, indicating MTAGCN's task-adaptive mechanisms yield better joint generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>MTCNN (an MTL method) indicates sharing features helps, but without reported dynamic attention/graph mechanisms its multi-task/severity performance is inferior to MTAGCN, suggesting that the mere presence of MTL is insufficient compared to adaptive task-aligned mechanisms.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2234.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2234.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JLCNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>JLCNN (joint learning CNN baseline reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Another multi-task CNN baseline used for comparison; performs well on type identification (FTI) but less well than MTAGCN on severity identification (FSI).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>JLCNN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Joint-learning convolutional neural network baseline (architectural specifics not provided in this paper); used as a comparative MTL method.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Multi-task shared features (details not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-task fault diagnosis</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>CRB: reported ≈98.09% overall accuracy; FSI lower (~93.78% on CRB Table 6); HUSTbearing: competitive on FTI but performs worse than MTAGCN on FSI across speeds (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Generally lower FSI performance than MTAGCN though decent FTI performance, indicating less effective task-specific adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>JLCNN shows that joint learning helps type diagnosis but without MTAGCN-style adaptive modules it yields poorer severity discrimination and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>JLCNN's results suggest sharing helps, but the lack of explicit dynamic task-alignment (as in MTAGCN) limits joint-task performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2234.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2234.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MRFGCN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MRFGCN (single-task GCN-based baseline reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-task learning GCN-based baseline used as comparison; typically performs well on single tasks but degrades when tasks vary or when multi-task signals are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MRFGCN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GCN-based single-task diagnostic model (details not provided in this paper); used to benchmark STL performance versus multi-task MTAGCN.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Single-task (uniform per-task) feature extraction, GCN-based structural modeling (as implied by name), details not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>single-task fault diagnosis (used as baseline vs multi-task)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>CRB: reported ≈97.49% overall accuracy (Table 11); FSI task lower (~89.14% in Table 6); performance drops in some HUSTbearing speed scenarios (Table 7 discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Performance drops observed when rotation speed or task conditions vary, indicating weaker generalization in multi-task contexts (paper discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>As an STL method, it underperforms multi-task MTAGCN on joint tasks and lacks ability to leverage shared features across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>STL GCN baseline delivers decent single-task performance but is less robust than MTAGCN when multiple tasks or varying conditions are present.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Comparison shows STL (uniform per-task models) are less capable at joint task performance and generalization than adaptive multi-task approaches like MTAGCN.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2234.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2234.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IDBN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IDBN (improved deep belief network baseline reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-task DL baseline (deep belief/autoencoder style) used for comparison; shows the lowest performance among reported baselines on multi-task severity diagnosis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>IDBN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An improved deep belief/autoencoder-based single-task diagnostic method (exact architecture details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Single-task uniform representations based on DBN/autoencoder family (details not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>single-task fault diagnosis (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>CRB: reported ≈96.84% overall accuracy and notably lower FSI performance (e.g., ~87.26% in Table 6); poorer confusion-matrix results and worse separation in t-SNE visualizations relative to MTL methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Does not perform well in multi-task settings; frequently misclassifies labels in severity tasks compared to MTAGCN.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>IDBN (STL) performs worst among compared methods in multi-task severity diagnosis, highlighting limitations of uniform single-task representations for joint tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Poor multi-task performance relative to MTAGCN supports the idea that adaptive, task-aligned representations improve joint-task outcomes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A Novel Multi-Task Self-Supervised Transfer Learning Framework for Cross-Machine Rolling Bearing Fault Diagnosis <em>(Rating: 2)</em></li>
                <li>Prior knowledge-informed multi-task dynamic learning for few-shot machinery fault diagnosis <em>(Rating: 2)</em></li>
                <li>Multi-source domain adversarial graph convolutional networks for rolling mill health states diagnosis under variable working conditions <em>(Rating: 2)</em></li>
                <li>Multi-task interaction learning for accurate segmentation and classification of breast tumors in ultrasound images <em>(Rating: 1)</em></li>
                <li>Self-Explanatory Fault Diagnosis Framework for Industrial Processes Using Graph Attention <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2234",
    "paper_id": "paper-278057024",
    "extraction_schema_id": "extraction-schema-61",
    "extracted_data": [
        {
            "name_short": "MTAGCN",
            "name_full": "Multi-Task Graph-Guided Convolutional Network with Attention Mechanism",
            "brief_description": "A multi-task deep network that combines convolutional blocks, instance-graph construction with graph convolution (ChebNet-style), and SE-style attention modules to dynamically select task-specific features from a shared backbone for simultaneous fault-type and fault-severity diagnosis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MTAGCN",
            "model_description": "Task-shared backbone of alternating Conv blocks and Graph blocks that builds per-instance sparse adjacency (Top-k) from Conv features, plus multiple SE-Net style attention modules that bridge the shared backbone to two task-specific heads (fault type and fault severity). Designed to learn task-invariant features while dynamically selecting and re-calibrating shared features for each task.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Task-shared vs task-specific modular decomposition plus SE-style channel attention modules for dynamic feature re-calibration and per-instance instance-graph construction with Top-k sparsification (graph blocks) to focus compute on strong relations.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "multi-task fault diagnosis of rotating machinery (fault type and fault severity) across multiple datasets and operating conditions",
            "performance_task_aligned": "CRB: 100% accuracy on both tasks (FTI and FSI); CRB metrics: AUC/Precision/Recall/F1 = 100% (Table 11). HUSTbearing: 100% accuracy on both FTI and FSI across reported speeds (65,70,75,80 Hz) (Table 7). MCC5-THU gearbox: T1 = 99.51% ±0.11, T2 = 99.47% ±0.14 (Table 8).",
            "performance_uniform_baseline": "Compared baselines (reported accuracies): MTCNN (example: CRB accuracy ≈ 99.14%), JLCNN (≈ 98.09%), MRFGCN (≈ 97.49%), IDBN (≈ 96.84%) for overall CRB accuracy; on severity tasks baselines drop substantially (see Tables 6-8).",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Total FLOPs reported for MTAGCN: 15,497,888 (Table 9). Implementation/training: PyTorch on NVIDIA RTX 3070, 300 epochs, batch size 32 (train) / 16 (test).",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": "Reported robustness across varying rotational speeds and datasets: MTAGCN maintains near-100% accuracy across rotational speeds (Table 7) and on a real gearbox dataset (MCC5-THU) with ≈99.5% accuracy, while some STL baselines drop notably under changed conditions (paper text & Table 7 discussion).",
            "interpretability_results": "t-SNE visualizations (HUSTbearing T1 at 65 Hz) show MTAGCN / MTL methods produce more separable, discriminative feature clusters vs STL baselines; attention modules used to re-calibrate channels (SE-Net) provide interpretable channel importance weights (architectural design).",
            "multi_task_performance": "MTAGCN provides excellent joint performance: achieves near-100% on both tasks simultaneously across datasets; other MTL baselines (MTCNN, JLCNN) maintain high type-diagnosis (FTI) but lower severity-diagnosis (FSI) performance (~90% level) indicating MTAGCN's task-specific adaptations improve joint performance (Tables 6-8).",
            "resource_constrained_results": "Design includes Top-k sparsification of adjacency to reduce graph computation; no quantified FLOP/memory comparison against baselines other than MTAGCN's own FLOPs. Training/inference performed on RTX 3070 with batch sizes as above.",
            "key_finding_summary": "Integrating per-instance instance-graph construction (sparse Top-k GCN blocks) with SE-style attention between a shared backbone and task-specific heads yields dynamic, task-aligned representations that substantially improve joint fault-type and fault-severity accuracy and stability versus competing methods and ablated variants.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Empirical results (100%/≈99.5% accuracy) and ablation comparisons show that dynamic task-aligned mechanisms (attention + GCN instance graphs) outperform variants lacking them, supporting the benefit of task-aligned adaptive representations over uniform representations in this domain.",
            "uuid": "e2234.0"
        },
        {
            "name_short": "M1 (MTAGCN w/o GCN)",
            "name_full": "MTAGCN variant without Graph Convolutional Network (graph blocks removed)",
            "brief_description": "Ablation variant of MTAGCN where graph blocks are removed to test the contribution of graph-based instance relations; retains Conv blocks and attention modules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "M1 (MTAGCN without GCNs)",
            "model_description": "Same multi-task Conv backbone and attention modules as MTAGCN but with the graph block (instance-graph construction and GCN layers) removed; meant to measure impact of structural graph modeling.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Task-shared vs task-specific modules plus SE-style attention, but without graph-based structural abstraction (so less structural adaptivity).",
            "is_dynamic_or_adaptive": true,
            "task_domain": "multi-task fault diagnosis (same datasets as MTAGCN)",
            "performance_task_aligned": "Ablation results (Table 10): 65 Hz = 99.04%, 70 Hz = 98.76%, 75 Hz = 97.89% (compared to MTAGCN's 100% at those speeds).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Worse than full MTAGCN on joint tasks (several percentage points lower in the ablation across speeds), indicating graph blocks contribute to multi-task performance.",
            "resource_constrained_results": null,
            "key_finding_summary": "Removing GCN graph blocks reduces multi-task diagnostic accuracy by ~0.96–2.11 percentage points across tested speeds, indicating graph-based structural features support task-aligned performance.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Performance drop when removing graph structure suggests that structural (graph) abstraction contributes to effective task-aligned representations.",
            "uuid": "e2234.1"
        },
        {
            "name_short": "M2 (MTAGCN w/o AM)",
            "name_full": "MTAGCN variant without Attention Mechanism (AM removed)",
            "brief_description": "Ablation variant of MTAGCN in which SE-style attention modules between shared backbone and task-specific heads are removed to test attention's role in dynamic task-specific selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "M2 (MTAGCN without AM)",
            "model_description": "MTAGCN architecture but with attention modules removed, so task-specific heads receive shared features without adaptive SE-style re-weighting.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Task-shared vs task-specific modules without dynamic channel re-calibration (i.e., more uniform transfer of shared features).",
            "is_dynamic_or_adaptive": false,
            "task_domain": "multi-task fault diagnosis",
            "performance_task_aligned": "Ablation results (Table 10): 65 Hz = 98.54%, 70 Hz = 97.69%, 75 Hz = 97.08% (compared to MTAGCN's 100% at those speeds).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Significantly worse than full MTAGCN across tested speeds, showing that the attention modules enabling task-specific reweighting are critical for joint task performance.",
            "resource_constrained_results": null,
            "key_finding_summary": "Removing the attention mechanism reduces accuracy by ~1.46–2.92 percentage points across speeds, demonstrating that dynamic channel re-calibration is an important adaptive mechanism for task-aligned representations.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Attention ablation causes performance degradation, supporting the view that dynamic, task-aligned feature allocation improves multi-task performance.",
            "uuid": "e2234.2"
        },
        {
            "name_short": "MTCNN",
            "name_full": "MTCNN (multi-task CNN baseline as reported in paper)",
            "brief_description": "A multi-task CNN baseline used for comparison; reported to perform well on fault-type diagnosis but less well on fault-severity diagnosis relative to MTAGCN.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MTCNN",
            "model_description": "Multi-task convolutional neural network baseline (paper does not provide internal architecture details); used as an MTL comparator in experiments.",
            "model_size": null,
            "uses_task_aligned_abstraction": null,
            "abstraction_mechanism": "Multi-task shared feature extraction (details not specified in this paper).",
            "is_dynamic_or_adaptive": null,
            "task_domain": "multi-task fault diagnosis (same datasets)",
            "performance_task_aligned": "CRB: reported ~99.14% accuracy (overall); CRB FSI ≈ 95.64% (Table 6). HUSTbearing: competitive on FTI but lower on FSI in several speed scenarios (Table 7).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": "Shown to degrade more than MTAGCN on severity task under varied rotation speed scenarios (Table 7 discussion).",
            "interpretability_results": null,
            "multi_task_performance": "High FTI performance (often &gt;94%), but FSI performance around mid-90s to ~90s depending on dataset; generally worse joint performance than MTAGCN.",
            "resource_constrained_results": null,
            "key_finding_summary": "MTCNN is a competent MTL baseline but underperforms MTAGCN on severity diagnosis and robustness across operating conditions, indicating MTAGCN's task-adaptive mechanisms yield better joint generalization.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "MTCNN (an MTL method) indicates sharing features helps, but without reported dynamic attention/graph mechanisms its multi-task/severity performance is inferior to MTAGCN, suggesting that the mere presence of MTL is insufficient compared to adaptive task-aligned mechanisms.",
            "uuid": "e2234.3"
        },
        {
            "name_short": "JLCNN",
            "name_full": "JLCNN (joint learning CNN baseline reported in paper)",
            "brief_description": "Another multi-task CNN baseline used for comparison; performs well on type identification (FTI) but less well than MTAGCN on severity identification (FSI).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "JLCNN",
            "model_description": "Joint-learning convolutional neural network baseline (architectural specifics not provided in this paper); used as a comparative MTL method.",
            "model_size": null,
            "uses_task_aligned_abstraction": null,
            "abstraction_mechanism": "Multi-task shared features (details not specified).",
            "is_dynamic_or_adaptive": null,
            "task_domain": "multi-task fault diagnosis",
            "performance_task_aligned": "CRB: reported ≈98.09% overall accuracy; FSI lower (~93.78% on CRB Table 6); HUSTbearing: competitive on FTI but performs worse than MTAGCN on FSI across speeds (Table 7).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Generally lower FSI performance than MTAGCN though decent FTI performance, indicating less effective task-specific adaptation.",
            "resource_constrained_results": null,
            "key_finding_summary": "JLCNN shows that joint learning helps type diagnosis but without MTAGCN-style adaptive modules it yields poorer severity discrimination and robustness.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "JLCNN's results suggest sharing helps, but the lack of explicit dynamic task-alignment (as in MTAGCN) limits joint-task performance.",
            "uuid": "e2234.4"
        },
        {
            "name_short": "MRFGCN",
            "name_full": "MRFGCN (single-task GCN-based baseline reported in paper)",
            "brief_description": "A single-task learning GCN-based baseline used as comparison; typically performs well on single tasks but degrades when tasks vary or when multi-task signals are needed.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MRFGCN",
            "model_description": "GCN-based single-task diagnostic model (details not provided in this paper); used to benchmark STL performance versus multi-task MTAGCN.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Single-task (uniform per-task) feature extraction, GCN-based structural modeling (as implied by name), details not provided.",
            "is_dynamic_or_adaptive": false,
            "task_domain": "single-task fault diagnosis (used as baseline vs multi-task)",
            "performance_task_aligned": "CRB: reported ≈97.49% overall accuracy (Table 11); FSI task lower (~89.14% in Table 6); performance drops in some HUSTbearing speed scenarios (Table 7 discussion).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": "Performance drops observed when rotation speed or task conditions vary, indicating weaker generalization in multi-task contexts (paper discussion).",
            "interpretability_results": null,
            "multi_task_performance": "As an STL method, it underperforms multi-task MTAGCN on joint tasks and lacks ability to leverage shared features across tasks.",
            "resource_constrained_results": null,
            "key_finding_summary": "STL GCN baseline delivers decent single-task performance but is less robust than MTAGCN when multiple tasks or varying conditions are present.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Comparison shows STL (uniform per-task models) are less capable at joint task performance and generalization than adaptive multi-task approaches like MTAGCN.",
            "uuid": "e2234.5"
        },
        {
            "name_short": "IDBN",
            "name_full": "IDBN (improved deep belief network baseline reported in paper)",
            "brief_description": "A single-task DL baseline (deep belief/autoencoder style) used for comparison; shows the lowest performance among reported baselines on multi-task severity diagnosis.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "IDBN",
            "model_description": "An improved deep belief/autoencoder-based single-task diagnostic method (exact architecture details not provided in this paper).",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Single-task uniform representations based on DBN/autoencoder family (details not provided).",
            "is_dynamic_or_adaptive": false,
            "task_domain": "single-task fault diagnosis (used as baseline)",
            "performance_task_aligned": "CRB: reported ≈96.84% overall accuracy and notably lower FSI performance (e.g., ~87.26% in Table 6); poorer confusion-matrix results and worse separation in t-SNE visualizations relative to MTL methods.",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Does not perform well in multi-task settings; frequently misclassifies labels in severity tasks compared to MTAGCN.",
            "resource_constrained_results": null,
            "key_finding_summary": "IDBN (STL) performs worst among compared methods in multi-task severity diagnosis, highlighting limitations of uniform single-task representations for joint tasks.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Poor multi-task performance relative to MTAGCN supports the idea that adaptive, task-aligned representations improve joint-task outcomes.",
            "uuid": "e2234.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A Novel Multi-Task Self-Supervised Transfer Learning Framework for Cross-Machine Rolling Bearing Fault Diagnosis",
            "rating": 2
        },
        {
            "paper_title": "Prior knowledge-informed multi-task dynamic learning for few-shot machinery fault diagnosis",
            "rating": 2
        },
        {
            "paper_title": "Multi-source domain adversarial graph convolutional networks for rolling mill health states diagnosis under variable working conditions",
            "rating": 2
        },
        {
            "paper_title": "Multi-task interaction learning for accurate segmentation and classification of breast tumors in ultrasound images",
            "rating": 1
        },
        {
            "paper_title": "Self-Explanatory Fault Diagnosis Framework for Industrial Processes Using Graph Attention",
            "rating": 2
        }
    ],
    "cost": 0.017235999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MTAGCN: Multi-Task Graph-Guided Convolutional Network with Attention Mechanism for Intelligent Fault Diagnosis of Rotating Machinery
23 April 2025</p>
<p>Bo Wang 0009-0008-0298-3799
School of Information and Artificial Intelligence
Nanchang Institute of Science &amp; Technology
330108NanchangChina</p>
<p>Shuai Zhao 0000-0002-4766-3886
School of Information and Artificial Intelligence
Nanchang Institute of Science &amp; Technology
330108NanchangChina</p>
<p>MTAGCN: Multi-Task Graph-Guided Convolutional Network with Attention Mechanism for Intelligent Fault Diagnosis of Rotating Machinery
23 April 2025E3961A7B44F76A7461722B737B5C202C10.3390/machines13050347Received: 1 April 2025 Revised: 20 April 2025 Accepted: 21 April 2025fault diagnosismulti-task learninggraph convolutional networksattention mechanism
Deep learning (DL)-based methods have shown great success in multi-category fault diagnosis due to their hierarchical networks and automatic feature extraction.However, their superior performance is mostly based on single-task learning, which makes them unsuitable for increasingly sophisticated engineering environments.In this paper, a novel multi-task graph-guided convolutional network with an attention mechanism for intelligent fault diagnosis, named MTAGCN, is proposed.Most existing fault diagnosis models are commonly bounded by a single diagnosis objective, especially when handling multiple tasks jointly.To address this limitation, a new multi-task fault diagnosis framework is designed, incorporating an attention mechanism between the task-specific module and taskshared modules.This framework enables multiple related tasks to be learned jointly while improving diagnostic and identification performance.Moreover, it is observed that most existing DL-based methods share incomplete fault representations, leading to unsatisfactory fault diagnosis.To overcome this issue, a graph convolutional network (GCN)-based fault diagnosis framework is introduced, which not only captures structural characteristics but also enhances diagnostic effectiveness.Comprehensive experiments based on three case studies demonstrate that the proposed MTAGCN outperforms state-of-the-art (SOTA) methods, striking a good balance between accuracy and multi-task learning.</p>
<p>Introduction</p>
<p>As fundamental components of rotating machinery, bearings, gears, couplings, and other related parts are essential in modern industry [1][2][3].However, harsh factory conditions make them highly prone to wear and performance degradation under prolonged working time.In the era of big data, traditional signal processing methods are increasingly limited due to insufficient accuracy, inefficiency, and dependence on expert knowledge and denoising [4].At the same time, the rapid development of deep learning (DL)-based techniques with sensor technology and the Internet of Things (IoT) has brought new opportunities for fault diagnosis.Most importantly, intelligent fault diagnosis (IFD)-based methods, which are known for their efficiency and accuracy, have received great attention from academia and industry in recent years and have been significantly applied in rotating machinery fault diagnosis [5].</p>
<p>As an efficient fault diagnosis tool, machine learning (ML)-based methods have received much attention from academia and have shown significant potential in rotating machinery fault diagnosis.Generally, the implementation process consists of three key steps: initial data acquisition, feature engineering with expert knowledge, and final health condition classification [6,7].Chen et al. proposed a hierarchical ML-based framework with only two layers to enable fault diagnosis in rotating machinery [8].Zhang et al. proposed a novel ML-based framework by integrating reverse feature elimination (RFE) and extreme learning machine (ELM) to achieve fault diagnosis of shield machines [9].However, manual feature extraction in ML-based methods heavily relies on expert knowledge, which can lead to significant time and labor costs.Meanwhile, they are commonly constrained by limited data, making them unsuitable for generalization and self-learning capabilities under complex engineering environments [10].In recent years, there has been growing interest in developing advanced methods for feature extraction and health condition identification in rotating machinery using large-scale monitoring data.</p>
<p>In recent years, deep learning (DL)-based diagnostic models have been developed to automatically learn fault-based features from monitoring data through deep hierarchical architectures.Compared to ML-based methods, DL-based models can directly establish the relationship between the input samples and health conditions of rotating machinery, which utilizes the learned features to assess the fault types or severities of rotating machinery.Several approaches, including deep belief networks (DBNs) [11], deep autoencoders (DAEs) [12], convolutional neural networks (CNNs) [13], and graph convolutional networks (GCNs) [14], have been explored for fault diagnosis.For example, Karimi et al. proposed a multi-source domain adaptation method for fault diagnosis, leveraging the attention mechanism, domain attribute loss, and knowledge fusion to enhance crossdomain diagnostic performance [15].Shi et al. designed the improved multi-sensor DBNs (MSIDBNs) with redefined pretraining and finetuning stages to effectively extract features in rolling mill fault diagnosis [16].Yu et al. introduced a novel DL-based approach by improving one-dimensional and two-dimensional CNNs (I1DCNNs and I2DCNNs) with group normalization (GN) and global average pooling (GAP) for enhanced intelligent fault diagnosis [17].Han et al. built a novel multi-source heterogeneous information fusion framework by utilizing improved DL-based methods to enhance the effectiveness and robustness of intelligent fault diagnosis [18].</p>
<p>Though DL-based methods have made significant advancements in intelligent fault diagnosis of rotating machinery, some drawbacks of these methods still remain.On the one hand, the majority of current DL-based methods assume that they can extract features only for a single task to achieve fault diagnosis while ignoring task-invariant features.On the other hand, most existing DL-based feature extraction methods rely mainly on class labels while overlooking the underlying structure characteristics of samples.This may lead to incomplete feature representations in deep networks.to enhance segmentation and classification in medical image analysis [22].However, these methods cannot adequately consider the interactions of different diagnostic tasks at different levels.Therefore, the design of the MTL-based methods still has much room for improvement.</p>
<p>To address the limitations of DL-based methods, a novel multi-task graph-guided convolutional network with an attention mechanism (MTAGCN) has been proposed for intelligent fault diagnosis of rotating machinery.In MTAGCN, multi-task diagnosis is achieved by effectively modeling the data structure, feature extraction, and attention mechanism (AM) in a unified deep network.The fault severity labels and fault type labels are modeled by two task-specific modules, respectively.In order to adjust the data structure from the graph data, task-invariant features are mined from the raw vibration signals using convolutional blocks (Conv).Then, the block is utilized to build instance graphs of the feature structure characteristics.After that, the task-shared module consisting of Conv and graph blocks is used to mine different levels of features.AM should be used to enable connections between task-shared modules and task-specific modules that capture the significance of task-shared characteristics and select valuable and representative characteristics for a particular diagnostic task.The outline of our proposed MTAGCN is shown in Figure 1.The task-shared module extracts valuable features from input vibration signals, while two task-specific modules further process these features to output diagnostic information, including fault type and severity.The remaining part of the paper proceeds as follows: In Section 2, related works about CNNs, GCNs, AM, and MTL are illustrated.In Section 3, the proposed MTAGCN is described.The feasibility and effectiveness of the proposed MTAGCN on MTL fault diagnosis are validated by three datasets in Section 4. In Section 5, a set of comparison experiments of MTAGCN is carried out.The summary and directions for future research are presented in Section 6 of this paper.</p>
<p>Related Works</p>
<p>Convolutional Neural Networks</p>
<p>CNNs, as one of the classic DL-based architectures, are widely used for processing data characterized by a grid-like structure, such as images and signals [23,24].Their core components include convolutional layers, pooling layers, and fully connected layers.CNNs have been extensively applied in tasks such as image classification, object detection, and semantic segmentation due to their strong capabilities in feature extraction and representation.The detailed architecture of CNNs is illustrated in Figure 2. Convolutional layers are the most fundamental component of CNNs and are used to extract coarser features by sliding the input data through a learnable filter or kernel.CNNs can realize equivariant representations through the learnable filters in the input data, while greatly reducing the input dimensionality.To learn about finer features, we usually stack convolutional layers at different depths of the network.Given that the input feature maps x l−1 i ∈ R H×W , where H and W stand for the height and width, respectively, they are input to the convolutional layer, defined as:
x l c = σ C l−1 ∑ i=1 k l i,c * x l−1 i + b l c (1)
where x n c represents the output feature of the l-th layer.σ(•) is the activation function (i.e., ReLU).k n i,c denotes the kernel function.W and C refer to the size of the kernel and the number of channels, separately.b n c means the bias of the l-th layer.The pooling layers are then followed by the convolutional layers to further reduce the dimensionality of the feature map and prevent overfitting.Max-pooling, as one of the common pooling operations, is used in this paper and is defined as follows:
P n = max x n c ∈S x l c (2)
where P n means the pooled feature maps of the l-th layer.max(•) denotes the max-pooling operation.</p>
<p>After these feature maps undergo several convolution and pooling operations, they are flattened into a one-dimensional vector and input into the fully connected layer for final fault diagnosis, as expressed below:
h m = σ m (W m ) T * x m−1 + b m(3)
where W m and b m are the weight matrix and bias of the m-th layer in the fully connected layer, respectively.</p>
<p>Finally, the probability distributions from the fully connected layer are converted through a softmax function, which is expressed as:
ŷi =       P(y i = 1) P(y i = 2)
. . . (1) e h oi (2)  . . .
P(y i = k)       = 1 k ∑ i=1 e h oi       e h oie h oi (k)      (4)
where h oi ∈ R k×1 means the output feature maps of input samples x i .</p>
<p>Graph Convolutional Networks</p>
<p>GCNs, as a novel architecture for DL-based methods, are specialized for processing graph-based data and non-Euclidean data [25].Compared to the CNNs mentioned above, GCNs not only extract node features but also capture the structural relationship of nodes.By aggregating information from neighboring nodes, GCNs can effectively learn fault-based representations.The detailed architecture of GCNs is illustrated in Figure 3.A graph G(V, E, F) consists of a set of nodes V, a set of edges E, and a set of features F. GCNs update node representations by aggregating information from neighboring nodes, enabling effective processing of graph-structured data.The mathematical equations for the graph convolution operation can be written as follows:
H (l+1) = σ D− 1 2 Ã D− 1 2 H (l) W (l)(5)
with Dii = ∑ j Ãij and Ã = A + I, where D is the degree matrix after adding self-loops and D means the adjacency matrix after adding self-loops.H (l) ∈ R N×d (l) is the node feature matrix at the l-th layer.N is the number of nodes.d (l) is the feature dimension at the l-th layer.W (l) ∈ R d (l) ×d (l+1) is the learnable weight matrix at the l-th layer.In this paper, Chebyshev graph convolutional networks (ChebNet) are introduced to capture more complex structural features of graphs.</p>
<p>For an undirected graph G, the normalized Laplacian matrix is written as follows:
L = I n − D −1/2 AD −1/2 (6)
where A denotes the adjacency matrix.D means the degree matrix.I n is the identity matrix.</p>
<p>To apply Chebyshev polynomials, it is necessary to rescale the eigenvalues of the Laplacian matrix L to the interval [−1, 1], resulting in:
L = 2L λ max − I n(7)
where λ max denotes the largest eigenvalue of the Laplacian matrix L.</p>
<p>Hence, the convolution operation in ChebNet can be expressed as:
g θ * x = K−1 ∑ k=0 θ k T k ( L)x(8)
where x ∈ R N denotes the input signal defined on the graph.θ ∈ R K represents the filter parameters.T k is the Chebyshev polynomial of order k.T k ( L) denotes the k-th order Chebyshev polynomial evaluated on the rescaled graph Laplacian matrix L.</p>
<p>For multi-channel input features, the layer-wise propagation rule of ChebNet can be formulated as:
H (l+1) = σ K−1 ∑ k=0 T k ( L)H (l) W (l) k (9)
where H (l) ∈ R N×F l denotes the node feature matrix at layer l.W (l) k ∈ R F l ×F l+1 is the learnable weight matrix associated with the k-th order Chebyshev polynomial.σ is a non-linear activation function.</p>
<p>Attention Mechanism</p>
<p>The visual attention mechanism (AM), widely used in deep learning (DL)-based models, is inspired by the capacity of the human visual system to focus on salient regions while filtering out irrelevant information [26,27].AM dynamically assigns greater weights to the most informative regions of feature maps, thereby enhancing model performance.By weighting different regions or feature maps according to their importance, AM enables the model to prioritize critical information, improving both interpretability and accuracy.Most importantly, AM can be classified into three types: spatial attention, channel attention, and spatial-channel attention.</p>
<p>Spatial attention mechanisms focus on identifying and assigning varying weights to different spatial locations within the input, depending on task-specific requirements.Jaderberg et al. introduced the spatial Transformer module, which enables convolutional neural networks (CNNs) to efficiently learn spatial invariance by performing adaptive transformations on feature maps [28].Channel attention mechanisms, on the other hand, prioritize important regions within feature maps by emphasizing the relative significance of different channels in the feature representations.Wang et al. proposed the efficient channel attention (ECA) module, which enhances CNN performance by leveraging a local cross-channel interaction strategy, avoiding dimensionality reduction [29].Spatial-channel attention mechanisms combine the strengths of both spatial and channel attention, allowing deep learning models to extract more informative and discriminative feature representations.He et al. introduced the multi-scale attention (EMA) module, which efficiently preserves per-channel information while reducing computational costs by grouping channels and enhancing spatial semantics within each group [30].These attention mechanisms are often integrated into deep learning architectures (e.g., CNNs or Transformers) to enhance the model's capacity to capture long-range dependencies and fine-grained information.</p>
<p>Proposed Method</p>
<p>The proposed MTAGCN consists of a task-shared module and two task-specific modules: fault type diagnosis (FTD) and fault severity diagnosis (FSD).The task-shared module is responsible for extracting task-invariant, structural, and discriminative features from the input data.Specifically, shared features in the task-shared module are propagated through convolutional (Conv) blocks, allowing the model to extract deep features at multiple levels and capture fine-grained information.The task-shared module and the two task-specific modules are interconnected through the proposed attention mechanism (AM), which acts as a feature selector, learning and extracting task-specific features.The detailed structure of the proposed MTAGCN is shown in Figure 4.</p>
<p>Task-Shared Module</p>
<p>The raw vibration signals, without any preprocessing, are directly used as input to the MTAGCN, thereby enabling an end-to-end fault diagnosis framework.The dimensionality of each input signal is 1024 with no overlap, represented as x ∈ R 1×1024 .To analyze the structural relationships within the input features, the extracted features are transformed into instance graphs.Initially, a convolutional layer (Conv) is applied to extract meaningful features from the raw vibration data.The detailed structure of the Conv block is illustrated in Figure 5a.This Conv block consists of a one-dimensional convolutional layer ( f 1DConv ), followed by batch normalization ( f bn ), and a rectified linear unit activation function ( f ReLU ).The raw vibration signals are fed into the Conv block, which can be mathematically expressed as follows:
       f 1DConv = 1DConv(x) f bn ( f 1DConv , B) = γ f 1DConv −µ(B) √ σ(B) 2 + β F = max(0, f bn )(10)
where B denotes the mini-batch.The parameters γ and β are learnable scaling and shifting factors in the batch normalization process.The mean µ(B) and variance σ 2 (B) of the mini-batch are computed as follows:
µ(B) = 1 m m ∑ i=1 x i(11)σ(B) = 1 m m ∑ i=1 ; (x i − µ(B)) 2(12)
where m represents the size of mini-batch B.</p>
<p>Next, the graph block is employed after the Conv block to construct the adjacency matrix A based on the extracted features F. To facilitate the generation of instance graphs, a top-k sorting mechanism is introduced to identify the k nearest neighbors, as illustrated in Figure 5b.The detailed formulation of the graph block is given as follows:
     F = MLP(F) A = normlize( F FT ) Ā = Top − k(A)(13)
where A denotes the constructed adjacency matrix, and F represents the output of the multilayer perceptron (MLP).The function normalize(•) is used to normalize the product of F and its transpose.To enhance computational efficiency, a sparse adjacency matrix Ā is derived by applying the Top-k(•) operation, which retains only the top k largest values in each row of A. This approach effectively reduces the computational overhead while preserving the most significant relational information.</p>
<p>It is noteworthy that the task-shared module is composed of six Conv blocks and three Graph blocks arranged in an alternating sequence.This architecture design enables the network to simultaneously capture both local temporal features (through Conv blocks) and global structural relationships (through Graph blocks) from the input vibration signals.By integrating these complementary feature extraction mechanisms, the MTAGCN framework can effectively learn discriminative representations that enhance fault diagnosis accuracy across different operating conditions.</p>
<p>Attention Mechanism Module</p>
<p>The detailed architecture of SE-Net is illustrated in Figure 6.Assuming the input data for the AM module is defined as x 1 ∈ R C 1 ×W×H , where C 1 , W, and H denote the number of channels, width, and height, respectively.A convolutional operation is then applied to extract deeper features, which can be formulated as follows:
x 2 = F tr (x 1 )(14)
where x 2 ∈ R C 2 ×W×H represents the deeper features with channel dimension C 2 , width W, and height H. F tr (•) denotes the convolution operation.Subsequently, global information is aggregated by a squeeze operation, resulting in a scalar vector x 3 ∈ R 1×1×C 2 that captures the global receptive field.The squeezing process is formulated as follows:
x 2 = F sq (x 1 )(15)
where F sq (•) denotes the global average pooling.Then, two fully connected (FC) layers are employed to perform the excitation operation, enabling the model to capture inter-channel dependencies within the feature maps.
s = σ(W 2 δ(W 1 x 2 )) σ = 1 1+e −x (16)
where W 1 and W 2 refer to the weight parameters of the two FC layers, separately.σ(•) denotes the non-linear activation function, which is ReLU.s represents the weights of the feature maps.</p>
<p>Finally, the scaling operation is applied to re-calibrate the features x 1 , as expressed below:
x = sx 1 (17)
Most importantly, the AM module not only learns task-specific features from different layers of the task-shared module but also identifies important features to enhance diagnostic performance.It enables the proposed MTAGCN to dynamically learn task-specific features within the task-shared module for use in the two task-specific modules.We integrated AM modules at multiple layers within the proposed MTAGCN to capture multi-level features.Specifically, incorporating AM modules at different levels in the task-shared module allows the two task-specific modules to capture rich, representative features.</p>
<p>Task-Specific Module</p>
<p>As illustrated in Figure 4, the task-specific modules include the fault type diagnosis (FTD) module and the fault severity diagnosis (FSD) module, both of which share the same architecture.Each task-specific module consists of three AM modules, three 1 × 1 Conv blocks, three max-pooling layers, and a classifier.</p>
<p>The task-specific modules facilitate the extraction and optimization of features tailored to specific fault diagnosis tasks (i.e., fault types or severities) by leveraging task-shared features through the AM module.The key advantage of these task-specific modules lies in their ability to mine the most discriminative and representative features for fault diagnosis, thereby enhancing the efficiency of multi-task learning (MTL) through the task-shared module.</p>
<p>Loss Function and Optimization</p>
<p>In general, the proposed MTAGCN framework is trained using a loss function that incorporates the two task-specific modules, L 1 and L 2 , simultaneously.The objective function can be expressed as:
L = λ 1 L 1 + λ 2 L 2 (18)
where λ 1 and λ 2 refer to the impact factors for balancing the two task-specific modules, respectively.Herein, the L 1 and L 2 can be expressed as
L 1 (X, Y 1 ) = − 1 pq ∑ p,q ; Y 1 (p, q) log Ŷ1 (p, q) (19) L 2 (X, Y 2 ) = − 1 pq ∑ p,q ; Y 2 (p, q) log Ŷ2 (p, q)(20)
where Y and Ŷ refer to the true and predicted labels of rotating machinery, respectively.</p>
<p>To further improve the fault diagnostic and identification performance of the proposed MTAGCN, the parameters can be fine-tuned and iteratively updated using the Adaptive Moment Estimation (Adam) optimization algorithm proposed by Kingma and Ba [31].This enables optimal parameter tuning and, thus, improves the effectiveness and feasibility of the proposed MTAGCN in MTL-based diagnostic tasks.The following equations represent the update process:
               r (epoch+1) = r (0) (1 − β epoch 2 )/1 − β epoch 1 m (epoch+1) 1 = β 1 m (epoch) 1 + (1 − β 1 )∆θ (epoch) f t m (epoch+1) 2 = β 2 m (epoch) 2 + (1 − β 2 )(∆θ (epoch) f t ) 2 θ (epoch+1) f t = θ (epoch) f t − r (epoch+1) m (epoch+1) 1 / m (epoch+1) 2 + ε (21)
where the hyperparameters β 1 , β 2 , and ε are used to balance the optimization process, with typical values of 0.9, 0.999, and 10 −8 , respectively.epoch indicates the total number of training iterations.The variables m 1 and m 2 are initialized to 0, corresponding to the first and second moment estimates, respectively.Meanwhile, r (epoch+1) denotes the updated learning rate after (epoch + 1) iteration, and r (0) denotes the initial learning rate, which can be customized by the user.</p>
<p>Experiments</p>
<p>Case 1: Cylindrical Roller Bearing (CRB) Dataset</p>
<p>The CRB dataset is widely employed in bearing fault detection research and serves as a standard for evaluating the performance and practicality of the proposed approach in comparison to other state-of-the-art (SOTA) models [32].As depicted in Figure 7, the experimental setup consists of a motor, an accelerometer, two bearings, and a load cell.</p>
<p>Vibration data from the test bearing were recorded with an accelerometer at a sampling rate of 70,000 Hz.The CRB dataset includes four distinct bearing health statuses: normal (N), inner race fault (IRF), outer race fault (ORF), and roller fault (RF).The specifications of the cylindrical rolling bearing are shown in Table 1.For each faulty bearing, five different fault diameters were introduced using electrical discharge machining (EDM), as described in Table 2.The experiments were conducted with a shaft rotation speed of 2050 rpm and a load of 200 N.The different fault types and their severity levels define two distinct task requirements.In total, twelve rolling bearing health conditions were identified, alongside the normal condition.For simplicity, the thirteen health conditions are denoted as N, IRF-I, IRF-II, IRF-III, IRF-IV, IRF-V, ORF-I, ORF-II, ORF-III, ORF-IV, ORF-V, RF-I, RF-II, RF-III, RF-IV, and RF-V, where IRF-I represents the smallest fault diameter in the inner ring fault category.Detailed diagrams of the different faults are illustrated in Figure 8.</p>
<p>Case 2: HUSTbearing Dataset</p>
<p>To further demonstrate the advancement and advantages of the proposed MTAGCN under different rotational speed conditions, we conducted experiments using the HUS-Tbearing dataset.This dataset includes bearing data collected from a simulation platform under varying rotational speeds, fault types, and severity levels [33].The parameters of the tested bearings are described in Table 3.As illustrated in Figure 9, the experimental platform consists of a speed controller, motor, shaft, acceleration sensor, bearing module, and data acquisition board.Unprocessed vibration signals were collected from the test bearings using accelerometers mounted on the bearing casing under four distinct operating conditions (i.e., 65 Hz, 70 Hz, 75 Hz, and 80 Hz), with a sampling rate of 25.6 kHz and a constant load maintained across all speeds.To simulate various fault types and severities, single-point defects were introduced into the inner ring, outer ring, and rolling elements of the test bearings, as shown in Figure 10.The bearing conditions are categorized into four groups: normal (N), inner ring fault (IRF), outer ring fault (ORF), and roller ball fault (RBF).Each of the IRF, ORF, and RBF conditions includes two severity levels, denoted as I and II.In total, the HUSTbearing dataset comprises seven distinct bearing health conditions.Based on the HUSTbearing dataset, four diagnostic tasks were constructed under varying rotational speeds, as summarized in Table 4.</p>
<p>Case 3: MCC5-THU Gearbox Dataset</p>
<p>In previous studies, vibration data were typically generated using simulated bearing platforms [34].However, these case studies often fall short of capturing the complexity of real-world rotating machinery, thus limiting the generalization capability of diagnostic models.To further evaluate the robustness and generalizability of the proposed method, we employed a real gearbox test rig to collect vibration signals.The experimental setup of the MCC5-THU gearbox test rig is depicted in Figure 11.It consists of two three-axis vibration acceleration sensors, a frequency inverter, a torque sensor, a magnetic powder brake, a parallel gearbox, an epicyclic gearbox, a speed sensor, a motor, and a data acquisition system.In this experimental study, four types of gear failures were induced: tooth crack, gear wear, tooth break, and gear pitting, each with three levels of fault severity (i.e., light, medium, and severe), as summarized in Table 5.Consequently, the dataset includes twelve distinct gear health conditions.Vibration data were acquired using a three-axis accelerometer mounted on the gearbox, sampled at 12.8 kHz over a 60 s duration.</p>
<p>Baseline Methods</p>
<p>To demonstrate the practicality and effectiveness of the proposed MTAGCN, we conducted a series of comparative experiments across three rotating machinery case studies.In these experiments, MTAGCN was benchmarked against four state-of-the-art (SOTA) methods, including two single-task learning (STL)-based approaches-MRFGCN and IDBN-and two multi-task learning (MTL)-based approaches-MTCNN and JLCNN.</p>
<p>Implementation Details</p>
<p>All experiments were implemented using the PyTorch 1.9.0 framework with Python 3.8.3.The computational workload was carried out on a workstation equipped with an Intel i5-10400F CPU, 32 GB of RAM, and an NVIDIA RTX 3070 GPU.In all three case studies, the acquired vibration signals were segmented into subsamples using a sliding window of 1024 points without overlap between consecutive segments.All models were trained for 300 epochs.For the proposed MTAGCN framework, the batch size was set to 32 during training and 16 during testing.</p>
<p>Diagnosis Results Analysis</p>
<p>The experimental results on the CRB dataset are summarized in Table 6 and illustrated in Figure 12.As shown in Table 6 and Figure 12, the proposed MTAGCN achieves 100% diagnostic accuracy on both the FTI and FSI tasks, demonstrating its superior performance and robustness.While MTCNN and JLCNN attain over 95% accuracy on the FTI task, their performance on the FSI task drops to approximately 90%, which is notably lower than that of MTAGCN.Moreover, other benchmark methods, such as MRFGCN and IDBN, exhibit even lower accuracy on the FSI task, with results not exceeding 90%.Additionally, the proposed MTAGCN displays significantly smaller error bars compared to other state-of-theart methods, further validating its enhanced stability and robustness on the CRB dataset.</p>
<p>As demonstrated in Table 7 and Figure 13, the proposed MTAGCN consistently surpasses other state-of-the-art methods in both the FTI and FSI tasks on the HUSTbearing dataset, underscoring its superior adaptability and diagnostic accuracy across varying rotational speed conditions.As shown in Table 7 and Figure 13, the proposed MTAGCN significantly outperforms existing state-of-the-art (SOTA) methods in both the FTI and FSI tasks across all four rotating speed scenarios.While MTCNN and JLCNN exhibit competitive performance in the FTI task, MTAGCN consistently surpasses JLCNN in the FSI task throughout all experiments.This demonstrates the effectiveness of MTAGCN in handling multiple tasks simultaneously and improving diagnostic performance holistically.In contrast, JLCNN and other single-task learning (STL)-based methods exhibit relatively lower recognition accuracy in both tasks.Notably, MTAGCN achieves an average diagnostic accuracy of 100% across both tasks, outperforming all comparison methods.Although MRFGCN and IDBN perform well under the T 1 scenario, their accuracy drops considerably in T 2 , primarily because STL-based methods treat tasks independently and fail to leverage shared features across tasks.Moreover, the diagnostic accuracy of most SOTA methods tends to decline as the rotation speed increases, highlighting the challenge of maintaining robustness under varying operational conditions.The diagnostic results from the MCC5-THU are described in Table 8, and its corresponding histogram is illustrated in Figure 14.</p>
<p>Feature Visualization</p>
<p>In this section, t-distributed stochastic neighbor embedding (t-SNE) is employed to visualize high-dimensional features in a low-dimensional space, thereby providing deeper insight into the strengths and limitations of each method.The HUSTbearing dataset is selected as a representative case for feature visualization.The detailed t-SNE visualization results for all compared methods under the T 1 scenario at 65 Hz are illustrated in Figure 15.</p>
<p>As illustrated in Figure 15, the extracted features corresponding to the four domains (i.e., four fault categories) are well aligned and clearly separable.Furthermore, the multitask learning (MTL)-based methods demonstrate superior capability in learning more discriminative and informative features compared to single-task learning (STL)-based methods, which is consistent with the diagnostic performance results.These observations confirm that the proposed method effectively captures task-invariant and classdiscriminative features, which are essential for enhancing the performance of MTL-based fault diagnosis models.</p>
<p>Classification Performance</p>
<p>To evaluate the diagnostic effectiveness of the proposed method, confusion matrices are utilized to present the relationship between true and predicted labels in a structured format.The confusion matrix results for all methods on the CRB dataset are shown in Figure 16.It is evident that the proposed MTAGCN accurately classifies all samples in the FTI task.In comparison to other MTL-based methods (i.e., MTAGCN, MTCNN, and JLCNN), the STL-based methods (i.e., MRFGCN and IDBN) demonstrate lower performance in identifying health conditions.</p>
<p>As shown in Figure 17, our proposed MTAGCN achieves the highest classification performance.In contrast, the worst-performing method (i.e., IDBN) frequently misclas-sifies Label 2 into other labels.Compared to the FTI task, the diagnostic accuracies of all methods-except for MTAGCN-decrease in the FSI task.</p>
<p>Model Discussions</p>
<p>Detailed Structure and Complexity of MTAGCN</p>
<p>To illustrate the detailed structure and explore the complexity of the proposed MTAGCN, the corresponding parameters are presented in Table 9.As observed from the results, the total number of FLOPs for the proposed MTAGCN is 15,497,888.</p>
<p>Ablation Study</p>
<p>In this section, an ablation study is conducted to evaluate the performance of each key innovation in our proposed method.The results of different variants are presented as follows: (1) Method 1 (M1), MTAGCN without GCNs, and (2) Method 2 (M2), MTAGCN without the attention mechanism (AM).</p>
<p>As shown in the data from Table 10, our proposed MTAGCN consistently outperforms the four variants in terms of accuracy.Specifically, MTAGCN surpasses M1 and M2 by 0.96% and 1.46% at 65 Hz, respectively.These significant improvements highlight the importance of incorporating GCNs and the attention mechanism (AM) into MTL-based fault diagnosis.</p>
<p>Evaluation Metrics</p>
<p>To further demonstrate the effectiveness and practicality of the proposed MTAGCN, four additional evaluation metrics-AUC, precision, recall, and F1-score-are introduced in this section.A detailed comparison of these evaluation metrics on the CRB dataset is presented in Table 11.The proposed MTAGCN achieves a mean accuracy, AUC, precision, recall, and F1score of 100% on the CRB dataset, significantly outperforming other state-of-the-art methods.This superior performance can be attributed to the ability of MTAGCN to effectively mine representative features from task-shared modules, a challenge for other methods, which leads to a decline in their fault diagnostic performance.</p>
<p>Conclusions</p>
<p>In this paper, we propose the MTAGCN framework to model two specific tasks within a unified deep network, enabling multi-task learning (MTL) for simultaneous fault diagnosis of fault types and severities.Extensive experimental results demonstrate the effectiveness of the proposed method and its superiority over various state-of-the-art (SOTA) approaches.The key conclusions of this work are as follows: (1) the graph block allows features extracted by the Conv block to be automatically transformed into instance graphs, complementing the fault-related information; (2) the task-shared module learns common features, while the two task-specific modules capture task-invariant features; (3) experimental results from three case studies of rotating machinery show that the proposed MTAGCN</p>
<p>Multi-task learning (MTL), as a practical and powerful technique for mitigating the first limitation, has been used to learn the task-shared features from various diagnostic tasks.Recently, MTL-based methods have received growing interest in handling multiple tasks jointly.MTL-based methods can enhance model generalization by integrating useful information from multiple tasks, sharing features across tasks, and training within a unified framework.MTL-based techniques have been widely used in various fields such as medical image classification, fault diagnosis, question answering, object detection, and so on.Zhao et al. proposed a novel MTL-based self-supervised transfer learning paradigm by integrating the multi-perspective feature transfer to achieve fault diagnosis [19].Zhang et al. proposed an auxiliary prior knowledge-informed framework based on MTL for few-shot fault diagnosis [20].Zheng et al. proposed an MTL-based framework with deep inter-task interactions to achieve breast tumor segmentation and classification [21].Zhang et al. proposed a novel uncertainty bidirectional guidance MTL-based framework (UBGM)</p>
<p>Figure 1 .
1
Figure 1.Overview of the proposed MTAGCN.The main contributions of this article are listed as follows.(1) The multi-task GCN is proposed to achieve end-to-end fault diagnosis by modeling the fault type diagnosis task and fault severity diagnosis task simultaneously in a unified DL-based framework.(2) The instance graphs are constructed to mine structural characteristics in the taskshared module.By integrating the AM modules at multiple levels, the proposed MTAGCN can more effectively capture critical task-specific information in specific task modules.(3) Extensive experiments are conducted to show the feasibility and effectiveness of our proposed method.</p>
<p>Figure 2 .
2
Figure 2. Detailed architecture of CNNs.</p>
<p>Figure 3 .
3
Figure 3. Detailed architecture of GCNs.</p>
<p>Figure 4 .
4
Figure 4. Detailed architecture of the proposed method.</p>
<p>Figure 4
4
Figure 4 illustrates the MTL-based architecture, which comprises three parallel branches.The top and bottom branches are task-specific pathways, each consisting of attention modules (AM blocks), convolutional blocks, and pooling layers with skip connections to enhance information flow.The middle branch functions as a shared feature extraction backbone, processing input waveform signals through alternating convolutional blocks, graph blocks, and pooling layers.The overall architecture integrates task-specific losses using the formula L = λ 1 L 1 + λ 2 L 2 and outputs to their respective classification matrices via global average pooling (GAP) and Softmax layers.This design demonstrates an effective multi-task learning paradigm that preserves task specificity while enabling efficient feature sharing.</p>
<p>Figure 5 .
5
Figure 5. Detailed process of the Conv block and instance graph generation.(a) Conv block.(b) Instance graph generation.</p>
<p>Figure 6 .
6
Figure 6.The detailed process of the SE-Net.</p>
<p>Figure 7 .
7
Figure 7. Experimental benchmark of the CRB dataset.(a) Typical photograph.(b) Schematic diagram.</p>
<p>Figure 8 .
8
Figure 8.The bearing faults in the CRB dataset.(a-d) Inner race fault.(e-h) Outer race fault.(i-l) Roller fault.</p>
<p>Figure 9 .
9
Figure 9. Experimental platform of the HUSTbearing dataset.</p>
<p>Figure 10 .
10
Figure 10.The bearing faults in the HUSTbearing dataset.(a) Normal.(b) Inner ring fault.(c) Outer ring fault.(d) Roller ball fault.(e) Inner ring fault.(f) Outer ring fault.(g) Roller ball fault.</p>
<p>Figure 11 .Table 5 .Fault
115
Figure 11.Experimental platform of the MCC5-THU Gearbox dataset.Table 5.The details of gear fault parameters.Fault Parameter of Gear Fault Degree Light Medium High Teeth Crack Depth 1/4 of the teeth height 1/2 of the teeth height 3/4 of the teeth height Gear Wear 1/3 of the teeth surface area</p>
<p>Figure 12 .
12
Figure 12.Ten experimental diagnostic results of all methods under the CRB dataset.(a) FTI task.(b) FSI task.</p>
<p>Figure 13 .
13
Figure 13.Diagrams for diagnostic results of all methods under four rotating speeds: (a) 65 Hz, (b) 70 Hz, (c) 75 Hz, and (d) 80 Hz.</p>
<p>Figure 14 .
14
Figure 14.Diagnostic accuracy of two fault diagnosis tasks under MCC5-THU dataset.</p>
<p>Figure 15 .
15
Figure 15.Cont.</p>
<p>Figure 16 .
16
Figure 16.Confusion matrix results of all methods for the FTI task on the CRB dataset.(a) Ours.(b) MTCNN.(c) JLCNN.(d) MRFGCN.(e) IDBN.</p>
<p>Figure 17 .
17
Figure 17.Confusion matrix results of all methods for the FSI task on the CRB dataset.(a) Ours.(b) MTCNN.(c) JLCNN.(d) MRFGCN.(e) IDBN.</p>
<p>Table 1 .
1
Specifications of the cylindrical rolling bearing.
ParametersValueDiameter (Inner race)25 mmDiameter (Outer race)52 mmPitch Diameter (D)38.9 mmBall Diameter (d)7.5 mmNumber of rolling elements (N)13Contact angle (φ)0 •</p>
<p>Table 2 .
2
Defect width of the cylindrical roller bearing.
Bearing ComponentLabelDefect Width (mm)IRF-I0.43 mmInner raceIRF-II IRF-III1.01 mm 1.56 mmIRF-IV2.03 mmORF-I0.42 mmOuter raceORF-II ORF-III0.86 mm 1.55 mmORF-IV1.97 mmRF-I0.49 mmRollerRF-II RF-III1.16 mm 1.73 mmRF-IV2.12 mm</p>
<p>Table 3 .
3
Parameters of the tested bearings.
ParametersValueShaft Diameter38.52 mmBall Diameter7.94 mmPitch Diameter (D)9Number of rolling balls (N)9</p>
<p>Table 4 .
4
Fault diagnosis based on the HUSTbearing dataset.
Rotating SpeedLoadSamplesTasks T 1T 265 Hz200 × 7NN70 Hz 75 HzEqual200 × 7 200 × 7IRF ORFI severity II severity80 Hz200 × 7RBF</p>
<p>Table 6 .
6
Diagnostic performance of two fault diagnosis tasks on the CRB dataset (T 1 means the fault type diagnosis (FTD) task; T 2 means the fault severity diagnosis (FSD) task).
OursMTCNNJLCNNMRFGCNIDBNT 1T 2T 1T 2T 1T 2T 1T 2T 1T 21001000.21 99.14 ±0.73 95.64 ±0.45 98.09 ±0.89 93.78 ±0.44 97.49 ±1.24 89.14 ±0.51 96.84 ±87.26</p>
<p>Table 7 .
7
Diagnostic accuracy (%) of two fault diagnosis tasks under four rotating speeds (T 1 means the fault type diagnosis (FTD) task; T 2 means the fault severity diagnosis (FSD) task).
SpeedOurs T 1T 2MTCNN T 1T 2JLCNN T 1T 2MRFGCN T 1T 2IDBN T 1T 265 Hz10010098.5096.8997.7495.9898.9485.2492.3878.2570 Hz10010094.2099.1693.5190.5597.6782.9790.2571.5675 Hz10010098.3196.4193.7291.0895.1981.1288.6466.7480 Hz10010097.5895.9492.5790.4195.8480.5689.0568.57Average10010097.1597.1094.3992.0196.9182.4790.0871.28</p>
<p>Table 8 .
8
Diagnostic accuracy of two fault diagnosis tasks under the MCC5-THU dataset.
OursMTCNNJLCNNMRFGCNIDBNT 1T 2T 1T 2T 1T 2T 1T 2T 1T 299.51 ±99.47 ±98.64 ±93.71 ±97.64 ±89.47 ±95.27 ±85.64 ±95.29 ±81.67 ±0.110.140.390.870.610.920.651.410.761.54</p>
<p>Table 9 .
9
Detailed structure and complexity of MTAGCN.
ComponentLayerFilterInput SizeOutput SizeParametersFLOPsConv116 × 7 × 11 × 102416 × 1024192229,376Conv216 × 7 × 116 × 102416 × 102418723,670,016Graph16 × 7 × 116 × 102416 × 102418723,670,016MaxPool4 × 1 (stride = 4)16 × 102416 × 256016,384Conv332 × 5 × 116 × 25632 × 256192229,376Task-sharedConv432 × 5 × 116 × 25632 × 25618723,670,016Graph32 × 5 × 116 × 25632 × 25618723,670,016MaxPool4 × 1 (stride = 4)16 × 25632 × 64016,384Conv564 × 3 × 132 × 6464 × 64192229,376Conv664 × 3 × 132 × 6464 × 6418723,670,016Graph64 × 3 × 132 × 6464 × 6418723,670,016Attentionk = 316 × 102416 × 10246192Att Conv132 × 1 × 116 × 102432 × 102452802,621,440Att MaxPool4 × 1 (stride = 4)32 × 102432 × 256016,384Attentionk = 332 × 25632 × 256016,384Task-specificAtt Conv264 × 1 × 132 × 25664 × 25652802,621,440Att MaxPool4 × 1 (stride = 4)64 × 25664 × 64016,384Attentionk = 364 × 6464 × 646192Att Conv3128 × 1 × 1128 × 64128 × 6452802,621,440Att MaxPool4 × 1 (stride = 4)128 × 64128 × 16016,384Attentionk = 316 × 102416 × 10246192Att Conv132 × 1 × 116 × 102432 × 102452802,621,440Att MaxPool4 × 1 (stride = 4)32 × 102432 × 256016,384Attentionk = 332 × 25632 × 256016,384Task-specificAtt Conv264 × 1 × 132 × 25664 × 25652802,621,440Att MaxPool4 × 1 (stride = 4)64 × 25664 × 64016,384Attentionk = 364 × 6464 × 646192Att Conv3128 × 1 × 1128 × 64128 × 6452802,621,440Att MaxPool4 × 1 (stride = 4)128 × 64128 × 16016,384AdaptiveAvgPool1 × 1128 × 16128 × 104096Task FC LayersTask 1 FC128 × 4128 × 14 × 15161024Task 2 FC128 × 4128 × 14 × 15161024</p>
<p>Table 10 .
10
Experimental results of the ablation study.
TasksM1M2Ours65 Hz99.04%98.54%100%70 Hz98.76%97.69%100%75 Hz97.89%97.08%100%</p>
<p>Table 11 .
11
Experimental results of the CRB dataset using five evaluation metrics.
MethodsAccuracy (%)AUCPrecision (%)Recall (%)F1-Score (%)Ours100100100100100MTCNN99.1497.4896.8596.9196.88JLCNN98.0995.6296.1796.6496.40MRFGCN97.4993.2894.5394.0794.29IDBN96.8493.6193.0792.5192.79
can effectively mine diagnostic features for both tasks, outperforming comparison methods in multi-task-based fault diagnosis.Future work on the MTAGCN framework will focus on (1) achieving domain adaptation to handle variable working conditions; (2) exploring multi-sensor data fusion to enhance robustness and anti-noise capabilities; and (3) advancing research in real-time fault diagnosis with the goal of enabling online multi-task learning.Data Availability Statement:The authors do not have permission to share private data.Conflicts of Interest:The authors declare no conflicts of interest.Author Contributions: B.W.: Conceptualization, methodology, software, validation, formal analysis, investigation, resources, data curation, writing-original draft preparation.S.Z.: writing-review and editing, visualization, supervision, project administration, funding acquisition.All authors have read and agreed to the published version of the manuscript.Informed Consent Statement: Informed consent was obtained from all subjects involved in the study.
MSIFT: A novel end-to-end mechanical fault diagnosis framework under limited &amp; imbalanced data using multi-source information fusion. Y Yu, H R Karimi, L Gelman, A E Cetin, Expert Syst. Appl. 2741269472025</p>
<p>Multi-source domain adversarial graph convolutional networks for rolling mill health states diagnosis under variable working conditions. S Zhao, L Bao, C Hou, Y Bai, Y Yu, 10.1177/14759217231225986Struct. Health Monit. 232024</p>
<p>Enhancing gearbox fault diagnosis through advanced feature engineering and data segmentation techniques. Machines. K Shukla, W Holderbaum, T Theodoridis, G Wei, 10.3390/machines12040261202412261</p>
<p>CPL-SLAM: Centralized Collaborative Multi-Robot Visual-Inertial SLAM Using Point-and-Line Features. X Liu, S Wen, H Liu, F R Yu, 10.1109/JIOT.2025.3548958IEEE Internet Things J. 2025</p>
<p>A two-stage importance-aware subgraph convolutional network based on multi-source sensors for cross-domain fault diagnosis. Y Yu, Y He, H R Karimi, L Gelman, A E Cetin, 10.1016/j.neunet.2024.106518Neural Netw. 1792024. 106518</p>
<p>Probability distributions and typical sparsity measures of Hilbert transform-based generalized envelopes and their application to machine condition monitoring. B Chen, W A Smith, Y Cheng, F Gu, F Chu, W Zhang, A D Ball, 10.1016/j.ymssp.2024.112026Mech. Syst. Signal Process. 2242025. 112026</p>
<p>A product envelope spectrum generated from spectral correlation/coherence for railway axle-box bearing fault diagnosis. B Chen, Y Cheng, P Allen, S Wang, F Gu, W Zhang, A D Ball, 10.1016/j.ymssp.2024.112262Mech. Syst. Signal Process. 2252025. 112262</p>
<p>Kernel extreme learning machine based hierarchical machine learning for multi-type and concurrent fault diagnosis. Q Chen, H Wei, M Rashid, Z Cai, 10.1016/j.measurement.2021.109923Measurement. 1842021. 109923</p>
<p>Fault diagnosis of shield machine based on RFE and ELM. Res. Sq. 2020, preprint. X Liu, W Qi, Y Wang, C Shao, Q Cong, 10.21203/rs.3.rs-20661/v1</p>
<p>An improved multi-object classification algorithm for visual SLAM under dynamic environment. S Wen, X Liu, Z Wang, H Zhang, Z Zhang, W Tian, 10.1007/s11370-021-00400-8Intell. Serv. Robot. 152022</p>
<p>A robust MPC approach for platooning control of automated vehicles with constraints on acceleration. C Huang, Q Shi, W Ding, P Mei, H R Karimi, 10.1016/j.conengprac.2023.105648Control Eng. Pract. 1392023. 105648</p>
<p>Battery state estimation methods and management system under vehicle-cloud collaboration: A Survey. P Mei, H R Karimi, J Xie, F Chen, L Ou, S Yang, C Huang, 10.1016/j.rser.2024.114857Renew. Sustain. Energy Rev. 2062024. 114857</p>
<p>Digital twin framework using real-time asset tracking for smart flexible manufacturing system. Machines. A Ullah, M Younas, M S Saharudin, 10.3390/machines13010037202513</p>
<p>Stage-Based Remaining Useful Life Prediction for Bearings Using GNN and Correlation-Driven Feature Extraction. Machines. G Huang, W Lei, X Dong, D Zou, S Chen, X Dong, 10.3390/machines1301004320251343</p>
<p>A new multi-source information domain adaption network based on domain attributes and features transfer for cross-domain fault diagnosis. Y Yu, H R Karimi, P Shi, R Peng, S Zhao, 10.1016/j.ymssp.2024.111194Mech. Syst. Signal Process. 2112024. 111194</p>
<p>Rolling mill health states diagnosing method based on multi-sensor information fusion and improved DBNs under limited datasets. Y Yu, P Shi, J Tian, X Xu, C Hua, 10.1016/j.isatra.2022.08.002ISA Trans. 1342023</p>
<p>A novel multi-source sensing data fusion driven method for detecting rolling mill health states under imbalanced and limited datasets. P Shi, Y Yu, H Gao, C Hua, 10.1016/j.ymssp.2022.108903Mech. Syst. Signal Process. 1712022. 108903</p>
<p>Multi-source heterogeneous information fusion fault diagnosis method based on deep neural networks under limited datasets. D Han, Y Zhang, Y Yu, J Tian, P Shi, 10.1016/j.asoc.2024.111371Appl. Soft Comput. 1542024. 111371</p>
<p>A Novel Multi-Task Self-Supervised Transfer Learning Framework for Cross-Machine Rolling Bearing Fault Diagnosis. L Zhao, Y He, D Dai, X Wang, H Bai, W Huang, 10.3390/electronics13234622202413</p>
<p>Prior knowledge-informed multi-task dynamic learning for few-shot machinery fault diagnosis. T Zhang, J Chen, Z Ye, W Liu, J Tang, 10.1016/j.eswa.2025.126439Expert Syst. Appl. 2712025. 126439</p>
<p>Multi-task interaction learning for accurate segmentation and classification of breast tumors in ultrasound images. S Zheng, J Li, L Qiao, X Gao, 10.1088/1361-6560/adae4dPhys. Med. Biol. 70650062025</p>
<p>Uncertainty bidirectional guidance of multi-task mamba network for medical image classification and segmentation. Signal Image Video Process. X Wu, G Gou, 10.1007/s11760-024-03633-z202519</p>
<p>A CNN-based methodology for identifying mechanical faults in induction motors using thermography. Machines. O Trejo-Chavez, I A Cruz-Albarran, E Resendiz-Ochoa, A Salinas-Aguilar, L A Morales-Hernandez, J A Basurto-Hurtado, C A Perez-Ramirez, 10.3390/machines11070752202311752</p>
<p>Fault identification of direct-shift gearbox using variational mode decomposition and convolutional neural network. Machines. R Kumar, P Kumar, G Vashishtha, S Chauhan, R Zimroz, S Kumar, R Kumar, M K Gupta, N S Ross, 10.3390/machines12070428202412</p>
<p>Multi-Sensor Information Fusion with Multi-Scale Adaptive Graph Convolutional Networks for Abnormal Vibration Diagnosis of Rolling Mill. Machines. R Peng, C Gong, S Zhao, 10.3390/machines13010030202513</p>
<p>Self-Explanatory Fault Diagnosis Framework for Industrial Processes Using Graph Attention. C S Kim, H B Kim, J M Lee, 10.1109/TII.2025.3526708IEEE Trans. Ind. Inform. 212025</p>
<p>Integrating self-attention mechanisms in deep learning: A novel dual-head ensemble transformer with its application to bearing fault diagnosis. Q Snyder, Q Jiang, E Tripp, 10.1016/j.sigpro.2024.109683Signal Process. 2025, 227, 109683. [CrossRef</p>
<p>Spatial transformer networks. M Jaderberg, K Simonyan, A Zisserman, Adv. Neural Inf. Process. Syst. 282015</p>
<p>ECA-Net: Efficient channel attention for deep convolutional neural networks. Q Wang, B Wu, P Zhu, P Li, W Zuo, Q Hu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSeattle, WA, USAJune 2020</p>
<p>Efficient multi-scale attention module with cross-spatial learning. D Ouyang, S He, G Zhang, M Luo, H Guo, J Zhan, Proceedings of the ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). the ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)Rhodes Island, GreeceJune 2023</p>
<p>D P Kingma, J Ba, Adam, arXiv:1412.6980A method for stochastic optimization. 2014</p>
<p>Bearing defect size assessment using wavelet transform based Deep Convolutional Neural Network (DCNN). A Kumar, Y Zhou, C P Gandhi, R Kumar, J Xiang, 10.1016/j.aej.2020.03.034Alex. Eng. J. 592020</p>
<p>Domain generalization for cross-domain fault diagnosis: An application-oriented perspective and a benchmark study. C Zhao, E Zio, W Shen, 10.1016/j.ress.2024.109964Reliab. Eng. Syst. Saf. 2452024. 109964</p>
<p>Multi-mode fault diagnosis datasets of gearbox under variable working conditions. Data Brief. S Chen, Z Liu, X He, D Zou, D Zhou, 10.1016/j.dib.2024.1104532024. 11045354</p>
<p>Disclaimer/Publisher's Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods. instructions or products referred to in the content</p>            </div>
        </div>

    </div>
</body>
</html>
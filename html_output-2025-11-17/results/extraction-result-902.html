<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-902 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-902</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-902</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-b1c9c455acb1393f1bf60ec4095ee343531e404e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b1c9c455acb1393f1bf60ec4095ee343531e404e" target="_blank">MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> MetaTool is introduced, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools, and recommends that tool developers choose an appropriate rewrite model for generating new descriptions based on the downstream LLM the tool will apply to.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific tools. However, in scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision-making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. Therefore, in this paper, we introduce MetaTool, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. Specifically, we create a dataset called ToolE within the benchmark. This dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-tool scenarios. Subsequently, we set the tasks for both tool usage awareness and tool selection. We define four subtasks from different perspectives in tool selection, including tool selection with similar choices, tool selection in specific scenarios, tool selection with possible reliability issues, and multi-tool selection. We conduct experiments involving eight popular LLMs and find that the majority of them still struggle to effectively select tools, highlighting the existing gaps between LLMs and genuine intelligent agents. However, through the error analysis, we found there is still significant room for improvement. Finally, we conclude with insights for tool developers -- we strongly recommend that tool developers choose an appropriate rewrite model for generating new descriptions based on the downstream LLM the tool will apply to. Our code is in https://github.com/HowieHwong/MetaTool.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e902.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e902.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaTool (TOOLE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaTool Benchmark and the TOOLE dataset for tool usage awareness and tool selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark and dataset (TOOLE) designed to evaluate LLMs' awareness of when to call external tools and their ability to select appropriate tools (single- and multi-tool), with controlled subtasks for similar-tool discrimination, scenario-specific selection, reliability (missing-ground-truth-tool) settings, and multi-tool selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Multiple LLMs evaluated in this paper (ChatGPT; ChatGLM2-6B; Llama2-7b, 13b; Vicuna-7b, 13b, 33b; Koala-13b)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General-purpose transformer-based LLMs (chat/tuned variants) evaluated via prompting; no new architectural modules were introduced in the experiments — evaluation used prompting/ in-context examples and tool descriptions as inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (ChatGLM2: 6B; Llama2: 7B/13B; Vicuna: 7B/13B/33B; Koala: 13B; ChatGPT size unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>TOOLE tasks: (1) Awareness of tool usage (decide whether to use a tool) and (2) Tool selection subtasks (similar choices, scenario-specific, reliability/missing-ground-truth, multi-tool selection)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / tool selection / multi-tool reasoning (interactive/procedural decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Summary across evaluated LLMs: Awareness (Table 2) zero-shot F1 varied widely (examples: ChatGPT F1=77.81%; many open models much lower, e.g., Llama2-13b F1=11.53%); 5-shot prompting raised F1 for many models (ChatGPT F1=81.87%; Llama2-13b F1=54.32%; maximal F1 delta up to +42.79 percentage points). Tool selection (similar-choices) CSR zero-shot ranged roughly from ~45% to ~73% across models (best ~73% Vicuna-7b zero-shot; ChatGPT ~69%), with modest few-shot gains (few %). Reliability (where ground-truth tool is absent) CSR was very low for most models (many < 5% zero-shot); ChatGPT improved from 50.35%→78.49% CSR on the reliability task with 5-shot prompting (Δ +28.14). Multi-tool selection: ChatGPT achieved high multi-tool correct-selection rates (2/2 CSR ≈ 88%), while many other models were far lower; some models relied on prompt-specified required number of tools (performance rose when prompt forced choosing two tools).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>No new architectural modules used in evaluated models; paper discusses the ReAct-style prompt decomposition (Thought vs Action) and evaluates models using tool-description inputs and prompt engineering; it does not add planners, external memory, or explicit tool-use modules to models.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Prompting / in-context few-shot used in experiments; paper references prior approaches (instruction tuning, self-instruction, ToolFormer-style self-supervision) but does not perform further model fine-tuning itself.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (few-shot in-context examples), dataset / prompt engineering (explicit 'reasons for tool usage' in Thought prompt), and tool-description rewriting (rewriting descriptions via other LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>1) Few-shot prompting: 5-shot in-context examples were provided for awareness and selection tasks; 2) Prompt template: explicit separation of Thought (whether to use a tool) and Action (which tool) with reasons for tool usage added; 3) Tool-description rewriting: authors rewrote tool descriptions via Llama2-70b and GPT-4 and re-evaluated model CSR to test sensitivity to wording; 4) Dataset processing: merging/decomposition of overlapping tools to create unique single-label mappings and curated multi-tool examples.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Quantitative effects reported in paper: Few-shot prompting often improved awareness F1 substantially (e.g., Llama2-13b F1 +42.79 pp, Vicuna-7b F1 +42.28 pp; ChatGPT F1 +4.06 pp). For tool-selection similar-choices CSR, few-shot improvements were modest (typically a few percentage points; e.g., ChatGPT +3.89 pp). On the reliability task, some models showed large gains with few-shot (ChatGPT reliability CSR +28.14 pp from 50.35%→78.49%; ChatGLM2 +9.05 pp). Tool-description rewriting had model-dependent effects: Llama2-70b rewrites improved Llama2-13b CSR by ~+7.83%, while GPT-4 rewrites harmed some models (sharp declines for ChatGLM and Llama2 series) but boosted Vicuna series, indicating sensitivity to rewrite source and model training corpus. Multi-tool behavior improved when prompts explicitly forced the model to select a specified number of tools (e.g., some Vicuna models' CSR rose >20% when told to return two tools).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper attributes the gap to multiple factors: models' lack of tool-usage awareness (not recognizing capability boundaries), hallucination and sycophancy (fabricating tools or selecting unrelated ones rather than returning 'none'), poor handling of long tool lists / long text, insufficient semantic discrimination among similar tools, scenario-specific biases and varying prior knowledge across domains, and reliance on prompt framing and prior training corpus alignment (sensitivity to description wording). The paper suggests these are not due to a single architectural change in evaluated models but rather due to training/data/prompting and information-retrieval-like selection challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>follow_on_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ToolLLM: Facilitating Large Language Models to Master 16000+ Real-World APIs <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>GPT4Tools: Teaching large language model to use tools via self-instruction <em>(Rating: 2)</em></li>
                <li>On the tool manipulation capability of open-source large language models <em>(Rating: 1)</em></li>
                <li>TPTU: Task planning and tool usage of large language model-based AI agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-902",
    "paper_id": "paper-b1c9c455acb1393f1bf60ec4095ee343531e404e",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "MetaTool (TOOLE)",
            "name_full": "MetaTool Benchmark and the TOOLE dataset for tool usage awareness and tool selection",
            "brief_description": "A benchmark and dataset (TOOLE) designed to evaluate LLMs' awareness of when to call external tools and their ability to select appropriate tools (single- and multi-tool), with controlled subtasks for similar-tool discrimination, scenario-specific selection, reliability (missing-ground-truth-tool) settings, and multi-tool selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Multiple LLMs evaluated in this paper (ChatGPT; ChatGLM2-6B; Llama2-7b, 13b; Vicuna-7b, 13b, 33b; Koala-13b)",
            "model_description": "General-purpose transformer-based LLMs (chat/tuned variants) evaluated via prompting; no new architectural modules were introduced in the experiments — evaluation used prompting/ in-context examples and tool descriptions as inputs.",
            "model_size": "various (ChatGLM2: 6B; Llama2: 7B/13B; Vicuna: 7B/13B/33B; Koala: 13B; ChatGPT size unspecified)",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "TOOLE tasks: (1) Awareness of tool usage (decide whether to use a tool) and (2) Tool selection subtasks (similar choices, scenario-specific, reliability/missing-ground-truth, multi-tool selection)",
            "interactive_task_type": "tool use / tool selection / multi-tool reasoning (interactive/procedural decision-making)",
            "interactive_performance": "Summary across evaluated LLMs: Awareness (Table 2) zero-shot F1 varied widely (examples: ChatGPT F1=77.81%; many open models much lower, e.g., Llama2-13b F1=11.53%); 5-shot prompting raised F1 for many models (ChatGPT F1=81.87%; Llama2-13b F1=54.32%; maximal F1 delta up to +42.79 percentage points). Tool selection (similar-choices) CSR zero-shot ranged roughly from ~45% to ~73% across models (best ~73% Vicuna-7b zero-shot; ChatGPT ~69%), with modest few-shot gains (few %). Reliability (where ground-truth tool is absent) CSR was very low for most models (many &lt; 5% zero-shot); ChatGPT improved from 50.35%→78.49% CSR on the reliability task with 5-shot prompting (Δ +28.14). Multi-tool selection: ChatGPT achieved high multi-tool correct-selection rates (2/2 CSR ≈ 88%), while many other models were far lower; some models relied on prompt-specified required number of tools (performance rose when prompt forced choosing two tools).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "No new architectural modules used in evaluated models; paper discusses the ReAct-style prompt decomposition (Thought vs Action) and evaluates models using tool-description inputs and prompt engineering; it does not add planners, external memory, or explicit tool-use modules to models.",
            "training_method": "Prompting / in-context few-shot used in experiments; paper references prior approaches (instruction tuning, self-instruction, ToolFormer-style self-supervision) but does not perform further model fine-tuning itself.",
            "intervention_type": "prompting strategy (few-shot in-context examples), dataset / prompt engineering (explicit 'reasons for tool usage' in Thought prompt), and tool-description rewriting (rewriting descriptions via other LLMs).",
            "intervention_description": "1) Few-shot prompting: 5-shot in-context examples were provided for awareness and selection tasks; 2) Prompt template: explicit separation of Thought (whether to use a tool) and Action (which tool) with reasons for tool usage added; 3) Tool-description rewriting: authors rewrote tool descriptions via Llama2-70b and GPT-4 and re-evaluated model CSR to test sensitivity to wording; 4) Dataset processing: merging/decomposition of overlapping tools to create unique single-label mappings and curated multi-tool examples.",
            "intervention_effect": "Quantitative effects reported in paper: Few-shot prompting often improved awareness F1 substantially (e.g., Llama2-13b F1 +42.79 pp, Vicuna-7b F1 +42.28 pp; ChatGPT F1 +4.06 pp). For tool-selection similar-choices CSR, few-shot improvements were modest (typically a few percentage points; e.g., ChatGPT +3.89 pp). On the reliability task, some models showed large gains with few-shot (ChatGPT reliability CSR +28.14 pp from 50.35%→78.49%; ChatGLM2 +9.05 pp). Tool-description rewriting had model-dependent effects: Llama2-70b rewrites improved Llama2-13b CSR by ~+7.83%, while GPT-4 rewrites harmed some models (sharp declines for ChatGLM and Llama2 series) but boosted Vicuna series, indicating sensitivity to rewrite source and model training corpus. Multi-tool behavior improved when prompts explicitly forced the model to select a specified number of tools (e.g., some Vicuna models' CSR rose &gt;20% when told to return two tools).",
            "hypothesized_cause_of_gap": "Paper attributes the gap to multiple factors: models' lack of tool-usage awareness (not recognizing capability boundaries), hallucination and sycophancy (fabricating tools or selecting unrelated ones rather than returning 'none'), poor handling of long tool lists / long text, insufficient semantic discrimination among similar tools, scenario-specific biases and varying prior knowledge across domains, and reliance on prompt framing and prior training corpus alignment (sensitivity to description wording). The paper suggests these are not due to a single architectural change in evaluated models but rather due to training/data/prompting and information-retrieval-like selection challenges.",
            "follow_on_papers": null,
            "uuid": "e902.0",
            "source_info": {
                "paper_title": "MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-World APIs",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2
        },
        {
            "paper_title": "GPT4Tools: Teaching large language model to use tools via self-instruction",
            "rating": 2
        },
        {
            "paper_title": "On the tool manipulation capability of open-source large language models",
            "rating": 1
        },
        {
            "paper_title": "TPTU: Task planning and tool usage of large language model-based AI agents",
            "rating": 1
        }
    ],
    "cost": 0.01578175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MetaTool Benchmark for Large Language MODELs: DECIDING Whether to Use Tools and WHICH TO USE</h1>
<p>Yue Huang ${ }^{1 <em>}$, Jiawen Shi ${ }^{2}$, Yuan $\mathbf{L i}^{3}$, Chenrui Fan ${ }^{2}$, Siyuan $\mathbf{W u}^{2}$, Qihui Zhang ${ }^{1 \dagger}$<br>Yixin Liu ${ }^{1}$, Pan Zhou ${ }^{2}$, Yao Wan ${ }^{2}$, Neil Zhenqiang Gong ${ }^{4}$, Lichao Sun ${ }^{1 </em>}$<br>Lehigh University ${ }^{1}$<br>Huazhong University of Science and Technology ${ }^{2}$<br>University of Cambridge ${ }^{3}$<br>Duke University ${ }^{4}$</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific tools. However, in scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision-making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. Therefore, in this paper, we introduce MetatoOL, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. Specifically, we create a dataset called TOOLE within the benchmark. This dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-tool scenarios. Subsequently, we set the tasks for both tool usage awareness and tool selection. We define four subtasks from different perspectives in tool selection, including tool selection with similar choices, tool selection in specific scenarios, tool selection with possible reliability issues, and multi-tool selection. We conduct experiments involving eight popular LLMs and find that the majority of them still struggle to effectively select tools, highlighting the existing gaps between LLMs and genuine intelligent agents. However, through the error analysis, we found there is still significant room for improvement. Finally, we conclude with insights for tool developers - we strongly recommend that tool developers choose an appropriate rewrite model for generating new descriptions based on the downstream LLM the tool will apply to. Our TOOLE dataset is available at URL and code is in Github.</p>
<h2>1 INTRODUCTION</h2>
<p>Tool-empowered large language models (LLMs) (Qin et al., 2023a;b; Patil et al., 2023; Ruan et al., 2023; Cao et al., 2023; Zhou et al., 2023a) have recently attracted widespread attention. An important milestone for LLMs marching toward intelligent agents (Park et al., 2023; Li et al., 2023e) is the flexible use of tools (e.g., APIs (Qin et al., 2023b; Rapid, 2023) and plugins (OpenAI, 2023d)) to fulfill users' requirements. By utilizing tools, LLMs can obtain real-time data, such as getting the latest weather forecast (GPTStore, 2023); enhance interactions with users, like helping users book flight tickets (Deng et al., 2023); and better deal with uncertain questions by querying knowledge bases (Li et al., 2023c; Hu et al., 2023) or Internet (Lazaridou et al., 2022). Moreover, LLMs can also leverage specific tools to process multimodal information, thereby acquiring the same capabilities as multimodal models (Zhang et al., 2023; Yan et al., 2023; Yuan et al., 2023b). The capacity to use</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Comparison of previous work and MetaTool.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dimension</th>
<th style="text-align: center;">APIBank <br> (Li et al., 2023d)</th>
<th style="text-align: center;">GPT4Tool <br> (Yang et al., 2023c)</th>
<th style="text-align: center;">APIBench <br> (Patil et al., 2023)</th>
<th style="text-align: center;">ToolLLM <br> (Qin et al., 2023b)</th>
<th style="text-align: center;">ToolBench <br> (Xu et al., 2023)</th>
<th style="text-align: center;">ToolQA <br> Zhuang et al. (2023)</th>
<th style="text-align: center;">MetaTool <br> (Ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Evaluation Range</td>
<td style="text-align: center;">(1)(2)(3)</td>
<td style="text-align: center;">(3)(4)</td>
<td style="text-align: center;">(3)(4)</td>
<td style="text-align: center;">(3)(4)</td>
<td style="text-align: center;">(3)(3)(4)</td>
<td style="text-align: center;">(3)(4)</td>
<td style="text-align: center;">(1)(2)</td>
</tr>
<tr>
<td style="text-align: center;">Number of Tasks</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Reliability Test</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Multi-Tool Test</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Different Scenarios</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>tools enables LLMs to break through their own limitations, acquire external information, and thereby make more accurate and effective responses, providing users with better service.</p>
<p>Previous research has focused on how to enhance the ability of LLMs to use tools, including training models with instruction related to tool usage (Qin et al., 2023b; Tang et al., 2023; Schick et al., 2023), or augmenting the model’s problem-solving capabilities for domain-specific tasks through external APIs (Yang et al., 2023c). A typical process of employing LLMs to use tools is illustrated in Figure 1. Initially, users input a question (i.e., query) that triggers the tool usage. Based on prior research (Yang et al., 2023c; Qin et al., 2023b), under the ReAct (Yao et al., 2022) prompt approach, the process of using tools can be divided into four stages: Firstly, LLMs consider whether to employ a tool (1) and if so, which tools to select (2). The tool selection process involves directly having LLMs choose from a provided tool list (Yang et al., 2023c) or selecting via a retriever (Qin et al., 2023b). Next, LLMs configure the users’ input as tool parameters (3), then handle the results from the tool (4), and finally return the outcomes to the user.</p>
<p>With the emergence of more and more LLMs like open-source Llama2 (Touvron et al., 2023), Vicuna (Chiang et al., 2023), and closed-source ones like ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b), designing a comprehensive benchmark to measure the tool-related capability of these models has become crucial. Current studies have proposed several benchmarks (Xu et al., 2023; Qin et al., 2023b; Li et al., 2023d) about tool usage for LLMs, with the main contributions being limited to the stages (3) and (4). However, the awareness of tool usage (1) and tool selection (2) ability are also important for LLMs when they’re acting as intelligent agents including AutoGPT (Significant-Gravitas, 2023), MetaGPT (geekan, 2023) and BabyAGI (babyagi, 2023), or in the multi-agent environment where LLMs need to use tools to solve collaborative tasks (Shen et al., 2023; Qian et al., 2023; Park et al., 2023; Cai et al., 2023). As a result, it is necessary to establish a benchmark to evaluate LLMs’ tool usage consciousness and tool selection ability.</p>
<p>The difficulty in establishing such a benchmark is reflected in two aspects. The first one is the dataset: previous research proposed datasets (Qin et al., 2023b; Xu et al., 2023) lacked diverse user inputs, making it hard to cover various real-world scenarios. Additionally, there is an issue of overlapping in the dataset, meaning that a user’s needs can be addressed by more than one tool, which makes it challenging to conduct evaluations since user inputs can correspond to multiple tools. The second aspect is the task setting: the benchmark should include different tasks to evaluate LLMs from different perspectives, such as reliability, the performance under different scenarios in daily life. To address these issues, we propose Metatool, a benchmark designed to evaluate the awareness of tool usage and tool selection capability of LLMs. As demonstrated in Table 1, Metatool distinguishes itself from previous research efforts and is structured into three primary components:</p>
<ul>
<li>TOOLE dataset. We introduce TOOLE, a comprehensive dataset that encompasses a wide range of 21,127 user queries, with both single-tool and multi-tool queries. Different from the previous single-method generation (Yang et al., 2023c; Qin et al., 2023b), these queries are generated using various prompting methods, including emotional generation, keyword generation, direct</li>
</ul>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: MetaTool benchmark architecture. It contains the dataset TOOoLE with diverse queries related to different tools (a), and based on it, we conduct the evaluation of the awareness of tool usage and tool selection (b) and finally obtain the results of eight prominent LLMs (c).
diverse generation, and detailed generation. Moreover, to address the challenge of overlapping tool functionality, we undertake tool merging and decomposition.</p>
<ul>
<li>Evaluation on awareness of tool usage and tool selection. We construct a test set to evaluate the awareness of tool usage based on TOOLE and existing instruction datasets. Moreover, we formulate four distinct tasks to evaluate the tool selection ability of LLMs. These tasks are thoughtfully designed to assess semantic comprehension, adaptability, reliability, and inferential capability, namely tool selection with similar choices, tool selection in specific scenarios, tool selection with possible reliability issues, and multi-tool selection.</li>
<li>Empirical analysis on results. We rigorously evaluate the performance of eight well-known LLMs. We have observed that most LLMs struggle to recognize their capability boundaries and lack a good awareness of tool usage. Regarding tool selection, we find that while LLMs possess basic tool selection capabilities, the tool selection of most LLMs remains unreliable, with noticeable variations in performance across different daily scenarios. Moreover, the error analysis indicates there is still room for improvement in tool selection. Finally, by analysis the tool description, we gained two insights for tool developers.</li>
</ul>
<h1>2 MetaTool Design</h1>
<h3>2.1 Preliminary \&amp; Required Abilities</h3>
<p>In this section, we first introduced the composition of the TOOLE dataset, outlining how we generated user queries related to tools. Subsequently, we explained how we set up the evaluation tasks, including tool usage awareness and tool selection. The evaluation mainly requires LLMs with the following properties and abilities: (1) Less hallucination and sycophancy. The awareness of tool usage can reflect the truthfulness about whether an LLM has a clear understanding of its capabilities (e.g., realizing its capability limitation about what problems it cannot solve well and using tools for assistance), thereby helping to mitigate issues of hallucination (Ji et al., 2023; Sun et al., 2024) and sycophancy (Wei et al., 2023). (2) Recommendation and retrieval. Moreover, existing research has tentatively explored the potential of LLMs in applications like LLM-based recommendation systems (e.g., tool recommendation for users) (Gao et al., 2023; Wang et al., 2023e; Dai et al., 2023). In LLM-as-agent scenarios, LLMs usually need to select the specific tool according to the text description (Park et al., 2023; Shen et al., 2023; Ruan et al., 2023), actually is a kind of information retrieval (Sun et al., 2023), making the ability of tool selection crucial. (3) Task-level abilities. In MetaTool, we set four tasks as shown in Table 12. Incorporating similar tools for selection (i.e., Task 1) requires a high-level semantic comprehension for LLMs, and tool selection in specific scenarios tests the flexibility of LLMs when using tools in different scenarios (e.g., finance (Wu et al., 2023a) and biomedical domain (Zhang et al., 2023; Wang et al., 2023c)). Task 3 aims to explore the internal hallucination and reliability extent of LLMs when using tools and Task 4 is designed to evaluate the inference ability (e.g., order of using multiple tools) (Creswell et al., 2022) of LLMs.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: The process of dataset generation.</p>
<h1>2.2 TOOLE DATASET</h1>
<p>In this section, we introduce the TOOLE dataset with 21.1 k diverse user queries related to tool usage. Each entry within the dataset comprises a user request (i.e., query) along with its corresponding tool name and tool description. These queries serve as triggers that prompt LLMs to utilize specific tools. The step-by-step process employed for generating the dataset is shown in Figure 3.</p>
<h3>2.2.1 DATASET GENERATION</h3>
<p>Tool description. Tool description is important for LLMs to use them (Hsieh et al., 2023). We retrieve tool names and descriptions from OpenAI's plugin list (OpenAI, 2023d). The reason for selecting Open AI plugins as the data source for our tools is that these tools have been installed in ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b), and they have been widely used, making them more practical. We obtained names and descriptions for a total of 390 tools across different domains. We show more details about tool descriptions in Appendix A.5.</p>
<p>Single-tool queries generation. Next, we describe how we generated queries. Inspired by prior studies (Qin et al., 2023b; Yang et al., 2023c), our approach revolves around incorporating a tool's description into a prompt while implementing specific constraints to guide the generation of user queries by ChatGPT/GPT-4. We adopt four distinct techniques for query creation: direct diverse generation, emotional generation, keyword generation, and details generation. We show the data examples generated by different prompt ways in Table 8 of the Appendix A.5. (1) Direct diverse generation. We introduced conditional criteria within the prompt to encourage ChatGPT/GPT-4 to produce a variety of query types, encompassing distinct tones (such as requests or orders) and levels of detail. (2) Emotional generation. Building on prior research (Li et al., 2023a; Goodside, 2023), which highlights the influence of emotion within prompts on model performance, we augmented the prompt with constraints to guide ChatGPT in generating content in different emotions. Here we used four distinct emotions - happiness, excitement, anger, and depression. (3) Keyword generation. Direct generation occasionally fell short in capturing specific description details, such as tools limited to particular regions, so we devised the generation way through keywords. This way involved ChatGPT extracting keywords from the tool's description and then we incorporated both the extracted keyword and the tool's description within the prompt, tasking ChatGPT with generating queries focused on the given keyword. (4) Details generation. To add more details to the queries, we instructed ChatGPT to add details to augment the original queries generated by direct diverse generation methods.</p>
<p>Overlapped issue. Overlapped issue refers to a query that can be solved by multiple tools. If left unaddressed, this overlap could potentially influence the computation of final metrics. For instance, given a query $q$, the corresponding tool in our dataset is $t_{a}$, yet an alternate tool $t_{b}$ could also feasibly address the same query $q$. In a single-label scenario, the accuracy of tool selection becomes compromised. To address this, we merge the group of tools with similar functions as a single tool. Meanwhile, if a tool can function for multiple purposes across the groups of tools, the corresponding generated queries cannot be simply merged into any one of them. So decompositions are needed for the queries of these tools before merging. After decomposition and merging, each query in our dataset has only one ground truth label. The decomposition and merging operation follows three steps, and details about this can be found in Appendix A.1. We also show the efficiency of our operation in Appendix A. 2 through the silhouette coefficient.</p>
<p>Multi-tool queries generation. Unlike single-tool queries, we generate multi-tool queries after addressing the overlapped issue because it is challenging to map the original labels to new labels in multi-label (i.e., multi-tool) situations. Here, we only consider queries related to two tools. We observe that if we obtain combinations of two tools by iterating through all the tools (i.e., $\mathrm{C}_{\mathrm{n}}^{2}$ iterations,</p>
<p>where $n$ is the size of the toolset), there would be many tool combinations that are not practical (i.e., rarely encountered in daily life, such as the combination of fortune-telling tools and currency exchange tools). Therefore, we select the top 15 most popular tools from the toolset, and for each pair of tools, we generate 5 queries. We determine the popularity of a tool based on the number of tools it is merged with, as shown in Appendix A.5. The multi-tool queries we generate can be divided into two types: The first category pertains to situations where tools are employed in parallel, indicating that the utilization of each tool operates independently of the others. The second category deals with cases where tools are used causally, signifying that the deployment of one tool may be contingent upon the outcomes of a preceding tool. Detailed prompt templates can be found in Appendix D.2. Similar to single-tool queries, we also manually verified multi-tool queries to ensure the combination of tools was reasonable and the query of the tool corresponded to the tool description.</p>
<p>Human checking. We conducted manual verification of all queries in TOOLE, including the removal of non-compliant queries and tools, as well as the handling of queries corresponding to special categories of tools. Detailed guidelines for human validation are provided in Appendix A.3.</p>
<h1>2.3 TASK FORMULATION</h1>
<p>We seek to address two research questions in this paper: (1) To what extent can LLMs be conscious of their limitations and ask for assistance from external tools? (2) How effectively can LLMs select the tools when they ask for assistance? To answer these questions, we design two tasks based on the TOOLE dataset to evaluate the capacity of LLMs regarding tool usage.</p>
<h3>2.3.1 AWARENESS OF TOOL USAGE</h3>
<p>In this part (i.e., Thought (①)), we aim to investigate the awareness of tool usage of LLMs; that is, whether LLMs can resort to external tools when they encounter problems they cannot solve. To this end, we need to construct the test set with both positive and negative samples. Positive samples are the queries that can not be solved by LLMs themselves and need tool usage, whereas negative samples are queries that can be directly solved by LLMs and therefore do not necessitate tool usage. For positive samples, we selected a subset of samples from TOOLE and conducted manual validation to confirm whether they would trigger LLMs to use the tool (the process of which is detailed in the Appendix B). As for negative samples, we select three recent instruction datasets, including instructions about downstream tasks (Wang et al., 2022), common-sense questions (Talmor et al., 2019), and high-quality instructions used in LIMA (Zhou et al., 2023b). Similarly, we conducted manual verification to ensure that these requests can be resolved by LLMs' intrinsic capabilities. Specifically, we use the prompt with a query to inquire the LLMs whether need to employ a tool or not, and the output of LLMs should be either "yes" or "no".</p>
<h3>2.3.2 TOOL SELECTION</h3>
<p>Preliminary. We propose four subtasks to evaluate LLMs in tool selection *(i.e., Action phase (②)). Generally, the prompt comprises a query $q \in Q$ (i.e., the user's input) and a tool list $L_{t}\left(L_{t} \subseteq T\right)$ containing $n$ potential tool candidates. In the single-tool tasks (Sub-task 1 3), we designate the corresponding tool for query $q$ as $t \in T$. In the multi-tool task (Sub-task 4), this corresponds to $S_{t} \subset T\left(\left|S_{t}\right|&gt;1\right)$. Consequently, we obtain $y_{\text {Action }} \subseteq\left(L_{t} \cup \varnothing\right)$ as the outcome of the tool selection process, where $y_{\text {Action }}$ represents the selected tool(s).</p>
<p>Sub-task 1: tool selection with similar choices. The task is designed to challenge LLMs to select the correct tool from a tool list containing similar tools, thereby testing their thorough understanding of tool functionality. Given a query $q$ with its label $t$, we task LLMs with selecting a tool from the specified tool list $L_{t}$ containing $n$ candidates. To construct $L_{t}$, we first obtain the embedding of $t$ 's description, denoted as $E(t)$, where $E(\cdot)$ represents the embedding function (here, we utilize the text-embedding-ada-002 model (OpenAI, 2023c) to generate embeddings). Denote the most similar tools of $t$ as top- $(n-1)<em t="t">{t}$, which are selected based on the cosine similarity of their embeddings: $\operatorname{top}-(n-1)</em>}=\arg \operatorname{top}-\mathrm{k<em t="t">{t^{\prime} \in T \backslash{t}} \operatorname{sim}\left(E(t), E\left(t^{\prime}\right)\right)$. Consequently, $L</em>$.}={t} \cup \operatorname{top}-(n-1)_{t</p>
<p>Sub-task 2: tool selection in specific scenarios. The objective of this task is to simulate how LLMs perform using tools when they act as controllers of a system (Shen et al., 2023) faced with different</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>scenarios. As LLMs are widely applied across various domains like biomedical domain (Zhang et al., 2023) and educational domain (Kasneci et al., 2023), in scenarios where the system caters to diverse demographics or professions (e.g., software engineer (Qian et al., 2023)), its set of tools also varies. This task allows us to explore the performance disparities of LLMs in selecting different kinds of tools, essentially highlighting a form of bias inherent to LLMs (Ferrara, 2023). In such cases, this task examines how effectively LLMs utilize the tools. Given a query $q$ with its label $t$, we specify the tool list $L_{t}$ containing $n$ candidates according to its corresponding scenarios. This task consists of two types of scenarios: the first one is the popularity-related scenario, and the second one is the group-related scenario. For the popularity-related scenario, we have selected the 5, 10, and 15 most popular tools based on the number of tools it's merged with (refer to Table 9 in the Appendix A. 5 for details.) to construct the tool list. As for the group-related scenario, we chose six usual occupations or identities and manually curated a tool list consisting of the 10 most relevant tools for each of them (see Table 10 in the Appendix A. 5 for details).</p>
<p>Sub-task 3: tool selection with possible reliability issues. The reliability of LLMs' tool selection is of utmost importance. However, issues like hallucination (Ji et al., 2023) and sycophancy (Wei et al., 2023) within LLMs' responses that will negatively impact their selection of tools. Therefore, we introduce sub-task 3. In this task, given a query $q$ and its corresponding tool $t$, we need to construct the tool list $L_{t}$ and ensure $t \notin L_{t}$. This aims to assess whether LLMs can answer questions honestly and avoid issues like choosing non-existent tools or selecting unrelated tools. It should be noted that this task closely resembles real-world scenarios, as not all existing tools capable of addressing user queries are present in the tool list controlled by LLMs. To be specific, we obtain the embedding of $t$ 's description $E(t)$ and get the top- $k_{t}$ similar tools about $t$ as the way in Task 1. Then we randomly sample $n$ tools from the rest tool set $T^{\prime}$ to construct $L_{t}$, denoted as $L_{t}=\left{t_{1}, t_{2}, \ldots, t_{n}\right}$ where $t_{i} \in T^{\prime}(1 \leq i \leq n)$ and $T^{\prime}=T \backslash\left({t} \cup\right.$ top- $\left.k_{t}\right)$. Overall, we remove the ground-truth tool $t$ of query $q$ and $t$ 's most similar $k$ tools to keep the tools in $L_{t}$ not related to $t$ as much as possible.</p>
<p>Sub-task 4: multi-tool selection. In addition to testing the selection of single tools, like previous research (Qin et al., 2023b), we set up a task for multi-tool selection which may evaluate the inference ability and more complex semantic comprehension in the tool selection. We tested whether LLMs would correctly choose the specified tools by inputting multi-tool queries. Specifically, given a query $q$ with its related tool set $S_{t}\left(\left|S_{t}\right|&gt;1\right)$, we construct the tool list $L_{t}$ containing $n$ tool candidates $\left(n&gt;\left|S_{t}\right|\right)$. Like the candidate selection way in sub-task 3, we obtain each tool $t$ 's embedding $E(t)$ where $t \in S_{t}$, and get the most $k$ similar tools of $t_{1}, t_{2}, \ldots, t_{\left|S_{t}\right|}$, denoted as top- $k_{t_{1}}$, top- $k_{t_{2}}, \ldots$ top- $k_{t_{\left|S_{t}\right|}}$. We randomly select $\left(n-\left|S_{t}\right|\right)$ tools from $T^{\prime}=T \backslash\left(S_{t} \cup\right.$ top- $\left.k_{t_{1}} \cup \operatorname{top}-k_{t_{2}} \cup \ldots \cup \operatorname{top}-k_{t_{\left|S_{t}\right|}}\right)$. Finally, these $\left(n-\left|S_{t}\right|\right)$ tools and the tools $\in S_{t}$ consist of the tool list $L_{t}$. The reason we do not include the most similar tool in $L_{t}$ like sub-task 3 rather than task 1 is that the multi-tool selection task itself is inherently challenging, and we do not want to further increase the difficulty.</p>
<h1>3 EXPERIMENTS</h1>
<h3>3.1 EXPERIMENTAL SETUP</h3>
<p>Model selection. We have chosen eight models that are currently excelling and popular in this field. These models include ChatGPT (OpenAI, 2023a), ChatGLM2 (6B) (THUDM, 2023), Llama2 (7b-chat, 13b-chat) (Touvron et al., 2023), Vicuna (7b, 13b, 33b) (Chiang et al., 2023), Baichuan2 (13b) (Baichuan, 2023) and Koala (13b) (Geng et al., 2023).</p>
<p>Prompt template and test samples. Due to the large scale of TOOLE, we sample from it as our test set (more details are shown in Appendix C). For a better understanding of the importance of tool usage and to tell LLMs when need to use tools, we add the reasons for tool usage in the prompt template of Thought (①) part. We show the detailed prompt template in Appendix D.2. We also conducted few-shot learning experiments for the first three tasks and details of the experimental design can be found in Appendix C. 4.</p>
<p>Metrics. For the awareness of tool usage evaluation, we use accuracy, recall, precision, and F1 score as the metrics. For tool selection, we propose the Correct Selection Rate (CSR) to calculate the percentage of correct selection action. Denote the output results for all queries as $Y={y_{1}, y_{2}, \ldots}$, for a specific output $y$, we use $A(y)$ to denote the tool(s) that the model chooses from the tool list. The CSR is computed as follows:</p>
<p>Table 2: The results for the awareness of tool usage test. We use accuracy (Acc.), precision (Pre.), recall (Rec.), and F1 score (F1) as evaluation metrics. And F1 $\Delta$ is the percentage change of F1 Score between zore-shot and five-shot, as calculated by $\mathrm{F1}<em x="0">{x=5}-\mathrm{F1}</em>$</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metrics</th>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">ChatGLM2</th>
<th style="text-align: center;">Llama2-7b</th>
<th style="text-align: center;">Llama2-13b</th>
<th style="text-align: center;">Vicuna-7b</th>
<th style="text-align: center;">Vicuna-13b</th>
<th style="text-align: center;">Vicuna-33b</th>
<th style="text-align: center;">Koala-13b</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">74.85</td>
<td style="text-align: center;">54.56</td>
<td style="text-align: center;">53.88</td>
<td style="text-align: center;">51.94</td>
<td style="text-align: center;">52.43</td>
<td style="text-align: center;">61.17</td>
<td style="text-align: center;">65.05</td>
</tr>
<tr>
<td style="text-align: center;">$x=0$</td>
<td style="text-align: center;">Pre.</td>
<td style="text-align: center;">69.63</td>
<td style="text-align: center;">61.90</td>
<td style="text-align: center;">52.71</td>
<td style="text-align: center;">80.00</td>
<td style="text-align: center;">74.51</td>
<td style="text-align: center;">82.49</td>
<td style="text-align: center;">79.92</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rec.</td>
<td style="text-align: center;">88.16</td>
<td style="text-align: center;">40.39</td>
<td style="text-align: center;">84.85</td>
<td style="text-align: center;">6.21</td>
<td style="text-align: center;">7.38</td>
<td style="text-align: center;">28.35</td>
<td style="text-align: center;">40.19</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">77.81</td>
<td style="text-align: center;">48.88</td>
<td style="text-align: center;">65.03</td>
<td style="text-align: center;">11.53</td>
<td style="text-align: center;">13.43</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">53.49</td>
</tr>
<tr>
<td style="text-align: center;">$x=5$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">79.71</td>
<td style="text-align: center;">56.02</td>
<td style="text-align: center;">50.39</td>
<td style="text-align: center;">57.86</td>
<td style="text-align: center;">56.12</td>
<td style="text-align: center;">55.15</td>
<td style="text-align: center;">61.55</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pre.</td>
<td style="text-align: center;">73.98</td>
<td style="text-align: center;">64.32</td>
<td style="text-align: center;">50.55</td>
<td style="text-align: center;">63.31</td>
<td style="text-align: center;">56.49</td>
<td style="text-align: center;">52.92</td>
<td style="text-align: center;">57.85</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rec.</td>
<td style="text-align: center;">91.65</td>
<td style="text-align: center;">51.46</td>
<td style="text-align: center;">99.03</td>
<td style="text-align: center;">47.57</td>
<td style="text-align: center;">54.95</td>
<td style="text-align: center;">96.89</td>
<td style="text-align: center;">85.83</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">81.87</td>
<td style="text-align: center;">57.17</td>
<td style="text-align: center;">66.93</td>
<td style="text-align: center;">54.32</td>
<td style="text-align: center;">55.71</td>
<td style="text-align: center;">68.45</td>
<td style="text-align: center;">69.12</td>
</tr>
<tr>
<td style="text-align: center;">F1 $\Delta$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$4.06 \uparrow$</td>
<td style="text-align: center;">$8.92 \uparrow$</td>
<td style="text-align: center;">$1.90 \uparrow$</td>
<td style="text-align: center;">$42.79 \uparrow$</td>
<td style="text-align: center;">$42.28 \uparrow$</td>
<td style="text-align: center;">$26.25 \uparrow$</td>
<td style="text-align: center;">$15.63 \uparrow$</td>
</tr>
</tbody>
</table>
<p>$$
\mathrm{CSR}=\frac{1}{|Y|} \sum_{y \in Y} \mathbb{I}\left(A(y)=\left{\begin{array}{ll}
t &amp; \text { for Task } 1,2 \
\varnothing &amp; \text { for Task } 3 \
S_{t} &amp; \text { for Task } 4
\end{array}\right)\right.
$$</p>
<h1>3.2 ReSults ANALYSIS</h1>
<p>Through the experiment results, we have gained the following conclusions:
Even under the few-shot prompts, the majority of LLMs still perform poorly in tool usage awareness. In Table 2, we observe that under the zero-shot prompt, only ChatGPT has both accuracy and F1 score exceeding $70 \%$, while the performance of other models is relatively poor, with the F1 score of llama2-13b being only $11.53 \%$. Under the five-shot prompt, some models show significant improvement in F1 scores, for example, llama2-13b increased by $42.79 \%$, and vicuna-7b by $42.28 \%$. This indicates that though few-shot learning generally improves the performance of LLMs in tool usage awareness, they still lack sufficient tool usage awareness.
When selecting similar tools, there is a significant performance disparity among existing LLMs, and the improvement brought by few-shot prompts is limited. Table 3 shows that under the zero-shot prompts, the best-performing LLM is Vicuna-7b, with nearly a $30 \%$ difference compared to the worst-performing Llama2-13b. The gap between the best-performing ChatGPT and the worst-performing Llama2-13b still exceeds $20 \%$ under 5 -shot prompts. Additionally, the maximum improvement brought by 5 -shot prompts does not exceed $7 \%$. Moreover, the performance of Vicuna7 b even declined by $10 \%$ under the five-shot condition, suggesting a potential bias in its 0 -shot performance, which reflects either a lack of robustness or over-sensitivity of the model.
LLMs still face serious challenges in dealing with reliability issues, for instance, reducing hallucination. As seen from Table 3, although few-shot prompts improve the performance of all LLMs, the CSR of most LLMs remains below 20\%. We find that LLMs sometimes fabricate non-existent tools, a severe hallucination issue that has a significant impact on LLM-based agents. Additionally, the potential sycophancy of LLMs may lead them to avoid returning a "none" answer, instead choosing irrelevant tools to respond to users.</p>
<p>Table 3: The CSR (\%) for tool selection with similar choices and with possible reliability issues. $\Delta$ is the percentage change of CSR between zore-shot and five-shot, as calculated by $\mathrm{CSR}<em x="0">{x=5}-\mathrm{CSR}</em>$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">ChatGLM2</th>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">Llama2-7b</th>
<th style="text-align: center;">Llama2-13b</th>
<th style="text-align: center;">Vicuna-7b</th>
<th style="text-align: center;">Vicuna-13b</th>
<th style="text-align: center;">Vicuna-33b</th>
<th style="text-align: center;">Koala-13b</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Similar</td>
<td style="text-align: center;">$x=0$</td>
<td style="text-align: center;">54.17</td>
<td style="text-align: center;">69.05</td>
<td style="text-align: center;">45.95</td>
<td style="text-align: center;">44.06</td>
<td style="text-align: center;">73.46</td>
<td style="text-align: center;">58.23</td>
<td style="text-align: center;">53.96</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$x=5$</td>
<td style="text-align: center;">57.44</td>
<td style="text-align: center;">72.94</td>
<td style="text-align: center;">51.12</td>
<td style="text-align: center;">49.85</td>
<td style="text-align: center;">63.67</td>
<td style="text-align: center;">63.15</td>
<td style="text-align: center;">60.54</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">$3.27 \uparrow$</td>
<td style="text-align: center;">$3.89 \uparrow$</td>
<td style="text-align: center;">$5.17 \uparrow$</td>
<td style="text-align: center;">$5.79 \uparrow$</td>
<td style="text-align: center;">$-9.79 \downarrow$</td>
<td style="text-align: center;">$4.92 \uparrow$</td>
<td style="text-align: center;">$6.58 \uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">Reliability</td>
<td style="text-align: center;">$x=0$</td>
<td style="text-align: center;">6.63</td>
<td style="text-align: center;">50.35</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">2.31</td>
<td style="text-align: center;">1.50</td>
<td style="text-align: center;">2.51</td>
<td style="text-align: center;">2.81</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$x=5$</td>
<td style="text-align: center;">15.68</td>
<td style="text-align: center;">78.49</td>
<td style="text-align: center;">2.51</td>
<td style="text-align: center;">5.93</td>
<td style="text-align: center;">1.81</td>
<td style="text-align: center;">3.42</td>
<td style="text-align: center;">3.11</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">$9.05 \uparrow$</td>
<td style="text-align: center;">$28.14 \uparrow$</td>
<td style="text-align: center;">$1.61 \uparrow$</td>
<td style="text-align: center;">$3.62 \uparrow$</td>
<td style="text-align: center;">$0.31 \uparrow$</td>
<td style="text-align: center;">$0.91 \uparrow$</td>
<td style="text-align: center;">$0.30 \uparrow$</td>
</tr>
</tbody>
</table>
<p>LLMs perform poorly in processing long texts. From Figure 4, we can see that the CSR of almost all LLMs decreases as the length of the tool list increases, especially in the range from top 5 to top 10. This indicates that LLMs still need improvement in understanding long texts. LLMs exhibit imbalances and biases in tool selection across different scenarios. For example, in Figure 5, LLMs generally have a higher CSR in tool selections related to the elderly and artists &amp; designers, while their CSR is lowest for tools related to students. This means that developers still need to enhance the generalization capabilities of LLMs. At the same time, for downstream applications, it is best to choose suitable LLMs based on different applied fields.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: The CSR results (%) of top <em>n</em> (<em>n</em>=5,10,15) tool in different scenarios.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: The CSR result (%) of tool selection in specific scenarios.</p>
<p>Table 4: Multi-tool selection results. We evaluate LLMs' performance based on two kinds of prompt templates: one is telling LLMs to choose zero, one, or two tools (i.e., multi-choice), while another is forcing LLMs to choose two tools (i.e., one-choice). We consider the different kinds of CSR (%) for the former one: the LLM selects two correct tools (2/2 CSR), selects only one tool and it's correct (1/1 CSR), and selects two but only one is correct (1/2 CSR).</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>ChatGPT</th>
<th>ChatGLM2</th>
<th>Llama2-7b</th>
<th>Llama2-13b</th>
<th>Vicuna-7b</th>
<th>Vicuna-13b</th>
<th>Vicuna-33b</th>
<th>Koala-13b</th>
</tr>
</thead>
<tbody>
<tr>
<td>Multi-choices</td>
<td>2/2 (CSR)</td>
<td>88.28</td>
<td>20.20</td>
<td>35.69</td>
<td>81.49</td>
<td>44.06</td>
<td>83.70</td>
<td>48.69</td>
</tr>
<tr>
<td></td>
<td>1/1 (CSR)</td>
<td>3.03</td>
<td>36.57</td>
<td>21.98</td>
<td>0.00</td>
<td>25.55</td>
<td>1.01</td>
<td>48.49</td>
</tr>
<tr>
<td></td>
<td>1/2 (CSR)</td>
<td>1.01</td>
<td>13.94</td>
<td>8.87</td>
<td>11.07</td>
<td>7.04</td>
<td>8.05</td>
<td>0.20</td>
</tr>
<tr>
<td>One-choice</td>
<td>CSR</td>
<td>88.53</td>
<td>23.34</td>
<td>57.34</td>
<td>77.87</td>
<td>64.34</td>
<td>78.47</td>
<td>91.15</td>
</tr>
</tbody>
</table>
<p>There are significant performance differences among LLMs in multi-tool selection. As shown in Table 4, ChatGPT, the top-performing model, outperforms ChatGLM2, the worst-performing model, by nearly 70%, highlighting the variability in the capabilities of different language models for this task. Furthermore, the most common error made by the models is omitting tool selection, such as in the case of Vicuna-33b, which only selected one tool in 48.49% of cases. Moreover, several LLMs overly rely on the explicitly specified number of tools they should select in the prompts. As shown in Table 4, when explicitly instructed to return two tools, Vicuna-33b's correct selection rate increased to over 90%, and Vicuna-7b also improved by over 20%. This indicates that these</p>
<p>Table 5: Error analysis results. The Top@k metric quantifies the proportion of incorrect choices by the model that are ranked within the Top@k positions of the similarity-ranked list.</p>
<table>
<thead>
<tr>
<th>Top@k</th>
<th>ChatGPT</th>
<th>ChatGLM2</th>
<th>Llama2-7b</th>
<th>Llama2-13b</th>
<th>Vicuna-7b</th>
<th>Vicuna-13b</th>
<th>Vicuna-33b</th>
<th>Koala-13b</th>
</tr>
</thead>
<tbody>
<tr>
<td>Top@1</td>
<td>18.44</td>
<td>19.89</td>
<td>14.37</td>
<td>15.12</td>
<td>15.03</td>
<td>15.91</td>
<td>16.62</td>
<td>15.04</td>
</tr>
<tr>
<td>Top@3</td>
<td>34.29</td>
<td>36.48</td>
<td>34.08</td>
<td>34.94</td>
<td>34.43</td>
<td>35.94</td>
<td>35.09</td>
<td>35.34</td>
</tr>
<tr>
<td>Top@5</td>
<td>47.26</td>
<td>43.63</td>
<td>51.41</td>
<td>49.81</td>
<td>50.55</td>
<td>48.15</td>
<td>48.28</td>
<td>49.62</td>
</tr>
</tbody>
</table>
<p>LLMs still possess good multi-tool selection capabilities but require prior knowledge, which makes it challenging to apply in LLM-based agents.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Percentage change on the description rewritten by Llama2-70b and GPT-4.</p>
<p>Error analysis. We further investigate the reasons for errors in LLM's tool selection. We employ the Top@ $k$ metric to analyze failure cases in tool selection with similar choices, as shown in Table 5. It suggests that, despite being incorrect, the choices made by the model often retain a degree of similarity to the correct tool. In general, all LLMs have a nearly $50 \%$ chance of choosing a tool from the Top@5 most similar to the correct tool, and more than a $15 \%$ chance of choosing the most similar one (i.e., Top@1). This indicates that there is still significant room for improvement in tool selection with LLMs.</p>
<p>Insights for Tool Developer. We also investigated the relationship between tool descriptions and CSR. We calculated CSR for the queries corresponding to $t$ and visualized them in Figure 7. There are two categories of tools: those that have been decomposed and merged (i.e., new tools) and those that have not been merged or decomposed (i.e., original tools). From the figure, we can draw the conclusion: The more detailed the description, the more efficient tool selection. As shown by the fitted line, as the length of the description increases, the CSR continuously increases, indicating that detailed descriptions can help LLMs better understand the functionality of tools, thus improving the accuracy of tool selection. Additionally, as shown in Figure 6, we built upon the original description by having two proficient LLMs rewrite it and then observed the performance changes of eight LLMs on the new descriptions. Different rewritten LLMs yielded varying benefits for different groups. For instance, descriptions rewritten by Llama2-70b resulted in a $7.83 \%$ improvement for llama2-13b, but did not significantly enhance the performance of the Vicuna series models. In contrast, descriptions
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: The CSR of tool selection and description length.
rewritten by GPT-4 caused a sharp decline in the performance of ChatGLM and Llama2 series, while significantly boosting the Vicuna series, possibly due to the Vicuna series' training corpus being largely sourced from ShareGPT [ShareGPT, 2023]. Therefore, we strongly recommend that tool developers choose an appropriate rewrite model for generating new descriptions based on the downstream LLM the tool will apply to.</p>
<h1>4 CONCLUSION</h1>
<p>In this paper, we introduce MetaTool, a benchmark for evaluating LLMs based on their tool usage awareness and tool selection capabilities. We propose TOOLE within the benchmark, which contains diverse queries to trigger LLMs to use tools. We found that most LLMs lack good tool usage awareness and exhibit a significant gap from real intelligent agents in tool selection.</p>
<h1>ACKNOWLEDGEMENT</h1>
<p>Lichao Sun and Yue Huang are supported by the National Science Foundation Grants CRII-2246067 and Microsoft Accelerate Foundation Models Research Award.</p>
<h2>REFERENCES</h2>
<p>Nomic AI. Nomic ai, 2023. https://atlas.nomic.ai/.
Abid Ali Awan. The 10 best chatgpt plugins for data science, 2023. https://www.datacamp.com/blog/ the-10-best-chat-gpt-plugins-for-data-science.
babyagi. Babyagi, 2023. https://github.com/yoheinakajima/babyagi.
Baichuan. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023. URL https://arxiv.org/abs/2309.10305.</p>
<p>NATALY BIRCH and ANDRIAN VALEANU. 16 best ai tools for web designers, 2023. https: //designmodo.com/ai-tools-designers/.</p>
<p>Adrian Bridgwater. Auto-tech series - octoml: Large language model (llm) automation for developers, 2023. https://www.computerweekly.com/blog/CW-Developer-Network/ Auto-tech-series-OctoML-Large-Language-Model-LLM-automation-for-developers.</p>
<p>Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers. arXiv preprint arXiv:2305.17126, 2023.</p>
<p>Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S Yu, and Lichao Sun. A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt. arXiv preprint arXiv:2303.04226, 2023.</p>
<p>Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages. ArXiv, abs/2305.04160, 2023. URL https://api.semanticscholar.org/CorpusID:258558106.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/.</p>
<p>Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, 2022.</p>
<p>Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. Uncovering chatgpt's capabilities in recommender systems. arXiv preprint arXiv:2305.02182, 2023.</p>
<p>Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. arXiv preprint arXiv:2306.06070, 2023.</p>
<p>Empresaria. House manager job description, 2023. https://www.greycoatlumleys.co.uk/ looking-for-a-job/career-advice/house-manager-job-description.</p>
<p>Emilio Ferrara. Should chatgpt be biased? challenges and risks of bias in large language models. arXiv preprint arXiv:2304.03738, 2023.</p>
<p>Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. Chatrec: Towards interactive and explainable llms-augmented recommender system. arXiv preprint arXiv:2303.14524, 2023.
geekan. Metagpt, 2023. https://github.com/geekan/MetaGPT.</p>
<p>Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala: A dialogue model for academic research. Blog post, April 2023. URL https://bair.berkeley.edu/blog/2023/04/03/koala/.</p>
<p>Riley Goodside. The past and future of prompt engineering, 2023. https://exchange.scale.com/public/ videos/the-past-and-future-of-prompt-engineering-2023-08-23.</p>
<p>GPTStore. Overview of ai/chatgpt plugin weather, 2023. https://gptstore.ai/plugins/ weather--vicentescode-repl-co.</p>
<p>Level Up Coding Youssef Hosni. 9 helpful chatgpt plugins for data scientists, 2023. https://levelup. gitconnected.com/9-helpful-chatgpt-plugins-for-data-scientists-32eceb8d07a8.</p>
<p>Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. Tool documentation enables zero-shot tool-usage with large language models. arXiv preprint arXiv:2308.00675, 2023.</p>
<p>Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb: Augmenting llms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901, 2023.</p>
<p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38, 2023.</p>
<p>Vartika Kashyap. 26 best designer tools for web and graphic design professionals, 2023. https: //www.proofhub.com/articles/designer-tools.</p>
<p>Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. Chatgpt for good? on opportunities and challenges of large language models for education. Learning and individual differences, 103:102274, 2023.</p>
<p>Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internetaugmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115, 2022.</p>
<p>Cheng Li, Jindong Wang, Kaijie Zhu, Yixuan Zhang, Wenxin Hou, Jianxun Lian, and Xing Xie. Emotionprompt: Leveraging psychology for large language models enhancement via emotional stimulus. arXiv preprint arXiv:2307.11760, 2023a.</p>
<p>Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-andvision assistant for biomedicine in one day. ArXiv, abs/2306.00890, 2023b. URL https: //api.semanticscholar.org/CorpusID:258999820.</p>
<p>Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, et al. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sql. arXiv preprint arXiv:2305.03111, 2023c.</p>
<p>Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: A benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244, 2023d.</p>
<p>Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents, 2023e.</p>
<p>Gemingtian Liu, Xinyu Ma, Yu Zhang, Boyan Su, and Pinan Liu. Gpt4: The indispensable helper for neurosurgeons in the new era. Annals of Biomedical Engineering, 51:2113 - 2115, 2023. URL https://api.semanticscholar.org/CorpusID:258787338.</p>
<p>Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and Zhaopeng Tu. Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration. ArXiv, abs/2306.09093, 2023. URL https://api.semanticscholar.org/CorpusID: 259165461 .</p>
<p>Teresa Mears. 10 essential tech tools for older adults, 2015. https://money.usnews.com/money/ retirement/articles/2015/11/16/10-essential-tech-tools-for-older-adults.</p>
<p>Ofer Mendelevitch. Large language models for code generation - part 2, 2023. https://vectara.com/ large-language-models-llms-for-code-generation-part-2/.</p>
<p>Manuel Odendahl. Llms will fundamentally change software engineering, 2023. https://dev.to/wesen/ llms-will-fundamentally-change-software-engineering-3oj8.</p>
<p>OpenAI. Introducing chatgpt, 2023a. URL https://openai.com/blog/chatgpt.
OpenAI. Gpt-4 technical report. 2023b. URL https://arxiv.org/pdf/2303.08774.pdf.
OpenAI. new-and-improved-embedding-model, 2023c. https://openai.com/blog/ new-and-improved-embedding-model.</p>
<p>OpenAI. Openai plugin, 2023d. https://openai.com/blog/chatgpt-plugins.
Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.</p>
<p>Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.</p>
<p>Zhi Qi, Yi Yu, Meiqi Tu, Junyi Tan, and Yongfeng Huang. Foodgpt: A large language model in food testing domain with incremental pre-training and knowledge graph prompt. ArXiv, abs/2308.10173, 2023. URL https://api.semanticscholar.org/CorpusID:261048937.</p>
<p>Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023.</p>
<p>Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint arXiv:2304.08354, 2023a.</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023b.</p>
<p>Salvatore Raieli. Fingpt: open-source llm for finance, 2023. https://levelup.gitconnected.com/ fingpt-open-source-llm-for-finance-e8ec10d0bf40.</p>
<p>Rapid. Rapid api, 2023. https://rapidapi.com/.
Peter J Rousseeuw. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics, 20:53-65, 1987.</p>
<p>Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. Tptu: Task planning and tool usage of large language model-based ai agents. arXiv preprint arXiv:2308.03427, 2023.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.</p>
<p>National Science and Technology Council. Emerging technologies to support an aging population, 2019. https://trumpwhitehouse.archives.gov/wp-content/uploads/2019/03/ Emerging-Tech-to-Support-Aging-2019.pdf.</p>
<p>ShareGPT. Sharegpt, 2023. https://sharegpt.com/.
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.</p>
<p>Significant-Gravitas. Autogpt, 2023. https://github.com/Significant-Gravitas/Auto-GPT.
Peter HA Sneath, Robert R Sokal, et al. Numerical taxonomy. The principles and practice of numerical classification. 1973.</p>
<p>Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, William Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, and Yue Zhao. Trustllm: Trustworthiness in large language models, 2024.</p>
<p>Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. Is chatgpt good at search? investigating large language models as re-ranking agent. arXiv preprint arXiv:2304.09542, 2023.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4149-4158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https: //aclanthology.org/N19-1421.</p>
<p>Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301, 2023.</p>
<p>ODSC Team. Have you met fingpt? a new open-source financial large language model, 2023. https://opendatascience.com/ have-you-met-fingpt-a-new-open-source-financial-large-language-model/.</p>
<p>THUDM. Chatglm2, 2023. https://github.com/THUDM/ChatGLM2-6B.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.</p>
<p>Nicolas Vidal. How ai and llms are streamlining financial services, 2023. https://www.forbes.com/ sites/forbestechcouncil/2023/05/05/how-ai-and-llms-are-streamlining-financial-services/?sh= 2ea8b923017a.</p>
<p>Leandro von Werra and Loubna Ben Allal. Starcoder: A state-of-the-art llm for code, 2023. https: //huggingface.co/blog/starcoder.</p>
<p>Guangyu Wang, Guoxing Yang, Zongxin Du, Longjun Fan, and Xiaohu Li. Clinicalgpt: Large language models finetuned with diverse medical data and comprehensive evaluation. ArXiv, abs/2306.09968, 2023a. URL https://api.semanticscholar.org/CorpusID:259187929.</p>
<p>Hao Wang, Chi-Liang Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo: Tuning llama model with chinese medical knowledge. ArXiv, abs/2304.06975, 2023b. URL https://api.semanticscholar.org/CorpusID:258170497.</p>
<p>Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo: Tuning llama model with chinese medical knowledge. arXiv preprint arXiv:2304.06975, 2023c.</p>
<p>Wen Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Y. Qiao, and Jifeng Dai. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. ArXiv, abs/2305.11175, 2023d. URL https://api.semanticscholar.org/CorpusID: 258762579 .</p>
<p>Xiaolei Wang, Xinyu Tang, Wayne Xin Zhao, Jingyuan Wang, and Ji-Rong Wen. Rethinking the evaluation for conversational recommendation in the era of large language models. arXiv preprint arXiv:2305.13112, 2023e.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions, 2022.</p>
<p>Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V Le. Simple synthetic data reduces sycophancy in large language models. arXiv preprint arXiv:2308.03958, 2023.</p>
<p>Matt Welsh. The future of software development with llms is here: Announcing fixie's developer preview and 17 m in seed funding, 2023. https://blog.fixie.ai/ the-future-of-software-development-with-llms-is-here-announcing-fixies-developer-preview-and-17m-cf6fca0c4041.</p>
<p>Stephen Wolfram Writings. Chatgpt gets its "wolfram superpowers"!, 2023. https://writings. stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/.</p>
<p>Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564, 2023a.</p>
<p>Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564, 2023b.</p>
<p>Daniel Xiao. Ai-based literature review tools, 2023. https://tamu.libguides.com/c.php?g=1289555.
Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. On the tool manipulation capability of open-source large language models. arXiv preprint arXiv:2305.16504, 2023.</p>
<p>Zhiling Yan, Kai Zhang, Rong Zhou, Lifang He, Xiang Li, and Lichao Sun. Multimodal chatgpt for medical applications: an experimental study of gpt-4v. arXiv preprint arXiv:2310.19061, 2023.</p>
<p>Hongyang Yang, Xiao-Yang Liu, and Chris Wang. Fingpt: Open-source financial large language models. ArXiv, abs/2306.06031, 2023a. URL https://api.semanticscholar.org/CorpusID:259129734.</p>
<p>Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. Fingpt: Open-source financial large language models. arXiv preprint arXiv:2306.06031, 2023b.</p>
<p>Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large language model to use tools via self-instruction. arXiv preprint arXiv:2305.18752, 2023c.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.</p>
<p>Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015, 2023a. URL https://arxiv.org/abs/2304.02015.</p>
<p>Zhengqing Yuan, Zhaoxu Li, and Lichao Sun. Tinygpt-v: Efficient multimodal large language model via small backbones. arXiv preprint arXiv:2312.16862, 2023b.</p>
<p>Jennifer Zhang. Unleashing the potential of llms: a new era for financial services, 2023. https: //www.wiz.ai/unleashing-the-potential-of-llms-a-new-era-for-financial-services/.</p>
<p>Kai Zhang, Jun Yu, Zhiling Yan, Yixin Liu, Eashan Adhikarla, Sunyang Fu, Xun Chen, Chen Chen, Yuyin Zhou, Xiang Li, et al. Biomedgpt: A unified and generalist biomedical generative pre-trained transformer for vision, language, and multimodal tasks. arXiv preprint arXiv:2305.17100, 2023.</p>
<p>Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, et al. A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. arXiv preprint arXiv:2302.09419, 2023a.</p>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023b.</p>
<p>Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: A dataset for llm question answering with external tools, 2023.</p>
<h1>Appendix</h1>
<h2>Table of Contents</h2>
<p>A TOOLE Dataset Details ..... 16
A. 1 Details of Overlapped Issue ..... 16
A. 2 Efficiency of the Operation ..... 17
A. 3 Guidelines for human validation ..... 18
A. 4 Flexibility of TOOLE ..... 18
A. 5 Others Statistics of TOOLE ..... 19
B Test Set for Evaluation on the Awareness of Tool Usage ..... 21
C Experimental Settings ..... 23
C. 1 Models and Test Samples ..... 23
C. 2 Answer Matching ..... 23
C. 3 Task Comparison ..... 23
C. 4 Few-Shot Prompt ..... 24
C. 5 Human Evaluation ..... 25
D Prompt Template ..... 25
D. 1 TOOLE Dataset Generation ..... 25
D. 2 Prompt Template of Experiments ..... 27
E Failure Case Study ..... 29</p>
<h2>A TOOLE DATASET DETAILS</h2>
<p>In this section, we show the details of TOOLE, including how we solve the overlapped issue (Section A.1), guidelines for human validation (Section A.3), and the statistics of TOOLE (Section A.5).</p>
<h2>A. 1 Details of Overlapped Issue</h2>
<h2>Operation pipeline.</h2>
<p>(1) Embeddings and Hierarchical Clustering. We initiated by generating embeddings for tool descriptions using the text-embedding-ada-002 model (OpenAI, 2023c), an API provided by OpenAI, aiming to perform hierarchical clustering (Sneath et al., 1973) on different tools, based on the similarity of their embeddings, to reveal underlying patterns among them. (2) Tool Merging and Decomposition. Based on the results of clustering, we manually merged and decomposed the data. Specifically, several popular topics (e.g., news, weather) were identified based on their functions if they overlapped func-
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Two examples of tool mergence and decomposition.
tionality with other tools (an example is shown in Figure 8). The criteria for merging and decomposition revolved around whether such tools are commonly encountered and make practical sense in daily life. For instance, it is logical to merge a tool that offers both flight and train ticket bookings with another tool that solely focuses on hotel</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: t-SNE (Van der Maaten \&amp; Hinton, 2008) visualization of original tool description embedding (a) and new tool description embedding (b).
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Silhouette score (Rousseeuw, 1987) of new tool description embedding and original tool description embedding in different cluster numbers.
reservations. For merged tools, we only needed to modify the original labels; for decomposed tools, we manually assigned original queries to the appropriate decomposed tools and changed their labels accordingly. We manually created the names of new tools and employed ChatGPT to generate descriptions for both merged and decomposed tools, based on the initial descriptions of these tools. (3) Similarity Verification and Human Validation. We iterated each tool and searched for the ten most similar tools by its description embedding, then we checked whether the tool could be further merged or decomposed with the tools in $L_{\text {sim }}$.</p>
<h1>A. 2 EFFICIENCY OF THE OPERATION</h1>
<p>To evaluate the effectiveness of our operations in solving overlapped issues, we use the silhouette coefficient (Rousseeuw, 1987) to measure the degree of functional overlap between tools. Based on it, we compare the changes in the silhouette coefficients before and after the operations. Specifically, we aim to significantly reduce the functional overlap between tools after merging and decomposition to achieve a more uniform distribution of tool functionalities in the embedding space. We embed the tool description before ( 390 tools) and after the operations (195 tools) and compared the changes in the silhouette coefficients under the same number of clusters. A greater silhouette coefficient indicates better clustering performance and higher functional overlap between tools, while a smaller coefficient suggests lower overlap, reflecting the effectiveness of the operations.</p>
<p>In Figure 10, we present the variation of silhouette coefficients with changes in the number of clusters. It can be observed that the silhouette coefficients of tools after the operations are significantly smaller than before, indicating that the operations have made the distribution of tools more uniform and effectively reduced tool functionality overlap. Additionally, in Figure 9, we visualize the results using t-SNE (Van der Maaten \&amp; Hinton, 2008) when the number of clusters is 30. It is evident that the distribution on the left side is more uniform compared to the right side.</p>
<h1>A. 3 GUIDELINES FOR HUMAN VALIDATION</h1>
<p>We conducted rigorous manual evaluations to ensure the integrity and quality of TOOLE. We established the following rules to guide the manual evaluation:</p>
<ul>
<li>Low-quality tool descriptions. In some cases, ChatGPT was unable to understand the purpose of a tool due to low-quality or overly brief tool descriptions. We conducted a manual review of these descriptions and eliminated tools with unclear or low-quality explanations.</li>
<li>High repetition queries. Since we generated multiple queries for a single tool in one batch, some batches had issues with high query repetition. To address this problem, we selected one query and removed the others.</li>
<li>Queries contain tool name. The inclusion of a tool's name in a query can significantly bias our evaluation as an obvious hint for all tasks in METATool. Therefore, we removed queries containing the tool's name. For example, 'How can I calculate my MBTI type through [tool name]?'</li>
<li>Calculation-related tools. TOOLE contained numerous tools related to calculations. For simple calculations (e.g., 'What is the value of $\sin 30$ degrees?' or ' $7 * 9=$ ?'), LLMs can perform them without the need for a tool. However, for complex calculations, recent research (Yuan et al., 2023a) suggests that LLMs still perform poorly. For queries corresponding to calculation-related tools, we removed queries involving simple calculations and retained those involving complex calculations.</li>
<li>Tool retrieval-related tools. We found that some tools were designed for users to retrieve other tools. This kind of tool conflicted with our task, so we removed these tools.</li>
<li>AI comprehensive tools. We identified some AI comprehensive tools that encompassed a wide range of AI-related tools, making them impractical for our evaluation. Therefore, we removed these tools.</li>
<li>Mentions of 'ChatGPT' in queries. Some queries included the term 'ChatGPT,' for example, 'Hi, ChatGPT! ...' We uniformly replaced 'ChatGPT' with 'Chatbot'."</li>
</ul>
<h2>A. 4 Flexibility of ToOLE</h2>
<p>Sometimes, when an LLM undergoes specialized training in a particular domain (Wu et al., 2023b; Wang et al., 2023a; Qi et al., 2023; Wang et al., 2023b; Yang et al., 2023a), its capabilities improve significantly, and in some cases, some LLMs are also capable of handling various types of information (e.g., images or audio) (Liu et al., 2023; Chen et al., 2023; Wang et al., 2023d; Lyu et al., 2023; Li et al., 2023b). These improvements render some external tools that may not be necessary for some LLMs in the future. Therefore, we have annotated the reasons why LLMs need to use these tools to solve user problems.</p>
<p>Specifically, we use four kinds of motivation for tool usage (some examples are shown in Table 6): A. Solving issues with real-time or external data, databases, or APIs. B. Handling specialized inputs/outputs. C. Enhancing domain tasks beyond LLM's capabilities. D. User
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: The motivation labeling interface.</p>
<p>Table 6: Possible reasons for the motivation of tool usage.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Tool</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">airqualityforecast</td>
<td style="text-align: left;">Planning something outdoors? Get the 2-day air quality forecast <br> for your city.</td>
<td style="text-align: left;">A</td>
</tr>
<tr>
<td style="text-align: left;">Now</td>
<td style="text-align: left;">Get Google Trends. In Japan, you can also get Twitter trends and <br> search Twitter keywords.</td>
<td style="text-align: left;">A</td>
</tr>
<tr>
<td style="text-align: left;">abc_to_audio</td>
<td style="text-align: left;">Converts ABC music notation to WAV, MIDI, and PostScript files. <br> The best way to read text from from any document. ChatOCR <br> will scan and read aloud any text document you provide.</td>
<td style="text-align: left;">B</td>
</tr>
<tr>
<td style="text-align: left;">FinanceTool</td>
<td style="text-align: left;">Begin an exciting journey through time, interact with historical <br> events, and learn about the past in a fun and engaging way.</td>
<td style="text-align: left;">C</td>
</tr>
<tr>
<td style="text-align: left;">LawTool</td>
<td style="text-align: left;">Enables quick search functionality for relevant laws. <br> Playing a game of Tic Tac Toe with varying board sizes. You can <br> submit your move and get the AI's response move.</td>
<td style="text-align: left;">C</td>
</tr>
<tr>
<td style="text-align: left;">Planfit</td>
<td style="text-align: left;">Get your tailored workout plan and instructions with videos - AI- <br> powered Workout Coach, Planfit.</td>
<td style="text-align: left;">D</td>
</tr>
</tbody>
</table>
<p>customization, personalization, and interaction.
We have enlisted the expertise of two experts to annotate the usage motivations of tools in TOOLE (the annotation interface is shown in Figure 11).</p>
<h1>A. 5 Others Statistics of Toole</h1>
<p>Data quantity. Due to the constraints of API usage costs, we exclusively employ GPT-4 for direct diverse generation, utilizing ChatGPT for all other forms of generation techniques. This strategy yielded a total of 29,000 dataset entries. Following a meticulous human review process, we ultimately curated the TOOLE dataset, culminating in a collection comprising 20,881 entries. A comprehensive overview of dataset statistics can be found in Table 11.</p>
<p>Length distribution. Figure 12 displays the distribution of dataset lengths. It can be observed that the majority of the data falls within 40 words or less, aligning with the typical question lengths in people's daily lives.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: Density distribution of all queries' length.</p>
<p>Visualization and release. We used Nomic AI (AI, 2023) to embed user queries in the TOOLE dataset, then clustered the embeddings, and finally visualized the results. The visualization is shown in Figure 13, and you can view it through the following link: https://atlas.nomic.ai/map/ a43a6a84-4453-428a-8738-2534d7bf0b89/b2b8134b-a37e-45d2-a0d9-765911f27df6.</p>
<p>Dataset comparison. As shown in Table 7, we compare the other datasets with TOOLE. Compared to other datasets, we believe that ToolE has two main advantages: (1) Our dataset exhibits greater diversity, and this diversity is tailored to real user scenarios, such as variations in expression style, mood, and level of detail. We employ various prompt methods to induce LLMs to generate a more diverse range of user inputs, ensuring that TOOLE covers a broad spectrum of inputs resembling those of actual users. (2) By employing a pipeline process to address overlapped issues, we can ensure the rigor of the data. As outlined in Appendix A.1, we employ multiple steps to address overlapped issues, ensuring that there is no overlap between tools, which is crucial for maintaining the quality of the dataset.</p>
<p>Tool description. These original tool descriptions encompass two distinct categories: machinereadable descriptions and user-facing descriptions. The machine-readable descriptions prioritize considerations such as token context length or keyword incorporation, aiming to enhance tool prompting within an 8,000-character limit. Conversely, the user-facing descriptions offer succinct</p>
<p>Table 7: Comparison of previous work and MetaTool.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dimension</th>
<th style="text-align: center;">APIBank</th>
<th style="text-align: center;">ToolLLM</th>
<th style="text-align: center;">ToolAlpaca</th>
<th style="text-align: center;">GPT4Tool</th>
<th style="text-align: center;">ToolQA</th>
<th style="text-align: center;">ToolE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Li et al. (2023d)</td>
<td style="text-align: center;">Qin et al. (2023b)</td>
<td style="text-align: center;">Tang et al. (2023)</td>
<td style="text-align: center;">Yang et al. (2023c)</td>
<td style="text-align: center;">Zhuang et al. (2023)</td>
<td style="text-align: center;">(Ours)</td>
</tr>
<tr>
<td style="text-align: left;">Multi-Tool</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">$\boldsymbol{\checkmark}$</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Real Scenario</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Overlapped Issue Solved</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Diversity Generation</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 8: Data examples in TOOLE.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Example</th>
<th style="text-align: left;">Type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">I can't seem to remember anything I study. I need help with learning and <br> retaining information effectively.</td>
<td style="text-align: left;">Emotion-angry</td>
</tr>
<tr>
<td style="text-align: left;">I'm feeling really down today, can you summarize this YouTube video for me? <br> [Link to YouTube video]</td>
<td style="text-align: left;">Emotion-depressed</td>
</tr>
<tr>
<td style="text-align: left;">Good day! I'm hoping to find some amazing bargains and discounts today. Can <br> you guide me in the right direction?</td>
<td style="text-align: left;">Emotion-happy</td>
</tr>
<tr>
<td style="text-align: left;">Hey Chatbot, I'm looking for a new pair of sneakers on GoFynd. Can you help <br> me find the latest designs and recommend some popular brands?</td>
<td style="text-align: left;">Emotion-excited</td>
</tr>
<tr>
<td style="text-align: left;">I'm interested in Japanese cosmetics. Can you recommend some good brands?</td>
<td style="text-align: left;">Direct-request</td>
</tr>
<tr>
<td style="text-align: left;">Draw a state diagram for a vending machine.</td>
<td style="text-align: left;">Direct-order</td>
</tr>
<tr>
<td style="text-align: left;">Twitter Trends: What are the trending topics on Twitter in Japan?</td>
<td style="text-align: left;">Keyword</td>
</tr>
<tr>
<td style="text-align: left;">Could you recommend some popular shopping malls or markets in Singapore <br> that offer a wide range of products, including local and international brands, <br> diverse food options, and unique shopping experiences?</td>
<td style="text-align: left;">Details</td>
</tr>
</tbody>
</table>
<p>Table 9: Top 15 tools ranked by the number of merged tools.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Tool Name</th>
<th style="text-align: center;">Merged Tools</th>
<th style="text-align: left;">Tool Name</th>
<th style="text-align: center;">Merged Tools</th>
<th style="text-align: left;">Tool Name</th>
<th style="text-align: center;">Merged Tools</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FinanceTool</td>
<td style="text-align: center;">22</td>
<td style="text-align: left;">ResearchFinder</td>
<td style="text-align: center;">7</td>
<td style="text-align: left;">TripAdviceTool</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">ProductSearch</td>
<td style="text-align: center;">19</td>
<td style="text-align: left;">NewsTool</td>
<td style="text-align: center;">7</td>
<td style="text-align: left;">WeatherTool</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">JobTool</td>
<td style="text-align: center;">12</td>
<td style="text-align: left;">RepoTool</td>
<td style="text-align: center;">6</td>
<td style="text-align: left;">HousePurchasingTool</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">TripTool</td>
<td style="text-align: center;">10</td>
<td style="text-align: left;">ResearchHelper</td>
<td style="text-align: center;">6</td>
<td style="text-align: left;">Discount</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">PDF\&amp;URLTool</td>
<td style="text-align: center;">8</td>
<td style="text-align: left;">CourseTool</td>
<td style="text-align: center;">6</td>
<td style="text-align: left;">MusicTool</td>
<td style="text-align: center;">5</td>
</tr>
</tbody>
</table>
<p>Table 10: The tool lists of different scenarios.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Scenario</th>
<th style="text-align: center;">Tools</th>
<th style="text-align: center;">Ref.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Software Engi- <br> neer</td>
<td style="text-align: center;">RepoTool, AI2sql, SSH, AutoInfra1, noteable, dart, hackit_web_scanner, LarkBaseImporter, webhooks, universal</td>
<td style="text-align: center;">(von Werra \&amp; Allal, 2023; Odendahl, 2023; Welsh, 2023; Bridgwater, 2023; Mendelevitch, 2023)</td>
</tr>
<tr>
<td style="text-align: center;">Elders</td>
<td style="text-align: center;">NewsTool, PolishTool, CharityTool, MapTool, Memo- <br> ryTool, WeatherTool, RestaurantBookingTool, DietTool, NotesTool, TripAdviceTool</td>
<td style="text-align: center;">(Science \&amp; Council, 2019; <br> Mears, 2015)</td>
</tr>
<tr>
<td style="text-align: center;">Finance Staff</td>
<td style="text-align: center;">FinanceTool, ChartTool, PDF\&amp;URLTool, NotesTool, ExchangeTool, CreditYelp, LawTool, DataRetrievalTool, fundsdbsearch, CompanyInfoTool</td>
<td style="text-align: center;">(Raieli, 2023; Zhang, 2023; Vidal, 2023; Yang et al., 2023b; Team, 2023)</td>
</tr>
<tr>
<td style="text-align: center;">House manager</td>
<td style="text-align: center;">tira, Discount, ProductSearch, ABCmouse, Restaurant- <br> BookingTool, IndoorPlants, recipe_retrieval, HouseR- <br> entingTool, TripTool, CreditYelp</td>
<td style="text-align: center;">(Empresaria, 2023)</td>
</tr>
<tr>
<td style="text-align: center;">Students</td>
<td style="text-align: center;">CourseTool, ResearchFinder, ResearchHelper, speak, noteable, search, MemoryTool, NotesTool, Mixer- <br> Box_Translate_AI_language_tutor, ABCmouse</td>
<td style="text-align: center;">(Xiao, 2023; Awan, 2023; Hosni, 2023; Writings, 2023)</td>
</tr>
<tr>
<td style="text-align: center;">Artists\&amp;designers</td>
<td style="text-align: center;">placid, find_agency, ArtCollection, ChartTool, story- <br> bird_stories, MediaModifyTool, PolishTool, MusicTool, <br> ImageSearch, BookTool</td>
<td style="text-align: center;">(Kashyap, 2023; BIRCH \&amp; VALEANU, 2023)</td>
</tr>
</tbody>
</table>
<p>Table 11: Dataset Statistics of TOOLE.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Generation method</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Sample number</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Direct generation</td>
<td style="text-align: center;">ChatGPT, GPT-4</td>
<td style="text-align: center;">11,700</td>
</tr>
<tr>
<td style="text-align: center;">Emotional generation</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">7,800</td>
</tr>
<tr>
<td style="text-align: center;">Keyword generation</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">1,950</td>
</tr>
<tr>
<td style="text-align: center;">Details generation</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">7,800</td>
</tr>
<tr>
<td style="text-align: center;">Multi-tool generation</td>
<td style="text-align: center;">ChatGPT, GPT-4</td>
<td style="text-align: center;">1624</td>
</tr>
<tr>
<td style="text-align: center;">After checking $\rightarrow$ 21,127 (20630 single-tool + 497 multi-tool)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>and simplified explanations of each tool's functionality. While the majority of tools share identical descriptions across both categories, we opt to adopt the user-facing descriptions as the definitive tool descriptions. This choice is informed by the tendency of machine-readable descriptions to be overly verbose, often delving into instructing the language models on how to handle tool input and output aspects that are not pertinent to our benchmark.</p>
<p>Generation times. For each type of original tool, we perform two rounds of direct diverse generation, producing ten queries each time. In the case of emotional generation, we generate five samples for each of the four distinct emotions. For keyword generation, we extract five keywords from the tool's description and subsequently formulate a query for each identified keyword. Concerning details generation, the number of samples generated aligns with that of the direct diverse generation.</p>
<h1>B Test Set for Evaluation on the Awareness of Tool Usage</h1>
<p>To assess to what extent LLMs are aware of their limitations, we construct the dataset for the awareness of tool usage by merging the positive samples from a subset of TOOLE dataset and the negative samples from some subsets of public datasets. To exclude the ambiguous situation where the queries in TOOLE can be solved either with or without the assistance of external tools, we manually check the output of the queries by feeding them directly into the LLMs and retain those whose responses are unsatisfactory(i.e. the model apologizes, the response contains errors, and etc). In this way, we verify that solving the queries in our selected subset(containing 515 samples) is beyond</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*We separate the prompt of Thought (①) and Action (②) to avoid the influence taken from different tool lists.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
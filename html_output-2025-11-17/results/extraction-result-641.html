<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-641 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-641</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-641</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-3a6447361b20c249f5306ae17dee43f645430e31</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3a6447361b20c249f5306ae17dee43f645430e31" target="_blank">Neural Logic Machines</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> The Neural Logic Machine is proposed, a neural-symbolic architecture for both inductive learning and logic reasoning that achieves perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world.</p>
                <p><strong>Paper Abstract:</strong> We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers. After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e641.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e641.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Logic Machines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural-symbolic architecture that implements lifted first-order Horn-clause reasoning by representing predicates as probabilistic tensors and realizing boolean operations and quantifiers with neural operators (MLPs) and tensor permutations/aggregations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural Logic Machines (NLM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NLM is a multi-layer, multi-group neural-symbolic system that represents r-ary logic predicates as probabilistic tensors over a fixed object set and implements rule induction / forward-chaining deduction by stacking neural operators. Each layer contains groups for predicates of arities 0..B (breadth), with inter-group Expansion/Reduction operations to implement quantifiers (introduce/eliminate variables) and intra-group Permute+MLP modules that approximate boolean combinations (AND/OR/NOT) over permutations of inputs. Depth D bounds the number of deduction steps; MLPs provide learned boolean functions while tensor Expand/Reduce implement lifted quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>First-order logic framed as Horn clauses (definite clauses) under closed-world assumption; predicates and quantifiers are explicitly represented (predicates as tensors, universal/existential quantifiers realized by min/max reduction); lifted rule semantics (implicit universal quantification) and forward-chaining logical deduction are the target symbolic formalism.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks (shared MLPs applied to tensorized predicate groundings), tensor operations (permute, concat, tile), and gradient-based optimization (Adam for supervised tasks; REINFORCE + entropy for reinforcement learning tasks); curriculum learning for training; optional integration with perception CNNs (e.g., LeNet) for noisy inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Tight neural-symbolic integration: symbolic constructs (predicates, permutations representing argument unification, quantifier expansion/reduction) are implemented as differentiable tensor operations; boolean formula evaluation is implemented by shared MLPs (Permute -> MLP -> sigmoid) applied uniformly to all groundings, enabling end-to-end training. Inter-group wiring (Expand/Reduce/Concat) composes different-arity relations. The whole architecture is fully differentiable and trained end-to-end (supervised or RL).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Learns lifted rules (rules that generalize across object identities) encoded in network weights; strong compositional generalization (trained on small instance sizes and generalize to much larger sizes); can implement multi-arity relational reasoning and quantification; recovers human-like rule structures implicitly and executes sequences of deductions as forward-chaining; supports integration with perception for noisy inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Relational reasoning (family tree relations, graph connectivity predicates), decision-making/planning tasks (Blocks World), algorithmic tasks (sorting arrays via swap actions, single-source single-target path finding), and a visualized AdjacentToNumber0 experiment (MNIST + reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Family tree & general graph reasoning: 100% accuracy on most predicates at test sizes m=20 and m=100 (Table 1). BlocksWorld (RL): 100% success rate; average Moves 12 (m=10) and 84 (m=50) (Table 2). Sorting (RL): 100% success; average Moves 8 (m=10) and 45 (m=50). Path (RL): 100% success; average Moves 4 (m=10 and m=50). AdjacentToNumber0 (vision+reasoning): 99.4% accuracy (m=50 test) when CNN is jointly trained (Appendix C).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Shows strong systematic/generalization: trained on small instances (e.g., family trees of size 20 or graphs of size 10) and generalizes perfectly to larger instances (e.g., m=100 or m=50) on many tasks; learns lifted (size-independent) rule representations. Depth D controls number of deduction steps (limit), breadth B controls max arity (factorial cost), so generalization holds under bounded B and sufficient D.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Partially interpretable: the architecture is explicitly designed to realize Horn clauses and forward-chaining, and a proof is provided that NLM can realize definite Horn clauses; however, specific learned rules are encoded implicitly in MLP weights and are not directly human-readable without additional rule-extraction methods. Authors note learned rules are not returned in explicit symbolic form.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Computational/representational limits: forward/backward complexity O(m^B D C^2) and parameter count O(D C^2); factorial growth in breadth B (arity) limits very high-arity reasoning; does not support cyclic (recursive) predicate references in current form; training is nontrivial and often requires curriculum learning; tensor grounding of all predicates limits scaling to very large object/entity sets (authors cite practical limit <~1000 objects). Some tasks have less stable training (graduation ratios <100% for some seeds/tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Authors provide a constructive argument and proof that NLMs can realize forward chaining of a partial set of Horn clauses by composition of Expand (introduce variables), Permute+MLP (boolean combination), and Reduce (quantification) operators; complexity analysis is given (O(m^B D C^2)). The design is motivated by mapping logic meta-rules (boolean combination and quantifiers) to differentiable neural operators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Logic Machines', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e641.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e641.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>∂ILP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Inductive Logic Programming (∂ILP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable implementation of ILP that learns weighted logical rules from grounded predicates using differentiable relaxation so gradient-based methods can be used to induce rules from examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning explanatory rules from noisy data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Differentiable Inductive Logic Programming (∂ILP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A differentiable ILP framework that takes grounded base predicates as input and learns rules (often from a restricted template space) in a differentiable manner so that gradient-based optimization can be used to induce rules that entail positive examples and not negatives. It operates over explicitly grounded predicates and uses a differentiable formulation of rule application/forward-chaining.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Inductive Logic Programming (ILP) / Horn-clause-style rule templates and grounded logical predicates; symbolic rule semantics are central.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Differentiable relaxation implemented with neural-style computation allowing gradient-based optimization (connectionist machinery to score and combine candidate rules).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Differentiable encoding of ILP: symbolic rule templates and groundings are represented in a continuous (soft) manner, enabling end-to-end gradient-based learning of rule weights; relies on grounding of all base predicates and template-driven search/parameterization.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Able to learn logical rules from examples with high accuracy on many ILP-style relational tasks, producing rule-based behavior with differentiable training; can achieve perfect accuracy on several relational reasoning benchmarks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Family tree reasoning and general graph reasoning benchmarks used in this paper (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Reported as 100% accuracy on the family tree predicates and on the listed graph reasoning predicates for tested sizes (e.g., m=20 and m=100 for family tree; m=10 and m=50 for graphs), except where scaling/memory limits prevented evaluation (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>In the reported experiments, ∂ILP generalized across tested instance sizes (100% reported on small and larger sizes for the tasks shown), but authors of this paper note that ∂ILP suffers from memory and scaling issues for some tasks (e.g., required 3-ary intentional predicate leading to N/A for 2-OutDegree), limiting applicability to tasks that require many intermediate predicates or large arities.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>More interpretable than fully neural approaches because it learns rule-like structures and uses logic templates; however, relies on template specification and may produce weighted/soft rules rather than crisp human-readable Horn clauses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Scaling/memory limitations: fails to scale to some tasks that require higher-arity intentional predicates (e.g., 2-OutDegree in the paper, marked N/A), and searching complex rule spaces remains expensive; often requires strong priors/templates to restrict search space.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Based on ILP and differentiable relaxation of symbolic inference to enable gradient-based learning; complexity grows exponentially with number of rules unless rule templates constrain search.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Logic Machines', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e641.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e641.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepProbLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deepproblog (Neural probabilistic logic programming)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural-probabilistic logic programming system that augments probabilistic logical reasoning (ProbLog) with neural components for perception, enabling end-to-end learning combining logical inference and neural perception.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepproblog: Neural probabilistic logic programming</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Deepproblog (Neural probabilistic logic programming)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A framework that combines ProbLog-style probabilistic logic programming with neural networks: neural components (e.g., CNNs for perception) provide probabilities or embeddings consumed by a probabilistic logical inference engine, allowing joint models that perform perception + symbolic (probabilistic) reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>ProbLog probabilistic logic program representation (logical rules with probabilistic facts / weighted inference).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks (for perception or sub-symbolic pattern extraction) that feed into the probabilistic logic layer.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular coupling: neural nets produce probabilistic facts or label distributions that are input to the probabilistic logic program; overall pipeline can be trained end-to-end (NeurIPS reference cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables combining perception and structured probabilistic logical reasoning in one framework, allowing handling of noisy inputs while retaining logical inference capabilities (mentioned as related work; no new performance reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Mentioned in related work as an approach for neural + logic integration; no direct evaluation data provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Not evaluated in this paper; cited as an example of neural augmentation of symbolic rule induction.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Probabilistic logic programs retain an explicit logical structure (interpretability), while neural modules introduce subsymbolic components; the paper does not detail interpretability of Deepproblog.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not discussed in detail in this paper; cited among approaches that augment symbolic reasoning with neural components but distinct from NLM in requiring more explicit rule/templates.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Combines probabilistic logical inference (ProbLog) with neural perceptual modules; cited as related work without further detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Logic Machines', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e641.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e641.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>End-to-end differentiable proving</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>End-to-end differentiable proving (Neural theorem proving)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable neural approach that models logical proving over knowledge bases by learning subsymbolic embeddings and performing soft unification and differentiable proof search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>End-to-end differentiable proving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>End-to-end differentiable proving (Rocktäschel & Riedel)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A neural approach to logical proving where subsymbolic embeddings are learned for predicates and entities and differentiable operations emulate aspects of symbolic proving, enabling end-to-end gradient-based learning for KB reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Logical proving / knowledge base rules (symbolic proof-like inference) as the conceptual target; uses symbolic provenance but operates on embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural embeddings and differentiable operators that implement soft unification and rule application; gradient-based learning.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Differentiable relaxation of theorem-proving operations; the symbolic proving process is emulated by differentiable modules so that neural embeddings can be learned jointly with soft proof procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Provides a way to learn subsymbolic representations that support approximate reasoning/proving over KBs; effective for KB completion tasks (cited as related work), but no new quantitative comparisons provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Cited in related work for knowledge-base reasoning / differentiable proving; not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Suggested to support reasoning-like behavior via learned embeddings; paper cites it as related method rather than reporting direct comparisons here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Operates on learned embeddings; reduces direct symbolic interpretability though it maintains an explicit differentiable proof structure.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not detailed in this paper; mentioned as part of the landscape of differentiable logic systems which have scaling/complexity challenges relative to NLM for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Differentiable emulation of symbolic proof search; learns embeddings and soft unification to approximate discrete inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Logic Machines', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e641.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e641.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DNC / NTM (hybrid memory machines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Turing Machines / Differentiable Neural Computer (neural with dynamic external memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hybrid neural architectures that couple a neural controller with an external, differentiable memory bank to emulate program-like behaviors and algorithmic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hybrid computing using a neural network with dynamic external memory.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural controller + external differentiable memory (DNC/NTM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Architectures that pair a recurrent neural controller with an addressable external memory matrix and differentiable read/write mechanisms; intended to emulate Turing-complete computation patterns and enable algorithmic tasks such as sorting or path-finding.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>No explicit formal declarative logic in original design, but external memory enables procedural/algorithmic symbolic-style manipulations; cited here as hybrid computing combining structured memory (symbol-like storage) and neural controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Recurrent neural controllers (LSTM-like), differentiable attention-based read/write mechanisms, and learned memory addressing; trained with gradient methods on algorithmic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Neural controller issues differentiable memory access operations (read/write/erase) to an external memory matrix; integration is tight and fully differentiable, enabling end-to-end training for procedural tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Can learn algorithmic behaviors (sorting, copying, path-finding) and to store/retrieve symbolic sequences in memory; exhibits some systematic generalization for algorithmic problems but with training difficulty and limited reliability on complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Compared/mentioned on Blocks World / family-tree 'finding' tasks and path-finding; cited prior results in the paper (Graves et al. results).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Cited results: For 'finding' MGUncle (task variant) DNC reached 81.8% (reference to Graves et al. 2016); for path-finding, Graves et al. reported ~55.3% probability of finding shortest path in a similar RL setting (paper cites these numbers as comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Some ability to generalize to longer sequences/problems, but empirical reliability is mixed; cited here to contrast NLM's stronger generalization on relational tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Behavior emerges in learned weights and memory usage patterns; not directly interpretable as symbolic rules, though memory traces can sometimes be inspected.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Training instability, difficulty scaling to complex symbolic rule induction; lower success probability on path-finding and family-tree 'finding' tasks compared to NLM in this paper's reported comparisons; does not natively implement logical quantifiers or lifted Horn-clause style reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Designed to mimic Turing-equivalent computation with differentiable read/write memory and neural controller; used as an example of neural architectures with symbolic-like memory capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Logic Machines', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning explanatory rules from noisy data <em>(Rating: 2)</em></li>
                <li>Deepproblog: Neural probabilistic logic programming <em>(Rating: 2)</em></li>
                <li>End-to-end differentiable proving <em>(Rating: 2)</em></li>
                <li>Hybrid computing using a neural network with dynamic external memory. <em>(Rating: 2)</em></li>
                <li>Neural module networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-641",
    "paper_id": "paper-3a6447361b20c249f5306ae17dee43f645430e31",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "NLM",
            "name_full": "Neural Logic Machines",
            "brief_description": "A neural-symbolic architecture that implements lifted first-order Horn-clause reasoning by representing predicates as probabilistic tensors and realizing boolean operations and quantifiers with neural operators (MLPs) and tensor permutations/aggregations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Neural Logic Machines (NLM)",
            "system_description": "NLM is a multi-layer, multi-group neural-symbolic system that represents r-ary logic predicates as probabilistic tensors over a fixed object set and implements rule induction / forward-chaining deduction by stacking neural operators. Each layer contains groups for predicates of arities 0..B (breadth), with inter-group Expansion/Reduction operations to implement quantifiers (introduce/eliminate variables) and intra-group Permute+MLP modules that approximate boolean combinations (AND/OR/NOT) over permutations of inputs. Depth D bounds the number of deduction steps; MLPs provide learned boolean functions while tensor Expand/Reduce implement lifted quantification.",
            "declarative_component": "First-order logic framed as Horn clauses (definite clauses) under closed-world assumption; predicates and quantifiers are explicitly represented (predicates as tensors, universal/existential quantifiers realized by min/max reduction); lifted rule semantics (implicit universal quantification) and forward-chaining logical deduction are the target symbolic formalism.",
            "imperative_component": "Neural networks (shared MLPs applied to tensorized predicate groundings), tensor operations (permute, concat, tile), and gradient-based optimization (Adam for supervised tasks; REINFORCE + entropy for reinforcement learning tasks); curriculum learning for training; optional integration with perception CNNs (e.g., LeNet) for noisy inputs.",
            "integration_method": "Tight neural-symbolic integration: symbolic constructs (predicates, permutations representing argument unification, quantifier expansion/reduction) are implemented as differentiable tensor operations; boolean formula evaluation is implemented by shared MLPs (Permute -&gt; MLP -&gt; sigmoid) applied uniformly to all groundings, enabling end-to-end training. Inter-group wiring (Expand/Reduce/Concat) composes different-arity relations. The whole architecture is fully differentiable and trained end-to-end (supervised or RL).",
            "emergent_properties": "Learns lifted rules (rules that generalize across object identities) encoded in network weights; strong compositional generalization (trained on small instance sizes and generalize to much larger sizes); can implement multi-arity relational reasoning and quantification; recovers human-like rule structures implicitly and executes sequences of deductions as forward-chaining; supports integration with perception for noisy inputs.",
            "task_or_benchmark": "Relational reasoning (family tree relations, graph connectivity predicates), decision-making/planning tasks (Blocks World), algorithmic tasks (sorting arrays via swap actions, single-source single-target path finding), and a visualized AdjacentToNumber0 experiment (MNIST + reasoning).",
            "hybrid_performance": "Family tree & general graph reasoning: 100% accuracy on most predicates at test sizes m=20 and m=100 (Table 1). BlocksWorld (RL): 100% success rate; average Moves 12 (m=10) and 84 (m=50) (Table 2). Sorting (RL): 100% success; average Moves 8 (m=10) and 45 (m=50). Path (RL): 100% success; average Moves 4 (m=10 and m=50). AdjacentToNumber0 (vision+reasoning): 99.4% accuracy (m=50 test) when CNN is jointly trained (Appendix C).",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Shows strong systematic/generalization: trained on small instances (e.g., family trees of size 20 or graphs of size 10) and generalizes perfectly to larger instances (e.g., m=100 or m=50) on many tasks; learns lifted (size-independent) rule representations. Depth D controls number of deduction steps (limit), breadth B controls max arity (factorial cost), so generalization holds under bounded B and sufficient D.",
            "interpretability_properties": "Partially interpretable: the architecture is explicitly designed to realize Horn clauses and forward-chaining, and a proof is provided that NLM can realize definite Horn clauses; however, specific learned rules are encoded implicitly in MLP weights and are not directly human-readable without additional rule-extraction methods. Authors note learned rules are not returned in explicit symbolic form.",
            "limitations_or_failures": "Computational/representational limits: forward/backward complexity O(m^B D C^2) and parameter count O(D C^2); factorial growth in breadth B (arity) limits very high-arity reasoning; does not support cyclic (recursive) predicate references in current form; training is nontrivial and often requires curriculum learning; tensor grounding of all predicates limits scaling to very large object/entity sets (authors cite practical limit &lt;~1000 objects). Some tasks have less stable training (graduation ratios &lt;100% for some seeds/tasks).",
            "theoretical_framework": "Authors provide a constructive argument and proof that NLMs can realize forward chaining of a partial set of Horn clauses by composition of Expand (introduce variables), Permute+MLP (boolean combination), and Reduce (quantification) operators; complexity analysis is given (O(m^B D C^2)). The design is motivated by mapping logic meta-rules (boolean combination and quantifiers) to differentiable neural operators.",
            "uuid": "e641.0",
            "source_info": {
                "paper_title": "Neural Logic Machines",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "∂ILP",
            "name_full": "Differentiable Inductive Logic Programming (∂ILP)",
            "brief_description": "A differentiable implementation of ILP that learns weighted logical rules from grounded predicates using differentiable relaxation so gradient-based methods can be used to induce rules from examples.",
            "citation_title": "Learning explanatory rules from noisy data",
            "mention_or_use": "use",
            "system_name": "Differentiable Inductive Logic Programming (∂ILP)",
            "system_description": "A differentiable ILP framework that takes grounded base predicates as input and learns rules (often from a restricted template space) in a differentiable manner so that gradient-based optimization can be used to induce rules that entail positive examples and not negatives. It operates over explicitly grounded predicates and uses a differentiable formulation of rule application/forward-chaining.",
            "declarative_component": "Inductive Logic Programming (ILP) / Horn-clause-style rule templates and grounded logical predicates; symbolic rule semantics are central.",
            "imperative_component": "Differentiable relaxation implemented with neural-style computation allowing gradient-based optimization (connectionist machinery to score and combine candidate rules).",
            "integration_method": "Differentiable encoding of ILP: symbolic rule templates and groundings are represented in a continuous (soft) manner, enabling end-to-end gradient-based learning of rule weights; relies on grounding of all base predicates and template-driven search/parameterization.",
            "emergent_properties": "Able to learn logical rules from examples with high accuracy on many ILP-style relational tasks, producing rule-based behavior with differentiable training; can achieve perfect accuracy on several relational reasoning benchmarks in this paper.",
            "task_or_benchmark": "Family tree reasoning and general graph reasoning benchmarks used in this paper (Table 1).",
            "hybrid_performance": "Reported as 100% accuracy on the family tree predicates and on the listed graph reasoning predicates for tested sizes (e.g., m=20 and m=100 for family tree; m=10 and m=50 for graphs), except where scaling/memory limits prevented evaluation (Table 1).",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "In the reported experiments, ∂ILP generalized across tested instance sizes (100% reported on small and larger sizes for the tasks shown), but authors of this paper note that ∂ILP suffers from memory and scaling issues for some tasks (e.g., required 3-ary intentional predicate leading to N/A for 2-OutDegree), limiting applicability to tasks that require many intermediate predicates or large arities.",
            "interpretability_properties": "More interpretable than fully neural approaches because it learns rule-like structures and uses logic templates; however, relies on template specification and may produce weighted/soft rules rather than crisp human-readable Horn clauses.",
            "limitations_or_failures": "Scaling/memory limitations: fails to scale to some tasks that require higher-arity intentional predicates (e.g., 2-OutDegree in the paper, marked N/A), and searching complex rule spaces remains expensive; often requires strong priors/templates to restrict search space.",
            "theoretical_framework": "Based on ILP and differentiable relaxation of symbolic inference to enable gradient-based learning; complexity grows exponentially with number of rules unless rule templates constrain search.",
            "uuid": "e641.1",
            "source_info": {
                "paper_title": "Neural Logic Machines",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "DeepProbLog",
            "name_full": "Deepproblog (Neural probabilistic logic programming)",
            "brief_description": "A neural-probabilistic logic programming system that augments probabilistic logical reasoning (ProbLog) with neural components for perception, enabling end-to-end learning combining logical inference and neural perception.",
            "citation_title": "Deepproblog: Neural probabilistic logic programming",
            "mention_or_use": "mention",
            "system_name": "Deepproblog (Neural probabilistic logic programming)",
            "system_description": "A framework that combines ProbLog-style probabilistic logic programming with neural networks: neural components (e.g., CNNs for perception) provide probabilities or embeddings consumed by a probabilistic logical inference engine, allowing joint models that perform perception + symbolic (probabilistic) reasoning.",
            "declarative_component": "ProbLog probabilistic logic program representation (logical rules with probabilistic facts / weighted inference).",
            "imperative_component": "Neural networks (for perception or sub-symbolic pattern extraction) that feed into the probabilistic logic layer.",
            "integration_method": "Modular coupling: neural nets produce probabilistic facts or label distributions that are input to the probabilistic logic program; overall pipeline can be trained end-to-end (NeurIPS reference cited in related work).",
            "emergent_properties": "Enables combining perception and structured probabilistic logical reasoning in one framework, allowing handling of noisy inputs while retaining logical inference capabilities (mentioned as related work; no new performance reported in this paper).",
            "task_or_benchmark": "Mentioned in related work as an approach for neural + logic integration; no direct evaluation data provided in this paper.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Not evaluated in this paper; cited as an example of neural augmentation of symbolic rule induction.",
            "interpretability_properties": "Probabilistic logic programs retain an explicit logical structure (interpretability), while neural modules introduce subsymbolic components; the paper does not detail interpretability of Deepproblog.",
            "limitations_or_failures": "Not discussed in detail in this paper; cited among approaches that augment symbolic reasoning with neural components but distinct from NLM in requiring more explicit rule/templates.",
            "theoretical_framework": "Combines probabilistic logical inference (ProbLog) with neural perceptual modules; cited as related work without further detail in this paper.",
            "uuid": "e641.2",
            "source_info": {
                "paper_title": "Neural Logic Machines",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "End-to-end differentiable proving",
            "name_full": "End-to-end differentiable proving (Neural theorem proving)",
            "brief_description": "A differentiable neural approach that models logical proving over knowledge bases by learning subsymbolic embeddings and performing soft unification and differentiable proof search.",
            "citation_title": "End-to-end differentiable proving",
            "mention_or_use": "mention",
            "system_name": "End-to-end differentiable proving (Rocktäschel & Riedel)",
            "system_description": "A neural approach to logical proving where subsymbolic embeddings are learned for predicates and entities and differentiable operations emulate aspects of symbolic proving, enabling end-to-end gradient-based learning for KB reasoning tasks.",
            "declarative_component": "Logical proving / knowledge base rules (symbolic proof-like inference) as the conceptual target; uses symbolic provenance but operates on embeddings.",
            "imperative_component": "Neural embeddings and differentiable operators that implement soft unification and rule application; gradient-based learning.",
            "integration_method": "Differentiable relaxation of theorem-proving operations; the symbolic proving process is emulated by differentiable modules so that neural embeddings can be learned jointly with soft proof procedures.",
            "emergent_properties": "Provides a way to learn subsymbolic representations that support approximate reasoning/proving over KBs; effective for KB completion tasks (cited as related work), but no new quantitative comparisons provided here.",
            "task_or_benchmark": "Cited in related work for knowledge-base reasoning / differentiable proving; not evaluated in this paper.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Suggested to support reasoning-like behavior via learned embeddings; paper cites it as related method rather than reporting direct comparisons here.",
            "interpretability_properties": "Operates on learned embeddings; reduces direct symbolic interpretability though it maintains an explicit differentiable proof structure.",
            "limitations_or_failures": "Not detailed in this paper; mentioned as part of the landscape of differentiable logic systems which have scaling/complexity challenges relative to NLM for some tasks.",
            "theoretical_framework": "Differentiable emulation of symbolic proof search; learns embeddings and soft unification to approximate discrete inference.",
            "uuid": "e641.3",
            "source_info": {
                "paper_title": "Neural Logic Machines",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "DNC / NTM (hybrid memory machines)",
            "name_full": "Neural Turing Machines / Differentiable Neural Computer (neural with dynamic external memory)",
            "brief_description": "Hybrid neural architectures that couple a neural controller with an external, differentiable memory bank to emulate program-like behaviors and algorithmic computation.",
            "citation_title": "Hybrid computing using a neural network with dynamic external memory.",
            "mention_or_use": "mention",
            "system_name": "Neural controller + external differentiable memory (DNC/NTM)",
            "system_description": "Architectures that pair a recurrent neural controller with an addressable external memory matrix and differentiable read/write mechanisms; intended to emulate Turing-complete computation patterns and enable algorithmic tasks such as sorting or path-finding.",
            "declarative_component": "No explicit formal declarative logic in original design, but external memory enables procedural/algorithmic symbolic-style manipulations; cited here as hybrid computing combining structured memory (symbol-like storage) and neural controllers.",
            "imperative_component": "Recurrent neural controllers (LSTM-like), differentiable attention-based read/write mechanisms, and learned memory addressing; trained with gradient methods on algorithmic tasks.",
            "integration_method": "Neural controller issues differentiable memory access operations (read/write/erase) to an external memory matrix; integration is tight and fully differentiable, enabling end-to-end training for procedural tasks.",
            "emergent_properties": "Can learn algorithmic behaviors (sorting, copying, path-finding) and to store/retrieve symbolic sequences in memory; exhibits some systematic generalization for algorithmic problems but with training difficulty and limited reliability on complex tasks.",
            "task_or_benchmark": "Compared/mentioned on Blocks World / family-tree 'finding' tasks and path-finding; cited prior results in the paper (Graves et al. results).",
            "hybrid_performance": "Cited results: For 'finding' MGUncle (task variant) DNC reached 81.8% (reference to Graves et al. 2016); for path-finding, Graves et al. reported ~55.3% probability of finding shortest path in a similar RL setting (paper cites these numbers as comparisons).",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Some ability to generalize to longer sequences/problems, but empirical reliability is mixed; cited here to contrast NLM's stronger generalization on relational tasks.",
            "interpretability_properties": "Behavior emerges in learned weights and memory usage patterns; not directly interpretable as symbolic rules, though memory traces can sometimes be inspected.",
            "limitations_or_failures": "Training instability, difficulty scaling to complex symbolic rule induction; lower success probability on path-finding and family-tree 'finding' tasks compared to NLM in this paper's reported comparisons; does not natively implement logical quantifiers or lifted Horn-clause style reasoning.",
            "theoretical_framework": "Designed to mimic Turing-equivalent computation with differentiable read/write memory and neural controller; used as an example of neural architectures with symbolic-like memory capabilities.",
            "uuid": "e641.4",
            "source_info": {
                "paper_title": "Neural Logic Machines",
                "publication_date_yy_mm": "2019-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning explanatory rules from noisy data",
            "rating": 2
        },
        {
            "paper_title": "Deepproblog: Neural probabilistic logic programming",
            "rating": 2
        },
        {
            "paper_title": "End-to-end differentiable proving",
            "rating": 2
        },
        {
            "paper_title": "Hybrid computing using a neural network with dynamic external memory.",
            "rating": 2
        },
        {
            "paper_title": "Neural module networks",
            "rating": 1
        }
    ],
    "cost": 0.020639499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>NEURAL LOGIC MACHINES</h1>
<p>Honghua Dong ${ }^{<em> 1}$, Jiayuan Mao ${ }^{</em> 1}$, Tian Lin ${ }^{2}$, Chong Wang ${ }^{3}$, Lihong Li ${ }^{2}$, and Denny Zhou ${ }^{2}$<br>${ }^{1}$ ITCS, IIIS, Tsinghua University {dhh14, mjy14}@mails.tsinghua.edu.cn<br>${ }^{2}$ Google Inc. {tianlin, lihong, dennyzhou}@google.com<br>${ }^{3}$ ByteDance Inc. chong.wang@bytedance.com</p>
<h4>Abstract</h4>
<p>We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks-as function approximators, and logic programming-as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers. After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>Deep learning has achieved great success in various applications such as speech recognition (Hinton et al., 2012), image classification (Krizhevsky et al., 2012; He et al., 2016), machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017), and game playing (Mnih et al., 2015; Silver et al., 2017). Starting from Fodor \&amp; Pylyshyn (1988), however, there has been a debate over the problem of systematicity (such as understanding recursive systems) in connectionist models (Fodor \&amp; McLaughlin, 1990; Hadley, 1994; Jansen \&amp; Watter, 2012).</p>
<p>Logic systems can naturally process symbolic rules in language understanding and reasoning. Inductive logic programming (ILP) (Muggleton, 1991; 1996; Friedman et al., 1999) has been developed for learning logic rules from examples. Roughly speaking, given a collection of positive and negative examples, ILP systems learn a set of rules (with uncertainty) that entails all of the positive examples but none of the negative examples. Combining both symbols and probabilities, many problems arose from high-level cognitive abilities, such as systematicity, can be naturally resolved. However, due to an exponentially large searching space of the compositional rules, it is difficult for ILP to scale beyond small-sized rule sets (Dantsin et al., 2001; Lin et al., 2014; Evans \&amp; Grefenstette, 2018).</p>
<p>To make the discussion concrete, let us consider the classic blocks world problem (Nilsson, 1982; Gupta \&amp; Nau, 1992). As shown in Figure 1, we are given a set of blocks on the ground. We can move a block $x$ and place it on the top of another block $y$ or the ground, as long as $x$ is moveable and $y$ is placeable. We call this operation Move $(x, y)$. A block is said to be moveable or placeable if there are no other blocks on it. The ground is always placeable, implying that we can place all blocks on the ground. Given an initial configuration of blocks world, our goal is to transform it into a target configuration by taking a sequence of Move operations.</p>
<p>Although the blocks world problem may appear simple at first glance, four major challenges exist in building a learning system to automatically accomplish this task:</p>
<ol>
<li>The learning system should recover a set of lifted rules (i.e., rules that apply to objects uniformly instead of being tied with specific ones) and generalize to blocks worlds which contain more blocks than those encountered during training. To get an intuition on this, we refer the readers who are not familiar with the blocks world domain to the task of learning to sort arrays (e.g.,
<sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<table>
<thead>
<tr>
<th style="text-align: right;">$\operatorname{On}(x, y)$</th>
<th style="text-align: left;">True if $x$ is on $y$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">$\operatorname{IsGround}(x)$</td>
<td style="text-align: left;">True if $x$ is the ground</td>
</tr>
<tr>
<td style="text-align: right;">$\operatorname{Clear}(x)$</td>
<td style="text-align: left;">True if there is no block on $x$</td>
</tr>
<tr>
<td style="text-align: right;">$\operatorname{Moveable}(x)$</td>
<td style="text-align: left;">$\neg \operatorname{IsGround}(x) \wedge \operatorname{Clear}(x)$</td>
</tr>
<tr>
<td style="text-align: right;">$\operatorname{Placeable}(x)$</td>
<td style="text-align: left;">$\operatorname{IsGround}(x) \vee \operatorname{Clear}(x)$</td>
</tr>
</tbody>
</table>
<p>Figure 1: (Left) A graphical illustration of the blocks world. Given an initial and a target worlds, the agent is required to move blocks to transform the initial configuration to the target one. (Right) A set of sentences used throughout the paper to define the blocks world.</p>
<p>Vinyals et al., 2015), where recurrent neural networks fail to generalize to arrays which are even just slightly longer than those for training.
2. The learning system should deal with high-order relational data and quantifiers, which goes beyond the scope of typical graph-structured neural networks (Kipf \&amp; Welling, 2017). For example, to apply the transitivity rule of a relation $r$, i.e. $r(a, c) \leftarrow \exists b r(a, b) \wedge r(b, c)$, we need to jointly inspect three objects $(a, b, c)$.
3. The learning system should scale up w.r.t. the complexity of the rules. ${ }^{2}$ Existing logic-driven approaches such as traditional ILP methods suffer an exponential computational complexity w.r.t. the number of logic rules to be learned (Dantsin et al., 2001; Lin et al., 2014; Evans \&amp; Grefenstette, 2018).
4. The learning system should recover rules based on a minimal set of learning priors. In contrast, traditional ILP methods usually require hand-coded and task-specific rule templates to restrict the size of searching spaces (Evans \&amp; Grefenstette, 2018).</p>
<p>In this paper, we propose Neural Logic Machines (NLMs) to address the aforementioned challenges. In a nutshell, NLMs offer a neural-symbolic architecture which realizes Horn clauses (Horn, 1951) in first-order logic (FOL). The key intuition behind NLMs is that logic operations such as logical ANDs and ORs can be efficiently approximated by neural networks, and the wiring among neural modules can realize the logic quantifiers.</p>
<p>The rest of the paper is organized as follows. We first revisit some useful definitions in symbolic logic systems and define our neural implementation of a rule induction system in Section 2. As a supplementary, we refer interested readers to Appendix A for implementation details. In Section 3 we evaluate the effectiveness of NLM on a broad set of tasks ranging from relational reasoning to decision making. We discuss related works in Section 4, and conclude the paper in Section 5.</p>
<h1>2 NeURal Logic Machines (NLM)</h1>
<p>The NLM is a neural realization of logic machines (under the Closed-World Assumption ${ }^{3}$ ). Given a set of base predicates, grounded on a set of objects (the premises), NLMs sequentially apply first-order rules to draw conclusions, such as a property about an object. For example, in the blocks world, based on premises IsGround $(u)$ and Clear $(u)$ of object $u$, NLMs can infer whether $u$ is moveable.</p>
<p>Internally, NLMs use tensors to represent logic predicates. This is done by grounding the predicate as True or False over a fixed set of objects. Based on the tensor representation, rules are implemented as neural operators that can be applied over the premise tensors and generate conclusion tensors. Such neural operators are probabilistic, lifted, and able to handle relational data with various orders (i.e., operating on predicates with different arities).</p>
<h3>2.1 Logic Predicates as Tensors</h3>
<p>We adopt a probabilistic tensor representation for logic predicates. Suppose we have a set of objects $\mathcal{U}=\left{u_{1}, u_{2}, \ldots, u_{m}\right}$. A predicate $p\left(x_{1}, x_{2}, \ldots, x_{r}\right)$, of arity $r$, can be grounded on the object set $\mathcal{U}$ (informally, we call it $\mathcal{U}$-grounding), resulting in a tensor $p^{\mathcal{U}}$ of shape $[m \varepsilon] \triangleq$ $[m, m-1, m-2, \ldots, m-r+1]$, where the value of each entry $p^{\mathcal{U}}\left(u_{i_{1}}, u_{i_{2}}, \ldots, u_{i_{r}}\right)$ of the tensor represents whether $p$ is True under the grounding that $x_{1}=u_{i_{1}}, x_{2}=u_{i_{2}}, \cdots, x_{r}=u_{i_{r}}$. Here, we restrict that the grounded objects of all $x_{i}$ 's are mutually exclusive, i.e., $i_{j} \neq i_{k}$ for all pairs of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An illustration of Neural Logic Machines (NLM). During forward propagation, NLM takes object properties and relations as input, performs sequential logic deduction, and outputs conclusive properties or relations of the objects. Implementation details can be found in Section 2.3.</p>
<p>indices $j$ and $k$. This restriction does not limit the generality of the representation, as the "missing" entries can be represented by the $\mathcal{U}$-grounding of other predicates with a smaller arity. For example, for a binary predicate $p$, the grounded values of the $p^{\mathcal{U}}(x, x)$ can be represented by the $\mathcal{U}$-grounding of a unary predicate $p^{\prime}(x) \triangleq p(x, x)$.</p>
<p>We extend this representation to a collection of predicates of the same arity. Let $C^{(r)}$ be the number of predicates of arity $r$. We stack the $\mathcal{U}$-grounding tensors of all predicates as a tensor of shape $\left[m^{\mathcal{L}}, C^{(r)}\right] \triangleq[m, m-1, m-2, \ldots, m-r+1, C^{(r)}]$, where the last dimension corresponds to the predicates. Intuitively, a group of $C^{(1)}$ unary predicates grounded on $m$ objects can be represented by a tensor of shape $\left[m, C^{(1)}\right]$, describing a group of "properties of objects", while a $\left[m, m-1, C^{(2)}\right]^{-}$ shaped tensor for $C^{(2)}$ binary predicates describes a group of "pairwise relations between objects". In practice, we set a maximum arity $B$ for the predicates of interest, called the breadth of the NLM.</p>
<p>In addition, NLMs take a probabilistic view of predicates. Each entry in $\mathcal{U}$-grounding tensors takes value from $[0,1]$, which can be interpreted as the probability being True. All premises, conclusions, and intermediate results in NLMs are represented by such probabilistic tensors. As a side note, we impose the restriction that all arguments in the predicates can only be variables or objects (i.e., constants) but not function symbols, which follows the setting of <em>Datalog</em> (Maier &amp; Warren, 1988).</p>
<h1>2.2 LOGIC RULES AS NEURAL OPERATORS</h1>
<p>Our goal is to build a neural architecture to learn rules that are both lifted and able to handle relational data with multiple arities. We present different modules of our neural operators by making analogies to a set of essential meta-rules in symbolic logic systems. Specifically, we discuss our neural implementation of (1) boolean logic rules, as lifted rules containing boolean operations (AND, OR, NOT) over a set of predicates; and (2) quantifications, which bridge predicates with different arities by logic quantifiers ( $\forall$ and $\exists$ ).
Next, we combine these neural units to compose NLMs. Figure 2 illustrates the overall multi-layer, multi-group architecture of an NLM. An NLM has layers of depth $D$ (horizontally), and each layer has $B+1$ computation units (vertically). These units operate on the tensor representations of predicates whose arities range from $[0, B]$, respectively. NLMs take input tensors of predicates (premises), perform layer-by-layer computations, and output tensors as conclusions.
As the number of layers increases, higher levels of abstraction can be formed. For example, the output of the first layer may represent $\operatorname{Clear}(x)$, while a deeper layer may output more complicated predicate like Moveable $(x)$. Thus, forward propagation in NLMs can be interpreted as a sequence of rule applications. We further show that NLMs can efficiently realize a partial set of Horn clauses.
We start from the neural boolean logic rules and the neural quantifiers.
Boolean logic. We use the following symbolic meta-rule for boolean logic:</p>
<p>$$
\hat{p}\left(x_{1}, x_{2}, \cdots, x_{r}\right) \leftarrow \operatorname{expression}\left(x_{1}, x_{2}, \cdots, x_{r}\right)
$$</p>
<p>where expression can be any boolean expressions consisting of predicates over all variables $\left(x_{1}, \ldots, x_{r}\right)$ and $\hat{p}(\cdot)$ is the conclusive predicate. For example, the rule Moveable $(x) \leftarrow$ $\neg$ IsGround $(x) \wedge \operatorname{Clear}(x)$ can be instantiated from this meta-rule.
Denote $\mathcal{P}=\left{p_{1}, \ldots, p_{k}\right}$ as the set of $|\mathcal{P}|$ predicates appeared in expression. By definition, all $p_{i}$ 's have the same arity $r$ and can be stacked as a tensor of shape $\left[m^{\mathcal{L}},|\mathcal{P}|\right]$. In Eq. 1, for</p>
<p>a specific grounding of the conclusive predicate $\hat{p}\left(x_{1} \cdots x_{r}\right)$, it is conditioned $r!\times|\mathcal{R}|$ grounding values with the same subset of objects, of arbitrary permutation as the arguments to all input predicates $\mathcal{P}$. For example, consider a specific ternary predicate $\hat{p}\left(x_{1}, x_{2}, x_{3}\right)$. For three different objects $a, b, c \in \mathcal{U}$, the grounding $\hat{p}(a, b, c)$ is conditioned on $p_{j}(a, b, c), p_{j}(a, c, b), p_{j}(b, a, c)$, $p_{j}(b, c, a), p_{j}(c, a, b), p_{j}(c, b, a)$ (all permutations of the parameters) for all $j$ (all input predicates).
Our neural implementation of boolean logic rules is a lifted neural module that uniformly applies to any grounding entries $\left(x_{1} \cdots x_{r}\right)$ in the output tensor $\hat{p}^{U}$. It has a Permute $(\cdot)$ operation transforming the tensor representation of $\mathcal{P}$, followed by a multi-layer perceptron (MLP). Given the tensor representation of $\mathcal{P}$, for each $p_{i}^{U}\left(x_{1}, x_{2}, \ldots, x_{r}\right)$, the $\operatorname{Permute}(\cdot)$ operation creates $r$ ! new tensors as $p_{i, 1}^{U}, \ldots, p_{i, r!}^{U}$ by permuting all axes that index objects, with all possible permutations. We stack all to form a $\left[m^{L}, r!\times|\mathcal{P}|\right]$-shaped tensor. An MLP uniformly applies to all $m^{L}$ object indices:</p>
<p>$$
\hat{p}\left(u_{i_{1}}, \cdots, u_{i_{r}}\right)=\sigma\left(\operatorname{MLP}\left(p_{1,1}\left(u_{i_{1}}, \ldots, u_{i_{r}}\right), \cdots, p_{k, r!}\left(u_{i_{1}}, \ldots, u_{i_{r}}\right)\right) ; \theta\right)
$$</p>
<p>where $\sigma$ is the sigmoid nonlinearity, $\theta$ is the trainable network parameters. For all sets of mutually exclusive indexes $i_{1}, \ldots, i_{r} \in{1,2, \ldots, m}$, the same MLP is applied. Thus, the size of $\theta$ is independent of the number of objects $m$. This property is analogous to the implicit unification property of Horn clauses: the rule $\hat{p}(x) \leftarrow p_{1}(x) \wedge p_{2}(x)$ implicitly means, $\forall x \hat{p}(x) \leftarrow p_{1}(x) \wedge p_{2}(x)$.
Quantification. We introduce two types of meta-rules for quantification, namely expansion and reduction. Let $p$ be a predicate, and we have
(Expansion) $\quad \forall x_{r+1} q\left(x_{1}, x_{2}, \cdots, x_{r}, x_{r+1}\right) \leftarrow p\left(x_{1}, x_{2}, \cdots, x_{r}\right)$,
where $x_{r+1} \notin\left{x_{i}\right}<em r_1="r+1">{i=1}^{r}$. The expansion operation constructs a new predicate $q$ from $p$, by introducing a new variable $x</em>$. For example, consider the following rule</p>
<p>$$
\operatorname{ValidMove}(x, y) \leftarrow \operatorname{Moveable}(x) \wedge \operatorname{Placeable}(y)
$$</p>
<p>This rule does not fit the meta-rule in Eq. 1 as some predicates on the RHS only take a subset of variables as inputs. However, it can be described by using the expansion and the boolean logic meta-rules jointly.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">1. $\forall z$ MoveableX $(x, z) \leftarrow$ Moveable $(x)$;</th>
<th style="text-align: left;">(from Eq. 3)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">2. $\forall z$ PlaceableY $(y, z) \leftarrow$ Placeable $(y)$;</td>
<td style="text-align: left;">(from Eq. 3)</td>
</tr>
<tr>
<td style="text-align: left;">3. ValidMove $(x, y) \leftarrow$ MoveableX $(x, y) \wedge$ PlaceableY $(y, x)$.</td>
<td style="text-align: left;">(from Eq. 1)</td>
</tr>
</tbody>
</table>
<p>The expansion meta-rule (Eq. 3) for a set of $C$ -ary predicates, represented by a $\left[m^{L}, C\right]$-shaped tensor, introduces a new and distinct variable $x_{r+1}$. Our neural implementation $\operatorname{Expand}(\cdot)$ repeats each predicate (their tensor representation) for $(m-r)$ times, and stacks in a new dimension. Thus the output shape is $\left[m^{r+1}, C\right]$.
The other meta-rule is for reduction:
(Reduction) $\quad q\left(x_{1}, x_{2}, \cdots, x_{r}\right) \leftarrow \forall x_{r+1} p\left(x_{1}, x_{2}, \cdots, x_{r}, x_{r+1}\right)$,
where the $\forall$ quantifier can also be replaced by $\exists$. The reduction operation reduces a variable in a predicate via the quantifier. As an example, the rule to deduce the moveability of objects,</p>
<p>$$
\operatorname{Moveable}(x) \leftarrow \neg \operatorname{IsGround}(x) \wedge \neg(\exists y \operatorname{On}(y, x))
$$</p>
<p>can be expressed using meta-rules as follows:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">1. Clear $(x) \leftarrow \forall y \neg \operatorname{On}(y, x)$;</th>
<th style="text-align: left;">(from Eq. 4)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">2. Moveable $(x) \leftarrow \neg$ IsGround $(x) \wedge$ Clear $(x)$.</td>
<td style="text-align: left;">(from Eq. 1)</td>
</tr>
</tbody>
</table>
<p>The reduction meta-rule (Eq. 4) for a set of $C(r+1)$-ary predicates, represented by a $\left[m^{r+1}, C\right]$ shaped tensor, eliminates the variable $x_{r+1}$ via quantifiers. For $\exists$ (or $\forall$ ), our neural implementation $\operatorname{Reduce}(\cdot)$ takes the maximum (or minimum) element along the dimension of $x_{r+1}$, and stacks the two resulting tensors. Therefore, the output shape becomes $\left[m^{L}, 2 C\right]$.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An illustration of the computational block inside NLM for binary predicates at layer $i . C_{i}^{(j)}$ denotes the number of output predicates of group $j$ at layer $i .[\cdot]$ denotes the shape of the tensor.</p>
<h1>2.3 Neural Logic Machines</h1>
<p>NLMs realize symbolic logic rules in a multi-layer multi-group architecture, illustrated in Figure 2. An NLM has $D$ layers, and each layer has $B+1$ computation units as groups. Between layers, we use intra-group computation (Eq. 1). The predicates at each layer are grouped by their arities, and inside each group, we use inter-group computation (Eq. 3 and 4).
We define $\mathcal{O}<em i="i">{i}=\left{O</em>}^{(0)}, O_{i}^{(1)}, \cdots, O_{i}^{(B)}\right}$ as the outputs of layer $i$, where $O_{i}^{(r)}$ is the output corresponding to the $r$-ary unit at layer $i$. For convenience, we denote $\mathcal{O<em 0="0">{0}=\left{O</em>}^{(0)}, O_{0}^{(1)}, \cdots, O_{0}^{(B)}\right}$ as the $\mathcal{U}$-grounding tensors for NLM's base predicates (the premises), and $\mathcal{O<em i-1="i-1">{D}$ at the last layer as the conclusions. The overall computation is performed layer-by-layer, from layer 1 to layer $D$. All computation units at layer $i$ work simultaneously, taking $\mathcal{O}</em>$.}$ as inputs and generating $\mathcal{O}_{i</p>
<p>Let us consider a specific group $r$ at layer $i$, and we show how to calculate $O_{i}^{(r)}$.
Inter-group computation. As shown in Figures 2 and 3, we connect tensors from the previous layer $i-1$ in vertically neighboring groups (i.e. $r-1, r$ and $r+1$ ), and aligns their shapes by expansion (Eq. 3) or reduction (Eq. 4) to form an intermediate tensor $I_{i}^{(r)}$ :</p>
<p>$$
I_{i}^{(r)}=\operatorname{Concat}\left(\operatorname{Expand}\left(O_{i-1}^{(r-1)}\right), O_{i-1}^{(r)}, \operatorname{Reduce}\left(O_{i-1}^{(r+1)}\right)\right)
$$</p>
<p>Nonexistent terms are ignored (e.g. when $r+1&gt;B$ or $r-1&lt;0$ ). Note that from the previous layer, $O_{i-1}^{(r-1)}, O_{i-1}^{(r)}, O_{i-1}^{(r+1)}$ have shapes $\left[m \underline{r-1}, C_{i-1}^{(r-1)}\right],\left[m \underline{r}, C_{i-1}^{(r)}\right],\left[m \underline{r+1}, C_{i-1}^{(r+1)}\right]$, respectively. After the concatenation, the resulting tensor $I_{i}^{(r)}$ is of shape $\left[m \underline{r}, \widehat{C}<em i="i">{i}^{(r)}\right]$, where the number of new predicates is $\widehat{C}</em>$, and the 2 comes from the two quantifiers ( $\forall$ and $\exists$ ). The inter-group computation essentially aligns predicates of neighboring arities. Relational representations of different orders get combined together through the neural quantification.
Intra-group computation. The intra-group computation is implemented as the neural boolean logic in Eq. 1. It take the intermediate tensor $I_{i}^{(r)}$ as input, permutes and generates the output tensor $O_{i}^{(r)}$ :}^{(r)} \triangleq C_{i-1}^{(r-1)}+C_{i-1}^{(r)}+2 C_{i-1}^{(r+1)</p>
<p>$$
O_{i}^{(r)}=\sigma\left(\operatorname{MLP}\left(\operatorname{Permute}\left(I_{i}^{(r)}\right) ; \theta_{i}^{(r)}\right)\right)
$$</p>
<p>where $\sigma$ is the sigmoid nonlinearity and $\theta_{i}^{(r)}$ denotes trainable parameters. We apply Permute function to $\widehat{C}<em i="i">{i}^{(r)}$ tensors in $I</em>}^{(r)}$ individually, and get $r!\widehat{C<em i="i">{i}^{(r)}$ tensors. We set the number of output neurons to be $C</em>\right]$.
Example. For concreteness, in Figure 3, consider group 2 (binary predicates) at layer $i$. The module begins with the inter-group computation. It first collects the output of vertically consecutive groups (unary, binary and ternary) from the previous layer $i-1$, where their shapes are shown in the figure. Then it uses expansion/reduction to compose the intermediate tensor $I_{i}^{(2)}$ containing $\widehat{C}}^{(r)}$, thus the shape of output tensor $O_{i}^{(r)}$ is $\left[m \underline{r}, C_{i}^{(r)<em i-1="i-1">{i}^{(2)} \triangleq C</em>(\cdot, \cdot)$ corresponds to the Permute operation, while the MLP is shared among all pairs of objects $(x, y)$.
Remark. It can be verified that NLMs can realize the forward chaining of a partial set of Horn clauses. In NLMs, we consider only finite cases. Thus, there should not exist cyclic references of}^{(1)}+$ $C_{i-1}^{(2)}+2 C_{i-1}^{(2)}$ predicates. For each object pair $(x, y)$, the output $\mathcal{U}$-grounding tensor of predicates is computed by intra-group computation $O_{i}^{(2)}(x, y)=\operatorname{MLP}\left(\operatorname{Concat}\left(I_{i}^{(2)}(x, y), I_{i}^{(2)}(y, x)\right) ; \theta_{i}^{(2)}\right)$, and the output shape is $\left[m, m-1, C_{i}^{(2)}\right]$. The $\operatorname{Concat</p>
<p>predicates among rules. The extension to support cyclic references is left as a future work. See the proof in Appendix D. Thus, given the training dataset containing pairs of (premises, conclusions), NLMs can induce lifted rules that entail the conclusions and generalize w.r.t. the number of objects during testing.</p>
<h1>2.4 EXPRESSIVENESS AND COMPUTATIONAL COMPLEXITY</h1>
<p>The expressive power of NLM depends on multiple factors:</p>
<ol>
<li>The depth $D$ of NLM (i.e., number of layers) restricts the maximum number of deduction steps.</li>
<li>The breadth $B$ of NLM (i.e., the maximum number of variables in all predicates considered) limits the arity of relations among objects. Practically, most (intermediate) predicates are binary or ternary and we set $B$ depending on the task (typically 2 or 3 , see Table 3 in Appendix B.)</li>
<li>The number of output predicates used at each layer ( $C_{i}^{(r)}$ in Figure 3). Let $C=\max <em i="i">{i, r} C</em>$, and this number is often small in our experiments (e.g., 8 or 16 ).}^{(r)</li>
<li>In Eq. 2, the expressive power of MLP (number of hidden layers and number of hidden neurons) restricts the complexity of the boolean logic to be represented. In our experiments, we usually prefer shallow networks (e.g., 0 or 1 hidden layer) with a small number of neurons (e.g., 8 or 16). This can be viewed as a low-dimension regularization on the logic complexity and encourages the learned rule to be simple.
The computational complexity of NLM's forward or backward propagation is $O\left(m^{B} D C^{2}\right)$ where $m$ is the number of objects. The network has $O\left(D C^{2}\right)$ parameters. Assuming $B$ is a small constant, the computational complexity of NLM is quadratic in the number of allowed predicates.</li>
</ol>
<h2>3 EXPERIMENTS</h2>
<p>In this section, we show that NLM can solve a broad set of tasks, ranging from relational reasoning to decision making. Furthermore, we show that NLM trained using small-sized instances can generalize to large-sized instances. In the experiments, Softmax-Cross-Entropy loss is used for supervised learning tasks, and REINFORCE (Williams, 1992) is used for reinforcement learning tasks.</p>
<p>Due to space limitation, interested readers are referred to Appendix A for details of training (including curriculum learning) in the decision making tasks, and Appendix B for more implementation details (such as residual connections (He et al., 2016)), hyper-parameters, and model selection criterion.</p>
<h3>3.1 BASELINES</h3>
<p>We consider two baselines as representatives of the connectionist and symbolicist: Memory Networks (MemNN) (Sukhbaatar et al., 2015) and Differentiable Inductive Logic Programming ( $\partial$ ILP) (Evans \&amp; Grefenstette, 2018), a state-of-the-art ILP framework. We also make comparisons with other models such as Differentiable Neural Computer (DNC) (Graves et al., 2016) and graph neural networks (Li et al., 2016) whenever eligible.</p>
<p>For MemNN, in order to handle an arbitrary number of inputs (properties, relations), we adopt the method from Graves et al. (2016). Specifically, each object is assigned with a unique identifier (a binary integer ranging from 0 to 255), as its "name". The memory of MemNN is now a set of "pre-conditions". For unary predicates, the memory slot contains a tuple $(\operatorname{id}(x), 0$, properties $(x))$ for each x , and for binary predicates $p(x, y)$, the memory slot contains a tuple $(\operatorname{id}(x), \operatorname{id}(y)$, relations $(x, y))$, for each pair of $(x, y)$. Both properties $(x)$ and relations $(x, y)$ are length- $k$ vectors $v$, where $k$ is the number of input predicates. We number each input predicate with an integer $i=1,2, \cdots, k$. If object $x$ has a property $p_{i}(x)$, then $v[i]=1$; otherwise, $v[i]=0$. If a pair of objects $(x, y)$ have relation $p_{i}(x, y)$, then $v[i]=1$; otherwise, $v[i]=0$. We extract the key and value for MemNN's to lookup on the given pre-conditions with 2-layer multi-layer perceptrons (MLP). MemNN relies on iterative queries to the memory to perform relational reasoning. Note that MemNN takes a sequential representation of the multi-relational data.</p>
<p>For $\partial$ ILP, the grounding of all base predicates is used as the input to the system.</p>
<h3>3.2 FAMILY TREE REASONING</h3>
<p>The family tree is a benchmark for inductive logic programming, where the machine is given a family tree containing $m$ members. The family tree is represented by the following relations (predicates):</p>
<p>Table 1: Comparison among MemNN, $\partial$ ILP and the proposed NLM in family tree and graph reasoning, where $m$ is the size of the testing family trees or graphs. Both $\partial$ ILP and NLM outperform the neural baseline and achieve perfect accuracy (100\%) on test set. Note N/A mark means that $\partial$ ILP cannot scale up in 2-OutDegree.</p>
<table>
<thead>
<tr>
<th>Family Tree</th>
<th>MemNN</th>
<th></th>
<th>$\partial$ ILP</th>
<th></th>
<th>NLM (Ours)</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$m=20$</td>
<td>$m=100$</td>
<td>$m=20$</td>
<td>$m=100$</td>
<td>$m=20$</td>
<td>$m=100$</td>
</tr>
<tr>
<td>HasFather</td>
<td>99.9\% / 99.9\%</td>
<td>59.8\% / 65.2\%</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
</tr>
<tr>
<td>HasSister</td>
<td>86.3\% / 85.5\%</td>
<td>59.8\% / 66.4\%</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
</tr>
<tr>
<td>IsGrandparent</td>
<td>96.5\% / 84.7\%</td>
<td>97.7\% / 63.7\%</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
</tr>
<tr>
<td>IsUncle</td>
<td>96.3\% / 85.8\%</td>
<td>96.0\% / 64.0\%</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
</tr>
<tr>
<td>IsMGUncle</td>
<td>99.7\% / 98.4\%</td>
<td>98.4\% / 81.7\%</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
</tr>
<tr>
<td>Graph</td>
<td>MemNN</td>
<td></td>
<td>$\partial$ ILP</td>
<td></td>
<td>NLM (Ours)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>$m=10$</td>
<td>$m=50$</td>
<td>$m=10$</td>
<td>$m=50$</td>
<td>$m=10$</td>
<td>$m=50$</td>
</tr>
<tr>
<td>AdjacentToRed</td>
<td>95.2\% / 94.6\%</td>
<td>93.1\% / 91.9\%</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
</tr>
<tr>
<td>4-Connectivity</td>
<td>92.3\% / 90.5\%</td>
<td>81.3\% / 88.0\%</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
</tr>
<tr>
<td>6-Connectivity</td>
<td>67.6\% / 58.8\%</td>
<td>43.9\% / 67.9\%</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
</tr>
<tr>
<td>1-OutDegree</td>
<td>99.8\% / 99.7\%</td>
<td>78.6\% / 81.2\%</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
</tr>
<tr>
<td>2-OutDegree</td>
<td>81.4\% / 61.8\%</td>
<td>96.7\% / 87.7\%</td>
<td>N/A</td>
<td>N/A</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
</tr>
</tbody>
</table>
<p>IsSon, IsDaughter, IsFather and IsMother. The goal of the task is to reason out other properties of family members or relations between them. Our results are summarized in Table 1.</p>
<p>For MemNN, we treat the problem of relation prediction as a question answering task. For example, to determine whether member $x$ has a father in the family tree, we input $\operatorname{id}(x)$ to MemNN as the question. MemNN then performs multiple queries to the memory and updates its hidden state. The finishing hidden state is used to classify whether HasFather $(x)$. For relations (binary predicates), the corresponding MemNN takes the concatenated embedding of $\operatorname{id}(x)$ and $\operatorname{id}(y)$ as the question.</p>
<p>For $\partial$ ILP, we take the grounded probability of the "target" predicate as the output; for an NLM with $D$ layers, we take the corresponding group of output predicates at the last layer (for property prediction, we use tensor $O_{D}^{(1)}$ to represent unary predicates, while for relation prediction we use tensor $O_{D}^{(2)}$ to represent binary predicates) and classify the property or relation with a linear layer.</p>
<p>All models are trained on instances of size 20 and tested on instances of size 20 and 100 (size is defined as the number of family members). The models are trained with fully supervised learning (labels are available for all objects or pairs of objects). During the testing phase, the accuracy is evaluated (and averaged) on all objects (for properties such as HasFather) or pairs of objects (for relations such as IsUncle). MGUncle is defined as one's maternal great uncle, which is also used by Differentiable Neural Computer (DNC) (Graves et al., 2016). We report the performance of MemNN in the format of Micro / Macro accuracy. We also try our best to replicate the setting used by Graves et al. (2016), and as a comparison, in the task of "finding" the MGUncle instead of "classifying", DNC reaches the accuracy of $81.8 \%$.</p>
<h1>3.3 GENERAL GRAPH REASONING</h1>
<p>We further extend the Family tree to general graphs and report the reasoning performance in Table 1.
We treat each node in the graph as an object (symbol). The (undirected) graph is fed into the model in the form of a "HasEdge" relation between nodes (which is an adjacent matrix). Besides, an extra property color represented by one-hot vectors is defined for every node. A node has the property of AdjacentToRed if it is adjacent to a red node by an outgoing edge. $k$-Connectivity is a relation between two nodes in the graph, which is true if two nodes are connected by a path with length at most $k$. A node has property $k$-OutDegree if its out-degree is exactly $k$. The N/A result of $\partial$ ILP in the 2-OutDegree task comes from its memory restriction (Evans \&amp; Grefenstette, 2018), where 3-ary intentional predicate is required. As an example, a human-written logic rule for 2-OutDegree can be -OutDegree $(a) \leftarrow \exists_{b} \exists_{c} \forall_{d}$ HasEdge $(a, b) \wedge$ HasEdge $(a, c) \wedge \neg$ HasEdge $(a, d)$ where $a, b, c$ and $d$ are distinct nodes in the graph.</p>
<p>All models are trained on instances of size 10 and tested on instances of size 10 and 50 (size is defined as the number of nodes in the graph).</p>
<p>Table 2: Comparison between MemNN and the proposed NLM in the blocks world, sorting integers, and finding shortest paths, where $m$ is the number of blocks in the blocks world environment or the size of the arrays/graphs in sorting/path environment. Both models are trained on instance size $m \leq 12$ and tested on $m=10$ or 50 . The performance is evaluated by two metrics and separated by "/": the probability of completing the task during the test, and the average Moves used by the agents when they complete the task. There is no result for $\partial$ ILP since it fails to scale up. MemNN fails to complete the blocks world within the maximum $m \times 4$ Moves.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>MemNN</th>
<th></th>
<th>NLM (Ours)</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$m=10$</td>
<td>$m=50$</td>
<td>$m=10$</td>
<td>$m=50$</td>
</tr>
<tr>
<td>BlocksWorld</td>
<td>$0 \% / \mathrm{N} / \mathrm{A}$</td>
<td>$0 \% / \mathrm{N} / \mathrm{A}$</td>
<td>$100 \% / 12$</td>
<td>$100 \% / 84$</td>
</tr>
<tr>
<td>Sorting</td>
<td>$100 \% / 22$</td>
<td>$90 \% / 986.6$</td>
<td>$100 \% / 8$</td>
<td>$100 \% / 45$</td>
</tr>
<tr>
<td>Path</td>
<td>$45 \% / 13.3$</td>
<td>$12 \% / 42.7$</td>
<td>$100 \% / 4$</td>
<td>$100 \% / 4$</td>
</tr>
</tbody>
</table>
<h1>3.4 Blocks World</h1>
<p>We also test NLM's capability of decision making in the classic blocks world domain (Nilsson, 1982; Gupta \&amp; Nau, 1992) by slightly extending the model to fit the formulation of Markov Decision Process (MDP) in reinforcement learning.
Shown in Figure 1, an instance of the blocks world environment contains two worlds: the initial world and the target world, each containing the ground and $m$ blocks. The task is to take actions in the operating world and make its configuration the same as the target world. The agent receives positive rewards only when it accomplishes the task and the sparse reward setting brings significant hardness. Each object (blocks or ground) can be represented by four properties: world_id, object_id, coordinate_x, coordinate_y. The ground has a fixed coordinate $(0,0)$. The input is the result of the numeral comparison among all pairs of objects (may come from different worlds). For example, in $x$-coordinate, the comparison produces three relations for each object pair $(i, j), i \neq j$ : $\operatorname{Left}(i, j)$ (whether $i$ is to the left of $j$, or $\mathbf{1}\left[x_{i}&lt;x_{j}\right]$ ), $\operatorname{SameX}(i, j)$ and $\operatorname{Right}(i, j)$.
The only operation is $\operatorname{Move}(i, j)$, which moves object $i$ onto the object $j$ in the operating world if $i$ is movable and $j$ is placeable. If the operation is invalid, it will have no effect; otherwise, the action takes effect and the state represented as coordinates will change accordingly. In our setting, an object $i$ is movable iff it is not the ground and there are no blocks on it, i.e. $\forall_{j} \neg(\operatorname{Up}(i, j) \wedge \operatorname{SameX}(i, j))$. Object $i$ is placeable iff it is the ground or there are no blocks on it.
To avoid the ambiguity of the $x$-coordinates while putting blocks onto the ground, we set the $x$ coordinate of block $i$ to be $i$ when it is placed onto the ground. The action space is $(m+1) \times m$ where $m$ is the number of blocks in the world and +1 comes from the "ground". For both MemNN and NLM, we apply a shared MLP on the output relational predicates of each pair of objects $O_{D}^{(2)}(x, y)$ and compute an action score $s(x, y)$. The probability for $\operatorname{Move}(x, y)$ is $\propto \exp s(x, y)$ (by taking a Softmax). The results are summarized in Table 2. For more discussion on the confidence bounds of the experiments, please refer to Appendix B.6.</p>
<h3>3.5 GENERAL ALGORITHMS</h3>
<p>We further show NLM's ability to excel at algorithmic tasks, such as Sorting and Path. We view an algorithm as a sequence of primitive actions and cast as a reinforcement learning problem.</p>
<p>Sorting. We first consider the problem of sorting integers. Given a length- $m$ array $a$ of integers, the algorithm needs to iterative swap elements to sort the array in ascending order. We treat each slot in the array as an object, and input their index relations (whether $i&lt;j$ ) and numeral relations (whether $a[i]&lt;a[j]$ ) to NLM or MemNN. The action space is $m \times(m-1)$ indicating the pair of integers to be swapped. Table 2 summarizes the learning performance.</p>
<p>As the comparisons between all pairs of elements in the array are given to the agent, sorting the array within the maximum number of swaps is an easy task. A trivial solution is to randomly swap an inversion ${ }^{4}$ in the array at each step.</p>
<p>Beyond being able to generalize to arrays of arbitrary length, with different hyper-parameters and random seeds, the learned algorithms can be interpreted as Selection-Sort, Bubble-Sort, etc. We include videos demonstrating some learned algorithms in our website. ${ }^{5}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Path finding. We also test the performance of finding a path (single-source single-target path) in a given graph as a sequential decision-making problem in reinforcement learning environment. Given an undirected graph represented by its adjacency matrix as relations, the algorithm needs to find a path from a start node $s$ (with property IsStart $(s)=$ True) to the target node $t$ (with property IsTarget $(t)=$ True). To restrict the number of deduction steps, we set the maximum distance between $s$ and $t$ to be 5 during the training and set the distance between $s$ and $t$ to be 4 during the testing, which replicates the setting of Graves et al. (2016). Table 2 summarizes the result.</p>
<p>Path task here can be seen as an extension of bAbI task 19 (path finding) (Weston et al., 2015) with symbolic representation. As a comparison with graph neural networks, Li et al. (2016) achieved 99\% accuracy on the bAbI task 19. Contrastively, we formulate the shortest path task as a more challenging reinforcement learning (decision-making) task rather than a supervised learning (prediction) task as in Graves et al. (2016). Specifically, the agent iteratively chooses the next node next along the path. At the next step, the starting node will become next (at each step, the agent will move to next). As a comparison, in Graves et al. (2016), Differentiable Neural Computer (DNC) finds the shortest path with probability $55.3 \%$ in a similar setting.</p>
<h1>4 Related Works and Discussions</h1>
<p>ILP and relational reasoning. Inductive logic programming (ILP) (Muggleton, 1991; 1996; Friedman et al., 1999) is a paradigm for learning logic rules derived from a limited set of rule templates from examples. Being a powerful way of reasoning over discrete symbols, it is successfully applied to various language-related problems, and has been integrated into modern learning frameworks (Kersting et al., 2000; Richardson \&amp; Domingos, 2006; Kimmig et al., 2012). Recently, Evans \&amp; Grefenstette (2018) introduces a differentiable implementation of ILP which works with connectionist models such as CNNs. Sharing a similar spirit, Rocktäschel \&amp; Riedel (2017) introduces an end-to-end differentiable logic proving system for knowledge base (KB) reasoning. A major challenge of these approaches is to scale up to a large number of complex rules. Searching a rule as complex as our ShouldMove example in Appendix E from scratch is beyond the scope of most systems that use weighted symbolic rules generated from templates.
As shown in Section 2.4, both computational complexity and parameter size of the NLM grow polynomially w.r.t. the number of allowed predicates (in contrast to the exponential dependence in $\partial$ ILP (Evans \&amp; Grefenstette, 2018)), but factorially w.r.t. the breadth (max arity, same as $\partial$ ILP). Therefore, our method can deal with more complex tasks such as the blocks world which requires using a large number of intermediate predicates, while $\partial$ ILP fails to search in such a large space.
Our paper also differs from existing approaches on using neural networks to augment symbolic rule induction (Lippi \&amp; Frasconi, 2009; Manhaeve et al., 2018). Specifically, we have no rule designed by humans as the input or the knowledge base for the model. NLMs are general neural architectures for learning lifted rules from only input-output pairs.
Our work is also related to symbolic relational reasoning, which has a wide application in processing discrete data structures such as knowledge graphs and social graphs (Zhu et al., 2014; Kipf \&amp; Welling, 2017; Zeng et al., 2017; Yang et al., 2017). Most symbolic relational reasoning approaches (e.g., Yang et al., 2017; Rocktäschel \&amp; Riedel, 2017) are developed for KB reasoning, in which the predicates on both sides of a rule is known in the KB. Otherwise, the complexity grows exponentially in the number of used rules for a conclusion, which is the case in the blocks world. Moreover, Yang et al. (2017) considers rues of the form query $(\mathrm{Y}, \mathrm{X}) \leftarrow \mathrm{R}<em _mathrm_n="\mathrm{n">{\mathrm{n}}\left(\mathrm{Y}, \mathrm{Z}</em>}}\right) \wedge \cdots \wedge \mathrm{R<em 1="1">{1}\left(\mathrm{Z}</em>\right)$, which is not for general reasoning. The key of Rocktäschel \&amp; Riedel (2017) and Campero et al. (2018) is to learn subsymbolic embeddings of entities and predicates for efficient KB completion, which differs from our focus. While NLMs can scale up to complex rules, the number of objects/entities or relations should be bounded as a small value (e.g., $&lt;1000$ ), since all predicates are represented as tensors. This is, to some extent, in contrast with the systems developed for knowledge base reasoning. We leave the scalability of NLMs to large entity sets as future works.}, \mathrm{X</p>
<p>Besides, modular networks (Andreas et al., 2016; 2017; Mascharka et al., 2018) are proposed for the reasoning over subsymbolic data such as images and natural language question answering. Santoro et al. (2017) implements a visual reasoning system based on "virtual" objects brought by receptive</p>
<p>fields in CNNs. Wu et al. (2017) tackles the problem of deriving structured representation from raw pixel-level inputs. Dai et al. (2018) combines structured visual representation and theorem proving.
Graph neural networks and relational inductive bias. Graph convolution networks (GCNs) (Bruna et al., 2014; Li et al., 2016; Defferrard et al., 2016; Kipf \&amp; Welling, 2017) is a family of neural architectures working on graphs. As a representative, Gilmer et al. (2017) proposes a message passing modeling for unifying various graph neural networks and graph convolution networks. GCNs achieved great success in tasks with intrinsic relational structures. However, most of the GCNs operate on pre-defined graphs with only nodes and binary connections. This restricts the expressive power of models in general-purpose reasoning tasks (Li et al., 2016).
In contrast, this work removes such restrictions and introduces a neural architecture to capture lifted rules defined on any set of objects. Quantitative results support the effectiveness of the proposed model in a broad set of tasks ranging from relational reasoning to modeling general algorithms (as decision-making process). Moreover, being fully differentiable, NLMs can be plugged into existing convolutional or recurrent neural architectures for logic reasoning.
Relational decision making. Logic-driven decision making is also related to Relational RL (Van Otterlo, 2009), which models the environment as a collection of objects and their relations. State transition and policies are both defined over objects and their interactions. Examples include OOMDP (Diuk et al., 2008; Kansky et al., 2017), symbolic models for learning in interactive domains (Pasula et al., 2007), structured task definition by object-oriented instructions (Denil et al., 2017), and structured policy learning (Garnelo et al., 2016). General planning methods solve these tasks via planning based on rules (Hu \&amp; De Giacomo, 2011; Srivastava et al., 2011; Jiménez et al., 2019). The goal of our paper is to introduce a neural architecture which learns lifted rules and handle relational data with multiple orders. We leave its application in other RL and planning tasks as future work.
Neural abstraction machines and program induction. Neural Turing Machine (NTM) (Graves et al., 2014; 2016) enables general-purpose neural problem solving such as sorting by introducing an external memory that mimics the execution of Turing Machine. Neural program induction and synthesis (Neelakantan et al., 2016; Reed \&amp; De Freitas, 2016; Kaiser \&amp; Sutskever, 2016; Parisotto et al., 2017; Devlin et al., 2017; Bunel et al., 2018; Sun et al., 2018) are recently introduced to solve problems by synthesizing computer programs with neural augmentations. Some works tackle the issue of the systematical generalization by introducing extra supervision (Cai et al., 2017). In Chen et al. (2018), more complex programs such as language parsing are studied. However, the neural programming and program induction approaches are usually hard to optimize in an end-to-end manner, and often require strong supervisions (such as ground-truth programs).</p>
<h1>5 CONCLUSIONS AND DISCUSSIONS</h1>
<p>In this paper, we propose a novel neural-symbolic architecture called Neural Logic Machines (NLMs) which can conduct first-order logic deduction. Our model is fully differentiable, and can be trained in an end-to-end fashion. Empirical evaluations show that our method is able to learn the underlying logical rules from small-scale tasks, and generalize to large-scale tasks.
The promising results open the door for several research directions. First, the maximum depth of the NLMs is a hyperparameter to be specified for individual problems. Future works may investigate how to extend the model, so that it can adaptively select the right depth for the problem at hand. Second, it is interesting to extend NLMs to handle vector inputs with real-valued components. Currently, NLM requires symbolic input that may not be easily available in applications like health care where many inputs (e.g., blood pressure) are real numbers. Third, training NLMs remains nontrivial, and techniques like curriculum learning have to be used. It is important to find an effective yet simpler alternative to optimize NLMs. Last but not least, unlike ILP methods that learn a set of rules in an explainable format, the learned rules of NLMs are implicitly encoded as weights of the neural networks. Extracting human-readable rules from NLMs would be a meaningful future direction.</p>
<h2>ACKNOWLEDGEMENTS</h2>
<p>We thank Rishabh Singh, Thomas Walsh, the area chair, and anonymous reviewers for their insightful comments.</p>
<h1>REFERENCES</h1>
<p>Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In CVPR, 2016.</p>
<p>Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy sketches. In ICML, 2017.</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In $I C L R, 2015$.</p>
<p>Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In ICML, 2009.</p>
<p>Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In $I C L R, 2014$.</p>
<p>Rudy R Bunel, Matthew Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Leveraging grammar and reinforcement learning for neural program synthesis. In $I C L R, 2018$.</p>
<p>Jonathon Cai, Richard Shin, and Dawn Song. Making neural programming architectures generalize via recursion. In $I C L R, 2017$.</p>
<p>Andres Campero, Aldo Pareja, Tim Klinger, Josh Tenenbaum, and Sebastian Riedel. Logical rule induction and theory learning using neural theorem proving. arXiv:1809.02193, 2018.</p>
<p>Xinyun Chen, Chang Liu, and Dawn Song. Towards synthesizing complex programs from inputoutput examples. In $I C L R, 2018$.</p>
<p>Wang-Zhou Dai, Qiu-Ling Xu, Yang Yu, and Zhi-Hua Zhou. Tunneling neural perception and logic reasoning through abductive learning. arXiv:1802.01173, 2018.</p>
<p>Evgeny Dantsin, Thomas Eiter, Georg Gottlob, and Andrei Voronkov. Complexity and expressive power of logic programming. ACM CSUR, 33:374-425, 2001.</p>
<p>Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In NIPS, 2016.</p>
<p>Misha Denil, Sergio Gómez Colmenarejo, Serkan Cabi, David Saxton, and Nando de Freitas. Programmable agents. arXiv:1706.06383, 2017.</p>
<p>Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Robustfill: Neural program learning under noisy i/o. In ICML, 2017.</p>
<p>Carlos Diuk, Andre Cohen, and Michael L Littman. An object-oriented representation for efficient reinforcement learning. In ICML, 2008.</p>
<p>Richard Evans and Edward Grefenstette. Learning explanatory rules from noisy data. JAIR, 61:1-64, 2018.</p>
<p>Jerry Fodor and Brian P McLaughlin. Connectionism and the problem of systematicity: Why smolensky's solution doesn't work. Cognition, 35(2):183-204, 1990.</p>
<p>Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3-71, 1988.</p>
<p>Nir Friedman, Lise Getoor, Daphne Koller, and Avi Pfeffer. Learning probabilistic relational models. In IJCAI, 1999.</p>
<p>Marta Garnelo, Kai Arulkumaran, and Murray Shanahan. Towards deep symbolic reinforcement learning. arXiv:1609.05518, 2016.</p>
<p>Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In ICML, 2017.</p>
<p>Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv:1410.5401, 2014.</p>
<p>Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471, 2016.</p>
<p>Naresh Gupta and Dana S Nau. On the complexity of blocks-world planning. Artif. Intell., 56(2-3): 223-254, 1992.</p>
<p>Robert F Hadley. Systematicity in connectionist language learning. Mind Lang., 9(3):273-287, 1994.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.</p>
<p>Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Process., 29(6):82-97, 2012.</p>
<p>Alfred Horn. On sentences which are true of direct unions of algebras. J. Symbolic Logic, 16(1): $14-21,1951$.</p>
<p>Yuxiao Hu and Giuseppe De Giacomo. Generalized planning: Synthesizing plans that work for multiple environments. In IJCAI, 2011.</p>
<p>Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, 2017.</p>
<p>Peter A Jansen and Scott Watter. Strong systematicity through sensorimotor conceptual grounding: an unsupervised, developmental approach to connectionist sentence processing. Connect. Sci., 24 (1):25-55, 2012.</p>
<p>Sergio Jiménez, Javier Segovia-Aguas, and Anders Jonsson. A review of generalized planning. The Knowledge Engineering Review, 34, 2019.</p>
<p>Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. In ICLR, 2016.
Ken Kansky, Tom Silver, David A Mély, Mohamed Eldawy, Miguel Lázaro-Gredilla, Xinghua Lou, Nimrod Dorfman, Szymon Sidor, Scott Phoenix, and Dileep George. Schema networks: Zero-shot transfer with a generative causal model of intuitive physics. In ICML, 2017.</p>
<p>Kristian Kersting, Luc De Raedt, and Stefan Kramer. Interpreting bayesian logic programs. In AAAI-2000 Workshop on Learning Statistical Models from Relational Data, 2000.</p>
<p>Angelika Kimmig, Stephen Bach, Matthias Broecheler, Bert Huang, and Lise Getoor. A short introduction to probabilistic soft logic. In NIPS Workshop on Probabilistic Programming: Foundations and Applications, 2012.</p>
<p>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017.</p>
<p>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.</p>
<p>Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proc. IEEE, 86(11):2278-2324, 1998.</p>
<p>Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. In ICLR, 2016.</p>
<p>Dianhuan Lin, Eyal Dechter, Kevin Ellis, Joshua B Tenenbaum, and Stephen H Muggleton. Bias reformulation for one-shot function induction. In ECAI, 2014.</p>
<p>Marco Lippi and Paolo Frasconi. Prediction of protein $\beta$-residue contacts by markov logic networks with grounding-specific weights. Bioinformatics, 25(18):2326-2333, 2009.</p>
<p>David Maier and David S. Warren. Computing with Logic: Logic Programming with Prolog. Benjamin-Cummings Publishing Co., Inc., Redwood City, CA, USA, 1988.</p>
<p>Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt. Deepproblog: Neural probabilistic logic programming. In NeurIPS, 2018.</p>
<p>David Mascharka, Philip Tran, Ryan Soklaski, and Arjun Majumdar. Transparency by design: Closing the gap between performance and interpretability in visual reasoning. In CVPR, 2018.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 022015.</p>
<p>Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML, 2016.</p>
<p>Stephen Muggleton. Inductive logic programming. New Gener. Comput., 8(4):295-318, 1991.
Stephen Muggleton. Stochastic logic programs. Advances in Inductive Logic Programming, 32: 254-264, 1996.</p>
<p>Arvind Neelakantan, Quoc V Le, and Ilya Sutskever. Neural programmer: Inducing latent programs with gradient descent. In $I C L R, 2016$.</p>
<p>Nils J Nilsson. Principles of Artificial Intelligence. Springer Science \&amp; Business Media, 1982.
Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet Kohli. Neuro-symbolic program synthesis. In $I C L R, 2017$.</p>
<p>Hanna M Pasula, Luke S Zettlemoyer, and Leslie Pack Kaelbling. Learning symbolic models of stochastic domains. JAIR, 29:309-352, 2007.</p>
<p>Scott Reed and Nando De Freitas. Neural programmer-interpreters. In $I C L R, 2016$.
Matthew Richardson and Pedro Domingos. Markov logic networks. Mach. Learn., 62(1-2):107-136, 2006.</p>
<p>Tim Rocktäschel and Sebastian Riedel. End-to-end differentiable proving. In NIPS, 2017.
Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Tim Lillicrap. A simple neural network module for relational reasoning. In NIPS, 2017.</p>
<p>David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017.</p>
<p>Siddharth Srivastava, Neil Immerman, and Shlomo Zilberstein. A new representation and associated algorithms for generalized planning. Artif. Intell., 175(2):615-647, 2011.</p>
<p>Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In NIPS, 2015.</p>
<p>Shao-Hua Sun, Hyeonwoo Noh, Sriram Somasundaram, and Joseph Lim. Neural program synthesis from diverse demonstration videos. In ICML, 2018.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In NIPS, 2014.</p>
<p>Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.</p>
<p>Martijn Van Otterlo. The Logic of Adaptive Behavior. IOS Press, 2009.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.</p>
<p>Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In NIPS, 2015.
Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv:1502.05698, 2015.</p>
<p>Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229-256, 1992.</p>
<p>Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning algorithms. Connect. Sci., 3(3):241-268, 1991.</p>
<p>Jiajun Wu, Joshua B Tenenbaum, and Pushmeet Kohli. Neural scene de-rendering. In CVPR, 2017.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv:1609.08144, 2016.</p>
<p>Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical rules for knowledge base reasoning. In NIPS, 2017.</p>
<p>Wenyuan Zeng, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Incorporating relation paths in neural relation extraction. In EMNLP, 2017.</p>
<p>Yuke Zhu, Alireza Fathi, and Li Fei-Fei. Reasoning about Object Affordances in a Knowledge Base Representation. In ECCV, 2014.</p>
<h1>SUPPLEMENTARY MATERIAL</h1>
<p>This supplementary material is organized as follows. First, we provide more details for our training method and introduce the curriculum learning used for reinforcement learning tasks in Appendix A. Second, in Appendix B, we provide more implementation details and hyper-parameters of each task in Section 3. Next, we provide deferred discussion of NLM extensions in Appendix C. Besides, we give a proof of how NLMs could realize the the forward chaining of a set of logic rules defined in Horn cluases. See Appendix D for details. In Appendix E, We also provide a list of sample rules for the blocks world problem in order to exhibit the complexity of describing a strategies or policies. Finally, we also provide a minimal implementation of NLM in TensorFlow for reference at the end of the supplementary material (Appendix F).</p>
<h2>A Training Method and Curriculum Learning</h2>
<p>In this section, we provide hyper-parameter details of our training method and introduce the examguided curriculum learning used for reinforcement learning tasks. We also provide details of the data generation method.</p>
<h2>A. 1 Training method</h2>
<p>We optimize both NLM and MemNN with Adam (Kingma \&amp; Ba (2015)) and use a learning rate of $\alpha=0.005$.</p>
<p>For all supervised learning tasks (i.e. family tree and general graph tasks), we use Softmax-CrossEntropy as loss function and a training batch size of 4.</p>
<p>For reinforcement learning tasks (i.e. the blocks world, sorting and shortest path tasks), we use REINFORCE algorithm (Sutton \&amp; Barto (1998)) for optimization. Each training batch is composed of a single episode of play. Similar to A3C (Mnih et al. (2016)), we add policy entropy term in the objective function (proposed by Williams \&amp; Peng (1991)) to help exploration. The update function for parameters $\theta$ of policy $\pi$ is</p>
<p>$$
\Delta \theta=\alpha\left[v_{t} \nabla_{\theta} \log \pi\left(a_{t} \mid s_{t} ; \theta\right)+\beta \nabla_{\theta} H\left(\pi\left(s_{t} ; \theta\right)\right)\right]
$$</p>
<p>where $H$ is the entropy function, $s_{t}$ and $a_{t}$ are the state and action at time $t, v_{t}$ is the discounted reward starting from time $t$. The hyper-parameter $\beta$ is set according to different environments and learning stages depending on the demand of exploration.</p>
<p>In all environments, the agent receives a reward of value 1.0 when it completes the task within a limited number of steps (which is related to the number of objects). To encourage the agent to use as few moves as possible, we give a reward of -0.01 for each move. The reward discount factor $\gamma$ is 0.99 for all tasks.</p>
<p>Table 3: Hyper-parameters for reinforcement learning tasks. The meaning of the hyper-parameters could be found in Section A. 1 and Section A.2. For the Path environment, the step limit is set to the actual distance between the starting point and the targeting point, to encourage the agents to find the shortest path.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Range</th>
<th style="text-align: center;">Step <br> Limit</th>
<th style="text-align: center;">$\beta_{\text {init }}$</th>
<th style="text-align: center;">$\Omega$</th>
<th style="text-align: center;">Epochs</th>
<th style="text-align: center;">Train Epoch <br> Episodes</th>
<th style="text-align: center;">Evaluation <br> Episodes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sorting</td>
<td style="text-align: center;">$m \in[4,10]$</td>
<td style="text-align: center;">2 m</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">Path</td>
<td style="text-align: center;">$m \in[3,12]$</td>
<td style="text-align: center;">opt</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">3000</td>
</tr>
<tr>
<td style="text-align: left;">Blocks World</td>
<td style="text-align: center;">$m \in[2,12]$</td>
<td style="text-align: center;">$4 m$</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">3000</td>
</tr>
</tbody>
</table>
<h2>A. 2 CURRICULUM LEARNING GUIDED BY EXAMS AND FAILS</h2>
<p>Inspired by the education system of humans, we employ an exam-guided curriculum learning (Bengio et al., 2009) approach for training Neural Logic Machines. We heuristically label each training</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">Curriculum</span><span class="w"> </span><span class="nt">learning</span><span class="w"> </span><span class="nt">guided</span><span class="w"> </span><span class="nt">by</span><span class="w"> </span><span class="nt">exams</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">fails</span>
<span class="nt">Function</span><span class="w"> </span><span class="nt">train</span><span class="o">(</span><span class="nt">model</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">M</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">lessons</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">)</span><span class="w"> </span><span class="o">:</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">ell</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">i</span><span class="o">=</span><span class="nt">0</span><span class="o">,</span><span class="nt">1</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">cdots</span><span class="w"> </span><span class="err">\</span><span class="nt">ell</span><span class="err">\</span><span class="o">)</span><span class="p">.</span><span class="nc">max_epochs</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="nt">accuracy</span><span class="o">,</span><span class="w"> </span><span class="nt">pos</span><span class="o">,</span><span class="w"> </span><span class="nt">neg</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">evaluate</span><span class="w"> </span><span class="err">\</span><span class="o">((</span><span class="nt">M</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">ell</span><span class="o">)</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Take</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">exam</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">collect</span><span class="w"> </span><span class="nt">samples</span><span class="o">.</span>
<span class="w">            </span><span class="nt">if</span><span class="w"> </span><span class="nt">accuracy</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">ell</span><span class="err">\</span><span class="o">)</span><span class="p">.</span><span class="nc">threshold</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                </span><span class="nt">break</span><span class="o">;</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="nt">Enter</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">next</span><span class="w"> </span><span class="nt">lesson</span><span class="w"> </span><span class="nt">if</span><span class="w"> </span><span class="nt">pass</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">exam</span><span class="o">.</span>
<span class="w">            </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">j</span><span class="o">=</span><span class="nt">0</span><span class="o">,</span><span class="nt">1</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">cdots</span><span class="w"> </span><span class="nt">K</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">                </span><span class="nt">data</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">sim</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">balanced</span><span class="w"> </span><span class="nt">sampling</span><span class="w"> </span><span class="nt">from</span><span class="w"> </span><span class="nt">pos</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">neg</span><span class="o">;</span>
<span class="w">                </span><span class="nt">Optimize</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">M</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">data</span><span class="o">;</span>
</code></pre></div>

<p>instances with its complexity. Training instances are grouped by their complexity (as lessons). For example, in the game of BlocksWorld, we consider the number of blocks in a game instance as its complexity. During the training, we present the training instances to the model from lessons with increasing difficulty. We periodically test models' performance (as exams) on novel instances of the same complexity as the ones in its current lesson. The well-performed model (whose accuracy reaches a certain threshold) will pass the exam and advance to a harder lesson (of more complex training instances). The exam-guided curriculum learning exploits the previously gained knowledge to ease the learning of more complex instances. Moreover, the performance on the final exam reaches above a threshold indicates the graduation of models.</p>
<p>In our experiments, each lesson contains training instances of the same number of objects. For example, the first lesson in the blocks world contains all possible instances consisting of 2 blocks (in each world). The instances of the second lesson contain 3 blocks in each world. And in the last lesson (totally 11 lessons) there are 12 blocks in each world. We report the range of the curriculum in Table 3 for three RL tasks.</p>
<p>Another essential ingredient for the efficient training of NLMs is to record models' failure cases. Specifically, we keep track of two sets of training instances: positive and negative (meaning the agent achieves the task or not). For each presented instance of the exam, it is recollected into positive or negative sets depending on whether the agent achieves the task or not. All training samples are sampled from the positive set with probability $\Omega$ and from the negative set with probability $1-\Omega$. This balanced sampling strategy prevents models from getting stuck at sub-optimal solutions. Algorithm 1 illustrates the pseudo-code of the curriculum learning guided by exams and fails.</p>
<p>The evaluation process ("exam") randomly samples examples from 3 recent lessons. The agent goes through these examples and gets the success rate (the ratio of achieving the task) as its performance, which is used to decide whether the agent passes the exam by comparing to a lesson-depend threshold. As we want a perfect model, the threshold for passing the last lesson (the "final exam") is $100 \%$. We linearly decrease the threshold by $0.5 \%$ for each former lessons, to prevent over-fitting(e.g., the threshold of the first lesson in the blocks world is $95 \%$ ). After the "exam", the examples are collected into positive and negative pools according to the outcome (success or not). During the training, we use balanced sampling for choosing training instances from positive and negative pools with probability $\Omega$ from positive. The hyper-parameters $\Omega$, the number of epochs, the number of episodes in each training epoch and the number of episodes in one evaluation are shown in Table 3 for three RL tasks.</p>
<h1>B IMPLEMENTATION DETAILS AND HYPER-PARAMETERS</h1>
<p>This section provides more implementation details for the model and experiments, and summarizes the hyper-parameters used in experiments for our NLM and the baseline algorithm MemNN.</p>
<h2>B. 1 RESIDUAL CONNECTION.</h2>
<p>Analog to the residual link in (He et al., 2016; Huang et al., 2017), we add residual connections to our model. Specifically, for each layer illustrated in Figure 2, the base predicates (inputs) are</p>
<p>concatenated to the conclusive predicates (outputs) group-wisely. That is, input unary predicates are concatenated to the deduced unary predicates while input binary predicates are concatenated to the conclusive binary predicates.</p>
<h1>B. 2 HYPER-PARAMETERS FOR NLM</h1>
<p>Table 4 shows hyper-parameters used by NLM for different tasks. For all MLPs inside NLMs, we use no hidden layer, and the hidden dimension (i.e., the number of intermediate predicates) of each layer is set to 8 across all our experiments. In supervised learning tasks, a model is called "graduated" if its training loss is below a threshold depending on the task (usually 1e-6). In reinforcement learning tasks, an agent is called "graduated" if it can pass the final exam, i.e., get $100 \%$ success rate on the evaluation process of the last lesson.</p>
<p>We note that in the randomly generated cases, the number of maternal great uncle (IsMGUncle) relation is relatively small. This makes the learning of this relation hard and results in a graduation ratio of only $20 \%$. If we increase the maximum number of people in training examples to 30 , the graduation ratio will grow to $50 \%$.</p>
<p>Table 4: Hyper-parameters for Neural Logic Machines. The definition of depth and breadth are illustrated in Figure 2. "Res." refers to the use of residual links. "Grad." refers to the ratio of successful graduation in 10 runs with different random seeds, which partially indicates the difficulty of the task. "Num. Examples/Episodes" means the maximum number of examples/episodes used to train the model in supervised learning and reinforcement learning cases.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Depth</th>
<th style="text-align: center;">Breath</th>
<th style="text-align: center;">Res.</th>
<th style="text-align: center;">Grad.</th>
<th style="text-align: center;">Num. Examples/Episodes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Family Tree</td>
<td style="text-align: center;">HasFather</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">50,000 examples</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HasSister</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">50,000 examples</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IsGrandparent</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">100,000 examples</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IsUncle</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">100,000 examples</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IsMGUncle</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">200,000 examples</td>
</tr>
<tr>
<td style="text-align: center;">General Graph</td>
<td style="text-align: center;">AdajacentToRed</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">100,000 examples</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4-Connectivity</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">50,000 examples</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">6-Connectivity</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">50,000 examples</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1-OutDegree</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">50,000 examples</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2-OutDegree</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">100,000 examples</td>
</tr>
<tr>
<td style="text-align: center;">General <br> Algorithm</td>
<td style="text-align: center;">Sorting</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">1,000 episodes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Path</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">24,000 episodes</td>
</tr>
<tr>
<td style="text-align: center;">BlocksWorld</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$40 \%$</td>
<td style="text-align: center;">50,000 episodes</td>
</tr>
</tbody>
</table>
<h2>B. 3 HYPER-PARAMETERS FOR MEMNN</h2>
<p>We set the number of iters/episodes used for baseline algorithms to be same as NLM. For the memory networks, each pre-condition in the memory is embedded into a key space and a value space. The dimensions of the spaces are 16 and 32 respectively. The hidden size of the LSTM in MemNN is 64. The number of queries is set to be 4 across all tasks (except that the Sorting task uses 1 query only). Empirically, we search for the optimal hyper-parameters but find that they have little effect on the performance.</p>
<h2>B. 4 DATA GENERATION</h2>
<p>We use random generation to generate training and testing data. more details and specific parameters used to generate the data could be found in our open source code.</p>
<p>In family tree tasks, we mimic the process of families growing using a timeline. For each newly created person, we randomly sample the gender and parents (could be none, indicating not included in the family tree) of the person. We also maintain lists of singles of each gender, and randomly pick two from each list to be married (each time when a person was created). We randomly permute the order of people.</p>
<p>In general graph tasks (include Path), We adopt the generation method from Graves et al. (2016), which samples $m$ nodes on a unit square, and the out-degree $k_{i}$ of each node is sampled. Then each node connects to $k_{i}$ nearest nodes on the unit square. In undirected graph cases, all generated edges are regarded as undirected edges.</p>
<p>In Sorting, we randomly generate permutations to be sorted in ascending order.
In Blocks World, We maintain a list of placeable objects (the ground included). Each newly created block places on one randomly selected placeable object. Then we randomly shuffle the $i d$ of the blocks.</p>
<h1>B. 5 Blocks World</h1>
<p>In the blocks world environment, to better aid the reinforcement learning process, we train the agent on an auxiliary task, which is to predict the validity or effect of the actions. This task is trained by supervised learning using cross-entropy loss. The overall loss is a summation of cross-entropy loss (with a weight of 0.1 ) and the REINFORCE loss.</p>
<p>We did not choose the Move to be taken directly based on the relational predicates at the last layer of NLM. Instead, we manually concatenate the object representation from the current and the target configuration, which share the same object ID. Then for each pair of objects, their relational representation is constructed by the concatenation of their own object representation. An extra fully-connected layer is applied to the relational representation, followed by a Softmax layer over all pairs of objects. We choose an action based on the Softmax score.</p>
<h2>B. 6 Accuracy DISCUSSION</h2>
<p>We cannot directly prove the accuracy of NLM by looking at the induced rules as in traditional ILP systems. Alternatively, we take an empirical way to estimate its accuracy by sampling testing examples. Throughout the experiments section, all accuracy statistics are reported in 1000 random generated data.</p>
<p>To show the confidence of this result, we test a specific trained model of Blocks World task with 100,000 samples. We get no fail cases in the testing. According to the multiplicative form of Chernoff Bound ${ }^{6}$, We are $99.7 \%$ confident that the accuracy is at least $99.98 \%$.</p>
<h2>C Neural Logic Machines (NLM) EXTENSIONS</h2>
<p>Reasoning over noisy input: integration with neural perception. Recall that NLM is fully differentiable. Besides taking logic pre-conditions (binary values) as input, the input properties or relations can be derived from other neural architectures (e.g., CNNs). As a preliminary example, we replace the input properties of nodes with images from the MNIST dataset. A convolutional neural network (CNN) is applied to the input extracting multiple features for future reasoning. CNN and NLM can be optimized jointly. This enables reasoning over noisy input.</p>
<p>We modify the AdjacentToRed task in general graph reasoning to AdjacentToNumber0. In detail, each node has a visual input from the MNIST dataset indicating its number. We say AdjacentToNumber $0(x)$ if and only if a node $x$ is adjacent to another node with number 0 . We use LeNet LeCun et al. (1998) to extract visual features for recognizing the number of each node. The output of LeNet for each node is a vector of length 10, with sigmoid activation.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We follow the train-test split from the original MNIST dataset. The joint model is trained on 100,000 training examples $(m=10)$ and gets $99.4 \%$ accuracy on 1000 testing examples $(m=50)$. Note that the LeNet modules are optimized jointly with the reasoning about AdjacentToNumber0.</p>
<h1>D Realization of Horn Clause</h1>
<p>In this section, we show that NLM can realize a partial set of Horn clauses (Horn, 1951) in first-order logic (FOL), up to the limit of the NLM's depth and breadth. In NLMs, we consider only finite cases. Thus, there should not exist cyclic references of predicates among rules. The extension to support cyclic references is left as a future work. Throughout the proof, we always assume the depth, breadth and number of predicates of NLM are flexible and large enough to realize the demanding rules.</p>
<p>Here, we only prove the realization of a definite clause, i.e., a Horn clause with exactly one positive literal and a non-zero number of negative literals in FOL [7]. It can be written in the implication form is $\hat{p} \leftarrow p_{1} \wedge p_{2} \wedge \cdots \wedge p_{k}$ (variables as arguments are implicitly universally quantified), where $\hat{p}$ is called the head predicate and $p_{1}, \ldots, p_{k}$ are called body predicates. We group the variables appearing in the rule into three subsets: (1) variables that only appear in the head predicate, (2) variables that appear in the body predicates, and (3) variables that appear in both head and body predicates.</p>
<p>Consider as an example a chain-like rule: $\forall x_{1} \forall x_{2} \forall x_{3} \forall x_{4} \quad \hat{p}\left(x_{1}, x_{3}, x_{4}\right) \leftarrow p_{1}\left(x_{1}, x_{2}\right) \wedge p_{2}\left(x_{2}, x_{3}\right)$. We rewrite the rule by classifying the variables:</p>
<p>$$
\forall x_{4}\left(\forall x_{1} \forall x_{3}\left(\hat{p}\left(x_{1}, x_{3}, x_{4}\right) \leftarrow \exists x_{2} p_{1}\left(x_{1}, x_{2}\right) \wedge p_{2}\left(x_{2}, x_{3}\right)\right)\right)
$$</p>
<p>That is, we move all variables that ony appear in body predicates to the right-hand side, and extract out all variables that only appear in the head predicate. We show how we can compositionally combines the computation units in NLMs to realize this rule, in the following 4 steps:</p>
<ol>
<li>We first align the arity of the body predicates to include all variables that appear in at least one of the body predicates (including variables of set 2 and set 3 ). This could be done by a sequence of Expand operations (Eq. 3). In this example, we will create helper predicates to make the right-hand side of the rule as</li>
</ol>
<p>$$
\exists x_{2} p_{1}^{\prime}\left(x_{1}, x_{2}, x_{3}\right) \wedge p_{2}^{\prime}\left(x_{2}, x_{3}, x_{1}\right)
$$</p>
<p>where $p_{1}^{\prime}\left(x_{1}, x_{2}, x_{3}\right) \triangleq p_{1}\left(x_{1}, x_{2}\right)$ and $p_{2}^{\prime}\left(x_{2}, x_{3}, x_{1}\right) \triangleq p_{2}\left(x_{2}, x_{3}\right)$.
2. We use neural boolean logic (Eq. 1) to realize the boolean formula inside all quantification symbols. Moreover, we use the Permute operation to transpose the tensor representation so that all variables being quantified on the right-hand side appear as the last several variables in the derived predicate $p^{\prime}$. Overall, we will derive another helper predicates,</p>
<p>$$
p^{\prime}\left(x_{1}, x_{3}, x_{2}\right) \triangleq p_{1}^{\prime}\left(x_{1}, x_{2}, x_{3}\right) \wedge p_{2}^{\prime}\left(x_{2}, x_{3}, x_{1}\right)
$$</p>
<ol>
<li>We use the Reduce operation to add quantifiers to the right-hand side (i.e., to the $p^{\prime}$ predicate). We will get:</li>
</ol>
<p>$$
p^{\prime \prime}\left(x_{1}, x_{3}\right) \triangleq \exists x_{2} p^{\prime}\left(x_{1}, x_{3}, x_{2}\right)=\exists x_{2} p_{1}^{\prime}\left(x_{1}, x_{2}, x_{3}\right) \wedge p_{2}^{\prime}\left(x_{2}, x_{3}, x_{1}\right)
$$</p>
<ol>
<li>Finally, we use the Expand operation (Eq. 3] to add variables that only appear in the head predicate to the derived predicate:</li>
</ol>
<p>$$
\hat{p}\left(x_{1}, x_{3}, x_{4}\right) \triangleq p^{\prime \prime}\left(x_{1}, x_{3}\right)
$$</p>
<p>Note that, all variables appeared in the head predicate are implicitly universally quantified. This is consistent with our setting, since all rules in NLMs are lifted.</p>
<p>Overall, a symbolic rule written as a Horn clause can be realized by NLMs as a computation flow which starts from multiple expansions followed by a neural boolean rule and multiple reductions, and ends with a set of expansions.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Next, we show that the forward propagation of NLMs realizes the forward chaining of a set of Horn clauses. Following the notation in Evans \&amp; Grefenstette (2018), the forward chaining starts from a set of initial facts, which are essentially the grounding of base predicates. The forward chaining process sequentially applies rules over the fact set, and concludes new facts. In NLM, we represent facts as the $\mathcal{U}$-grounding of predicates.</p>
<p>If we consider a set of rules that does not have recursive references, all rules can be sorted in an topological order $\mathcal{R}=\left(r_{1}, r_{2}, \ldots, r_{k}\right)$. We only allow references of $r_{i}$ from $r_{j}$, where $i&lt;j$. Without loss of generality, we assume that the grounding of $r_{k}$ is of interest. Given the topologically resolved set of rules $\mathcal{R}$, we build a set of NLMs where each NLM realizes a specific rule $r_{i}$. By stacking the NLMs sequentially, we can conclude $r_{k}$. As a side note, for multiple rules referring to the same head predicate $\hat{p}$, they implicitly indicate the logical disjunction $(\vee)$ of the rules. We can rename these head predicates as $\hat{p}<em 2="2">{1}, \hat{p}</em>$ 's.}, \cdots$, and use an extra NLM to implement the logical disjunction of all $\hat{p}_{i</p>
<h1>E Sample Blocks World Rules</h1>
<p>This example shows a complex reasoning in the seemingly simple Blocks World domain, which can be solved by our NLMs but requires great efforts of create manual rules by human experts in contrast.</p>
<p>Suppose we are interested in knowing whether a block should be moved in order to reach the target configuration. Here, a block should be moved if (1) it is moveable; and (2) there is at least one block below it that does not match the target configuration. Call the desired predicate "ShouldMove(x)".</p>
<p>Input Relations. (Specified in the last paragraph of Section 3.4) SameWorldID, SmallerWorldID, LargerWorldID; SameID, SmallerID, LargerID; Left, SameX, Right, Below, SameY, Above. The relations are given on all pairs of objects across both worlds.</p>
<p>Here is one way to produce the desired predicate by defining several helper predicates, designed by "human experts":</p>
<ol>
<li>IsGround(x) $\leftarrow \forall$ y Above(y, x)</li>
<li>SameXAbove(x, y) $\leftarrow$ SameWorldID(x, y) $\wedge$ SameX(x, y) $\wedge$ Above(x, y)</li>
<li>Clear(x) $\leftarrow \forall$ y SameXAbove(y, x)</li>
<li>Moveable(x) $\leftarrow$ Clear(x) $\wedge \neg$ IsGround(x)</li>
<li>InitialWorld(x) $\leftarrow \forall$ y $\neg$ SmallerWorldID(y, x)</li>
<li>Match(x, y) $\leftarrow \neg$ SameWorldID(x, y) $\wedge$ SameID(x, y) $\wedge$ SameX(x, y) $\wedge$ SameY(x, y)</li>
<li>Matched(x) $\leftarrow \exists$ y Match(x, y)</li>
<li>HaveUnmatchedBelow(x) $\leftarrow \exists$ y SameXAbove(x, y) $\wedge \neg$ Matched(y)</li>
<li>ShouldMove(x) $\leftarrow$ InitialWorld(x) $\wedge$ Moveable(x) $\wedge$ HaveUnmatchedBelow(x)</li>
</ol>
<p>We can also write the logic forms in one line: ShouldMove(x) $\leftarrow(\forall$ y $\neg$ SmallerWorldID(y, x)) $\wedge(\forall$ y $\neg$ (SameWorldID(y, x) $\wedge$ SameX(y, x) $\wedge$ Above(y, x))) $\wedge \neg(\forall$ y Above(y, x)) $\wedge((\exists$ y SameWorldID(x, y) $\wedge$ SameX(x, y) $\wedge$ Above(x, y)) $\wedge \neg(\exists$ z SameWorldID(y, z) $\wedge$ SameID(y, z) $\wedge$ SameX(y, z) $\wedge$ SameY(y, z)) ).</p>
<p>Note that this is only a part of logic rules needed to complete the Blocks World challenge. The learner also needs to figure out where should the block be moved onto. The proposed NLM can learn policies that solve the Blocks World from the sparse reward signal indicating only whether the agent has finished the game. More importantly, the learned policy generalizes well to larger instances (consisting more blocks).</p>
<h2>F Implement NLM in TensorFlow</h2>
<p>The following python code contains a minimal implementation for one Neural Logic Machines layer with breadth equals 3 in TensorFlow. The neural_logic_layer_breath3 is the main function. The syntax is highlighted and is best viewed in color.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">itertools</span><span class="w"> </span><span class="kn">import</span> <span class="n">permutations</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.layers</span><span class="w"> </span><span class="kn">import</span> <span class="n">dense</span>
<span class="k">def</span><span class="w"> </span><span class="nf">expand</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Expands input at its second last dimension (e.g., [B, ...,</span>
<span class="sd">        ~ Ni, Nj] to [B, ..., Ni, M, Nj]) by replicating tensors.&quot;&quot;&quot;</span>
    <span class="n">ndims</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">ndims</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">multiplies</span> <span class="o">=</span> <span class="p">[</span><span class="n">M</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">ndims</span> <span class="o">-</span> <span class="mi">2</span> <span class="k">else</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndims</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">multiples</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">reduce</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reduces max and min at the second last dimension, except for</span>
<span class="sd">        ~ diagonal elements.&quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">_reduce_mask</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">M</span><span class="p">)[</span><span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="nb">input</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="nb">input</span> <span class="o">*</span> <span class="n">mask</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mask</span><span class="p">),</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
    <span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">neural_logic</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;An MLP layer applied on permutations of the input.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">dense</span><span class="p">(</span><span class="n">_input_permutations</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">hidden_dim</span><span class="p">,</span>
        <span class="o">~</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">neural_logic_layer_breath3</span><span class="p">(</span><span class="n">input0</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">input3</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
    <span class="o">~</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">residual</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A neural logic layer with breath 3.</span>
<span class="sd">    Args:</span>
<span class="sd">        input0: float Tensor of shape [B, hidden_dim], nullary</span>
<span class="sd">    ~ predicates.</span>
<span class="sd">        input1: float Tensor of shape [B, M, hidden_dim], unary</span>
<span class="sd">    ~ predicates.</span>
<span class="sd">        input2: float Tensor of shape [B, M, M, hidden_dim], binary</span>
<span class="sd">    ~ predicates.</span>
<span class="sd">        input3: float Tensor of shape [B, M, M, M, hidden_dim],</span>
<span class="sd">    ~ tenary predicates.</span>
<span class="sd">        M: int, number of objects.</span>
<span class="sd">        hidden_dim: int, hidden dimension.</span>
<span class="sd">        residual: boolean, use the residual link or not.</span>
<span class="sd">    Returns:</span>
<span class="sd">        4 float Tensors, output nullary, unary, binary tenary</span>
<span class="sd">    ~ predicates respectively.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">agg0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">input0</span><span class="p">,</span> <span class="n">reduce</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">M</span><span class="p">)],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">agg1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">input1</span><span class="p">,</span> <span class="n">expand</span><span class="p">(</span><span class="n">input0</span><span class="p">,</span> <span class="n">M</span><span class="p">),</span> <span class="n">reduce</span><span class="p">(</span><span class="n">input2</span><span class="p">,</span>
        <span class="o">~</span> <span class="n">M</span><span class="p">)],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">agg2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">input2</span><span class="p">,</span> <span class="n">expand</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">M</span><span class="p">),</span> <span class="n">reduce</span><span class="p">(</span><span class="n">input3</span><span class="p">,</span>
        <span class="o">~</span> <span class="n">M</span><span class="p">)],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">agg3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">input3</span><span class="p">,</span> <span class="n">expand</span><span class="p">(</span><span class="n">input2</span><span class="p">,</span> <span class="n">M</span><span class="p">)],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">neural_logic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="n">agg0</span><span class="p">,</span> <span class="n">agg1</span><span class="p">,</span>
        <span class="o">~</span> <span class="n">agg2</span><span class="p">,</span> <span class="n">agg3</span><span class="p">]]</span>
    <span class="k">if</span> <span class="n">residual</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span>
            <span class="o">~</span> <span class="p">[</span><span class="n">input0</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">input3</span><span class="p">])]</span>
    <span class="k">return</span> <span class="n">outputs</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_reduce_mask</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ As for other types of Horn clauses: facts are realized as the tensor representations of predicates, while we implicitly views the output of NLMs as the goal clauses.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
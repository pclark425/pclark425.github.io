<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-633 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-633</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-633</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-71c7104eaed93497824cf197949c77e7d6cb36d3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/71c7104eaed93497824cf197949c77e7d6cb36d3" target="_blank">PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> PullNet is described, an integrated framework for learning what to retrieve and reasoning with this heterogeneous information to find the best answer in an open-domain question answering setting.</p>
                <p><strong>Paper Abstract:</strong> We consider open-domain question answering (QA) where answers are drawn from either a corpus, a knowledge base (KB), or a combination of both of these. We focus on a setting in which a corpus is supplemented with a large but incomplete KB, and on questions that require non-trivial (e.g., “multi-hop”) reasoning. We describe PullNet, an integrated framework for (1) learning what to retrieve and (2) reasoning with this heterogeneous information to find the best answer. PullNet uses an iterative process to construct a question-specific subgraph that contains information relevant to the question. In each iteration, a graph convolutional network (graph CNN) is used to identify subgraph nodes that should be expanded using retrieval (or “pull”) operations on the corpus and/or KB. After the subgraph is complete, another graph CNN is used to extract the answer from the subgraph. This retrieve-and-reason process allows us to answer multi-hop questions using large KBs and corpora. PullNet is weakly supervised, requiring question-answer pairs but not gold inference paths. Experimentally PullNet improves over the prior state-of-the art, and in the setting where a corpus is used with incomplete KB these improvements are often dramatic. PullNet is also often superior to prior systems in a KB-only setting or a text-only setting.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e633.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e633.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PullNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An integrated iterative retrieval+reasoning QA framework that constructs a question-specific heterogeneous subgraph (entities, KB facts, entity-linked sentences) by learning where to apply retrieval (“pull”) operations and then applies graph CNN reasoning to select answers; designed to combine an incomplete KB with a text corpus for multi-hop questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PullNet</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PullNet begins from the question and its linked entities and iteratively expands a heterogeneous question-subgraph using learned 'pull' operations that retrieve KB facts (ranked by a learned relation–question similarity) and entity-linked documents (IDF/Lucene). A graph convolutional network (GRAFTNet-style graph CNN) computes node representations over the heterogeneous graph (entity nodes, fact nodes, text nodes). A learned graph CNN classifier (used both for selecting which nodes to expand and for final answer selection) guides iterative retrieval. The update step injects newly retrieved facts, docs and entities into the subgraph; after T iterations a final graph-CNN based classify_answer selects the answer entity.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Knowledge base represented as relational triples (fact nodes of form (subject, relation, object)) and explicit entity nodes; entity linking maps text spans to KB entity nodes. The KB is used as a symbolic graph (triples and explicit edges) that forms part of the heterogeneous subgraph.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural components: graph convolutional networks (GRAFTNet-style graph CNN), LSTM encoder for the question and for encoding document nodes, learned relation embeddings for ranking facts, and classification heads (neural classifiers) that decide which entities to expand and which entity is the answer. Retrieval uses procedural components (Lucene IDF retrieval for text) and a learned similarity scorer (dot product of LSTM question encoding and relation embedding fed through sigmoid).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Early fusion into a single heterogeneous graph (entities, facts, and entity-linked text) with an iterative loop: a neural graph CNN computes node representations that inform a neural classifier which entities to expand; expansion invokes symbolic/imperative retrieval ops (KB fact lookup and Lucene-based text retrieval) to add explicit symbolic nodes; those nodes are linked into the same graph and re-consumed by the graph CNN. Training uses weak supervision (question–answer pairs) with teacher-forcing style augmentation (candidate intermediate entities found from shortest paths in the complete KB are used as positive retrieval targets). The integration is not purely differentiable end-to-end because retrieval and entity linking are external procedures, but the neural ranking/classification components are learned jointly within the iterative loop.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>1) High-recall, compact question-specific subgraphs: iterative, question-guided retrieval yields much smaller graphs with higher effective recall than single-shot heuristics. 2) Improved multi-hop reasoning: ability to follow long compositional inference chains by dynamically retrieving the necessary intermediate facts/documents. 3) Robustness to KB incompleteness: can back off to text retrieval and combine KB and text evidence to answer questions when KB triples are missing. 4) Traceability: the constructed subgraph provides inspectable retrieval and intermediate entities (partial interpretability). 5) Retrieval efficiency: fewer retrieved entities required for high recall compared to graph-sampling baselines (PageRank-Nibble).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Open-domain question answering requiring multi-hop reasoning, evaluated on MetaQA (1/2/3-hop), WebQuestionsSP, and Complex WebQuestions (benchmarks combining KB and text).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Multiple reported metrics (Hits@1): MetaQA 3-hop KB-only: 91.4% hits@1; MetaQA 3-hop 50% KB + Text: 85.2% hits@1; MetaQA 2-hop KB-only: 99.9% hits@1; WebQuestionsSP (incomplete KB + text): PullNet outperforms GRAFT-Net (e.g., Complex WebQuestions dev 50% KB + Text: 51.9% hits@1 for PullNet). Training/retrieval performance: PullNet epochs are ~3.5x slower than GRAFT-Net per epoch but surpasses its final accuracy after more wall-clock time.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>Text-only PullNet numbers reported: e.g., MetaQA 3-hop Text-only: 78.2% hits@1 (this is the system run using only corpus retrieval/components).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>PullNet generalizes better than single-source or single-shot retrieval baselines on multi-hop and incomplete-KB settings: the paper shows improved out-of-distribution robustness to missing KB facts (simulated by randomly dropping 50% of triples) when paired with text. It empirically provides stronger compositional/multi-hop generalization (especially for 3-hop questions) than prior early-fusion or single-shot retrieval methods.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Moderate interpretability: the explicit question subgraph (entities, facts, documents) and the sequence of iterative pulls form a humaninterpretable reasoning trace (retrieved facts and documents can be inspected). However, the internal decision functions (graph-CNN node scores, LSTM encodings) are neural and not fully transparent; retrieval steps (Lucene) and entity-linker are external, deterministic components that aid traceability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>1) Training and wall-clock time: interleaving retrieval with learning increases per-epoch time (reported ~3.5x slower per epoch than GRAFT-Net), requiring more compute to reach peak performance. 2) Reliance on external non-differentiable components (Lucene retrieval, entity linking) which can limit end-to-end optimization and can introduce brittle failures when entity linking or retrieval fails. 3) Weak supervision: training uses shortest-path heuristic labels from a complete KB; in settings where such heuristics are misleading or unavailable the weak labels may be imperfect. 4) Still dependent on quality of KB and corpus indexing; retrieval misses can cause failures. 5) Complexity: iterative process and heterogeneous graph increase system complexity and engineering cost.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>No formal mathematical theory is proposed; the approach is justified by an operational principle of early fusion and division of labor: learn a policy (via graph-CNN classifier) to decide where to retrieve (pull) and then perform neural reasoning over a fused symbolic/neural graph representation. The paper motivates complementarity between symbolic KB structure and neural representation-based reasoning, and frames training as weakly supervised learning to recover shortest-path intermediate entities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e633.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e633.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRAFT-Net</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GRAFT-Net: Graphs of Relations Among Facts and Text Networks (as presented in prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior early-fusion QA architecture that constructs a heterogeneous question-specific subgraph containing KB facts and entity-linked text and applies a specialized graph CNN (GRAFTNet) to propagate and combine representations for multi-hop reasoning over KB+text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Open domain question answering using early fusion of knowledge bases and text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GRAFT-Net</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>GRAFT-Net builds a heterogeneous graph with entity nodes, fact nodes, and document/text nodes (entity-linked sentences), encodes document nodes with LSTMs, and uses a graph convolutional network variant (with mechanisms to pass LSTM mention states into/out of document nodes) to propagate information and perform node classification to select answer entities. The graph is typically constructed with heuristics or a separate retrieval preprocessing step (single-shot retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Knowledge base triples represented explicitly as fact nodes connected to entity nodes (symbolic relational graph).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural graph convolutional networks (graph CNN), LSTM encoders for documents, neural classifiers for answer selection and node representation propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Early fusion: symbolic KB facts and neural text encodings are combined into a single heterogeneous graph; neural graph CNN propagates and integrates information across symbolic and textual nodes. Retrieval of subgraph is a pre-processing step (non-iterative) and is separate from the learned graph CNN reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables multi-hop reasoning that jointly uses KB structure and textual evidence; can answer questions requiring chains that traverse both KB triples and text mentions. Allows representation-passing between document mentions and entity nodes (improves combining text and KB signals).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Open-domain multi-hop QA on datasets combining KB and text (MetaQA, WebQuestionsSP, Complex WebQuestions).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Reported in this paper as a baseline: MetaQA 3-hop KB-only: 77.7% hits@1 (GRAFT-Net reimplementation baseline in table); various other dataset numbers shown in Tables (e.g., ComplexWebQ dev KB: 66.4% hits@1).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>Text-only GRAFT-Net numbers reported as text-only baseline: e.g., MetaQA 3-hop Text-only: 40.2% hits@1 (as reported in table).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Improves over late-fusion ensembling approaches by reasoning jointly on fused symbolic and textual nodes; however, single-shot retrieval preprocessing can limit recall for long multi-hop questions and therefore limit generalization to longer inference chains.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Moderate: the constructed subgraph (facts and documents) provides inspectable evidence and paths, but the internal graph-CNN computations are neural and not fully transparent.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Depends on a single-shot retrieval pre-processing step that can produce very large subgraphs or miss relevant evidence; heuristics used for retrieval can result in graphs that are much larger than needed or fail to include answers, limiting downstream reasoning and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Operational early-fusion principle: combine symbolic KB and text into a single heterogeneous graph and apply graph neural networks to propagate and combine information for QA. No formal proof-theoretic or symbolic-differentiable framework provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e633.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e633.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KV-Mem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Key-Value Memory Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural memory-augmented model that stores facts and text as key–value pairs in a memory and uses differentiable attention-based reading (multiple hops) to answer questions by retrieving and aggregating memory values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Key-value memory networks for directly reading documents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Key-Value Memory Networks (KV-Mem)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>KV-Mem encodes KB facts and text passages into key–value pairs; at inference a neural reader attends over keys conditioned on the question (possibly multiple hops) and aggregates corresponding values to produce an answer. In the context of this paper KV-Mem is used as a baseline for text-only, KB-only, and combined settings (after an initial retrieval step to limit memory).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>KB facts and textual content are encoded into key-value pairs derived from symbolic facts or from documents (surface forms mapped to values). The original KB triples are used to produce the key/value encodings (symbolic data encoded into memory).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural attention-based memory network architecture (key-value attention, multiple reasoning hops implemented by repeated attention and update steps).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Stores heterogeneous information (facts and text) in a unified neural memory structure; retrieval and reading are performed by attention mechanisms over the key space. In this paper, memory contents are produced after an external retrieval/preprocessing stage due to scale limits.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Can aggregate distributed evidence across memory slots and perform soft multi-step reasoning via multiple attention hops; flexible handling of heterogeneous evidence within the neural memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Open-domain QA (MetaQA/WikiMovies and other reading-comprehension-style tasks); used as baseline on MetaQA in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Reported baseline numbers in this paper (Hits@1): MetaQA 3-hop KB-only: 48.9% hits@1; MetaQA 3-hop Text-only: 19.5% hits@1; other cells in Table 3 provide more context-dependent numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>KV-Mem is itself a neural (imperative) system; text-only numbers above reflect its neural-only performance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Limited by memory size and the need to pre-retrieve a limited set of facts/documents; single-shot retrieval and fixed-memory capacity limit ability to follow long multi-hop inference chains on very large KBs/corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Lower interpretability: reasoning is distributed across attention weights in the neural memory and is less directly traceable than explicit symbolic subgraphs, though attention patterns can provide some signal.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Memory size constraints force expensive and lossy pre-retrieval; single-shot retrieval makes multi-hop questions difficult; performance degrades on longer multi-hop tasks compared to graph-based early fusion approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Neural memory-augmented computation: store information in differentiable memory and perform attention-driven read operations (no explicit symbolic reasoning layer).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e633.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e633.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Symbolic Machines (NSM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural–symbolic approach that uses a neural controller to generate programs (symbolic queries or logical forms) executed by a symbolic executor; trained with weak supervision for semantic parsing on KB QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural Symbolic Machines (NSM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NSM combines a neural sequence model (usually an encoder–decoder) that generates discrete programs (logical forms) conditioned on natural language, with a symbolic executor that executes the generated program against a KB. Learning is done with weak supervision (question-answer pairs) using REINFORCE or other weakly supervised optimization to encourage programs that yield correct answers.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic executor operating over a structured KB (logical queries / program operations executed on Freebase or similar KBs).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural sequence models (controllers) that generate programs/logical forms; learning via neural optimization (policy gradient / weak supervision).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular neural-symbolic pipeline: neural model proposes discrete symbolic programs which are executed by a separate symbolic module. Training aligns neural proposals with symbolic execution outcomes under weak supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables combining neural generalization in parsing natural language with exact symbolic execution on a KB (precise semantics), producing interpretable programs and high semantic fidelity when program generation is correct.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Semantic parsing and KB question answering (e.g., WebQuestionsSP); cited in the paper with performance reports (NSM reported 69.0 F1 on WebQuestionsSP in referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Cited performance on WebQuestionsSP from referenced work: 69.0 F1 (metric reported in original NSM paper; reported in this paper's related work table).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Combines neural ability to generalize over language with symbolic executor's exactness; can generalize to programmatic compositions if the neural controller learns to synthesize appropriate program fragments, but suffers when weak supervision is sparse.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability for executed programs: discrete programs/logical forms are explicit and human-readable as answers-execution traces, providing strong explainability compared to fully neural readers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Training with weak supervision is challenging (credit assignment); neural program generation can be brittle and search over program space can be expensive; scaling to very large KBs and to integrating noisy textual evidence is non-trivial.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Neural–symbolic hybridization: separation of concerns where neural models handle language-to-program mapping and symbolic executors handle exact logical semantics; no unified differentiable semantics across the discrete execution was proposed in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e633.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e633.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SplitQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SplitQA (as described in The web as a knowledge-base for answering complex questions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-step text-centered QA approach that decomposes complex natural-language questions into simpler subquestions, issues retrieval for each subquestion (web search/snippets), and applies reading-comprehension models to retrieved snippets to produce answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The web as a knowledge-base for answering complex questions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SplitQA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SplitQA decomposes a complex question into sub-questions, uses web search to retrieve relevant snippets for each sub-question (iterative retrieval per subquery), and applies a reading comprehension model to extract answers which are then composed into the final answer. It is text-only (does not integrate a structured KB) and uses an external search engine as its retrieval backend.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>None (text-based); not designed to operate over a KB or symbolic triples.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Procedural decomposition module (question decomposition), web retrieval via search engine, and neural reading comprehension models for answer extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Decomposition + iterative retrieval + neural reading comprehension pipeline (modular, procedural integration).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Can leverage large web corpora and search engine snippets to answer complex compositional questions by breaking them into simpler steps; can sometimes outperform KB-only approaches on certain datasets when web snippets capture the necessary evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Complex WebQuestions (multi-hop question answering over web/snippets); compared in the paper discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>In the discussion the paper notes SplitQA achieves higher Hits@1 on Complex WebQuestions with its snippet corpus (paper reports that PullNet is 4.5% lower Hits@1 than SplitQA on that specific snippet corpus), but SplitQA relies on non-open components and is text-only.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Effective when decomposition and search retrieve required intermediate evidence; generalization depends heavily on quality of decomposition and availability of web snippets; not designed to combine with KB reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Moderate interpretability via explicit decomposition steps and retrieved snippets which can be inspected; internal neural reader remains less interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Relies on an external web search engine and curated snippet corpus (non-reproducible components), biased toward retrieval setup; not designed to use KBs, so cannot exploit structured KB facts directly.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Procedural decomposition and retrieve-then-read paradigm; no formal hybrid symbolic/neural theory in the cited discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e633.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e633.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AQQU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AQQU (AQQU and successors for KB+text QA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic/structured approach that maps natural language to KB query graphs and uses KB facts (and sometimes text constraints) to answer; historically focused on mapping questions to Freebase-style structured queries with optional text-based constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>More accurate question answering on freebase</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AQQU (and successors)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AQQU is an approach that translates questions into structured KB query graphs (symbolic logical forms) and applies KB lookup to obtain candidate answers, optionally applying textual restrictions derived from question phrasing. It is primarily a symbolic/semantic-parsing based pipeline (rule and grammar driven), not trained as an end-to-end neural model in its original instantiation.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Structured KB query graphs and semantic parsing to map natural language into symbolic queries executed against Freebase or similar KBs.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Procedural pipeline elements for candidate generation and text-based constraint handling; not fundamentally neural in original work (later successors may incorporate learned components).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Primarily symbolic semantic parsing executed against a KB, with heuristics or additional modules to incorporate text-based constraints; in literature successors have tried combining with learned modules.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>High precision when questions map cleanly to KB query patterns; provides explicit, interpretable logical forms and exact execution semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Simple KB question answering benchmarks (e.g., SimpleWebQuestions) where questions correspond to KB relations with optional textual constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Not evaluated in this paper as an experimental baseline; cited as a related approach and historically influential.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Strong when questions follow KB-compatible templates; limited for opentextual variation and multi-hop questions without extensions.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability via explicit logical forms and query graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Hard to scale to multi-hop compositional questions and to incorporate diverse natural-language expressions; extending to noisy text evidence is non-trivial.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Symbolic semantic parsing and KB query generation; no unified differentiable hybridization described in the cited context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Open domain question answering using early fusion of knowledge bases and text <em>(Rating: 2)</em></li>
                <li>Key-value memory networks for directly reading documents <em>(Rating: 2)</em></li>
                <li>Neural symbolic machines: Learning semantic parsers on freebase with weak supervision <em>(Rating: 2)</em></li>
                <li>The web as a knowledge-base for answering complex questions <em>(Rating: 2)</em></li>
                <li>More accurate question answering on freebase <em>(Rating: 1)</em></li>
                <li>Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-633",
    "paper_id": "paper-71c7104eaed93497824cf197949c77e7d6cb36d3",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "PullNet",
            "name_full": "PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text",
            "brief_description": "An integrated iterative retrieval+reasoning QA framework that constructs a question-specific heterogeneous subgraph (entities, KB facts, entity-linked sentences) by learning where to apply retrieval (“pull”) operations and then applies graph CNN reasoning to select answers; designed to combine an incomplete KB with a text corpus for multi-hop questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "PullNet",
            "system_description": "PullNet begins from the question and its linked entities and iteratively expands a heterogeneous question-subgraph using learned 'pull' operations that retrieve KB facts (ranked by a learned relation–question similarity) and entity-linked documents (IDF/Lucene). A graph convolutional network (GRAFTNet-style graph CNN) computes node representations over the heterogeneous graph (entity nodes, fact nodes, text nodes). A learned graph CNN classifier (used both for selecting which nodes to expand and for final answer selection) guides iterative retrieval. The update step injects newly retrieved facts, docs and entities into the subgraph; after T iterations a final graph-CNN based classify_answer selects the answer entity.",
            "declarative_component": "Knowledge base represented as relational triples (fact nodes of form (subject, relation, object)) and explicit entity nodes; entity linking maps text spans to KB entity nodes. The KB is used as a symbolic graph (triples and explicit edges) that forms part of the heterogeneous subgraph.",
            "imperative_component": "Neural components: graph convolutional networks (GRAFTNet-style graph CNN), LSTM encoder for the question and for encoding document nodes, learned relation embeddings for ranking facts, and classification heads (neural classifiers) that decide which entities to expand and which entity is the answer. Retrieval uses procedural components (Lucene IDF retrieval for text) and a learned similarity scorer (dot product of LSTM question encoding and relation embedding fed through sigmoid).",
            "integration_method": "Early fusion into a single heterogeneous graph (entities, facts, and entity-linked text) with an iterative loop: a neural graph CNN computes node representations that inform a neural classifier which entities to expand; expansion invokes symbolic/imperative retrieval ops (KB fact lookup and Lucene-based text retrieval) to add explicit symbolic nodes; those nodes are linked into the same graph and re-consumed by the graph CNN. Training uses weak supervision (question–answer pairs) with teacher-forcing style augmentation (candidate intermediate entities found from shortest paths in the complete KB are used as positive retrieval targets). The integration is not purely differentiable end-to-end because retrieval and entity linking are external procedures, but the neural ranking/classification components are learned jointly within the iterative loop.",
            "emergent_properties": "1) High-recall, compact question-specific subgraphs: iterative, question-guided retrieval yields much smaller graphs with higher effective recall than single-shot heuristics. 2) Improved multi-hop reasoning: ability to follow long compositional inference chains by dynamically retrieving the necessary intermediate facts/documents. 3) Robustness to KB incompleteness: can back off to text retrieval and combine KB and text evidence to answer questions when KB triples are missing. 4) Traceability: the constructed subgraph provides inspectable retrieval and intermediate entities (partial interpretability). 5) Retrieval efficiency: fewer retrieved entities required for high recall compared to graph-sampling baselines (PageRank-Nibble).",
            "task_or_benchmark": "Open-domain question answering requiring multi-hop reasoning, evaluated on MetaQA (1/2/3-hop), WebQuestionsSP, and Complex WebQuestions (benchmarks combining KB and text).",
            "hybrid_performance": "Multiple reported metrics (Hits@1): MetaQA 3-hop KB-only: 91.4% hits@1; MetaQA 3-hop 50% KB + Text: 85.2% hits@1; MetaQA 2-hop KB-only: 99.9% hits@1; WebQuestionsSP (incomplete KB + text): PullNet outperforms GRAFT-Net (e.g., Complex WebQuestions dev 50% KB + Text: 51.9% hits@1 for PullNet). Training/retrieval performance: PullNet epochs are ~3.5x slower than GRAFT-Net per epoch but surpasses its final accuracy after more wall-clock time.",
            "declarative_only_performance": "null",
            "imperative_only_performance": "Text-only PullNet numbers reported: e.g., MetaQA 3-hop Text-only: 78.2% hits@1 (this is the system run using only corpus retrieval/components).",
            "has_comparative_results": true,
            "generalization_properties": "PullNet generalizes better than single-source or single-shot retrieval baselines on multi-hop and incomplete-KB settings: the paper shows improved out-of-distribution robustness to missing KB facts (simulated by randomly dropping 50% of triples) when paired with text. It empirically provides stronger compositional/multi-hop generalization (especially for 3-hop questions) than prior early-fusion or single-shot retrieval methods.",
            "interpretability_properties": "Moderate interpretability: the explicit question subgraph (entities, facts, documents) and the sequence of iterative pulls form a humaninterpretable reasoning trace (retrieved facts and documents can be inspected). However, the internal decision functions (graph-CNN node scores, LSTM encodings) are neural and not fully transparent; retrieval steps (Lucene) and entity-linker are external, deterministic components that aid traceability.",
            "limitations_or_failures": "1) Training and wall-clock time: interleaving retrieval with learning increases per-epoch time (reported ~3.5x slower per epoch than GRAFT-Net), requiring more compute to reach peak performance. 2) Reliance on external non-differentiable components (Lucene retrieval, entity linking) which can limit end-to-end optimization and can introduce brittle failures when entity linking or retrieval fails. 3) Weak supervision: training uses shortest-path heuristic labels from a complete KB; in settings where such heuristics are misleading or unavailable the weak labels may be imperfect. 4) Still dependent on quality of KB and corpus indexing; retrieval misses can cause failures. 5) Complexity: iterative process and heterogeneous graph increase system complexity and engineering cost.",
            "theoretical_framework": "No formal mathematical theory is proposed; the approach is justified by an operational principle of early fusion and division of labor: learn a policy (via graph-CNN classifier) to decide where to retrieve (pull) and then perform neural reasoning over a fused symbolic/neural graph representation. The paper motivates complementarity between symbolic KB structure and neural representation-based reasoning, and frames training as weakly supervised learning to recover shortest-path intermediate entities.",
            "uuid": "e633.0",
            "source_info": {
                "paper_title": "PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "GRAFT-Net",
            "name_full": "GRAFT-Net: Graphs of Relations Among Facts and Text Networks (as presented in prior work)",
            "brief_description": "A prior early-fusion QA architecture that constructs a heterogeneous question-specific subgraph containing KB facts and entity-linked text and applies a specialized graph CNN (GRAFTNet) to propagate and combine representations for multi-hop reasoning over KB+text.",
            "citation_title": "Open domain question answering using early fusion of knowledge bases and text",
            "mention_or_use": "use",
            "system_name": "GRAFT-Net",
            "system_description": "GRAFT-Net builds a heterogeneous graph with entity nodes, fact nodes, and document/text nodes (entity-linked sentences), encodes document nodes with LSTMs, and uses a graph convolutional network variant (with mechanisms to pass LSTM mention states into/out of document nodes) to propagate information and perform node classification to select answer entities. The graph is typically constructed with heuristics or a separate retrieval preprocessing step (single-shot retrieval).",
            "declarative_component": "Knowledge base triples represented explicitly as fact nodes connected to entity nodes (symbolic relational graph).",
            "imperative_component": "Neural graph convolutional networks (graph CNN), LSTM encoders for documents, neural classifiers for answer selection and node representation propagation.",
            "integration_method": "Early fusion: symbolic KB facts and neural text encodings are combined into a single heterogeneous graph; neural graph CNN propagates and integrates information across symbolic and textual nodes. Retrieval of subgraph is a pre-processing step (non-iterative) and is separate from the learned graph CNN reasoning.",
            "emergent_properties": "Enables multi-hop reasoning that jointly uses KB structure and textual evidence; can answer questions requiring chains that traverse both KB triples and text mentions. Allows representation-passing between document mentions and entity nodes (improves combining text and KB signals).",
            "task_or_benchmark": "Open-domain multi-hop QA on datasets combining KB and text (MetaQA, WebQuestionsSP, Complex WebQuestions).",
            "hybrid_performance": "Reported in this paper as a baseline: MetaQA 3-hop KB-only: 77.7% hits@1 (GRAFT-Net reimplementation baseline in table); various other dataset numbers shown in Tables (e.g., ComplexWebQ dev KB: 66.4% hits@1).",
            "declarative_only_performance": "null",
            "imperative_only_performance": "Text-only GRAFT-Net numbers reported as text-only baseline: e.g., MetaQA 3-hop Text-only: 40.2% hits@1 (as reported in table).",
            "has_comparative_results": true,
            "generalization_properties": "Improves over late-fusion ensembling approaches by reasoning jointly on fused symbolic and textual nodes; however, single-shot retrieval preprocessing can limit recall for long multi-hop questions and therefore limit generalization to longer inference chains.",
            "interpretability_properties": "Moderate: the constructed subgraph (facts and documents) provides inspectable evidence and paths, but the internal graph-CNN computations are neural and not fully transparent.",
            "limitations_or_failures": "Depends on a single-shot retrieval pre-processing step that can produce very large subgraphs or miss relevant evidence; heuristics used for retrieval can result in graphs that are much larger than needed or fail to include answers, limiting downstream reasoning and efficiency.",
            "theoretical_framework": "Operational early-fusion principle: combine symbolic KB and text into a single heterogeneous graph and apply graph neural networks to propagate and combine information for QA. No formal proof-theoretic or symbolic-differentiable framework provided.",
            "uuid": "e633.1",
            "source_info": {
                "paper_title": "PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "KV-Mem",
            "name_full": "Key-Value Memory Networks",
            "brief_description": "Neural memory-augmented model that stores facts and text as key–value pairs in a memory and uses differentiable attention-based reading (multiple hops) to answer questions by retrieving and aggregating memory values.",
            "citation_title": "Key-value memory networks for directly reading documents",
            "mention_or_use": "use",
            "system_name": "Key-Value Memory Networks (KV-Mem)",
            "system_description": "KV-Mem encodes KB facts and text passages into key–value pairs; at inference a neural reader attends over keys conditioned on the question (possibly multiple hops) and aggregates corresponding values to produce an answer. In the context of this paper KV-Mem is used as a baseline for text-only, KB-only, and combined settings (after an initial retrieval step to limit memory).",
            "declarative_component": "KB facts and textual content are encoded into key-value pairs derived from symbolic facts or from documents (surface forms mapped to values). The original KB triples are used to produce the key/value encodings (symbolic data encoded into memory).",
            "imperative_component": "Neural attention-based memory network architecture (key-value attention, multiple reasoning hops implemented by repeated attention and update steps).",
            "integration_method": "Stores heterogeneous information (facts and text) in a unified neural memory structure; retrieval and reading are performed by attention mechanisms over the key space. In this paper, memory contents are produced after an external retrieval/preprocessing stage due to scale limits.",
            "emergent_properties": "Can aggregate distributed evidence across memory slots and perform soft multi-step reasoning via multiple attention hops; flexible handling of heterogeneous evidence within the neural memory.",
            "task_or_benchmark": "Open-domain QA (MetaQA/WikiMovies and other reading-comprehension-style tasks); used as baseline on MetaQA in this paper.",
            "hybrid_performance": "Reported baseline numbers in this paper (Hits@1): MetaQA 3-hop KB-only: 48.9% hits@1; MetaQA 3-hop Text-only: 19.5% hits@1; other cells in Table 3 provide more context-dependent numbers.",
            "declarative_only_performance": "null",
            "imperative_only_performance": "KV-Mem is itself a neural (imperative) system; text-only numbers above reflect its neural-only performance.",
            "has_comparative_results": true,
            "generalization_properties": "Limited by memory size and the need to pre-retrieve a limited set of facts/documents; single-shot retrieval and fixed-memory capacity limit ability to follow long multi-hop inference chains on very large KBs/corpora.",
            "interpretability_properties": "Lower interpretability: reasoning is distributed across attention weights in the neural memory and is less directly traceable than explicit symbolic subgraphs, though attention patterns can provide some signal.",
            "limitations_or_failures": "Memory size constraints force expensive and lossy pre-retrieval; single-shot retrieval makes multi-hop questions difficult; performance degrades on longer multi-hop tasks compared to graph-based early fusion approaches.",
            "theoretical_framework": "Neural memory-augmented computation: store information in differentiable memory and perform attention-driven read operations (no explicit symbolic reasoning layer).",
            "uuid": "e633.2",
            "source_info": {
                "paper_title": "PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "NSM",
            "name_full": "Neural Symbolic Machines (NSM)",
            "brief_description": "A neural–symbolic approach that uses a neural controller to generate programs (symbolic queries or logical forms) executed by a symbolic executor; trained with weak supervision for semantic parsing on KB QA tasks.",
            "citation_title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision",
            "mention_or_use": "mention",
            "system_name": "Neural Symbolic Machines (NSM)",
            "system_description": "NSM combines a neural sequence model (usually an encoder–decoder) that generates discrete programs (logical forms) conditioned on natural language, with a symbolic executor that executes the generated program against a KB. Learning is done with weak supervision (question-answer pairs) using REINFORCE or other weakly supervised optimization to encourage programs that yield correct answers.",
            "declarative_component": "Symbolic executor operating over a structured KB (logical queries / program operations executed on Freebase or similar KBs).",
            "imperative_component": "Neural sequence models (controllers) that generate programs/logical forms; learning via neural optimization (policy gradient / weak supervision).",
            "integration_method": "Modular neural-symbolic pipeline: neural model proposes discrete symbolic programs which are executed by a separate symbolic module. Training aligns neural proposals with symbolic execution outcomes under weak supervision.",
            "emergent_properties": "Enables combining neural generalization in parsing natural language with exact symbolic execution on a KB (precise semantics), producing interpretable programs and high semantic fidelity when program generation is correct.",
            "task_or_benchmark": "Semantic parsing and KB question answering (e.g., WebQuestionsSP); cited in the paper with performance reports (NSM reported 69.0 F1 on WebQuestionsSP in referenced work).",
            "hybrid_performance": "Cited performance on WebQuestionsSP from referenced work: 69.0 F1 (metric reported in original NSM paper; reported in this paper's related work table).",
            "declarative_only_performance": "null",
            "imperative_only_performance": "null",
            "has_comparative_results": false,
            "generalization_properties": "Combines neural ability to generalize over language with symbolic executor's exactness; can generalize to programmatic compositions if the neural controller learns to synthesize appropriate program fragments, but suffers when weak supervision is sparse.",
            "interpretability_properties": "High interpretability for executed programs: discrete programs/logical forms are explicit and human-readable as answers-execution traces, providing strong explainability compared to fully neural readers.",
            "limitations_or_failures": "Training with weak supervision is challenging (credit assignment); neural program generation can be brittle and search over program space can be expensive; scaling to very large KBs and to integrating noisy textual evidence is non-trivial.",
            "theoretical_framework": "Neural–symbolic hybridization: separation of concerns where neural models handle language-to-program mapping and symbolic executors handle exact logical semantics; no unified differentiable semantics across the discrete execution was proposed in this paper's discussion.",
            "uuid": "e633.3",
            "source_info": {
                "paper_title": "PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "SplitQA",
            "name_full": "SplitQA (as described in The web as a knowledge-base for answering complex questions)",
            "brief_description": "A multi-step text-centered QA approach that decomposes complex natural-language questions into simpler subquestions, issues retrieval for each subquestion (web search/snippets), and applies reading-comprehension models to retrieved snippets to produce answers.",
            "citation_title": "The web as a knowledge-base for answering complex questions",
            "mention_or_use": "mention",
            "system_name": "SplitQA",
            "system_description": "SplitQA decomposes a complex question into sub-questions, uses web search to retrieve relevant snippets for each sub-question (iterative retrieval per subquery), and applies a reading comprehension model to extract answers which are then composed into the final answer. It is text-only (does not integrate a structured KB) and uses an external search engine as its retrieval backend.",
            "declarative_component": "None (text-based); not designed to operate over a KB or symbolic triples.",
            "imperative_component": "Procedural decomposition module (question decomposition), web retrieval via search engine, and neural reading comprehension models for answer extraction.",
            "integration_method": "Decomposition + iterative retrieval + neural reading comprehension pipeline (modular, procedural integration).",
            "emergent_properties": "Can leverage large web corpora and search engine snippets to answer complex compositional questions by breaking them into simpler steps; can sometimes outperform KB-only approaches on certain datasets when web snippets capture the necessary evidence.",
            "task_or_benchmark": "Complex WebQuestions (multi-hop question answering over web/snippets); compared in the paper discussion.",
            "hybrid_performance": "In the discussion the paper notes SplitQA achieves higher Hits@1 on Complex WebQuestions with its snippet corpus (paper reports that PullNet is 4.5% lower Hits@1 than SplitQA on that specific snippet corpus), but SplitQA relies on non-open components and is text-only.",
            "declarative_only_performance": "null",
            "imperative_only_performance": "null",
            "has_comparative_results": false,
            "generalization_properties": "Effective when decomposition and search retrieve required intermediate evidence; generalization depends heavily on quality of decomposition and availability of web snippets; not designed to combine with KB reasoning.",
            "interpretability_properties": "Moderate interpretability via explicit decomposition steps and retrieved snippets which can be inspected; internal neural reader remains less interpretable.",
            "limitations_or_failures": "Relies on an external web search engine and curated snippet corpus (non-reproducible components), biased toward retrieval setup; not designed to use KBs, so cannot exploit structured KB facts directly.",
            "theoretical_framework": "Procedural decomposition and retrieve-then-read paradigm; no formal hybrid symbolic/neural theory in the cited discussion.",
            "uuid": "e633.4",
            "source_info": {
                "paper_title": "PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "AQQU",
            "name_full": "AQQU (AQQU and successors for KB+text QA)",
            "brief_description": "A symbolic/structured approach that maps natural language to KB query graphs and uses KB facts (and sometimes text constraints) to answer; historically focused on mapping questions to Freebase-style structured queries with optional text-based constraints.",
            "citation_title": "More accurate question answering on freebase",
            "mention_or_use": "mention",
            "system_name": "AQQU (and successors)",
            "system_description": "AQQU is an approach that translates questions into structured KB query graphs (symbolic logical forms) and applies KB lookup to obtain candidate answers, optionally applying textual restrictions derived from question phrasing. It is primarily a symbolic/semantic-parsing based pipeline (rule and grammar driven), not trained as an end-to-end neural model in its original instantiation.",
            "declarative_component": "Structured KB query graphs and semantic parsing to map natural language into symbolic queries executed against Freebase or similar KBs.",
            "imperative_component": "Procedural pipeline elements for candidate generation and text-based constraint handling; not fundamentally neural in original work (later successors may incorporate learned components).",
            "integration_method": "Primarily symbolic semantic parsing executed against a KB, with heuristics or additional modules to incorporate text-based constraints; in literature successors have tried combining with learned modules.",
            "emergent_properties": "High precision when questions map cleanly to KB query patterns; provides explicit, interpretable logical forms and exact execution semantics.",
            "task_or_benchmark": "Simple KB question answering benchmarks (e.g., SimpleWebQuestions) where questions correspond to KB relations with optional textual constraints.",
            "hybrid_performance": "Not evaluated in this paper as an experimental baseline; cited as a related approach and historically influential.",
            "declarative_only_performance": "null",
            "imperative_only_performance": "null",
            "has_comparative_results": false,
            "generalization_properties": "Strong when questions follow KB-compatible templates; limited for opentextual variation and multi-hop questions without extensions.",
            "interpretability_properties": "High interpretability via explicit logical forms and query graphs.",
            "limitations_or_failures": "Hard to scale to multi-hop compositional questions and to incorporate diverse natural-language expressions; extending to noisy text evidence is non-trivial.",
            "theoretical_framework": "Symbolic semantic parsing and KB query generation; no unified differentiable hybridization described in the cited context.",
            "uuid": "e633.5",
            "source_info": {
                "paper_title": "PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text",
                "publication_date_yy_mm": "2019-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Open domain question answering using early fusion of knowledge bases and text",
            "rating": 2
        },
        {
            "paper_title": "Key-value memory networks for directly reading documents",
            "rating": 2
        },
        {
            "paper_title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision",
            "rating": 2
        },
        {
            "paper_title": "The web as a knowledge-base for answering complex questions",
            "rating": 2
        },
        {
            "paper_title": "More accurate question answering on freebase",
            "rating": 1
        },
        {
            "paper_title": "Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.019064249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text</h1>
<p>Haitian Sun Tania Bedrax-Weiss William W. Cohen<br>Google Research<br>{haitiansun,tbedrax,wcohen}@google.com</p>
<h4>Abstract</h4>
<p>We consider open-domain question answering (QA) where answers are drawn from either a corpus, a knowledge base (KB), or a combination of both of these. We focus on a setting in which a corpus is supplemented with a large but incomplete KB, and on questions that require non-trivial (e.g., "multi-hop") reasoning. We describe PullNet, an integrated framework for (1) learning what to retrieve and (2) reasoning with this heterogeneous information to find the best answer. PullNet uses an iterative process to construct a question-specific subgraph that contains information relevant to the question. In each iteration, a graph convolutional network (graph CNN) is used to identify subgraph nodes that should be expanded using retrieval (or "pull") operations on the corpus and/or KB. After the subgraph is complete, another graph CNN is used to extract the answer from the subgraph. This retrieve-and-reason process allows us to answer multi-hop questions using large KBs and corpora. PullNet is weakly supervised, requiring question-answer pairs but not gold inference paths. Experimentally PullNet improves over the prior state-ofthe art, and in the setting where a corpus is used with incomplete KB these improvements are often dramatic. PullNet is also often superior to prior systems in a KB-only setting or a text-only setting.</p>
<h2>1 Introduction</h2>
<p>Open domain Question Answering (QA) is the task of finding answers to questions posed in natural language, usually using text from a corpus (Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017), or triples from a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Yih et al., 2015). Both of these approaches have limitations. Even the largest KBs are incomplete (Min et al., 2013), which limits recall of a KB-based QA system. On the other
hand, while a large corpus may contain more answers than a KB, the diversity of natural language makes corpus-based QA difficult (Chen et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019; Yang et al., 2018).</p>
<p>In this paper we follow previous research (Sawant et al., 2019; Sun et al., 2018) in deriving answers using both a corpus and a KB. We focus on tasks in which questions require compositional (sometimes called "multi-hop") reasoning, and a setting in which the KB is incomplete, and hence must be supplemented with information extracted from text. We also restrict ourselves in this paper to answers which correspond to KB entities. For this setting, we propose an integrated framework for (1) learning what to retrieve, from either a corpus, a KB, or a combination, and (2) combining this heterogeneous information into a single data structure that allows the system to reason and find the best answer. In prior work, this approach was termed an early fusion approach, and shown to improve over late fusion methods, in which two QA systems, one corpus-based and one KB-based, are combined in an ensemble.</p>
<p>Our system, PullNet, builds on the GRAFTNet $^{1}$ (Sun et al., 2018) early fusion system. GRAFT-Net uses heuristics to build a questionspecific subgraph which contains sentences from the corpus, and entities and facts from the KB. A graph CNN (Kipf and Welling, 2016; Li et al., 2016; Schlichtkrull et al., 2017) variant is then used to reason over this graph and select an answer. However, as we will show experimentally, GRAFT-Net's heuristics often produce subgraphs that are far from optimal-often they are much larger than necessary, and sometimes do not contain the answer.</p>
<p>Like GRAFT-Net, PullNet also uses a reason-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ing process based on a graph CNN to find answers. However, PullNet learns how to construct the subgraph, rather than using an ad hoc subgraph-building strategy. More specifically, PullNet relies on a small set of retrieval operations, each of which expands a graph node by retrieving new information from the KB or the corpus. PullNet learns when and where to apply these "pull" operations with another graph CNN classifier. The "pull" classifier is weakly supervised, using question-answer pairs.</p>
<p>The end result is a learned iterative process for subgraph construction, which begins with a small subgraph containing only the question text and the entities which it contains, and gradually expands the subgraph to contain information from the KB and corpus that are likely to be useful. The incremental question-guided subgraph construction process results in high-recall subgraphs that are much smaller than the ones created heuristically, making the final answer extraction process easier. The process is especially effective for multi-hop questions, which naively would require expanding the subgraph to include all corpus and KB elements that are $k$ hops away from the question.</p>
<p>PullNet improves over the current state-of-the-art for KB-only QA on several benchmark datasets, and is superior to, or competitive with, corpus-only QA on several others. For multi-hop questions, this improvement is often dramatic. For instance, MetaQA (Zhang et al., 2018) contains multi-hop questions based on a small movie KB, originally associated with the WikiMovies dataset (Miller et al., 2016). In a KB-only setting, PullNet improves hits-at-one performance for 3-hop MetaQA questions from $62.5 \%$ to $91.4 \%$. Perhaps more interestingly, PullNet obtains performance of $85.2 \%$ hits-at-one with a KB from which half of the triples have been removed, if that KB is supplemented with a corpus. We note that this result improves by $7 \%$ (absolute improvement) over a pure corpus-based QA system, and by more than $25 \%$ over a pure KB-based QA system. In a similar incomplete-KB setting, PullNet improves over GRAFT-Net by $6.8 \%$ on the ComplexWebQuestions dataset (Talmor and Berant, 2018).</p>
<h2>2 Related Work</h2>
<p>This paper has focused on QA for multi-hop questions using large KBs and text corpora as the information sources from which answers can be
drawn. The main technical contribution is an iterative question-guided retrieval mechanism that retrieves information from KBs, corpora, or combinations of both, which makes it possible to follow long paths of reasoning on large KB.</p>
<p>A long line of QA models have been developed which answer questions based on a single passage of text (Dhingra et al., 2016; Yu et al., 2018; Seo et al., 2016; Gao et al., 2018; Liu et al., 2017; Devlin et al., 2018). Generally, these "reading comprehension" systems are operated by encoding the passage and question into an embedding space, and due to memory limitations cannot be applied to a large corpus instead of a short passage. To address this limitation a number of systems have been designed which use a "retrieve and read" pipeline (Chen et al., 2017; Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Wang et al., 2018, 2017), in which a retrieval system with high recall is piped into a reading comprehension system that can find the answer. An alternative approach is "phrase-indexed" QA (Seo et al., 2018), where embedded phrases in a document are indexed and searched over. Existing systems are also not able to use both KB and text for QA. They also differ from PullNet in using only a single round of retrieval; however, for questions that require multi-hop reasoning, it is difficult for a single retrieval step to find the relevant information.</p>
<p>SplitQA (Talmor and Berant, 2018) is a textbased QA system that decomposes complex questions (e.g., with conjunction or composition) into simple subquestions, and performs retrieval sequentially for the subquestions. Although it uses iterative retrieval for multi-hop questions, unlike PullNet, SplitQA does not also use a KB as an information source. Also, SplitQA has been applied only to the Complex WebQuestions dataset, so it is unclear how general this approach is.</p>
<p>There has also been much work on QA from KBs alone, often using methods based on memory networks (Sukhbaatar et al., 2015), semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005) or reinforcement learning Das et al. (2017). Extending such KB-based work to also use text, however, is non-trivial. Another line of QA work from text and KBs is exemplified by AQQU (Bast and Haussmann, 2015) and its successors (Sawant et al., 2019). These systems focus on questions (e.g. SimpleWebQuestions), that can</p>
<p>be interpreted as identifying an entity based on a relationship and related entity, plus additional restrictions described in text, and it is unclear how to extend such approaches to multi-hop questions.</p>
<p>GRAFT-Net (Sun et al., 2018) supports multihop reasoning on both KBs and text by introducing a question subgraph built with facts and text, and uses a learned graph representation (Kipf and Welling, 2016; Li et al., 2016; Schlichtkrull et al., 2017; Scarselli et al., 2009) to perform the reasoning required to select the answer. We use the same representation and reasoning scheme as GRAFTNet, but do not require that the entire graph be retrieved in a single step. In our experimental comparisons, this gives significant performance gains for multi-hop reasoning tasks.</p>
<p>Combinations of KBs and text have also been used for relation extraction and Knowledge Base Completion (KBC) (Lao et al., 2012; Toutanova et al., 2015; Das et al., 2017). The QA task differs from KBC in that in QA, the inference process must be conditioned on a natural-language question, which leads to different constraints on which methods can be used.</p>
<h2>3 The PullNet Model</h2>
<p>PullNet retrieves from two "knowledge sources", a text corpus and a KB. Given a question, PullNet will use these to construct a question subgraph that can be used to answer the question. The question subgraph is constructed iteratively. Initially the subgraph depends only on the question. PullNet then iteratively expands the subgraph by choosing nodes from which to "pull" information about, from the KB or corpus as appropriate. The question subgraph is heterogeneous, and contains both entities, KB triples, and entity-linked text.</p>
<p>In this section, we will first introduce notation defining the heterogeneous graph structure we use. Then we will introduce the general iterative retrieval process. Finally, we will discuss the retrieval operations used on the corpus and KB, and the classification operations on the graph which determine where to perform the retrievals.</p>
<h3>3.1 The Question Subgraph</h3>
<p>A question subgraph for question $q$, denoted $\mathcal{G}_{q}=$ ${\mathcal{V}, \mathcal{E}}$, is a hetogeneous graph that contains information from both the text corpus and the KB relevant to $q$. Let $\mathcal{V}$ denote the set of vertices, which we also call nodes. Following GRAFT-</p>
<p>Net (Sun et al., 2018), there are three types of nodes: entity nodes $\mathcal{V}<em d="d">{e}$, text nodes $\mathcal{V}</em>}$, and fact nodes $\mathcal{V<em e="e">{f}$, with $\mathcal{V}=\mathcal{V}</em>} \cup \mathcal{V<em f="f">{d} \cup \mathcal{V}</em>}$. An entity node $v_{e} \in \mathcal{V<em d="d">{e}$ represents an entity from the knowledge base. A text node $v</em>} \in \mathcal{V<em 1="1">{d}$ represents a document from the corpus, with a sequence of tokens denoted $\left(w</em>}, \ldots, w_{|d|}\right)$. In this paper, a document is always a single sentence, to which an entity linker (Ji et al., 2014) has been applied to detect and ground entity mentions. A fact node $v_{f} \in \mathcal{V<em s="s">{f}$ represents a triplet $\left(v</em>}, r, v_{o}\right)$ from the KB, with subject and objects $v_{s}, v_{o} \in \mathcal{V<em f="f">{e}$ and relation $r$. Let $\mathcal{E}$ denote the set of edges between nodes. An edge connects a fact node $v</em>$ iff the entity is mentioned in the text.}$ and an entity node $v_{e}$ iff fact $v_{f}$ has $v_{e}$ as its subject or object. An edge connects a text node $v_{d}$ with entity node $v_{e</p>
<h3>3.2 Iterative subgraph construction</h3>
<h3>3.2.1 Overview</h3>
<p>We start with a question subgraph $\mathcal{G}<em q__i="q_{i">{q}^{0}=\left{\mathcal{V}^{0}, \mathcal{E}^{0}\right}$ where $\mathcal{V}^{0}=\left{e</em>$ until it contains the information required to answer the question.}}\right}$ is the list of entities in the question and $\mathcal{E}^{0}$ is an empty set. We iteratively expand the question subgraph $\mathcal{G}_{q}^{0</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 PullNet
    Initialize question graph \(G_{q}^{0}\) with question \(q\) and question
    entities, with \(\mathcal{V}^{0}=\left\{e_{q_{i}}\right\}\) and \(\mathcal{E}^{0}=\emptyset\).
    for \(t=1, \cdots, T\) do
        Classify the entity nodes in the graph and select those
            with probability larger than \(\epsilon\)
                \(\left\{v_{e_{i}}\right\}=\operatorname{classify} \_\)pullnodes \(\left(G_{q}^{t}, k\right)\)
        for all \(v_{e}\) in \(\left\{v_{e_{i}}\right\}\) do
            Perform pull operation on selected entity nodes
                \(\left\{v_{d_{i}}\right\}=\operatorname{pull} \_\)docs \(\left(v_{e}, q\right)\)
                \(\left\{v_{f_{i}}\right\}=\operatorname{pull} \_\)facts \(\left(v_{e}, q\right)\)
            for all \(v_{d}\) in \(\left\{v_{d_{i}}\right\}\) do
            Extract entities from new document nodes
                \(\left\{v_{e(d)_{i}}\right\}=\operatorname{pull} \_\)entities \(\left(v_{d}\right)\)
            for all \(v_{f}\) in \(\left\{v_{f_{i}}\right\}\) do
                Extract head and tail of new fact nodes
                    \(\left\{v_{e(f)_{i}}\right\}=\operatorname{pull} \_\operatorname{headtail}\left(v_{f}\right)\)
    Add new nodes and edges to question graph
        \(G_{q}^{t+1}=\) update \(\left(G_{q}^{t}\right)\)
    Select entity node in final graph that is the best answer
        \(v_{\text {ans }}=\operatorname{classify} \_\)answer \(\left(G_{q}^{T}\right)\)
</code></pre></div>

<p>The algorithm is shown in Alg 1. Briefly, we expand the graph in $T$ iterations. In each iteration, we choose entities whose probability is larger than $\epsilon$ to expand, and then for each selected entity,</p>
<p>we retrieve a set of related documents, and also a set of related facts. The new documents are then passed through an entity-linking system to identify entities that occur in them, and the head and tail entities of each fact are also extracted. The last stage in each iteration is to update the question graph by adding all these new edges. After the $T$-th iteration of expansion, an additional classification step is applied to the final question subgraph to predict the answer entity.</p>
<h3>3.2.2 Pull Operations</h3>
<p>Pull operations either retrieve information from a knowledge source, or extract entities from a fact or document.</p>
<p>The two extraction operations are relatively simple. The pull_entities $\left(v_{d}\right)$ operation inputs a document node $v_{d}$, calls an entity linker and returns all entities mentioned in $v_{d}$. The pull_headtail $\left(v_{f}\right)$ operation inputs a fact node $v_{f}$ and returns the subject and object entity of fact $v_{f}$.</p>
<p>The retrieval operations are more complex. The pull_docs $\left(v_{e}, q\right)$ ) operation retrieves relevant documents from the corpus. We use an IDF-based retrieval system, Lucene (McCandless et al., 2010) and assume that all sentences have been entitylinked prior to being indexed. The retrieved documents are constrained to link to entity $v_{e}$, and are ranked by their IDF similarity to the question $q$. Only the top $N_{d}$ ranked documents are returned.</p>
<p>The pull_facts $\left(v_{e}, q\right)$ operation retrieves the top $N_{f}$ facts from the KB about entity $v_{e}$. The retrieved facts are constrained to have $v_{e}$ as their subject or object, and are ranked based on the similarity $S(r, q)$ between the fact's relation $r$ and the question $q$. Since it is not obvious how to assess relevance of a fact to a question $q$, we learn $S(r, q)$ as follows. Let $h_{r}$ be an embedding of relation $r$, which is looked up from an embedding table, and let $q=\left(w_{1}, \ldots, w_{|q|}\right)$ be the sequence of words for question $q$. Similarity is defined as the dotproduct of the last-state LSTM representation for $q$ with the embedding for $r$. This dot-product is then passed through a sigmoid function to bring it into a range of $[0,1]$ : as we explain below, we will train this similarity function as a classifier which predicts which retrieved facts are relevant to the question $q$. The final ranking method for facts is</p>
<p>$$
\begin{aligned}
&amp; h_{q}=\operatorname{LSTM}\left(w_{1}, \ldots, w_{|q|}\right) \in \mathbb{R}^{n} \
&amp; S(r, q)=\operatorname{sigmoid}\left(h_{r}^{T} h_{q}\right)
\end{aligned}
$$</p>
<h3>3.2.3 Classify Operations</h3>
<p>Two types of classify operations are applied to the nodes in a subgraph $G_{q}^{t}$. These operations are applied only to the entity nodes in the graph, but they are based on node representations computed by the graph CNN, so the non-entity nodes and edges also affect the classification results.</p>
<p>During subgraph construction, the classify_pullnodes $\left(G_{q}^{t}\right)$ operation returns the probability entity node $v_{e}$ should be expanded in the next iteration. We choose the $k$ nodes with the highest probability in each iteration. After the subgraph is complete, the classify_answer $\left(G_{q}^{t}\right)$ operation predicts whether an entity node answers the question. The highest-scoring entity node is returned as the final answer.</p>
<p>We use the same CNN architecture used by GRAFT-Net (Sun et al., 2018) for classification. GRAFT-Net supports node classification on heterogeneous graphs containing facts, entities, and documents. GRAFT-Net differs from other graph CNN implementations in using special mechanisms to distribute representations across different types of nodes and edges: notably, document nodes are represented with an LSTM encoding, extended by mechanisms that allow the representations for entity nodes $v_{e}$ to be passed into a document $v_{d}$ that mentions $v_{e}$, and mechanisms that allow the LSTM hidden states associated with an entity mention to be passed out of $v_{d}$ to the associated entity node $v_{e}$.</p>
<h3>3.2.4 The Update Operation</h3>
<p>The update operation takes the question subgraph $G_{q}^{t-1}$ from the previous iteration and updates it by adding the newly retrieved entity nodes $\left{v_{e(f)<em>{i}}\right} \cup\left{v</em>{e(d)<em d__i="d_{i">{i}}\right}$, the text nodes $\left{v</em>$ based on the definitions of Section 3.1. Note that some new edges are derived when pull operations are performed on text and fact nodes, but other new edges may connect newly-added nodes with nodes that already exist in the previous subgraph.}}\right}$, and the fact nodes $\left{v_{f_{i}}\right}$. It also updates the set of edges $\mathcal{E</p>
<h3>3.3 Training</h3>
<p>To train PullNet, we assume that we only observe question and answer pairs, i.e., the actual inference chain required to answer the question is latent. We thus need to use weak supervision to train the classifiers described above.</p>
<p>To train these models, we form an approximation of the ideal question subgraph for question $q$ as follows. Note that in training, the answer entities are available. We use these to find all shortest paths in the KB between the question entities and answer entities. Each entity $e$ that appears in such a shortest path will be marked as a candidate intermediate entities. For each candidate intermediate entity $e$ we record its minimal distance $t_{e}$ from the question nodes.</p>
<p>When we train the classify_pullnodes classifier in iteration $t$, we treat as positive examples only those entities $e^{\prime}$ that are connected to a candidate intermediate entity $e$ with distance $e_{t}=t+1$. Likewise in training the similarity function $S\left(h_{r}, q\right)$ we treat as positive relations leading to candidate intermediate entities $e$ at distance $e_{t}=t+1$. This encourages the retrieval to focus on nodes that lie on shortest paths to an answer.</p>
<p>In training we use a variant of teacher forcing. We pull from all entity nodes with a predicted score larger than some threshold $\epsilon$, rather than only the top $k$ nodes. If, during training, a candidate intermediate entity is not retrieved in iteration $t_{e}$, we add it to the graph anyway. The values $T$ and $\epsilon$ are hyperparameters, but here we always pick for $T$ the maximum length of the inference chain needed to ensure full coverage of the answers.</p>
<p>We use the same classifier in the retrieval step as in answer selection, except that we change the last fully-connected layer. The classifiers used for retrieval in the different iterations are identical.</p>
<p>The learned parts of the model are implemented in PyTorch, using an ADAM optimizer (Kingma and $\mathrm{Ba}, 2014$ ), and the full retrieval process of Alg 1 is performed on each minibatch.</p>
<h2>4 Experiments and Results</h2>
<h3>4.1 Datasets</h3>
<p>MetaQA (Zhang et al., 2018) contains more than 400 k single and multi-hop (up to 3-hop) questions in the domain of movies. The questions were constructed using the knowledge base provided with the WikiMovies (Miller et al., 2016) dataset. We use the "vanilla" version of the queries ${ }^{2}$ We use the KB and text corpus supplied with the WikiMovies dataset, and use exact match on surface forms to perform entity linking. The KB used here is relatively small, with about 43 k entities and 135k</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>triples.
WebQuestionsSP (Yih et al., 2015) contains 4737 natural language questions that are answerable using Freebase. ${ }^{3}$ The questions require up to 2-hop reasoning from knowledge base, and 1-hop reasoning using the corpus. We use Freebase as our knowledge base but for ease of experimentation restrict it to a subset of Freebase which contains all facts that are within 2 -hops of any entity mentioned in the questions of WebQuestionsSP. We also exclude a few "very common" entities, e.g. the ones that describe the hierarchical structure of KB. This smaller KB contains 43 million facts and 12 million entities. We use Wikipedia as our corpus and use a simple entity-linker: we link entities by exact matching to any surface form annotated the FACC1 dataset (Gabrilovich et al., 2013). ${ }^{4}$
Complex WebQuestions 1.1 (Complex WebQ) (Talmor and Berant, 2018) is generated from WebQuestionsSP by extending the question entities or adding constraints to answers, in order to construct multi-hop questions. ${ }^{5}$ There are four types of question: composition (45\%), conjunction (45\%), comparative (5\%), and superlative (5\%). The questions require up to 4 -hops of reasoning on the KB and thr 2-hops on corpus. We use the same KB and corpus as for WebQuestionsSP.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MetaQA 1-hop</td>
<td style="text-align: center;">96,106</td>
<td style="text-align: center;">9,992</td>
<td style="text-align: center;">9,947</td>
</tr>
<tr>
<td style="text-align: left;">MetaQA 2-hop</td>
<td style="text-align: center;">118,980</td>
<td style="text-align: center;">14,872</td>
<td style="text-align: center;">14,872</td>
</tr>
<tr>
<td style="text-align: left;">MetaQA 3-hop</td>
<td style="text-align: center;">114,196</td>
<td style="text-align: center;">14,274</td>
<td style="text-align: center;">14,274</td>
</tr>
<tr>
<td style="text-align: left;">WebQuestionsSP</td>
<td style="text-align: center;">2,848</td>
<td style="text-align: center;">250</td>
<td style="text-align: center;">1,639</td>
</tr>
<tr>
<td style="text-align: left;">Complex WebQ</td>
<td style="text-align: center;">27,623</td>
<td style="text-align: center;">3,518</td>
<td style="text-align: center;">3,531</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of all datasets.</p>
<h3>4.2 Tasks</h3>
<p>We explored several different QA settings: complete KB only, corpus only, incomplete KB only, and incomplete KB paired with the corpus.</p>
<p>In the complete $K B$ only setting, the answer always exists in knowledge base: for all of these datasets, this is true because the questions were crowd-sourced to enforce this conditions. This is the easiest setting for QA, but arguably unrealistic, since with a more natural distribution of ques-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>tions, a KB is likely to be incomplete. In the text only setting we use only the corpus. In the incomplete $K B$ setting, we simulate KB-based QA on an incomplete KB by randomly discarding some of the triples in the KB: specifically, we randomly drop a fact from the knowledge base with probability $p=50 \%$. This setting is presented mainly as a baseline for the incomplete $K B$ plus text setting, where we pair the same incomplete knowledge base with the corpus. In principle this allows a learned QA system to adopt many different hybrid strategies: e.g., for 1-hop queries, a model "back off" to text when the KB is missing information, while in more complex queries, reasoning can involve combining inferences done with text and inferences done with KB triples.</p>
<p>Comment. One point to note is that our training procedure is based on finding shortest paths in a complete KB, and we use this same procedure on the incomplete KB setting. Thus the weak training that we use should, in the incomplete-KB settings, be viewed as a form of weak supervision, with labels that are intermediate in informativeness between pure distant training (with only questionanswer pairs) and gold inference paths (a setting that has been extensively investigated on some of these datasets, in particular WebQuestionsSP).</p>
<h3>4.3 Baselines</h3>
<p>We choose Key-Value Memory Networks (Miller et al., 2016) and GRAFT-Net (Sun et al., 2018) as our baseline models: to the best of our knowledge, these are the only ones that can use both text and KBs for question answering. However, both models are limited by the number of facts and text that can fit into memory. Thus, we create a separate retrieval process as a pre-processing step, which will be discussed below.</p>
<p>Key-Value Memory Networks (KVMem) (Miller et al., 2016) maintain a memory table which stores KB facts and text encoded into keyvalue pairs. Our encoding of KB facts is the same as is presented in Miller et al. (2016). For text, we use an bi-directional LSTM to encode the text for the key and take the entities mentioned in the text as values. Our implementation shows comparable performance on the WikiMovies (MetaQA 1-hop) dataset, as shown in Table 3.</p>
<p>For GRAFT-Net (Sun et al., 2018), we use the implementation published by the author; ${ }^{6}$ how-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>|  | WikiM-KB | WikiM-KB (50\%) |
| :-- | :--: | :--: |
| MetaQA-1hop | 0.995 / 9.17 | 0.544 / 4.58 |
| MetaQA-2hop | 0.983 / 47.3 | 0.344 / 28.6 |
| MetaQA-3hop | 0.923 / 459.2 | 0.522 / 316.6 |
|  | Freebase | Freebase (50\%) |
| WebQuestionsSP | 0.927 / 1876.9 | 0.485 / 1212.5 |
| ComplexWebQ | 0.644 / 1948.7 | 0.542 / 1849.2 |</p>
<p>Table 2: Retrieval results (recall / # entities in graph) on MetaQA ( $m=500$ ), and WebQuestionsSP and Complex WebQuestions with Freebase ( $m=2000$ ).
ever, we retrieve data with a simpler process, as described in Section 4.4.</p>
<h3>4.4 Subgraph Retrieval for Baseline Models</h3>
<p>Text is retrieved (non-iteratively) using IDF-based similarity to the question. It is not obvious how to perform KB retrieval: we would like to retrieve as many facts as possible to maximize the recall of answers, but it is infeasible to take all facts that are within $k$-hops of question entities since the number grows exponentially. Based on prior work in the database community on sampling from graphs (Leskovec and Faloutsos, 2006) and local partitioning (Andersen et al., 2006a), we ran an approximate version of personalized PageRank (aka random walk with reset) to find the KB entities closest to the entities in the question-specifically, we used the PageRank-Nibble algorithm (Andersen et al., 2006b) with $\epsilon=1 e^{-6}$ and the picked the $m$ top-scoring entities. ${ }^{7}$ We then eliminate all top$m$ entities that are more than $k$-hops of the question entities, and finally, retrieve all facts from the KB connecting retrieved entities.</p>
<p>The results for several tasks are shown in Table 2, which gives the answer recall, and the average number of entities retrieved. For the smaller MetaQA KB with $m=500$, the retrieval method finds high-coverage graphs when the KB is complete. For ComplexWebQuestions, even with $m=$ 2000, the recall is $64 \%$-which is expected, since retrieving relevant entities for a multi-hop question from a KB with millions of entities is difficult.</p>
<h3>4.5 Main Results</h3>
<h3>4.5.1 MetaQA</h3>
<p>The experimental results for MetaQA are shown in Table 3. For 1-hop questions (which is identical to the WikiMovies dataset), PullNet is comparable</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">MetaQA (1-hop) / wikimovies</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MetaQA (2-hop)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MetaQA (3-hop)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KB</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">$\begin{gathered} 50 \% \ \text { KB } \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 50 \% \mathrm{~KB} \ +\text { Text } \end{gathered}$</td>
<td style="text-align: center;">KB</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">$\begin{gathered} 50 \% \ \text { KB } \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 50 \% \mathrm{~KB} \ +\text { Text } \end{gathered}$</td>
<td style="text-align: center;">KB</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">$\begin{gathered} 50 \% \ \text { KB } \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 50 \% \mathrm{~KB} \ +\text { Text } \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">KV-Mem*</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">(63.6)</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">(41.8)</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">(37.6)</td>
<td style="text-align: center;">35.2</td>
</tr>
<tr>
<td style="text-align: center;">GRAFT-Net*</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">(64.0)</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">(52.6)</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">(59.2)</td>
<td style="text-align: center;">66.4</td>
</tr>
<tr>
<td style="text-align: center;">PullNet (Ours)</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">(65.1)</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">(52.1)</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">(59.7)</td>
<td style="text-align: center;">85.2</td>
</tr>
<tr>
<td style="text-align: center;">KV-Mem</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GRAFT-Net</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">86.6</td>
<td style="text-align: center;">(68.0)</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">VRN</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 3: Hits@1 on MetaQA compared to baseline models. Number below the double line are from original papers: KV-Mem (KB) (Miller et al., 2016), KV-Mem (Text) (Watanabe et al., 2017), GRAFT-Net (Sun et al., 2018), and VRN (Zhang et al., 2018). *Reimplemented or different retrieval process.
to the state-of-the-art. ${ }^{8}$ We also see that our simplified retrieval pipeline has slightly lower performance than the original (Sun et al., 2018) method. However, for multi-hop questions, PullNet generally shows a large improvement over the baseline models. In the KB-only setting, the absolute performance improvement over the best previously published results is 10 points for 2-hop and almost 30 points for 3-hop questions-and there are even larger improvements relative to the baseline models in the settings using text. In the incomplete KB plus text setting, PullNet also consistently improves over either the text-only or incomplete-KBonly setting. ${ }^{9}$ This supports our claim that PullNet is able to effectively combine a KB and text. ${ }^{10}$</p>
<h3>4.5.2 WebQuestionsSP</h3>
<p>Table 4 presents similar results on the WebQuestionsSP dataset. PullNet improves over GRAFT-Net in the incomplete KB plus text setting, and the addition of text to the incomplete KB also improves performance for PullNet.</p>
<h3>4.5.3 Complex WebQuestions</h3>
<p>Complex WebQuestions contains multi-hop questions against Freebase: intuitively, one would expect that single-shot retrieval of facts and text would not be able to always find sufficient information to answer such questions. Table 5 shows our results for Complex WebQuestions on the development set. As expected, PullNet shows significant improvement over GRAFT-Net and KVMem on all four settings. Once again we see some</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 4: Hits@1 on WebQuestionsSP compared to baseline models. Number below the double line are from the original papers: GRAFT-Net (Sun et al., 2018), and NSM (Liang et al., 2017) (which only reports F1 in their paper). * Reimplemented or different retrieval process.
improvement when pairing the incomplete knowledge base with text, compared to using the incomplete knowledge base only or the text only.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">KB</th>
<th style="text-align: center;">Text</th>
<th style="text-align: center;">$50 \%$ <br> KB</th>
<th style="text-align: center;">$50 \%$ KB <br> + Text</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">KV-Mem*</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">$(32.7)$</td>
<td style="text-align: center;">31.6</td>
</tr>
<tr>
<td style="text-align: left;">GRAFT-Net*</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">$(48.2)$</td>
<td style="text-align: center;">49.7</td>
</tr>
<tr>
<td style="text-align: left;">PullNet (Ours)</td>
<td style="text-align: center;">$\mathbf{6 8 . 1}$</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">$(50.3)$</td>
<td style="text-align: center;">$\mathbf{5 1 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">GRAFT-Net</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">$\mathbf{2 5 . 3}$</td>
<td style="text-align: center;">$(47.7)$</td>
<td style="text-align: center;">49.9</td>
</tr>
<tr>
<td style="text-align: left;">NSM</td>
<td style="text-align: center;">69.0 (F1)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 5: Hits@1 on Complex WebQuestions compared to baseline models. Above the double line results are on the dev set, below on the test set. *Reimplemented or different retrieval process.</p>
<p>Researchers are only allowed limited submissions on the Complex WebQuestions test set, however, some results for the test set are shown in Table 5. On the test set, our model has $45.9 \%$ Hits@1 in the KB only setting and $13.8 \%$ Hits@1 (not listed in Table 5) in the text only setting with Wikipedia corpus, which are comparable to the dev set results.</p>
<p>For completeness, we also compare to SplitQA (Talmor and Berant, 2018). SplitQA decomposes a multi-hop question into several simpler sub-questions, and sends each sub-question to the Google search engine. In then applies a reading comprehension model to these web snippets to find answers. The snippet corpus is made available</p>
<p>with the Complex WebQuestions dataset, but it is arguably biased toward the SplitQA model, since it was collected specifically to support it, and also relies on non-reproducible, non-open-source components. With this corpus our model has $4.5 \%$ lower Hits@1 than SplitQA. However, using the KB, PullNet has much better performance than SplitQA, with an absolute gain of $11.7 \%$.</p>
<h3>4.6 Additional Results</h3>
<h3>4.6.1 Retrieval Performance of PullNet</h3>
<p>In Figure 1a we compare the retrieval performance of PullNet to that of PageRank-Nibble on multihop questions with a complete KB, on two multihop problems, varying the number of entities retrieved by PageRank-Nibble. PullNet retrieves far fewer entities but obtains higher recall.
<img alt="img-0.jpeg" src="img-0.jpeg" />
(a) Recall of graphs retrieved by PageRank-Nibble compared with PullNet.
<img alt="img-1.jpeg" src="img-1.jpeg" />
(b) Recall of a single round of retrieval with Apache Lucene compared with PullNet.</p>
<p>Figure 1: Retrieval performance on different datasets.</p>
<p>In Figure 1b we evaluate the effectiveness of iterative retrieval for multi-hop questions on a text corpus. PullNet, with multiple iterations of retrieval, greatly outperfoms a single iteration of IDF-based retrieval on 3-hop MetaQA, and slighly outperforms IDF on Complex WebQuestions.</p>
<p>Figure 2 shows the recall of question subgraphs on 3-hop MetaQA questions as training proceeds. Performance of the retrieval components of PullNet converges relatively quickly, with recall saturating after 10-20,000 examples (about 10-20\% of a single epoch).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2: Recall of question subgraph on MetaQA 3hop questions.</p>
<h3>4.6.2 Training Time</h3>
<p>PullNet's algorithm is quite different from prior systems, since learning and retrieval are interleaved: in most prior systems, including GRAFTNet, retrieval is performed only once, before learning. Intuitively, interleaving learning with the relatively slow operation of retrieval is potentially slower; on the other hand, PullNet's final question subgraph is smaller than GRAFT-Net, which makes learning potentially faster.</p>
<p>To study these issues, we plot the Hits@1 performance of learned model versus wall clock time in Figure 3. This experiment is run on Complex WebQuestions in the KB-only setting, using one high-end GPU. ${ }^{11}$ GRAFT-Net takes an average of 31.9 minutes per epoch, while PullNet takes an average of 114 minutes per epoch, about 3.5 times slower.</p>
<p>As the graph shows, initially PullNet's performance is better, since GRAFT-Net cannot start learning until the preprocessing finishes. GRAFTNet's faster learning speed then allows it to dominates for some time. GRAFT-Net reaches its peak in about 3.6 hours. PullNet passes GRAFT-Net after around 6 hours (about 3 epochs for PullNet).</p>
<h2>5 Conclusions</h2>
<p>PullNet is a novel integrated QA framework for (1) learning what to retrieve from a KB and/or corpus and (2) reasoning with this heterogeneous data to find the best answer. Unlike prior work, PullNet uses an iterative process to construct a question-specific subgraph that contains information relevant to the question. In each iteration, a</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Performance of PullNet and GRAFT-Net under wall clock training time.
graph CNN is used to identify subgraph nodes that should be expanded using "pull" operations on the corpus and/or KB. This iterative process makes it possible to retrieve a small graph that contains just the information relevant to a multi-hop question.</p>
<p>Experimentally PullNet improves over the prior state-of-the-art for the setting in which questions are answered with a corpus plus an incomplete KB, or in settings in which questions need "multihop" reasoning. Sometimes the performance improvements are dramatic: e.g., an improvement from $62.5 \%$ Hits@1 to $91.4 \%$ Hits@1 for 3-hop MetaQa with a KB, or improvements from $32.8 \%$ to $47.2 \%$ for Complex WebQuestions with a KB.</p>
<h2>References</h2>
<p>Reid Andersen, Fan Chung, and Kevin Lang. 2006a. Local graph partitioning using pagerank vectors. In 2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS'06), pages 475-486. IEEE.</p>
<p>Reid Andersen, Fan Chung, and Kevin Lang. 2006b. Local graph partitioning using pagerank vectors. In 2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS'06), pages 475-486. IEEE.</p>
<p>Hannah Bast and Elmar Haussmann. 2015. More accurate question answering on freebase. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 1431-1440. ACM.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer opendomain questions. In Association for Computational Linguistics (ACL).</p>
<p>Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishnamurthy, Alex Smola, and Andrew McCallum. 2017.</p>
<p>Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning. arXiv preprint arXiv:1711.05851.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. 2016. Gated-attention readers for text comprehension. arXiv preprint arXiv:1606.01549.</p>
<p>Bhuwan Dhingra, Kathryn Mazaitis, and William W Cohen. 2017. Quasar: Datasets for question answering by search and reading. arXiv preprint arXiv:1707.03904.</p>
<p>Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017. Searchqa: A new q\&amp;a dataset augmented with context from a search engine. arXiv preprint arXiv:1704.05179.</p>
<p>Evgeniy Gabrilovich, Michael Ringgaard, and Amarnag Subramanya. 2013. Facc1: Freebase annotation of clueweb corpora, version 1 (release date 2013-0626, format version 1 , correction level 0 ).</p>
<p>Mingfei Gao, Ruichi Yu, Ang Li, Vlad I Morariu, and Larry S Davis. 2018. Dynamic zoom-in network for fast object detection in large images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6926-6935.</p>
<p>Heng Ji, Joel Nothman, Ben Hachey, et al. 2014. Overview of tac-kbp2014 entity discovery and linking tasks. In Proc. Text Analysis Conference (TAC2014), pages 1333-1339.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</p>
<p>Thomas N Kipf and Max Welling. 2016. Semisupervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics.</p>
<p>Ni Lao, Amarnag Subramanya, Fernando Pereira, and William W Cohen. 2012. Reading the web with learned syntactic-semantic inference rules. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1017-1026. Association for Computational Linguistics.</p>
<p>Jure Leskovec and Christos Faloutsos. 2006. Sampling from large graphs. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 631-636. ACM.</p>
<p>Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. 2016. Gated graph sequence neural networks. ICLR.</p>
<p>Chen Liang, Jonathan Berant, Quoc Le, Kenneth D Forbus, and Ni Lao. 2017. Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. $A C L$.</p>
<p>Rui Liu, Wei Wei, Weiguang Mao, and Maria Chikina. 2017. Phase conductor on multi-layered attentions for machine comprehension. arXiv preprint arXiv:1710.10504.</p>
<p>Michael McCandless, Erik Hatcher, and Otis Gospodnetic. 2010. Lucene in action: covers Apache Lucene 3.0. Manning Publications Co.</p>
<p>Alexander Miller, Adam Fisch, Jesse Dodge, AmirHossein Karimi, Antoine Bordes, and Jason Weston. 2016. Key-value memory networks for directly reading documents. EMNLP.</p>
<p>Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant supervision for relation extraction with an incomplete knowledge base. In NAACL HLT 2013 - 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Main Conference, pages 777-782. Association for Computational Linguistics (ACL).</p>
<p>Uma Sawant, Saurabh Garg, Soumen Chakrabarti, and Ganesh Ramakrishnan. 2019. Neural architecture for question answering using a knowledge graph and web corpus. Information Retrieval Journal, pages $1-26$.</p>
<p>Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2009. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80.</p>
<p>Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2017. Modeling relational data with graph convolutional networks. arXiv preprint arXiv:1703.06103.</p>
<p>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention flow for machine comprehension. arXiv preprint arXiv:1611.01603.</p>
<p>Minjoon Seo, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi, and Hannaneh Hajishirzi. 2018. Phraseindexed question answering: A new challenge for scalable document comprehension. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 559-564.</p>
<p>Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. 2015. End-to-end memory networks. In NIPS.</p>
<p>Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William W Cohen. 2018. Open domain question answering using early fusion of knowledge bases and text. EMNLP.
A. Talmor and J. Berant. 2018. The web as a knowledge-base for answering complex questions. In North American Association for Computational Linguistics (NAACL).</p>
<p>Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and Michael Gamon. 2015. Representing text for joint embedding of text and knowledge bases. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1499-1509.</p>
<p>Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. 2018. R 3: Reinforced ranker-reader for open-domain question answering. In Thirty-Second AAAI Conference on Artificial Intelligence.</p>
<p>Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. 2017. Evidence aggregation for answer reranking in open-domain question answering. CoRR, abs/1711.05116.</p>
<p>Yusuke Watanabe, Bhuwan Dhingra, and Ruslan Salakhutdinov. 2017. Question answering from unstructured text by retrieval and comprehension. arXiv preprint arXiv:1703.08885.</p>
<p>Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association of Computational Linguistics, 6:287-302.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380.</p>
<p>Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. 2015. Semantic parsing via staged query graph generation: Question answering with knowledge base. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1321-1331, Beijing, China. Association for Computational Linguistics.</p>
<p>Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. Qanet: Combining local convolution with global self-attention for reading comprehension. arXiv preprint arXiv:1804.09541.</p>
<p>John M Zelle and Raymond J Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the Thirteenth National conference on Artificial intelligence-Volume 2, pages 1050-1055. AAAI Press.</p>
<p>Luke S Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: structured classification with probabilistic categorial grammars. In Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence, pages 658-666. AUAI Press.</p>
<p>Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J Smola, and Le Song. 2018. Variational reasoning for question answering with knowledge graph. In AAAI.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ To be fair to GRAFT-Net, we used a fast in-memory implementation of PageRank-Nibble (based on SciPy sparse matrices), which takes about 40 minutes.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>GraftNet
${ }^{7}$ Notice that in this process we do not use answer entities which are of course not available at test time.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
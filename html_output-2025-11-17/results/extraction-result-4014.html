<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4014 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4014</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4014</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-93.html">extraction-schema-93</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-edf260dee56a06d897547fb460a1e317d7eb571b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/edf260dee56a06d897547fb460a1e317d7eb571b" target="_blank">Can Language Models Recognize Convincing Arguments?</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> It is shown that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, surpassing human performance.</p>
                <p><strong>Paper Abstract:</strong> The capabilities of large language models (LLMs) have raised concerns about their potential to create and propagate convincing narratives. Here, we study their performance in detecting convincing arguments to gain insights into LLMs' persuasive capabilities without directly engaging in experimentation with humans. We extend a dataset by Durmus and Cardie (2018) with debates, votes, and user traits and propose tasks measuring LLMs' ability to (1) distinguish between strong and weak arguments, (2) predict stances based on beliefs and demographic characteristics, and (3) determine the appeal of an argument to an individual based on their traits. We show that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, surpassing human performance. The data and code released with this paper contribute to the crucial effort of continuously evaluating and monitoring LLMs' capabilities and potential impact. (https://go.epfl.ch/persuasion-llm)</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4014.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4014.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 vs Humans (RQ1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 as judge compared to individual human voters (judging better debater)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct experimental comparison of GPT-4's accuracy at judging which side gave more convincing arguments in debates (RQ1) against an individual-voter human baseline (majority-agreement metric).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Language Models Recognize Convincing Arguments?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>LLM-as-a-judge for which debater was more convincing (single-label: Pro/Con/Tie) on PoliProp (RQ1).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Argument quality judgment in online political debates (debate.org PoliProp dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>GPT-4 accuracy = 60.50% (95% CI: 57.26–63.87) vs human individual-voter majority agreement = 60.69% (95% CI: 59.56–61.79).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Paper reports similar aggregate accuracy but does not report detailed qualitative discrepancy (e.g., types of rhetorical moves missed) between GPT-4 and humans; authors note GPT-4 attains 'human-like' performance in aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper notes overall human performance is only ~60% (task is difficult); matching that level does not imply flawless judgment. Some LLMs (other than GPT-4) perform much worse (see separate entries).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>GPT-4 matches the measured human baseline (individual-voter agreement) on RQ1, indicating parity at this coarse accuracy metric.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>No task-specific mitigation beyond reporting CIs; authors generally recommend continued monitoring and complementary evaluation modalities (see stacking and supervised baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Rescala et al., Can Language Models Recognize Convincing Arguments? (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Recognize Convincing Arguments?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4014.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4014.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs vs MTurk (RQ2/RQ3)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs (GPT-3.5, GPT-4, Llama 2, Mistral) compared to crowdsourced human judgments (MTurk) for predicting user stances before/after debates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison of multiple LLMs' zero-shot accuracy to crowdworker judgments on whether a user would agree with a proposition before (RQ2) and after (RQ3) reading a debate (PoliIssues dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Language Models Recognize Convincing Arguments?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>LLM-as-a-judge predicting individual user stance (Pro/Con/Tie) before-reading (RQ2) and after-reading (RQ3) using demographics and 'big issues'; compared to MTurk crowdworkers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Social-sensing style prediction of user stances in political debates.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3.5, GPT-4, Llama 2, Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>RQ2: LLM accuracies ranged 41.39%–42.82% (GPT-4 42.82%); MTurk = 39.32% (95% CI: 35.89–42.88). RQ3: GPT-4 = 44.38% (95% CI: 40.91–47.73); MTurk = 39.86% (95% CI: 36.44–43.42).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Authors report that although aggregate accuracy is similar/slightly higher for LLMs, there is no in-depth qualitative error analysis comparing the kinds of mistakes made by humans vs LLMs; they note LLMs make similar assumptions in reasoning but succeed on different subsets of examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Zero-shot LLMs underperform a supervised XGBoost model; demographic and 'big issue' features may not strongly predict individual reactions, limiting both human and LLM performance. Also some LLM outputs require post-processing because they fail to follow label-only output instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>LLMs (individually) perform on par with crowdworkers in aggregate accuracy; stacked LLM predictions (ensemble) significantly outperform crowdworkers on RQ2 and RQ3 (p < 0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Authors suggest stacking multiple LLMs' outputs improves performance; caution against interpreting parity with humans as safety reassurance and recommend combining LLM outputs or supervised learning for stronger prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Rescala et al., Can Language Models Recognize Convincing Arguments? (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Recognize Convincing Arguments?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4014.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4014.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stacked LLMs (Ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stacked LLM predictions (logistic regression on multiple LLM outputs) compared to single LLMs, humans, and supervised models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of a simple stacking/ensemble strategy that uses multiple LLM outputs as features to a supervised classifier, yielding performance improvements over single LLMs and crowdworkers on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Language Models Recognize Convincing Arguments?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Stacked LLM-as-a-judge: outputs of GPT-3.5, GPT-4, Llama 2, and Mistral used as features in logistic regression (20-fold CV) for RQ1-RQ3.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Argument judgement and user-stances prediction in political debates.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Stacked ensemble of GPT-3.5, GPT-4, Llama 2, Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>RQ1 stacked = 61.94% (95% CI: 58.54–65.34) vs human majority = 60.69%; RQ2 stacked = 45.91% (95% CI: 40.02–51.81) vs MTurk 39.32%; RQ3 stacked = 46.86% (95% CI: 41.17–52.55) vs MTurk 39.86%.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Stacking exploits complementary strengths of models; authors observed models made similar high-level assumptions but performed well on different subsets, so ensemble increases coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although stacking improves over single LLMs and over crowdworkers, it still does not reach the supervised XGBoost classifier (XGBoost accuracy reported for PoliIssues = 55.34% in Table 2), indicating limits to zero-shot LLM information capture.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Stacked LLMs significantly outperformed crowdworkers on RQ2/RQ3 (p < 0.05) and provided a small boost for RQ1, showing ensembles can exceed human crowd baselines in these settings.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Authors recommend combining multiple LLM outputs (stacking) as a practical mitigation to improve reliability of LLM-as-judge evaluations; stacking reduces but does not eliminate gap to supervised methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Rescala et al., Can Language Models Recognize Convincing Arguments? (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Recognize Convincing Arguments?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4014.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4014.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inter-model Agreement (Cohen's kappa)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Low inter-annotator agreement across LLMs measured via Cohen's kappa</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantified pairwise agreement across LLMs (GPT-3.5, GPT-4, Llama 2, Mistral) showing low Cohen's kappa (< 0.2 for most model pairs), despite similar-looking reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Language Models Recognize Convincing Arguments?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Inter-annotator agreement analysis among multiple LLMs' judgments on RQ2 (PoliIssues), reported with Cohen's kappa and visualized in Figure 5.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Comparing consistency of LLM judgments when used as annotators/judges for stance prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3.5, GPT-4, Llama 2, Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Cohen's kappa < 0.2 for most pairs of models (low inter-model agreement).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Authors note models often give similar-sounding reasoning but disagree on which label to assign, implying complementary biases and that ensemble methods capture disparate strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low agreement undermines the notion that any single LLM can reliably substitute for human annotators; inconsistent judgments across models indicate unstable LLM-as-judge behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Low kappa motivated stacking; the complementary errors mean ensembles can improve aggregate accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Do not rely on a single LLM as sole judge; use multiple models and consider ensembling or human oversight to mitigate model-specific biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Rescala et al., Can Language Models Recognize Convincing Arguments? (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Recognize Convincing Arguments?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4014.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4014.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction-following / Output-format failures</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs' failure to adhere to label-only output constraints and related post-processing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study reports many LLM responses did not follow the required single-token label format ('Pro','Con','Tie'), necessitating regex-based extraction and reducing the practical usable output rate for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Language Models Recognize Convincing Arguments?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Analysis of how often models produced answers in the required 'Pro/Con/Tie' form and the recoverable-answer rate after post-processing (PoliProp dataset, Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Practical reliability of LLM outputs when constrained to label-only annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3.5, GPT-4, Llama 2, Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Table 4 reports 'Correct Form (%)' and 'Answer Extracted (%)': e.g., Q1 correct form GPT-3.5 99.88% (extracted 100%), GPT-4 99.06% (extracted 100%), Llama 2 0.00% (extracted 95.08%), Mistral Q1 correct form 62.76% (extracted 95.55%). Similar patterns across Q2/Q3 with Llama 2 showing 0% correct form in some cases but high extractable-answer rates.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Some models (especially open-source Llama 2 and Mistral) routinely output full sentences or refusals instead of the required token, reducing immediate usability; GPT models adhered to format far more reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Outputs sometimes required post-processing; some responses were 'I cannot determine' (no answer); thus practical label extraction can be noisy and models may refuse or give non-label reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>High instruction compliance from GPT-3.5 and GPT-4 (≈99% correct form) shows some LLMs are reliable for label-only judgement tasks given careful prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>When using LLMs as judges, validate instruction-following rates, implement robust post-processing, prefer models with high label-compliance, and report 'extractable answer' ceilings as done in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Rescala et al., Can Language Models Recognize Convincing Arguments? (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Recognize Convincing Arguments?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4014.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4014.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-specific failure examples & dataset caveats</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Noted model failures (e.g., Llama 2 below-random on RQ1) and dataset/training-data limitations affecting LLM-as-judge validity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents specific failure cases (Llama 2 accuracy 24.91% on RQ1, instances of no-answer refusals) and limitations such as dataset non-representativeness and possible training-data leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Language Models Recognize Convincing Arguments?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Empirical reporting of per-model accuracies and qualitative limitations affecting the interpretation of LLM-as-judge results.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Argument evaluation and social-sensing tasks on debate.org-derived datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Llama 2 (example), GPT-3.5, GPT-4, Mistral 7B (contextual)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Example failure: Llama 2 RQ1 accuracy = 24.91% (95% CI: 20.65–26.53), which is worse than random (33.33%). Other models sometimes returned no-answer strings; paper reports extractable-answer ceilings.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Paper emphasizes that aggregate parity with humans doesn't imply equivalence in reasoning; some models systematically fail in format adherence or perform poorly on certain debate subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Dataset may be unrepresentative of general population; possible training-data overlap (models might have seen debate.org content); inability to test non-English; some models perform below random, and some responses are non-informative refusals.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>The paper checked 100 debate excerpts with GPT-4 and could not retrieve complete samples (to probe memorization) but acknowledges this is not sufficient to rule out contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Authors recommend caution interpreting LLM-as-judge parity, stress dataset representativeness checks, guard against potential memorization, and extend evaluations to other languages and diverse populations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Rescala et al., Can Language Models Recognize Convincing Arguments? (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Recognize Convincing Arguments?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Are large language models reliable argument quality annotators? <em>(Rating: 2)</em></li>
                <li>Measuring the persuasiveness of language models <em>(Rating: 2)</em></li>
                <li>On the conversational persuasiveness of large language models: A randomized controlled trial <em>(Rating: 2)</em></li>
                <li>Wisdom of the silicon crowd: Llm ensemble prediction capabilities match human crowd accuracy <em>(Rating: 2)</em></li>
                <li>Argument quality assessment in the age of instruction-following large language models <em>(Rating: 2)</em></li>
                <li>The Persuasive Power of Large Language Models <em>(Rating: 1)</em></li>
                <li>The role of prior beliefs for argument persuasion <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4014",
    "paper_id": "paper-edf260dee56a06d897547fb460a1e317d7eb571b",
    "extraction_schema_id": "extraction-schema-93",
    "extracted_data": [
        {
            "name_short": "GPT-4 vs Humans (RQ1)",
            "name_full": "GPT-4 as judge compared to individual human voters (judging better debater)",
            "brief_description": "Direct experimental comparison of GPT-4's accuracy at judging which side gave more convincing arguments in debates (RQ1) against an individual-voter human baseline (majority-agreement metric).",
            "citation_title": "Can Language Models Recognize Convincing Arguments?",
            "mention_or_use": "use",
            "evaluation_setting": "LLM-as-a-judge for which debater was more convincing (single-label: Pro/Con/Tie) on PoliProp (RQ1).",
            "task_or_domain": "Argument quality judgment in online political debates (debate.org PoliProp dataset).",
            "llm_model_name": "GPT-4",
            "agreement_rate": "GPT-4 accuracy = 60.50% (95% CI: 57.26–63.87) vs human individual-voter majority agreement = 60.69% (95% CI: 59.56–61.79).",
            "qualitative_differences": "Paper reports similar aggregate accuracy but does not report detailed qualitative discrepancy (e.g., types of rhetorical moves missed) between GPT-4 and humans; authors note GPT-4 attains 'human-like' performance in aggregate.",
            "limitations_or_failure_cases": "The paper notes overall human performance is only ~60% (task is difficult); matching that level does not imply flawless judgment. Some LLMs (other than GPT-4) perform much worse (see separate entries).",
            "counterexamples_or_strengths": "GPT-4 matches the measured human baseline (individual-voter agreement) on RQ1, indicating parity at this coarse accuracy metric.",
            "recommendations_or_best_practices": "No task-specific mitigation beyond reporting CIs; authors generally recommend continued monitoring and complementary evaluation modalities (see stacking and supervised baselines).",
            "citation": "Rescala et al., Can Language Models Recognize Convincing Arguments? (2024)",
            "uuid": "e4014.0",
            "source_info": {
                "paper_title": "Can Language Models Recognize Convincing Arguments?",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "LLMs vs MTurk (RQ2/RQ3)",
            "name_full": "LLMs (GPT-3.5, GPT-4, Llama 2, Mistral) compared to crowdsourced human judgments (MTurk) for predicting user stances before/after debates",
            "brief_description": "Empirical comparison of multiple LLMs' zero-shot accuracy to crowdworker judgments on whether a user would agree with a proposition before (RQ2) and after (RQ3) reading a debate (PoliIssues dataset).",
            "citation_title": "Can Language Models Recognize Convincing Arguments?",
            "mention_or_use": "use",
            "evaluation_setting": "LLM-as-a-judge predicting individual user stance (Pro/Con/Tie) before-reading (RQ2) and after-reading (RQ3) using demographics and 'big issues'; compared to MTurk crowdworkers.",
            "task_or_domain": "Social-sensing style prediction of user stances in political debates.",
            "llm_model_name": "GPT-3.5, GPT-4, Llama 2, Mistral 7B",
            "agreement_rate": "RQ2: LLM accuracies ranged 41.39%–42.82% (GPT-4 42.82%); MTurk = 39.32% (95% CI: 35.89–42.88). RQ3: GPT-4 = 44.38% (95% CI: 40.91–47.73); MTurk = 39.86% (95% CI: 36.44–43.42).",
            "qualitative_differences": "Authors report that although aggregate accuracy is similar/slightly higher for LLMs, there is no in-depth qualitative error analysis comparing the kinds of mistakes made by humans vs LLMs; they note LLMs make similar assumptions in reasoning but succeed on different subsets of examples.",
            "limitations_or_failure_cases": "Zero-shot LLMs underperform a supervised XGBoost model; demographic and 'big issue' features may not strongly predict individual reactions, limiting both human and LLM performance. Also some LLM outputs require post-processing because they fail to follow label-only output instructions.",
            "counterexamples_or_strengths": "LLMs (individually) perform on par with crowdworkers in aggregate accuracy; stacked LLM predictions (ensemble) significantly outperform crowdworkers on RQ2 and RQ3 (p &lt; 0.05).",
            "recommendations_or_best_practices": "Authors suggest stacking multiple LLMs' outputs improves performance; caution against interpreting parity with humans as safety reassurance and recommend combining LLM outputs or supervised learning for stronger prediction.",
            "citation": "Rescala et al., Can Language Models Recognize Convincing Arguments? (2024)",
            "uuid": "e4014.1",
            "source_info": {
                "paper_title": "Can Language Models Recognize Convincing Arguments?",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Stacked LLMs (Ensemble)",
            "name_full": "Stacked LLM predictions (logistic regression on multiple LLM outputs) compared to single LLMs, humans, and supervised models",
            "brief_description": "Evaluation of a simple stacking/ensemble strategy that uses multiple LLM outputs as features to a supervised classifier, yielding performance improvements over single LLMs and crowdworkers on some tasks.",
            "citation_title": "Can Language Models Recognize Convincing Arguments?",
            "mention_or_use": "use",
            "evaluation_setting": "Stacked LLM-as-a-judge: outputs of GPT-3.5, GPT-4, Llama 2, and Mistral used as features in logistic regression (20-fold CV) for RQ1-RQ3.",
            "task_or_domain": "Argument judgement and user-stances prediction in political debates.",
            "llm_model_name": "Stacked ensemble of GPT-3.5, GPT-4, Llama 2, Mistral 7B",
            "agreement_rate": "RQ1 stacked = 61.94% (95% CI: 58.54–65.34) vs human majority = 60.69%; RQ2 stacked = 45.91% (95% CI: 40.02–51.81) vs MTurk 39.32%; RQ3 stacked = 46.86% (95% CI: 41.17–52.55) vs MTurk 39.86%.",
            "qualitative_differences": "Stacking exploits complementary strengths of models; authors observed models made similar high-level assumptions but performed well on different subsets, so ensemble increases coverage.",
            "limitations_or_failure_cases": "Although stacking improves over single LLMs and over crowdworkers, it still does not reach the supervised XGBoost classifier (XGBoost accuracy reported for PoliIssues = 55.34% in Table 2), indicating limits to zero-shot LLM information capture.",
            "counterexamples_or_strengths": "Stacked LLMs significantly outperformed crowdworkers on RQ2/RQ3 (p &lt; 0.05) and provided a small boost for RQ1, showing ensembles can exceed human crowd baselines in these settings.",
            "recommendations_or_best_practices": "Authors recommend combining multiple LLM outputs (stacking) as a practical mitigation to improve reliability of LLM-as-judge evaluations; stacking reduces but does not eliminate gap to supervised methods.",
            "citation": "Rescala et al., Can Language Models Recognize Convincing Arguments? (2024)",
            "uuid": "e4014.2",
            "source_info": {
                "paper_title": "Can Language Models Recognize Convincing Arguments?",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Inter-model Agreement (Cohen's kappa)",
            "name_full": "Low inter-annotator agreement across LLMs measured via Cohen's kappa",
            "brief_description": "Quantified pairwise agreement across LLMs (GPT-3.5, GPT-4, Llama 2, Mistral) showing low Cohen's kappa (&lt; 0.2 for most model pairs), despite similar-looking reasoning.",
            "citation_title": "Can Language Models Recognize Convincing Arguments?",
            "mention_or_use": "use",
            "evaluation_setting": "Inter-annotator agreement analysis among multiple LLMs' judgments on RQ2 (PoliIssues), reported with Cohen's kappa and visualized in Figure 5.",
            "task_or_domain": "Comparing consistency of LLM judgments when used as annotators/judges for stance prediction.",
            "llm_model_name": "GPT-3.5, GPT-4, Llama 2, Mistral 7B",
            "agreement_rate": "Cohen's kappa &lt; 0.2 for most pairs of models (low inter-model agreement).",
            "qualitative_differences": "Authors note models often give similar-sounding reasoning but disagree on which label to assign, implying complementary biases and that ensemble methods capture disparate strengths.",
            "limitations_or_failure_cases": "Low agreement undermines the notion that any single LLM can reliably substitute for human annotators; inconsistent judgments across models indicate unstable LLM-as-judge behavior.",
            "counterexamples_or_strengths": "Low kappa motivated stacking; the complementary errors mean ensembles can improve aggregate accuracy.",
            "recommendations_or_best_practices": "Do not rely on a single LLM as sole judge; use multiple models and consider ensembling or human oversight to mitigate model-specific biases.",
            "citation": "Rescala et al., Can Language Models Recognize Convincing Arguments? (2024)",
            "uuid": "e4014.3",
            "source_info": {
                "paper_title": "Can Language Models Recognize Convincing Arguments?",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Instruction-following / Output-format failures",
            "name_full": "LLMs' failure to adhere to label-only output constraints and related post-processing",
            "brief_description": "The study reports many LLM responses did not follow the required single-token label format ('Pro','Con','Tie'), necessitating regex-based extraction and reducing the practical usable output rate for some models.",
            "citation_title": "Can Language Models Recognize Convincing Arguments?",
            "mention_or_use": "use",
            "evaluation_setting": "Analysis of how often models produced answers in the required 'Pro/Con/Tie' form and the recoverable-answer rate after post-processing (PoliProp dataset, Table 4).",
            "task_or_domain": "Practical reliability of LLM outputs when constrained to label-only annotations.",
            "llm_model_name": "GPT-3.5, GPT-4, Llama 2, Mistral 7B",
            "agreement_rate": "Table 4 reports 'Correct Form (%)' and 'Answer Extracted (%)': e.g., Q1 correct form GPT-3.5 99.88% (extracted 100%), GPT-4 99.06% (extracted 100%), Llama 2 0.00% (extracted 95.08%), Mistral Q1 correct form 62.76% (extracted 95.55%). Similar patterns across Q2/Q3 with Llama 2 showing 0% correct form in some cases but high extractable-answer rates.",
            "qualitative_differences": "Some models (especially open-source Llama 2 and Mistral) routinely output full sentences or refusals instead of the required token, reducing immediate usability; GPT models adhered to format far more reliably.",
            "limitations_or_failure_cases": "Outputs sometimes required post-processing; some responses were 'I cannot determine' (no answer); thus practical label extraction can be noisy and models may refuse or give non-label reasoning.",
            "counterexamples_or_strengths": "High instruction compliance from GPT-3.5 and GPT-4 (≈99% correct form) shows some LLMs are reliable for label-only judgement tasks given careful prompting.",
            "recommendations_or_best_practices": "When using LLMs as judges, validate instruction-following rates, implement robust post-processing, prefer models with high label-compliance, and report 'extractable answer' ceilings as done in this paper.",
            "citation": "Rescala et al., Can Language Models Recognize Convincing Arguments? (2024)",
            "uuid": "e4014.4",
            "source_info": {
                "paper_title": "Can Language Models Recognize Convincing Arguments?",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Model-specific failure examples & dataset caveats",
            "name_full": "Noted model failures (e.g., Llama 2 below-random on RQ1) and dataset/training-data limitations affecting LLM-as-judge validity",
            "brief_description": "The paper documents specific failure cases (Llama 2 accuracy 24.91% on RQ1, instances of no-answer refusals) and limitations such as dataset non-representativeness and possible training-data leakage.",
            "citation_title": "Can Language Models Recognize Convincing Arguments?",
            "mention_or_use": "use",
            "evaluation_setting": "Empirical reporting of per-model accuracies and qualitative limitations affecting the interpretation of LLM-as-judge results.",
            "task_or_domain": "Argument evaluation and social-sensing tasks on debate.org-derived datasets.",
            "llm_model_name": "Llama 2 (example), GPT-3.5, GPT-4, Mistral 7B (contextual)",
            "agreement_rate": "Example failure: Llama 2 RQ1 accuracy = 24.91% (95% CI: 20.65–26.53), which is worse than random (33.33%). Other models sometimes returned no-answer strings; paper reports extractable-answer ceilings.",
            "qualitative_differences": "Paper emphasizes that aggregate parity with humans doesn't imply equivalence in reasoning; some models systematically fail in format adherence or perform poorly on certain debate subsets.",
            "limitations_or_failure_cases": "Dataset may be unrepresentative of general population; possible training-data overlap (models might have seen debate.org content); inability to test non-English; some models perform below random, and some responses are non-informative refusals.",
            "counterexamples_or_strengths": "The paper checked 100 debate excerpts with GPT-4 and could not retrieve complete samples (to probe memorization) but acknowledges this is not sufficient to rule out contamination.",
            "recommendations_or_best_practices": "Authors recommend caution interpreting LLM-as-judge parity, stress dataset representativeness checks, guard against potential memorization, and extend evaluations to other languages and diverse populations.",
            "citation": "Rescala et al., Can Language Models Recognize Convincing Arguments? (2024)",
            "uuid": "e4014.5",
            "source_info": {
                "paper_title": "Can Language Models Recognize Convincing Arguments?",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Are large language models reliable argument quality annotators?",
            "rating": 2
        },
        {
            "paper_title": "Measuring the persuasiveness of language models",
            "rating": 2
        },
        {
            "paper_title": "On the conversational persuasiveness of large language models: A randomized controlled trial",
            "rating": 2
        },
        {
            "paper_title": "Wisdom of the silicon crowd: Llm ensemble prediction capabilities match human crowd accuracy",
            "rating": 2
        },
        {
            "paper_title": "Argument quality assessment in the age of instruction-following large language models",
            "rating": 2
        },
        {
            "paper_title": "The Persuasive Power of Large Language Models",
            "rating": 1
        },
        {
            "paper_title": "The role of prior beliefs for argument persuasion",
            "rating": 1
        }
    ],
    "cost": 0.01402125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Can Language Models Recognize Convincing Arguments?</h1>
<p>Paula Dolores Rescala ${ }^{1}$, Manoel Horta Ribeiro ${ }^{1}$, Tiancheng $\mathbf{H u}^{2}$, Robert West ${ }^{1}$<br>${ }^{1}$ EPFL, ${ }^{2}$ University of Cambridge<br>Correspondence: manoel.hortaribeiro@epfl.ch</p>
<h4>Abstract</h4>
<p>The capabilities of large language models (LLMs) have raised concerns about their potential to create and propagate convincing narratives. Here, we study their performance in detecting convincing arguments to gain insights into LLMs' persuasive capabilities without directly engaging in experimentation with humans. We extend a dataset by Durmus and Cardie (2018) with debates, votes, and user traits and propose tasks measuring LLMs' ability to (1) distinguish between strong and weak arguments, (2) predict stances based on beliefs and demographic characteristics, and (3) determine the appeal of an argument to an individual based on their traits. We show that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, surpassing human performance. The data and code released with this paper contribute to the crucial effort of continuously evaluating and monitoring LLMs' capabilities and potential impact. (https://go.epfl.ch/persuasion-llm)</p>
<h2>1 Introduction</h2>
<p>As LLMs rise in capacity and popularity, so has the concern that they may help create and propagate tailor-made, convincing narratives (De Angelis et al., 2023; Buchanan et al., 2021). While "tailormade misinformation" predates LLMs (DiResta et al., 2019), frontier models such as GPT-4, Claude 3, and Gemini 1.5 could add to the problem by allowing malicious actors to easily create diverse, personalized content (Bommasani et al., 2021; Goldstein et al., 2023) or enable the detection (and amplification) of existing content that would be particularly persuasive to individuals with specific demographics or beliefs (Broniatowski et al., 2018).</p>
<p>Previous work has found LLMs to be persuasive in the generative setting (Simchon et al., 2024; Hackenburg and Margetts, 2024; Breum et al., 2024); for example, Salvi et al. (2024) found that,
when provided with personal attributes, GPT-4 outperformed crowdworkers in a debate setting. Yet, assessing models' capacity to generate arguments requires continuous human experimentation as LLMs evolve, which can be time-consuming and resource-intensive. On the contrary, measuring a model's capacity to detect content persuasive to specific demographics can be done quickly and without interaction with human subjects, making it a more efficient approach for benchmarking the persuasive capabilities of LLMs.</p>
<p>Present Work. We study whether LLMs can detect content that would be persuasive to individuals with specific demographics or beliefs. We center our investigation around three research questions. Namely, can LLMs...</p>
<ul>
<li>RQ1: judge the quality of arguments and identify convincing arguments and humans?</li>
<li>RQ2: judge how demographics and beliefs influence people's stances on specific topics?</li>
<li>RQ3: determine how arguments appeal to individuals depending on their demographics?</li>
</ul>
<p>To investigate these questions, we extend a dataset collected by Durmus and Cardie (2018) from a defunct debate platform (debate.org). We annotate 833 politics-related debates with clear propositions, such as "The electoral college should remain unchanged." Each debate contains arguments for ("Pro") and against ("Con") the proposition, along with votes from debate.org participants indicating the winning side. Importantly, the dataset includes demographic information of the voters as well as their stances on 48 so-called "big issues". For 121 debates with 751 votes on three of the most prominent topics in the dataset, we obtained crowdsourced labels to compare the capabilities of LLMs to those of humans. Then, using this enriched dataset, we evaluate the performance of four</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Our approach to study LLMs' persuasiveness capabilities. We measure to which extent LLMs can reproduce human judgments on the quality and persuasiveness of arguments. Suppose LLMs can predict users' positions on stances (e.g., The death penalty should be legal) before and after reading a debate and judge who the better debater was. In that case, they would be well suited to power personalized misinformation and propaganda.</p>
<p>LLMs (GPT-3.5, GPT-4, Llama 2, and Mistral 7B) on three tasks: 1) identifying the side with more convincing arguments (<strong>RQ1</strong>); 2) predicting individuals' stances on specific propositions <strong>before</strong> the debate, given their demographic and basic belief information (<strong>RQ2</strong>); and 3) predicting individuals' stances on specific propositions <em>after</em> the debate, given their demographic and basic belief information (<strong>RQ3</strong>). Figure 1 illustrates our approach.</p>
<p>Our key finding is that LLM exhibits human-like performance across the three proposed tasks. In judging the better debater (<strong>RQ1</strong>), GPT-4 (Accuracy: 60.50%) is as good as an individual voter in the dataset (Accuracy: 60.69%). When predicting users' stances on specific issues before and after reading the debate (<strong>RQ2</strong> and <strong>RQ3</strong>), LLMs again perform similarly to humans. For instance, in the <em>before</em> the debate scenario (<strong>RQ2</strong>), Mistral yields an accuracy of 42.27%, whereas crowdworkers achieve 39.86% (random guessing would yield 33.3% accuracy). However, zero-shot prediction with LLMs still underperforms a supervised machine learning model [XGBoost (Chen and Guestrin, 2016)], which achieves 58.25% accuracy in cross-validation. Nevertheless, stacking the predictions of LLMs and using them as features in a supervised learning setting reduces the performance gap (45.91%).</p>
<p>Overall, our work contributes to the growing body of research on the societal impact of LLMs (Bommasani et al., 2021; Solaiman et al., 2023; Weidinger et al., 2023, <em>inter alia</em>). We shed light on the potential misuse of LLMs by investigating their ability to <em>detect</em> persuasive content tailored to specific demographics.</p>
<h2>2 Related Work</h2>
<p>We review related work in three broad directions closely related to the tasks proposed.</p>
<p><strong>Demographics, beliefs, and persuasion.</strong> Demographics have long been known to impact people's political beliefs and attitudes. Group-level demographic factors such as race, religion, and education shape individuals' perspectives on various political issues and voting behavior in the U.S. context (Campbell et al., 1960; Erikson and Tedin, 2019). For example, 78% of Black, 72% of Asian, and 65% of Hispanic workers see efforts on increasing diversity, equity, and inclusion at work positively, compared to 47% of White workers (Minkin, 2023). Similarly, previous work indicates that persuasion depends on the message recipients' existing values and that individual differences can influence persuasion (O'Keefe, 2015). For instance, Hirsh et al. (2012) demonstrated that tailoring messages to different personality traits can make them more persuasive; Orji et al. (2015) showed that men and women differ significantly in their responsiveness to the different persuasive strategies. Closer to the work at hand, Durmus and Cardie (2018) and Al Khatib et al. (2020) showed considering demographic characteristics can enhance the prediction of argument persuasiveness. However, the extent to which LLMs can utilize demographic characteristics in persuasiveness judgment remains underexplored. In this work, we examine how LLMs can capture the correlations between demographics and beliefs (<strong>RQ2</strong>) and how personal attributes determine the persuasiveness of arguments (<strong>RQ3</strong>).</p>
<p>Argument Mining and Argument quality. Defining argument quality is no easy task, or as persuasion scholars O'Keefe and Jackson (1995) have put it: "there is no clear general abstract characterization of what constitutes argument quality." An argument may be deemed good due to its effectiveness in convincing people (O'Keefe and Jackson, 1995), its cogency from individually accepted premises that lead to a conclusion (Johnson and Blair, 2006), or its reasonableness in contributing to resolving a disagreement (Walton, 2005). Over recent decades, there has been significant interest in automatically extracting arguments from text (Habernal and Gurevych, 2016, 2017; Swanson et al., 2015, inter alia), as detailed in surveys like (Cabrio and Villata, 2018; Lawrence and Reed, 2020). Additionally, research has explored computational argument quality and persuasiveness analysis (Habernal and Gurevych, 2016; Tan et al., 2016; Wachsmuth et al., 2017, inter alia). Contemporary works begin to explore the potential of LLMs in argument quality judgement (Mirzakhmedova et al., 2024; Wachsmuth et al., 2024). Our work complements existing work by examining the extent to which LLMs can identify higher-quality arguments in a debate setting (RQ1) and determine argument effectiveness across individuals with different demographics and beliefs (RQ3).</p>
<h2>Personalized misinformation and propaganda.</h2>
<p>Microtargeting or "personalized persuasion" refers to tailoring the language or content of messages to individuals based on their characteristics (e.g., demographics and prior political beliefs) to make them maximally persuasive. Evidence on the effect of microtargeting is mixed (Guess and Coppock, 2020; Coppock et al., 2020; Matz et al., 2017; Tappin et al., 2023), which has led Teeny et al. (2021) to propose that research on microtargeting should move from "does microtargeting work?", to "when micro-targeting works?" At the same time, the increasing popularity and capabilities of LLMs have raised concerns that they may not only make microtargeting cheaper and more effective but also enable new forms of "microtargeting" misinformation and propaganda, such as through personalized chatbots-see Goldstein et al. (2023) for a comprehensive discussion. These concerns are corroborated by recent studies suggesting that LLMs are capable of generating messages perceived as equally or more persuasive than humans (Bai et al., 2023; Durmus et al., 2024); that they can personal- ize messages to make them more persuasive (Simchon et al., 2024); and that LLMs can successfully persuade humans in debates by exploiting their personal traits (Salvi et al., 2024). One key drawback, though, is that these studies typically involve large and expensive experiments that cannot easily be replicated when a new LLM is released or explore the large hyperparameter space of existing models (e.g., prompting strategy and decoding algorithm). In our work, we argue that we could instead evaluate the effectiveness of LLM in determining whether someone of a specific set of demographic characteristics would find an argument convincing (RQ3) and view this as a proxy of the LLM's ability to perform political microtargeting.</p>
<p>Social sensing. Prediction tasks where individuals are asked to determine the preferences and opinions of others have been broadly referred to as social sensing (Galesic et al., 2021). Previous work using this approach has shown that human social sensing outperformed traditional polling in forecasting elections (Galesic et al., 2018) and that the approach is useful in predicting disease outbreaks (Christakis and Fowler, 2010). Here, the tasks associated with RQ2 and RQ3 are, in their essence, social sensing tasks, as we ask LLMs (and crowdworkers) about the preferences and opinions of others. Although informative, predictions obtained through human social sensing are known to be subjected to biases (Ross et al., 1977; Chambers and Windschitl, 2004), and therefore, it is possible that so are predictions obtained through LLM social sensing.</p>
<h2>3 Data</h2>
<p>Data for this study was collected by Durmus and Cardie (2018) from an online debate platform (debate.org; no longer operational). The platform allowed users to participate in and vote on debates covering a breadth of topics, including politics, religion, and science. Each debate within the dataset consists of multiple rounds, each round with an argument from both the "Pro" and "Con" perspectives. Users on the platform could vote on various aspects of the debate, such as which side they believed provided a more convincing argument. The raw dataset contains 78,376 debates, 45,348 users, and 195,724 votes. Each user has corresponding demographic information, such as gender and age, as well as their stances on 48 so-called "big issues," such as abortion, capital punishment, and national</p>
<table>
<thead>
<tr>
<th>Original Title</th>
<th>Hand-written Proposition</th>
</tr>
</thead>
<tbody>
<tr>
<td>A Debate On The Electoral College</td>
<td>The electoral college should remain unchanged.</td>
</tr>
<tr>
<td>Gay Marriage Should Be Legal</td>
<td>Gay marriage and equal rights.</td>
</tr>
<tr>
<td>US Hegemony</td>
<td>U.S. hegemony is desirable.</td>
</tr>
<tr>
<td>Abortion</td>
<td>Abortion should be illegal.</td>
</tr>
</tbody>
</table>
<p>Table 1: Examples of titles used for debates in the dataset and the corresponding manually written propositions we created to replace them.
health care (see Appendix B for details). Nevertheless, most demographic data is missing from the dataset. Most important to the work at hand, voters had to indicate which side: 1) Made more convincing arguments; 2) They agreed with before the debate; 3) They agreed with after the debate. We measure LLMs' capacity to recognize convincing arguments by predicting the responses to these three questions, each of which could be answered "Pro," "Con," or "Tie." Note that predicting question #1-#3 corresponds to our research questions RQ1-RQ3.</p>
<p>Although each debate in the dataset has a corresponding title indicative of its content, these titles are user-defined and do not always take the form of a proposition. As a result, it is not always clear from reading the debate title alone what the "Pro" and "Con" stances are. Hence, we contribute clear, manually written propositions for 833 debates that (1) were categorized under "Politics," (2) contained at least 300 total tokens (tokens are counted using the tiktoken library with the GPT-3.5-turbo model encoding), (3) contained at least two complete rounds, (4) The debater who spoke the most in the debate did not speak more than $25 \%$ more than the other debater, (5) the debate had at least three votes. We discarded an additional 199 debates that fulfilled the aforementioned criteria but were troll debates (e.g., just profanity toward the other debater), incorrectly categorized as Politics, or impossible to paraphrase into a proposition (see Table 1 for examples).</p>
<p>PoliProp [PP]. We study these 833 annotated debates, considering all votes $(n=4,871)$ in these debates for users with no more than five missing values in demographic information ( 4,871 out of 7,797 ). We also trimmed each debate in the dataset larger than the smallest context window (4096 tokens) among LLMs considered. Trimming is done by removing one round at a time from the end of the debate until the token count is small enough, an approach that equally penalizes both debaters (unlike simply removing tokens at the end of the debate). Hereafter, we call this the PoliProp dataset.</p>
<p>PoliIssues [IS]. We also separately consider all debates on abortion $(n=50)$, gay marriage $(n=$ 51), and capital punishment $(n=31)$, the most prominent topics in the dataset. Given that debates within the three themes are similar, we use this data to compare LLM performances with traditional machine learning methods, predicting participants' votes using their demographic and stances on big issues as features. To obtain a human baseline, we collect crowdsourced labels using Amazon Mechanical Turk (MTurk) for each of the 751 votes cast on these 121 debates. Crowdworkers are essentially presented with the same questions as the LLMs. Given a debate, we ask who gave the better arguments. Given a set of characteristics by a voter as well as the debate, we ask whether the voter would have agreed with the proposition before and after reading the debate. Hereafter, we call this dataset the PoliIssues dataset. For more information on crowdsourcing, see Appendix C.</p>
<h2>4 Methods</h2>
<p>LLMs considered. For this study, we compare the performance of two open-source LLMs, namely Mistral 7B (Mistral-7b-Instruct-v0.1) and Meta's Llama 2 70B (Llama-2-70b-chat), with OpenAI's closed-source GPT-3.5 (gpt-3.5-turbo-1106) and GPT-4 (gpt-4-0613). We use the standard temperatures for each model.</p>
<p>Prompting. We follow Staab et al. (2023) to develop our prompt: each had a system role, context, question, and constraint. We experimented with different structures and found that, overall, the structure mattered little as long as the wording was clear and concise. Since we had three research</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Prompt structure used in RQ1.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Prompt structure used in RQ2.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Prompt structure used in RQ3.</p>
<p>questions to answer, we had three prompt structures that combined the debate proposition, the debate itself, and user demographics. We show the prompt structure used for RQ1 in Figure 2. All prompts indicated that LLMs should respond only with the labels "Pro," "Con," or "Tie". Nevertheless, many of the models failed to adhere to this instruction, necessitating post-processing to extract the actual answer. Generally, instances of incorrect responses involved the answer accompanied by additional spaces or punctuation or presented in a complete sentence format, such as: "Based on the given demographics, the person is most likely to agree with the 'Con' side in the debate." We used heuristics to extract the responses in these cases. Additionally, there were occasions when the LLMs failed to produce any answer, resulting in responses akin to: "I cannot determine the person's position in the debate without additional information." We depict the remaining prompts in Figures 3 and 4 and provide further details in Appendix A.</p>
<p>Evaluation. We evaluate the accuracy of language models by comparing the answers they provide with the ground truth data from PoliProp and PoliIssues. We obtain confidence intervals through bootstrapping. Besides considering each LLM individually, we also consider the performance of stacked LLM predictions, obtained by using the output of different LLMs as features in a supervised machine learning model (Hastie et al., 2009).</p>
<p>Baselines/Benchmarks We interpret the LLM accuracies by establishing the following baselines and benchmarks as metrics of comparison:</p>
<ul>
<li>Random; (RQ1—RQ3) Since there are three possible stances (Pro, Con, and Tie) for any given task and each debate and voter pair, the random baseline has an accuracy of $33.3 \%$.</li>
<li>Majority; (RQ1) For RQ1, the ground truth was established by aggregating the votes for who made more convincing arguments in each debate through a simple majority vote. The Majority benchmark is the percentage of users in our dataset that agreed with the computed ground truth for this question.</li>
<li>
<p>MTurk; (RQ2-RQ3) We crowdsourced the tasks for each research question for the PoliIssues dataset, obtaining a human equivalent answer to the questions we asked the LLMs. These are detailed in Appendix C.</p>
</li>
<li>
<p>XGBoost: (RQ2) For each issue (abortion, gay marriage, capital punishment) in the PoliIssues dataset, we train a Gradient Boosting classifier to predict the stance of a user as in RQ2. We train one model separately per issue since labels are not equivalent (e.g., Proabortion differs from Pro-capital punishment), but we report the aggregated accuracy.</p>
</li>
</ul>
<h2>5 Results</h2>
<p>Judging argument quality (RQ1). Considering the PoliProp dataset [PP], we summarize the accuracy of the different LLMs and baseline methods in determining argument quality in Table 2 (rows #1-#7). We find a substantial performance gap between GPT-4 ( $60.50 \%$ accuracy) and the other models, e.g., Llama 2, which performs worse than random guessing ( $24.91 \%$ ). GPT-4 performance is similar to human performance, as measured by the agreement of any individual vote with the remaining votes in each debate (Majority; 60.69\%).</p>
<p>Correlating beliefs and demographic characteristics with stances (RQ2). Considering the PoliIssues dataset [IS], we summarize the accuracy of the different LLMs and baseline methods in correlating beliefs and demographic characteristics with stances on Table 2 (rows #8-#15). Here, the accuracy range of different LLMs is much more narrow, ranging from $41.39 \%$ (Mistral) to $42.82 \%$ (GPT4). Most important, however, is that the performance of LLMs is similar to that of crowdworkers (39.32\%; MTurk).</p>
<p>Recognizing convincing arguments (RQ3). Again, considering the PoliIssues dataset [IS], we summarize the accuracy of the different methods in recognizing users' opinion after reading the debate on Table 2 (rows #16-#22). Different models perform similarly on the task and similar to crowdworkers, e.g., GPT-4: $44.38 \%$ of vs. crowdworkers: $39.86 \%$.</p>
<p>LLMs vs. supervised learning. Considering RQ2, we train a Gradient Boosting classifier to predict stances given user traits (row #14). We run a 20 -fold cross-validation and report the mean accuracy. This model performs significantly better than LLMs at predicting stances (Accuracy: 58.25\%; $95 \%$ CI: $[54.02,62.47])$.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Inter-annotator agreement for different models in RQ2.</p>
<p>Sensitivity to prompt. We study whether the results obtained were sensitive to the prompt used by re-running the analysis from RQ2 on the PoliIssues dataset. For each model, we rerun the analysis considering the "big issues" in user profiles and/or asking for models to reason before answering. ${ }^{1} \mathrm{Re}$ sults are shown in Table 3. Overall, we find that the results are not sensitive to the experimented changes.</p>
<p>Stacking LLMs. While the performance of language models is similar in RQ2, we find that their inter-annotator agreement is quite low (Cohen's $\kappa$ is smaller than 0.2 for most pairs of models, see Figure 5). This is surprising since, upon our inspection of the reasoning different LLMSs' provided for their answers, all made similar assumptions. Nevertheless, each model seems to perform well on a different subset of the debates. This motivated us to experiment with stacking LLMs, i.e., using the outputs of the different large language models outlined above as input to a simple logistic regression model. We find this strategy yields a small boost in accuracy in RQ1 (see row #5; Table 2), but a substantial one in RQ2 and RQ3 (see rows</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">#</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Accuracy (\%)</th>
<th style="text-align: center;">95\% Confidence Interval</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">RQ1</td>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">PP</td>
<td style="text-align: center;">24.91</td>
<td style="text-align: center;">$(20.65,26.53)$</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mistral 7B</td>
<td style="text-align: center;">PP</td>
<td style="text-align: center;">37.69</td>
<td style="text-align: center;">$(32.89,39.26)$</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">PP</td>
<td style="text-align: center;">42.74</td>
<td style="text-align: center;">$(39.38,46.1)$</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">PP</td>
<td style="text-align: center;">60.50</td>
<td style="text-align: center;">$(57.26,63.87)$</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Stacked</td>
<td style="text-align: center;">PP</td>
<td style="text-align: center;">61.94</td>
<td style="text-align: center;">$(58.54,65.34)$</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Majority</td>
<td style="text-align: center;">PP</td>
<td style="text-align: center;">60.69</td>
<td style="text-align: center;">$(59.56,61.79)$</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">PP</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">RQ2</td>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">IS</td>
<td style="text-align: center;">41.56</td>
<td style="text-align: center;">$(38.16,45.1)$</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mistral 7B</td>
<td style="text-align: center;">IS</td>
<td style="text-align: center;">41.39</td>
<td style="text-align: center;">$(38.4,44.86)$</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">IS</td>
<td style="text-align: center;">41.73</td>
<td style="text-align: center;">$(38.52,44.98)$</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">IS</td>
<td style="text-align: center;">42.82</td>
<td style="text-align: center;">$(39.59,46.53)$</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MTurk</td>
<td style="text-align: center;">IS</td>
<td style="text-align: center;">39.32</td>
<td style="text-align: center;">$(35.89,42.88)$</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Stacked</td>
<td style="text-align: center;">IS</td>
<td style="text-align: center;">45.91</td>
<td style="text-align: center;">$(40.02,51.81)$</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">IS</td>
<td style="text-align: center;">55.34</td>
<td style="text-align: center;">$(50.45,60.22)$</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">IS</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">RQ3</td>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">IS</td>
<td style="text-align: center;">41.24</td>
<td style="text-align: center;">$(37.2,44.02)$</td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mistral 7B</td>
<td style="text-align: center;">IS</td>
<td style="text-align: center;">42.28</td>
<td style="text-align: center;">$(32.06,38.28)$</td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">IS</td>
<td style="text-align: center;">38.97</td>
<td style="text-align: center;">$(35.41,42.11)$</td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">IS</td>
<td style="text-align: center;">44.38</td>
<td style="text-align: center;">$(40.91,47.73)$</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MTurk</td>
<td style="text-align: center;">IS</td>
<td style="text-align: center;">39.86</td>
<td style="text-align: center;">$(36.44,43.42)$</td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Stacked</td>
<td style="text-align: center;">IS</td>
<td style="text-align: center;">46.86</td>
<td style="text-align: center;">$(41.17,52.55)$</td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">IS</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 2: Key results for RQ1-RQ3. We show that LLMs perform on par with humans across various tasks related to recognizing convincing arguments. When stacked using a logistic regression, LLMs outperform humans in predicting stances on prepositions before and after the debate (RQ2, RQ3). The random baseline has accuracy of $33.33 \%$ for all settings.
#13 and #21). Indeed, in this scenario, the accuracy is significantly better than crowdworkers for both research questions $(p&lt;0.05)$. Note that the accuracy reported for the stacked model is the average of a 20 -fold cross-validation.</p>
<h2>6 Discussion and Conclusion</h2>
<p>Here we studied LLM's persuasive capabilities by considering its ability to identify convincing arguments in general and for people with specific arguments. We argue that if LLMs can detect content that is highly persuasive to specific demographics, they may be used to detect and amplify tailor-made misinformation and propaganda. Our findings indicate that LLMs demonstrate human-level performance in (1) judging argument quality, (2) predicting users' stances on specific topics given users' demographics and basic beliefs, and (3) detecting arguments that would be persuasive to individuals with specific demographics or beliefs.</p>
<p>However, the overall human performance is not high in each of the three tasks [around $60 \%$ for (1), and around $40 \%$ for (2) and (3)], which could be due to the inherent difficulty of the tasks, as well as variance and randomness in the data. This does not necessarily imply that LLMs do not pose any additional risk of tailor-made misinformation in the future. It is plausible that with access to more personal information about an individual, such as personality traits, LLMs could perform better at detecting persuasive arguments (Hirsh et al., 2012). Nevertheless, it is important to consider that the more fine-grained the target, the harder and more costly it becomes to reach the targeted population, and the cost-benefit analysis is not straightforward (Tappin et al., 2023).</p>
<p>One hypothesis that could explain the relatively low accuracy for both LLMs and human performance is that these demographic questions and big-issue stances may not be highly relevant for the task, as suggested by Hu and Collier (2024).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Big Issues</th>
<th style="text-align: center;">Reasoning</th>
<th style="text-align: center;">Accuracy (\%)</th>
<th style="text-align: center;">$95 \%$ CI</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Llama 2</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">41.30</td>
<td style="text-align: center;">$(37.92,44.38)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">40.05</td>
<td style="text-align: center;">$(36.48,43.3)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">38.92</td>
<td style="text-align: center;">$(34.81,41.51)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">37.38</td>
<td style="text-align: center;">$(29.07,35.17)$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral 7B</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">40.67</td>
<td style="text-align: center;">$(37.2,44.02)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">41.83</td>
<td style="text-align: center;">$(38.04,44.98)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">40.60</td>
<td style="text-align: center;">$(36.96,44.14)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">40.91</td>
<td style="text-align: center;">$(36.6,43.18)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">42.94</td>
<td style="text-align: center;">$(39.83,46.17)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">39.45</td>
<td style="text-align: center;">$(36.0,42.58)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">41.80</td>
<td style="text-align: center;">$(38.28,45.1)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">37.80</td>
<td style="text-align: center;">$(34.57,41.03)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">42.70</td>
<td style="text-align: center;">$(39.47,46.17)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">43.30</td>
<td style="text-align: center;">$(39.95,46.65)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">42.46</td>
<td style="text-align: center;">$(39.11,45.93)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">45.03</td>
<td style="text-align: center;">$(41.51,48.09)$</td>
</tr>
</tbody>
</table>
<p>Table 3: We repeat the analysis to answer RQ2 using the PoliIssues dataset but varying the prompt, either by considering big issues in the prompt (Big Issues) or by asking the LLM to reason before answering the question (Reasoning). The scenario without 'Big issues' or 'Reasoning' corresponds to lines #8-10 in Table 2,</p>
<p>However, this is contradicted by the fact that a supervised XGBoost model trained with these factors yields much better results. Interestingly, stacking various LLM predictions yields performance closer to XGBoost. This indicates that while an individual LLM may not excel at detecting persuasive arguments for an individual, combining the predictions of several LLMs could achieve much more competitive performance (perhaps because each LLM's biases differ). Consequently, LLMs can potentially detect highly effective tailored misinformation and propaganda, particularly in a multiagent setting (Schoenegger et al., 2024).</p>
<h2>Limitations</h2>
<p>Our dataset, from debate.org, may not be representative of the general population. The demographics of individuals opting to participate in online debates are likely skewed compared to the U.S. population and even more so globally. Additionally, we could not test the language models on non-English data due to data access limitations. However, recent research has shown that language models' performance is considerably lower for non-English languages, especially low-resource ones (Ahuja et al., 2023). Consequently, it is plausible that the risk of misuse for microtargeting in non-English settings
is currently lower. Nevertheless, as language models continue to improve, it is crucial to expand this line of research to a wider range of languages and demographics to ensure a comprehensive understanding of the risks associated with personalized persuasion. It is also essential to conduct empirical studies to understand whether LLMs are, in fact, being used for persuasion in online settings (e.g., in social media platforms). Another limitation of the work at hand is that the LLMs studied might have seen content from debate.org in their training data. To address this concern, we queried 100 debate excerpts from the dataset using GPT4 and couldn't obtain complete samples. Yet, this is not sufficient to rule out this possibility.</p>
<h2>Ethical Considerations</h2>
<p>In this study, we employ demographic and beliefrelated questions drawn from datasets that are publicly accessible and have been anonymized before release. It is crucial to emphasize the importance of responsible development and deployment of LLMs and the need for ongoing research into mitigating their potential risks (Bommasani et al., 2021). Our work can inform the development of safeguards and countermeasures against the misuse of LLMs for personalized misinformation and propaganda.</p>
<p>In this study, we employed crowdsourcing to evaluate the persuasiveness of debates. We paid crowd workers, all based in the U.S., at a rate of $\$ 12.00$ per hour, higher than the federal minimum wage in the United States.</p>
<h2>References</h2>
<p>Kabir Ahuja et al. 2023. MEGA: Multilingual evaluation of generative AI. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4232-4267, Singapore. Association for Computational Linguistics.</p>
<p>Khalid Al Khatib, Michael Völske, Shahbaz Syed, Nikolay Kolyada, and Benno Stein. 2020. Exploiting personal characteristics of debaters for predicting persuasiveness. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7067-7072.</p>
<p>Hui Bai, Jan G. Voelkel, johannes Christopher Eichstaedt, and Robb Willer. 2023. Artificial intelligence can persuade humans on political issues.</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.</p>
<p>Simon Martin Breum, Daniel Vædele Egdal, Victor Gram Mortensen, Anders Giovanni Møller, and Luca Maria Aiello. 2024. The Persuasive Power of Large Language Models. Proceedings of the International AAAI Conference on Web and Social Media, 18:152-163.</p>
<p>David A Broniatowski, Amelia M Jamison, SiHua Qi, Lulwah AlKulaib, Tao Chen, Adrian Benton, Sandra C Quinn, and Mark Dredze. 2018. Weaponized health communication: Twitter bots and russian trolls amplify the vaccine debate. American journal of public health, 108(10):1378-1384.</p>
<p>Ben Buchanan, Andrew Lohn, Micah Musser, and Katerina Sedova. 2021. Truth, lies, and automation. Center for Security and Emerging Technology, 1(1):2.</p>
<p>Elena Cabrio and Serena Villata. 2018. Five years of argument mining: A data-driven analysis. In IJCAI, volume 18, pages 5427-5433.</p>
<p>Angus Campbell, Philip E. Converse, Warren E. Miller, and Donald E. Stokes. 1960. The American Voter. Wiley.</p>
<p>John R Chambers and Paul D Windschitl. 2004. Biases in social comparative judgments: the role of nonmotivated factors in above-average and comparative-optimism effects. Psychological bulletin, 130(5):813.</p>
<p>Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16, page 785-794, New York, NY, USA. Association for Computing Machinery.</p>
<p>Nicholas A Christakis and James H Fowler. 2010. Social network sensors for early detection of contagious outbreaks. PloS one, 5(9):e12948.</p>
<p>Alexander Coppock, Seth J Hill, and Lynn Vavreck. 2020. The small effects of political advertising are small regardless of context, message, sender, or receiver: Evidence from 59 real-time randomized experiments. Science advances, 6(36):eabc4046.</p>
<p>Luigi De Angelis, Francesco Baglivo, Guglielmo Arzilli, Gaetano Pierpaolo Privitera, Paolo Ferragina, Alberto Eugenio Tozzi, and Caterina Rizzo. 2023. Chatgpt and the rise of large language models: the new ai-driven infodemic threat in public health. Frontiers in Public Health, 11:1166120.</p>
<p>Renee DiResta, Kris Shaffer, Becky Ruppel, David Sullivan, Robert Matney, Ryan Fox, Jonathan Albright, and Ben Johnson. 2019. The tactics \&amp; tropes of the internet research agency.</p>
<p>Esin Durmus and Claire Cardie. 2018. Exploring the role of prior beliefs for argument persuasion. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers).</p>
<p>Esin Durmus, Liane Lovitt, Alex Tamkin, Stuart Ritchie, Jack Clark, and Deep Ganguli. 2024. Measuring the persuasiveness of language models.</p>
<p>Robert S Erikson and Kent L Tedin. 2019. American public opinion: Its origins, content, and impact. Routledge.</p>
<p>Mirta Galesic, Wāndi Bruine de Bruin, Marion Dumas, Arie Kapteyn, JE Darling, and Erik Meijer. 2018. Asking about social circles improves election predictions. Nature Human Behaviour, 2(3):187-193.</p>
<p>Mirta Galesic et al. 2021. Human social sensing is an untapped resource for computational social science. Nature, 595(7866):214-222.</p>
<p>Josh A Goldstein, Girish Sastry, Micah Musser, Renee DiResta, Matthew Gentzel, and Katerina Sedova. 2023. Generative language models and automated influence operations: Emerging threats and potential mitigations. arXiv preprint arXiv:2301.04246.</p>
<p>Andrew Guess and Alexander Coppock. 2020. Does counter-attitudinal information cause backlash? results from three large survey experiments. British Journal of Political Science, 50(4):1497-1515.</p>
<p>Ivan Habernal and Iryna Gurevych. 2016. What makes a convincing argument? empirical analysis and detecting attributes of convincingness in web argumentation. In Proceedings of the 2016 conference on empirical methods in natural language processing, pages 1214-1223.</p>
<p>Ivan Habernal and Iryna Gurevych. 2017. Argumentation mining in user-generated web discourse. Computational Linguistics, 43(1):125-179.</p>
<p>Kobi Hackenburg and Helen Margetts. 2024. Evaluating the persuasive influence of political microtargeting with large language models. Proceedings of the National Academy of Sciences, 121(24):e2403116121.</p>
<p>Trevor Hastie, Robert Tibshirani, Jerome Friedman, Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 2009. Model inference and averaging. The elements of statistical learning: Data mining, inference, and prediction, pages 261-294.</p>
<p>Jacob B Hirsh, Sonia K Kang, and Galen V Bodenhausen. 2012. Personalized persuasion: Tailoring persuasive appeals to recipients' personality traits. Psychological science, 23(6):578-581.</p>
<p>Tiancheng Hu and Nigel Collier. 2024. Quantifying the persona effect in llm simulations. arXiv preprint arXiv:2402.10811.</p>
<p>Ralph Henry Johnson and J Anthony Blair. 2006. Logical self-defense. Idea.</p>
<p>John Lawrence and Chris Reed. 2020. Argument mining: A survey. Computational Linguistics, 45(4):765818 .</p>
<p>Sandra C Matz, Michal Kosinski, Gideon Nave, and David J Stillwell. 2017. Psychological targeting as an effective approach to digital mass persuasion. Proceedings of the national academy of sciences, 114(48):12714-12719.</p>
<p>Rachel Minkin. 2023. Diversity, equity and inclusion in the workplace: A survey report.</p>
<p>Nailia Mirzakhmedova, Marcel Gohsen, Chia Hao Chang, and Benno Stein. 2024. Are large language models reliable argument quality annotators? arXiv preprint arXiv:2404.09696.</p>
<p>Daniel J O’Keefe. 2015. Persuasion: Theory and research. Sage Publications.</p>
<p>Rita Orji, Regan L Mandryk, and Julita Vassileva. 2015. Gender, age, and responsiveness to cialdini's persuasion strategies. In Persuasive Technology: 10th International Conference, PERSUASIVE 2015, Chicago, IL, USA, June 3-5, 2015, Proceedings 10, pages 147159. Springer.</p>
<p>Daniel J O’Keefe and Sally Jackson. 1995. Argument quality and persuasive effects: A review of current approaches. In Argumentation and values: Proceedings of the ninth Alta conference on argumentation, pages $88-92$.</p>
<p>Lee Ross, David Greene, and Pamela House. 1977. The "false consensus effect": An egocentric bias in social perception and attribution processes. Journal of experimental social psychology, 13(3):279-301.</p>
<p>Francesco Salvi, Manoel Horta Ribeiro, Riccardo Gallotti, and Robert West. 2024. On the conversational persuasiveness of large language models: A randomized controlled trial. arXiv preprint arXiv:2403.14380.</p>
<p>Philipp Schoenegger, Indre Tuminauskaite, Peter S Park, and Philip E Tetlock. 2024. Wisdom of the silicon crowd: Llm ensemble prediction capabilities match human crowd accuracy. arXiv preprint arXiv:2402.19379.</p>
<p>Almog Simchon, Matthew Edwards, and Stephan Lewandowsky. 2024. The persuasive effects of political microtargeting in the age of generative artificial intelligence. PNAS Nexus, 3(2):pgae035.</p>
<p>Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahmad, Dylan Baker, Su Lin Blodgett, Hal Daumé III, Jesse Dodge, Ellie Evans, Sara Hooker, et al. 2023. Evaluating the social impact of generative ai systems in systems and society. arXiv preprint arXiv:2306.05949.</p>
<p>Robin Staab, Mark Vero, Mislav Balunović, and Martin Vechev. 2023. Beyond memorization: Violating privacy via inference with large language models. Preprint, arXiv:2310.07298.</p>
<p>Reid Swanson, Brian Ecker, and Marilyn Walker. 2015. Argument mining: Extracting arguments from online dialogue. In Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 217-226, Prague, Czech Republic. Association for Computational Linguistics.</p>
<p>Chenhao Tan, Vlad Niculae, Cristian Danescu-Niculescu-Mizil, and Lillian Lee. 2016. Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions. In Proceedings of the 25th international conference on world wide web, pages 613-624.</p>
<p>Ben M Tappin, Chloe Wittenberg, Luke B Hewitt, Adam J Berinsky, and David G Rand. 2023. Quantifying the potential persuasive returns to political microtargeting. Proceedings of the National Academy of Sciences, 120(25):e2216261120.</p>
<p>Jacob D Teeny, Joseph J Siev, Pablo Briñol, and Richard E Petty. 2021. A review and conceptual framework for understanding personalized matching effects in persuasion. Journal of Consumer Psychology, 31(2):382-414.</p>
<p>Henning Wachsmuth, Gabriella Lapesa, Elena Cabrio, Anne Lauscher, Joonsuk Park, Eva Maria Vecchi, Serena Villata, and Timon Ziegenbein. 2024. Argument quality assessment in the age of instructionfollowing large language models. In Proceedings of</p>
<p>the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1519-1538, Torino, Italia. ELRA and ICCL.</p>
<p>Henning Wachsmuth, Nona Naderi, Yufang Hou, Yonatan Bilu, Vinodkumar Prabhakaran, Tim Alberdingk Thijm, Graeme Hirst, and Benno Stein. 2017. Computational argumentation quality assessment in natural language. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 176-187.</p>
<p>Douglas Walton. 2005. Fundamentals of critical argumentation. Cambridge University Press.</p>
<p>Laura Weidinger et al. 2023. Sociotechnical safety evaluation of generative AI systems. arXiv preprint arXiv:2310.11986.</p>
<h2>A Prompts</h2>
<p>In all tasks conducted for this study, the LLMs were prompted to respond to questions using only one of the options: "Pro," "Con," or "Tie," without any additional words or punctuation. Nevertheless, many of the models failed to adhere to this instruction, necessitating post-processing to extract the actual answer. Generally, incorrect responses involved the answer being accompanied by additional spaces or punctuation or presented in a complete sentence format, such as: "Based on the given demographics, the person is most likely to agree with the 'Con' side in the debate." ${ }^{2}$ To extract the answer in these cases, we used a simple Regex expression, finding the first occurrence of the words "Pro," "Con," or "Tie."</p>
<p>Table 4 shows what percentage of responses followed instructions and the corresponding percentage from which answers could be successfully extracted for the PoliProp dataset across each question. The answer extracted percentage indicates the highest achievable accuracy for each model in its results. Notably, both open-source models encountered challenges in complying with the instructions, with particular difficulty in addressing Q3.</p>
<h2>B Demographics and Big Issues</h2>
<p>The dataset by Durmus and Cardie (2018) contained the following demographic information about participants: birthday, education, ethnicity,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>gender, income, party, political ideology, religious ideology.</p>
<p>It also contained participants' opinions on socalled "big issues." They were: abortion, affirmative action, animal rights, Barack Obama, border fence, capitalism, civil unions, death penalty, drug legalization, electoral college, environmental protection, estate tax, European Union, euthanasia, federal reserve, flat tax, free trade, gay marriage, global warming exists, globalization, gold standard, gun rights, homeschooling, Internet censorship, Iran-Iraq war, labor union, legalized prostitution, Medicaid and medicare, medical marijuana, military intervention, minimum wage, national health care, national retail sales tax, occupy movement, progressive tax, racial profiling, redistribution, smoking ban, social programs, social security, socialism, stimulus spending, term limits, torture, United Nations, war in Afghanistan, war on terror, and welfare.</p>
<h2>C Crowdsourcing</h2>
<p>We recruited participants for our study through Amazon Mechanical Turk between December 2023 and March 2024, requiring that they be 18+ years old, located in the US, and have a master's qualification provided by Amazon. The study was paid $\$ 2.25$ and had a median completion time of 11 minutes, corresponding to a pay rate of about $\$ 12.00 /$ hour. To ensure the quality of answers, we asked users to justify their responses to each question. We then manually assessed the responses and considered them to be high-quality. We reproduce the crowdsourcing questions on the next page. We also provide an example justification below.</p>
<ul>
<li>S#1: Being a Democrat and to a lesser extent white and female all correlate with being proLGBTQ.</li>
<li>S#3: The con side goes off on an unusual, libertarian leaning bend that probably just wouldn't appeal to this type of person who would simply connect with the pro side more.</li>
<li>S#3: The con side argues less directly about this particular topic and more about some kind of libertarian; the state should have nothing to do with any of this kind of thing, which just isn't as compelling as the pro side making clear why gay people should be integrated into the current system. The con side also repeatedly appeals to</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Correct Form (\%)</th>
<th style="text-align: center;">Answer Extracted (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">99.88</td>
<td style="text-align: center;">100.00</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">99.06</td>
<td style="text-align: center;">100.00</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">95.08</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Mistral</td>
<td style="text-align: center;">62.76</td>
<td style="text-align: center;">95.55</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">99.71</td>
<td style="text-align: center;">99.77</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">99.80</td>
<td style="text-align: center;">99.80</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">97.17</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Mistral</td>
<td style="text-align: center;">67.14</td>
<td style="text-align: center;">100.00</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">99.82</td>
<td style="text-align: center;">99.94</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">99.61</td>
<td style="text-align: center;">99.98</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">97.17</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Mistral</td>
<td style="text-align: center;">17.19</td>
<td style="text-align: center;">81.48</td>
</tr>
</tbody>
</table>
<p>Table 4: Some models had difficulty following instructions and giving the answer in the correct form of either "Pro," "Con," or "Tie." In this table, we see what percentage of the answers were given in the correct form and what percentages contained an answer after processing the result for the PoliProp dataset.
some really weak slippery slope stuff and doesn't engage well with how the pro side responds.</p>
<h2>Subtask 1</h2>
<p>Read the following proposition, i.e., a statement that affirms or denies something.</p>
<p>Proposition: Gay marriage should be legal.
Consider an individual with the following demographic characteristics.</p>
<ol>
<li>Education: Graduate Degree</li>
<li>Gender: Female</li>
<li>Party: Undecided</li>
<li>Political Ideology: Progressive</li>
<li>
<p>Religious Ideology: Christian</p>
</li>
<li>
<p>In your opinion, would this person agree (Pro), disagree (Con), or be neutral or undecided (Tie) with the proposition?</p>
</li>
<li>Write a brief justification for your answer. A sensible justification is required for your HIT to get approved.</li>
</ol>
<h2>Subtask 2</h2>
<p>Consider the following debate on the proposition, where one individual argues for the proposition (Pro) and another against (Con).</p>
<p>Proposition: Gay marriage should be legal.</p>
<h2>[debate]</h2>
<p>Consider an individual with the following demographic characteristics.</p>
<ol>
<li>Education: Graduate Degree</li>
<li>Gender: Female</li>
<li>Party: Undecided</li>
<li>Political Ideology: Progressive</li>
<li>
<p>Religious Ideology: Christian</p>
</li>
<li>
<p>Given this information, what stance do you think this person would take on the above proposition after reading the debate? Answer the same as before if you believe the debate had no effect on their opinion, and choose a different answer if you believe the debate had an effect on their opinion.</p>
</li>
<li>Write a brief justification for your answer. A sensible justification is required for your HIT to get approved.</li>
</ol>
<h2>Subtask 3</h2>
<p>Again, consider the same debate on the proposition.</p>
<p>Proposition: Gay marriage should be legal.</p>
<h2>[debate]</h2>
<ul>
<li>Disregarding your own point of view on the debate, please determine which debater you believe had more convincing arguments. The individual arguing for the proposition (Pro) or against it (Con)? If both were similarly convincing, indicate that it was a "Tie."</li>
<li>Write a brief justification for your answer. A sensible justification is required for your HIT to get approved.</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ There were occasions when the LLMs failed to produce any answer, resulting in responses akin to: "I cannot determine the person's position in the debate without additional information."&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
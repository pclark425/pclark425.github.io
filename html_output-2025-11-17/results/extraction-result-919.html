<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-919 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-919</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-919</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-0a2c3e734349232781eb95319dc41f8d8fb671a0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0a2c3e734349232781eb95319dc41f8d8fb671a0" target="_blank">SmartPlay : A Benchmark for LLMs as Intelligent Agents</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> SmartPlay is introduced: both a challenging benchmark and a methodology for evaluating LLMs as agents, which serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies</p>
                <p><strong>Paper Abstract:</strong> Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. We release our benchmark at github.com/Microsoft/SmartPlay</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e919.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e919.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SmartPlay</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SmartPlay benchmark for LLMs as intelligent agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A suite of 6 text-augmented games (Two-Armed Bandits, Rock-Paper-Scissors, Tower of Hanoi, Messenger, Crafter, Minecraft) and an evaluation methodology to measure LLMs' interactive/agent capabilities (planning, spatial reasoning, learning from interactions, error handling, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>SmartPlay benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unified OpenAI Gym-style interface that provides manuals, textual observations (including directional visual descriptors for 2D/3D games), history windows, and flat categorical actions to evaluate LLMs acting as agents across stochastic and deterministic environments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Bandits, RockPaperScissors, Hanoi, Messenger (L1/L2), Crafter, Minecraft</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>sequential decision-making, planning, multi-step reasoning, learning from interactions, 2D/3D embodied navigation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper discusses multiple reasons for LLMs' poorer interactive performance relative to their strengths on static QA-style tasks: (1) existing LLM benchmarks focus on static knowledge/reasoning and do not capture long-horizon planning, spatial reasoning, handling randomness, or learning from interaction; (2) possible dataset/environment contamination leads LLMs to know canonical starting solutions but not intermediate states (e.g., Tower of Hanoi intermediate configurations not seen in training); (3) weakness in tracking intermediate state and memoizing multi-step trajectories; (4) poor spatial reasoning and inconsistent action sequences; (5) limited ability to learn from interactions in long-horizon procedurally-generated worlds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SmartPlay : A Benchmark for LLMs as Intelligent Agents', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e919.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e919.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT prompting + action selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought prompting followed by constrained action selection prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experimental prompting strategy used in the paper: first ask the LLM to 'think step by step' for the next action, then explicitly ask it to choose the best executable action from a provided action list, mapping its output to environment actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Applied across all SmartPlay games (Bandits, RPS, Hanoi, Messenger, Crafter, Minecraft)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>prompting strategy applied to planning, multi-step reasoning, sequential decision-making, embodied navigation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Used in all experiments reported in the paper; reported model scores (per-model per-game) are obtained with this prompting protocol (see Table 2). No ablation (before/after) numbers for removing this prompting are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>chain-of-thought style prompting (eliciting intermediate reasoning before action selection)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Two-step prompt: (1) 'What is the next action to take, let's think step by step.' with manual, history, observation context; (2) 'Choose the best executable action from the list of all actions. Write the exact chosen action.' to force mapping to environment action.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>While CoT prompting helps elicit reasoning, the paper notes that even with CoT the top models (GPT-4 variants) still underperform humans on long-horizon planning, spatial reasoning, and complex procedural tasks, suggesting prompting alone is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SmartPlay : A Benchmark for LLMs as Intelligent Agents', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e919.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e919.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Crafter context string</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Crafter manual/context string (parsed LaTeX 'context' augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An augmented, machine-parsed context string derived from the Crafter environment source (IATEX), provided as extra manual/context to the LLM to improve understanding of game dynamics and crafting requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Crafter</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>open-world sequential decision-making, multi-step crafting/planning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>The paper reports experiments using this 'context' string and notes (citing Wu et al. (2023c)) that it 'greatly improve[s] performance of GPT-4 and text-davinci-003 on Crafter'; absolute normalized scores with the context are in Table 2 (GPT-4-0613: 0.26, GPT-4-0314: 0.32, text-davinci-003: 0.07), but no before/after numbers in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting / context augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Provide a machine-parsed, detailed environment 'context' string (extracted from the Crafter paper/source) in the manual input to the LLM so it has more procedural and rule information about crafting and achievements.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Reported (via citation) to substantially improve performance for some LLMs on Crafter; this paper reports absolute normalized scores obtained while using the context but does not provide explicit before/after ablation numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Lack of explicit procedural knowledge in the raw manual/observation can limit agent performance; adding a detailed context supplies missing rules/requirements and partially closes the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SmartPlay : A Benchmark for LLMs as Intelligent Agents', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e919.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e919.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Directional visual descriptor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Directional textual visual descriptor for 2D/3D observations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observation engineering technique that converts raw dense environment observations (MineDojo block matrices / segmentation) into a textual list of grouped objects with relative directional positions and screen-space percentages to make visual scenes comprehensible to LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Messenger, Crafter, Minecraft (visual/text conversion applied)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>embodied navigation, 2D/3D spatial reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Used as the canonical observation representation for visual games in SmartPlay; enables LLMs to act in Minecraft/Crafter/Messenger but per-model performance remains substantially below human baseline on complex spatial tasks (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>input representation / observation engineering</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Group connected components of same block/object, describe each group's closest block's relative position (e.g., 'grass, 3.87 blocks away to your bottom, taking 38% of screen') to supply spatial information as text to LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Enables LLMs to make meaningful progress in visual/3D tasks that they could not access otherwise; nevertheless, the paper reports continued poor performance on 3D spatial reasoning (Minecraft) even with this representation.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Even when observations are textualized, LLMs show inconsistent trajectory-level spatial reasoning (e.g., oscillating contradictory moves), indicating deficits beyond raw perceptual encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SmartPlay : A Benchmark for LLMs as Intelligent Agents', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e919.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e919.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (0613 variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary GPT-4 variant evaluated as an LLM agent across all SmartPlay games; achieved the highest scores among evaluated models but still underperformed humans on complex interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary transformer-based large language model (GPT-4 family). Paper does not report architecture details beyond being a GPT-4 variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Bandits, RockPaperScissors, Hanoi (3-disk), MessengerL1, MessengerL2, Crafter, Minecraft</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>sequential decision-making, planning, multi-step reasoning, 2D/3D navigation, learning from interactions</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Normalized human-relative scores (Table 2): Bandit 1.00, RPS 0.91, Hanoi 0.83, MessengerL1 0.90, MessengerL2 0.93, Crafter 0.26, Minecraft 0.61 (values are normalized to human baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper notes that despite high performance on simple/statistical tasks (Bandits, RPS) and decent performance on some planning tasks, GPT-4 variants show large gaps on long-horizon planning and open-world procedural tasks (Crafter, Minecraft), attributed to challenges in long-horizon planning, spatial reasoning, and learning from interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SmartPlay : A Benchmark for LLMs as Intelligent Agents', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e919.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e919.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-0314</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (0314 variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Another GPT-4 variant evaluated in the benchmark; performs similarly to GPT-4-0613 with small differences per capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4-0314</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary GPT-4 variant. Paper does not provide architecture or size details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Bandits, RockPaperScissors, Hanoi, MessengerL1, MessengerL2, Crafter, Minecraft</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>sequential decision-making, planning, multi-step reasoning, 2D/3D navigation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Normalized human-relative scores (Table 2): Bandit 0.97, RPS 0.98, Hanoi 0.90, MessengerL1 0.87, MessengerL2 0.97, Crafter 0.32, Minecraft 0.59.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Similar observations as GPT-4-0613: performs well on short-horizon / statistical tasks but lags on complex procedural and spatial tasks; paper suggests limitations in learning from interactions, error handling, and spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SmartPlay : A Benchmark for LLMs as Intelligent Agents', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e919.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e919.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-003 (GPT-3.5-era model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performing proprietary model from the GPT-3.5 family evaluated as an agent; shows strong performance on Bandits but markedly lower on planning/spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary transformer-based LLM (GPT-3.5 family). Paper provides no architecture or parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Bandits, RockPaperScissors, Hanoi, MessengerL1, MessengerL2, Crafter, Minecraft</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>sequential decision-making, planning, multi-step reasoning, embodied navigation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Normalized human-relative scores (Table 2): Bandit 1.04, RPS 0.40, Hanoi 0.50, MessengerL1 0.62, MessengerL2 0.46, Crafter 0.07, Minecraft 0.45.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper notes model exhibits relative strength in randomness/interaction learning in Bandits but weak instruction following and planning, indicating heterogeneous skills across QA-like and interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SmartPlay : A Benchmark for LLMs as Intelligent Agents', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e919.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e919.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude (unnamed variant in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary conversational LLM evaluated on the benchmark; shows intermediate performance between GPT-4 variants and smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Claude</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary conversational LLM (Anthropic). No architecture/size details reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Bandits, RockPaperScissors, Hanoi, MessengerL1, MessengerL2, Crafter, Minecraft</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>sequential decision-making, planning, multi-step reasoning, embodied navigation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Normalized human-relative scores (Table 2): Bandit 0.72, RPS 0.47, Hanoi 0.67, MessengerL1 0.44, MessengerL2 0.60, Crafter 0.05, Minecraft 0.50.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper highlights comparatively poorer performance on long-horizon / procedural tasks; indicates that different proprietary models have different capability profiles (Claude stronger in planning/reasoning vs. Bard).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SmartPlay : A Benchmark for LLMs as Intelligent Agents', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e919.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e919.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Bard (unnamed variant in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary conversational LLM evaluated in SmartPlay; shows moderate performance on some tasks but weaker planning and Crafter performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Bard</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary conversational LLM (Google). Paper contains no architectural or size details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Bandits, RockPaperScissors, Hanoi, MessengerL1, MessengerL2, Crafter, Minecraft</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>sequential decision-making, planning, multi-step reasoning, embodied navigation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Normalized human-relative scores (Table 2): Bandit 0.86, RPS 0.30, Hanoi 0.67, MessengerL1 0.61, MessengerL2 0.40, Crafter 0.04, Minecraft 0.54.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper notes heterogeneous capability profile and an inability to match GPT-4 on comprehensive agent benchmarks like Crafter, indicating limitations beyond standard conversational/QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SmartPlay : A Benchmark for LLMs as Intelligent Agents', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e919.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e919.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>llama-2-13b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2 13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 13B-parameter family model evaluated as an agent in SmartPlay; performs substantially worse than GPT-4 variants across most interactive benchmarks but shows some parity on Minecraft.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>llama-2-13b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer LLM (13B parameter family) as provided by Meta; paper does not detail architectural variants beyond name.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Bandits, RockPaperScissors, Hanoi, MessengerL1, MessengerL2, Crafter, Minecraft</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>sequential decision-making, planning, multi-step reasoning, 2D/3D navigation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Normalized human-relative scores (Table 2): Bandit 0.50, RPS 0.35, Hanoi 0.37, MessengerL1 0.12, MessengerL2 0.13, Crafter 0.04, Minecraft 0.61.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Open-source models lag substantially on instruction-following, planning, and multi-hop reasoning tasks relative to proprietary SOTA, indicating that scale + training recipe likely matter; paper notes llama-2 shows somewhat better planning/reasoning vs. base LLaMA-1 in some capability aggregates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SmartPlay : A Benchmark for LLMs as Intelligent Agents', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e919.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e919.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>llama-13b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 1 13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier open-source 13B-parameter model evaluated for agent performance; generally similar to llama-2 but with slightly different capability strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>llama-13b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer LLM (13B). No extra architecture/training details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Bandits, RockPaperScissors, Hanoi, MessengerL1, MessengerL2, Crafter, Minecraft</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>sequential decision-making, planning, multi-step reasoning, embodied navigation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Normalized human-relative scores (Table 2): Bandit 0.68, RPS 0.50, Hanoi 0.33, MessengerL1 0.16, MessengerL2 0.06, Crafter 0.04, Minecraft 0.50.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Similar to other open-source models: scale + training/data differences relative to GPT-4 variants manifest as poorer agent capabilities, especially for long-horizon and multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SmartPlay : A Benchmark for LLMs as Intelligent Agents', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e919.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e919.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>vicuna-13b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna 13B (fine-tuned LLaMA variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A community fine-tuned instruction-following 13B model (Vicuna family) evaluated as an agent; fine-tuning appears to degrade some planning and reasoning capabilities compared to base LLaMA in this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>vicuna-13b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-fine-tuned 13B LLM (community Vicuna); paper notes it is fine-tuned and performs worse than base LLAMA-13b on many agent capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Bandits, RockPaperScissors, Hanoi, MessengerL1, MessengerL2, Crafter, Minecraft</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>sequential decision-making, planning, multi-step reasoning, embodied navigation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Normalized human-relative scores (Table 2): Bandit 0.64, RPS 0.17, Hanoi 0.07, MessengerL1 0.00, MessengerL2 0.12, Crafter 0.02, Minecraft 0.43.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>instruction fine-tuning (community Vicuna fine-tune)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method (fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Vicuna is a fine-tuned instruction-following version of base LLaMA; the paper reports that Vicuna-13b 'loses a lot of reasoning, planning, long text understanding, and error/mistake handling capabilities after fine-tuning.'</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Fine-tuning (Vicuna) coincides with degraded interactive-agent performance vs. base LLaMA-13b on SmartPlay tasks (see normalized scores), though no controlled ablation isolating fine-tuning variables is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>The paper suggests that certain fine-tuning recipes (e.g., instruction tuning) may trade off capabilities useful for multi-step planning and internal state tracking, resulting in worse agent performance despite stronger instruction-following for conversational tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SmartPlay : A Benchmark for LLMs as Intelligent Agents', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Plan, eliminate, and track-language models are good teachers for embodied agents <em>(Rating: 2)</em></li>
                <li>Spring: Gpt-4 out-performs rl algorithms by studying papers and reasoning <em>(Rating: 2)</em></li>
                <li>Benchmarking the spectrum of agent capabilities <em>(Rating: 2)</em></li>
                <li>Minedojo: Building open-ended embodied agents with internet-scale knowledge <em>(Rating: 2)</em></li>
                <li>Grounding language to entities and dynamics for generalization in reinforcement learning <em>(Rating: 2)</em></li>
                <li>Alfworld: Aligning text and embodied environments for interactive learning <em>(Rating: 1)</em></li>
                <li>Read and reap the rewards: Learning to play atari with the help of instruction manuals <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-919",
    "paper_id": "paper-0a2c3e734349232781eb95319dc41f8d8fb671a0",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "SmartPlay",
            "name_full": "SmartPlay benchmark for LLMs as intelligent agents",
            "brief_description": "A suite of 6 text-augmented games (Two-Armed Bandits, Rock-Paper-Scissors, Tower of Hanoi, Messenger, Crafter, Minecraft) and an evaluation methodology to measure LLMs' interactive/agent capabilities (planning, spatial reasoning, learning from interactions, error handling, etc.).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "SmartPlay benchmark",
            "model_description": "Unified OpenAI Gym-style interface that provides manuals, textual observations (including directional visual descriptors for 2D/3D games), history windows, and flat categorical actions to evaluate LLMs acting as agents across stochastic and deterministic environments.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Bandits, RockPaperScissors, Hanoi, Messenger (L1/L2), Crafter, Minecraft",
            "interactive_task_type": "sequential decision-making, planning, multi-step reasoning, learning from interactions, 2D/3D embodied navigation",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Paper discusses multiple reasons for LLMs' poorer interactive performance relative to their strengths on static QA-style tasks: (1) existing LLM benchmarks focus on static knowledge/reasoning and do not capture long-horizon planning, spatial reasoning, handling randomness, or learning from interaction; (2) possible dataset/environment contamination leads LLMs to know canonical starting solutions but not intermediate states (e.g., Tower of Hanoi intermediate configurations not seen in training); (3) weakness in tracking intermediate state and memoizing multi-step trajectories; (4) poor spatial reasoning and inconsistent action sequences; (5) limited ability to learn from interactions in long-horizon procedurally-generated worlds.",
            "uuid": "e919.0",
            "source_info": {
                "paper_title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CoT prompting + action selection",
            "name_full": "Chain-of-thought prompting followed by constrained action selection prompt",
            "brief_description": "Experimental prompting strategy used in the paper: first ask the LLM to 'think step by step' for the next action, then explicitly ask it to choose the best executable action from a provided action list, mapping its output to environment actions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": null,
            "model_description": null,
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Applied across all SmartPlay games (Bandits, RPS, Hanoi, Messenger, Crafter, Minecraft)",
            "interactive_task_type": "prompting strategy applied to planning, multi-step reasoning, sequential decision-making, embodied navigation",
            "interactive_performance": "Used in all experiments reported in the paper; reported model scores (per-model per-game) are obtained with this prompting protocol (see Table 2). No ablation (before/after) numbers for removing this prompting are provided.",
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": true,
            "architectural_features": "chain-of-thought style prompting (eliciting intermediate reasoning before action selection)",
            "training_method": null,
            "intervention_type": "prompting strategy",
            "intervention_description": "Two-step prompt: (1) 'What is the next action to take, let's think step by step.' with manual, history, observation context; (2) 'Choose the best executable action from the list of all actions. Write the exact chosen action.' to force mapping to environment action.",
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "While CoT prompting helps elicit reasoning, the paper notes that even with CoT the top models (GPT-4 variants) still underperform humans on long-horizon planning, spatial reasoning, and complex procedural tasks, suggesting prompting alone is insufficient.",
            "uuid": "e919.1",
            "source_info": {
                "paper_title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Crafter context string",
            "name_full": "Crafter manual/context string (parsed LaTeX 'context' augmentation)",
            "brief_description": "An augmented, machine-parsed context string derived from the Crafter environment source (IATEX), provided as extra manual/context to the LLM to improve understanding of game dynamics and crafting requirements.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": null,
            "model_description": null,
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Crafter",
            "interactive_task_type": "open-world sequential decision-making, multi-step crafting/planning",
            "interactive_performance": "The paper reports experiments using this 'context' string and notes (citing Wu et al. (2023c)) that it 'greatly improve[s] performance of GPT-4 and text-davinci-003 on Crafter'; absolute normalized scores with the context are in Table 2 (GPT-4-0613: 0.26, GPT-4-0314: 0.32, text-davinci-003: 0.07), but no before/after numbers in this paper.",
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": "prompting / context augmentation",
            "intervention_description": "Provide a machine-parsed, detailed environment 'context' string (extracted from the Crafter paper/source) in the manual input to the LLM so it has more procedural and rule information about crafting and achievements.",
            "intervention_effect": "Reported (via citation) to substantially improve performance for some LLMs on Crafter; this paper reports absolute normalized scores obtained while using the context but does not provide explicit before/after ablation numbers.",
            "hypothesized_cause_of_gap": "Lack of explicit procedural knowledge in the raw manual/observation can limit agent performance; adding a detailed context supplies missing rules/requirements and partially closes the gap.",
            "uuid": "e919.2",
            "source_info": {
                "paper_title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Directional visual descriptor",
            "name_full": "Directional textual visual descriptor for 2D/3D observations",
            "brief_description": "Observation engineering technique that converts raw dense environment observations (MineDojo block matrices / segmentation) into a textual list of grouped objects with relative directional positions and screen-space percentages to make visual scenes comprehensible to LLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": null,
            "model_description": null,
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Messenger, Crafter, Minecraft (visual/text conversion applied)",
            "interactive_task_type": "embodied navigation, 2D/3D spatial reasoning",
            "interactive_performance": "Used as the canonical observation representation for visual games in SmartPlay; enables LLMs to act in Minecraft/Crafter/Messenger but per-model performance remains substantially below human baseline on complex spatial tasks (see Table 2).",
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": "input representation / observation engineering",
            "intervention_description": "Group connected components of same block/object, describe each group's closest block's relative position (e.g., 'grass, 3.87 blocks away to your bottom, taking 38% of screen') to supply spatial information as text to LLMs.",
            "intervention_effect": "Enables LLMs to make meaningful progress in visual/3D tasks that they could not access otherwise; nevertheless, the paper reports continued poor performance on 3D spatial reasoning (Minecraft) even with this representation.",
            "hypothesized_cause_of_gap": "Even when observations are textualized, LLMs show inconsistent trajectory-level spatial reasoning (e.g., oscillating contradictory moves), indicating deficits beyond raw perceptual encoding.",
            "uuid": "e919.3",
            "source_info": {
                "paper_title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4-0613",
            "name_full": "GPT-4 (0613 variant)",
            "brief_description": "A proprietary GPT-4 variant evaluated as an LLM agent across all SmartPlay games; achieved the highest scores among evaluated models but still underperformed humans on complex interactive tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-4-0613",
            "model_description": "Proprietary transformer-based large language model (GPT-4 family). Paper does not report architecture details beyond being a GPT-4 variant.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Bandits, RockPaperScissors, Hanoi (3-disk), MessengerL1, MessengerL2, Crafter, Minecraft",
            "interactive_task_type": "sequential decision-making, planning, multi-step reasoning, 2D/3D navigation, learning from interactions",
            "interactive_performance": "Normalized human-relative scores (Table 2): Bandit 1.00, RPS 0.91, Hanoi 0.83, MessengerL1 0.90, MessengerL2 0.93, Crafter 0.26, Minecraft 0.61 (values are normalized to human baseline).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Paper notes that despite high performance on simple/statistical tasks (Bandits, RPS) and decent performance on some planning tasks, GPT-4 variants show large gaps on long-horizon planning and open-world procedural tasks (Crafter, Minecraft), attributed to challenges in long-horizon planning, spatial reasoning, and learning from interactions.",
            "uuid": "e919.4",
            "source_info": {
                "paper_title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4-0314",
            "name_full": "GPT-4 (0314 variant)",
            "brief_description": "Another GPT-4 variant evaluated in the benchmark; performs similarly to GPT-4-0613 with small differences per capability.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-4-0314",
            "model_description": "Proprietary GPT-4 variant. Paper does not provide architecture or size details.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Bandits, RockPaperScissors, Hanoi, MessengerL1, MessengerL2, Crafter, Minecraft",
            "interactive_task_type": "sequential decision-making, planning, multi-step reasoning, 2D/3D navigation",
            "interactive_performance": "Normalized human-relative scores (Table 2): Bandit 0.97, RPS 0.98, Hanoi 0.90, MessengerL1 0.87, MessengerL2 0.97, Crafter 0.32, Minecraft 0.59.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Similar observations as GPT-4-0613: performs well on short-horizon / statistical tasks but lags on complex procedural and spatial tasks; paper suggests limitations in learning from interactions, error handling, and spatial reasoning.",
            "uuid": "e919.5",
            "source_info": {
                "paper_title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "text-davinci-003",
            "name_full": "text-davinci-003 (GPT-3.5-era model)",
            "brief_description": "A high-performing proprietary model from the GPT-3.5 family evaluated as an agent; shows strong performance on Bandits but markedly lower on planning/spatial tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "text-davinci-003",
            "model_description": "Proprietary transformer-based LLM (GPT-3.5 family). Paper provides no architecture or parameter count.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Bandits, RockPaperScissors, Hanoi, MessengerL1, MessengerL2, Crafter, Minecraft",
            "interactive_task_type": "sequential decision-making, planning, multi-step reasoning, embodied navigation",
            "interactive_performance": "Normalized human-relative scores (Table 2): Bandit 1.04, RPS 0.40, Hanoi 0.50, MessengerL1 0.62, MessengerL2 0.46, Crafter 0.07, Minecraft 0.45.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Paper notes model exhibits relative strength in randomness/interaction learning in Bandits but weak instruction following and planning, indicating heterogeneous skills across QA-like and interactive tasks.",
            "uuid": "e919.6",
            "source_info": {
                "paper_title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Claude",
            "name_full": "Anthropic Claude (unnamed variant in paper)",
            "brief_description": "A proprietary conversational LLM evaluated on the benchmark; shows intermediate performance between GPT-4 variants and smaller models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Claude",
            "model_description": "Proprietary conversational LLM (Anthropic). No architecture/size details reported in paper.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Bandits, RockPaperScissors, Hanoi, MessengerL1, MessengerL2, Crafter, Minecraft",
            "interactive_task_type": "sequential decision-making, planning, multi-step reasoning, embodied navigation",
            "interactive_performance": "Normalized human-relative scores (Table 2): Bandit 0.72, RPS 0.47, Hanoi 0.67, MessengerL1 0.44, MessengerL2 0.60, Crafter 0.05, Minecraft 0.50.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Paper highlights comparatively poorer performance on long-horizon / procedural tasks; indicates that different proprietary models have different capability profiles (Claude stronger in planning/reasoning vs. Bard).",
            "uuid": "e919.7",
            "source_info": {
                "paper_title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Bard",
            "name_full": "Google Bard (unnamed variant in paper)",
            "brief_description": "A proprietary conversational LLM evaluated in SmartPlay; shows moderate performance on some tasks but weaker planning and Crafter performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Bard",
            "model_description": "Proprietary conversational LLM (Google). Paper contains no architectural or size details.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Bandits, RockPaperScissors, Hanoi, MessengerL1, MessengerL2, Crafter, Minecraft",
            "interactive_task_type": "sequential decision-making, planning, multi-step reasoning, embodied navigation",
            "interactive_performance": "Normalized human-relative scores (Table 2): Bandit 0.86, RPS 0.30, Hanoi 0.67, MessengerL1 0.61, MessengerL2 0.40, Crafter 0.04, Minecraft 0.54.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Paper notes heterogeneous capability profile and an inability to match GPT-4 on comprehensive agent benchmarks like Crafter, indicating limitations beyond standard conversational/QA tasks.",
            "uuid": "e919.8",
            "source_info": {
                "paper_title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "llama-2-13b",
            "name_full": "LLaMA 2 13B",
            "brief_description": "An open-source 13B-parameter family model evaluated as an agent in SmartPlay; performs substantially worse than GPT-4 variants across most interactive benchmarks but shows some parity on Minecraft.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "llama-2-13b",
            "model_description": "Open-source transformer LLM (13B parameter family) as provided by Meta; paper does not detail architectural variants beyond name.",
            "model_size": "13B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Bandits, RockPaperScissors, Hanoi, MessengerL1, MessengerL2, Crafter, Minecraft",
            "interactive_task_type": "sequential decision-making, planning, multi-step reasoning, 2D/3D navigation",
            "interactive_performance": "Normalized human-relative scores (Table 2): Bandit 0.50, RPS 0.35, Hanoi 0.37, MessengerL1 0.12, MessengerL2 0.13, Crafter 0.04, Minecraft 0.61.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Open-source models lag substantially on instruction-following, planning, and multi-hop reasoning tasks relative to proprietary SOTA, indicating that scale + training recipe likely matter; paper notes llama-2 shows somewhat better planning/reasoning vs. base LLaMA-1 in some capability aggregates.",
            "uuid": "e919.9",
            "source_info": {
                "paper_title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "llama-13b",
            "name_full": "LLaMA 1 13B",
            "brief_description": "An earlier open-source 13B-parameter model evaluated for agent performance; generally similar to llama-2 but with slightly different capability strengths.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "llama-13b",
            "model_description": "Open-source transformer LLM (13B). No extra architecture/training details provided in this paper.",
            "model_size": "13B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Bandits, RockPaperScissors, Hanoi, MessengerL1, MessengerL2, Crafter, Minecraft",
            "interactive_task_type": "sequential decision-making, planning, multi-step reasoning, embodied navigation",
            "interactive_performance": "Normalized human-relative scores (Table 2): Bandit 0.68, RPS 0.50, Hanoi 0.33, MessengerL1 0.16, MessengerL2 0.06, Crafter 0.04, Minecraft 0.50.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Similar to other open-source models: scale + training/data differences relative to GPT-4 variants manifest as poorer agent capabilities, especially for long-horizon and multi-step tasks.",
            "uuid": "e919.10",
            "source_info": {
                "paper_title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "vicuna-13b",
            "name_full": "Vicuna 13B (fine-tuned LLaMA variant)",
            "brief_description": "A community fine-tuned instruction-following 13B model (Vicuna family) evaluated as an agent; fine-tuning appears to degrade some planning and reasoning capabilities compared to base LLaMA in this benchmark.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "vicuna-13b",
            "model_description": "Instruction-fine-tuned 13B LLM (community Vicuna); paper notes it is fine-tuned and performs worse than base LLAMA-13b on many agent capabilities.",
            "model_size": "13B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Bandits, RockPaperScissors, Hanoi, MessengerL1, MessengerL2, Crafter, Minecraft",
            "interactive_task_type": "sequential decision-making, planning, multi-step reasoning, embodied navigation",
            "interactive_performance": "Normalized human-relative scores (Table 2): Bandit 0.64, RPS 0.17, Hanoi 0.07, MessengerL1 0.00, MessengerL2 0.12, Crafter 0.02, Minecraft 0.43.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "instruction fine-tuning (community Vicuna fine-tune)",
            "intervention_type": "training method (fine-tuning)",
            "intervention_description": "Vicuna is a fine-tuned instruction-following version of base LLaMA; the paper reports that Vicuna-13b 'loses a lot of reasoning, planning, long text understanding, and error/mistake handling capabilities after fine-tuning.'",
            "intervention_effect": "Fine-tuning (Vicuna) coincides with degraded interactive-agent performance vs. base LLaMA-13b on SmartPlay tasks (see normalized scores), though no controlled ablation isolating fine-tuning variables is provided.",
            "hypothesized_cause_of_gap": "The paper suggests that certain fine-tuning recipes (e.g., instruction tuning) may trade off capabilities useful for multi-step planning and internal state tracking, resulting in worse agent performance despite stronger instruction-following for conversational tasks.",
            "uuid": "e919.11",
            "source_info": {
                "paper_title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Plan, eliminate, and track-language models are good teachers for embodied agents",
            "rating": 2
        },
        {
            "paper_title": "Spring: Gpt-4 out-performs rl algorithms by studying papers and reasoning",
            "rating": 2
        },
        {
            "paper_title": "Benchmarking the spectrum of agent capabilities",
            "rating": 2
        },
        {
            "paper_title": "Minedojo: Building open-ended embodied agents with internet-scale knowledge",
            "rating": 2
        },
        {
            "paper_title": "Grounding language to entities and dynamics for generalization in reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Alfworld: Aligning text and embodied environments for interactive learning",
            "rating": 1
        },
        {
            "paper_title": "Read and reap the rewards: Learning to play atari with the help of instruction manuals",
            "rating": 1
        }
    ],
    "cost": 0.020193999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SmartPlay : A Benchmark for LLMs as IntelliGENT AGENTS</h1>
<p>Yue Wu ${ }^{12}$; Xuan Tang ${ }^{1}$, Tom Mitchell ${ }^{1}$, Yuanzhi Li ${ }^{12}$<br>${ }^{1}$ Carnegie Mellon University, ${ }^{2}$ Microsoft Research</p>
<h4>Abstract</h4>
<p>Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. We release our benchmark at github.com/microsoft/SmartPlay.</p>
<h2>1 INTRODUCTION</h2>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: SmartPlay provides a unified and expandable API with text observations and guidance to perform turn by turn LLM inference on Two-armed Bandits, Rock Paper Scissors, Messenger (Hanjie et al., 2021), Crafter (Hafner, 2021), and Minecraft (Fan et al., 2022) creative navigation tasks.</p>
<p>Creating intelligent agents (Wooldridge \&amp; Jennings, 1995), that perceives its environment and perform autonomous actions, has been one of the core objectives of A.I. (Laird et al., 1987; Russell, 2010) Recently, large language models (LLMs) (Smith et al., 2022; Chowdhery et al., 2022; OpenAI, 2023; Manyika; Driess et al., 2023; Touvron et al., 2023) have made remarkable progress in various tasks (Bubeck et al., 2023). Some language models demonstrate exceptional planning (Ahn et al., 2022; Wu et al., 2023b), reasoning (Wu et al., 2023a; Shinn et al., 2023), and problemsolving (Madaan et al., 2023; Kim et al., 2023) abilities, enabling the potential as generalist agents for virtual-reality (Park et al., 2023) or real-world problem-solving.</p>
<p>Such potential has attracted strong interest on applications where LLM systems actively invoke tools and APIs to complete a wide range of tasks goals (Significant-Gravitas; Yoheinakajima; Reworkd;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Wang et al., 2023a; Qin et al., 2023), and actively interact and make changes in an environment to achieve specific results (Wang et al., 2023b,a; Wu et al., 2023b,c). LLMs as agents could be seen as an important step toward next-gen automation.</p>
<p>Despite great public attention, the capabilities of LLMs as agents have not been systematically studied, partly due to the lack of standardized LLM benchmark for agent-environment interaction. Current LLM benchmarks have been designed for static knowledge and reasoning (Hendrycks et al., 2020; Liang et al., 2022; Srivastava et al., 2022a; Zhong et al., 2023), or helpful and harmless conversations (Bai et al., 2022; Zheng et al., 2023a; Dubois et al., 2023), overlooking applications to intelligent agents.</p>
<p>We note 4 key challenges for intelligent LLM agents not captured in previous benchmarks. First, lots of real-world tasks require an agent to do long-horizon planning. Second, many events are probabilistic and an intelligent agent is expected to understand the odds. Third, an intelligent agent needs spatial reasoning to understand our 3D world. Fourth, when encountered with unseen situations, an intelligent agent should be able to learn from interactions or mistakes.</p>
<p>On the other hand, games have long been identified as go-to benchmarks for intelligent generalist agents (Pell, 2011; Genesereth et al., 2005; Whiteson et al., 2010; Schaul et al., 2011; Bellemare et al., 2013; Ct et al., 2019; Hafner, 2021; Guss et al., 2021; Fan et al., 2022). At the core of game design (Koster, 2013), successful games often involve "problem-solving", "calculation of odds", "spatial reasoning", "changing difficulties", and "well-defined and quantifiable outcome", therefore offering perfect complement to existing LLM benchmarks. Finally, some game environments are procedurally generated and game states grow exponentially, making games more robust against evaluation dataset contamination as observed in recent works (Touvron et al., 2023). Experimentally, we observe LLMs struggle to memoize intermediate states of a simple 3-disk Tower of Hanoi game.</p>
<p>Taking a unique agent perspective in benchmarking LLMs, we introduce SmartPlay, a benchmark from 6 distinct games augmented with language descriptors for visual observation (Figure 1), offering up to 20 different settings and infinite environment variations. Each game presents unique challenges that span multiple dimensions of intelligent agents, as detailed in Table 3. The games range in complexity, from requiring simple one-step reasoning and rule-following in Bandits, to intricate long-term planning, multi-hop dependencies, and learning from interactions in Crafter (Hafner, 2021) and Hanoi. SmartPlay engages LLM agents in both deterministic and stochastic settings, demanding skills from basic text understanding to 3D spatial reasoning.</p>
<p>Games in SmartPlay have been built with well-defined objectives and evaluation metrics: completion rate, reward, score. Therefore, SmartPlay provides a fully automated pipeline to conduct standardized evaluation for LLMs. We use SmartPlay to compare the agent performance of recent LLMs, and identify several research gaps for applying LLMs as agents. We believe that SmartPlay sets a goal that is reachable in a short time-frame yet formidable to require new breakthroughs.</p>
<h1>2 CAPABILITIES NECESSARY FOR INTELLIGENT AGENTS</h1>
<p>Borrowing concepts from game design (Koster, 2013), we identify 9 key abilities important for intelligent LLM agents, and identify multiple degrees for each capability:
a) Long text understanding: general LLM capability.</p>
<ul>
<li>We define 4 degrees based on document length and syntactic variations: 1) few fixed lines, 2) few fixed paragraphs, 3) with syntactic variations, 4) and longer than 1 page ( 500 words).
b) Reasoning: multi-hop logical reasoning and deduction, often required for analyzing the interactions of in-game objects or action conditions/dependencies.</li>
<li>We define 3 degrees based on reasoning hops: 1) $(0 \sim 1), 2)(2 \sim 3), 3)(&gt;3)$.
c) Instruction/Rule following: follow rules and instructions set by environment or users.</li>
<li>We define 3 degrees based on number of game rules: 1) single rule, 2) $(&lt;5), 3)(5+)$
d) Planning: long-horizon in-context planning to achieve a complex goal.</li>
<li>
<p>We define 3 degrees based on planning steps, and concurrent objectives which requires goal prioritization: 1) $&lt;5$ planning steps, 2) $5+$ planning steps, 3) concurrent objectives
e) Generalization: Excels at a wide range of tasks.</p>
</li>
<li>
<p>We define 3 degrees based on the variability the game provides: 1) fixed environment, 2) fixed game word with random objectives, 3) procedurally generated game world
f) Understanding the odds: analyze and estimate the probability of random events.</p>
</li>
<li>We define 3 degrees based on the importance randomness in the environment: 1) no randomness, 2) randomness present in game, 3) randomness as core game mechanism
g) Learning from interactions: acquire environment knowledge from live interactions.</li>
<li>We define 4 degrees based on the number of unique interactions to learn from: 1) no learning required, 2) single interaction, 3$)&lt;5$ interactions, 4) $5+$ interactions
h) Error/Mistake handling: recover from mistakes (e.g., correcting from erroneous trajectory).</li>
<li>We define 3 degrees based on if mistake handling may be necessary and if additional reasoning and re-planning is necessary: 1) not required, 2) simple rollback corrects error, 3) reasoning and re-planning required to correct error.
i) Spatial reasoning: understand our world in 2D/3D. Spatial reasoning is typically required to understand directions and navigate through the game world (e.g., navigating the 2D/3D world).
(a) We define 3 degrees based on dimensionality: 1) $0 \sim 1 \mathrm{D}, 2) 2 \mathrm{D}, 3) 3 \mathrm{D}$</li>
</ul>
<h1>3 GAMES IN SMARTPLAY</h1>
<h3>3.1 RESEARCH CHALLENGES</h3>
<p>The SmartPlay benchmark encapsulates a diverse set of challenges that evaluate various AI capabilities, as itemized in Figure 2. For instance, Bandits primarily focuses on understanding the odds, requiring minimum text understanding and rule-following. On the other hand, Rock Paper Scissors uniquely puts an emphasis on understanding the odds and multiple game rules. Hanoi presents an advanced setting for object dependency reasoning, strategic planning, and handling mistakes. Messenger puts challenge on 2D spatial reasoning, reading syntactic variations and conducting multi-hop reasoning. Meanwhile, Minecraft offers a unique challenge in 3D spatial reasoning and generalization within a procedurally generated world. We hope the SmartPlay benchmark would serve as a tool for identifying these nuanced gaps and directing future research.
While each game poses its unique challenges, the SmartPlay benchmark also evaluates an agent's capability to integrate these skills. For example, Crafter stands as the most comprehensive testbed, combining long texts, multiple interactions, concurrent objectives, and error handling into a single environment. Crafter highlight the need for future research to focus not just on isolated skills, but also on combining these skills into a unified, adaptive agent.</p>
<h3>3.2 Two Armed Bandits</h3>
<p>The two armed bandit benchmark is inspired by popular implementations ${ }^{1}$ of bandit problems.
The LLM agent is provided two slot machines with hidden pre-defined reward probabilities $p_{1}, p_{2}$. For slot machine $i$, the reward for the two possible out-comes are: $r_{i}$ for pay-off event and $-r_{i}$ for no-pay-off event. The goal of the game is to find the arm with better return and maximize the reward over the course of 50 rounds. The human written manual informs the LLM of the number of slot machines (two) and the objective.</p>
<p>An agent must keep track of win/losses from its past roll-out and balance exploration across the two slot machines vs. exploitation of the more rewarding one. Overall, the challenges include: 1) long context understanding, 2) understanding randomness, 3) learning from interactions.</p>
<p>To prevent game exploitation caused by biased actions, we randomize the score and probabilities for each action by shuffling the order of the paired list: $\left[\left(p_{1}, r_{1}\right),\left(p_{2}, r_{2}\right)\right]$.</p>
<h3>3.3 Rock Paper Scissors</h3>
<p>Same rules as the famous zero-sum game Rock Paper Scissors ${ }^{2}$.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: We identify a set of 9 important capabilities (section 2) for an intelligent agent. We identify different degrees of challenge for each capability as shown on the left. Each game in SmartPlay challenges unique set of capabilities at different degrees, as shown in the spider charts. We include numerical values of the spider plots in Table 3.</p>
<p>The LLM agent plays against a hand-coded opponent that follows a hidden pre-defined strategy with probabilities $p_{1}, p_{2}, p_{3}$ for rock, paper, and scissors respectively. The scores for winning under each action is pre-defined and revealed to the LLM as $s_{1}, s_{2}, s_{3}$. The human written manual provides instruction on the possible actions and how the win/draw/lose of each round is calculated.</p>
<p>An agent must keep track of win/losses from its past roll-outs to analyze the opponent behavior, and then exploit the opponent to maximize payoff. Overall, the challenges include: 1) long text understanding, 2) understanding the odds, 3) learning from interactions, 4) instruction following.</p>
<p>To prevent game exploitation caused by biased actions, we randomize the score and probabilities for each action by shuffling the order of the paired list: $\left[\left(p_{1}, s_{1}\right),\left(p_{2}, s_{2}\right),\left(p_{3}, s_{3}\right)\right]$.</p>
<h1>3.4 TOWER OF HANOI</h1>
<p>The Tower of Hanoi ${ }^{3}$ is a classic puzzle game that challenges the player to move a stack of disks from one rod to another, using a third rod as an auxiliary. The game has two rules: only one disk can be moved at a time, and a larger disk cannot be placed on top of a smaller one.</p>
<p>The goal of the game is to move all the disks from the first rod to the last one in the minimum number of moves, and the game can be solved using a recursive algorithm that follows these steps:</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ol>
<li>Move n - 1 disks from the source rod to the auxiliary rod, using the destination rod as an intermediate.</li>
<li>Move the largest disk from the source rod to the destination rod.</li>
<li>Move n - 1 disks from the auxiliary rod to the destination rod, using the source rod as an intermediate.</li>
</ol>
<p>The human written manual contains a description of the game set-up and allowed actions. In addition, we also include an example illustration of the starting and goal configuration, alongside an example of allowed/disallowed moves.</p>
<p>The Tower of Hanoi requires the agent to think strategically and plan ahead, and put strict requirements on the LLM agents' ability to understand and follow the rules of the game. The game can become more challenging if the agent makes a mistake. Sometimes, an agent may have to undo several moves to correct an error. Overall, the challenges include: 1) planing, 2) reasoning, 3) error handling.</p>
<h1>3.5 MESSENGER</h1>
<p>MESSENGER (Hanjie et al., 2021) which features multiple game variants with procedurally generated game dynamics and accompanying text manuals. The overall game mechanics of MESSENGER involve obtaining a message and delivering it to a goal. The benchmark is shipped with 3 levels of difficulties (referred as stages in Hanjie et al. (2021)).</p>
<p>To succeed in MESSENGER, an agent must first relate entities and dynamics of the environment to their reference synonyms in the manual, identify message and goal objects, and navigate to bring the message to the goal while avoiding the enemy. The manual, by design, is challenging to understand even for human readers. Level 1 primarily challenges the agent's 1) long text understanding and 2) generalization. Level 2 includes additional challenge on the agent's 3) reasoning, and 4) 2D spatial reasoning. Level 3 increases difficulty by adding distraction objects.</p>
<p>The original manuals provided by Hanjie et al. (2021) contain descriptions of the entities and world dynamics obtained through crowd-sourced human writers. We augment the manual with a specification on the game objective, and an "advice" for LLM agent to first identify goal objects and then approach its objective. The "advice" reduces the difficulty of the hard-to-parse manual.</p>
<h3>3.6 CRAFter</h3>
<p>The Crafter environment (Hafner, 2021) is a procedurally generated, open-world survival game designed to test RL algorithms. Inspired by Minecraft, it features a grid-world with top-down observation and a discrete action space of 17. The game includes 22 achievements in a tech-tree of depth 7 and provides information on the player's health, food, water, rest levels, and inventory. Crafter captures many of Minecraft's key research challenges, offering a more streamlined and faster environment for conducting experiments and gathering results.</p>
<p>We provide the "context" string from Wu et al. (2023c) as the manual, generated by parsing the IATEX source-code of (Hafner, 2021). The "context" string has been shown to greatly improve performance of GPT-4 and text-davinci-003 on Crafter (Wu et al., 2023c).</p>
<p>To succeed in Crafter an LLM agent has to first understand and master a variety of reusable skills composed of 17 actions. The agent needs to learn to navigate through up to 52 D terrains (biomes), avoiding obstacles and dangerous creatures. The agent also needs to collect different resources and craft more advanced weapons/tools to unlock more skills and achievements, while at the same time balance crafting goals with survival goals like maintaining health, thirst, food, and rest (Hafner, 2021). Overall, the challenges include: 1) 2D spatial reasoning, 2) mistake handling, 3) long text understanding, 4) planning, 5) generalization, 6) correcting from mistakes. Interestingly, the "context" string does not capture all information necessary to succeed in the game, i.e., it requires 2 woods to craft the crafting table, and 8 stones to craft the furnace. The agent has to 7) learn from interaction.</p>
<h3>3.7 Minecraft</h3>
<p>Minecraft is one of the most popular games in history ${ }^{4}$. The game world is virtually infinite and procedurally generated. The game observation is composed of rough 3D objects representing various materials, such as dirt, stone, ores, tree trunks, water, and lava. Minecraft has been widely studied as</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>a benchmark for intelligent multi-tasking agents (Guss et al., 2021; Fan et al., 2022; Hafner et al., 2023; Yuan et al., 2023; Wang et al., 2023b;a). However, due to the fact that most current LLMs do not have vision capabilities, we simplify the Minecraft benchmark (Fan et al., 2022) and only consider a small set of creative tasks where the primary objective is to find specific biomes, so an LLM could control a hand-coded agent to perform navigation in the 3D world.</p>
<p>For the human written instruction manual, we inform the agent that its goal is to find a specific biome $g$ in Minecraft, and offer an advice on how to interpret the visual descriptor output for Minecraft.</p>
<p>To succeed in the creative "find" tasks, a LLM agent has to have enough domain knowledge about different biomes in Minecraft, and be able to correlate visual observation (text description of visual world) with domain knowledge, and navigate in a 3D environment. Overall, the challenges include: 1) planning, 2) domain knowledge, 3) 3D spatial reasoning, 4) generalization.</p>
<h1>4 Using SmartPlay</h1>
<h3>4.1 Environment Interface and Evaluation Protocol</h3>
<table>
<thead>
<tr>
<th style="text-align: center;">Env</th>
<th style="text-align: center;">Input</th>
<th style="text-align: center;">Manual</th>
<th style="text-align: center;">History</th>
<th style="text-align: center;">Rollout</th>
<th style="text-align: center;">Action Space</th>
<th style="text-align: center;">Trials</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Bandits</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">Background</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;">RockPaperScissors</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">Background,Rules</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;">Hanoi</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">Background,Rules,Examples</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">Messenger</td>
<td style="text-align: center;">Visual description</td>
<td style="text-align: center;">Background,Rules,Advice</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$4 \sim 128$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Crafter</td>
<td style="text-align: center;">Visual description</td>
<td style="text-align: center;">Background,Rules,Advice</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10 k</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">Minecraft</td>
<td style="text-align: center;">Visual description</td>
<td style="text-align: center;">Objective</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">20</td>
</tr>
</tbody>
</table>
<p>Table 1: Specifications for each game in SmartPlay. In addition to the table, the manual input contains a list available actions for all games. Input, manual, action space, and rollout length should not be modified. History length and trial numbers could be increased to suite future needs.</p>
<p>For ease of use and wide compatibility, SmartPlay follows a unified OpenAI Gym interface (Brockman et al., 2016) for all games, with text-based observations, text-based manuals with content as described in Table 1, text describing historical actions and observations covering past steps of length "history length", and flat categorical actions. Due to the randomness in some games, we recommend running each game multiple times and reporting the average metrics.</p>
<p>Input, manual, action space, rollout length (the maximum environment steps allowed by each game), and trial numbers for each game are specified in Table 1. These settings are fixed and should not be modified. However, future research may require longer history length or more trials for some games. These parameters can be adjusted to suit specific needs, but the changes should be explicitly stated. We provide recommended values (also used in our experiments) for the parameters in Table 1.</p>
<p>For completeness, we provide example inputs for each game in Appendix C. Note that all directions in SmartPlay are described in "east, south, west, north, above, below" In the actual gameplay, SmartPlay API also includes a list of actions for the LLM agent to pick from.</p>
<h3>4.2 Evaluation Metrics</h3>
<p>We define three metrics: reward, completion rate, score. To ensure compatibility with prior works, reward aligns with the score/reward definition in games originally designed for RL (i.e., Bandits, Rock Paper Scissors, Messenger, Crafter (Hanjie et al., 2021; Hafner, 2021)). Completion rate measures the rate of successful completion for games with quantifiable objectives (i.e., Hanoi, Messenger, Minecraft). Finally, we introduce score for every game in the benchmark to provide a summary of performance. For Bandits and Rock Paper Scissors, the score is defined the number of times the LLM action matches the environment optimal action; for Hanoi, the score is defined as the number of disks successfully moved to the goal peg; for Messenger, the score is the same as the reward (Hanjie et al., 2021) of each round of game; for Crafter, the score is defined as the number of unlocked achievements at every step, summed across the whole game; for Minecraft, the score is defined as the indicator of whether the "find" objective for the game has been completed.</p>
<h1>5 EXPERIMENTAL RESULTS</h1>
<p>Using the SmartPlay API, we follow Wu et al. (2023c) and directly prompt an LLM: "What is the next action to take, let's think step by step.", with manual, history, and current observation as context. We then query the LLM: "Choose the best executable action from the list of all actions. Write the exact chosen action." for an answer directly mapped to one of the environment actions.</p>
<h3>5.1 Quantitative Analysis</h3>
<table>
<thead>
<tr>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">Bandit</th>
<th style="text-align: center;">RPS</th>
<th style="text-align: center;">Hanoi</th>
<th style="text-align: center;">MessengerL1</th>
<th style="text-align: center;">MessengerL2</th>
<th style="text-align: center;">Crafter</th>
<th style="text-align: center;">Minecraft</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Human Baseline</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4-0613</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.61</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4-0314</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.59</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-003</td>
<td style="text-align: center;">1.04</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: center;">Claude</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: center;">Bard</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.54</td>
</tr>
<tr>
<td style="text-align: center;">llama-2-13b</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.61</td>
</tr>
<tr>
<td style="text-align: center;">llama-13b</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: center;">vicuna-13b</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.43</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of performance of different LLMs in terms of average score on BanditTwoArmedHighLowFixed-v0, RockPaperScissorBasic-v0, Hanoi3Disk-v0, MessengerL1v0, MessengerL2-v0, Crafter-v0, MinedojoCreative0-v0. All scores are normalized relative to human performance (unnormalized version in Table 4). GPT-4 variants out-perform other LLMs by significant margins, but still greatly under-perform human baselines. We observe significant performance gaps between SOTA LLMs and human baseline on Hanoi, Crafter, and Minecraft. Hanoi, Crafter challenges planning and reasoning, and Minecraft challenges 3D spatial reasoning.
To reduce the cost of queries, we pick 7 settings that requires a minimal experimentation but provides comprehensive coverage over important agent capabilities. We experiment with 9 recent popular open-source and proprietary LLMs and report the average score in Table 2.
Overall, GPT-4 variants significantly out performs other proprietary models, which outperform open-source models by significant margins.
There is still significant room for improvement for LLM as agents: Despite the impressive performance of GPT-4 variants, there is still a significant gap between GPT-4 and human baseline performance on more challenging benchmarks, with a $10 \%$ gap on 3DiskHanoi, $40 \%$ on Minecraft creative tasks, and $70 \%$ on Crafter.
Other proprietary LLMs struggle to keep up with GPT-4: We observe a more than 20\% gap between GPT-4 and other proprietary models like Claude, Bard, and text-davinci-003 across all games except Minecraft. Furthermore, on comprehensive benchmarks like Crafter, GPT-4 variants achieves 3 times higher scores than other proprietary models.
Open-source LLMs have a long way to go: Open-source LLMs achieves less than half the performance of GPT-4 variants on simple Bandit and Rock-Paper-Scissors tasks, and 1/8 the performance on more challenging tasks. The fine-tuned Vicuna-13b model performs much worse than the base LLAMA-13b.
3D Spatial reasoning remains a challenge for LLMs: The Minecraft benchmark appears equally challenging to all LLMs due to its unique requirement for 3D spatial reasoning. All LLMs behave similarly in Minecraft creative tasks, with the best model at $60 \%$ of human baseline performance.
To offer additional insights into the individual agent capabilities of LLMs as identified in Figure 2, we compute, for each capability $c$, the capability score $p_{L L M}^{c}$ of an LLM as the average of human normalized score $s_{g}$ over each game $g$, weighted by the degree $d_{c}^{g}$ at game $g$ presents challenge $c$ : $p_{L L M}^{c}=\frac{\sum_{g} d_{c}^{g} s_{g}}{\sum_{g} d_{c}^{g}}$. We plot the capability scores into 3 groups in Figure 3: GPT-4 variants, other proprietary models, and open-source models.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Left: comparing the two GPT-4 variants with Human Baseline performance as reference. Middle: comparing text-davinci-003, Claude, and Bard. Right: comparing open-source llama-2-13b, llama-13b, vicuna-13b models.</p>
<p>The two GPT-4 variants perform similarly overall, with GPT-0614 doing slightly worse on planning and reasoning. We also identify that GPT-4 variants score lower on learning from interactions, error/mistake handling, and spatial reasoning.</p>
<p>Claude demonstrates overall better performance than Bard, especially in planning, reasoning, instruction following. Compared to the other two proprietary models, text-davinci-003 appears biased toward learning from interaction and randomness, and is particularly weaker at instruction following, planning and reasoning.</p>
<p>LLAMA-2-13b and LLAMA-1-13b performs similar on the high level, with LLAMA-2-13b performing better at planning, reasoning, and error handling, but worse in learning from randomness and interactions. Vicuna-13b loses a lot of reasoning, planning, long text understanding, and error/mistake handling capabilities after fine-tuning.</p>
<h1>5.2 Qualitative Analysis</h1>
<p>Learning from interactions: In Bandits and Rock Paper Scissors, proprietary LLMs demonstrate promising potential for learning from history and interactions. We observe the agents first following a exploratory strategy and then exploiting the biased opponent based on the past observations. In Crafter, GPT-4 variants consistently attempts to build crafting table with 1 wood and recovers from the failure to build crafting table with 2 woods.
Data/environment contamination: For the Tower of Hanoi, it is expected that the LLMs have been trained on the exact same problem. Surprisingly, although all LLMs are able to provide the solution at the starting configuration where all disks are on the first rod (some may even write out the recurrence for the solution), most LLMs could not solve the problem and gets confused quickly after a few moves, where the disks are distributed over all three rods. We suspect that this is due to the intermediate states do not appear often in the LLM's training sets. Such observation verifies our belief that games could be more robust to dataset contamination.</p>
<p>Spatial Reasoning: We observe that LLMs often have a bad sense of spatial locations and struggle with navigating to new locations. For example, in Minecraft, we often observe LLMs often take moves that are contradictory over time, i.e., a bunch of "move north" followed by a bunch of "move south", undoing a lot of its own efforts at exploration.</p>
<h2>6 Related Works</h2>
<h3>6.1 LLM Evaluation</h3>
<p>The task of evaluating LLM performance has become increasingly challenging given the rapid progression of LLMs. Generalist benchmarks usually employ a wide range of tasks and languages to test general knowledge and reasoning (Hendrycks et al., 2020; Liang et al., 2022; Srivastava et al., 2022a; Zhong et al., 2023), where small language models are getting close performance compared to the state-of-the-art large language models Li et al. (2023); Gunasekar et al. (2023); Eldan \&amp; Li (2023). However, those benchmarks struggle to cover interaction styles like instruction following Ziegler et al. (2019) or conversations Bai et al. (2022). The go-to approach for evaluating LLM for conversation</p>
<p>is pairwise model comparison, which performs pair-wise comparison of output of the LLM and a reference LLMs to produce a ranking (Zheng et al., 2023b). The ranking was originally performed by human, but could be automated with a significantly more powerful LLM (Chiang \&amp; Lee, 2023; Zheng et al., 2023a; Dubois et al., 2023). However, such evaluation techniques depend on an expert model or human who can reliably compare the performance of different LLMs, which limits the application to SOTA LLMs like Claude-2 or GPT-4. Moreover, existing benchmarks fail to capture key characteristics of intelligent agents like understanding of randomness, spatial reasoning, and error handling.</p>
<h1>6.2 Using Games to Evaluate Generalist Agents</h1>
<p>The idea of using games to evaluate the performance of agents has a long history in A.I. Pell (2011); Schaul et al. (2011); Whiteson et al. (2011) presented early ideas and motivation for using games to measure the general capabilities of an agent, and discussed challenges in measuring A.I. agent performance. A series of popular benchmarks (Brockman et al., 2016; Vinyals et al., 2017; Tunyasuvunakool et al., 2020) were created including Atari (Bellemare et al., 2013) and DeepMind lab (Beattie et al., 2016). As the capabilities of A.I. agents improve, researchers developed openended generalist games (Savva et al., 2019; Abramson et al., 2020; Hafner, 2021; Srivastava et al., 2022b) like NetHack (Kttler et al., 2020) or Minecraft (Guss et al., 2021; Fan et al., 2022).</p>
<p>SmartPlay takes a suite of benchmarks (Brockman et al., 2016; Hafner, 2021; Fan et al., 2022) developed over different times to best represent a broad range of difficulties and skills.</p>
<h3>6.3 Creating/Converting to Text Games</h3>
<p>Text games (Ct et al., 2018; Kttler et al., 2020; Zhong et al., 2019; Hanjie et al., 2021) are interactive simulations where the game state and action space are in natural language, often used to benchmark skills like planning, exploration, and memory. SmartPlay features a text game (Messenger) with procedural game rule generation (Hanjie et al., 2021) to test the generalization of the LLM agents at language understanding and planning.</p>
<p>To capture real-world challenges like spatial-reasoning, we study converting 2D/3D games into text-games. Shridhar et al. (2020b) demonstrated the possibility of converting a 3D embodied indoor environment (Shridhar et al., 2020a) into a TextWorld (Ct et al., 2018) game by "listing" all the objects in text. However, such conversion relies on low-level controllers and teleportation, trivializing the environments for current LLMs (Micheli \&amp; Fleuret, 2021; Wu et al., 2023b). Therefore, we follow Wu et al. (2023c) to offer a list of objects/observations with directional relationship to the agent: "to your south-east." Such description allows LLMs to make meaningful progress without low-level controllers (Wu et al., 2023c).</p>
<h2>7 CONCLUSION</h2>
<p>In this work, we introduce SmartPlay, both a challenging benchmark and a methodology for evaluating LLMs' performance as agents. Our initial release of SmartPlay consists of Two-armed Bandits, Rock Paper Scissors, Messenger (Hanjie et al., 2021), Crafter (Hafner, 2021), and Minecraft (Fan et al., 2022) creative navigation tasks. SmartPlay benchmarks not only basic abilities like instruction following and in-context reasoning, but also evaluates capabilities like planning, understanding of randomness, 2D/3D spatial reasoning, and error handling, which are often underrepresented in existing LLM benchmarks. To achieve next-gen automation, we believe that language models should go beyond speaking fluent language (Eldan \&amp; Li, 2023), and become more intelligent agents that could interact with the world and human users. We hope that SmartPlay would catalyze research on building more capable and reliable LLM agents.</p>
<p>Finally, SmartPlay offers guidelines for easily adding games to the benchmarking suite. SmartPlay will be continuously improved to provide up-to-date challenges for next-gen LLMs.</p>
<h1>REFERENCES</h1>
<p>Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, et al. Imitating interactive intelligence. arXiv preprint arXiv:2012.05672, 2020.</p>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.</p>
<p>Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Kttler, Andrew Lefrancq, Simon Green, Vctor Valds, Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016.</p>
<p>Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47: 253-279, 2013.</p>
<p>Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.</p>
<p>Sbastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.</p>
<p>Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937, 2023.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Marc-Alexandre Ct, Akos Kdr, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: A learning environment for text-based games. In Workshop on Computer Games, pp. 41-75. Springer, 2018.</p>
<p>Marc-Alexandre Ct, Akos Kdr, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: A learning environment for text-based games. In Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers 7, pp. 41-75. Springer, 2019.</p>
<p>Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback, 2023.</p>
<p>Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english? arXiv preprint arXiv:2305.07759, 2023.</p>
<p>Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. arXiv preprint arXiv:2206.08853, 2022.</p>
<p>Michael Genesereth, Nathaniel Love, and Barney Pell. General game playing: Overview of the aaai competition. AI magazine, 26(2):62-62, 2005.</p>
<p>Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Csar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023.</p>
<p>William H Guss, Mario Ynocente Castro, Sam Devlin, Brandon Houghton, Noboru Sean Kuno, Crissman Loomis, Stephanie Milani, Sharada Mohanty, Keisuke Nakata, Ruslan Salakhutdinov, et al. The minerl 2020 competition on sample efficient reinforcement learning using human priors. arXiv preprint arXiv:2101.11071, 2021.</p>
<p>Danijar Hafner. Benchmarking the spectrum of agent capabilities. arXiv preprint arXiv:2109.06780, 2021.</p>
<p>Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023.</p>
<p>Austin W Hanjie, Victor Y Zhong, and Karthik Narasimhan. Grounding language to entities and dynamics for generalization in reinforcement learning. In International Conference on Machine Learning, pp. 4051-4062. PMLR, 2021.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.</p>
<p>Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. arXiv preprint arXiv:2303.17491, 2023.</p>
<p>Raph Koster. Theory of fun for game design. " OReilly Media, Inc.", 2013.
Heinrich Kttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici, Edward Grefenstette, and Tim Rocktschel. The nethack learning environment. Advances in Neural Information Processing Systems, 33:7671-7684, 2020.</p>
<p>John E Laird, Allen Newell, and Paul S Rosenbloom. Soar: An architecture for general intelligence. Artificial intelligence, 33(1):1-64, 1987.</p>
<p>Yuanzhi Li, Sbastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.</p>
<p>James Manyika. An overview of bard: an early experiment with generative ai. https://ai. google/static/documents/google-about-bard.pdf. Accessed: May 27, 2023.</p>
<p>Vincent Micheli and Franois Fleuret. Language models are few-shot butlers. arXiv preprint arXiv:2104.07972, 2021.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Joon Sung Park, Joseph C OBrien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.</p>
<p>Barney Pell. Strategy generation and evaluation for meta-game playing. KI-Knstliche Intelligenz, 25(1):71-72, 2011.</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023.</p>
<p>Reworkd. reworkd/agentgpt: Assemble, configure, and deploy autonomous ai agents in your browser. URL https://github.com/reworkd/AgentGPT.</p>
<p>Stuart J Russell. Artificial intelligence a modern approach. Pearson Education, Inc., 2010.
Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied ai research. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9339-9347, 2019.</p>
<p>Tom Schaul, Julian Togelius, and Jrgen Schmidhuber. Measuring intelligence through games. arXiv preprint arXiv:1109.1314, 2011.</p>
<p>Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.</p>
<p>Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10740-10749, 2020a.</p>
<p>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Ct, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020b.</p>
<p>Significant-Gravitas. Significant-gravitas/auto-gpt: An experimental open-source attempt to make gpt-4 fully autonomous. URL https://github.com/Significant-Gravitas/ Auto-GPT.</p>
<p>Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zheng, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. Using deepspeed and megatron to train megatron-turing NLG 530b, A large-scale generative language model. CoRR, abs/2201.11990, 2022. URL https://arxiv.org/abs/2201.11990.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022a.</p>
<p>Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Martn-Martn, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. In Conference on Robot Learning, pp. 477-490. PMLR, 2022b.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa. dm_control: Software and tasks for continuous control. Software Impacts, 6:100022, 2020. ISSN 2665-9638. doi: https://doi. org/10.1016/j.simpa.2020.100022. URL https://www.sciencedirect.com/science/ article/pii/S2665963820300099.</p>
<p>Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Kttler, John Agapiou, Julian Schrittwieser, et al. Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782, 2017.</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.</p>
<p>Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023b.</p>
<p>Shimon Whiteson, Brian Tanner, and Adam White. Report on the 2008 reinforcement learning competition. AI Magazine, 31(2):81-81, 2010.</p>
<p>Shimon Whiteson, Brian Tanner, Matthew E Taylor, and Peter Stone. Protecting against evaluation overfitting in empirical reinforcement learning. In 2011 IEEE symposium on adaptive dynamic programming and reinforcement learning (ADPRL), pp. 120-127. IEEE, 2011.</p>
<p>Michael Wooldridge and Nicholas R Jennings. Intelligent agents: Theory and practice. The knowledge engineering review, 10(2):115-152, 1995.</p>
<p>Yue Wu, Yewen Fan, Paul Pu Liang, Amos Azaria, Yuanzhi Li, and Tom M Mitchell. Read and reap the rewards: Learning to play atari with the help of instruction manuals. arXiv preprint arXiv:2302.04449, 2023a.</p>
<p>Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Yuanzhi Li, Tom Mitchell, and Shrimai Prabhumoye. Plan, eliminate, and track-language models are good teachers for embodied agents. arXiv preprint arXiv:2305.02412, 2023b.</p>
<p>Yue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Tom Mitchell, and Yuanzhi Li. Spring: Gpt-4 out-performs rl algorithms by studying papers and reasoning. arXiv preprint arXiv:2305.15486, 2023c.</p>
<p>Yoheinakajima. yoheinakajima/babyagi. URL https://github.com/yoheinakajima/ babyagi.</p>
<p>Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing Lu. Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks. arXiv preprint arXiv:2303.16563, 2023.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023a.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023b.</p>
<p>Victor Zhong, Tim Rocktschel, and Edward Grefenstette. Rtfm: Generalising to novel environment dynamics via reading. arXiv preprint arXiv:1910.08210, 2019.</p>
<p>Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023.</p>
<p>Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.</p>
<h1>A RESEARCH CHALLENGES</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Games</th>
<th style="text-align: center;">Bandits</th>
<th style="text-align: center;">Rock Paper Scissors</th>
<th style="text-align: center;">Hanoi</th>
<th style="text-align: center;">MessengerL2+</th>
<th style="text-align: center;">Crafter</th>
<th style="text-align: center;">Minecraft</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Long text Understanding <br> 1. few pre-defined lines <br> 2. few paragraphs <br> 3. syntactic variations <br> 4. longer than 1 page</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Reasoning <br> 1. $0 \sim 1$-hop <br> 2. $2 \sim 3$-hop <br> 3. multi-hop</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Instruction/Rule Following <br> 1. single game rule <br> 2. $&lt;5$ game rules <br> 3. $5+$ game rules</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">Planning <br> 1. $&lt;5$ planning steps <br> 2. $5+$ planning steps <br> 3. concurrent objectives</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Generalization <br> 1. fixed environment <br> 2. fixed world, random objective <br> 3. procedurally generated world</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">Understanding the Odds <br> 1. no randomness <br> 2. randomness present in game <br> 3. randomness as core mechanism</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">Learning from Interactions <br> 1. no learning <br> 2. single interaction <br> 3. $&lt;5$ interactions <br> 4. $5+$ interactions</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Error/Mistake Handling <br> 1. not required <br> 2. rollback only <br> 3. reason and re-plan</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">Spatial Reasoning <br> 1. 1D - no spatial reasoning <br> 2. 2D reasoning required <br> 3. 3D reasoning required</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p>Table 3: Research challenges associated with each of the 6 games. Since MessengerL1 does not cover multi-hop reasoning, only L2+ is included.</p>
<h2>B MINECRAFT VISUAL DESCRIPTOR</h2>
<p>The raw ground truth MineDojo (Fan et al., 2022) is a block level matrix (2D matrix for lidar rays and 3D matrix for surrounding blocks), which is very hard for human or LLMs to comprehend. Inspired by Wu et al. (2023c), we adopt a directional visual description scheme to encode the scene observation in text. Specifically, we first run connected component algorithm to group the same blocks that are connect into groups, and then describe the group's relative position to the agent based on the closest block from the group. For example, an observation in Figure 4 will be described as:</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Left: Raw Minecraft environment observation Right: Segmentation map of the environment observation. as detected by lidar rays in MineDojo (Fan et al., 2022).</p>
<p>You see:</p>
<ul>
<li>grass, 3.87 blocks away to your bottom, taking $38 \%$ of screen</li>
<li>plant, 22.10 blocks away to your bottom-right, taking $2 \%$ of screen</li>
<li>water, 33.88 blocks away to your bottom-left, taking $1 \%$ of screen</li>
<li>wood, 18.76 blocks away to your bottom-left, taking $0 \%$ of screen</li>
<li>flower, 32.98 blocks away to your right, taking $0 \%$ of screen</li>
<li>wood, 24.50 blocks away to your left, taking $0 \%$ of screen</li>
<li>leaves, 11.63 blocks away to your left, taking $6 \%$ of screen</li>
<li>leaves, 56.55 blocks away to your right, taking $1 \%$ of screen</li>
</ul>
<h1>C EXAMPLE INPUTS</h1>
<h2>C. 1 BANDITS</h2>
<h2>Example input</h2>
<p>Instruction Manual:
You are in the casino with 2 slot machines in front of you.
Your goal is to try to earn the most from those slot machines.
Observation Example:
You pulled slot machine $\$ 1 \$$, you received reward $\$ r_{-} 1 \$$.</p>
<h2>C. 2 ROCK PAPER SCISSORS</h2>
<h2>Example input</h2>
<p>Instruction Manual:
For the game Rock Paper Scissors, you and the opponent choose one of three options: rock, paper, or scissors. After both players have chosen, the winner is determined as follows:
Rock crushes scissors (Rock wins, score \$s_1\$)
Scissors cut paper (Scissors win, score \$s_2\$)
Paper covers rock (Paper wins, score \$s_3\$)
If you lose, your score is the negative of the winner's score.
If both players choose the same option, it's a draw (score 0 ).
Your goal is to maximize your score.
Observation Example:
You chose \$Rock\$, and the opponent chose \$Scissor\$. You \$won\$ and received score \$s_1\$.
New round begins.</p>
<h2>C. 3 HANOI</h2>
<h2>Example input</h2>
<p>Instruction Manual:
The game consists of three rods ( $A, B, C$ ) and a number of disks of various sizes, which can go onto any rod. The game begins with the disks stacked on rod A in order of decreasing size, the smallest at the top (righthand side). The objective is to move the entire stack to rod C, obeying the following rules:</p>
<ul>
<li>Only one disk may be moved at a time.</li>
<li>Each move consists of taking the top disk from one of the stacks and placing it on top of another stack or on an empty rod.</li>
<li>You cannot place a bigger disk on top of a smaller disk.</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="k">For</span><span class="w"> </span><span class="nv">example</span>,<span class="w"> </span><span class="nv">considering</span><span class="w"> </span><span class="nv">movements</span><span class="w"> </span><span class="nv">from</span><span class="w"> </span><span class="nv">B</span><span class="w"> </span><span class="nv">under</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">following</span><span class="w"> </span><span class="nv">setting</span>:
<span class="o">-</span><span class="w"> </span><span class="nv">A</span>:<span class="w"> </span><span class="o">|</span><span class="nv">bottom</span>,<span class="w"> </span>[<span class="mi">0</span>],<span class="w"> </span><span class="nv">top</span><span class="o">|</span>
<span class="o">-</span><span class="w"> </span><span class="nv">B</span>:<span class="w"> </span><span class="o">|</span><span class="nv">bottom</span>,<span class="w"> </span>[<span class="mi">1</span>],<span class="w"> </span><span class="nv">top</span><span class="o">|</span>
<span class="o">-</span><span class="w"> </span><span class="nv">C</span>:<span class="w"> </span><span class="o">|</span><span class="nv">bottom</span>,<span class="w"> </span>[<span class="mi">2</span>],<span class="w"> </span><span class="nv">top</span><span class="o">|</span>
<span class="nv">You</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">only</span><span class="w"> </span><span class="nv">allowed</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">move</span><span class="w"> </span><span class="nv">from</span><span class="w"> </span><span class="nv">B</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">C</span><span class="w"> </span><span class="nv">but</span><span class="w"> </span><span class="nv">not</span><span class="w"> </span><span class="nv">A</span>,<span class="w"> </span><span class="nv">since</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">top</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">B</span><span class="w"> </span><span class="ss">(</span><span class="mi">1</span><span class="ss">)</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">smaller</span><span class="w"> </span><span class="nv">than</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">top</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">C</span><span class="w"> </span><span class="ss">(</span><span class="mi">2</span><span class="ss">)</span>
<span class="nv">but</span><span class="w"> </span><span class="nv">bigger</span><span class="w"> </span><span class="nv">than</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">top</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">A</span><span class="w"> </span><span class="ss">(</span><span class="mi">0</span><span class="ss">)</span>.
<span class="nv">Finally</span>,<span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">starting</span><span class="w"> </span><span class="nv">configuration</span><span class="w"> </span><span class="nv">is</span>:
<span class="o">-</span><span class="w"> </span><span class="nv">A</span>:<span class="w"> </span><span class="o">|</span><span class="nv">bottom</span>,<span class="w"> </span>[<span class="mi">2</span>,<span class="mi">1</span>,<span class="mi">0</span>],<span class="w"> </span><span class="nv">top</span><span class="o">|</span>
<span class="o">-</span><span class="w"> </span><span class="nv">B</span>:<span class="w"> </span><span class="o">|</span><span class="nv">bottom</span>,<span class="w"> </span>[],<span class="w"> </span><span class="nv">top</span><span class="o">|</span>
<span class="o">-</span><span class="w"> </span><span class="nv">C</span>:<span class="w"> </span><span class="o">|</span><span class="nv">bottom</span>,<span class="w"> </span>[],<span class="w"> </span><span class="nv">top</span><span class="o">|</span>
<span class="nv">and</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">goal</span><span class="w"> </span><span class="nv">configuration</span><span class="w"> </span><span class="nv">is</span>:
<span class="o">-</span><span class="w"> </span><span class="nv">A</span>:<span class="w"> </span><span class="o">|</span><span class="nv">bottom</span>,<span class="w"> </span>[],<span class="w"> </span><span class="nv">top</span><span class="o">|</span>
<span class="o">-</span><span class="w"> </span><span class="nv">B</span>:<span class="w"> </span><span class="o">|</span><span class="nv">bottom</span>,<span class="w"> </span>[],<span class="w"> </span><span class="nv">top</span><span class="o">|</span>
<span class="o">-</span><span class="w"> </span><span class="nv">C</span>:<span class="w"> </span><span class="o">|</span><span class="nv">bottom</span>,<span class="w"> </span>[<span class="mi">2</span>,<span class="mi">1</span>,<span class="mi">0</span>],<span class="w"> </span><span class="nv">top</span><span class="o">|</span>
<span class="nv">with</span><span class="w"> </span><span class="nv">top</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">right</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">bottom</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">left</span>
<span class="nv">Observation</span><span class="w"> </span><span class="nv">Example</span>:
<span class="nv">You</span><span class="w"> </span><span class="nv">tried</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">move</span><span class="w"> </span><span class="nv">top</span><span class="w"> </span><span class="nv">disk</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">rod</span><span class="w"> </span><span class="nv">b</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">top</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">rod</span><span class="w"> </span><span class="nv">a</span>.<span class="w"> </span><span class="nv">Current</span><span class="w"> </span><span class="nv">configuration</span>:
<span class="o">-</span><span class="w"> </span><span class="nv">A</span>:<span class="w"> </span><span class="o">|</span><span class="nv">bottom</span>,<span class="w"> </span>[<span class="mi">2</span>,<span class="w"> </span><span class="mi">1</span>,<span class="w"> </span><span class="mi">0</span>],<span class="w"> </span><span class="nv">top</span><span class="o">|</span>
<span class="o">-</span><span class="w"> </span><span class="nv">B</span>:<span class="w"> </span><span class="o">|</span><span class="nv">bottom</span>,<span class="w"> </span>[],<span class="w"> </span><span class="nv">top</span><span class="o">|</span>
<span class="o">-</span><span class="w"> </span><span class="nv">C</span>:<span class="w"> </span><span class="o">|</span><span class="nv">bottom</span>,<span class="w"> </span>[],<span class="w"> </span><span class="nv">top</span><span class="o">|</span>
</code></pre></div>

<h1>C. 4 MESSENGER</h1>
<h2>Example input</h2>
<p>Instruction Manual:
In the game, MESSENGER, each entity can take on one of three roles: an enemy, message, or goal.
The agent's objective is to bring the message to the goal while avoiding the enemies.
If the agent encounters an enemy at any point in the game, or the goal without first obtaining the message, it loses the game and obtains a reward of -1 .
the dangerous enemy can be found next to the plane, which can not be moved.
you are being approached by a restricted document that is a robot.
the whale is the main objective.
To solve a game, you may find it helpful to list the objects that you see. Then for each object, match it with an entity description, and identify whether it is good or bad to interact with the object.
The name specifications of in-game objects may not be exact matches. Please try identifying with synonyms.
Observation Example:
You took action Move South.
You (agent) don't have the message.
You see:</p>
<ul>
<li>airplane 7 steps to your south</li>
<li>fish 13 steps to your south-east</li>
<li>robot 5 steps to your south-east</li>
</ul>
<h2>C. 5 CRAFter</h2>
<h2>Example input</h2>
<p>Instruction Manual:
Write all information helpful for the game in a numbered list.</p>
<ol>
<li>Collect resources such as wood, stone, and iron to craft tools and weapons.</li>
<li>Build shelters to protect yourself from monsters at night.</li>
<li>Use tools and weapons to defend yourself against monsters.</li>
<li>Build bridges to cross lakes and rivers.</li>
<li>Dig tunnels to surprise monsters and outsmart them.</li>
<li>Plant saplings and defend them against monsters to ensure a steady food supply.</li>
<li>Eat Cow to restore health.</li>
<li>Collect Drink to restore thirst.</li>
<li>Place a Plant to eat for health.</li>
<li>Make a Wood Pickaxe to collect Stone.</li>
<li>Make a Wood Sword to defeat Zombies.</li>
<li>Make a Stone Pickaxe to collect Iron.</li>
<li>Make a Stone Sword to defeat Skeletons.</li>
<li>Place a Furnace to smelt Iron.</li>
<li>Collect Coal to smelt Iron.</li>
<li>Collect Iron to make an Iron Pickaxe and Sword.</li>
<li>Make an Iron Pickaxe to collect Diamond.</li>
<li>Make an Iron Sword to defeat Zombies and Skeletons.</li>
<li>Collect Diamond to progress further.</li>
<li>
<p>Unlock achievements to receive rewards.</p>
</li>
<li>
<p>Wake Up to start the episode.</p>
</li>
</ol>
<p>In plain text. List all objects I need to interact/avoid to survive in the game.
Use "I would like to X object Y" in each step. Replace Y by the actual object, X by the actual interaction.
I would like to avoid zombies, skeletons, and spiders.
I would like to collect saplings.
I would like to craft a wood pickaxe.
I would like to collect wood.
I would like to craft a stone pickaxe.
I would like to collect stone.
I would like to craft a furnace.
I would like to collect coal.
I would like to collect iron.
I would like to craft an iron pickaxe.
I would like to collect diamonds.
I would like to craft an iron sword.
I would like to chase cows.
I would like to grow fruits.
I would like to drink from a lake.
I would like to sleep in a safe place.
I would like to craft a table.
I would like to eat food.
I would like to drink water.
I would like to rest.
I would like to build stone tools to defend myself against monsters.
I would like to build bridges to cross lakes.
I would like to dig tunnels to hide from monsters.
I would like to block arrows with stones.
I would like to dig through walls to surprise skeletons.
I would like to seek shelter in caves.
I would like to build plantations of saplings and defend them against monsters.
I would like to eat the growing fruits to ensure a steady food supply.
I would like to place a table.
I would like to eat a cow.
I would like to place a plant.
I would like to defeat a zombie.
I would like to place stone.
I would like to eat a plant.
I would like to defeat a skeleton.
I would like to wake up.
I would like to place a furnace.
Write all game objectives numbered list. For each objective, list its requirements.</p>
<ol>
<li>Collect Wood: No requirements</li>
<li>Place Table: Requires Collect Wood</li>
<li>Eat Cow: No requirements</li>
<li>Collect Sampling: No requirements</li>
<li>Collect Drink: No requirements</li>
<li>Make Wood Pickaxe: Requires Place Table</li>
<li>Make Wood Sword: Requires Place Table</li>
<li>Place Plant: Requires Collect Sampling</li>
<li>Defeat Zombie: No requirements</li>
<li>Collect Stone: Requires Make Wood Pickaxe</li>
<li>Place Stone: Requires Collect Stone</li>
<li>Eat Plant: Requires Place Plant</li>
<li>Defeat Skeleton: No requirements</li>
<li>Make Stone Pickaxe: Requires Collect Stone</li>
<li>Make Stone Sword: Requires Collect Stone</li>
<li>Wake Up: No requirements</li>
<li>Place Furnace: Requires Collect Stone</li>
<li>Collect Coal: Requires Make Wood Pickaxe</li>
<li>Collect Iron: Requires Make Stone Pickaxe</li>
<li>Make Iron Pickaxe: Requires Place Furnace, Collect Coal, and Collect Iron</li>
<li>Make Iron Sword: Requires Place Furnace, Collect Coal, and Collect Iron</li>
<li>Collect Diamond: Requires Make Iron Pickaxe</li>
</ol>
<p>Write all actions as a numbered list. For each action, list its requirements.</p>
<ol>
<li>Move West: Flat ground west of the agent.</li>
<li>Move East: Flat ground east of the agent.</li>
<li>Move North: Flat ground north of the agent.</li>
<li>Move South: Flat ground south of the agent.</li>
<li>Do: Facing creature or material; have necessary tool.</li>
<li>Sleep: Energy level is below maximum.</li>
<li>Place Stone: Stone in inventory.</li>
<li>Place Table: Wood in inventory.</li>
<li>Place Furnace: Stone in inventory.</li>
<li>Place Plant: Sapling in inventory.</li>
<li>Make Wood Pickaxe: Nearby table; wood in inventory.</li>
<li>Make Stone Pickaxe: Nearby table; wood, stone in inventory.</li>
<li>Make Iron Pickaxe: Nearby table, furnace; wood, coal, iron an inventory.</li>
<li>
<p>Make Wood Sword: Nearby table; wood in inventory.</p>
</li>
<li>
<p>Make Stone Sword: Nearby table; wood, stone in inventory.</p>
</li>
<li>Make Iron Sword: Nearby table, furnace; wood, coal, iron in inventory.</li>
<li>Noop: Always applicable.</li>
</ol>
<p>Observation Example:
You took action move_west.
You see:</p>
<ul>
<li>water 5 steps to your south-west</li>
<li>grass 1 steps to your west</li>
<li>sand 4 steps to your south-west</li>
</ul>
<p>You face grass at your front.
Your status:</p>
<ul>
<li>health: $9 / 9$</li>
<li>food: $9 / 9$</li>
<li>drink: $9 / 9$</li>
<li>energy: $9 / 9$</li>
</ul>
<p>You have nothing in your inventory.</p>
<h1>C. 6 Minecraft</h1>
<h2>Example input</h2>
<p>Instruction Manual:
You are in Minecraft and your goal is to find a forest biome. You are not allowed to craft anything.
In your observation, you are provided the amount of space an object takes in your field of view. Note that objects of the same size takes more space when they are closer to you.</p>
<p>Observation Example:
You took action Move East.
Coordinate $(-253.12,71.75,248.64)$. Facing east.
You're not aiming at any block.
Around you:</p>
<ul>
<li>leaves, 3.96 blocks away, above you to north-west</li>
<li>wood, 4.24 blocks away, to north-west</li>
<li>grass block, 1.34 blocks away, below you to north-east</li>
<li>dirt, 3.33 blocks away, below you to north-east</li>
<li>stone, 4.67 blocks away, below you to north-east</li>
</ul>
<p>You see:</p>
<ul>
<li>grass block, 1.36 blocks away, below you to north-west, taking $51 \%$ of screen</li>
<li>sand, 8.29 blocks away, below you to south-west, taking $4 \%$ of screen</li>
<li>leaves, 4.47 blocks away, above you to north-west, taking $17 \%$ of screen</li>
<li>grass, 5.49 blocks away, above you to north-west, taking $1 \%$ of screen</li>
<li>wood, 11.07 blocks away, above you to north-west, taking $0 \%$ of screen</li>
</ul>
<h2>D ADDITIONAL EXPERIMENTAL RESULTS</h2>
<h2>D. 1 HUMAN BASELINE</h2>
<p>3 players (including the authors) who are very familiar with the environments and API played the games through the SmartPlay interface. Each human player performed $\mathbf{3}$ rounds of Bandit, RPS; $\mathbf{1}$ round of Hanoi, Crafter, Minecraft; 5 rounds of MessengerL1, MessengerL2. We report the final average score over all trials and all players.</p>
<h2>D. 2 NORMALIZED HUMAN SCORE</h2>
<p>Given the game score of an LLM on game $g, s_{g}^{\text {(raw) }}$, we compute normalized human score $s_{g}$ from the human baseline on $g, s_{g}^{\text {(human) }}$, and the minimum possible game score $s_{g}^{\text {(min) }}$ :</p>
<p>$$
s_{g}=\frac{s_{g}^{\text {(human) }}-s_{g}^{\text {(raw })}}{s_{g}^{\text {(human) }}-s_{g}^{\text {(min }})}
$$</p>
<p>D. 3 RAW SCORES</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">Bandit</th>
<th style="text-align: center;">RPS</th>
<th style="text-align: center;">Hanoi</th>
<th style="text-align: center;">MessengerL1</th>
<th style="text-align: center;">MessengerL2</th>
<th style="text-align: center;">Crafter</th>
<th style="text-align: center;">Minecraft</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Human Baseline</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2680</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4-0613</td>
<td style="text-align: center;">45.09</td>
<td style="text-align: center;">39.25</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">700</td>
<td style="text-align: center;">0.61</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4-0314</td>
<td style="text-align: center;">43.86</td>
<td style="text-align: center;">42.05</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">845.6</td>
<td style="text-align: center;">0.592</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-003</td>
<td style="text-align: center;">46.92</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">-0.07</td>
<td style="text-align: center;">186.25</td>
<td style="text-align: center;">0.449</td>
</tr>
<tr>
<td style="text-align: center;">Claude</td>
<td style="text-align: center;">32.43</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">-0.12</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">143.3</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;">Bard</td>
<td style="text-align: center;">38.85</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">-0.21</td>
<td style="text-align: center;">112.3</td>
<td style="text-align: center;">0.54</td>
</tr>
<tr>
<td style="text-align: center;">llama-2-13b</td>
<td style="text-align: center;">22.33</td>
<td style="text-align: center;">15.05</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">-0.76</td>
<td style="text-align: center;">-0.745</td>
<td style="text-align: center;">115.3</td>
<td style="text-align: center;">0.606</td>
</tr>
<tr>
<td style="text-align: center;">llama-13b</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-0.68</td>
<td style="text-align: center;">-0.885</td>
<td style="text-align: center;">100.2</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;">vicuna-13b</td>
<td style="text-align: center;">28.81</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">-0.76</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">0.43</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of performance of different LLMs in terms of average score on BanditTwoArmedHighLowFixed-v0, RockPaperScissorBasic-v0, Hanoi3Disk-v0, MessengerL1v0, MessengerL2-v0, Crafter-v0, MinedojoCreative0-v0.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ wikipedia.org/wiki/Minecraft&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
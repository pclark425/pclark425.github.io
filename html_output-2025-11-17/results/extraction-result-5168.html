<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5168 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5168</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5168</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-109.html">extraction-schema-109</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <p><strong>Paper ID:</strong> paper-266291567</p>
                <p><strong>Paper Title:</strong> Get Spatial from Non-Spatial Information: Inferring Spatial Information from Textual Descriptions by Conceptual Spaces</p>
                <p><strong>Paper Abstract:</strong> : With the rapid growth of social media, textual content is increasingly growing. Unstruc-tured texts are a rich source of latent spatial information. Extracting such information is useful in query processing, geographical information retrieval (GIR), and recommender systems. In this paper, we propose a novel approach to infer spatial information from salient features of non-spatial nature in text corpora. We propose two methods, namely DCS and RCS, to represent place-based concepts. In addition, two measures, namely the Shannon entropy and the Moran’s I, are proposed to calculate the degree of geo-indicativeness of terms in texts. The methodology is compared with a Latent Dirichlet Allocation (LDA) approach to estimate the accuracy improvement. We evaluated the methods on a dataset of rental property advertisements in Iran and a dataset of Persian Wikipedia articles. The results show that our proposed approach enhances the relative accuracy of predictions by about 10% in case of the renting advertisements and by 13% in case of the Wikipedia articles. The average distance error is about 13.3 km for the advertisements and 10.3 km for the Wikipedia articles, making the method suitable to infer the general region of the city in which a property is located. The proposed methodology is promising for inferring spatial knowledge from textual content that lacks spatial terms.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5168.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5168.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conceptual Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conceptual spaces (Gärdenfors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A geometric, similarity-based knowledge representation framework that represents concepts as regions in a metric space spanned by quality dimensions; similarity and topology are primary primitives and concepts are convex regions with prototypes as central points.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Conceptual Spaces: The Geometry of Thought</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Conceptual spaces</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Conceptual spaces posit that conceptual knowledge is represented functionally as points and convex regions in a multi-dimensional metric space whose axes are interpretable quality dimensions; concepts correspond to convex regions, prototypes to central points, properties to values along single dimensions, and similarity is given by geometric (metric) distance.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>geometry-based / prototype-region (feature-based continuous metric space)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Similarity as metric distance; concepts = convex regions; prototypes as central exemplars; interpretable quality dimensions (domains); supports topological relations and graded membership; amenable to geometric operations (intersection, projection).</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Used in cognitive modeling and applied work cited by the paper (e.g., cognitive science MDS studies); in this paper, data-driven instantiations (RCS, DCS) built from corpora produced interpretable semantic surfaces and enabled spatial inference tasks that outperform an LDA baseline (DCS reduced geolocation error and provided intuitive direction vectors).</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Requires identification of relevant quality dimensions (often not known a priori); prototype assumption may fail when prototypical central instances are rare in empirical data; data-driven construction needs careful preprocessing and can be sensitive to noise (noted by authors when constructing RCS/DCS).</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Concept representation and comparison, categorization, similarity judgments, semantic modeling of text (this paper: geolocation inference from text), recommender systems, topological reasoning over concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Contrasted with symbolic (FOL) models: conceptual spaces provide graded similarity and continuous representations rather than discrete symbols; contrasted with associationist/connectionist models: conceptual spaces are more interpretable and geometrically explicit while connectionist models are distributed and less interpretable; compared empirically here against LDA topic models and shown to be more interpretable and (DCS) more accurate for the spatial inference task.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Concept formation as convex regions in quality-dimension space; similarity-based retrieval by metric distance; prototypes as central points guide classification; operations such as intersection/combination yield new concepts; dimensions can be discovered from data (MDS) and documents placed as points/instances.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How to select/learn semantically meaningful dimensions robustly from noisy corpora; scalability and stability of convex-region assumptions in high-dimensional data; formal criteria for when prototype representations are appropriate; principled thresholds for geo-indicativeness in applied instantiations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5168.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5168.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prototype theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prototype theory (as used in conceptual spaces)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A view that concepts are organized around prototypical central instances rather than strict necessary-and-sufficient features; membership is graded by similarity to the prototype.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Prototype theory</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Conceptual categories are represented functionally by prototypical exemplars (central tendency) and graded membership is determined by similarity to these prototypes rather than Boolean rules.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>prototype / graded exemplar-based (central tendency in feature space)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Graded membership, typicality effects, central prototype as most representative instance, similarity-based categorization, non-classical boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Widely reported in behavioral categorization literature (cited in conceptual-spaces literature); in this paper prototype idea motivates convex-region/prototype representation, and authors note that prototypical instances may be rare in data-driven corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Some real-world categories lack single clear prototypes; prototype-centered geometry can be inappropriate when categories are multi-modal or highly heterogeneous; authors note prototype assumption is sometimes unrealistic for corpus-derived concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Categorization, naming, typicality judgments, semantic similarity modeling; used as conceptual basis for conceptual spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared implicitly to symbolic rule-based models (which use necessary/sufficient conditions) and exemplar models (which store many specific instances); prototype theory offers compact representation but can lose fine-grained instance information.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Categorization by computing similarity (distance) to prototype; prototypicality drives inference and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>When and how prototypes emerge from data; handling multi-modal categories; operationalizing prototype centrality in data-driven, high-dimensional spaces.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5168.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5168.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic models (FOL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic representation and rule-based (first-order logic) models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models that represent knowledge as discrete symbols and structured relations manipulated by explicit rules, typically executed sequentially.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Symbolic (rule-based) models / First-Order Logic</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Conceptual knowledge is represented as discrete symbols and relational structures (predicates, rules) that are operated on by symbolic inference mechanisms (e.g., logical deduction) in a serial, rule-driven manner.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>symbolic / rule-based</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Discrete compositional structure, explicit logic/rules, high interpretability, supports compositional semantics and systematic manipulation of symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Historical success in formal knowledge representation and reasoning; mentioned as a contrasting functional-level model to conceptual spaces and associationist models in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Poor fit to graded similarity phenomena, less natural for similarity-based categorization, limited parallel processing, and often low interpretability when mapping to perceptual feature similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Knowledge representation, theorem proving, formal semantic systems, symbolic AI.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Strength: compositional structure and explicit inference; Weakness: poor at graded similarity and continuous feature-based reasoning compared to conceptual spaces and connectionist models.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Symbol manipulation via rule application; concept combination via symbolic composition.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Integration with perceptual similarity data; modeling graded category membership and prototypicality.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5168.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5168.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Associationist / Connectionist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Associationist / connectionist (distributed) models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Functional models that represent concepts as patterns across many distributed units (weights); knowledge emerges from associations and parallel activation rather than discrete symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Associationist / connectionist models</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Concepts are represented by distributed activation patterns across units (features) in a network; processing is parallel and associative, with concepts arising from learned weight patterns rather than symbolic rules.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>distributed / connectionist</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Distributed overlapping representations, graceful degradation, parallel processing, learning-driven emergence of structure, often less interpretable than geometric symbolic systems.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Connectionist/neural-network models successfully capture many learning and generalization phenomena; paper contrasts them with conceptual spaces noting interpretability differences.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Lower interpretability of internal representations; difficulty mapping distributed codes to semantically interpretable dimensions; may not give explicit similarity geometry understandable to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Perceptual categorization, pattern recognition, many machine learning tasks (e.g., embeddings), distributed semantic representations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Better at capturing learning dynamics and distributed generalization than symbolic models; less interpretable and less directly compositional than conceptual-spaces representations.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Learning via weight updates; concept activation via spreading activation across units; retrieval by pattern completion.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How distributed codes correspond to interpretable quality dimensions; tradeoffs between performance and interpretability; integrating symbolic compositionality.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5168.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5168.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Regional Conceptual Spaces (RCS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Regional Conceptual Spaces (RCS) — data-driven instantiation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-driven method in this paper that constructs a conceptual space by embedding documents (via PPMI/TF-IDF + MDS), clustering documents into regions (k-means), and representing concepts as convex regions (prototypes = convex-hull centers) to infer spatial information from text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Regional Conceptual Spaces (RCS)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>RCS operationalizes conceptual spaces by mapping documents to points in an MDS-derived similarity space (from TF-IDF or PPMI-weighted BOWs), clustering similar documents to form regions, and treating clusters' convex hulls/prototypes as concept regions for similarity-based retrieval and spatial inference.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>data-driven geometric regions / prototype-region</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Documents as points; clusters = concept regions; prototypes as cluster centers or convex-hull-derived points; interpretable grouping at region-level; depends on clustering (k-means) and dimension choice.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Applied to two corpora (Divar rental ads N=11,393; Persian Wikipedia N=704). RCS produced identifiable geo-indicative clusters (2/20 clusters geo-indicative in Divar), but overall accuracy was worse than DCS and LDA in geolocation prediction (some RCS predictions equaled random baseline); silhouette-guided k=20 chosen for clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Authors report RCS produced less interpretable cluster meanings, sensitivity to choice of k and dimensions, and poorer predictive accuracy (some predictions failed entirely); prototype location sometimes far from geographical hotspots indicating semantic clusters that do not map to geographic clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Geolocation inference from text, clustering of documents in semantic space, exploratory semantic mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared directly to DCS and LDA in experiments: RCS underperformed both (worse accuracy, sometimes equal to random), though it preserves the regional/convex-region idea of classical conceptual spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Construct BOW weight vectors (TF-IDF/PPMI) → apply MDS to get document coordinates → cluster (k-means) → map clusters to geographic divisions via overlap of conceptual and geographic regions → assign prototype centroids for inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Cluster interpretability and stability; choosing k and MDS dimensionality; mapping cluster geometry to real-world spatial regions reliably; sensitivity to corpus preprocessing and frequency thresholds.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5168.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5168.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Directional Conceptual Spaces (DCS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Directional Conceptual Spaces (DCS) — adapted / data-driven</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-driven directional variant of conceptual spaces used in the paper that represents salient words as meaningful directions (hyperplane normals from SVMs) in an MDS similarity space; documents' projections onto these directions provide similarity measures for spatial inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Directional Conceptual Spaces (DCS)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>DCS represents conceptual distinctions functionally as directions in a document similarity space: for each candidate word, an SVM partitions documents into containing vs. not-containing classes; the hyperplane normal defines a direction along which salience varies, and documents' angular proximity to clustered directions yields similarity and geospatial inference.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>directional vector-based geometric (interpretable direction axes)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Directionality (word-specific axes), SVM-derived hyperplanes as interpretable feature directions, projection-based similarity measures, dimensionality-sensitive, supports clustering of similar directions to yield robust semantic axes.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>In experiments DCS outperformed LDA and RCS: average distance error ~13.3 km (Divar) and ~10.3 km (Wikipedia), predicting general city region; DCS improved location prediction over LDA by ~10% (Divar) and ~13% (Wikipedia). PPMI weighting typically yielded better SVM classification than TF-IDF; SVM classification accuracy threshold of >80% used to select strong directions; higher MDS dimensions improved classification accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Computationally intensive to train SVM for many words; requires selection/filtering of top words (authors used top 10% by TF-IDF/PPMI) and dimensionality tuning; some broad terms (e.g., 'luxurious') yielded poor classification regardless of weighting.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Geolocation inference from text, recommender systems (similarity comparisons), interpretable semantic axis discovery from corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Shown to be more accurate than LDA and RCS for the geolocation task and argued to be more interpretable than connectionist embeddings; DCS is an adaptation of Abbasi & Alesheikh approach but uses angular differences for similarity rather than projected-length differences.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Compute BOW weights → MDS for document coordinates → select top salient words → train linear SVM per word to get hyperplane normal (direction) → cluster similar directions → compute angular distances of documents to directions → assign geographic division with minimal angular difference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Scalability over large vocabularies; automated selection criteria for geo-indicative directions; sensitivity to preprocessing thresholds and MDS dimensionality; need for principled stopping rules for direction clustering.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5168.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5168.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LDA (topic models)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent Dirichlet Allocation (LDA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative probabilistic topic model that represents documents as mixtures of topics and topics as distributions over words; used here as a baseline semantic representation for spatial inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Latent dirichlet allocation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Latent Dirichlet Allocation (LDA)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>LDA models documents functionally as probabilistic mixtures over a fixed number of latent topics, with topics being multinomial distributions over words; document-topic probabilities can be projected spatially (e.g., KDE) to derive spatial signatures (semantic signatures).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>probabilistic topic model (statistical latent-variable)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Documents = mixtures of latent topics; topics = word probability distributions; captures co-occurrence structure; outputs probability surfaces per topic; interpretable to some degree via top words.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Prior work (Adams & Janowicz) used LDA + KDE to derive semantic signatures for places; in this paper LDA was applied (k=6 for ads, k=10 for Wikipedia) and KDE/IDW used to make spatial surfaces; LDA produced geo-indicative topics but was outperformed by DCS on geolocation error.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Topic surfaces via IDW were too point-respecting/rough; LDA lacks direct geometric interpretability of conceptual spaces and in this task produced higher mean geolocation error than DCS (DCS improved accuracy by ~10–13%).</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Topic modeling, semantic signature mapping, spatial semantic inference, document clustering and summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>LDA (probabilistic latent) vs conceptual spaces (geometric): LDA is statistical and less geometrically interpretable; conceptual-spaces variants yielded better spatial inference and interpretability for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Estimate topic-word and document-topic distributions via Dirichlet priors; use document-topic probabilities to compute spatial probability surfaces (KDE/IDW) and compute entropy/Moran's I to assess geo-indicativeness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Choosing k (topics) affects interpretability; mapping probabilistic topics to concrete geographic regions can be noisy and sensitive to sampling density; smoothing/interpolation choices (IDW vs KDE) affect surfaces.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5168.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5168.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MDS (constructive method)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multidimensional scaling (MDS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dimensionality-reduction technique that embeds objects (here documents) into a low-dimensional metric space preserving pairwise dissimilarities and is used here to instantiate quality dimensions of a conceptual space from data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multidimensional scaling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Multidimensional scaling (MDS) as a method for constructing conceptual spaces</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>MDS takes pairwise similarity/dissimilarity measures and computes coordinates in a low-dimensional Euclidean space that preserve those relations as distances; in conceptual-spaces instantiation, MDS provides the interpretable geometric coordinates (quality-dimension axes) for documents and concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>data-driven geometric embedding / dimension-learning method</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Preserves pairwise dis/similarities as metric distances; configurable embedding dimensionality; widely used in cognitive studies to elicit perceived similarity structures.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Authors use MDS on TF-IDF / PPMI-weighted document vectors to construct conceptual spaces for both RCS and DCS; higher embedding dimensionality (5,10,20 tested) improved SVM classification accuracy for DCS.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Choice of dimensionality affects downstream classifiers and interpretability; MDS outputs may be sensitive to noisy or sparse similarity estimates, requiring careful preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Constructing low-dimensional similarity-preserving representations, cognitive similarity modeling, as preprocessing step for conceptual-space methods.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared implicitly to neural embedding methods (doc2vec) — authors prefer MDS for interpretability even if neural embeddings may offer performance gains.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Compute dis/similarity matrix from weighted BOW vectors → solve for coordinates minimizing stress to place documents as points in Euclidean space → use coordinates for clustering, SVM direction-finding, and projection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How to link MDS dimensions to semantically meaningful quality dimensions; stability under corpus changes; scaling to very large corpora.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Conceptual Spaces: The Geometry of Thought <em>(Rating: 2)</em></li>
                <li>A metric conceptual space algebra <em>(Rating: 2)</em></li>
                <li>Latent dirichlet allocation <em>(Rating: 2)</em></li>
                <li>A Place Recommendation Approach Using Word Embeddings in Conceptual Spaces <em>(Rating: 2)</em></li>
                <li>Data-driven Conceptual Spaces: Creating Semantic Representations for Linguistic Descriptions Of Numerical Data <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5168",
    "paper_id": "paper-266291567",
    "extraction_schema_id": "extraction-schema-109",
    "extracted_data": [
        {
            "name_short": "Conceptual Spaces",
            "name_full": "Conceptual spaces (Gärdenfors)",
            "brief_description": "A geometric, similarity-based knowledge representation framework that represents concepts as regions in a metric space spanned by quality dimensions; similarity and topology are primary primitives and concepts are convex regions with prototypes as central points.",
            "citation_title": "Conceptual Spaces: The Geometry of Thought",
            "mention_or_use": "use",
            "theory_or_model_name": "Conceptual spaces",
            "theory_or_model_description": "Conceptual spaces posit that conceptual knowledge is represented functionally as points and convex regions in a multi-dimensional metric space whose axes are interpretable quality dimensions; concepts correspond to convex regions, prototypes to central points, properties to values along single dimensions, and similarity is given by geometric (metric) distance.",
            "representation_format_type": "geometry-based / prototype-region (feature-based continuous metric space)",
            "key_properties": "Similarity as metric distance; concepts = convex regions; prototypes as central exemplars; interpretable quality dimensions (domains); supports topological relations and graded membership; amenable to geometric operations (intersection, projection).",
            "empirical_support": "Used in cognitive modeling and applied work cited by the paper (e.g., cognitive science MDS studies); in this paper, data-driven instantiations (RCS, DCS) built from corpora produced interpretable semantic surfaces and enabled spatial inference tasks that outperform an LDA baseline (DCS reduced geolocation error and provided intuitive direction vectors).",
            "empirical_challenges": "Requires identification of relevant quality dimensions (often not known a priori); prototype assumption may fail when prototypical central instances are rare in empirical data; data-driven construction needs careful preprocessing and can be sensitive to noise (noted by authors when constructing RCS/DCS).",
            "applied_domains_or_tasks": "Concept representation and comparison, categorization, similarity judgments, semantic modeling of text (this paper: geolocation inference from text), recommender systems, topological reasoning over concepts.",
            "comparison_to_other_models": "Contrasted with symbolic (FOL) models: conceptual spaces provide graded similarity and continuous representations rather than discrete symbols; contrasted with associationist/connectionist models: conceptual spaces are more interpretable and geometrically explicit while connectionist models are distributed and less interpretable; compared empirically here against LDA topic models and shown to be more interpretable and (DCS) more accurate for the spatial inference task.",
            "functional_mechanisms": "Concept formation as convex regions in quality-dimension space; similarity-based retrieval by metric distance; prototypes as central points guide classification; operations such as intersection/combination yield new concepts; dimensions can be discovered from data (MDS) and documents placed as points/instances.",
            "limitations_or_open_questions": "How to select/learn semantically meaningful dimensions robustly from noisy corpora; scalability and stability of convex-region assumptions in high-dimensional data; formal criteria for when prototype representations are appropriate; principled thresholds for geo-indicativeness in applied instantiations.",
            "uuid": "e5168.0"
        },
        {
            "name_short": "Prototype theory",
            "name_full": "Prototype theory (as used in conceptual spaces)",
            "brief_description": "A view that concepts are organized around prototypical central instances rather than strict necessary-and-sufficient features; membership is graded by similarity to the prototype.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Prototype theory",
            "theory_or_model_description": "Conceptual categories are represented functionally by prototypical exemplars (central tendency) and graded membership is determined by similarity to these prototypes rather than Boolean rules.",
            "representation_format_type": "prototype / graded exemplar-based (central tendency in feature space)",
            "key_properties": "Graded membership, typicality effects, central prototype as most representative instance, similarity-based categorization, non-classical boundaries.",
            "empirical_support": "Widely reported in behavioral categorization literature (cited in conceptual-spaces literature); in this paper prototype idea motivates convex-region/prototype representation, and authors note that prototypical instances may be rare in data-driven corpora.",
            "empirical_challenges": "Some real-world categories lack single clear prototypes; prototype-centered geometry can be inappropriate when categories are multi-modal or highly heterogeneous; authors note prototype assumption is sometimes unrealistic for corpus-derived concepts.",
            "applied_domains_or_tasks": "Categorization, naming, typicality judgments, semantic similarity modeling; used as conceptual basis for conceptual spaces.",
            "comparison_to_other_models": "Compared implicitly to symbolic rule-based models (which use necessary/sufficient conditions) and exemplar models (which store many specific instances); prototype theory offers compact representation but can lose fine-grained instance information.",
            "functional_mechanisms": "Categorization by computing similarity (distance) to prototype; prototypicality drives inference and generalization.",
            "limitations_or_open_questions": "When and how prototypes emerge from data; handling multi-modal categories; operationalizing prototype centrality in data-driven, high-dimensional spaces.",
            "uuid": "e5168.1"
        },
        {
            "name_short": "Symbolic models (FOL)",
            "name_full": "Symbolic representation and rule-based (first-order logic) models",
            "brief_description": "Models that represent knowledge as discrete symbols and structured relations manipulated by explicit rules, typically executed sequentially.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Symbolic (rule-based) models / First-Order Logic",
            "theory_or_model_description": "Conceptual knowledge is represented as discrete symbols and relational structures (predicates, rules) that are operated on by symbolic inference mechanisms (e.g., logical deduction) in a serial, rule-driven manner.",
            "representation_format_type": "symbolic / rule-based",
            "key_properties": "Discrete compositional structure, explicit logic/rules, high interpretability, supports compositional semantics and systematic manipulation of symbols.",
            "empirical_support": "Historical success in formal knowledge representation and reasoning; mentioned as a contrasting functional-level model to conceptual spaces and associationist models in the paper.",
            "empirical_challenges": "Poor fit to graded similarity phenomena, less natural for similarity-based categorization, limited parallel processing, and often low interpretability when mapping to perceptual feature similarity.",
            "applied_domains_or_tasks": "Knowledge representation, theorem proving, formal semantic systems, symbolic AI.",
            "comparison_to_other_models": "Strength: compositional structure and explicit inference; Weakness: poor at graded similarity and continuous feature-based reasoning compared to conceptual spaces and connectionist models.",
            "functional_mechanisms": "Symbol manipulation via rule application; concept combination via symbolic composition.",
            "limitations_or_open_questions": "Integration with perceptual similarity data; modeling graded category membership and prototypicality.",
            "uuid": "e5168.2"
        },
        {
            "name_short": "Associationist / Connectionist",
            "name_full": "Associationist / connectionist (distributed) models",
            "brief_description": "Functional models that represent concepts as patterns across many distributed units (weights); knowledge emerges from associations and parallel activation rather than discrete symbols.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Associationist / connectionist models",
            "theory_or_model_description": "Concepts are represented by distributed activation patterns across units (features) in a network; processing is parallel and associative, with concepts arising from learned weight patterns rather than symbolic rules.",
            "representation_format_type": "distributed / connectionist",
            "key_properties": "Distributed overlapping representations, graceful degradation, parallel processing, learning-driven emergence of structure, often less interpretable than geometric symbolic systems.",
            "empirical_support": "Connectionist/neural-network models successfully capture many learning and generalization phenomena; paper contrasts them with conceptual spaces noting interpretability differences.",
            "empirical_challenges": "Lower interpretability of internal representations; difficulty mapping distributed codes to semantically interpretable dimensions; may not give explicit similarity geometry understandable to humans.",
            "applied_domains_or_tasks": "Perceptual categorization, pattern recognition, many machine learning tasks (e.g., embeddings), distributed semantic representations.",
            "comparison_to_other_models": "Better at capturing learning dynamics and distributed generalization than symbolic models; less interpretable and less directly compositional than conceptual-spaces representations.",
            "functional_mechanisms": "Learning via weight updates; concept activation via spreading activation across units; retrieval by pattern completion.",
            "limitations_or_open_questions": "How distributed codes correspond to interpretable quality dimensions; tradeoffs between performance and interpretability; integrating symbolic compositionality.",
            "uuid": "e5168.3"
        },
        {
            "name_short": "Regional Conceptual Spaces (RCS)",
            "name_full": "Regional Conceptual Spaces (RCS) — data-driven instantiation",
            "brief_description": "A data-driven method in this paper that constructs a conceptual space by embedding documents (via PPMI/TF-IDF + MDS), clustering documents into regions (k-means), and representing concepts as convex regions (prototypes = convex-hull centers) to infer spatial information from text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_or_model_name": "Regional Conceptual Spaces (RCS)",
            "theory_or_model_description": "RCS operationalizes conceptual spaces by mapping documents to points in an MDS-derived similarity space (from TF-IDF or PPMI-weighted BOWs), clustering similar documents to form regions, and treating clusters' convex hulls/prototypes as concept regions for similarity-based retrieval and spatial inference.",
            "representation_format_type": "data-driven geometric regions / prototype-region",
            "key_properties": "Documents as points; clusters = concept regions; prototypes as cluster centers or convex-hull-derived points; interpretable grouping at region-level; depends on clustering (k-means) and dimension choice.",
            "empirical_support": "Applied to two corpora (Divar rental ads N=11,393; Persian Wikipedia N=704). RCS produced identifiable geo-indicative clusters (2/20 clusters geo-indicative in Divar), but overall accuracy was worse than DCS and LDA in geolocation prediction (some RCS predictions equaled random baseline); silhouette-guided k=20 chosen for clustering.",
            "empirical_challenges": "Authors report RCS produced less interpretable cluster meanings, sensitivity to choice of k and dimensions, and poorer predictive accuracy (some predictions failed entirely); prototype location sometimes far from geographical hotspots indicating semantic clusters that do not map to geographic clustering.",
            "applied_domains_or_tasks": "Geolocation inference from text, clustering of documents in semantic space, exploratory semantic mapping.",
            "comparison_to_other_models": "Compared directly to DCS and LDA in experiments: RCS underperformed both (worse accuracy, sometimes equal to random), though it preserves the regional/convex-region idea of classical conceptual spaces.",
            "functional_mechanisms": "Construct BOW weight vectors (TF-IDF/PPMI) → apply MDS to get document coordinates → cluster (k-means) → map clusters to geographic divisions via overlap of conceptual and geographic regions → assign prototype centroids for inference.",
            "limitations_or_open_questions": "Cluster interpretability and stability; choosing k and MDS dimensionality; mapping cluster geometry to real-world spatial regions reliably; sensitivity to corpus preprocessing and frequency thresholds.",
            "uuid": "e5168.4"
        },
        {
            "name_short": "Directional Conceptual Spaces (DCS)",
            "name_full": "Directional Conceptual Spaces (DCS) — adapted / data-driven",
            "brief_description": "A data-driven directional variant of conceptual spaces used in the paper that represents salient words as meaningful directions (hyperplane normals from SVMs) in an MDS similarity space; documents' projections onto these directions provide similarity measures for spatial inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_or_model_name": "Directional Conceptual Spaces (DCS)",
            "theory_or_model_description": "DCS represents conceptual distinctions functionally as directions in a document similarity space: for each candidate word, an SVM partitions documents into containing vs. not-containing classes; the hyperplane normal defines a direction along which salience varies, and documents' angular proximity to clustered directions yields similarity and geospatial inference.",
            "representation_format_type": "directional vector-based geometric (interpretable direction axes)",
            "key_properties": "Directionality (word-specific axes), SVM-derived hyperplanes as interpretable feature directions, projection-based similarity measures, dimensionality-sensitive, supports clustering of similar directions to yield robust semantic axes.",
            "empirical_support": "In experiments DCS outperformed LDA and RCS: average distance error ~13.3 km (Divar) and ~10.3 km (Wikipedia), predicting general city region; DCS improved location prediction over LDA by ~10% (Divar) and ~13% (Wikipedia). PPMI weighting typically yielded better SVM classification than TF-IDF; SVM classification accuracy threshold of &gt;80% used to select strong directions; higher MDS dimensions improved classification accuracy.",
            "empirical_challenges": "Computationally intensive to train SVM for many words; requires selection/filtering of top words (authors used top 10% by TF-IDF/PPMI) and dimensionality tuning; some broad terms (e.g., 'luxurious') yielded poor classification regardless of weighting.",
            "applied_domains_or_tasks": "Geolocation inference from text, recommender systems (similarity comparisons), interpretable semantic axis discovery from corpora.",
            "comparison_to_other_models": "Shown to be more accurate than LDA and RCS for the geolocation task and argued to be more interpretable than connectionist embeddings; DCS is an adaptation of Abbasi & Alesheikh approach but uses angular differences for similarity rather than projected-length differences.",
            "functional_mechanisms": "Compute BOW weights → MDS for document coordinates → select top salient words → train linear SVM per word to get hyperplane normal (direction) → cluster similar directions → compute angular distances of documents to directions → assign geographic division with minimal angular difference.",
            "limitations_or_open_questions": "Scalability over large vocabularies; automated selection criteria for geo-indicative directions; sensitivity to preprocessing thresholds and MDS dimensionality; need for principled stopping rules for direction clustering.",
            "uuid": "e5168.5"
        },
        {
            "name_short": "LDA (topic models)",
            "name_full": "Latent Dirichlet Allocation (LDA)",
            "brief_description": "A generative probabilistic topic model that represents documents as mixtures of topics and topics as distributions over words; used here as a baseline semantic representation for spatial inference.",
            "citation_title": "Latent dirichlet allocation",
            "mention_or_use": "use",
            "theory_or_model_name": "Latent Dirichlet Allocation (LDA)",
            "theory_or_model_description": "LDA models documents functionally as probabilistic mixtures over a fixed number of latent topics, with topics being multinomial distributions over words; document-topic probabilities can be projected spatially (e.g., KDE) to derive spatial signatures (semantic signatures).",
            "representation_format_type": "probabilistic topic model (statistical latent-variable)",
            "key_properties": "Documents = mixtures of latent topics; topics = word probability distributions; captures co-occurrence structure; outputs probability surfaces per topic; interpretable to some degree via top words.",
            "empirical_support": "Prior work (Adams & Janowicz) used LDA + KDE to derive semantic signatures for places; in this paper LDA was applied (k=6 for ads, k=10 for Wikipedia) and KDE/IDW used to make spatial surfaces; LDA produced geo-indicative topics but was outperformed by DCS on geolocation error.",
            "empirical_challenges": "Topic surfaces via IDW were too point-respecting/rough; LDA lacks direct geometric interpretability of conceptual spaces and in this task produced higher mean geolocation error than DCS (DCS improved accuracy by ~10–13%).",
            "applied_domains_or_tasks": "Topic modeling, semantic signature mapping, spatial semantic inference, document clustering and summarization.",
            "comparison_to_other_models": "LDA (probabilistic latent) vs conceptual spaces (geometric): LDA is statistical and less geometrically interpretable; conceptual-spaces variants yielded better spatial inference and interpretability for this task.",
            "functional_mechanisms": "Estimate topic-word and document-topic distributions via Dirichlet priors; use document-topic probabilities to compute spatial probability surfaces (KDE/IDW) and compute entropy/Moran's I to assess geo-indicativeness.",
            "limitations_or_open_questions": "Choosing k (topics) affects interpretability; mapping probabilistic topics to concrete geographic regions can be noisy and sensitive to sampling density; smoothing/interpolation choices (IDW vs KDE) affect surfaces.",
            "uuid": "e5168.6"
        },
        {
            "name_short": "MDS (constructive method)",
            "name_full": "Multidimensional scaling (MDS)",
            "brief_description": "A dimensionality-reduction technique that embeds objects (here documents) into a low-dimensional metric space preserving pairwise dissimilarities and is used here to instantiate quality dimensions of a conceptual space from data.",
            "citation_title": "Multidimensional scaling",
            "mention_or_use": "use",
            "theory_or_model_name": "Multidimensional scaling (MDS) as a method for constructing conceptual spaces",
            "theory_or_model_description": "MDS takes pairwise similarity/dissimilarity measures and computes coordinates in a low-dimensional Euclidean space that preserve those relations as distances; in conceptual-spaces instantiation, MDS provides the interpretable geometric coordinates (quality-dimension axes) for documents and concepts.",
            "representation_format_type": "data-driven geometric embedding / dimension-learning method",
            "key_properties": "Preserves pairwise dis/similarities as metric distances; configurable embedding dimensionality; widely used in cognitive studies to elicit perceived similarity structures.",
            "empirical_support": "Authors use MDS on TF-IDF / PPMI-weighted document vectors to construct conceptual spaces for both RCS and DCS; higher embedding dimensionality (5,10,20 tested) improved SVM classification accuracy for DCS.",
            "empirical_challenges": "Choice of dimensionality affects downstream classifiers and interpretability; MDS outputs may be sensitive to noisy or sparse similarity estimates, requiring careful preprocessing.",
            "applied_domains_or_tasks": "Constructing low-dimensional similarity-preserving representations, cognitive similarity modeling, as preprocessing step for conceptual-space methods.",
            "comparison_to_other_models": "Compared implicitly to neural embedding methods (doc2vec) — authors prefer MDS for interpretability even if neural embeddings may offer performance gains.",
            "functional_mechanisms": "Compute dis/similarity matrix from weighted BOW vectors → solve for coordinates minimizing stress to place documents as points in Euclidean space → use coordinates for clustering, SVM direction-finding, and projection.",
            "limitations_or_open_questions": "How to link MDS dimensions to semantically meaningful quality dimensions; stability under corpus changes; scaling to very large corpora.",
            "uuid": "e5168.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Conceptual Spaces: The Geometry of Thought",
            "rating": 2,
            "sanitized_title": "conceptual_spaces_the_geometry_of_thought"
        },
        {
            "paper_title": "A metric conceptual space algebra",
            "rating": 2,
            "sanitized_title": "a_metric_conceptual_space_algebra"
        },
        {
            "paper_title": "Latent dirichlet allocation",
            "rating": 2,
            "sanitized_title": "latent_dirichlet_allocation"
        },
        {
            "paper_title": "A Place Recommendation Approach Using Word Embeddings in Conceptual Spaces",
            "rating": 2,
            "sanitized_title": "a_place_recommendation_approach_using_word_embeddings_in_conceptual_spaces"
        },
        {
            "paper_title": "Data-driven Conceptual Spaces: Creating Semantic Representations for Linguistic Descriptions Of Numerical Data",
            "rating": 1,
            "sanitized_title": "datadriven_conceptual_spaces_creating_semantic_representations_for_linguistic_descriptions_of_numerical_data"
        }
    ],
    "cost": 0.01812325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Get Spatial from Non-Spatial Information: Inferring Spatial Information from Textual Descriptions by Conceptual Spaces
11 December 2023</p>
<p>Jiancang Zhuang 
Gang Hu 
Lan Cheng 
Guanqiu Qi 
Omid Reza Abbasi oabbasi@mail.kntu.ac.ir 0000-0002-2274-6191
Department of Geospatial Information Systems
Toosi University of Technology
19697TehranK. NIran</p>
<p>Ali Asghar Alesheikh alesheikh@kntu.ac.ir 0000-0001-9537-9401
Department of Geospatial Information Systems
Toosi University of Technology
19697TehranK. NIran</p>
<p>Seyed Vahid Razavi-Termeh 0000-0001-5898-9892
Department of Computer Science &amp; Engineering and Convergence Engineering for Intelligent Drone
XR Research Center
Sejong University
05006SeoulRepublic of Korea</p>
<p>Get Spatial from Non-Spatial Information: Inferring Spatial Information from Textual Descriptions by Conceptual Spaces
11 December 2023A54A0B0D9ED16E2662A6F6B15D80DA1410.3390/math11244917Received: 4 October 2023 Revised: 1 December 2023 Accepted: 7 December 2023textual contentgeo-indicativenessconceptual spacestopic modeling MSC: 68T5068T30
With the rapid growth of social media, textual content is increasingly growing.Unstructured texts are a rich source of latent spatial information.Extracting such information is useful in query processing, geographical information retrieval (GIR), and recommender systems.In this paper, we propose a novel approach to infer spatial information from salient features of non-spatial nature in text corpora.We propose two methods, namely DCS and RCS, to represent place-based concepts.In addition, two measures, namely the Shannon entropy and the Moran's I, are proposed to calculate the degree of geo-indicativeness of terms in texts.The methodology is compared with a Latent Dirichlet Allocation (LDA) approach to estimate the accuracy improvement.We evaluated the methods on a dataset of rental property advertisements in Iran and a dataset of Persian Wikipedia articles.The results show that our proposed approach enhances the relative accuracy of predictions by about 10% in case of the renting advertisements and by 13% in case of the Wikipedia articles.The average distance error is about 13.3 km for the advertisements and 10.3 km for the Wikipedia articles, making the method suitable to infer the general region of the city in which a property is located.The proposed methodology is promising for inferring spatial knowledge from textual content that lacks spatial terms.</p>
<p>Introduction</p>
<p>Geospatial information services are typically based on collected quantitative data about objects and phenomena.They can provide answers, limited by the accuracy of the underlying data, to spatial queries such as 'where is . . .?' The answer is extracted from the stored data, which is basically coordinate-based.These services have been extremely helpful in a broad spectrum of disciplines ranging from agriculture [1], ecology [2,3], mining [4,5], and archeology [6] to urban planning [7,8], public health [9], and water resources [10].The role of the rapid growth of spatial data acquisition technologies in the success of GIS is undeniable.Mappers all over the world are using equipment such as surveying tools, GPS receivers, and drones to provide accurate data for organizations and corporations.Clearly, this is a costly and time-consuming procedure.In addition, it requires a great number of people to be involved in the procedure.</p>
<p>Due to the developments of the internet and the high penetration rate of social networks in people's daily lives, a vast resource of information has been enabled.Of this, a noticeable share is enriched with spatial information.This information is mainly latent in textual content in the form of online books, user reviews, and blog posts [11].As users utilize a natural language to communicate through the internet, this information appears in terms of qualitative descriptions of objects and events [12].Although these descriptions rarely contain the spatial location of objects (e.g., in terms of postal addresses [13]), they may contain place names, spatial relations, and directions.Research on the extraction of spatial relations from textual content and natural languages has been growing rapidly in recent years [14][15][16].Such studies aim at extracting spatial information in three forms that correspondingly answer three types of queries.Specifically, they intend to find topological states, proximity relations, and spatial directions.Topological relations describe the way objects are connected.It is very useful in geographic information retrieval (GIR) and qualitative reasoning (QR) to know systematically how places noted in a text are located in relation to each other [17,18].These relations often appear in natural language as prepositions [19].While they inform us about the way objects are contacting each other, they do not compare the situation where the objects are in one topological form.For instance, if two objects are topologically disjoint, the topological models (e.g., RCC-8 [20]) do not provide information about how much the two objects are disjoint.Proximity queries aim to find objects near a reference object represented by a place name in text.As the exact locations of objects in textual corpora are not known, the interpretation of how much of a distance is considered as near is rather subjective [21].Directions, whether cardinal or relative, and orientations are important spatial relations usually observable in textual content.Again, they may be subjectively interpreted and are rarely implemented in GIS [15].</p>
<p>While the focus of the existing literature on the subject has been on the extraction of the mentioned spatial information from spatial descriptors within texts, we argue that some spatial information can be inferred from non-spatial terms.As only a few sentences in textual resources contain spatial descriptors [22], the extraction of spatial information from non-spatial terms would be of high interest in applications that need to locate textual resources.Our approach is especially useful in applications where resources, rather than toponyms within resources, are to be located.To give an example, consider the description 'The house is furnished, has a roof garden, and a good view of the mountains is available' for a house in the city of Tehran that has been advertised on the web.In this description, there is no sign of explicit spatial descriptors.However, when seeing the description, a resident of the city may learn that the house is located in a northern neighborhood.The resident knows that some aspects of housing in northern Tehran are different from those in other neighborhoods.In addition, there may be some terms in the description that do describe the house but imply the region in which it is located.In the example above, a roof garden is a feature of houses mainly located in northern Tehran.Also, the houses located in northern Tehran often have a clear view of the mountains.</p>
<p>Stock, et al. [19] categorized expressions within texts into three classes: geospatial, other-spatial, and non-spatial terms.The geospatial class includes those expressions that contain a geographical reference and a spatial relation term, hence referring to an absolute geographic coordinate.The other-spatial category refers to expressions that contain spatial relations but lack a geographic reference, leading to relative coordinates.Non-spatial expressions are those terms that do not fall within the previous categories.In this paper, we further extend Stock's non-spatial class into two distinct classes.In fact, we recognize another class of expressions that, while lacking spatial relations, contain terms implying geospatial information.We borrow the term geo-indicative from Adams and Janowicz [23] to refer to this class.The difference between the two classes is neither trivial nor explicit, as they both do not contain geospatial or other-spatial terms.However, geo-indicative expressions contain words that describe some characteristics belonging to a certain geographical region.This is in contrast with the non-spatial expressions, where terms describe features that do not belong to a specific region in the study area.Therefore, some terms that are identified as geo-indicative in some study areas may be non-spatial in other regions.While spatial relations appear in texts often as prepositional phrases, geo-indicative terms are latent in texts as nouns and adjectives.Table 1 shows real-world examples for each class of expressions.</p>
<p>Class of Expression Definition Example 1</p>
<p>Geospatial expressions Contain direct references to geographical features, such as place names</p>
<p>The house is located on the south side of the Valiasr metro station.</p>
<p>Other-spatial expressions Contain terms that refer to the relative position of features, such as topological relations A supermarket is available across the street.</p>
<p>Geo-indicative expressions</p>
<p>Contain terms that do not directly refer to geographical features but describe the characteristics of a certain geographical region</p>
<p>The house is furnished, has a roof garden, and has a good view of the mountains.</p>
<p>Non-spatial expressions</p>
<p>Contain terms that neither refer to nor describe any specific geographical region</p>
<p>The house has two rooms.The building has been recently renovated, and the walls are wallpapered. 1Words that can be used to infer spatial information from are bold.</p>
<p>In this paper, we aim to extract spatial information from the non-spatial terms that imply a space (i.e., geo-indicative terms) in large corpora with the help of the theory of conceptual spaces.The proposed methods prove more versatile than other approaches reliant on geospatial terms in texts, considering that few texts contain such terms.The method finds broad applicability in cases where a system is to be designed to georeference texts without explicit geospatial terms and conduct spatial analysis or recommend textual items to users.For instance, in the Divar application, from which we extracted one of our datasets, rental property advertisers have the option to provide exact location information by placing a pin on a map.However, if a user chooses not to specify the exact location of the property, the neighborhood in which the property is situated must be manually determined via a search box.In this real-world scenario, our proposed method could offer two valuable services: Firstly, the platform can utilize our approach to automatically predict and suggest the neighborhood name for the advertiser in advance, streamlining the process.Secondly, the platform's recommendation engine presently suggests items in other areas based on the adjacency of the neighborhoods to the current neighborhood.With our proposed method, the recommendation system gains greater flexibility since our estimation approach does not rely on the physical topology of neighborhoods.As a result, the recommendations can be more relevant and encompass a wider range of options, enhancing user experience and satisfaction.The contribution of our study is three-fold:</p>
<p>1.</p>
<p>We propose two novel methods, specifically the regional conceptual space (RCS) and an adapted version of directional conceptual space (DCS) from [24].These methods are founded on the conceptual spaces theory and are designed to represent terms in textual content within a high-dimensional space.</p>
<p>2.</p>
<p>We suggest utilizing Moran's I as a measure for assessing the geo-indicativeness of terms in textual content.As far as we are aware, Moran's I has not been employed previously for the specific purpose of gauging the geo-indicativeness of terms in text.</p>
<p>3.</p>
<p>We contrast the methods we propose with a previously suggested approach relying on Latent Dirichlet Allocation (LDA).</p>
<p>In Section 2, we review some studies conducted on the topic of spatial information extraction from textual content.In Section 3, the theory of conceptual spaces is introduced, and our adaptation of the theory is explained.In Section 4, the methodology of the work is presented.Then, we provide the results and discuss them in Section 5. Finally, we conclude the paper and highlight some suggestions for future research.</p>
<p>Related Work</p>
<p>In this section, we provide a review of research conducted on extracting spatial information from textual content.In this area of research, most studies have focused on identifying spatial relations, including topological relations, in texts.In other words, a significant share of such studies have chosen geospatial and other-spatial expressions to extract spatial information from corpora.Since the focus of our study is on the extraction of spatial information from geo-indicative expressions, we limit our review to this topic.</p>
<p>Most studies in the area of the extraction of spatial information from text have utilized both geospatial and place names to estimate the location of resources.In almost all of these efforts, no distinction is made between place names and geo-indicative expressions, and both types are included in the computations.For instance, Han, et al. [25] focused on the feature selection procedure to enhance the accuracy of georeferencing.Their idea was that many words in texts are not useful (i.e., they are non-spatial).Hence, they proposed three feature selection methods to distinguish between terms that highly imply spatial information and non-spatial terms.They developed term frequency-inverse corpus frequency (TF-ICF), analogous to TF-IDF, along with information gain ratio (IGR) and maximum entropy (ME) as criteria to select features.The methods were applied on large geographical scales.A grid of 0.5 in 0.5 divided the Earth, and the tweets far from cities were eliminated.They reported a 10% improvement in the accuracy of georeferencing using only geospatial and geo-indicative words in comparison with using full features.They also noted that place names played a noticeable role in the improvement.</p>
<p>Place name recognition is vital to the georeferencing and enrichment of geospatial gazetteers.However, it is a time-consuming task and needs massive labeled datasets [26].In most cases, there is even no place name within the text.It is especially true for short texts such as messages on social media and advertisements across the web.A third group of researchers focused only on the geo-indicative terms.For instance, Hollenstein and Purves [27] explored the terms used by users to describe city centers.They used geotagged Flickr photos to examine how different city centers across the globe are described.Then, the terms were used to derive the boundaries of the city centers.Wang, et al. [28] employed content analysis to explore the perception of green spaces in Beijing, China.They developed a structured lexicon based on which landscape features were extracted and fed into machine learning techniques.Their approach relies on prior knowledge about the phenomenon under study.Chang, et al. [29] proposed a probabilistic framework to estimate the city-level location of Twitter users based on the content of their tweets.In their approach, a classification component is used to automatically identify words with a high probability of referring to a specific city (the word rockets probably refers to Houston).Their results showed that 51% of Twitter users can be placed within 100 miles of their actual location.McKenzie and Janowicz [30] explored the regional variation between Foursquare's POI types.They applied a statistical hypothesis test to the dataset and found that there is regional variation between POI types that is not the result of random fluctuations.Adams and Janowicz [23] chose georeferenced Wikipedia articles and travel blog posts to train a Latent Dirichlet Allocation (LDA) model.They estimated a probability surface over the Earth for each topic.A Kernel Density Estimation (KDE) analysis was used to represent the probability of correspondence of each topic to the regions.They applied the method to places in the United States and achieved an accuracy of about 500 km for 75% of the test Wikipedia articles.In [31], the same authors extended their work and viewed the subject from a semantic signature perspective.Analogous to spectral signatures, semantic signatures can be used to illustrate various thematic features of the space.They showed that their proposed approach automatically categorizes documents into place types in existing ontologies.The focus of the mentioned study has been on the enrichment of knowledge graphs and inferring the relationships between place types in ontologies.While the topic of their study is closely related to ours, we proposed the use of conceptual spaces, rather than LDA, to build a similarity space of unstructured text.Therefore, the proposed method is more suitable in applications such as recommender systems, where similarity is at the core of computations.</p>
<p>The aim of the latter studies is similar to ours.They use only geo-indicative terms to infer the location of textual resources.In this article, we seek to extract geo-indicative expressions.It should be noted that geo-indicative terms refer to a specific region of the study area.Hence, the spatial distribution of documents containing geo-indicative terms would be clustered.Therefore, geo-indicative terms inherently play the role of place names in text.Our study employs the theory of conceptual spaces, which, as a cognitive knowledge representation framework, is more suitable to the task of analyzing semantic information in textual corpora.In addition, as we construct a similarity space in which more semantically similar documents are placed closer together, our methodology becomes more interpretable and the properties of the space can be discussed.Another feature of our research is the size of the study area.In this study, we estimate the location of textual resources within the city of Tehran, the capital of Iran.According to the definition of geoindicativeness, the different parts of the underlying study area must have distinguishing properties so that the language can describe them and discriminate from other regions.The previous studies mainly applied their method to large-scale areas such as the world and countries.Obviously, different regions of a country have specific properties, such as the accent, culture, geography, and climate, among many others, and can be described by diverse and distinguishing terms.We applied our method to the level of a city, which makes the prediction task more challenging.Unlike some other studies which seek to find the relationships between the location of a text and the user, our proposed method aims to infer the location of general textual resources, irrespective of the location of text's creator.Our approach does not inherently violate user privacy.Of course, if the method is to be applied in scenarios where privacy concerns matter, the users' consent is required.In the forthcoming sections, we present and scientifically compare our results with those of the above studies.</p>
<p>Materials and Methods</p>
<p>Figure 1 demonstrates the workflow of our study.The details of the methods shown in the figure are described in this section.First, the LDA method, as the reference method with which we compare our results, is explained.In order to provide the readers an insight into how the words and topics are represented over the study area, Kernel Density Estimation (KDE) is introduced.Then, the theory of conceptual spaces, as proposed by Gärdenfors [32] and formalized by Adams and Raubal [33], is briefly highlighted.After introducing the original theory, the modified versions that we utilized in the study is introduced.We propose two data-driven adaptations of conceptual spaces.The results of both methods are evaluated and compared against the LDA and a random estimator.</p>
<p>Semantic Signatures</p>
<p>Adams and Janowicz [23] proposed semantic signatures to address the problem of discovering latent spatial information in natural language.The method is based on Latent Dirichlet Allocation (LDA), an unsupervised probabilistic method over discrete data such as text corpora [34,35].LDA is commonly used in the tasks of topic modeling [36] and recommendation [37].It assumes that documents are probability distributions of topics, and each topic is a probability distribution of words within documents.Formally, it is assumed that a text corpus is comprised of k topics z n and contains |V| words.For a document D containing N words, the probability of assigning each topic to document D is computed as [35]:
p(w|α, β) = θ p(θ|α) N ∏ n=1 k ∑ z n =1 p(w n |z n , β )p(z n |θ) dθ (1)
where w is the set of words in document D, and θ is a k-dimensional random variable sampled from a Dirichlet distribution, and α and β are the parameters of the model.In order to represent the probabilities provided by LDA as surfaces over the study area, Adams and Janowicz [23] suggested using Kernel Density Estimation (KDE).Suppose {X 1 , . . . ,X n } ∈ R d be an independent, identically distributed (iid) random sample from an unknown probability distribution P. KDE aims at estimating P by [38]:
p(x) = 1 nh d n ∑ i=1 K x − X i h (2)
where K is a kernel function and h ∈ R + is called bandwidth.KDE assigns a probability value to each sample.Then, the values are summed to yield a density estimator.The form of the distribution depends on the kernel function used in the equation.Two commonly employed kernels are Gaussian and spherical kernels.A Gaussian kernel is defined as [39]:
K(x) = exp − x 2 2 exp − x 2 2 dx(</p>
<p>Semantic Signatures</p>
<p>Adams and Janowicz [23] proposed semantic signatures to address the problem of discovering latent spatial information in natural language.The method is based on Latent Dirichlet Allocation (LDA), an unsupervised probabilistic method over discrete data such as text corpora [34,35].LDA is commonly used in the tasks of topic modeling [36] and recommendation [37].It assumes that documents are probability distributions of topics, and each topic is a probability distribution of words within documents.Formally, it is assumed that a text corpus is comprised of k topics   and contains || words.For a document  containing  words, the probability of assigning each topic to document D is</p>
<p>Conceptual Spaces</p>
<p>The theory of conceptual spaces [40] is a knowledge representation method [32] that uses geometrical structures to represent information, allowing for the definition of similarity relations between concepts [41].This approach is distinct from associationist and symbolic levels of knowledge representation.Associationist models focus on connections among the many parts of a system, while symbolic models manipulate symbols using rules written in First-Order Logic (FOL).Symbolic models typically process information sequentially, one symbol or rule at a time.This is due to the fact that symbolic representations are discrete and well-defined, and each symbol or rule can be processed independently of the others [42].In contrast, associationist models often process information in parallel, with multiple connections between concepts being activated simultaneously [43].This is due to the fact that associationist representations are distributed and overlapping, and the activation of one concept can lead to the activation of many others.</p>
<p>The theory of conceptual spaces tries to represent information based on geometrical structures.A significant advantage of this kind of representation is the ability to define similarity relations on information, and to relate the concepts topologically [44].Moreover, inductive reasoning can be studied more conveniently at the conceptual level [45].The geometrical forms of concepts can be used in both the explanatory tasks (for example, see Poth [46]), in which a cognitive theory is formulated and verified by means of experiments and observations, and the constructive tasks (for example, see Pol, et al. [47]), in which an artificial agent or system is designed to solve a specific problem.In this article, the focus is on the latter, as we aim to utilize the conceptual spaces to solve the task of inferring spatial information rather than to fathom the underlying complexities of human thinking regarding places.</p>
<p>A conceptual space is a similarity space spanned by a set D of quality dimensions.Each dimension d ∈ D is an aspect by which an object can be identified.Some quality dimensions that are related to each other are integrated into domains.While the initial works on Gärdenfors' conceptual spaces mainly included explicitly defined quality dimensions, there has been some recent effort to infer the conceptual space in a data-driven approach [48].In this paper, we construct the conceptual space using a data-driven manner.In what follows, we introduce the elements used in the theory of conceptual spaces.While in the literature there are other elements such as fuzzy concepts [49] or hierarchical conceptual spaces [50], we define only the core elements needed by a conceptual space for the sake of simplicity.Definition 1.A concept is defined as C = R,</p>
<p>Conceptual Spaces</p>
<p>The theory of conceptual spaces [40] is a knowledge representation method [32] that uses geometrical structures to represent information, allowing for the definition of similarity relations between concepts [41].This approach is distinct from associationist and symbolic levels of knowledge representation.Associationist models focus on connections among the many parts of a system, while symbolic models manipulate symbols using rules written in First-Order Logic (FOL).Symbolic models typically process information sequentially, one symbol or rule at a time.This is due to the fact that symbolic representations are discrete and well-defined, and each symbol or rule can be processed independently of the others [42].In contrast, associationist models often process information in parallel, with multiple connections between concepts being activated simultaneously [43].This is due to the fact that associationist representations are distributed and overlapping, and the activation of one concept can lead to the activation of many others.</p>
<p>The theory of conceptual spaces tries to represent information based on geometrical structures.A significant advantage of this kind of representation is the ability to define similarity relations on information, and to relate the concepts topologically [44].Moreover, inductive reasoning can be studied more conveniently at the conceptual level [45].The geometrical forms of concepts can be used in both the explanatory tasks (for example, see Poth [46]), in which a cognitive theory is formulated and verified by means of experiments and observations, and the constructive tasks (for example, see Pol, et al. [47]), in which an artificial agent or system is designed to solve a specific problem.In this article, the focus is on the latter, as we aim to utilize the conceptual spaces to solve the task of inferring spatial information rather than to fathom the underlying complexities of human thinking regarding places.</p>
<p>A conceptual space is a similarity space spanned by a set  of quality dimensions.Each dimension  ∈  is an aspect by which an object can be identified.Some quality dimensions that are related to each other are integrated into domains.While the initial works on Gärdenfors' conceptual spaces mainly included explicitly defined quality dimensions, there has been some recent effort to infer the conceptual space in a data-driven approach [48].In this paper, we construct the conceptual space using a data-driven manner.In what follows, we introduce the elements used in the theory of conceptual spaces.While in the literature there are other elements such as fuzzy concepts [49] or hierarchical conceptual spaces [50], we define only the core elements needed by a conceptual space for the sake of simplicity.Definition 1.A concept is defined as  = 〈ℛ, 〉, where ℛ denotes a set of convex regions in the space. represents a prototype of the concept in the space.The prototype instance of the concept is assumed to be a point in the space, theoretically located in the center of the convex region representing the concept.</p>
<p>, where R denotes a set of convex regions in the space.</p>
<p>Conceptual Spaces</p>
<p>The theory of conceptual spaces [40] is a knowledge representation method [32] that uses geometrical structures to represent information, allowing for the definition of similarity relations between concepts [41].This approach is distinct from associationist and symbolic levels of knowledge representation.Associationist models focus on connections among the many parts of a system, while symbolic models manipulate symbols using rules written in First-Order Logic (FOL).Symbolic models typically process information sequentially, one symbol or rule at a time.This is due to the fact that symbolic representations are discrete and well-defined, and each symbol or rule can be processed independently of the others [42].In contrast, associationist models often process information in parallel, with multiple connections between concepts being activated simultaneously [43].This is due to the fact that associationist representations are distributed and overlapping, and the activation of one concept can lead to the activation of many others.</p>
<p>The theory of conceptual spaces tries to represent information based on geometrical structures.A significant advantage of this kind of representation is the ability to define similarity relations on information, and to relate the concepts topologically [44].Moreover, inductive reasoning can be studied more conveniently at the conceptual level [45].The geometrical forms of concepts can be used in both the explanatory tasks (for example, see Poth [46]), in which a cognitive theory is formulated and verified by means of experiments and observations, and the constructive tasks (for example, see Pol, et al. [47]), in which an artificial agent or system is designed to solve a specific problem.In this article, the focus is on the latter, as we aim to utilize the conceptual spaces to solve the task of inferring spatial information rather than to fathom the underlying complexities of human thinking regarding places.</p>
<p>A conceptual space is a similarity space spanned by a set  of quality dimensions.Each dimension  ∈  is an aspect by which an object can be identified.Some quality dimensions that are related to each other are integrated into domains.While the initial works on Gärdenfors' conceptual spaces mainly included explicitly defined quality dimensions, there has been some recent effort to infer the conceptual space in a data-driven approach [48].In this paper, we construct the conceptual space using a data-driven manner.In what follows, we introduce the elements used in the theory of conceptual spaces.While in the literature there are other elements such as fuzzy concepts [49] or hierarchical conceptual spaces [50], we define only the core elements needed by a conceptual space for the sake of simplicity.Definition 1.A concept is defined as  = 〈ℛ, 〉, where ℛ denotes a set of convex regions in the space. represents a prototype of the concept in the space.The prototype instance of the concept is assumed to be a point in the space, theoretically located in the center of the convex region representing the concept.represents a prototype of the concept in the space.The prototype instance of the concept is assumed to be a point in the space, theoretically located in the center of the convex region representing the concept.Definition 2. A property is defined as the value of a concept in only one quality dimension.Properties can be understood as seeing a concept from only one aspect.Definition 3.An instance  is defined as a point within a conceptual space that represents an object in the real world.An instance is produced when a value is determined for all quality dimensions in the space.</p>
<p>Definition 3. An instance</p>
<p>In this article, we construct the conceptual space in a data-driven manner and propose a modification in the representation of concepts.</p>
<p>Data-Driven Conceptual Spaces</p>
<p>As stated in the previous section, the theory of conceptual spaces represents concepts as convex regions in a metric space.In most cases, since there is no prior knowledge about the underlying phenomenon, the quality dimensions cannot be identified explicitly.Therefore, a data-driven approach is essential to building the conceptual space from the bottom up.In this article, we learn the conceptual space of our corpus using some techniques used in machine learning and Natural Language Processing (NLP).</p>
<p>Suppose we are going to represent a corpus  containing documents  , … ,  .Each document is first tokenized into a Bag of Words (BOW) containing all of the words in the documents.Similar to any NLP task, and as some tasks need only specific parts of speech (POS), the BOWs are preprocessed, and the tokens that are not useful for the specific task are removed.Then, each token in each BOW is assigned an index to provide the salience of the words in the documents.TF-IDF is a common index to this aim.Given a set  of documents, TF-IDF scores the word  in the document  as [51]:</p>
<p>is defined as a point within a conceptual space that represents an object in the real world.An instance is produced when a value is determined for all quality dimensions in the space.</p>
<p>In this article, we construct the conceptual space in a data-driven manner and propose a modification in the representation of concepts.</p>
<p>Data-Driven Conceptual Spaces</p>
<p>As stated in the previous section, the theory of conceptual spaces represents concepts as convex regions in a metric space.In most cases, since there is no prior knowledge about the underlying phenomenon, the quality dimensions cannot be identified explicitly.Therefore, a data-driven approach is essential to building the conceptual space from the bottom up.In this article, we learn the conceptual space of our corpus using some techniques used in machine learning and Natural Language Processing (NLP).</p>
<p>Suppose we are going to represent a corpus C containing documents {D 1 , . . . ,D N }.Each document is first tokenized into a Bag of Words (BOW) containing all of the words in the documents.Similar to any NLP task, and as some tasks need only specific parts of speech (POS), the BOWs are preprocessed, and the tokens that are not useful for the specific task are removed.Then, each token in each BOW is assigned an index to provide the salience of the words in the documents.TF-IDF is a common index to this aim.Given a set D of documents, TF-IDF scores the word w in the document d as [51]:
TF_IDF(w, D, C) = TF(w, D) × IDF(w, C)(4)
where
TF(w, D) = c(w, D) |D|(5)
and
IDF(w, C) = log |C| |{D ∈ C|w ∈ D }|(6)
where c(w, D) is the number of times a term w occurs in the document D. Also, |D| and |C| denote the number of words in the document D and the number of documents in the corpus C, respectively.</p>
<p>Positive point-wise mutual information (PPMI) is similar to TF-IDF in that it also scores the words in a document.Some researchers have shown that it works better than other scoring methods for the purpose of semantic similarity [46].The index is calculated as:
PPMI(w, D) = max 0, log p w,D p w, * × p * ,D(7)
where
p w,D = c(w, D) ∑ w ∑ D c(w , D )(8)
and
p w, * = ∑ D p w,D(9)
and
p * ,D = ∑ w p w ,D(10)
By applying the scoring index to each document D, a vector v D is formed.This vector contains the index values of all words in the corpus.Since the index vectors will be very sparse, they are not suitable to be directly used in constructing the conceptual space.Hence, a Multi-Dimensional Scaling (MDS) technique [52] is applied to the vectors to transform them into a similarity space of documents.While MDS itself is not specific to conceptual spaces theory, it has been denoted by Gärdenfors [40] and others [53] as a method of constructing the conceptual spaces and identifying the quality dimensions.Given the pairwise similarities of a set of objects, MDS estimates their coordinates as points in a similarity space [52].It has been widely used in cognitive sciences [53] and also in the theory of conceptual spaces [54].</p>
<p>TF-IDF and PPMI offer a clear and easily understandable representation of words based on their significance within a specific document.The assigned weights by TF-IDF and PPMI indicate how relevant a word is to a document relative to a larger corpus, which is particularly useful when analyzing the geo-indicativeness of words in our spatial inference approach.In contrast, more recent methods such as doc2vec employ neural networks for model training, which reduces their interpretability.It is essential to note that one advantage of conceptual spaces over connectionist methods such as neural networks is their interpretability.In addition, TF-IDF and PPMI are relatively simple and computationally efficient methods for representing textual data.In contrast, word embedding methods often involve more complex models and require extensive training on large corpora.</p>
<p>In order to represent concepts as regions in the resulting space, spatial clustering would classify similar documents, represented by points, into single classes.Finally, the convex hull of the points yields the desired regions.In this paper, the conceptual spaces provided by the above procedure are called regional conceptual spaces (RCS).</p>
<p>Although the original theory of conceptual space revolves around the prototype theory [41], in the case of inferring the conceptual space from data, it might be rare to have a prototypical instance that represents the ideal case of the category in the middle of each class.Therefore, we propose a modification to the way classes are represented.To this end, we partition the resulting similarity space for each word using a Support Vector Machine (SVM) classifier.This leads to two classes of points: a class representing documents that do not contain a given word (negative class), and another class representing documents that contain that word (positive class).Then, we calculate the equation of the hyper-plane classifier.The direction perpendicular to the hyper-plane demonstrates the direction in which the word plays a prominent role.The farther a specific document is placed from the hyper-plane in the direction of positive documents, the more salience the word has in the document.As this classification does not potentially yield perfect accuracy, we consider only those words for which the classification has promising accuracy.This is to ascertain that the directions found have strong meanings.However, even after constructing the hyper-plane only for those words with high accuracy of classification, there may remain a high number of directions depending on the number of words considered.In addition, some vectors may have similar meanings.As we are handling a similarity space, the vectors with similar meanings are pointing towards a close direction.The directions can be clustered to achieve more robust and meaningful directions within the constructed space.We call the result of this procedure directional conceptual spaces (DCS).Figure 2 compares the two approaches to constructing the data-driven conceptual space mentioned above.Abbasi and Alesheikh [24] presented a method similar to DCS with a focus on utilizing it in recommendation systems.They measure the similarity of items of a recommender system by calculating the difference between the lengths of vectors projected on meaningful directions.The DCS method used in the present study uses the angular differences between the vectors to calculate their closeness.They evaluated the results by real users of a recommender system.The main difference between the present study and [24] is that we propose both DCS and RCS and compare their performance against LDA a random estimator, without focusing on recommendation algorithms.In addition, we partitioned the study area into equally-sized cells to compensate for the problem of the density of points in KDE, which is discussed in the 'Results' section.Furthermore, we suggest utilizing Moran's I, along with the Shannon entropy, as a measure for assessing the geo-indicativeness of terms in textual content. is that we propose both DCS and RCS and compare their performance against LDA a random estimator, without focusing on recommendation algorithms.In addition, we partitioned the study area into equally-sized cells to compensate for the problem of the density of points in KDE, which is discussed in the 'Results' section.Furthermore, we suggest utilizing Moran's I, along with the Shannon entropy, as a measure for assessing the geoindicativeness of terms in textual content.After constructing the conceptual space, the documents need to be located in the geographical space.To this aim, the study area is partitioned into different divisions.Then, for a new document, the most appropriate division is determined based on the similarity of their associated concepts or their closeness to meaningful directions.That is, in the case of RCS, the division with which the document shares the most common regions in the conceptual space is selected as the estimated division, and its geographical centre is assigned to the document.In the case of DCS, the division with which the document has the After constructing the conceptual space, the documents need to be located in the geographical space.To this aim, the study area is partitioned into different divisions.Then, for a new document, the most appropriate division is determined based on the similarity of their associated concepts or their closeness to meaningful directions.That is, in the case of RCS, the division with which the document shares the most common regions in the conceptual space is selected as the estimated division, and its geographical centre is assigned to the document.In the case of DCS, the division with which the document has the least angular difference is selected as the estimated division.</p>
<p>Datasets</p>
<p>In this paper, we used a dataset of advertisements for rental properties and a dataset of Persian Wikipedia articles, both related to the city of Tehran, Iran.To collect the first dataset, we crawled the Divar website (https://divar.ir,accessed on 21 March 2023), a popular platform used in Iran to advertise properties for sale or rent.The dataset contains about 200,000 records.After removing records that do not contain the coordinates of the property or lack a textual description, 11,393 records of advertisements remained.Each record includes a title and a description of the property in Persian, pricing information, the construction year, the area of the property, the name of the neighborhood where it is located, and locational data including latitude and longitude.In our study, we only processed the descriptions related to each advertisement.In addition, all place names were removed from the descriptions.In order to process the descriptions, we utilized the Stanza library [55], a Python natural language processing package that is equipped with pre-trained models for Persian language.The second dataset was extracted from the Wikimedia Query Service by writing queries in SPARQL.It contains 704 Persian Wikipedia articles that are geolocated in Tehran, Iran.The articles are full-length and cover various topics such as cultural sites, monuments, parks and gardens, and events.In our study, we use 80% of the records for the training purpose, and employ the remaining records for testing.In Table 2, the datasets used in the study are listed, and an example of each dataset is provided.</p>
<p>Results and Discussion</p>
<p>In this section, we present the results of applying the methods described in Section 3. Specifically, we apply our proposed approaches using conceptual spaces and compare the results with those of the LDA method.</p>
<p>In order to preprocess the textual descriptions, all of those properties for which the textual description is less than 10 words were removed.Considering that the length of feature vectors is equal to the number of all words in the corpus, this would help in compensating for the issue of feature vectors' sparsity.First, the descriptions were segmented into sentences, and then sentences were tokenized into words.To reduce the volume of data in processing, in this paper we only considered nouns and adjectives in sentences.On the other hand, by emphasizing nouns and adjectives, we aim to prioritize words that provide more explicit geo-indicative and descriptive context.We emphasize that the choice of parts of speech is dependent on the use case.In our study, verbs do not seem to be geo-indicative for rental property advertisements and Wikipedia articles about Tehran.However, the use of verbs might be better suited in scenarios where identifying functional regions is important.In addition, we considered only those words that were pointed out in the whole corpus at least 50 times.This choice is based on a heuristic threshold to filter out less frequent and potentially noise-inducing words and to focus on the most informative and discriminative words, capturing the key features and characteristics of the documents.Since data-driven conceptual spaces are very prone to the quality of the data, if the data is noisy or incomplete, then the conceptual space may be inaccurate or misleading.This ensures the strength of the meaning found in future steps and enhances the accuracy of classification.The same pre-processing procedure was used for both LDA and the proposed methods to ensure a fair comparison between them.</p>
<p>We selected the number of LDA topics k through qualitative evaluation of topic interpretability on each dataset.The appropriate k value depends on the semantic domain of the corpus.Since the rental advertisements cover a narrower domain than Wikipedia, their content reflects fewer meaningful topics.After testing different k values, we chose k = 6 for the rental advertisements and k = 10 for the Wikipedia articles by judging which values produced the most coherent, interpretable topics.In Table 3, example topics from each dataset, both geo-indicative and non-geoindicative, are shown.After training the LDA model, we construct a probability surface using IDW to test whether a routine interpolation technique can suitably represent the probabilities.Figure 3 illustrates the geographical distribution of advertisements belonging to Topic #D1 in Table 3, which contains the words pool, sauna, jacuzzi, conference_hall, and luxurious.The surfaces are drawn based on the probabilities yielded by the LDA topic modeling.which values produced the most coherent, interpretable topics.In Table 3, example topics from each dataset, both geo-indicative and non-geoindicative, are shown.After training the LDA model, we construct a probability surface using IDW to test whether a routine interpolation technique can suitably represent the probabilities.Figure 3 illustrates the geographical distribution of advertisements belonging to Topic #D1 in Table 3, which contains the words pool, sauna, jacuzzi, conference_hall, and luxurious.The surfaces are drawn based on the probabilities yielded by the LDA topic modeling.3) found in the corpus.</p>
<p>Table 3.Some topics identified in each dataset along with their entropy values.As seen in the figure, the surface shows a probability distribution for this topic, which is strongly correlated with the northern areas of the city.However, the surface highly respects single points, and the map is too rough.We calculated KDE as another approach to representing the probability surface.As discussed in Section 3.1, the location of documents is involved in the calculations of KDE.Therefore, those areas with more advertisements would have higher probabilities.In order to compensate for this issue, following Adams and Janowicz [23], we partitioned the study area into different divisions.A grid comprising cells of size 1 km in 1 km each divided the city.Then, the averages of probabilities for each topic were calculated and assigned to the centroid of the cells.Figure 4 manifests the distribution of the same topic (Topic #D1) represented by KDE.  3) found in the corpus.</p>
<p>Dataset</p>
<p>As seen in the figure, the surface shows a probability distribution for this topic, which is strongly correlated with the northern areas of the city.However, the surface highly respects single points, and the map is too rough.We calculated KDE as another approach to representing the probability surface.As discussed in Section 3.1, the location of documents is involved in the calculations of KDE.Therefore, those areas with more advertisements would have higher probabilities.In order to compensate for this issue, following Adams and Janowicz [23], we partitioned the study area into different divisions.A grid comprising cells of size 1 km in 1 km each divided the city.Then, the averages of probabilities for each topic were calculated and assigned to the centroid of the cells.Figure 4   The resulting surface provided by KDE is much smoother than that provided by IDW.This is due to the fact that, according to Equation (2), KDE averages the probability distributions over sample points, compensating the role of single points with high values.However, in IDW, single points can have significant effect on the resulting surface.Following Abbasi and Alesheikh [24], we further analyzed the topics found to measure the degree of geo-indicativeness of each topic.From one point of view, a topic is geo-indicative if it is associated with some neighborhoods while having low association probabilities with other neighborhoods.As a result, a geo-indicative topic would not be uniformly dispersed through the study area.To measure the dispersion, we compute the Shannon entropy of the probabilities by [56]: The resulting surface provided by KDE is much smoother than that provided by IDW.This is due to the fact that, according to Equation (2), KDE averages the probability distributions over sample points, compensating the role of single points with high values.However, in IDW, single points can have significant effect on the resulting surface.Following Abbasi and Alesheikh [24], we further analyzed the topics found to measure the degree of geo-indicativeness of each topic.From one point of view, a topic is geo-indicative if it is associated with some neighborhoods while having low association probabilities with other neighborhoods.As a result, a geo-indicative topic would not be uniformly dispersed through the study area.To measure the dispersion, we compute the Shannon entropy of the probabilities by [56]:
𝐻(𝑝) = − ∑ 𝑝 𝑖 𝑙𝑜𝑔(𝑝 𝑖 ) 𝑁 1 (11)H(p) = − N ∑ 1 p i log(p i )(11)
Shannon entropy calculates how much the outcome of a probability distribution would be surprising [57].The lower the entropy, the lower the amount of surprise.This is interestingly in accordance with the concept of geo-indicativeness.The terms of a geoindicative topic have to, unsurprisingly, describe the region to which they belong.Table 3 lists some topics found and their associated entropies.</p>
<p>In addition to Shannon entropy, we propose the Moran's I index to measure the geoindicativeness of topics.The Moran's I is a measure of spatial autocorrelation of data.It works based on both feature location and attributes.That is, it measures how much near features have similar attributes [58].It is calculated as:
I = n S 0 ∑ n i=1 ∑ n j=1 w i,j z i z j ∑ n i=1 z 2 i (12)
where z i is the deviation of an attribute for feature i from its mean, and w i,j is the spatial weight between feature i and j. S 0 is the aggregate of all of the spatial weights, and is calculated as:
S 0 = n ∑ i=1 n ∑ j=1 w i,j(13)
For a detailed explanation of computing Moran's I, the reader should refer to [59].The value of Moran's I fluctuates between −1 and 1, where a value of −1 indicates a strong dispersion of similar attributes, a value of 0 indicates a random spatial distribution of similar attributes, and a value of 1 indicates a strong clustering of similar attributes.We expect that geo-indicative topics should have a positive Moran's I.If the value of Moran's I is close to one, it is highly probable that the topic is geo-indicative, as it means the topic is spatially clustered.Conversely, if it is negative, the topic is not geo-indicative.If the Moran's I is close to zero, then the topic is probably not geo-indicative.We suggest that the two criteria, i.e., the Shannon entropy and the Moran's I, be considered simultaneously.This is especially important in the case that the Moran's I shows a borderline value for a topic.</p>
<p>The first three topics in the rental property advertisements listed in Table 3 show high degrees of geo-indicativeness (low entropy).Topic #D1 is the one illustrated in Figures 3  and 4, which refers to properties mostly located in northern neighborhoods of the city.Topic #D2 is vividly related to the central neighborhoods that are the commercial heart of the city and are known for their ease of accessibility.Topic #D3 refers to those properties that are under construction and are mostly located in the vicinity of the city.While the above topics clearly represent specific regions of the city, we could not interpret the rest of the topics as geo-indicative.For instance, Topic #D4 contains general features that can be seen in any neighborhood in the study area.This is in contrast with Topics #D1 and #D2 where their related words are associated with northern and central neighborhoods, respectively.In the case of the Wikipedia articles, the entropy values of topics are generally higher than those of the advertisements dataset.This is due to the fact that the themes of the Wikipedia articles are diverse, and many of them describe features that are not associated with their underlying space.For example, Topic #W2 refers to the articles about universities and research institutes.The universities and research institutions in Tehran are not located in a specific region.The entropy of Topic #W4 is more interesting.The topic contains articles that introduce neighborhoods of the city.Although the documents in this topic inherently describe their underlying space, they are, expectedly, distributed over the study area, leading to high entropy and negative Moran's I. Topic #W1 is related to museums and historical sites, such as palaces, which in turn relate mostly to central neighborhoods and some of the northern neighborhoods.Figure 5  of the Wikipedia articles are diverse, and many of them describe features that are not associated with their underlying space.For example, Topic #W2 refers to the articles about universities and research institutes.The universities and research institutions in Tehran are not located in a specific region.The entropy of Topic #W4 is more interesting.The topic contains articles that introduce neighborhoods of the city.Although the documents in this topic inherently describe their underlying space, they are, expectedly, distributed over the study area, leading to high entropy and negative Moran's I. Topic #W1 is related to museums and historical sites, such as palaces, which in turn relate mostly to central neighborhoods and some of the northern neighborhoods.Figure 5   The right panel of Figure 5 verifies the centrality, explained above, associated with Topic #D2.However, for the left panel, the hotspots are almost evenly distributed over the study area, and a specific region cannot be easily adopted for the topic.</p>
<p>We also applied the proposed data-driven conceptual spaces methods (Sections 3.3) to the same datasets.For this purpose, we considered only nouns and adjectives to reduce the volume of BOWs and enhance the speed of computations.In addition, all unnecessary tokens, such as pronouns and stop words, were removed from the BOWs.Both the TF- The right panel of Figure 5 verifies the centrality, explained above, associated with Topic #D2.However, for the left panel, the hotspots are almost evenly distributed over the study area, and a specific region cannot be easily adopted for the topic.</p>
<p>We also applied the proposed data-driven conceptual spaces methods (Section 3.3) to the same datasets.For this purpose, we considered only nouns and adjectives to reduce the volume of BOWs and enhance the speed of computations.In addition, all unnecessary tokens, such as pronouns and stop words, were removed from the BOWs.Both the TF-IDF and PPMI indices were calculated to score the salience of each word in the documents.For the purpose of implementing MDS, clustering the points in RCS, and applying SVM to partition the space in DCS, the scikit-learn package was employed.An issue with the DCS approach is that considering all words in the corpus and applying SVM to each is very time-consuming and computationally intensive.In addition, many words do not appear to have signs of being geo-indicative.Therefore, after computing TF-IDF and PPMI, we proceeded with only the top 10 percent of words in terms of these indices.Then, a dissimilarity matrix, as the input for MDS, was computed for the documents.Since we have no information about the dimensions of the resulting conceptual space, we computed spaces with 5, 10, and 20 dimensions.An SVM classifier with a linear kernel was trained to construct the DCS.The accuracy score of the classification was estimated, and those higher than 80 percent were considered as strong directions in the corpus.Table 4 shows the accuracy score of classification for 10 words also found by LDA and listed in Table 3 as Topic #D1 and Topic #W1.</p>
<p>In most cases, the PPMI score outperforms the TF-IDF, which suggests that it is a better scoring scheme for constructing directional conceptual spaces.Nevertheless, the classification for broad terms such as luxurious (Persian:</p>
<p>) yields the worst results, regardless of the scoring index.The importance of the number of dimensions should not be overlooked.The pattern of convergence can be seen in the accuracies in most cases, and higher dimensions generally provide better classification accuracy.We applied a vector clustering algorithm to group the five directions of the words.Analogous to the prototypes in RCS, the farthest document from the classifier hyper-plane is identified as the most prominent feature for the word under consideration.In order to find the similarities among documents, we first projected the vectors onto the found direction and calculated the distance between those documents and the others in the found direction.Then, we sorted out the documents based on their normalized similarities.Figure 6 illustrates the location of In the case of RCS, documents with similar words were grouped into one cluster.To group similar documents, we utilized k-means clustering.We tested k in the range of 1-30 clusters, running k-means with each value five times to account for variability.To select the best k, we evaluated each clustering using the Silhouette Coefficient metric, which measures how tightly grouped the data is within each cluster.The Silhouette Coefficient was highest with  = 20 clusters.In the case of rental property advertisements, out of 20 clusters, only two seemed geo-indicative.The results of RCS are shown in Figure 7, where the top panels illustrate the two geo-indicative clusters and the bottom panel shows a nongeo-indicative group.In the case of RCS, documents with similar words were grouped into one cluster.To group similar documents, we utilized k-means clustering.We tested k in the range of 1-30 clusters, running k-means with each value five times to account for variability.To select the best k, we evaluated each clustering using the Silhouette Coefficient metric, which measures how tightly grouped the data is within each cluster.The Silhouette Coefficient was highest with k = 20 clusters.In the case of rental property advertisements, out of 20 clusters, only two seemed geo-indicative.The results of RCS are shown in Figure 7, where the top panels illustrate the two geo-indicative clusters and the bottom panel shows a non-geo-indicative group.</p>
<p>clusters, running k-means with each value five times to account for variability.To select the best k, we evaluated each clustering using the Silhouette Coefficient metric, which measures how tightly grouped the data is within each cluster.The Silhouette Coefficient was highest with  = 20 clusters.In the case of rental property advertisements, out of 20 clusters, only two seemed geo-indicative.The results of RCS are shown in Figure 7, where the top panels illustrate the two geo-indicative clusters and the bottom panel shows a nongeo-indicative group.A disadvantage of RCS is that the meaning of clusters is not intuitive, as we have no interpretation of the dimensions or the clusters.Since the classification is done manually in DCS, the issue is more or less compensated in DCS.A suitable measure of the geoindicativeness of the clusters in RCS is the distance between the prototype and the cluster mapped on the geographical space.As seen in Figure 7, although for the two geo-indicative clusters the prototype is located almost in the middle of the hot spots, the location of the prototype for the non-geo-indicative cluster is far from the hot spots.This means that while the documents in the semantic space have been close to each other, they are dispersed in the geographical space, pertaining to the fact that the semantics for this cluster do not imply a specific region in the geographical space.In order to compare the results of the three approaches, we computed the accuracy of the location prediction in terms of the distance between the predicted locations and the true locations for the test dataset (20 percent).Figure 8 plots the cumulative distance error for all methods and datasets.</p>
<p>The plot also shows the results of a random estimator as a benchmark for the prediction.All of the methods have better accuracies than the random estimator.However, the accuracy of RCS is worse than the other two in both datasets and, in some places, is even equal to the random prediction.DCS is the most accurate approach with an average distance error of 13.3 km for the Divar dataset and 10.3 km for the Wikipedia dataset.Since the size of the city is about 40 km by 20 km, this accuracy is suitable to predict the general region in which a property is located.That is, it can be inferred that a given document is located in the northern parts of Tehran.The worst prediction of DCS has been about 22.5 km, which is approximately equal to half of the length of the city.This value for RCS and LDA is about 31 km and 28 km, which shows that they completely fail in predicting some documents.DCS can promisingly predict the cardinal direction in which the property is located.It is predicting the actual locations better than LDA by 10% and 13% on average, respectively, for the Divar dataset and the Wikipedia articles.</p>
<p>indicativeness of the clusters in RCS is the distance between the prototype and the cluster mapped on the geographical space.As seen in Figure 7, although for the two geo-indicative clusters the prototype is located almost in the middle of the hot spots, the location of the prototype for the non-geo-indicative cluster is far from the hot spots.This means that while the documents in the semantic space have been close to each other, they are dispersed in the geographical space, pertaining to the fact that the semantics for this cluster do not imply a specific region in the geographical space.In order to compare the results of the three approaches, we computed the accuracy of the location prediction in terms of the distance between the predicted locations and the true locations for the test dataset (20 percent).Figure 8 plots the cumulative distance error for all methods and datasets.The plot also shows the results of a random estimator as a benchmark for the prediction.All of the methods have better accuracies than the random estimator.However, the accuracy of RCS is worse than the other two in both datasets and, in some places, is even equal to the random prediction.DCS is the most accurate approach with an average distance error of 13.3 km for the Divar dataset and 10.3 km for the Wikipedia dataset.Since the size of the city is about 40 km by 20 km, this accuracy is suitable to predict the general region in which a property is located.That is, it can be inferred that a given document is located in the northern parts of Tehran.The worst prediction of DCS has been about 22.5 km, which is approximately equal to half of the length of the city.This value for RCS and LDA is about 31 km and 28 km, which shows that they completely fail in predicting some documents.DCS can promisingly predict the cardinal direction in which the property is located.It is predicting the actual locations better than LDA by 10% and 13% on average, respectively, for the Divar dataset and the Wikipedia articles.</p>
<p>Conclusions</p>
<p>In this paper, we propose a method to extract spatial information and estimate the location of textual content lacking place names or spatial relations.We argued that some terms within texts do not directly refer to places in geographical space but do imply spatial information.The focus of past studies on inferring spatial information from text has been</p>
<p>Conclusions</p>
<p>In this paper, we propose a method to extract spatial information and estimate the location of textual content lacking place names or spatial relations.We argued that some terms within texts do not directly refer to places in geographical space but do imply spatial information.The focus of past studies on inferring spatial information from text has been on extracting spatial relations and place name identification and disambiguation.We proposed two different approaches to infer data-driven conceptual spaces (DCS and RCS) in an unsupervised manner.The proposed method was applied to a dataset of rental properties and a dataset of Persian Wikipedia articles, both for Tehran, Iran.We calculated the degree of geo-indicativeness of terms by their entropy.Then, we compared the results with those of LDA.The results showed that DCS outperformed LDA and RCS in terms of accuracy of prediction.By utilizing DCS, the location of properties can be predicted at 9.6 km, which is suitable for inferring the general region in which a property is located.Considering the volume of textual content lacking spatial information, this method can be a promising approach to inferring spatial information from texts.</p>
<p>The main advantage of our proposed method is that it is grounded in the theory of conceptual spaces, which provides a natural way to represent the meaning of words and their relationships in a geometric space.This means that the properties of the space can be discussed and interpreted in terms of the concepts that they represent.For example, we can visualize the relationships between different concepts in the space and see how they relate to each other.In contrast, LDA relies on statistical models that are not easily interpretable.While it can provide useful insights into the topics and themes that are present in a corpus, it can be challenging to understand how these topics relate to each other and to the underlying concepts that they represent.On the other hand, the proposed method offers a more flexible knowledge representation approach.Since documents are represented by points in conceptual spaces, they can easily be grouped into clusters and form concepts, which provides a multitude of analytical capabilities, such as the topological relationships between different concepts.Furthermore, since similarity computations are more intuitive in conceptual spaces, the proposed method is of great help in recommender systems.</p>
<p>One potential implication of our approach is that it can be used to analyze social media data and identify the geographic locations of social media users.This can be useful for understanding the spatial distribution of social media users and their behavior patterns, which can be valuable information for marketing, social science research, and public policy.While our proposed method infers spatial information of texts, rather users, respecting individuals' privacy is a fundamental principle, and any location inference or data processing should adhere to applicable privacy laws and regulations.Furthermore, our approach is scalable and can handle large volumes of textual data efficiently.This makes it suitable for processing large datasets such as social media posts, news articles, historical documents, and online reviews.In terms of uses, our approach can be applied in various domains such as disaster management, tourism, and urban planning.For example, in tourism, it can be used to identify popular tourist destinations and understand the behavior patterns of tourists.In urban planning, it can be used to analyze social media data and understand the spatial distribution of urban activities, which can inform city planning decisions.</p>
<p>The identification of geo-indicative topics in our paper is based on interpreting the entropy and Moran's I values.This interpretation is manually carried out by comparing the entropies among different topics.As a suggestion, future studies can focus on defining a baseline by which a topic is considered geo-indicative.In addition, various operations can be defined over the conceptual spaces, which makes logical inferences possible.For example, the intersection of two concepts (i.e., regions or directions) may lead to the construction of a new meaningful concept.By studying such operations, the analytical capabilities of the proposed approaches can be improved.</p>
<p>3 ) 21 Figure 1 .
3211
Figure 1.The workflow of the proposed methodology.</p>
<p>Figure 1 .
1
Figure 1.The workflow of the proposed methodology.</p>
<p>Mathematics 2023 ,
2023
11, x FOR PEER REVIEW 10 of 21</p>
<p>Figure 2 .
2
Figure 2. A schematic comparison of the two proposed versions of data-driven conceptual spaces (modified from [24]).(a) Regional Conceptual Spaces represent concepts as convex regions in a highdimensional space; (b) Directional Conceptual Spaces represent concepts as directions in a highdimensional space.</p>
<p>Figure 2 .
2
Figure 2. A schematic comparison of the two proposed versions of data-driven conceptual spaces (modified from [24]).(a) Regional Conceptual Spaces represent concepts as convex regions in a high-dimensional space; (b) Directional Conceptual Spaces represent concepts as directions in a high-dimensional space.</p>
<p>Figure 3 .
3
Figure 3.The probability surface calculated by IDW for Topic #D1 (refer to Table3) found in the corpus.</p>
<p>Figure 3 .
3
Figure 3.The probability surface calculated by IDW for Topic #D1 (refer to Table3) found in the corpus.</p>
<p>manifests the distribution of the same topic (Topic #D1) represented by KDE.Mathematics 2023, 11, x FOR PEER REVIEW 13 of 21</p>
<p>Figure 4 .
4
Figure 4.The probability surface calculated by KDE for Topic #D1.</p>
<p>Figure 4 .
4
Figure 4.The probability surface calculated by KDE for Topic #D1.</p>
<p>compares a geo-indicative topic (Topic #D2) with a non-geo-indicative topic (Topic #D4).</p>
<p>compares a geo-indicative topic (Topic #D2) with a non-geo-indicative topic (Topic #D4).</p>
<p>Figure 5 .
5
Figure 5.A comparison of the distribution of a geo-indicative (Topic #D2) versus a non-geo-indicative (Topic #D4) topic.</p>
<p>Figure 5 .
5
Figure 5.A comparison of the distribution of a geo-indicative (Topic #D2) versus a non-geo-indicative (Topic #D4) topic.</p>
<p>Figure 6 .
6
Figure 6.The locations of the most prominent documents containing the word jacuzzi in the Divar dataset and the word museum in the Wikipedia dataset and their similarity surface.</p>
<p>Figure 6 .
6
Figure 6.The locations of the most prominent documents containing the word jacuzzi in the Divar dataset and the word museum in the Wikipedia dataset and their similarity surface.</p>
<p>Figure 7 .
7
Figure 7.The probability surfaces generated by the RCS method for two geo-indicative and one non-geo-indicative regions in the case of rental property advertisements.</p>
<p>Figure 8 .
8
Figure 8.A comparison of the implemented methods in terms of cumulative distance error for both datasets.</p>
<p>Figure 8 .
8
Figure 8.A comparison of the implemented methods in terms of cumulative distance error for both datasets.</p>
<p>Table 1 .
1
Different classes of expressions, their definitions, and real-world examples from our dataset.</p>
<p>Table 2 .
2
The description of the datasets used in the study.
DatasetNo. of RecordsExample 1"220 m, 3 bedrooms, very well designed, large andspacious living room, fully furnished kitchen,southern exposure, very large meeting hall, the mostDivar11,393cozy region, very stylish and luxurious and furnished lobby, the roof garden is very wellequipped and wooded, lobby, and resident janitor; ithas two parking lots, private pool, sauna,and Jacuzzi.""The National Museum of Iran is located in Tehran,Iran. It is an institution formed of two complexes;the Museum of Ancient Iran and the Museum ofIslamic Archaeology and Art of Iran, which wereopened in 1937 and 1972, respectively. ThePersian Wikipedia704institution hosts historical monuments dating back through preserved ancient and medieval Iranianantiquities, including pottery vessels, metal objects,textile remains, and some rare books and coins. Italso includes a number of research departments,categorized by different historical periods andarchaeological topics."
1The examples are translated into English.</p>
<p>Table 3 .
3
Some topics identified in each dataset along with their entropy values.
DatasetIDTopic 1EntropyMoran's I#D1(pool, sauna, Jacuzzi, conference_hall, luxurious)4.520.344Divar#D2 #D3(city_center, bazaar, store, accessibility, commercial) (loan, under_construction, cooperation, phase, vacant)5.67 10.600.472 0.022#D4(owner, room, alley, door, telephone)33.02−0.471#W1(museum, register, national, the_King, traditional)7.510.453Persian#W2(university, faculty, science, institute, research_center)10.210.320Wikipedia#W3(station, line, metro, neighborhood, square)19.550.154#W4(neighborhood, district, city, municipality, area)29.02−0.745
1The words are translated into English.</p>
<p>Data Availability Statement:The data that support the findings of this study are available at https://figshare.com/s/abe341b40e90c566b061 (accessed on 1 August 2023).Funding acquisition, S.V.R.-T.; Investigation, O.R.A. and A.A.A.; Methodology, O.R.A.; Project administration, A.A.A.; Resources, S.V.R.-T.; Software, O.R.A.; Supervision, A.A.A.; Validation, O.R.A. and S.V.R.-T.; Visualization, O.R.A.; Writing-original draft, O.R.A.; Writing-review &amp; editing, S.V.R.-T.and A.A.A.All authors have read and agreed to the published version of the manuscript.Funding: This research received no external funding.Institutional Review Board Statement: Not applicable.Informed Consent Statement: Not applicable.the most prominent document for the words Jacuzzi and museum, and their corresponding similarity surface.
Local, national, and global applications of GIS in agriculture. J P Wilson, Geographical Information Systems: Principles, Techniques, Management, and Applications. Hoboken, NJ, USAJohn Wiley &amp; Sons1999</p>
<p>Design and development of a web-based interactive twin platform for watershed management. Y Qiu, H Duan, H Xie, X Ding, Y Jiao, 10.1111/tgis.12904Trans. GIS. 262022</p>
<p>Development of a collaborative framework for quantitative monitoring and accumulation prediction of harmful algal blooms in nearshore areas of lakes. Y Qiu, H Liu, F Liu, D Li, C Liu, W Liu, J Huang, Q Xiao, J Luo, H Duan, 10.1016/j.ecolind.2023.111154Ecol. Indic. 1562023. 111154</p>
<p>Review of GIS-based applications for mining: Planning, operation, and environmental management. Y Choi, J Baek, S Park, 10.3390/app10072266Appl. Sci. 102020. 2266</p>
<p>An Overview of GIS-Based Assessment and Mapping of Mining-Induced Subsidence. J Suh, 10.3390/app10217845Appl. Sci. 102020</p>
<p>Spatial Technology and Archaeology: The Archaeological Applications of GIS. D Wheatley, M Gillings, 2013CRC PressBoca Raton, FL, USA</p>
<p>Does mapping improve public participation? Exploring the pros and cons of using public participation GIS in urban planning practices. M Kahila-Tani, M Kytta, S Geertman, 10.1016/j.landurbplan.2019.02.019Landsc. Urban Plan. 1862019</p>
<p>Urban planning and geographic information systems. I Masser, H Ottens, Geographic Information Systems to Spatial Data Infrastructure. Boca Raton, FL, USACRC Press2019</p>
<p>Why public health needs GIS: A methodological overview. F Wang, 10.1080/19475683.2019.1702099Ann. GIS. 262019</p>
<p>Applications of GIS in Management of Water Resources to Attain Zero Hunger. A Sharma, M Kumar, N Hasteer, Advances in Water Resources Engineering and Management. Berlin/Heidelberg, GermanySpringer2020</p>
<p>Extracting Place Functionality from Crowdsourced Textual Data Using Semantic Space Modeling. M Karimi, M S Mesgari, 10.1109/ACCESS.2023.3332854IEEE Access. 112023</p>
<p>A comparative assessment of machine learning methods in extracting place functionality from textual content. M Karimi, M S Mesgari, R S Purves, 10.1111/tgis.12999Trans. GIS. 262022</p>
<p>Postal address extraction from the web: A comprehensive survey. M Kayed, S Dakrory, A A Ali, 10.1007/s10462-021-09983-1Artif. Intell. Rev. 552021</p>
<p>Educing knowledge from text: Semantic information extraction of spatial concepts and places. E Papadias, M Kokla, E Tomai, 10.5194/agile-giss-2-38-2021Agil. GIScience Ser. 2021, 2, 38. [CrossRef</p>
<p>Geographic information retrieval: Progress and challenges in spatial search of text. R S Purves, P Clough, C B Jones, M H Hall, V Murdock, 10.1561/1500000034Found. Trends Inf. Retr. 122018</p>
<p>Spatiotemporal and semantic information extraction from Web news reports about natural hazards. W Wang, K Stewart, 10.1016/j.compenvurbsys.2014.11.001Comput. Environ. Urban Syst. 502015</p>
<p>Spatially oriented convolutional neural network for spatial relation extraction from natural language texts. Q Qiu, Z Xie, K Ma, Z Chen, L Tao, 10.1111/tgis.12887Trans. GIS. 262021</p>
<p>Qualitative spatial reasoning on topological relations by combining the semantic web and constraint satisfaction. Y Wang, M Qiao, H Liu, X Ye, 10.1080/10095020.2018.1430659Geo-spatial Inf. Sci. 212017</p>
<p>Detecting geospatial location descriptions in natural language text. K Stock, C B Jones, S Russell, M Radke, P Das, N Aflaki, 10.1080/13658816.2021.1987441Int. J. Geogr. Inf. Sci. 362021</p>
<p>A spatial logic based on regions and connection. D A Randell, Z Cui, A G Cohn, 199292</p>
<p>Hybrid Geo-spatial Query Methods on the Semantic Web with a Spatially-Enhanced Index of DBpedia. E M G Younis, C B Jones, V Tanasescu, A I Abdelmoty, Proceedings of the International Conference on Geographic Information Science. the International Conference on Geographic Information ScienceColumbus, OH, USASeptember 2012</p>
<p>Creating a corpus of geospatial natural language. K Stock, R C Pasley, Z Gardner, P Brindley, J Morley, C Cialone, Proceedings of the International Conference on Spatial Information Theory. the International Conference on Spatial Information TheoryScarborough, UK; GermanyBerlin/Heidelberg2-6 September 2013. 2013</p>
<p>On the geo-indicativeness of non-georeferenced text. B Adams, K Janowicz, Proceedings of the International AAAI Conference on Web and Social Media. the International AAAI Conference on Web and Social MediaDublin, IrelandJune 20126</p>
<p>A Place Recommendation Approach Using Word Embeddings in Conceptual Spaces. O R Abbasi, A A Alesheikh, 10.1109/ACCESS.2023.3241806IEEE Access. 112023</p>
<p>Geolocation prediction in social media data by finding location indicative words. B Han, P Cook, T Baldwin, Proceedings of the COLING 2012. the COLING 2012Mumbai, India8-15 December 2012</p>
<p>ChineseTR: A weakly supervised toponym recognition architecture based on automatic training data generator and deep neural network. Q Qiu, Z Xie, S Wang, Y Zhu, H Lv, K Sun, 10.1111/tgis.12902Trans. GIS. 2022</p>
<p>Exploring place through user-generated content: Using Flickr tags to describe city cores. L Hollenstein, R Purves, J. Spat. Inf. Sci. 12010</p>
<p>Fine-grained assessment of greenspace satisfaction at regional scale using content analysis of social media and machine learning. Z Wang, Z Zhu, M Xu, S Qureshi, 10.1016/j.scitotenv.2021.145908Sci. Total. Environ. 7762021. 145908</p>
<p>Phillies tweeting from Philly? Predicting Twitter user locations with spatial word usage. H Chang, D -W.; Lee, M Eltaher, J Lee, Proceedings of the 2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining. the 2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and MiningIstanbul, TurkeyAugust 2012</p>
<p>The effect of regional variation and resolution on geosocial thematic signatures for points of interest. G Mckenzie, K Janowicz, Societal Geo-Innovation: Selected Papers of the 20th AGILE Conference on Geographic Information Science. Wageningen, The Netherlands; Cham, SwitzerlandSpringerMay 2017. 2017</p>
<p>Thematic signatures for cleansing and enriching place-related linked data. B Adams, K Janowicz, 10.1080/13658816.2014.989855Int. J. Geogr. Inf. Sci. 292015</p>
<p>Conceptual spaces as a framework for knowledge representation. P Gärdenfors, Mind Matter. 22004</p>
<p>A metric conceptual space algebra. B Adams, M Raubal, Proceedings of the International Conference on Spatial Information Theory. the International Conference on Spatial Information TheoryAber Wrac'h, France; Berlin/Heidelberg, GermanySpringerSeptember. 2009</p>
<p>Latent dirichlet allocation. D M Blei, A Y Ng, M I Jordan, Advances in Neural Information Processing Systems. Cambridge, MA, USAMIT Press200114</p>
<p>Latent dirichlet allocation. D M Blei, A Y Ng, M I Jordan, J. Mach. Learn. Res. 32003</p>
<p>Latent Dirichlet allocation (LDA) and topic modeling: Models, applications, a survey. H Jelodar, Y Wang, C Yuan, X Feng, X Jiang, Y Li, L Zhao, 10.1007/s11042-018-6894-4Multimed. Tools Appl. 782019</p>
<p>Forum latent Dirichlet allocation for user interest discovery. C Chen, J Ren, 10.1016/j.knosys.2017.04.006Knowl.-Based Syst. 1262017</p>
<p>A tutorial on kernel density estimation and recent advances. Y.-C Chen, 10.1080/24709360.2017.1396742Biostat. Epidemiol. 12017</p>
<p>Robust kernels for kernel density estimation. S Wang, A Li, K Wen, X Wu, 10.1016/j.econlet.2020.109138Econ. Lett. 2020, 191, 109138</p>
<p>Conceptual Spaces: The Geometry of Thought. P Gärdenfors, 2004MIT PressCambridge, MA, USA</p>
<p>Conceptual spaces and the strength of similarity-based arguments. I Douven, S Elqayam, P Gärdenfors, P Mirabile, 10.1016/j.cognition.2021.104951Cognition. 2182021. 104951</p>
<p>A: A novel semantic similarity measure for description logics reducing inter-concept to interinstance similarity. K Janowicz, M Wilkes, Sim-Dl, Proceedings of the European Semantic Web Conference. the European Semantic Web ConferenceHeraklion, Greece; GermanyBerlin/Heidelberg31 May-4 June 2009. 2009</p>
<p>Associationism and cognition: Human contingency learning at 25. D R Shanks, 10.1080/17470210601000581Q. J. Exp. Psychol. 602007</p>
<p>Prototypes, poles, and tessellations: Towards a topological theory of conceptual spaces. Synthese. T Mormann, 2021199</p>
<p>What conceptual spaces can do for Carnap's late inductive logic. M Sznajder, 10.1016/j.shpsa.2015.12.001Stud. Hist. Philos. Sci. Part A. 562016</p>
<p>Conceptual spaces, generalisation probabilities and perceptual categorization. N L Poth, Conceptual Spaces: Elaborations and Applications. Berlin/Heidelberg, GermanySpringer2019</p>
<p>Explanatory AI for Pertinent Communication in Autonomic Systems. M Pol, J.-L Dessalles, A Diaconescu, Proceedings of the SAI Intelligent Systems Conference. the SAI Intelligent Systems ConferenceLondon, UK6-9 September 2019</p>
<p>Data-driven Conceptual Spaces: Creating Semantic Representations for Linguistic Descriptions Of Numerical Data. H Banaee, E Schaffernicht, A Loutfi, 10.1613/jair.1.11258J. Artif. Intell. Res. 632018</p>
<p>A Parameterized Representation of Uncertain Conceptual Spaces. O Ahlqvist, 10.1111/j.1467-9671.2004.00198.xTrans. GIS. 82004</p>
<p>Hierarchical conceptual spaces for concept combination. M Lewis, J Lawry, 10.1016/j.artint.2016.04.008Artif. Intell. 2372016</p>
<p>Arabic Questions Classification Using Modified TF-IDF. A S Alammary, 10.1109/ACCESS.2021.3094115IEEE Access. 92021</p>
<p>Multidimensional scaling. M C Hout, M H Papesh, S D Goldinger, 10.1002/wcs.1203Wiley Interdiscip. Rev. Cogn. Sci. 42013</p>
<p>A Review of Multidimensional Scaling (MDS) and its Utility in Various Psychological Domains. N Jaworska, A Chupetlovska-Anastasova, 10.20982/tqmp.05.1.p001Tutorials Quant. Methods Psychol. 52009</p>
<p>Art expertise: A study of concepts and conceptual spaces. D Augustin, H Leder, Psychol. Sci. 481352006</p>
<p>P Qi, Y Zhang, Y Zhang, J Bolton, C D Manning, Stanza, arXiv:2003.07082A Python Natural Language Processing Toolkit for Many Human Languages. 2020</p>
<p>A rigorous notion at the crossroads between probability, information theory, dynamical systems and statistical physics. A Lesne, Shannon Entropy, 10.1017/S0960129512000783Math. Struct. Comput. Sci. 242014. e240311</p>
<p>A computational theory of surprise. P Baldi, Information, Coding and Mathematics. Boston, MA, USASpringer2002</p>
<p>Spatial autocorrelation. A Getis, Handbook of Applied Spatial Analysis: Software Tools, Methods and Applications. GermanyBerlin/Heidelberg2009</p>
<p>Spatial autocorrelation. S W Myint, Encyclopedia of Geography. Sage: London, UK; Thousand Oaks, CA, USA2010</p>
<p>Disclaimer/Publisher's Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods. instructions or products referred to in the content</p>            </div>
        </div>

    </div>
</body>
</html>
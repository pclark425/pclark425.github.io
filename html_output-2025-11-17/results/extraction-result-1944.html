<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1944 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1944</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1944</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-43.html">extraction-schema-43</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments or studies that measure bloat, diversity, and executability (or related metrics like validity, correctness, or fitness) in genetic programming or evolutionary computation systems, particularly focusing on how these metrics interact and trade off against each other.</div>
                <p><strong>Paper ID:</strong> paper-280649897</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.10780v1.pdf" target="_blank">Learning Task Execution Hierarchies for Redundant Robots</a></p>
                <p><strong>Paper Abstract:</strong> Modern robotic systems, such as mobile manipulators, humanoids, and aerial robots with arms, often possess high redundancy, enabling them to perform multiple tasks simultaneously. Managing this redundancy is key to achieving reliable and flexible behavior. A widely used approach is the Stack of Tasks (SoT), which organizes control objectives by priority within a unified framework. However, traditional SoTs are manually designed by experts, limiting their adaptability and accessibility. This paper introduces a novel framework that automatically learns both the hierarchy and parameters of a SoT from user-defined objectives. By combining Reinforcement Learning and Genetic Programming, the system discovers task priorities and control strategies without manual intervention. A cost function based on intuitive metrics such as precision, safety, and execution time guides the learning process. We validate our method through simulations and experiments on the mobile-YuMi platform, a dual-arm mobile manipulator with high redundancy. Results show that the learned SoTs enable the robot to dynamically adapt to changing environments and inputs, balancing competing objectives while maintaining robust task execution. This approach provides a general and user-friendly solution for redundancy management in complex robots, advancing human-centered robot programming and reducing the need for expert design.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1944.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1944.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments or studies that measure bloat, diversity, and executability (or related metrics like validity, correctness, or fitness) in genetic programming or evolutionary computation systems, particularly focusing on how these metrics interact and trade off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SoT-GP (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stack-of-Tasks Genetic Programming + Reinforcement Learning framework (Learning Task Execution Hierarchies for Redundant Robots)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experimental application of Genetic Programming (GP) integrated with Reinforcement Learning to evolve structured 'Stack of Tasks' (SoT) policies for redundant robots; fitness is a user-defined cost combining precision, safety, manipulability and time, and GP evolves task ordering, parameters and Boolean activation flags.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SoT-GP (GP + RL for Stack-of-Tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A Genetic Programming optimizer that evolves structured SoT individuals (a fixed-length stack data structure encoding task priority order, per-task parameters, and Boolean activation flags). Representation is a structured/array-like SoT (not traditional program trees in the experiments), where each element encodes a task id, its parameters θ, and an activation flag; genetic operators (crossover/mutation) operate on SoT entries; evaluation is episodic via simulation (Gazebo) using an RL-style cost (reward) signal.</td>
                        </tr>
                        <tr>
                            <td><strong>bloat_metric</strong></td>
                            <td>Not explicitly measured as classic GP bloat (e.g., tree/node counts). The paper uses 'active task count' / activation flags as a phenotype-length proxy and enforces parameter bounds to limit uncontrolled growth; also uses a distance-based measure between SoTs but not as a size/bloat metric.</td>
                        </tr>
                        <tr>
                            <td><strong>bloat_measurements</strong></td>
                            <td>No numerical measurements of bloat (no tree/node size traces). The paper reports that inactive tasks are preserved in genotype (introns) so phenotype length can be smaller than genotype, but gives no quantitative growth over generations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td>Two diversity-related measures were used: (1) a formal distance between SoTs d(SoT_a,SoT_b) defined in Eq.9 combining parameter distance δ_θ(·,·) (L2 norm of parameter differences) and priority-position difference δ_prior (absolute difference of priority indices) across tasks; (2) empirical generational statistics such as percent of individuals in a generation that share the final priority order (genotypic/structural similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_measurements</strong></td>
                            <td>Qualitative / per-experiment reports: percent-of-individuals-with-final-order rises over generations (figure shown); convergence to final order occurs in 2–3 generations for simple base-only scenarios and requires more generations when more tasks (e.g., arm+base). Parameter-learning phase reached stationarity after ~6–7 generations in reported examples. No absolute numeric series of the distance metric values are tabulated in the text, but Fig.11 and Fig.14 visualize the trends (percent matching final order; minimum index of a non-relevant task over generations).</td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Fitness / executability is measured by a user-defined cost function C used as the minimization objective (weighted sum of cost components): precision (||k_d - k||^2), safety (pseudo-energy from LiDAR distances), manipulability (1 / w_max.manip or related measures), distance from joint limits, and time (t^2). Success/failure events (e.g., collisions, singularities) impose high cost or termination. The cost is used as the fitness/executability measure.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_measurements</strong></td>
                            <td>Reported outcomes: example cost functions C1 and C2 with specific weightings (C1 = 0.4*c_prec + 0.5*c_min.dist + 0.1*c_max.manip; C2 = 0.5*c_prec + 0.5*c_time). The GP search found stationary (converged) solutions in the reported cases after 6–7 generations (parameters learning) and 2–3 generations (simple priority learning); lab tests confirm zero-shot sim-to-real transfer with qualitatively similar cost trajectories (Fig.15) but with slower dynamic responses in real robot due to sensor noise/delay. No report of % syntactically valid programs or compilation/interpretation rates (not applicable here).</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>selection_method</strong></td>
                            <td>Tournament selection (binary tournament) as stated in the GP pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>population_size</strong></td>
                            <td>Not numerically specified in the reported results; GUI and text allow the user to configure population size and number of generations, but the paper does not give a fixed numeric population size used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>special_operators</strong></td>
                            <td>Activation-flag 'introns' mechanism (Boolean task activation flags that preserve inactive tasks in the genotype), null-space projection applied to task outputs (domain-specific operator impacting phenotype behavior), parameter bounds (hard constraints on task parameter ranges to ensure stability), and a distance-based stopping/threshold metric for convergence based on d(SoT_a,SoT_b).</td>
                        </tr>
                        <tr>
                            <td><strong>observed_tradeoffs</strong></td>
                            <td>Explicitly observed trade-offs include: (a) safety vs precision/time — evolved solutions often prioritize Obstacle Avoidance (safety) with highest priority, sometimes at the cost of missing the precise target or increasing execution time (Fig.16 example where robot moves away to avoid human and may miss target); (b) adding non-relevant (distracting) tasks increases search difficulty and slows convergence (diversity/noise in the search vs time-to-convergence / executability); (c) intron-like inactive tasks preserve latent behaviors (increasing genotypic diversity) while keeping phenotype simple, trading off potential genotypic 'bloat' for robustness against losing useful modules.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Reported qualitative dynamics: priority-order learning: convergence in ~2–3 generations when only base tasks (OA, IK) are relevant; with arm+base and more tasks, convergence requires more generations (examples shown); parameter learning: stationary points reached after ~6–7 generations for the two example cost functions. Rejection/movement of non-relevant tasks toward low priorities is observable over generations but with stochastic timing (Fig.14). Executability (cost) typically decreases during evolution and reaches a plateau at stationarity; in lab tests cost trajectories differ slightly due to noise/delays but show similar qualitative trends.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_triangle_constraint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_to_triangle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_explanation</strong></td>
                            <td>Mechanisms discussed: (1) Introns (preserved inactive tasks via Boolean activation flags) act as latent modules that protect potentially useful genetic material from destructive crossover, preserving genotypic diversity and enabling reactivation later—this is argued to reduce premature convergence; (2) tournament selection combined with parameter bounds enforces selection pressure while preventing parameter runaway (stability constraints); (3) distance-based convergence check (d(SoT_a,SoT_b)) measures both parameter and priority differences to detect population homogenization; (4) domain-specific null-space projection ensures that lower-priority tasks do not interfere with higher-priority tasks, letting the phenotype behavior reflect task priorities directly, which couples fitness (executability) tightly to task ordering and parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_type</strong></td>
                            <td>Robotic control / Stack-of-Tasks policy synthesis for redundant manipulators (sim-to-real robotics application).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>Structured/array-like SoT representation encoding a fixed-length stack of task entries (task id, parameters θ, Boolean activation flag). Although tree-based GP is discussed in background, the implemented representation and genetic operators operate over SoT entries (structured genome), not strictly classical tree-based program trees in the reported experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Explicitly Defined Introns and Destructive Crossover in Genetic Programming <em>(Rating: 2)</em></li>
                <li>The benefits of computing with introns <em>(Rating: 2)</em></li>
                <li>A Genetic Programming Tutorial <em>(Rating: 1)</em></li>
                <li>Evolving simple software agents: comparing genetic algorithm and genetic programming performance <em>(Rating: 1)</em></li>
                <li>On Using Surrogates with Genetic Programming <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1944",
    "paper_id": "paper-280649897",
    "extraction_schema_id": "extraction-schema-43",
    "extracted_data": [
        {
            "name_short": "SoT-GP (this paper)",
            "name_full": "Stack-of-Tasks Genetic Programming + Reinforcement Learning framework (Learning Task Execution Hierarchies for Redundant Robots)",
            "brief_description": "Experimental application of Genetic Programming (GP) integrated with Reinforcement Learning to evolve structured 'Stack of Tasks' (SoT) policies for redundant robots; fitness is a user-defined cost combining precision, safety, manipulability and time, and GP evolves task ordering, parameters and Boolean activation flags.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SoT-GP (GP + RL for Stack-of-Tasks)",
            "system_description": "A Genetic Programming optimizer that evolves structured SoT individuals (a fixed-length stack data structure encoding task priority order, per-task parameters, and Boolean activation flags). Representation is a structured/array-like SoT (not traditional program trees in the experiments), where each element encodes a task id, its parameters θ, and an activation flag; genetic operators (crossover/mutation) operate on SoT entries; evaluation is episodic via simulation (Gazebo) using an RL-style cost (reward) signal.",
            "bloat_metric": "Not explicitly measured as classic GP bloat (e.g., tree/node counts). The paper uses 'active task count' / activation flags as a phenotype-length proxy and enforces parameter bounds to limit uncontrolled growth; also uses a distance-based measure between SoTs but not as a size/bloat metric.",
            "bloat_measurements": "No numerical measurements of bloat (no tree/node size traces). The paper reports that inactive tasks are preserved in genotype (introns) so phenotype length can be smaller than genotype, but gives no quantitative growth over generations.",
            "diversity_metric": "Two diversity-related measures were used: (1) a formal distance between SoTs d(SoT_a,SoT_b) defined in Eq.9 combining parameter distance δ_θ(·,·) (L2 norm of parameter differences) and priority-position difference δ_prior (absolute difference of priority indices) across tasks; (2) empirical generational statistics such as percent of individuals in a generation that share the final priority order (genotypic/structural similarity).",
            "diversity_measurements": "Qualitative / per-experiment reports: percent-of-individuals-with-final-order rises over generations (figure shown); convergence to final order occurs in 2–3 generations for simple base-only scenarios and requires more generations when more tasks (e.g., arm+base). Parameter-learning phase reached stationarity after ~6–7 generations in reported examples. No absolute numeric series of the distance metric values are tabulated in the text, but Fig.11 and Fig.14 visualize the trends (percent matching final order; minimum index of a non-relevant task over generations).",
            "executability_metric": "Fitness / executability is measured by a user-defined cost function C used as the minimization objective (weighted sum of cost components): precision (||k_d - k||^2), safety (pseudo-energy from LiDAR distances), manipulability (1 / w_max.manip or related measures), distance from joint limits, and time (t^2). Success/failure events (e.g., collisions, singularities) impose high cost or termination. The cost is used as the fitness/executability measure.",
            "executability_measurements": "Reported outcomes: example cost functions C1 and C2 with specific weightings (C1 = 0.4*c_prec + 0.5*c_min.dist + 0.1*c_max.manip; C2 = 0.5*c_prec + 0.5*c_time). The GP search found stationary (converged) solutions in the reported cases after 6–7 generations (parameters learning) and 2–3 generations (simple priority learning); lab tests confirm zero-shot sim-to-real transfer with qualitatively similar cost trajectories (Fig.15) but with slower dynamic responses in real robot due to sensor noise/delay. No report of % syntactically valid programs or compilation/interpretation rates (not applicable here).",
            "crossover_rate": null,
            "mutation_rate": null,
            "selection_method": "Tournament selection (binary tournament) as stated in the GP pipeline.",
            "population_size": "Not numerically specified in the reported results; GUI and text allow the user to configure population size and number of generations, but the paper does not give a fixed numeric population size used in experiments.",
            "special_operators": "Activation-flag 'introns' mechanism (Boolean task activation flags that preserve inactive tasks in the genotype), null-space projection applied to task outputs (domain-specific operator impacting phenotype behavior), parameter bounds (hard constraints on task parameter ranges to ensure stability), and a distance-based stopping/threshold metric for convergence based on d(SoT_a,SoT_b).",
            "observed_tradeoffs": "Explicitly observed trade-offs include: (a) safety vs precision/time — evolved solutions often prioritize Obstacle Avoidance (safety) with highest priority, sometimes at the cost of missing the precise target or increasing execution time (Fig.16 example where robot moves away to avoid human and may miss target); (b) adding non-relevant (distracting) tasks increases search difficulty and slows convergence (diversity/noise in the search vs time-to-convergence / executability); (c) intron-like inactive tasks preserve latent behaviors (increasing genotypic diversity) while keeping phenotype simple, trading off potential genotypic 'bloat' for robustness against losing useful modules.",
            "temporal_dynamics": "Reported qualitative dynamics: priority-order learning: convergence in ~2–3 generations when only base tasks (OA, IK) are relevant; with arm+base and more tasks, convergence requires more generations (examples shown); parameter learning: stationary points reached after ~6–7 generations for the two example cost functions. Rejection/movement of non-relevant tasks toward low priorities is observable over generations but with stochastic timing (Fig.14). Executability (cost) typically decreases during evolution and reaches a plateau at stationarity; in lab tests cost trajectories differ slightly due to noise/delays but show similar qualitative trends.",
            "supports_triangle_constraint": null,
            "counterexample_to_triangle": null,
            "mechanism_explanation": "Mechanisms discussed: (1) Introns (preserved inactive tasks via Boolean activation flags) act as latent modules that protect potentially useful genetic material from destructive crossover, preserving genotypic diversity and enabling reactivation later—this is argued to reduce premature convergence; (2) tournament selection combined with parameter bounds enforces selection pressure while preventing parameter runaway (stability constraints); (3) distance-based convergence check (d(SoT_a,SoT_b)) measures both parameter and priority differences to detect population homogenization; (4) domain-specific null-space projection ensures that lower-priority tasks do not interfere with higher-priority tasks, letting the phenotype behavior reflect task priorities directly, which couples fitness (executability) tightly to task ordering and parameters.",
            "domain_type": "Robotic control / Stack-of-Tasks policy synthesis for redundant manipulators (sim-to-real robotics application).",
            "representation_type": "Structured/array-like SoT representation encoding a fixed-length stack of task entries (task id, parameters θ, Boolean activation flag). Although tree-based GP is discussed in background, the implemented representation and genetic operators operate over SoT entries (structured genome), not strictly classical tree-based program trees in the reported experiments.",
            "uuid": "e1944.0"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Explicitly Defined Introns and Destructive Crossover in Genetic Programming",
            "rating": 2
        },
        {
            "paper_title": "The benefits of computing with introns",
            "rating": 2
        },
        {
            "paper_title": "A Genetic Programming Tutorial",
            "rating": 1
        },
        {
            "paper_title": "Evolving simple software agents: comparing genetic algorithm and genetic programming performance",
            "rating": 1
        },
        {
            "paper_title": "On Using Surrogates with Genetic Programming",
            "rating": 1
        }
    ],
    "cost": 0.01218775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning Task Execution Hierarchies for Redundant Robots
14 Aug 2025</p>
<p>Alessandro Adami 
Dept. of Information Engineering
University of Padova
Italy</p>
<p>Polytechnic of Bari Dept. of Electrical and Information Engineering
Italy</p>
<p>Aris Synodinos 
Matteo Iovino 
Ruggero Carli 
Dept. of Information Engineering
University of Padova
Italy</p>
<p>Pietro Falco 
Dept. of Information Engineering
University of Padova
Italy</p>
<p>ABB Corporate Research
VästeråsSweden</p>
<p>Learning Task Execution Hierarchies for Redundant Robots
14 Aug 20255FE86C2C25953277CFEFE8959BD86C33arXiv:2508.10780v1[cs.RO]Genetic ProgrammingReinforcement LearningLearning Stack of TasksRedundancyTask PrioritizationRedundant Robots
Modern robotic systems -such as mobile manipulators, humanoids, and aerial robots with arms -often possess high redundancy, enabling them to perform multiple tasks simultaneously.Managing this redundancy is key to achieving reliable and flexible behavior.A widely used approach is the Stack of Tasks (SoT), which organizes control objectives by priority within a unified framework.However, traditional SoTs are manually designed by experts, limiting their adaptability and accessibility.This paper introduces a novel framework that automatically learns both the hierarchy and parameters of a SoT from user-defined objectives.By combining Reinforcement Learning and Genetic Programming, the system discovers task priorities and control strategies without manual intervention.A cost function based on intuitive metrics -such as precision, safety, and execution time -guides the learning process.We validate our method through simulations and experiments on the mobile-YuMi platform, a dual-arm mobile manipulator with high redundancy.Results show that the learned SoTs enable the robot to dynamically adapt to changing environments and inputs, balancing competing objectives while maintaining robust task execution.This approach provides a general and userfriendly solution for redundancy management in complex robots, advancing human-centered robot programming and reducing the need for expert design.Note to Practitioners-In real-world robotics applications such as collaborative manufacturing, inspection, or service tasks, robots often have more degrees of freedom than strictly needed for a single task.This redundancy allows them to perform multiple tasks simultaneously, such as holding a tool steady while avoiding nearby humans or reaching a target while maximizing manipulability.Traditionally, engineers must manually program task priorities and control strategies to exploit this redundancy, a complex and time-consuming process.This paper introduces a learning-based framework that allows robots to automatically generate and adapt task hierarchies, using a combination of Genetic Programming and RL.The user specifies their objectives through a simple and customizable cost function (e.g., prioritize obstacle avoidance or precision), and the system evolves a 'Stack of Tasks' that balances different goals in real time.The method is validated on a real ABB mobile YuMi platform, showing how the learned behaviors enable the robot to adaptively coordinate multiple objectives under changing conditions.This framework is general and can be applied to any redundant robotic platform, reducing integration effort and increasing autonomy in complex settings.</p>
<p>I. INTRODUCTION</p>
<p>M ODERN robotic systems are increasingly deployed in complex, unstructured, and dynamic environments where traditional rigid control architectures struggle to meet the operational demands.In particular, mobile manipulators, such as the mobile-YuMi research platform, exemplify the next generation of robotic systems that combine high dexterity with the ability to navigate and interact with objects within unpredictable settings.These systems are characterized by their high degree of redundancy, which, while affording significant flexibility and the capability to perform a wide variety of tasks, also imposes substantial challenges in terms of control, planning, and task prioritization.Redundancy in robotic manipulators implies that the system possesses more degrees of freedom (DOF) than strictly necessary to execute a specific task.This surplus of DOF can be exploited to optimize multiple objectives, such as avoiding obstacles, maximizing manipulability [26], or maintaining safe distances from mechanical joint limits.However, managing this redundancy is non-trivial, particularly when the robot is required to perform multiple tasks concurrently.Deciding which of many possible task execution strategies to follow is a highly challenging problem and an active research field.In [9], the authors present a unified control framework for redundant robots that addresses kinematic and dynamic control while handling hierarchical constraints and optimizing redundancy through a generalized null space algorithm.In [36], the authors present an online kinematic control approach for mobile bimanual manipulation in dynamic environments, using distributed distance sensors and priority-constrained optimization to enable real-time collision avoidance and task execution.In current practice, the redundancy management problem is handled manually by expert programmers who hard-code task sequences and decision rules.This manual approach is not only time-consuming and costly, but also inflexible: adapting the robot to a new mission or environment typically requires reprogramming the entire task logic.To address these limitations, we propose a method that automatically learns the stack of tasks required to fulfill a high-level mission by using a cost function to guide task parameters and prioritization tuning.This approach reduces reliance on manual tuning and enables scalable, adaptive robot behavior across different scenarios.Concurrently, Genetic Programming has emerged as a powerful evolutionary method for automatically generating control programs [12].Unlike traditional genetic algorithms that operate on fixed-length strings [15], Genetic Programming evolves variable-length programs, thus capturing the hierarchical and</p>
<p>Reinforcement Learning</p>
<p>RL Simulation</p>
<p>SoTs are tested on the same task</p>
<p>Fitness evaluation</p>
<p>Cost is assigned to each SoT</p>
<p>Genetic Programming</p>
<p>Fitness minimizer</p>
<p>Genetic Selection</p>
<p>Reduce population</p>
<p>Genetic Recombination</p>
<p>Restore population size  First, all the Stack of Tasks are executed, and a cost function is assigned through RL.Then, Genetic Programming evolves new potential solutions, which return a lower cost function.The process is iterated until a stationary point is reached, and then the best Stack of Tasks is found.Finally, the evolved SoT strategies were tested on a real mobile dual-arm robot (mobile YuMi produced by ABB).</p>
<p>dynamic nature of complex tasks [34].This paper proposes an integrated framework that leverages both RL and Genetic Programming to address the dual challenges of task prioritization and redundancy management in mobile manipulators (as shown in Fig. 1).Central to our approach is the introduction of the "Stack of Tasks" (SoT) representation in Genetic Programming, a structured format that encapsulates the priority order of each task that comprises the high-level mission, associated control parameters, and the activation status within a single composite entity.The SoT not only provides a natural and efficient means of representing task hierarchies, but also facilitates the application of genetic operators, such as mutation and crossover, to evolve and optimize the task stack.The evolution of the SoT is guided by a user-defined cost function, which is expressed as a weighted combination of multiple performance metrics.These metrics are defined during the algorithm development in order to exploit features of the robot that may be useful in a realworld scenario.They include precision (measured by the deviation from a desired pose), safety (evaluated based on the proximity to obstacles), manipulability (which is strictly connected to kinematic performances), and execution time.</p>
<p>The integration of RL with Genetic Programming within our framework offers several distinct advantages.First, it allows the robotic system to handle the redundancy automatically, without designing the stack manually, enhancing robustness and flexibility.Second, the evolutionary aspect of Genetic Programming enables the automatic discovery of optimal task sequences and control parameters, reducing the need for manual tuning and prior knowledge.Third, by explicitly incorporating a user-defined cost function, our approach ensures that the optimization process remains aligned with the specific objectives and constraints of the application at hand.Because the task order and parameters are learned independently of the environment, they can be generalized to any real-world setting where the same mission and cost function apply.This enables scalable, adaptive robot behavior without the need for manual reprogramming.The proposed framework is validated through a comprehensive experimental study that includes both simulation and real-world experiments.Simulations are conducted using the Gazebo environment, where the robot operates within a strictly defined dynamic environment.These simulations are crucial in evaluating the convergence properties of the Genetic Programming algorithm and in finetuning the RL parameters before deployment on physical hardware.Subsequent experiments on the mobile-YuMi research platform further demonstrate the practical viability of the approach, showcasing its ability to adapt to unforeseen changes, such as moving obstacles and variations in task priorities, while ensuring stable and efficient operation.In summary, the main contributions of this paper are twofold.</p>
<p>• The core contribution of this work is a method for learning the structure of a SoT -including task ordering, parameter tuning, and task activation -for redundant robots, based on high-level user-defined cost functions.</p>
<p>In particular, we frame the problem as policy search in a reinforcement learning-style setting, where the policy corresponds to a structured SoT.Genetic Programming is used as the global optimizer to discover effective SoT configurations.Unlike conventional RL, we do not train a policy function: instead, we search directly in a discretecontinuous policy space using episodic cost as feedback.</p>
<p>• Because the task order is automatically learned with task parameters -rather than manually designed or tailored to a specific environment -it can be directly generalized to different real-world settings where the same high-level mission and cost function apply.This fully automated approach eliminates the need for manual reprogramming, enabling scalable and adaptive robot behavior across diverse deployment scenarios.We demonstrate the practical efficacy of our approach through simulations and laboratory experiments, highlighting its robustness in dynamic and unpredictable environments while maintaining a strong emphasis on safety and performance.The remainder of this paper is organized as follows.Section II reviews related work in redundancy management and task prioritization in robotic systems.Section III and IV detail the theoretical framework underpinning our integrated approach, including a discussion of the RL and Genetic Programming methodologies.Section V describes the experimental setup, including the simulation environment and the mobile-YuMi testbed.Section VI presents and analyzes the experimental results, and Section VII concludes the paper while outlining directions for future research.</p>
<p>II. RELATED WORKS</p>
<p>The SoT framework has been widely used in robot control for managing redundancy and task prioritization.A significant body of literature has demonstrated the versatility and effectiveness of SoT in various contexts.However, the definition of the tasks and their hierarchical structure is typically performed manually.Authors in [24] introduced a generalized SoT framework for humanoid robot control, where tasks such as position, orientation, and posture were manually specified and prioritized.Similarly, in the context of dynamic motion capture, in [31] the SoT paradigm is applied to reproduce and edit human motions on robots.In later work [23], SoT formulation is extended to incorporate unilateral constraints such as contact forces.While this allowed for more expressive control, the definition and prioritization of tasks and constraints remained manual.Authors in [16] integrated visuo-tactile perception into the SoT framework to enable fine manipulation.The resulting tasks were still manually defined based on sensor interpretation and task heuristics.More recently, in [5] it was proposed to combine SoT with Behavior Trees to introduce modularity and high-level decision-making.Nevertheless, even in this hybrid architecture, individual tasks within the SoT were explicitly specified by the developer.While the Stack of Tasks (SoT) paradigm is widely used for redundancy management in robotics, task priorities, control parameters, and activation structures are still typically manually designed by experts, limiting scalability and adaptability in dynamic environments.Some recent works introduce data-driven methods -such as Reinforcement Learning or Behavior Trees -to automate task selection or control policy generation, but they stop short of explicitly learning full SoT structures.In recent years, Genetic Algorithms (GA) and Genetic Programming combined with Behavior Trees (BT) have been effectively used in robotics to address the challenges associated with redundancy for the control and path planning of robots with more degrees of freedom than needed.This characteristic allows great flexibility but also introduces complexity in task computation.In [27], the authors address the challenge of point-to-point motion in redundant manipulators operating in environments with obstacles.GAs are employed to find optimal paths that avoid collisions while ensuring precise movement.In [28], the authors present a modified GA to address the inverse kinematics problem in redundant robot manipulators working in environments with obstacles.The approach formulates the problem as an optimization task, aiming to minimize both the positional error of the end-effector and the joint displacements of the robot, showing improved accuracy over traditional methods.Furthermore, hierarchical task prioritization has been explored extensively, with methods such as null space projection and behavior trees providing a framework for managing redundant degrees of freedom in manipulators [14,6].These methods have shown that such hierarchical structures not only improve task execution reliability but also facilitate smoother integration of multiple concurrent tasks.In [4], the authors propose a two-layer motion planning framework that combines a sampling-based Cartesian-space planner with set-based inverse kinematics to enable efficient and reactive control of redundant manipulators in dynamic, partially unstructured environments.However, its performance may be limited by reliance on accurate perception and environmental modeling, and scalability challenges may arise in complex, high-dimensional tasks that affect real-time responsiveness.Emerging techniques, including deep Reinforcement Learning (deep RL), have demonstrated significant promise in handling complex decision-making problems in robotics [19].Deep RL can learn robust control policies directly from highdimensional sensor data.However, these methods often lack explicit representations for task prioritization, making it difficult to interpret and adjust the control hierarchy when safety or precision requirements change.However, these works do not aim to automatically discover or optimize SoT hierarchies with task-specific null-space projections and control-level prioritization.In contrast to these existing approaches, our work introduces an integrated framework that automatically handles redundancy management using a sim-to-real transfer without a fine-tuning method based on Genetic Programming.In most state-of-the-art robotics systems, redundancy management is handled by an expert programmer, who manually defines the SoT.However, this manual approach is expensive and inflexible.On top of that, manually designing SoTs for highly-redundant robots such as dual-arm mobile manipulators is impractical because it becomes overly complex and unmanageable as the number of tasks increases [5].Our approach not only evolves the optimal order of task execution, but also adapts control parameters in real time to minimize a user-defined cost function.By dynamically learning both task priorities and associated parameters, the proposed framework directly addresses the limitations in prior methods of static task hierarchies and manual tuning.To the best of our knowledge, no prior work jointly learns the task order, control parameters, and task activation flags of a Stack of Tasks architecture for redundant robots using a performance-driven approach.Our work fills this gap by treating SoT as a structured policy representation and evolving it based on user-defined cost functions, enabling sim-to-real transfer without fine-tuning and robust behavior adaptation in complex scenarios automatically.</p>
<p>III. PROBLEM STATEMENT AND PROPOSED SOLUTION</p>
<p>Managing the redundancy of robotic manipulators with multiple degrees of freedom (DOF) is a critical challenge.Redundancy in a robotic manipulator occurs when there are more degrees of freedom than are strictly necessary to perform a given task.Formally, for a manipulator with n degrees of freedom operating in a task space of dimension m, redundancy is defined as the condition where n &gt; m.As an example, consider a 7-DOF robotic arm (namely, an arm with seven revolute joints) tasked with positioning its end-effector in a specific pose in 3D space defined as an element of SE(3).Since specifying a unique pose in SE(3) requires m = 6 parameters (three for position and three for orientation), only 6 DOFs are necessary to fully constrain the task.Therefore, a 7-DOF arm (n = 7) is said to be kinematically redundant, as it possesses one extra degree of freedom beyond what is required for the task.Such redundant manipulators admit infinite joint configurations that achieve the same end-effector pose, making them highly versatile for constrained or dynamic environments [33].To choose one among the infinite joint configurations that solve the task mentioned above, we can introduce a secondary task to fulfill at the same time, for example, maximizing the manipulability.This way, we define a simple stack of two tasks.It is important to note that the SoT approach allows for handling at the same time multiple tasks exploiting the redundancy.Thus, redundancy can be exploited to optimize multiple objectives, such as avoiding obstacles, minimizing energy consumption, or avoiding getting close to joint limits.However, while redundancy management is a well-studied problem, our focus lies in automatically handling this additional freedom to determine which of the many possible solutions best aligns with the overall system goals.This requires not just advanced control strategies but a mechanism for autonomously selecting and prioritizing actions based on task parameters and mission objectives.Although this redundancy offers the potential for greater flexibility and adaptability, it also introduces complexity in decision-making because the robot can achieve the task with infinitely many configurations with varying degrees of efficiency, precision, encumbrance, and other performance indices.In dynamic and unpredictable environments, the ability of a robot to adapt its movements is crucial to achieving optimal performance.Without a robust method to select the most appropriate configuration, the benefits of redundancy can be easily lost, leading to inefficient results or even failure in task execution.The central problem we address is the automatic derivation of an optimal task execution management referred to as a SoT -that allows a robot to successfully complete a highlevel mission in dynamic or unknown environments.Rather than relying on manually defined task orders, which are often inflexible and environment-specific, our objective is to learn this stack from simulation and transfer it directly to real environments, using task parameters and a cost function to guide the prioritization.This enables adaptive, scalable, and context-aware redundancy management without the need for traditional hard-coded SoT formulation.The proposed solution (schematized in Fig. 1) comprises RL and Genetic Programming techniques, combining and enhancing the characteristics of both methodologies to find an optimal solution for the problem.This framework optimizes the execution and prioritization of different tasks to perform a global mission while minimizing the cost associated with individual tasks.The SoT architecture represents the tasks that the robot is allowed to execute by encoding the priority order of their execution, while carrying information about the parameters and the fitness measure.In our solution, RL and Genetic Programming are exploited to optimize the SoT structure representation of a high-level mission.The RL component enables the system to learn optimal SoTs through environmental feedback using the Genetic Programming optimizer.Specifically, the agent receives a reward based on the performance metrics defined by a user-specific cost function.This reward signal is then used to update the SoT according to established algorithms [37].The cost function to be minimized is customized by the user based on the specific priorities of the application, such as minimizing time or maximizing precision.Genetic Programming is currently employed to evolve the structure of the SoT.Genetic operators such as mutation and crossover are applied to candidate solutions, each encoding a particular task priority and associated control parameters.The fitness evaluation in the Genetic Programming process is directly informed by the RL-derived cost function.Initially, all SoTs are executed, and RL assigns a corresponding cost function.Genetic Programming then explores new potential solutions aimed at minimizing this cost.This iterative process continues until a stationary point is reached, at which stage the optimal Stack of Tasks is determined.This integration ensures that the evolutionary process favors SoTs that not only achieve low cost but are also adaptive to the dynamic feedback provided by the RL component.In the algorithmic pipeline, the user defines a cost function C based on the requirements that must be satisfied in a specific use case.For ease of use, the user directly specifies the desired high-level requirements and their weights for the given scenario.Then, the choices are automatically mapped into a convex cost function.Moreover, state-of-the-art approaches can be adapted and used to generate reward functions directly from natural language task descriptions [38].Then, an initial population of SoTs candidates of size n T -where n T is the number of tasks -is randomly created, and through RL, the cost is evaluated for each of them.The same high-level mission is executed with randomized initial conditions for all of the candidates until the goal is reached or the execution fails.Once all SoTs of the current population are evaluated, Genetic Programming techniques are used to create new individuals and potentially find a better solution to the problem.As a first step, genetic selection is applied, pairing the SoTs and propagating only the one with the lowest cost (namely, the one with the best performance) in the next generation and discarding the least effective solution, as it does not contribute to improving the overall performance.Therefore, the framework employs tournament selection [8], a common method in Genetic Programming, to evolve better task prioritization strategies.This binary tournament approach ensures that better-performing solutions are more likely to propagate, guiding the search towards more efficient and adaptive task management policies.In the second step, the initial amount of individuals will be restored through Genetic Programming operations (crossover Fig. 3 and mutation Fig. 4).Then, new SoTs individuals will be evaluated as well.This loop continues until the fitness scores converge to a stationary value, meaning that the evolved SoT individuals are almost equal to each other.Two different SoTs are considered to be almost equal when their distance d (see Sec. IV-E) is below a threshold that is chosen manually based on empirical observation.In this context, the use of Genetic Programming provides a framework for the robot to learn task prioritization and parameters.The robot explores various potential solutions, continuously refining its approach to achieve the optimal balance of task execution.After that, the best SoT -the one with the lowest cost in the final population -will be tested in a real laboratory environment, to prove the effectiveness of the algorithm in finding an optimal solution for a high-level mission.The pseudo-code of the proposed algorithm is reported in Alg. 1, while more details on the theoretical framework and the techniques used are reported in the following section.In this section, we provide details about the theoretical framework illustrated in the above section, with a focus on the methodology and the implementation.</p>
<p>A. Tasks</p>
<p>The robot can execute tasks selected from a predefined dictionary D = {T 1 , T 2 , ... , T N }.For each task T , we define the task variable to be controlled as x ∈ R m and the system configuration as q ∈ R n .Let us represent the relationship between x and q with the function k:
x = k(q).(1)
Under the assumption that k is differentiable, we have:
ẋ = J(q) q = ∂k(q) ∂q q,(2)
where q and ẋ are the time derivatives of q and x respectively, while J(q) is the Jacobian associated with the task T .Given a desired trajectory x d (t) for the task variable, fulfilling a task ideally means to generate a trajectory q d (t) such that k(q d (t)) = x d (t).An effective way to generate motion references q d (t) for the robot system starting from desired values x d (t) -and possibly ẋd (t) -is to act at differential level by computing velocity references q(t) for the robot system [1].Depending on the particular type of task, we compute q(t) using one of the two following approaches.</p>
<p>• Closed loop: the reference generation for this group of tasks is based on the CLIK (Closed-Loop Inverse Kinematics) approach [1] [7] and foresees closed-loop integration of the error associated with the task with a feedback mechanism:
qd (t) = J † (q(t)) ( ẋd (t) + γ CL (x d (t) − k(q(t))))(3)
where J † = (J T J) −1 J T is the Moore-Penrose pseudoinverse and γ CL is the feedback gain.At each time instant t, the feedback error is calculated as the difference between the desired task quantity x d (t) and the current one k(q(t)).The introduction of the derivative ẋd and the dependence on time of the desired task trajectory allow us to define a trajectory that leads the system from an initial pose x 0 to the goal pose x d in the time interval [0, t s ] (trajectory time).An example of a closed-loop task is Cartesian tracking via inverse kinematics.• Open loop: these tasks do not implement the feedback mechanism.The optimization objective is to minimize or maximize the task variable, which is usually a scalar.</p>
<p>To distinguish from closed-loop tasks, we indicate the relationship between task and configuration variable with the function w.We use k(q) in closed-loop tasks to describe task variables that follow a desired trajectory or a goal, whereas w(q) is used in tasks that optimize a scalar objective, such as manipulability.Therefore, following the gradient-based approach in [33], we express the desired joint velocities as a partial derivative of w(q(t)) scaled by the gain γ OL :
qd (t) = γ CL ∂w(q(t)) ∂q(t) T .(4)
An example of an open-loop task is maximizing manipulability or distance from joint limits.More generally, we can characterize each task T with a parameter θ, which is included in the learning objective.According to this parametrization, in closed-loop tasks we have θ = γ CL , while in open-loop tasks we have θ = γ OL .</p>
<p>B. Null-Space Projection</p>
<p>Tasks can be stacked in an ordered tuple, which is immutable once the stack is deployed on the robot.From this, a task with a lower priority can iteratively be projected into the null space of another with higher priority [26] [33] using Jacobian matrices.By taking two tasks as an example, the projection is defined as:
qd = q1 + I n − J † 1 J 1 q2 . (5)
where q1 and q2 are the joint velocity commands obtained from solving the primary and secondary tasks, respectively, and J 1 is the Jacobian matrix associated with the higher priority task.</p>
<p>When a robot has to execute multiple tasks simultaneously, null space projection is a crucial tool to ensure that the secondary tasks do not interfere with the primary ones.</p>
<p>C. Cost function</p>
<p>The cost function C is a crucial aspect of this framework, as it defines the performance of different SoTs in the same scenario.This allows for embedding user preferences and enhancing the ease of use.Moreover, it is a key component of Genetic Programming processes in RL frameworks, as it serves as a basis for genetic selection and performance comparison.The cost function can be defined by the user as a weighted combination of predefined cost operators that regulate the robot's priorities while performing a certain combination of tasks.The user can combine the costs with the relative weights as:
C = α 1 cost 1 + α 2 cost 2 + ... + α n cost n ,(6)
where weights are normalized such that n i=0 α i = 1.This function is pivotal in determining the parameters to optimize during the learning phase, while taking into account user preferences.This implementation allows the cost function to be tailored to the specific requirements of the application, ensuring that the optimization process aligns with both operational objectives and safety constraints.The user is allowed to select the high-level requirements and preferences that define the cost function at the beginning of the training phase through a Graphical User Interface, as described in Sec.V-C.Therefore, once the user defines the weights for the fitness measure, C, the framework automatically evolves a SoF in a simulated environment.</p>
<p>D. Stack of Tasks</p>
<p>This work aims to automatically manage the redundancy of complex robotic systems by learning an optimized and prioritized order for a set of tasks using Genetic Programming and RL techniques, guided by a user-defined cost function that reflects the specific performance criteria relevant to the task, such as minimizing execution time or prioritizing perceived safety.In the following, the Genetic Programming framework is applied to a SoT structure.Each individual in the population of potential best solutions of our problem is represented as a stack in which the task descriptions are stored together with their parameters.The specific order in the data structure defines the task priorities, which are guaranteed through null space projection.</p>
<p>In the implementation, the first position in the stack, namely at index 0 in the array of Fig. 2, the value associated with the cost function is reported.So, all stacks carry all the necessary information to compare them (a cost function C) and execute a high-level mission (a set of parameters θ and a priority order P ).In Fig. 2, a simple example of SoT is reported.In this case, the task labeled as 1 st is fully executed with the highest priority, the task labeled as 2 nd is then projected in the null space of the first, while the 3 rd is projected in the combined null space of the two tasks with higher priority [26].Note that other tasks may be concatenated in the SoT structure.Priority order, active tasks, and parameters can be changed and combined with Genetic Programming to find the solution that minimizes the cost function.The parameters, denoted as θ, are restricted to be in a specific range dictated by prior knowledge of the task.For example, the Inverse Kinematic gain is constrained to be positive and lower than 2 to fulfill necessary stability conditions [7] for safety reasons.We introduce a biologically inspired Boolean activation mechanism for task inclusion in a prioritized control framework.This mechanism, based on introns [40], allows for the preservation of latent task modules-analogous to recessive alleles in genetics-enabling exploration without structural disruption [29].To our knowledge, this approach has not been formally applied to motion control or Stack of Tasks frameworks, and provides a novel method for retaining potentially beneficial behaviors during evolutionary optimization.Each task in the control hierarchy is associated with a Boolean activation flag that determines whether it contributes to the overall control output.When the flag is set to True, the task is considered active and participates in the computation of the desired joint velocities via its corresponding projection in the Stack of Tasks.If the flag is False, the task is skipped in the projection computation and does not influence the resulting motion, but its parameters are preserved in the genotype.These inactive tasks function as latent traits; they are not expressed in the current behavior but may become active in future generations through mutation or recombination.This mechanism enables the evolutionary algorithm to vary not only the parameters of the tasks but also the effective length and composition of the control stack, without permanently discarding potentially useful behaviors.It supports a more flexible search process by allowing solutions to retain and later reintroduce tasks that may become advantageous in different environmental contexts or task configurations.As a result, the population maintains greater behavioral diversity over time, which can reduce the risk of premature convergence and improve the robustness and generalization of the final solution.The cost is evaluated once the goal of the general highlevel mission is reached, or a maximum amount of time has expired, or a breaking point of the simulation is met (the robot collides with an obstacle, or an arm configuration reaches a singularity).Once the cost is computed, it can be used for genetic selection.Then, a genetic operation is applied to the SoT structure.In case of a mutation, a parameter, the priority order of the tasks, or the presence of a task in the stack can be randomly changed, producing a new offspring which is added to the next population.If crossover is applied instead, another survived SoT is randomly chosen as the second parent, and the offspring is then derived.Therefore, the proposed framework combines Genetic Programming and RL to find the set of priority order P and parameters θ that minimizes the cost function C:
[C, [1 st task],[2 nd task], [3 rd task], ... ]P , θ = argmin P,θ C. (7)
Each Stack of Tasks can be formally represented as a map S, which maps the priority order, parameters, and tasks T from the dictionary D to the desired joint velocity, given the cost function input by the user:
S : (P, θ, T |C) −→ qd .(8)
In summary, SoT is a structured representation in which each element of the stack corresponds to an individual task.Each task entry comprises:</p>
<p>• Priority Level: Determines the execution order, with higher-priority tasks executed first.• Control Parameters: Specific settings for task execution.</p>
<p>• Activation Flag: Indicates whether a task is active or inactive, allowing dynamic modification of the stack without losing potential task information.</p>
<p>E. Distance between Stacks of Tasks</p>
<p>Given two different SoTs a and b (of length n), the distance between them -which serves to quantify how dissimilar the two solutions are -can be computed as:
d(SoT a , SoT b ) = n tasks i=1 δ θ (θ i a , θ i b ) + δ prior (T i a , T i b ) ,(9)
for all the n tasks T i present in the list.δ θ (θ i a , θ i b ) = ∥θ i a −θ i b ∥ returns the sum of the differences between the parameters of two different realizations of the same task in different SoTs (namely T i a and T i b ) and
δ prior (T i a , T i b ) = |π i a − π i b | if T i a and</p>
<p>F. Genetic Programming as Reinforcement Learning cost minimizer</p>
<p>Genetic Programming [12] is a computational methodology inspired by the principles of natural evolution.Its primary goal is to automatically generate computer programs that can effectively solve specific tasks.Unlike Genetic Algorithms (GA) [15], which operate on genomes encoded as fixedlength strings, Genetic Programming evolves variable-length programs, allowing for more flexible and dynamic solutions.For our purposes, Genetic Programming serves as a basis for the development of RL algorithms [32,37], allowing us to minimize the fitness measure.Unlike supervised learning, where models rely on labeled datasets, RL agents refine their strategy by receiving feedback in the form of rewards in an iterative trial-and-error process.Thus, fitness evaluation is a crucial component of RL and has undergone significant improvements with the integration of Genetic Programming paradigms [25,2,11,39,21,42].The integration of Genetic Programming with RL techniques has opened new avenues for developing adaptive policies and strategies in dynamic environments [37].Genetic Programming can be employed to directly evolve policies, representing them as programs.Our framework can be interpreted as an RL approach, where a policy (SoT) is selected, executed in an environment (simulation or real robot), and evaluated via a scalar cost.However, rather than learning with gradients or value functions, we use Genetic Programming as the policy optimizer.This enables policy search over interpretable task hierarchies.In our work, we use the method as a zero-shot sim-to-real learning approach since the robot learns in simulation and does not need to perform fine-tuning on the real environment.Traditionally, Genetic Programming represents programs as tree-based structures [41,35] (as in Fig. 3 and Fig. 4), reflecting the hierarchical nature of many biological languages [14,13].Furthermore, the ability of Genetic Programming to evolve temporally extended actions enhances an agent's capacity to learn long-term strategies, a critical factor in many real-world applications.In practical applications, Genetic Programming has demonstrated its versatility and effectiveness in a wide range of domains [18,30,17].In robotics, for example, Genetic Programming is utilized to evolve control programs that enable robots to exhibit adaptive and resilient behaviors in dynamic environments [13,20].A fundamental mechanism in Genetic Programming is the application of genetic operators [12], particularly crossover and mutation after genetic selection, which drives the evolutionary process by generating variation in the population.Genetic selection allows for selecting the best algorithms to thrive in the process, while the worst are discarded.This process will be based on the fitness measure associated with the individuals.Crossover is a recombination operator that exchanges genetic material between two parent programs to create offspring.In tree-based Genetic Programming, crossover is typically implemented by selecting a random sub-tree in each parent and swapping these sub-trees to produce new individuals.This mechanism enables the exchange of functional building blocks between solutions, facilitating the discovery of novel program structures.Mathematically, given two parent programs P 1 and P 2 , the crossover operation (schematized in Fig. 3) can be expressed as:
O = crossover(P 1 , P 2 ) (11)
where the resulting offspring O inherits traits from both parents, preserving useful characteristics while potentially introducing diversity.
P 1 P 2 O Figure 3.
Example of crossover with a tree structure.From two parents P 1 and P 2 , an offspring O is generated by shuffling their characteristics.</p>
<p>Mutation, on the other hand, is an operator that introduces small, random changes to a program to enhance diversity and explore new regions of the search space, while preventing premature convergence to a local optimum.In tree-based Genetic Programming, mutation typically involves selecting a random node and replacing it with a newly generated node or sub-tree.Formally, the mutation (schematized in Fig. 4) can be defined as:
O = mutation(P 1 )(12)
where O is a variant of P 1 with an altered structure that encourages adaptation and refinement over successive generations.
P 1 O Figure 4.
Example of mutation with a tree structure.From a parent P 1 , an offspring O is generated, changing its characteristics.</p>
<p>V. EXPERIMENTAL SETUP In this section, we introduce the setup of the learning phase -where a simulated environment is used -and the test phase -which is performed with a real robot in a controlled laboratory environment.The robot model chosen for the test phase is the mobile-YuMi research platform, available at ABB Corporate Research in Västerås, Sweden.Its architecture is based on the Dual-Arm ABB YuMi, which is a bimanual manipulator with 7 DOF for each arm.The manipulator is installed on top of a mobile base, adding navigation capabilities.The base itself is treated as 3 additional DOF elements (translation along x and y axes and rotation around the z axis), which can be added to the kinematic chain or used as a single entity to move the platform.Only the mobile base is equipped with distance sensors (a pair of LiDAR scanners, one at the front and one at the rear of the mobile base).Thus, it was possible to deal with the obstacle avoidance task only in the case where the base was involved, as the arm is bereaved of distance sensors.The learning phase was conducted through repeated simulations in a Gazebo environment, while the real-world tests were carried out at ABB Corporate Research.Both the simulated and the real robots can be controlled by applying joint velocities to the arms and 3 velocities to the base -linear velocities along x and y and angular velocity around zwhich are translated by the robot software into steering and spinning velocities for the wheels.The interfaces to the real and the simulated robots were implemented in ROS2 Humble.</p>
<p>A. Proposed Tasks</p>
<p>In our experimental evaluation, the set of tasks that the robot is allowed to perform is explicitly defined in a dedicated task dictionary.This dictionary serves as a structured reference that enumerates and categorizes all permissible tasks.For clarity and analytical purposes, we divide these tasks into two main groups, each corresponding to a distinct type of interaction or objective within the evaluation framework.One includes useful tasks for the execution of a high-level mission, and the other includes tasks that do not contribute to the completion of the high-level goal and are therefore disturbing the execution.Non-relevant tasks are introduced in the framework to prove the robustness of the proposed solution, given a cost function.Both groups are made up of 4 different tasks, and all of them return a desired velocity vector for the joints qd ∈ R n .The set of useful tasks is composed of the following:</p>
<p>• Inverse Kinematic (IK) with a trajectory time t as represented in Fig. 5.In this work, the desired velocity for the inverse kinematics (IK) task is not determined solely by the pose error e IK = k d − k(q(t)) ∈ R 6 between the desired pose and the current one.Instead, a trajectory is generated by selecting a sequence of intermediate points from the initial pose to the target pose over a predefined duration (set as a task parameter).The desired velocity at each time step is then computed to drive the system toward the next point along this trajectory.Thus, at each time step, the end-effector (or the mobile base) attempts to reach a specific point of this trajectory.The input z(t) of the task is given by the desired endeffector pose k d , which is time variant as it is computed through a trajectory, and the desired Cartesian space velocities.The desired velocity of the task is given by the solution of the following equation:
qd (t) = J † (q(t)) kd (t) + γ CL (k d (t) − k(q(t))) ,(13)
where k(t) ∈ R 6 is the vector describing the pose of the end-effector at time t, k d ∈ R 6 is the desired target pose and kd its derivative, which are computed following the desired trajectory of Fig. 6.J ∈ R 6×n is the Jacobian matrix of the kinematic chain involved in the task.• Obstacle Avoidance with the trajectory described by Fig. 7. Similarly to [6], the sensors are treated like l fictitious springs with a rest length r l and an associated pseudo-energy.Once an obstacle is sensed, when the sensed distance d is smaller than the rest length of the spring, the task acts by pushing away the mobile base, targeting zero pseudo-energy.If a sensor is activated, i.e. when d ≤ r k , the pseudo-energy associated with it is
ϵ l = 1 2 (r l − d) 2
, and ϵ l = 0 otherwise.Like in the IK task, a trajectory is followed in this case as well.Because the robot uses LiDAR scanners instead of singleray range finders, the array of values is down-sampled to obtain a 180 • map (even if the range of the scanner is 270 • ), centered on the zero position (x, y) of the mobile base through a map.The values sensed for each degree were treated as a single unique range finder sensor.The angular values were constrained to the [−90 • , 90 • ] angle range, as an overlap of two different sensors in the same direction would have been interpreted by the robot as an obstacle sensed twice, leading to unstable behavior.In the case of 0 • and 180 • , where both sensors should be active, the frontal sensor has priority over the rear sensor, which is turned off.This does not compromise the efficacy of the task since the sensed obstacle is the same, but guarantees stability in the solution.With this solution, the robot can sense obstacles all around the base, downsampling the original 270 • scanner range in a 180 • one.For this task, the desired velocity is given by: qd (t) = J † o (q(t)) ( σd (t) + γ CL (σ d (t) − σ(q(t)))) , (14) where σ(q(t)) ∈ R l is the global pseudo-energy in the current state and it is given by the sum of the pseudoenergies ϵ l (q(t)) associated to all the m sensors, namely σ(q(t)) = m l=1 ϵ l (q(t)).J o (q(t)) is the Jacobian matrix associated with this task and is computed with pseudo-energy derivatives [6].Unlike the Jacobian J, which is always non-empty since the arm has a mapping between Cartesian and operational velocities at any time, the Jacobian J o ∈ R l×n for this task may be empty.If there are no obstacles closer than d to the robot, the matrix does not prevent subsequent tasks from being fully executed.As both sensors are active during the execution of the task and are treated as rangefinder sensors for each degree of the half-circumference that they describe, l = 181 + 179 = 360 in this specific case.Thus, the sensors describe a full circumference around the robot.The input z(t) of this task is given by the useful samples of the sensors, the target pseudo-energy, and its derivative.As for the IK case, the target value and the desired velocity are time-dependent since they are computed as a trajectory of consecutive points.The solution of the desired velocity is given by the following equation:
qd (t) = γ OL ∂w(q(t)) ∂q(t) T ,(15)
where w(q(t)) is the manipulability measure defined as w(q(t)) = det(J(q(t))J T (q(t))), (16) where J(q(t)) is the Jacobian matrix of the kinematic chain involved in the task as a function of the actual joints configuration.The derivative of the manipulability can be analytically computed as [10]:
∂w(q) ∂q = w • tr (JJ T ) −1 ∂J ∂q J T , (17)
where the partial derivatives of the Jacobian were analytically calculated to speed up the computation.Thus, the input z(t) of this task is simply given by the manipulability measure, which is a scalar value.• Maximization of distances from Mechanical Joint Limits (M.J.L.) as shown in Fig. 8.This task moves each joint as far as possible from its mechanical limits, expressed as the minimum and maximum angles.As a consequence, the joints tend to move toward their mean position.It is important to note that it is guaranteed that if a point of minimum is reached, it is the global one and not a local one.The optimization process used in other tasks, for example, in the maximization of the Manipulability, relies on gradient-based methods.These methods adjust the joint angles incrementally, following the direction that most improves manipulability at each step.In that way, the manipulability maximization method would not explore the entire configuration space, as in this latter case, considering nearby configurations only.As a result, the algorithm stops if it reaches a point where no small change leads to further improvement.Those points are local minima, namely the best solution within a small neighborhood, but not necessarily the best possible solution overall (global minimum).Finding the global minimum would require a more exhaustive search of the full configuration space, which is often computationally infeasible for high-DOF systems, like redundant robotic arms.Thus, the solution of those tasks and eventually the cost associated with them are strictly dependent on the initial configuration of the arm.</p>
<p>On the other hand, in the maximization of distances from the M.J.L. case, the maximum value achieved by the task, i.e., the minimum cost, is not dependent on the initial configuration, and it is a global point.This happens because, unlike manipulability, which depends on complex nonlinear interactions between joint angles and the end-effector movement, the distance from mechanical joint limits is defined by a convex function.Each joint's distance to its limits can be calculated independently, and the overall cost function has a well-defined, smooth structure.This makes the optimization convex and thus easier to solve for global solutions.The desired velocities are computed as in the previous task (Eq.15) where
w(q(t)) = − 1 2n n i=1 q i (t) − q q imax − q imin 2 (18)
is the measure of the distance from mechanical joint limits.q imax and q imin are, respectively, the maximum and minimum mechanical limits of the i th joint for a kinematic chain of n elements.In this case, as well, the analytical expression of the derivative was exploited to calculate the desired joint velocities: The input u(t) of this task is given by the measure w(t) and the constant values q max and q min .
∂w(q) ∂q = − 1 n n i=1 q i − q q imax − q imin . (19)
All functions that solve the tasks return the desired velocity, a Jacobian matrix used to apply the null space projection [26], and all useful parameters to compute the cost function or study the behavior of the execution.Except for the obstacle avoidance task, all other tasks were implemented as described in [33].</p>
<p>The distracting tasks of the second group are designed to move the arm or the base without achieving a meaningful objective.Two of them are specific for the base, performing a circular trajectory or rotating around the initial pose along the z axis.</p>
<p>The other two control an arm, describing a circumference with the end effector or moving the joints independently from each other, with a trajectory that oscillates from one mechanical bound to the other.The non-relevant tasks can return different types of Jacobians that can be selected.It is possible to have a Jacobian matrix that prevents subsequent tasks from being executed completely or partially, or random Jacobians that lead to unpredictable behavior in the projection phase.</p>
<p>B. Proposed costs</p>
<p>The cost set used in the experimental setup is composed of the following:</p>
<p>• precision, which is given by the squared norm of the pose error
cost = ||k d − k|| 2 . (20)
This can be divided into position precision and orientation precision.</p>
<p>• safety, which can be computed with respect to all distances lower than the rest length, applying pseudoenergies as in the Obstacle Avoidance task, r k ,
cost = m l=1 1 2 (d − r l ) 2 , if d i &lt; r l 0, otherwise(21)
or with respect to the minimum length
cost = 1 2 (d − r min ) 2 , if d &lt; r min 0, otherwise . (22)
• maximization of the manipulability, which is given by cost = 1 w 2 max.manip (23) where w max.manip is the manipulability measure, which increases as the arm moves further from singularities.</p>
<p>• maximization of distance from mechanical joint limits, which is given by cost = w 2 M.J.L. (24) where w M.J.L. is the measure of distance from mechanical joint limits, which goes to zero as the joint angles reach their mean values.• time, which is given by
cost = t 2 (25)
where t is the simulation time from the beginning of the task to the end.Using t 2 increases the cost more sharply for longer durations, which penalizes slow executions and encourages faster task completion -a common approach in optimization when efficiency is prioritized.In the experiments, we normalize the costs to make them comparable and combine them as in Eq.IV-C.</p>
<p>C. Graphical user Interface G.U.I.</p>
<p>To facilitate ease-of-use and broaden accessibility, a dedicated Graphical User Interface (GUI) was developed as part of the system.The GUI is designed to provide an intuitive environment, allowing users to interact with the application without requiring deep technical expertise or command-line interaction.By abstracting complex operations into simple visual components such as buttons, drop-down menus, and input fields, the GUI significantly streamlines the workflow.The GUI reported in Fig. 9 allows users to configure key parameters for the simulation without requiring direct code manipulation.It is divided into two main sections: cost parameters and simulation settings.In the cost parameters section, users can assign relative weights to various evaluation criteria, including, for example, Accuracy and Safety.Each criterion has an associated numeric input field where the user can Figure 9. Graphical User Interface for configuring learning simulation parameters.The interface allows users to define the relative weights of cost criteria (Accuracy, Safety, Manipulability, Speed) and set simulation parameters including population size, number of iterations, and simulation duration.Designed for ease of use, it supports intuitive navigation with dedicated controls for starting or exiting the simulation.specify its weight, thereby customizing the learning objectives based on their priorities.Radio buttons indicate which cost criteria are active, guiding user attention and parameter selection.The simulation settings section enables users to define the structural aspects of the simulation.Users can set the number of stacks in the initial population, the number of iterations, and simulation time (in seconds) using clearly labeled input fields.The interface is minimalistic and designed with clarity and functionality in mind, facilitating the configuration of experiments.</p>
<p>D. Gazebo Simulation &amp; Learning</p>
<p>We developed a framework in which tasks are simulated under controlled conditions, allowing us to analyze their behavior and correlations.Each task is associated with a set of learned parameters -such as estimated duration, success probability, resource consumption, and contextual constraints -which are used to compute a priority order based on a configurable cost function.Through repeated simulations, the framework learns not only the characteristics of individual tasks but also how they contribute to the overall mission efficiency.This approach enables generalization across different realworld environments.Since the priority order is derived from task parameters and their role in fulfilling a given high-level mission -rather than from specific environmental features -it remains valid in any setting where the same mission and cost function apply.This decoupling of task logic from environmental details makes the framework robust, flexible, and applicable to a wide range of deployment scenarios.For the simulation, we used a Gazebo model of the laboratory facility at ABB Corporate Research.Both the environment and the robot model (Fig. 10 on the left) were provided by WARA Robotics 1 .The robot control functions and the Genetic Programming algorithm were developed in Python.For each different cost function, the learning phase is initialized with a maximum number of generations -depending on the learning complexity -and the number of individuals in 1 https://wara-robotics.se/the population.Then, each individual is a randomly generated SoT.The initial population can be created with random priority order, random parameters, or both, depending on the scope.All the parameters are constrained to be in a range defined with prior knowledge on the tasks in order to maintain the stability of the model and have reasonable results.Then, the Genetic Programming algorithm evolves the individuals until a stability point is reached, namely an iteration in which all the survived SoT have small differences in cost.At the end of each learning process, only the best SoT (i.e., the one with the lowest cost in the final population) is considered to be the final result of the algorithm, and it is selected to be tested.During the execution, if two competing SoTs have the same associated cost, only one is randomly chosen to survive.In this way, we make sure to keep a constant number of individuals in the population for each generation.In a first phase, the task parameters were fixed, and only the order of the tasks was allowed to be changed by the genetic operations.In this way, the priority order of the architecture can be learned.Then, the priority order is fixed, and the parameters are allowed to change in a range constrained by boundaries dictated by previous knowledge on the tasks.For example, the gain used in the definition of the desired velocities qd for the Inverse Kinematic is restricted to assume values in the range γ CL = (0, 2] [7, 3] because outside this range there are no stability guarantees for that task.During the learning phase, the initial position of the arms can be randomly initialized, with the constraint of keeping the endeffector in an area that avoids collisions at the beginning of the simulation.Moreover, velocities applied to the arms are reduced towards position limits to not to overcome them and risk damaging the robot:
ϵ(q imin − q i ) ≤ qi ≤ ϵ(q imax − q i )(26)
for each i th joint, with q imin its minimum allowed position and q imax its maximum.Both static and dynamically changing environments were developed in order to allow the robot to react to unpredictable changes in the environment.In the second case, a moving object was introduced in the Gazebo environment to simulate the presence of an operator close to the robot to test the safety measures of the algorithm.Finally, non-relevant tasks are introduced in the dictionary of possible tasks to show the resilience of the algorithm to superfluous and distracting tasks with respect to its main goal.Even if it is not defined by the user, a high cost is associated with a collision of the robot, and it is always included in case it happens.</p>
<p>Since the LiDAR scanners are mounted away from the outer edge of the mobile base and positioned more centrally, a variable offset is applied to the measured distances.This correction accounts for the physical displacement between the scanner's position and the chassis, providing an accurate estimate of the distance from obstacles to the actual body of the robot.If this distance is lower than a certain threshold, the robot is considered to be damaged due to a collision, and the cost increases accordingly.</p>
<p>E. Laboratory tests</p>
<p>Each stack for the specific cost functions was tested in a controlled laboratory environment, demonstrating that the proposed framework allows us to transfer the learned solution from the simulated environment to the real world.The robot is controlled through the ROS2 interface, feeding it with the desired joint velocities for the arms, and the desired linear [x, y] and angular z velocities for the mobile base.The tests were conducted in the following settings:</p>
<p>• mobile base platform in a static environment,</p>
<p>• mobile base platform in a dynamically changing environment, • one arm in a static environment,</p>
<p>• mobile base platform and one arm in a static environment,</p>
<p>• mobile base platform and one arm in a dynamically changing environment.To create a dynamically changing environment, a person was allowed to walk close to the robot while executing the task.Due to safety guarantees, the robot could always be stopped by the user while executing a task in a real-world scenario.</p>
<p>VI. EXPERIMENTAL RESULTS</p>
<p>In this section, the results obtained during the learning and in subsequent laboratory tests are reported, evaluating the performance and validity of the proposed methodology.</p>
<p>A. Learning of the Stack of Tasks</p>
<p>The learning phase was divided into two parts: in the first one, we learn the priority order of the task, and in the second, we learn the parameters.</p>
<p>1) Priority order Learning: In this phase, parameters were fixed with values that do not minimize the cost function given by the user, but they guarantee stability and the execution of all tasks.In the case where only the mobile base is used, the problem is simpler as only two different tasks are relevant, as the maximization of the manipulability and the distance from mechanical joint limits are manipulator-specific.During a simulation, in case of collision with an obstacle, the cost is high, and so the robot prioritizes the Obstacle Avoidance task with respect to the Inverse Kinematics, independently from the global mission, in a few generations (2 or 3, depending on the initial distribution).On the other hand, when the number of tasks in the stack increases, the algorithm requires more iterations to reach the final priority order (Fig. 11).Finally, the final learned order is given by:</p>
<p>• 1 st Obstacle Avoidance • 2 nd Inverse Kinematic • 3 rd Maximization of the Manipulability measure • 4 th Maximization of distance from M.J.L.At times, the 3 rd and 4 th positions could be switched according to the specific cost function.Nevertheless, this configuration helps to avoid singularities, making it a suitable option for a general case.Furthermore, this order can be used as an initial benchmark for the following phase, allowing the tasks to switch their positions and adapt to any cost function while learning parameters.In this way, since the order is already almost learned, the learning of the parameters would be faster.</p>
<p>2) Parameters Learning: Once the initial priority order is fixed, it is possible to let the parameters assume any value in the prior defined range.In this way, the algorithm is allowed to exploit as many combinations as possible, to find a minimum of the cost function.During this phase, the algorithm may take different generations to reach a minimum point, depending on the initial set of SoTs.On the basis of this, the solution derived at the end of the process may be different for the same task.All simulations were stopped once a stationary behavior was reached.In this case, it is not possible to find a general solution that may be adapted to all the cost functions, as was done for the priority order.The parameters of a task are heavily dependent on the cost function defined by the user in terms of desired performance.To show how the adopted solution works, we propose the following two examples.First, a cost function C 1 that focuses on some user requirements was decided: Then, starting from the derived initial order, a random population was created.After that, the algorithm runs, and a stationary point is found after 7 generations.With this cost function, the robot is encouraged to stay as far away from obstacles as possible while trying to reach the desired pose and maintaining a high manipulability.At the end of the computation, the order of the SoT remains the same as the previously found one.All the tasks involved in the fitness measure are still active, while the last one (i.e., the maximization of distances from M.J.L.) is labeled as inactive, being not relevant for the execution of the high-level mission.The purpose of the robot is to reach a specific pose in the environment, avoiding dynamic and static obstacles.In this case, the learning was conducted in a dynamic environment.
C 1 = 0.
In Fig. 12, the best SoT -chosen by the optimization algorithm -is reported.It is possible to observe how the Obstacle Avoidance task is in first position, as in the case of collision, the price paid by the robot is higher than not reaching the desired pose.Note that the order and the parameters of the stack reported in Fig. 12 are consistent with the cost function described by the user.The higher priority task is Obstacle Avoidance, since the robot wants to avoid collisions and stay far from  obstacles.Since the environment is dynamic, the time that is necessary to avoid an obstacle is quite low, and the gain is high enough to guarantee a fast response.Then, Inverse Kinematics is prioritized.In this case, the gain is high enough to guarantee convergence without instability, while the trajectory time is not relevant.Maximization of the Manipulability task is after the Inverse Kinematics one, allowing a maximization of the measure without compromising the desired pose.Also in this case, the gain allows the robot to reach the objective while maintaining stability.Maximization of the distance from Mechanical Joint Limits is in the last position, and even inactive, since it is not relevant in the minimization of the cost function.To make a comparison, a second cost function can be defined:
C 2 = 0.5 • c prec. + 0.5 • c time .(28)
In this case, the robot is not required to satisfy distances from obstacles but only to be precise and fast to achieve this result.The resulting SoT from the learning phase, obtained after 6 generations, is reported in Fig. 13.Also in this case, the order is the same as before.Despite not being strictly specified by the user, the robot still avoids obstacles with maximum priority to avoid collisions.In fact, in the event of a collision, the robot is considered to be broken, and the assigned cost is high.In the case of the Inverse Kinematic task, the gain is slightly higher, and the trajectory time is shorter, so the end-effector converges faster to the desired pose.Note that the two maximization tasks are not relevant in the cost function, and then one is not active, while the other one has a low gain, so its involvement is quite weak in the solution of the high-level mission.</p>
<p>3) Robustness to non-relevant tasks present in the dictionary tasks: In this phase, up to 4 non-relevant tasks present in the dictionary tasks were added to the possible SoT choices, in random positions, to show the resilience of the algorithm to the noise.These tasks prevent the robot from achieving its final goal if they have a priority order high enough to block other useful tasks.Thus, the objective of the algorithm is to make them inactive or move them to the last positions in the SoT.We can state that the result is always reached, namely that the non-relevant tasks are always rejected, but the performance of the algorithm in doing this during the  learning phase changes.The number of generations in which this is achieved is heavily dependent on the initial random configuration or on the random nature of the crossover and mutation operations.If there is a high percentage of individuals in the population who feature non-relevant tasks in the first positions, the convergence will be slower.For example, a case for a 10-DOF kinematic chain is reported in which the nonrelevant task starts at the first priority position.As can be seen in Fig. 14, the task was rejected and moved to the last position.Note that 5 th is the last possible index.The curve obtained does not show a specific behavior, it is only a non-decreasing function, since all the improvements are subject to random choices of the algorithm.</p>
<p>B. Laboratory tests of the learned stacks of tasks</p>
<p>All the SoTs obtained during the learning phase were tested on the robot in a laboratory environment, with the appropriate safety measures.For this scope, the mobile base velocity was restricted to the range [−0.2, 0.2] m/s for linear velocities and [−0.2, 0.2] rad/s for angular velocity.Moreover, joint velocities were restricted to be in the range [−1, 1] rad/s.This choice clearly prevents the robot from satisfying a certain trajectory in time if the specified time is too low.Because of that, all trajectory times during the test phase were scaled with a fixed offset.This choice does not affect the validity of our proposed framework.</p>
<p>For the experimental validation, a static environment with fixed obstacles was tested first.Then, a person was allowed to walk close to the robot (Fig. 16), testing the performances in a dynamic environment and simulating a collaboration case between the operator and the robot.As expected (as the Gazebo simulation of the mobile platform and dual-arm YuMi robot is sufficiently accurate), the learned SoTs also work in a real laboratory environment.Anyhow, due to some inaccuracies in the simulation, the results are slightly different but still consistent.The resulting SoT was also tested under some different conditions, such as a different initial configuration, different targets, or different obstacles.The result always satisfied the requirements, achieving the best performance in terms of the cost function.The results of our lab experiments confirm that the task stack learned entirely in simulation can be successfully applied in a real-world environment.Despite variations in environmental details, the robot was able to execute the same high-level mission effectively, guided solely by the automatically derived task priorities.This outcome validates the core premise of our approach: that a simulation-based learning process, driven by task parameters and a cost function, produces control strategies that are robust and generalizable.By decoupling task logic from environment-specific constraints, the framework enables adaptive behavior across deployment scenarios, significantly reducing engineering effort -being the SoT learned automatically once the cost function is designed -and making robotic systems more scalable and reusable.However, some limitations arise during tests with a real machine.First, because of the noise and delay measurements from the sensors, the response to the introduction of a dynamic obstacle in the environment was not as fast as in the simulations.Another issue that emerged during simulations is the lack of a global planner for the obstacle avoidance task.Without it, the robot cannot navigate around obstacles intelligently and instead gets pushed as far away from them as possible -often moving unnecessarily far from its intended target.In Fig. 15, some results obtained during tests, with the two costs defined in previous sections, are reported.</p>
<p>An important point concerns the comparison with existing approaches.To the best of our knowledge, a direct quantitative comparison with prior SoT methods is not feasible, as existing works rely on manually defined, hard-coded task hierarchies tailored to specific scenarios.Nonetheless, the task priorities autonomously learned by our method are qualitatively aligned with those manually defined and adopted in classical Velocities applied to the joints Velocities applied to the joints over Time q 1 rad/s q 2 rad/s q 3 rad/s q 4 rad/s q 5 rad/s q 6 rad/s q 7 rad/s Velocities applied to the joints Velocities applied to the joints over Time base q 1 m/s base q 2 m/s base q 3 rad/s Velocities applied to the joints Velocities applied to the joints over Time q 1 rad/s q 2 rad/s q 3 rad/s q 4 rad/s q 5 rad/s q 6 rad/s q 7 rad/s From left to right, we reported the cost function C, velocities q applied to the arm joints and to the base, and the error of the pose for Inverse Kinematics.Note that, in the case of C 1 , since the minimum distance sensed by the LiDARs is included in the cost function, the fitness measure has some steps.These may occur when an object enters the range of the sensors or exits, updating the measure.Moreover, in this case, the cost reaches the minimum cost during execution.In the case of C 2 , the distance with respect to objects in the environment is not included, but the robot does not collide in any case since the task is active.In this case, the fitness measure always increases due to the square term in a function of time.For this eventuality, it is crucial to consider a task as completed and stop the time if the target is reached up to a threshold.In fact, after t = 10 seconds, it is possible to see the quadratic behavior of the time cost function, since the precision cost is almost zero.</p>
<p>frameworks for similar robotic missions, such as in [6].This coherence with expert-designed strategies supports the practical validity of our learned configurations, despite their fully automated derivation.</p>
<p>VII. CONCLUSION AND FUTURE WORKS</p>
<p>In this work, we presented, to the best of our knowledge, the first approach that automatically learns a full Stack of Tasks (SoTs) -including task priorities, control parameters, and activation logic -directly from a cost function defined by intuitive user preferences.Our method combines Genetic Programming and Reinforcement Learning to evolve interpretable and adaptive task execution hierarchies for redundant robotic systems.</p>
<p>We demonstrated that Genetic Programming is an effective framework for optimizing both task order and control gains in response to a high-level objective function.This integration enables the robot to manage multiple tasks dynamically, prioritizing safety-critical behaviors such as obstacle avoidance while maintaining overall precision and performance.</p>
<p>Experimental validation -carried out in both simulation (Gazebo) and on a real dual-arm mobile robot (mobile-YuMi) -confirms that the learned SoTs generalize across varying and unpredictable conditions without requiring manual tuning.Importantly, our framework supports zero-shot simto-real transfer: the SoTs learned entirely in simulation were successfully deployed on the real system without fine-tuning.This decoupling between environment-specific modeling and control strategy marks a key advancement toward scalable, robust robot autonomy.Despite these promising results, several challenges remain.The simulation environment, while instrumental for safe and rapid learning, is constrained by execution speed and requires manual setup, limiting full automation.Bridging the simulation-to-reality gap remains an open issue, with sources of discrepancy including sensor noise, communication delays, and the lack of a global obstacle avoidance planner.Moreover, while the cost function effectively guides task learning in simple scenarios, complex task interactions and parameter dependencies merit deeper exploration.Future work will focus on: (i) designing richer cost models, (ii) integrating global planning layers [22] for more robust navigation, (iii) extending the framework to multirobot coordination, and (iv) investigating adaptive, real-time tuning of control parameters based on sensory feedback.We also plan to explore more sophisticated evolutionary strategies and alternative genetic operators to accelerate convergence.Ultimately, we anticipate that advances in Genetic Programming and RL will significantly enhance the generality, robustness, and scalability of task execution architectures for redundant robotic systems operating in complex real-world environments.</p>
<p>Figure 1 .
1
Figure 1.Overall schematic representation of our framework.RL serves as the basis for Genetic Programming to develop a solution for the given problem.First, all the Stack of Tasks are executed, and a cost function is assigned through RL.Then, Genetic Programming evolves new potential solutions, which return a lower cost function.The process is iterated until a stationary point is reached, and then the best Stack of Tasks is found.Finally, the evolved SoT strategies were tested on a real mobile dual-arm robot (mobile YuMi produced by ABB).</p>
<p>Figure 2 .
2
Figure 2. Stack of Task (SoT) representation.The cost associated with the SoT execution is placed at the beginning of the representation, followed by the tasks ordered according to their priority.</p>
<p>T i b are both active n otherwise ( 10 )
10
where π i a and π i b are the priority positions of task T i a and T i b in the set of active tasks for each SoT.</p>
<p>Figure 5 .
5
Figure 5. Inverse Kinematic sequence of the mobile-YuMi research platform.</p>
<p>Figure 6 .
6
Figure 6.Plot of an example of the desired Inverse Kinematic position computed as trajectory (left) and the desired linear velocities (right), with trajectory time 10s.</p>
<p>Figure 7 .
7
Figure 7. Obstacle Avoidance sequence of the mobile-YuMi research platform.The robot moves to a position behind the obstacle, finding a path that allows it to be far from it.Blue arrows show the behavior that the base would adopt in case of the IK task only, red ones show the overall control applied, including the obstacle avoidance task.Note that without the OA task, the robot would have hit the obstacle.•Maximization of the Manipulability measure.With this task, the robot can maximize the manipulability of an arm by moving away from singularities.The solution of the desired velocity is given by the following equation:</p>
<p>Figure 8 .
8
Figure 8. Maximization of the Distances from Mechanical Joint Limits sequence of the mobile-YuMi.</p>
<p>Figure 10 .
10
Figure 10.Gazebo simulation of the model (left), real robot (right).</p>
<p>Figure 11 .
11
Figure 11.Example with the result of the % of individuals in a generation with the same order as the final solution.The base case is reported on the left, in which the order is [OA, IK].On the right, the result obtained with the right arm + base, in which the order is [OA, IK, Max.Manip., Max MJL].</p>
<p>4 • c prec.+ 0.5 • c min.dist.+ 0.1 • c max.manip. .(27)</p>
<p>1</p>
<p>st OA (True) r k = 0.102, γ CL = 1.726, t traj.= 1.385 2 nd IK (True) γ CL = 1.255, t traj.= 3.609 3 rd Max.Manip.(True) γ OL = 35.307 4 th Max.dist.M.J.L. (False) γ OL = 15.001</p>
<p>Figure 12 .
12
Figure 12.Example of a learned Stack of Tasks with cost function C 1 = 0.4 • cprec.+ 0.5 • c min.dist.+ 0.1 • c max.manip. .</p>
<p>1</p>
<p>st OA (True) r k = 0.477, γ CL = 1.257, t traj.= 1.030 2 nd IK (True) γ CL = 1.623, t traj.= 2.641 3 rd Max.Manip.(True) γ OL = 8.628 4 th Max.dist.M.J.L. (False) γ OL = 51.074</p>
<p>Figure 13 .
13
Figure 13.Example of a learned Stack of Tasks with cost function C 2 = 0.5 • cprec.+ 0.5 • c time .</p>
<p>Figure 14 .
14
Figure 14.Minimum priority order index for a non-relevant task.</p>
<p>Figure 16 .
16
Figure16.The mobile-YuMi research platform moving away from a human operator while navigating in the environment.On the left, it reaches a desired pose in the laboratory, and then it moves away, avoiding collision with the user as shown in the right picture, prioritizing safety at the cost of missing its target as learned in simulations.</p>
<p>Figure 15 .
15
Figure15.In this picture, data obtained in laboratory tests with the mobile-YuMi research platform are reported for C 1 (above) and C 2 (below).From left to right, we reported the cost function C, velocities q applied to the arm joints and to the base, and the error of the pose for Inverse Kinematics.Note that, in the case of C 1 , since the minimum distance sensed by the LiDARs is included in the cost function, the fitness measure has some steps.These may occur when an object enters the range of the sensors or exits, updating the measure.Moreover, in this case, the cost reaches the minimum cost during execution.In the case of C 2 , the distance with respect to objects in the environment is not included, but the robot does not collide in any case since the task is active.In this case, the fitness measure always increases due to the square term in a function of time.For this eventuality, it is crucial to consider a task as completed and stop the time if the target is reached up to a threshold.In fact, after t = 10 seconds, it is possible to see the quadratic behavior of the time cost function, since the precision cost is almost zero.</p>
<p>Laboratory Tests Test of the best Stack of Tasks
New SoTsBest SoTis selectedFramework Validationwith laboratory testsAfter fitness eval. Genetic Programming is applied</p>
<p>SoT ′ end if if gen op is mutation then Make a change in SoT and get SoT ′ end if new pop ← SoT ′ end for Clear pop pop ← new pop end while The best SoT in pop is selected as solution of the problem IV.THEORETICAL FRAMEWORK DETAILS</p>
<p>Algorithm 1 Pseudo-code of the proposed solution The user defines a cost function C A random initial population of SoT pop is created while a stop condition is not reached do Execute all the tasks without assigned cost C in pop Couple the SoT s and apply Genetic Selection for each survived SoT do new pop ← SoT Select a genetic operation gen op from subset {crossover, mutation} if gen op is crossover then Select a second survived SoT Merge the two SoT s in a new individual</p>
<p>These experiments were carried out in the WASP Research Arena (WARA)-Robotics, hosted by ABB Corporate Research Center in Västerås, Sweden and financially supported by the Wallenberg AI, Autonomous Systems, and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.Project co-funded by the European Union -Next Generation Eu -under the National Recovery and Resilience Plan (NRRP), Mission 4 Component 2, Investment 3.3 -Decree no.630 (24 th April 2024) of Italian Ministry of University and Research; Concession Decree no.1956 del 05 th December 2024 adopted by the Italian Ministry of University and Research, CUP D93D24000270003, within the national PhD Programme in Autonomous Systems (XL cycle).
The null-space-based behavioral control for autonomous robotic systems. Gianluca Antonelli, Filippo Arrichiello, Stefano Chiaverini, Intelligent Service Robotics. 112008</p>
<p>Evolutionary Approaches to Expensive Optimisation. Maumita Bhattacharya, International Journal of Advanced Research in Artificial Intelligence. 2Mar. 2013</p>
<p>Stability Analysis of a Hierarchical Architecture for Discrete-Time Sensor-Based Control of Robotic Systems. Magnus Bjerkeng, IEEE Transactions on Robotics. 302014</p>
<p>Merging Global and Local Planners: Real-Time Replanning Algorithm of Redundant Robots Within a Task-Priority Framework. Paolo Di Lillo, Daniele Vito, Gianluca Antonelli, IEEE Transactions on Automation Science and Engineering. Jan. 2022</p>
<p>A Stack-of-Tasks Approach Combined With Behavior Trees: A New Framework for Robot Control. David Dominguez, IEEE Robotics and Automation Letters PP. Oct. 2022</p>
<p>Low-level flexible planning for mobile manipulators: A distributed perception approach. Pietro Falco, Ciro Natale, Advanced Robotics. 28Oct. 2014</p>
<p>On the Stability of Closed-Loop Inverse Kinematics Algorithms for Redundant Robots. Pietro Falco, Ciro Natale, Robotics, IEEE Transactions on 27. Sept. 2011</p>
<p>A Review of Tournament Selection in Genetic Programming. Yongsheng Fang, Jun Li, Oct. 2010</p>
<p>A General Framework for Hierarchical Redundancy Resolution Under Arbitrary Constraints. Mario Fiore, IEEE Transactions on Robotics. June 2023</p>
<p>Maximising Manipulability During Resolved-Rate Motion Control. Jesse Haviland, Peter Corke, Feb. 2020</p>
<p>On Using Surrogates with Genetic Programming. Torsten Hildebrandt, Jürgen Branke, Evolutionary Computation. 233Sept. 2015</p>
<p>A Genetic Programming Tutorial. John I , Riccardo Poli, June 2003</p>
<p>A Framework for Learning Behavior Trees in Collaborative Robotic Applications. Matteo Iovino, 2023 IEEE 19th International Conference on Automation Science and Engineering. 2023</p>
<p>Learning Behavior Trees with Genetic Programming in Unpredictable Environments. Matteo Iovino, 2021 IEEE International Conference on Robotics and Automation (ICRA). 2021</p>
<p>A review on genetic algorithm: past, present, and future. Sourabh Katoch, Sumit Chauhan, Vijay Chahar, Multimedia Tools and Applications. 80Oct. 2020</p>
<p>Formulating Intuitive Stack-of-Tasks with Visuo-Tactile Perception for Collaborative Human-Robot Fine Manipulation. Sunny Katyara, Mar. 2021</p>
<p>A recent survey on the applications of genetic programming in image processing. Asifullah Khan, Computational Intelligence. 37June 2021</p>
<p>A Comprehensive Survey of Genetic Programming Applications in Modern Biological Research. Mohammad Wahab, Khan , Medinformatics. Dec. 2024</p>
<p>Reinforcement Learning in Robotics: A Survey. Jens Kober, J Bagnell, Jan Peters, The International Journal of Robotics Research. 32Sept. 2013</p>
<p>Automatic Robot Task Planning by Integrating Large Language Model with Genetic Programming. Azizjon Kobilov, Jianglin Lan, Feb. 2025</p>
<p>Evolving Simple Symbolic Regression Models by Multi-Objective Genetic Programming. Michael Kommenda, Genetic Programming Theory and Practice XIII. Rick Riolo, Springer International Publishing2016</p>
<p>The Marathon 2: A Navigation System. Steve Macenski, Feb. 2020</p>
<p>A Unified Approach to Integrate Unilateral Constraints in the Stack of Tasks. Nicolas Mansard, Oussama Khatib, Abderrahmane Kheddar, IEEE Transactions on Robotics. 252009</p>
<p>A versatile Generalized Inverted Kinematics implementation for collaborative working humanoid robots: The Stack Of Tasks. Nicolas Mansard, July 2009</p>
<p>A Hybrid Neural Network-Genetic Programming Intelligent Control Approach. Francesco Marchetti, Edmondo Minisci, Nov. 2020</p>
<p>Set-Based Tasks within the Singularity-Robust Multiple Task-Priority Inverse Kinematics Framework: General Formulation, Stability Analysis, and Experimental Results. Signe Moe, Frontiers in Robotics and AI. 3Apr. 2016</p>
<p>Application of genetic algorithms to point-to-point motion of redundant manipulators. A C Nearchou, N A Aspragathos, Mechanism and Machine Theory. 311996</p>
<p>Solving the inverse kinematics problem of redundant robots operating in complex environments via a modified genetic algorithm. Andreas Nearchou, Mechanism and Machine Theory. 33Apr. 1998</p>
<p>Explicitly Defined Introns and Destructive Crossover in Genetic Programming. Peter Nordin, Frank Francone, July 1995</p>
<p>Genetic Programming (GP): An Introduction and Practical Application. Arman Oliazadeh, July 2022</p>
<p>Dynamic motion capture and edition using a stack of tasks. Oscar E Ramos, 11th IEEE-RAS International Conference on Humanoid Robots. 2011. 2011</p>
<p>Reinforcement learning algorithms: A brief survey. Ashish Kumar Shakya, Gopinatha Pillai, Sohom Chakrabarty, Expert Systems with Applications. 2311204952023</p>
<p>Robotics: Modelling, Planning and Control. 1st. Bruno Siciliano, 2008Springer Publishing CompanyIncorporated</p>
<p>Evolving simple software agents: comparing genetic algorithm and genetic programming performance. M C Sinclair, S H Shami, Second International Conference On Genetic Algorithms In Engineering Systems: Innovations And Applications. 1997</p>
<p>A Study on Graph Representations for Genetic Programming. Léo Sotto, June 2020</p>
<p>Pick-and-place in dynamic environments with a mobile dual-arm robot equipped with distributed distance sensors. Sotiris Stavridis, Pietro Falco, Zoe Doulgeri, July 2021</p>
<p>Reinforcement Learning: An Introduction. Second. Richard S Sutton, Andrew G Barto, 2018The MIT Press</p>
<p>Towards Autonomous Reinforcement Learning for Real-World Robotic Manipulation with Large Language Models. Niccolò Turcato, Mar. 2025</p>
<p>Fitness Approximation through Machine Learning. Itai Tzruia, Sept. 2023</p>
<p>The benefits of computing with introns. Mark Wineberg, Franz Oppacher, Proceedings of the 1st Annual Conference on Genetic Programming. the 1st Annual Conference on Genetic ProgrammingStanford, CaliforniaMIT Press1996</p>
<p>Survey on Genetic Programming and Machine Learning Techniques for Heuristic Design in Job Shop Scheduling. Fangfang Zhang, IEEE Transactions on Evolutionary Computation PP. Jan. 2023</p>
<p>Multi-strategy competitive-cooperative coevolutionary algorithm and its application. Xiangbing Zhou, Information Sciences. 635Mar. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
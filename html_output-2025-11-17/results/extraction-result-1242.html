<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1242 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1242</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1242</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-257496038</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2303.07109v1.pdf" target="_blank">Transformer-based World Models Are Happy With 100k Interactions</a></p>
                <p><strong>Paper Abstract:</strong> Deep neural networks have been successful in many reinforcement learning settings. However, compared to human learners they are overly data hungry. To build a sample-efficient world model, we apply a transformer to real-world episodes in an autoregressive manner: not only the compact latent states and the taken actions but also the experienced or predicted rewards are fed into the transformer, so that it can attend flexibly to all three modalities at different time steps. The transformer allows our world model to access previous states directly, instead of viewing them through a compressed recurrent state. By utilizing the Transformer-XL architecture, it is able to learn long-term dependencies while staying computationally efficient. Our transformer-based world model (TWM) generates meaningful, new experience, which is used to train a policy that outperforms previous model-free and model-based reinforcement learning algorithms on the Atari 100k benchmark.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1242.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1242.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TWM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-based World Model (TWM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive Transformer-XL world model that consumes discrete categorical latent states, actions, and predicted rewards to generate imagined trajectories for policy learning; transformer used during training only so inference is efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-based World Model (TWM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Observation model: VAE-style encoder producing discrete latent z (32 categorical variables × 32 categories) and decoder trained with pixel NLL; Dynamics/deterministic aggregation model: causally-masked Transformer-XL that takes modality-specific linear embeddings of past latent states, actions, and rewards and produces deterministic hidden states h_t; predictors (MLPs) output next-latent categorical distribution, reward mean (Gaussian), and discount (Bernoulli). World model is autoregressive in its own outputs (ẑ, r, γ) during imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (neural autoregressive latent model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games (Atari 100k benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Observation decoder negative log-likelihood (pixel NLL), consistency cross-entropy H(q_phi(z|o), p_psi(ẑ|h)), encoder entropy H(q_phi(z|o)), and the dynamics loss composed of the cross-entropy for latent prediction plus negative log-likelihood for reward and discount predictors; final task fidelity measured by human-normalized Atari scores (median, IQM, mean).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No absolute next-state MSE reported; dynamics / consistency losses are optimized and lower with balanced sampling; on task-level fidelity TWM outperforms prior model-based and model-free methods on Atari 100k aggregate metrics (median, IQM, mean) and narrows optimality gap; achieves higher normalized mean than SimPLe after 25K interactions and higher than prior methods after 50K interactions (numerical aggregate scores provided in paper figures/tables but no single scalar fidelity metric like MSE).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partially interpretable: attention maps and attention-rollout visualizations reveal which past states/actions/rewards the transformer attends to; imagined trajectories and reconstructed frame-stacks allow qualitative assessment of predicted dynamics and stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Attention visualization (attention maps, attention rollout), visualization of imagined trajectories, reconstructed frame-stacks from predicted latents.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>World model (observation model φ 8.2M + dynamics ψ 10.8M) ≈ 19M params; actor-critic ≈ 2.6M params; total ≈ 21.6M params; encoder+actor at inference ≈ 4.4M params. Training budget per run: roughly 10 hours on a single NVIDIA A100 (paper statement) while Table 3 reports ≈23.3 hours on a P100 for full training/eval; Transformer-XL variant is almost twice as fast as vanilla transformer at throughput level; vanilla transformer runtime reported ≈15.5 hours (A100 measurement) vs Transformer-XL faster.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>TWM is >20× faster than SimPLe (paper's comparison); Transformer-XL recurrence and relative positional encodings yield large inference/training efficiency improvements vs vanilla transformer; because the transformer is only used in training and not at inference the deployed policy is as efficient as model-free baselines (encoder+actor only). However, TWM training is slower than pure model-free training in absolute wall-clock (paper notes model-free methods are faster overall).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Outperforms previous sample-efficient model-based and model-free methods on the 26-game Atari 100k benchmark in aggregate metrics (median, IQM, mean) and reduces the optimality gap; shows strong sample-efficiency (higher normalized mean than several baselines at 25K and 50K interactions). Exact per-game and aggregate numbers are reported in the paper's tables/figures.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High-quality stochastic latent predictions plus autoregressive conditioning on rewards lead to generated imagined trajectories that improve policy learning (ablation shows feeding predicted rewards and balanced sampling improve task scores); task-relevant predictive fidelity (events that cause high reward) appear prioritized by attention and translate into improved policy utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Using a transformer provides better direct access to past predicted latents and long-term dependencies (improved fidelity) but increases training compute; design mitigates inference cost by not requiring the transformer at runtime. Shorter history length (=4) harms performance; conditioning the policy on h (requiring transformer at inference) can hurt final score due to distributional shift. Balanced sampling and thresholded entropy stabilize learning but introduce additional hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Transformer-XL (recurrence + relative pos encodings); autoregressive input of predicted rewards; discrete categorical latent states (32×32) adopted from DreamerV2; separate observation and dynamics models (φ and ψ); balanced cross-entropy loss (disentangled from entropy term) and thresholded entropy loss for actor; balanced (oversampling recent) dataset sampling; history length = 16; imagination horizon H = 15; policy conditioned on z only (not h) to avoid running transformer at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to RNN-based world models (Dreamer / DreamerV2), TWM directly attends to previous latents and captures long-term dependencies with Transformer-XL; compared to SimPLe (action-conditional video prediction) TWM is much faster and more sample-efficient on Atari 100k; compared to Transdreamer (concurrent work), TWM differs by feeding predicted rewards into the transformer and by not requiring transformer at inference (Transdreamer reportedly needs transformer outputs at inference); compared to MuZero, TWM is less computationally expensive at inference and simpler (MuZero uses MCTS and is computationally heavy).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends Transformer-XL backbone, autoregressive conditioning including predicted rewards, discrete categorical latent variables, separate observation and dynamics modules, balanced cross-entropy + entropy regularization, thresholded entropy for the actor, balanced dataset sampling emphasizing recent data, history length = 16, policy conditioned on z (not h) to keep inference efficient. These choices are empirically supported by ablations in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-based World Models Are Happy With 100k Interactions', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1242.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1242.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV2 (Mastering Atari with discrete world models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stochastic recurrent latent world model that uses discrete categorical latents and an improved objective; used here as the basis for the observation model architecture and latent design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering Atari with discrete world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV2 observation/dynamics architecture (adopted parts)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DreamerV2 employs a VAE-like encoder/decoder producing discrete categorical latents plus a recurrent dynamics prior; in this paper the observation encoder/decoder architecture and discrete latent design (categorical vector of 32 variables × 32 categories) are adopted with slight modifications for the observation model (φ).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (stochastic recurrent latent model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari / pixel-based RL tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Decoder negative log-likelihood, KL/consistency between encoder and prior (balanced KL / cross-entropy), reward prediction NLL; task performance measured via Atari scores.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not re-evaluated independently in this paper; elements of DreamerV2 architecture were used because of their good empirical fidelity on Atari reported in original DreamerV2 work.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>No special interpretability beyond general latent reconstructions discussed here; DreamerV2 provides latent reconstructions and imagined rollouts in its original work (not expanded on here).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Reconstruction visualization and use of discrete latents (inherited), though specific DreamerV2 interpretability methods are not reintroduced beyond being adopted.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>DreamerV2 original costs not re-stated; in this paper only the observation model architecture (8.2M params) is part of total model.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>DreamerV2-style discrete latent representation used to balance expressivity and efficiency; TWM replaces recurrent aggregation with transformer for dynamics to gain direct access to previous latents.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Used as a building block; overall system (TWM) outperforms baseline methods including Dreamer-family approaches on Atari 100k in aggregate metrics according to the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Discrete categorical latents provide a compact representation that stabilizes the policy training in latent imagination; combined with TWM dynamics they enable efficient imagined data generation for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Discrete latents reduce decoder complexity and improve stability, but policies must adapt to changes in z distribution during training (mitigated by consistency and entropy regularizers).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Adoption of discrete categorical latent state design and observation encoder/decoder architecture from DreamerV2 with added consistency loss and entropy regularizer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to continuous latents or RNN-only dynamics, DreamerV2 discrete latents are compact and effective; TWM swaps RNN dynamics for Transformer-XL while keeping DreamerV2 observation design.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Use discrete categorical latents with consistency and entropy regularization; pair with a dynamics model that can access long-term dependencies (here: Transformer-XL).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-based World Models Are Happy With 100k Interactions', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1242.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1242.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreamer (Dream to control)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of latent world models using stochastic RNNs to capture environment stochasticity and long-term dependencies for learning in imagination; cited as related prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer (stochastic RNN world model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Stochastic recurrent latent dynamics model splitting latent into stochastic and deterministic parts, trained with reconstruction and KL-based objectives; used to generate imagined trajectories for policy optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (stochastic RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari, continuous control and pixel-based RL</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction NLL and KL divergence terms between encoder and dynamics prior; downstream task performance (episode returns) when training in imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Referenced as prior strong model-based baseline; exact fidelity numbers not reproduced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper; Dreamer previously visualizes imagined rollouts but remains largely a neural black box.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Recurrent dynamics (RNN) can be sequential at inference generation time; TWM argues transformer allows parallel sequence processing during training and direct access to previous outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>TWM claims advantages: sequences processed in parallel in training (vs RNN sequential processing) and Transformer-XL recurrence for efficient long-term dependency capture.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Dreamer family is a strong baseline on many tasks; TWM places itself as an efficient transformer-based alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Dreamer demonstrates that imagined trajectories from a learned latent model can train strong policies; TWM builds on that idea replacing the RNN aggregator with a transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>RNN-based dynamics are simpler but may compress history into recurrent state; transformers provide direct access to previous outputs but raise compute cost at training time.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Stochastic RNN latent predictor vs autoregressive transformer predictor in TWM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Dreamer uses RNN-based dynamics; TWM replaces RNN with Transformer-XL to better attend to past outputs and capture long-range dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-based World Models Are Happy With 100k Interactions', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1242.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1242.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimPLe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SimPLe (Model-based RL for Atari)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An action-conditional video-prediction world model that trains policies on imagined pixel-level rollouts; earlier sample-efficient method for Atari.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Model based reinforcement learning for atari</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SimPLe (action-conditional video prediction model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Video-prediction style model that outputs pixel predictions conditioned on actions; used iteratively to generate imagined data for policy learning (PPO in original).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>pixel-level world model (video-prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Pixel prediction NLL / visual fidelity; downstream task return.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>SimPLe is an earlier sample-efficient baseline on Atari 100k; TWM reports outperforming SimPLe in aggregate metrics and being >20× faster in runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Reported to be very computationally expensive in the paper (SimPLe runtime ≈500 hours in Table 3), much slower than TWM.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>TWM is reported as >20× faster than SimPLe and produces stronger aggregate task performance on Atari 100k.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>SimPLe was an important baseline for sample-efficiency; TWM surpasses it on Atari 100k aggregate metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Pixel-level models can capture rich visuals but are computationally heavy; TWM's latent approach gains efficiency while retaining task utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High-fidelity pixel predictors can be costly and slow to train; latent models like TWM trade pixel detail for computational and training efficiency while preserving task-relevant dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Action-conditional pixel predictor (SimPLe) vs latent categorical + Transformer-XL autoregressive dynamics (TWM).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>SimPLe is more computationally expensive; TWM is much faster and more sample-efficient on Atari 100k per paper comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-based World Models Are Happy With 100k Interactions', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1242.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1242.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuZero (planning with a learned model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned sequence model of rewards and values combined with Monte-Carlo Tree Search to plan without learning explicit observation representations; cited as a planning-based model-based method that is computationally heavy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari, go, chess and shogi by planning with a learned model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns a representation, dynamics and prediction functions for value/reward/policy and uses MCTS at inference for planning; does not learn a full observation decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>model-based planning with learned latent dynamics + search</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari and board games (Go, Chess, Shogi)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Performance after planning (game scores); model fidelity implicitly measured by planning success rather than explicit next-frame reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>MuZero achieves very strong results on large-data benchmarks (e.g., 50M frames); paper notes MuZero is computationally expensive and requires substantial engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Computationally expensive at inference due to MCTS; noted as more computationally demanding than TWM.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>TWM is designed for sample efficiency in low-data regimes (100k interactions) and to be computationally cheaper at inference than MuZero's planning + MCTS pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>MuZero achieves top performance in high-data regimes; TWM focuses on low-data Atari 100k setting and trades planning compute for efficient imagined training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>MuZero's planning yields strong asymptotic performance but is costly; TWM prioritizes training-time modeling fidelity and inference efficiency for sample-limited settings.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Planning-based methods like MuZero trade inference compute (MCTS) for stronger performance; TWM trades off some planning capability for efficiency and sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Model as a planner (MuZero) vs model for imagination and offline policy learning (TWM).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>MuZero is more suitable when computation and engineering effort for planning/MCTS are acceptable; TWM is preferable when inference-time efficiency and sample-efficiency are priorities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-based World Models Are Happy With 100k Interactions', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1242.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1242.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transdreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transdreamer (Reinforcement learning with transformer world models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based replacement for Dreamer's RNN dynamics; mentioned as concurrent work that uses transformers in world models but with different design choices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transdreamer: Reinforcement learning with transformer world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transdreamer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Replaces the RNN aggregator in Dreamer with a transformer to model dynamics; reported to outperform Dreamer on certain tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (transformer-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Reinforcement learning tasks (Hidden Order Discovery tasks mentioned in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Task performance and quality of imagined rollouts (as measured in the Transdreamer paper); specific metrics not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Cited as outperforming Dreamer on Hidden Order Discovery tasks in their original work; in this paper Transdreamer is noted as not evaluated on Atari 100k and lacking reward inputs (per authors' description).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed here; authors note Transdreamer requires transformer outputs at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Transdreamer requires transformer outputs at inference, increasing inference cost relative to TWM which avoids transformer at runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>TWM emphasizes inference efficiency by not depending on transformer outputs at runtime; Transdreamer reportedly requires transformer at inference, making it more costly at deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not compared directly on Atari 100k in this paper; Transdreamer showed improvements vs Dreamer on other tasks per its own publication.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Transformer-based dynamics can improve modeling fidelity, but requiring transformer at inference increases deployment cost; TWM chooses a policy input design (z only) to avoid that.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Transformer dynamics give better expressivity but can raise inference cost if policy conditions on h; TWM chooses to avoid this tradeoff by conditioning policy on z and using transformer only during training.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Transdreamer: transformer dynamics used during both training and inference (per original work); TWM: transformer used only during world-model training and imagination, not at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Transdreamer vs TWM: both use transformers for dynamics; TWM differs by including predicted rewards in transformer inputs and by preventing dependence on transformer outputs at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-based World Models Are Happy With 100k Interactions', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1242.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1242.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Decision Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decision Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence modeling approach that frames RL as conditional sequence generation, trained on offline trajectories to output actions conditioned on returns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Decision transformer: Reinforcement learning via sequence modeling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decision Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer conditioned on states, actions, and returns to produce actions as a sequence modeling task; trained offline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>sequence-modeling for control (transformer-based policy model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Offline RL / control tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Behavioral cloning-like metrics and downstream task returns when conditioned appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Mentioned as related work; not directly compared on Atari 100k in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Transformer-based at inference if deployed as the policy; cost depends on model size and conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Different paradigm (sequence modeling for policy) rather than a learned world model for imagination; tradeoffs differ from TWM's objective of world-model-mediated imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not assessed within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Decision Transformer demonstrates sequence modeling can produce policies but is conceptually different from learning a generative world model for imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Offline conditioning on returns vs online imagination-based policy learning; those are different uses of transformers in RL.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Conditioning on returns and framing action prediction as autoregressive generation vs using a world model to generate imagined rollouts for actor-critic training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Decision Transformer is an offline sequence-modeling approach; TWM is a generative world model for on-policy iterative imagination-based training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-based World Models Are Happy With 100k Interactions', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1242.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1242.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Trajectory Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Trajectory Transformer (Trajectory-level sequence model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sequence-modeling approach that trains a transformer to model distributions over trajectories of states, actions, and rewards and can be used for planning; included as related transformer-for-RL work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Offline reinforcement learning as one big sequence modeling problem</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Trajectory Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer trained on offline trajectories of states/actions/rewards to model trajectory distributions; can be used to sample trajectories for planning or control.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>sequence-modeling world model (offline trajectory model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Offline RL and planning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Log-likelihood / next-token prediction metrics on trajectories; downstream planning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Mentioned in related work; no direct performance numbers in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Transformer training cost depends on dataset size; planning with sampled trajectories can incur compute in downstream use.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Conceptually similar to TWM in using transformers for trajectories, but TWM emphasizes Transformer-XL recurrence and autoregressive conditioning including rewards plus training-time-only transformer to reduce inference cost.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Trajectory Transformer shows transformers can model trajectories effectively; TWM extends this idea into an iterative imagination-training pipeline with design choices for sample efficiency and inference efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Offline trajectory modeling vs online iterative world-model learning and imagination: different data regimes and usage patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Trajectory Transformer: offline autoregressive trajectory modeling; TWM: online iterative training with balanced sampling and reward-feedback in inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-based World Models Are Happy With 100k Interactions', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1242.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1242.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ha&Schmidhuber</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ha & Schmidhuber world model (VAE + RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Early instantiation of a world model consisting of a VAE encoder and an RNN dynamics model used to generate imagined trajectories for policy learning (world model concept introduction).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent world models facilitate policy evolution</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VAE + RNN world model (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>VAE encodes observations into latent codes and an RNN models sequence dynamics of latents conditioned on actions; imagined rollouts used to evolve policies offline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (VAE encoder + RNN dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Control tasks and simple RL environments (original work)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction NLL for VAE and RNN prediction accuracy; downstream policy performance when trained on imagined rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Historical baseline demonstrating the viability of world models; not re-evaluated numerically here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Provides interpretable latent visualizations via VAE reconstructions; not a primary focus in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Reconstruction visualization and inspection of latent codes in original work.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>RNN dynamics are sequential; computational cost modest relative to pixel-level video predictors but can limit parallel training over sequences compared with transformer-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Transformers (as in TWM) process sequences more parallelly in training and can attend to past outputs directly versus compressed recurrent states in RNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Original work showed world models can generate useful imagined experiences for policy learning; TWM builds on this paradigm with transformer dynamics and further improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Demonstrated that learned generative models of environments can bootstrap policy learning with fewer real interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>RNNs compress history into hidden states which can hide detailed past outputs; transformer-based autoregressive models expose previous outputs directly but require different engineering choices.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>VAE for compact visual latents + RNN for dynamics vs TWM’s VAE + Transformer-XL dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-based World Models Are Happy With 100k Interactions', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1242.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1242.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VQ-VAE world model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Smaller world models for reinforcement learning (VQ-VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A compact world-model approach using vector-quantized VAEs to create smaller latent representations for dynamics modeling; cited as prior work by the paper's authors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Smaller world models for reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VQ-VAE based world model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses VQ-VAE to compress observations into discrete codebook indices and builds dynamics on top of this compact discrete representation to reduce parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (VQ-VAE compressed latents)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Reinforcement learning tasks (pixel-based)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss (decoder) and downstream policy performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Prior work from same authors showed smaller models are viable; no direct numerical replication in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Discrete codebook entries provide a degree of interpretability in latent representation (codebook visualizations), but not discussed in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Codebook / reconstruction visualizations in original work (not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Designed to reduce parameter count compared with larger VAEs; not quantitatively compared within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Motivates design choices that favor smaller latent representations for efficiency; TWM uses categorical latents (inspired by DreamerV2) and keeps total model size moderate (≈19M world-model params).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not directly compared in this paper; included as related work showing diverse latent modeling choices.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Smaller latent representations can reduce compute while preserving enough fidelity for policy learning in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Compression reduces compute/storage but may remove fine-grained details; design must balance compression and task-relevant fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of quantized discrete latents to keep models compact; alternative to categorical latents used in TWM/DreamerV2.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-based World Models Are Happy With 100k Interactions', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mastering Atari with discrete world models <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Model based reinforcement learning for atari <em>(Rating: 2)</em></li>
                <li>Mastering atari, go, chess and shogi by planning with a learned model <em>(Rating: 2)</em></li>
                <li>Transdreamer: Reinforcement learning with transformer world models <em>(Rating: 2)</em></li>
                <li>Transformers are sample efficient world models <em>(Rating: 2)</em></li>
                <li>Recurrent world models facilitate policy evolution <em>(Rating: 2)</em></li>
                <li>Smaller world models for reinforcement learning <em>(Rating: 2)</em></li>
                <li>Offline reinforcement learning as one big sequence modeling problem <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1242",
    "paper_id": "paper-257496038",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "TWM",
            "name_full": "Transformer-based World Model (TWM)",
            "brief_description": "An autoregressive Transformer-XL world model that consumes discrete categorical latent states, actions, and predicted rewards to generate imagined trajectories for policy learning; transformer used during training only so inference is efficient.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer-based World Model (TWM)",
            "model_description": "Observation model: VAE-style encoder producing discrete latent z (32 categorical variables × 32 categories) and decoder trained with pixel NLL; Dynamics/deterministic aggregation model: causally-masked Transformer-XL that takes modality-specific linear embeddings of past latent states, actions, and rewards and produces deterministic hidden states h_t; predictors (MLPs) output next-latent categorical distribution, reward mean (Gaussian), and discount (Bernoulli). World model is autoregressive in its own outputs (ẑ, r, γ) during imagination.",
            "model_type": "latent world model (neural autoregressive latent model)",
            "task_domain": "Atari games (Atari 100k benchmark)",
            "fidelity_metric": "Observation decoder negative log-likelihood (pixel NLL), consistency cross-entropy H(q_phi(z|o), p_psi(ẑ|h)), encoder entropy H(q_phi(z|o)), and the dynamics loss composed of the cross-entropy for latent prediction plus negative log-likelihood for reward and discount predictors; final task fidelity measured by human-normalized Atari scores (median, IQM, mean).",
            "fidelity_performance": "No absolute next-state MSE reported; dynamics / consistency losses are optimized and lower with balanced sampling; on task-level fidelity TWM outperforms prior model-based and model-free methods on Atari 100k aggregate metrics (median, IQM, mean) and narrows optimality gap; achieves higher normalized mean than SimPLe after 25K interactions and higher than prior methods after 50K interactions (numerical aggregate scores provided in paper figures/tables but no single scalar fidelity metric like MSE).",
            "interpretability_assessment": "Partially interpretable: attention maps and attention-rollout visualizations reveal which past states/actions/rewards the transformer attends to; imagined trajectories and reconstructed frame-stacks allow qualitative assessment of predicted dynamics and stochasticity.",
            "interpretability_method": "Attention visualization (attention maps, attention rollout), visualization of imagined trajectories, reconstructed frame-stacks from predicted latents.",
            "computational_cost": "World model (observation model φ 8.2M + dynamics ψ 10.8M) ≈ 19M params; actor-critic ≈ 2.6M params; total ≈ 21.6M params; encoder+actor at inference ≈ 4.4M params. Training budget per run: roughly 10 hours on a single NVIDIA A100 (paper statement) while Table 3 reports ≈23.3 hours on a P100 for full training/eval; Transformer-XL variant is almost twice as fast as vanilla transformer at throughput level; vanilla transformer runtime reported ≈15.5 hours (A100 measurement) vs Transformer-XL faster.",
            "efficiency_comparison": "TWM is &gt;20× faster than SimPLe (paper's comparison); Transformer-XL recurrence and relative positional encodings yield large inference/training efficiency improvements vs vanilla transformer; because the transformer is only used in training and not at inference the deployed policy is as efficient as model-free baselines (encoder+actor only). However, TWM training is slower than pure model-free training in absolute wall-clock (paper notes model-free methods are faster overall).",
            "task_performance": "Outperforms previous sample-efficient model-based and model-free methods on the 26-game Atari 100k benchmark in aggregate metrics (median, IQM, mean) and reduces the optimality gap; shows strong sample-efficiency (higher normalized mean than several baselines at 25K and 50K interactions). Exact per-game and aggregate numbers are reported in the paper's tables/figures.",
            "task_utility_analysis": "High-quality stochastic latent predictions plus autoregressive conditioning on rewards lead to generated imagined trajectories that improve policy learning (ablation shows feeding predicted rewards and balanced sampling improve task scores); task-relevant predictive fidelity (events that cause high reward) appear prioritized by attention and translate into improved policy utility.",
            "tradeoffs_observed": "Using a transformer provides better direct access to past predicted latents and long-term dependencies (improved fidelity) but increases training compute; design mitigates inference cost by not requiring the transformer at runtime. Shorter history length (=4) harms performance; conditioning the policy on h (requiring transformer at inference) can hurt final score due to distributional shift. Balanced sampling and thresholded entropy stabilize learning but introduce additional hyperparameters.",
            "design_choices": "Transformer-XL (recurrence + relative pos encodings); autoregressive input of predicted rewards; discrete categorical latent states (32×32) adopted from DreamerV2; separate observation and dynamics models (φ and ψ); balanced cross-entropy loss (disentangled from entropy term) and thresholded entropy loss for actor; balanced (oversampling recent) dataset sampling; history length = 16; imagination horizon H = 15; policy conditioned on z only (not h) to avoid running transformer at inference.",
            "comparison_to_alternatives": "Compared to RNN-based world models (Dreamer / DreamerV2), TWM directly attends to previous latents and captures long-term dependencies with Transformer-XL; compared to SimPLe (action-conditional video prediction) TWM is much faster and more sample-efficient on Atari 100k; compared to Transdreamer (concurrent work), TWM differs by feeding predicted rewards into the transformer and by not requiring transformer at inference (Transdreamer reportedly needs transformer outputs at inference); compared to MuZero, TWM is less computationally expensive at inference and simpler (MuZero uses MCTS and is computationally heavy).",
            "optimal_configuration": "Paper recommends Transformer-XL backbone, autoregressive conditioning including predicted rewards, discrete categorical latent variables, separate observation and dynamics modules, balanced cross-entropy + entropy regularization, thresholded entropy for the actor, balanced dataset sampling emphasizing recent data, history length = 16, policy conditioned on z (not h) to keep inference efficient. These choices are empirically supported by ablations in the paper.",
            "uuid": "e1242.0",
            "source_info": {
                "paper_title": "Transformer-based World Models Are Happy With 100k Interactions",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "DreamerV2",
            "name_full": "DreamerV2 (Mastering Atari with discrete world models)",
            "brief_description": "A stochastic recurrent latent world model that uses discrete categorical latents and an improved objective; used here as the basis for the observation model architecture and latent design.",
            "citation_title": "Mastering Atari with discrete world models",
            "mention_or_use": "use",
            "model_name": "DreamerV2 observation/dynamics architecture (adopted parts)",
            "model_description": "DreamerV2 employs a VAE-like encoder/decoder producing discrete categorical latents plus a recurrent dynamics prior; in this paper the observation encoder/decoder architecture and discrete latent design (categorical vector of 32 variables × 32 categories) are adopted with slight modifications for the observation model (φ).",
            "model_type": "latent world model (stochastic recurrent latent model)",
            "task_domain": "Atari / pixel-based RL tasks",
            "fidelity_metric": "Decoder negative log-likelihood, KL/consistency between encoder and prior (balanced KL / cross-entropy), reward prediction NLL; task performance measured via Atari scores.",
            "fidelity_performance": "Not re-evaluated independently in this paper; elements of DreamerV2 architecture were used because of their good empirical fidelity on Atari reported in original DreamerV2 work.",
            "interpretability_assessment": "No special interpretability beyond general latent reconstructions discussed here; DreamerV2 provides latent reconstructions and imagined rollouts in its original work (not expanded on here).",
            "interpretability_method": "Reconstruction visualization and use of discrete latents (inherited), though specific DreamerV2 interpretability methods are not reintroduced beyond being adopted.",
            "computational_cost": "DreamerV2 original costs not re-stated; in this paper only the observation model architecture (8.2M params) is part of total model.",
            "efficiency_comparison": "DreamerV2-style discrete latent representation used to balance expressivity and efficiency; TWM replaces recurrent aggregation with transformer for dynamics to gain direct access to previous latents.",
            "task_performance": "Used as a building block; overall system (TWM) outperforms baseline methods including Dreamer-family approaches on Atari 100k in aggregate metrics according to the paper.",
            "task_utility_analysis": "Discrete categorical latents provide a compact representation that stabilizes the policy training in latent imagination; combined with TWM dynamics they enable efficient imagined data generation for policy learning.",
            "tradeoffs_observed": "Discrete latents reduce decoder complexity and improve stability, but policies must adapt to changes in z distribution during training (mitigated by consistency and entropy regularizers).",
            "design_choices": "Adoption of discrete categorical latent state design and observation encoder/decoder architecture from DreamerV2 with added consistency loss and entropy regularizer.",
            "comparison_to_alternatives": "Compared to continuous latents or RNN-only dynamics, DreamerV2 discrete latents are compact and effective; TWM swaps RNN dynamics for Transformer-XL while keeping DreamerV2 observation design.",
            "optimal_configuration": "Use discrete categorical latents with consistency and entropy regularization; pair with a dynamics model that can access long-term dependencies (here: Transformer-XL).",
            "uuid": "e1242.1",
            "source_info": {
                "paper_title": "Transformer-based World Models Are Happy With 100k Interactions",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Dreamer",
            "name_full": "Dreamer (Dream to control)",
            "brief_description": "A family of latent world models using stochastic RNNs to capture environment stochasticity and long-term dependencies for learning in imagination; cited as related prior work.",
            "citation_title": "Dream to control: Learning behaviors by latent imagination",
            "mention_or_use": "mention",
            "model_name": "Dreamer (stochastic RNN world model)",
            "model_description": "Stochastic recurrent latent dynamics model splitting latent into stochastic and deterministic parts, trained with reconstruction and KL-based objectives; used to generate imagined trajectories for policy optimization.",
            "model_type": "latent world model (stochastic RNN)",
            "task_domain": "Atari, continuous control and pixel-based RL",
            "fidelity_metric": "Reconstruction NLL and KL divergence terms between encoder and dynamics prior; downstream task performance (episode returns) when training in imagination.",
            "fidelity_performance": "Referenced as prior strong model-based baseline; exact fidelity numbers not reproduced in this paper.",
            "interpretability_assessment": "Not discussed in this paper; Dreamer previously visualizes imagined rollouts but remains largely a neural black box.",
            "interpretability_method": "Not described here.",
            "computational_cost": "Recurrent dynamics (RNN) can be sequential at inference generation time; TWM argues transformer allows parallel sequence processing during training and direct access to previous outputs.",
            "efficiency_comparison": "TWM claims advantages: sequences processed in parallel in training (vs RNN sequential processing) and Transformer-XL recurrence for efficient long-term dependency capture.",
            "task_performance": "Dreamer family is a strong baseline on many tasks; TWM places itself as an efficient transformer-based alternative.",
            "task_utility_analysis": "Dreamer demonstrates that imagined trajectories from a learned latent model can train strong policies; TWM builds on that idea replacing the RNN aggregator with a transformer.",
            "tradeoffs_observed": "RNN-based dynamics are simpler but may compress history into recurrent state; transformers provide direct access to previous outputs but raise compute cost at training time.",
            "design_choices": "Stochastic RNN latent predictor vs autoregressive transformer predictor in TWM.",
            "comparison_to_alternatives": "Dreamer uses RNN-based dynamics; TWM replaces RNN with Transformer-XL to better attend to past outputs and capture long-range dependencies.",
            "uuid": "e1242.2",
            "source_info": {
                "paper_title": "Transformer-based World Models Are Happy With 100k Interactions",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SimPLe",
            "name_full": "SimPLe (Model-based RL for Atari)",
            "brief_description": "An action-conditional video-prediction world model that trains policies on imagined pixel-level rollouts; earlier sample-efficient method for Atari.",
            "citation_title": "Model based reinforcement learning for atari",
            "mention_or_use": "mention",
            "model_name": "SimPLe (action-conditional video prediction model)",
            "model_description": "Video-prediction style model that outputs pixel predictions conditioned on actions; used iteratively to generate imagined data for policy learning (PPO in original).",
            "model_type": "pixel-level world model (video-prediction)",
            "task_domain": "Atari games",
            "fidelity_metric": "Pixel prediction NLL / visual fidelity; downstream task return.",
            "fidelity_performance": "SimPLe is an earlier sample-efficient baseline on Atari 100k; TWM reports outperforming SimPLe in aggregate metrics and being &gt;20× faster in runtime.",
            "interpretability_assessment": "Not discussed in this paper.",
            "interpretability_method": "Not discussed here.",
            "computational_cost": "Reported to be very computationally expensive in the paper (SimPLe runtime ≈500 hours in Table 3), much slower than TWM.",
            "efficiency_comparison": "TWM is reported as &gt;20× faster than SimPLe and produces stronger aggregate task performance on Atari 100k.",
            "task_performance": "SimPLe was an important baseline for sample-efficiency; TWM surpasses it on Atari 100k aggregate metrics.",
            "task_utility_analysis": "Pixel-level models can capture rich visuals but are computationally heavy; TWM's latent approach gains efficiency while retaining task utility.",
            "tradeoffs_observed": "High-fidelity pixel predictors can be costly and slow to train; latent models like TWM trade pixel detail for computational and training efficiency while preserving task-relevant dynamics.",
            "design_choices": "Action-conditional pixel predictor (SimPLe) vs latent categorical + Transformer-XL autoregressive dynamics (TWM).",
            "comparison_to_alternatives": "SimPLe is more computationally expensive; TWM is much faster and more sample-efficient on Atari 100k per paper comparisons.",
            "uuid": "e1242.3",
            "source_info": {
                "paper_title": "Transformer-based World Models Are Happy With 100k Interactions",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "MuZero",
            "name_full": "MuZero (planning with a learned model)",
            "brief_description": "A learned sequence model of rewards and values combined with Monte-Carlo Tree Search to plan without learning explicit observation representations; cited as a planning-based model-based method that is computationally heavy.",
            "citation_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "mention_or_use": "mention",
            "model_name": "MuZero",
            "model_description": "Learns a representation, dynamics and prediction functions for value/reward/policy and uses MCTS at inference for planning; does not learn a full observation decoder.",
            "model_type": "model-based planning with learned latent dynamics + search",
            "task_domain": "Atari and board games (Go, Chess, Shogi)",
            "fidelity_metric": "Performance after planning (game scores); model fidelity implicitly measured by planning success rather than explicit next-frame reconstruction.",
            "fidelity_performance": "MuZero achieves very strong results on large-data benchmarks (e.g., 50M frames); paper notes MuZero is computationally expensive and requires substantial engineering.",
            "interpretability_assessment": "Not discussed in this paper.",
            "interpretability_method": "Not discussed here.",
            "computational_cost": "Computationally expensive at inference due to MCTS; noted as more computationally demanding than TWM.",
            "efficiency_comparison": "TWM is designed for sample efficiency in low-data regimes (100k interactions) and to be computationally cheaper at inference than MuZero's planning + MCTS pipeline.",
            "task_performance": "MuZero achieves top performance in high-data regimes; TWM focuses on low-data Atari 100k setting and trades planning compute for efficient imagined training.",
            "task_utility_analysis": "MuZero's planning yields strong asymptotic performance but is costly; TWM prioritizes training-time modeling fidelity and inference efficiency for sample-limited settings.",
            "tradeoffs_observed": "Planning-based methods like MuZero trade inference compute (MCTS) for stronger performance; TWM trades off some planning capability for efficiency and sample efficiency.",
            "design_choices": "Model as a planner (MuZero) vs model for imagination and offline policy learning (TWM).",
            "comparison_to_alternatives": "MuZero is more suitable when computation and engineering effort for planning/MCTS are acceptable; TWM is preferable when inference-time efficiency and sample-efficiency are priorities.",
            "uuid": "e1242.4",
            "source_info": {
                "paper_title": "Transformer-based World Models Are Happy With 100k Interactions",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Transdreamer",
            "name_full": "Transdreamer (Reinforcement learning with transformer world models)",
            "brief_description": "A transformer-based replacement for Dreamer's RNN dynamics; mentioned as concurrent work that uses transformers in world models but with different design choices.",
            "citation_title": "Transdreamer: Reinforcement learning with transformer world models",
            "mention_or_use": "mention",
            "model_name": "Transdreamer",
            "model_description": "Replaces the RNN aggregator in Dreamer with a transformer to model dynamics; reported to outperform Dreamer on certain tasks.",
            "model_type": "latent world model (transformer-based)",
            "task_domain": "Reinforcement learning tasks (Hidden Order Discovery tasks mentioned in paper)",
            "fidelity_metric": "Task performance and quality of imagined rollouts (as measured in the Transdreamer paper); specific metrics not reproduced here.",
            "fidelity_performance": "Cited as outperforming Dreamer on Hidden Order Discovery tasks in their original work; in this paper Transdreamer is noted as not evaluated on Atari 100k and lacking reward inputs (per authors' description).",
            "interpretability_assessment": "Not discussed here; authors note Transdreamer requires transformer outputs at inference time.",
            "interpretability_method": "Not described in this paper.",
            "computational_cost": "Transdreamer requires transformer outputs at inference, increasing inference cost relative to TWM which avoids transformer at runtime.",
            "efficiency_comparison": "TWM emphasizes inference efficiency by not depending on transformer outputs at runtime; Transdreamer reportedly requires transformer at inference, making it more costly at deployment.",
            "task_performance": "Not compared directly on Atari 100k in this paper; Transdreamer showed improvements vs Dreamer on other tasks per its own publication.",
            "task_utility_analysis": "Transformer-based dynamics can improve modeling fidelity, but requiring transformer at inference increases deployment cost; TWM chooses a policy input design (z only) to avoid that.",
            "tradeoffs_observed": "Transformer dynamics give better expressivity but can raise inference cost if policy conditions on h; TWM chooses to avoid this tradeoff by conditioning policy on z and using transformer only during training.",
            "design_choices": "Transdreamer: transformer dynamics used during both training and inference (per original work); TWM: transformer used only during world-model training and imagination, not at inference time.",
            "comparison_to_alternatives": "Transdreamer vs TWM: both use transformers for dynamics; TWM differs by including predicted rewards in transformer inputs and by preventing dependence on transformer outputs at inference.",
            "uuid": "e1242.5",
            "source_info": {
                "paper_title": "Transformer-based World Models Are Happy With 100k Interactions",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Decision Transformer",
            "name_full": "Decision Transformer",
            "brief_description": "A sequence modeling approach that frames RL as conditional sequence generation, trained on offline trajectories to output actions conditioned on returns.",
            "citation_title": "Decision transformer: Reinforcement learning via sequence modeling",
            "mention_or_use": "mention",
            "model_name": "Decision Transformer",
            "model_description": "Transformer conditioned on states, actions, and returns to produce actions as a sequence modeling task; trained offline.",
            "model_type": "sequence-modeling for control (transformer-based policy model)",
            "task_domain": "Offline RL / control tasks",
            "fidelity_metric": "Behavioral cloning-like metrics and downstream task returns when conditioned appropriately.",
            "fidelity_performance": "Mentioned as related work; not directly compared on Atari 100k in this paper.",
            "interpretability_assessment": "Not detailed in this paper.",
            "interpretability_method": "Not described here.",
            "computational_cost": "Transformer-based at inference if deployed as the policy; cost depends on model size and conditioning.",
            "efficiency_comparison": "Different paradigm (sequence modeling for policy) rather than a learned world model for imagination; tradeoffs differ from TWM's objective of world-model-mediated imagination.",
            "task_performance": "Not assessed within this paper.",
            "task_utility_analysis": "Decision Transformer demonstrates sequence modeling can produce policies but is conceptually different from learning a generative world model for imagination.",
            "tradeoffs_observed": "Offline conditioning on returns vs online imagination-based policy learning; those are different uses of transformers in RL.",
            "design_choices": "Conditioning on returns and framing action prediction as autoregressive generation vs using a world model to generate imagined rollouts for actor-critic training.",
            "comparison_to_alternatives": "Decision Transformer is an offline sequence-modeling approach; TWM is a generative world model for on-policy iterative imagination-based training.",
            "uuid": "e1242.6",
            "source_info": {
                "paper_title": "Transformer-based World Models Are Happy With 100k Interactions",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Trajectory Transformer",
            "name_full": "Trajectory Transformer (Trajectory-level sequence model)",
            "brief_description": "Sequence-modeling approach that trains a transformer to model distributions over trajectories of states, actions, and rewards and can be used for planning; included as related transformer-for-RL work.",
            "citation_title": "Offline reinforcement learning as one big sequence modeling problem",
            "mention_or_use": "mention",
            "model_name": "Trajectory Transformer",
            "model_description": "Autoregressive transformer trained on offline trajectories of states/actions/rewards to model trajectory distributions; can be used to sample trajectories for planning or control.",
            "model_type": "sequence-modeling world model (offline trajectory model)",
            "task_domain": "Offline RL and planning tasks",
            "fidelity_metric": "Log-likelihood / next-token prediction metrics on trajectories; downstream planning performance.",
            "fidelity_performance": "Mentioned in related work; no direct performance numbers in this paper.",
            "interpretability_assessment": "Not described in this paper.",
            "interpretability_method": "Not described here.",
            "computational_cost": "Transformer training cost depends on dataset size; planning with sampled trajectories can incur compute in downstream use.",
            "efficiency_comparison": "Conceptually similar to TWM in using transformers for trajectories, but TWM emphasizes Transformer-XL recurrence and autoregressive conditioning including rewards plus training-time-only transformer to reduce inference cost.",
            "task_performance": "Not evaluated here.",
            "task_utility_analysis": "Trajectory Transformer shows transformers can model trajectories effectively; TWM extends this idea into an iterative imagination-training pipeline with design choices for sample efficiency and inference efficiency.",
            "tradeoffs_observed": "Offline trajectory modeling vs online iterative world-model learning and imagination: different data regimes and usage patterns.",
            "design_choices": "Trajectory Transformer: offline autoregressive trajectory modeling; TWM: online iterative training with balanced sampling and reward-feedback in inputs.",
            "uuid": "e1242.7",
            "source_info": {
                "paper_title": "Transformer-based World Models Are Happy With 100k Interactions",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Ha&Schmidhuber",
            "name_full": "Ha & Schmidhuber world model (VAE + RNN)",
            "brief_description": "Early instantiation of a world model consisting of a VAE encoder and an RNN dynamics model used to generate imagined trajectories for policy learning (world model concept introduction).",
            "citation_title": "Recurrent world models facilitate policy evolution",
            "mention_or_use": "mention",
            "model_name": "VAE + RNN world model (Ha & Schmidhuber)",
            "model_description": "VAE encodes observations into latent codes and an RNN models sequence dynamics of latents conditioned on actions; imagined rollouts used to evolve policies offline.",
            "model_type": "latent world model (VAE encoder + RNN dynamics)",
            "task_domain": "Control tasks and simple RL environments (original work)",
            "fidelity_metric": "Reconstruction NLL for VAE and RNN prediction accuracy; downstream policy performance when trained on imagined rollouts.",
            "fidelity_performance": "Historical baseline demonstrating the viability of world models; not re-evaluated numerically here.",
            "interpretability_assessment": "Provides interpretable latent visualizations via VAE reconstructions; not a primary focus in this paper.",
            "interpretability_method": "Reconstruction visualization and inspection of latent codes in original work.",
            "computational_cost": "RNN dynamics are sequential; computational cost modest relative to pixel-level video predictors but can limit parallel training over sequences compared with transformer-based approaches.",
            "efficiency_comparison": "Transformers (as in TWM) process sequences more parallelly in training and can attend to past outputs directly versus compressed recurrent states in RNNs.",
            "task_performance": "Original work showed world models can generate useful imagined experiences for policy learning; TWM builds on this paradigm with transformer dynamics and further improvements.",
            "task_utility_analysis": "Demonstrated that learned generative models of environments can bootstrap policy learning with fewer real interactions.",
            "tradeoffs_observed": "RNNs compress history into hidden states which can hide detailed past outputs; transformer-based autoregressive models expose previous outputs directly but require different engineering choices.",
            "design_choices": "VAE for compact visual latents + RNN for dynamics vs TWM’s VAE + Transformer-XL dynamics.",
            "uuid": "e1242.8",
            "source_info": {
                "paper_title": "Transformer-based World Models Are Happy With 100k Interactions",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "VQ-VAE world model",
            "name_full": "Smaller world models for reinforcement learning (VQ-VAE)",
            "brief_description": "A compact world-model approach using vector-quantized VAEs to create smaller latent representations for dynamics modeling; cited as prior work by the paper's authors.",
            "citation_title": "Smaller world models for reinforcement learning",
            "mention_or_use": "mention",
            "model_name": "VQ-VAE based world model",
            "model_description": "Uses VQ-VAE to compress observations into discrete codebook indices and builds dynamics on top of this compact discrete representation to reduce parameter count.",
            "model_type": "latent world model (VQ-VAE compressed latents)",
            "task_domain": "Reinforcement learning tasks (pixel-based)",
            "fidelity_metric": "Reconstruction loss (decoder) and downstream policy performance.",
            "fidelity_performance": "Prior work from same authors showed smaller models are viable; no direct numerical replication in this paper.",
            "interpretability_assessment": "Discrete codebook entries provide a degree of interpretability in latent representation (codebook visualizations), but not discussed in detail here.",
            "interpretability_method": "Codebook / reconstruction visualizations in original work (not detailed in this paper).",
            "computational_cost": "Designed to reduce parameter count compared with larger VAEs; not quantitatively compared within this paper.",
            "efficiency_comparison": "Motivates design choices that favor smaller latent representations for efficiency; TWM uses categorical latents (inspired by DreamerV2) and keeps total model size moderate (≈19M world-model params).",
            "task_performance": "Not directly compared in this paper; included as related work showing diverse latent modeling choices.",
            "task_utility_analysis": "Smaller latent representations can reduce compute while preserving enough fidelity for policy learning in some settings.",
            "tradeoffs_observed": "Compression reduces compute/storage but may remove fine-grained details; design must balance compression and task-relevant fidelity.",
            "design_choices": "Use of quantized discrete latents to keep models compact; alternative to categorical latents used in TWM/DreamerV2.",
            "uuid": "e1242.9",
            "source_info": {
                "paper_title": "Transformer-based World Models Are Happy With 100k Interactions",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mastering Atari with discrete world models",
            "rating": 2,
            "sanitized_title": "mastering_atari_with_discrete_world_models"
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        },
        {
            "paper_title": "Model based reinforcement learning for atari",
            "rating": 2,
            "sanitized_title": "model_based_reinforcement_learning_for_atari"
        },
        {
            "paper_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "rating": 2,
            "sanitized_title": "mastering_atari_go_chess_and_shogi_by_planning_with_a_learned_model"
        },
        {
            "paper_title": "Transdreamer: Reinforcement learning with transformer world models",
            "rating": 2,
            "sanitized_title": "transdreamer_reinforcement_learning_with_transformer_world_models"
        },
        {
            "paper_title": "Transformers are sample efficient world models",
            "rating": 2,
            "sanitized_title": "transformers_are_sample_efficient_world_models"
        },
        {
            "paper_title": "Recurrent world models facilitate policy evolution",
            "rating": 2,
            "sanitized_title": "recurrent_world_models_facilitate_policy_evolution"
        },
        {
            "paper_title": "Smaller world models for reinforcement learning",
            "rating": 2,
            "sanitized_title": "smaller_world_models_for_reinforcement_learning"
        },
        {
            "paper_title": "Offline reinforcement learning as one big sequence modeling problem",
            "rating": 2,
            "sanitized_title": "offline_reinforcement_learning_as_one_big_sequence_modeling_problem"
        }
    ],
    "cost": 0.0231315,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TRANSFORMER-BASED WORLD MODELS ARE HAPPY WITH 100K INTERACTIONS</p>
<p>Jan Robine 
Department of Computer Science
Technical University of Dortmund
Germany</p>
<p>Marc Höftmann 
Department of Computer Science
Technical University of Dortmund
Germany</p>
<p>Tobias Uelwer 
Department of Computer Science
Technical University of Dortmund
Germany</p>
<p>Stefan Harmeling 
Department of Computer Science
Technical University of Dortmund
Germany</p>
<p>TRANSFORMER-BASED WORLD MODELS ARE HAPPY WITH 100K INTERACTIONS
Published as a conference paper at ICLR 2023
Deep neural networks have been successful in many reinforcement learning settings. However, compared to human learners they are overly data hungry. To build a sample-efficient world model, we apply a transformer to real-world episodes in an autoregressive manner: not only the compact latent states and the taken actions but also the experienced or predicted rewards are fed into the transformer, so that it can attend flexibly to all three modalities at different time steps. The transformer allows our world model to access previous states directly, instead of viewing them through a compressed recurrent state. By utilizing the Transformer-XL architecture, it is able to learn long-term dependencies while staying computationally efficient. Our transformer-based world model (TWM) generates meaningful, new experience, which is used to train a policy that outperforms previous model-free and model-based reinforcement learning algorithms on the Atari 100k benchmark. Our code is available at https://github.com/jrobine/twm. ∞ t=1 γ t−1 r t , where γ ∈ [0, 1) is the discount factor. Learning in imagination consists of three steps that are repeated iteratively: learning the dynamics, learning a policy, and interacting in the real environment. In this section, we describe our world model and policy, concluding with the training procedure.</p>
<p>INTRODUCTION</p>
<p>Deep reinforcement learning methods have shown great success on many challenging decision making problems. Notable methods include DQN (Mnih et al., 2015), PPO (Schulman et al., 2017), and MuZero (Schrittwieser et al., 2019). However, most algorithms require hundreds of millions of interactions with the environment, whereas humans often can achieve similar results with less than 1% of these interactions, i.e., they are more sample-efficient. The large amount of data that is necessary renders a lot of potential real world applications of reinforcement learning impossible.</p>
<p>Recent works have made a lot of progress in advancing the sample efficiency of RL algorithms: model-free methods have been improved with auxiliary objectives (Laskin et al., 2020b), data augmentation (Yarats et al., 2021, Laskin et al., 2020a, or both . Model-based methods have been successfully applied to complex image-based environments and have either been used for planning, such as EfficientZero (Ye et al., 2021), or for learning behaviors in imagination, such as SimPLe (Kaiser et al., 2020). Observations o t− :t are encoded using a CNN. Linear embeddings of stochastic, discrete latent states z t− :t , actions a t− :t , and rewards r t− :t are fed into a transformer, which computes a deterministic hidden state h t at each time step. Predictions of the reward r t , discount factor γ t , and next latent state z t+1 are computed based on h t using MLPs.
o t− z t− ô t− a t− h t− r t− · · · · · ·
1 arXiv:2303.07109v1 [cs.LG] 13 Mar 2023</p>
<p>Published as a conference paper at ICLR 2023 A promising model-based concept is learning in imagination (Ha &amp; Schmidhuber, 2018;Kaiser et al., 2020;Hafner et al., 2020;Hafner et al., 2021): instead of learning behaviors from the collected experience directly, a generative model of the environment dynamics is learned in a (self-)supervised manner. Such a so-called world model can create new trajectories by iteratively predicting the next state and reward. This allows for potentially indefinite training data for the reinforcement learning algorithm without further interaction with the real environment. A world model might be able to generalize to new, unseen situations, because of the nature of deep neural networks, which has the potential to drastically increase the sample efficiency. This can be illustrated by a simple example: in the game of Pong, the paddles and the ball move independently. In the best case, a successfully trained world model would imagine trajectories with paddle and ball configurations that have never been observed before, which enables learning of improved behaviors.</p>
<p>In this paper, we propose to model the world with transformers (Vaswani et al., 2017), which have significantly advanced the field of natural language processing and have been successfully applied to computer vision tasks (Dosovitskiy et al., 2021). A transformer is a sequence model consisting of multiple self-attention layers with residual connections. In each self-attention layer the inputs are mapped to keys, queries, and values. The outputs are computed by weighting the values by the similarity of keys and queries. Combined with causal masking, which prevents the self-attention layers from accessing future time steps in the training sequence, transformers can be used as autoregressive generative models. The Transformer-XL architecture (Dai et al., 2019) is much more computationally efficient than vanilla transformers at inference time and introduces relative positional encodings, which remove the dependence on absolute time steps.</p>
<p>Our contributions: The contributions of this work can be summarized as follows:</p>
<ol>
<li>We present a new autoregressive world model based on the Transformer-XL (Dai et al., 2019) architecture and a model-free agent trained in latent imagination. Running our policy is computationally efficient, as the transformer is not needed at inference time. This is in contrast to related works (Hafner et al., 2020;Chen et al., 2022) that require the full world model during inference. 2. Our world model is provided with information on how much reward has already been emitted by feeding back predicted rewards into the world model. As shown in our ablation study, this improves performance. 3. We rewrite the balanced KL divergence loss of Hafner et al. (2021) to allow us to fine-tune the relative weight of the involved entropy and cross-entropy terms. 4. We introduce a new thresholded entropy loss that stabilizes the policy's entropy during training and hereby simplifies the selection of hyperparameters that behave well across different games. 5. We propose a new effective sampling procedure for the growing dataset of experience, which balances the training distribution to shift the focus towards the latest experience. We demonstrate the efficacy of this procedure with an ablation study. 6. We compare our transformer-based world model (TWM) on the Atari 100k benchmark with recent sample-efficient methods and obtain excellent results. Moreover, we report empirical confidence intervals of the aggregate metrics as suggested by Agarwal et al. (2021).</li>
</ol>
<p>METHOD</p>
<p>We consider a partially observable Markov decision process (POMDP) with discrete time steps t ∈ N, scalar rewards r t ∈ R, high-dimensional image observations o t ∈ R h×w×c , and discrete actions a t ∈ {1, . . . , m}, which are generated by some policy a t ∼ π(a t | o 1:t , a 1:t−1 ), where o 1:t and a 1:t−1 denote the sequences of observations and actions up to time steps t and t − 1, respectively. Episode ends are indicated by a boolean variable d t ∈ {0, 1}. Observations, rewards, and episode ends are jointly generated by the unknown environment dynamics o t , r t , d t ∼ p(o t , r t , d t | o 1:t−1 , a 1:t−1 ). The goal is to find a policy π that maximizes the expected sum of discounted rewards E π 2.1 WORLD MODEL Our world model consists of an observation model and a dynamics model, which do not share parameters. Figure 1 illustrates our combined world model architecture.</p>
<p>Observation Model: The observation model is a variational autoencoder (Kingma &amp; Welling, 2014), which encodes observations o t into compact, stochastic latent states z t and reconstructs the observations with a decoder, which in our case is only required to obtain a learning signal for z t :
Observation encoder: z t ∼ p φ (z t | o t ) Observation decoder:ô t ∼ p φ (ô t | z t ).(1)
We adopt the neural network architecture of DreamerV2 (Hafner et al., 2021) with slight modifications for our observation model. Thus, a latent state z t is discrete and consists of a vector of 32 categorical variables with 32 categories. The observation decoder reconstructs the observation and predicts the means of independent standard normal distributions for all pixels. The role of the observation model is to capture only non-temporal information about the current time step, which is different from Hafner et al. (2021). However, we include short-time temporal information, since a single observation o t consists of four frames (aka frame stacking, see also Section 2.2).</p>
<p>Autoregressive Dynamics Model: The dynamics model predicts the next time step conditioned on the history of its past predictions. The backbone is a deterministic aggregation model f ψ which computes a deterministic hidden state h t based on the history of the previously generated latent states, actions, and rewards. Predictors for the reward, discount, and next latent state are conditioned on the hidden state. The dynamics model consists of these components:
Aggregation model: h t = f ψ (z t− :t , a t− :t , r t− :t−1 ) Reward predictor:r t ∼ p ψ (r t | h t ) Discount predictor:γ t ∼ p ψ (γ t | h t ) Latent state predictor:ẑ t+1 ∼ p ψ (ẑ t+1 | h t ).
(2)</p>
<p>The aggregation model is implemented as a causally masked Transformer-XL (Dai et al., 2019), which enhances vanilla transformers (Vaswani et al., 2017) with a recurrence mechanism and relative positional encodings. With these encodings, our world model learns the dynamics independent of absolute time steps. Following Chen et al. (2021), the latent states, actions, and rewards are sent into modality-specific linear embeddings before being passed to the transformer. The number of input tokens is 3 − 1, because of the three modalities (latent states, actions, rewards) and the last reward not being part of the input. We consider the outputs of the action modality as the hidden states and disregard the outputs of the other two modalities (see Figure 1; orange boxes vs. gray boxes).</p>
<p>The latent state, reward, and discount predictors are implemented as multilayer perceptrons (MLPs) and compute the parameters of a vector of independent categorical distributions, a normal distribution, and a Bernoulli distribution, respectively, conditioned on the deterministic hidden state. The next state is determined by sampling from p ψ (ẑ t+1 | h t ). The reward and discount are determined by the mean of p ψ (r t | h t ) and p ψ (γ t | h t ), respectively.</p>
<p>As a consequence of these design choices, our world model has the following beneficial properties:</p>
<ol>
<li>The dynamics model is autoregressive and has direct access to its previous outputs. 2. Training is efficient since sequences are processed in parallel (compared with RNNs). 3. Inference is efficient because outputs are cached (compared with vanilla Transformers). 4. Long-term dependencies can be captured by the recurrence mechanism.</li>
</ol>
<p>We want to provide an intuition on why a fully autoregressive dynamics model is favorable: First, the direct access to previous latent states enables to model more complex dependencies between them, compared with RNNs, which only see them indirectly through a compressed recurrent state. This also has the potential to make inference more robust, since degenerate predictions can be ignored more easily. Second, because the model sees which rewards it has produced previously, it can react to its own predictions. This is even more significant when the rewards are sampled from a probability distribution, since the introduced noise cannot be observed without autoregression.</p>
<p>Loss Functions: The observation model can be interpreted as a variational autoencoder with a temporal prior, which is provided by the latent state predictor. The goal is to keep the distributions of the encoder and the latent state predictor close to each other, while slowly adapting to new observations and dynamics. Hafner et al. (2021) apply a balanced KL divergence loss, which lets them control which of the two distributions should be penalized more. To control the influences of its subterms more precisely, we disentangle this loss and obtain a balanced cross-entropy loss that computes the cross-entropy H(p φ (z t+1 | o t+1 ), p ψ (ẑ t+1 | h t )) and the entropy H(p φ (z t | o t )) explicitly. Our derivation can be found in Appendix A.2. We call the cross-entropy term for the observation model the consistency loss, as its purpose is to prevent the encoder from diverging from the dynamics model. The entropy regularizes the latent states and prevents them from collapsing to one-hot distributions. The observation decoder is optimized via negative log-likelihood, which provides a rich learning signal for the latent states. In summary, we optimize a self-supervised loss function for the observation model that is the expected sum over the decoder loss, the entropy regularizer and the consistency loss
L Obs. φ = E T t=1 − ln p φ (o t | z t ) decoder − α 1 H(p φ (z t | o t )) entropy regularizer + α 2 H(p φ (z t | o t ), p ψ (ẑ t | h t−1 )) consistency ,(3)
where the hyperparameters α 1 , α 2 ≥ 0 control the relative weights of the terms.</p>
<p>For the balanced cross-entropy loss, we also minimize the cross-entropy in the loss of the dynamics model, which is how we train the latent state predictor. The reward and discount predictors are optimized via negative log-likelihood. This leads to a self-supervised loss for the dynamics model
L Dyn. ψ = E T t=1 H(p φ (z t+1 | o t+1 ), p ψ (ẑ t+1 | h t )) latent state predictor − β 1 ln p ψ (r t | h t ) reward predictor − β 2 ln p ψ (γ t | h t ) discount predictor ,(4)
with coefficients β 1 , β 2 ≥ 0 and where γ t = 0 for episode ends (d t = 1) and γ t = γ otherwise.</p>
<p>POLICY</p>
<p>Our policy π θ (a t |ẑ t ) is trained on imagined trajectories using a mainly standard advantage actorcritic (Mnih et al., 2016) approach. We train two separate networks: an actor a t ∼ π θ (a t |ẑ t ) with parameters θ and a critic v ξ (ẑ t ) with parameters ξ. We compute the advantages via Generalized Advantage Estimation (Schulman et al., 2016) while using the discount factors predicted by the world modelγ t instead of a fixed discount factor for all time steps. As in DreamerV2 (Hafner et al., 2021), we weight the losses of the actor and the critic by the cumulative product of the discount factors, in order to softly account for episode ends.</p>
<p>Thresholded Entropy Loss: We penalize the objective of the actor with a slightly modified version of the usual entropy regularization term (Mnih et al., 2016). Our penalty normalizes the entropy and only takes effect when the entropy falls below a certain threshold
L Ent. θ = max 0, Γ − H(π θ ) ln(m) ,(5)
where 0 ≤ Γ ≤ 1 is the threshold hyperparameter, H(π θ ) is the entropy of the policy, m is the number of discrete actions, and ln(m) is the maximum possible entropy of the categorical action distribution. By doing this, we explicitly control the percentage of entropy that should be preserved across all games independent of the number of actions. This ensures exploration in the real environment and in imagination without the need for -greedy action selection or changing the temperature of the action distribution. We also use the same stochastic policy when evaluating our agent in the experiments. The idea of applying a hinge loss to the entropy was first introduced by Pereyra et al. (2017) in the context of supervised learning. In Appendix A.1 we show the effect of this loss.</p>
<p>Choice of Policy Input: The policy computes an action distribution π θ (a t | x t ) given some view x t of the state. For instance, x t could be o t , z t , or [z t , h t ] at inference time, i.e., when applied to the real environment, or the respective predictions of the world modelô t ,ẑ t , or [ẑ t , h t ] at training time. This view has to be chosen carefully, since it can have a significant impact on the performance  (6)) for different values of τ with uniform sampling (τ = ∞). The x-axes correspond to the entries in dataset D in the order they are experienced. The left plot shows the number of times an entry has been selected for training the world model. The right plot shows the relative amount of training time that has been spent on the data up to that entry. E.g., with uniform sampling, 50% of the training time is used for the first 19K entries, whereas for τ = 20 approximately the same time is spend on both halves of the dataset.</p>
<p>of the policy and it affects the design choices for the world model. Using x t = o t (orô t ) is relatively stable even with imperfect reconstructionsô t , as the underlying distribution of observations p(o) does not change during training. However, it is also less computationally efficient, since it requires reconstructing the observations during imagination and additional convolutional layers for the policy. Using x t = z t (orẑ t ) is slightly less stable, as the policy has to adopt to the changes of the distributions p φ (z t | o t ) and p ψ (ẑ t+1 | h t ) during training. Nevertheless, the entropy regularizer and consistency loss in Equation (3) 
stabilize these distributions. Using x t = [z t , h t ] (or [ẑ t , h t ])
provides the agent with a summary of the history of experience, but it also adds the burden of running the transformer at inference time. Model-free agents already perform well on most Atari games when using a stack of the most recent frames (e.g., Mnih et al. 2015;Schulman et al. 2017). Therefore, we choose x t = z t and apply frame stacking at inference time in order to incorporate short-time information directly into the latent states. At training time we use x t =ẑ t , i.e., the predicted latent states, meaning no frame stacking is applied. As a consequence, our policy is computationally efficient at training time (no reconstructions during imagination) and at inference time (no transformer when running in the real environment).</p>
<p>TRAINING</p>
<p>As is usual for learning with world models, we repeatedly (i) collect experience in the real environment with the current policy, (ii) improve the world model using the past experience, (iii) improve the policy using new experience generated by the world model.</p>
<p>During training we build a dataset D = [(o 1 , a 1 , r 1 , d 1 ), . . . , (o T , a T , r T , d T )] of the collected experience. After collecting new experience with the current policy, we improve the world model by sampling N sequences of length from D and optimizing the loss functions in Equations (3) and (4) using stochastic gradient descent. After performing a world model update, we select M of the N × observations and encode them into latent states to serve as initial states for new trajectories. The dynamics model iteratively generates these M trajectories of length H based on actions provided by the policy. Subsequently, the policy is improved with standard model-free objectives, as described in Section 2.2. In Algorithm 1 we present pseudocode for training the world model and the policy.</p>
<p>Balanced Dataset Sampling: Since the dataset grows slowly during training, uniform sampling of trajectories focuses too heavily on early experience, which can lead to overfitting especially in the low data regime. Therefore, we keep visitation counts v 1 , . . . , v T , which are incremented every time an entry is sampled as start of a sequence. These counts are converted to probabilities using the softmax function
(p 1 , . . . , p T ) = softmax − v1 τ , . . . , − v T τ ,(6)
where τ &gt; 0 is a temperature hyperparameter. With our sampling procedure, new entries in the dataset are oversampled and are selected more often than old ones. Setting τ = ∞ restores uniform sampling as a special case, whereas reducing τ increases the amount of oversampling. See Figure 2 for a comparison. We empirically show the effectiveness in Section 3.3. Human Normalized Score Figure 3: Aggregate metrics on the Atari 100k benchmark with 95% stratified bootstrap confidence intervals (Agarwal et al., 2021). Higher median, interquartile mean (IQM), and mean, but lower optimality gap indicate better performance. Scores for previous methods are from Agarwal et al. (2021) with 100 runs per game (except SimPLe with 5 runs). We evaluate 5 runs per game, leading to wider confidence intervals.</p>
<p>EXPERIMENTS</p>
<p>To compare data-efficient reinforcement learning algorithms, Kaiser et al. (2020) proposed the Atari 100k benchmark, which uses a subset of 26 Atari games from the Arcade Learning Environment (Bellemare et al., 2013) and limits the number of interactions per game to 100K. This corresponds to 400K frames (because of frame skipping) or roughly 2 hours of gameplay, which is 500 times less than the usual 200 million frames (e.g., Mnih et al. 2015;Schulman et al. 2017;Hafner et al. 2021).</p>
<p>We compare our method with five strong competitors on the Atari 100k benchmark: (i) SimPLe : Performance profiles on the Atari 100k benchmark based on score distributions (Agarwal et al., 2021). It shows the fraction of runs across all games (y-axis) above a human normalized score (x-axis). Shaded regions show pointwise 95% confidence bands.</p>
<p>We follow the advice of Agarwal et al. (2021) who found significant discrepancies between reported point estimates of mean (and median) scores and a thorough statistical analysis that includes statistical uncertainty. Thus, we report confidence interval estimates of the aggregate metrics median, interquartile mean (IQM), mean, and optimality gap in Figure 3 and performance profiles in Figure 4, which we created using the toolbox provided by Agarwal et al. (2021). The metrics are computed on human normalized scores, which are calculated as (score_agent -score_random)/ (score_human -score_random). We report the unnormalized scores per game in Table 1. We compare with new scores for DER, CURL, DrQ, and SPR that were evaluated on 100 runs and provided by Agarwal et al. (2021). They report scores for the improved DrQ(ε), which is DrQ evaluated with standard ε-greedy parameters. We perform 5 runs per game and compute the average score over 100 episodes at the end of training for each run. TWM shows a significant improvement over previous approaches in all four aggregate metrics and brings the optimality gap closer to zero.   </p>
<p>ANALYSIS</p>
<p>In Figure 5 we show imagined trajectories of our world model. In Figure 6 we visualize an attention map of the transformer for an imagined sequence. In this example a lot of weight is put on the current action and the last three states. However, the transformer also attends to states and rewards in the past, with only past actions being mostly ignored. The two high positive rewards also get high attention, which confirms that the rewards in the input sequence are used by the world model. We hypothesize that these rewards correspond to some events that happened in the environment and this information can be useful for prediction.</p>
<p>An extended analysis can be found in Appendix A.1, including more imagined trajectories and attention maps (and a description of the generation of the plots), sample efficiency, stochasticity of the world model, long sequence imagination, and frame stacking.</p>
<p>s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a  Figure 7: Comparison of the proposed balanced sampling procedure with uniform sampling on a random subset of games. We show the development of the human normalized score in the course of training. The score is higher with balanced sampling, demonstrating its importance.</p>
<p>ABLATION STUDIES</p>
<p>Uniform Sampling: To show the effectiveness of the sampling procedure described in Section 2.3, we evaluate three games with uniform dataset sampling, which is equivalent to setting τ = ∞ in Equation (6). In Figure 7 we show that balanced dataset sampling significantly improves the performance in these games. At the end of training, the dynamics loss from Equation (4) is lower when applying balanced sampling. One reason might be that the world model overfits on early training data and performs bad in later stages of training.</p>
<p>No Rewards: As described in Section 2.1, the predicted rewards are fed back into the transformer. In Figure 8 we show on three games that this can significantly increase the performance. In some games the performance is equivalent, probably because the world model can make correct predictions solely based on the latent states and actions.</p>
<p>In Appendix A.1 we perform additional ablation studies, including the thresholded entropy loss, a shorter history length, conditioning the policy on [z, h], and increasing the sample efficiency.   Figure 8: Effect of removing rewards from the input. We show the human normalized score during training of a random subset of games. Conditioning on rewards can significantly increase the performance. Some games do not benefit from the rewards and the score stays roughly the same.</p>
<p>RELATED WORK</p>
<p>The Dyna architecture (Sutton, 1991) introduced the idea of training a model of the environment and using it to further improve the value function or the policy. Ha &amp; Schmidhuber (2018) introduced the notion of a world model, which tries to completely imitate the environment and is used to generate experience to train a model-free agent. They implement a world model as a VAE (Kingma &amp; Welling, 2014) and an RNN and learn a policy in latent space with an evolution strategy. With SimPLe, Kaiser et al. (2020) propose an iterative training procedure that alternates between training the world model and the policy. Their policy operates on pixel-level and is trained using PPO (Schulman et al., 2017). Hafner et al. (2020) present Dreamer and implement a world model as a stochastic RNN that splits the latent state in a stochastic part and a deterministic part; this idea was first introduced by Hafner et al., 2019. This allows their world model to capture the stochasticity of the environment and simultaneously facilitates remembering information over multiple time steps. Robine et al. (2020) use a VQ-VAE to construct a world model with drastically lower number of parameters. DreamerV2 (Hafner et al., 2021) achieves great performance on the Atari 50M benchmark after making some changes to Dreamer, the most important ones being categorical latent variables and an improved objective. Another direction of model-based reinforcement learning is planning, where the model is used at inference time to improve the action selection by looking ahead several time steps into the future. The most prominent work is MuZero (Schrittwieser et al., 2019), where a learned sequence model of rewards and values is combined with Monte-Carlo Tree Search (Coulom, 2006) without learning explicit representations of the observations. MuZero achieves impressive performance on the Atari 50M benchmark, but it is also computationally expensive and requires significant engineering effort. EfficientZero (Ye et al., 2021) improves MuZero and achieves great performance on the Atari 100k benchmark.</p>
<p>Transformers (Vaswani et al., 2017) advanced the effectiveness of sequence models in multiple domains, such as natural language processing and computer vision (Dosovitskiy et al., 2021). Recently, they have also been applied to reinforcement learning tasks. The Decision Transformer (Chen et al., 2021) and the Trajectory Transformer (Janner et al., 2021) are trained on an offline dataset of trajectories. The Decision Transformer is conditioned on states, actions, and returns, and outputs optimal actions. The Trajectory Transformer trains a sequence model of states, actions, and rewards, and is used for planning. Chen et al. (2022) replace the RNN of Dreamer with a transformer and outperform Dreamer on Hidden Order Discovery tasks. However, their transformer has no access to previous rewards and they do not evaluate their method on the Atari 100k benchmark. Moreover, their policy depends on the outputs of the transformer, leading to higher computational costs during inference time. Concurrent to and independent from our work, Micheli et al. (2022) apply a transformer to sequences of frame tokens and actions and achieve state-of-the-art results on the Atari 100k benchmark.</p>
<p>CONCLUSION</p>
<p>In this work, we discuss a reinforcement learning approach using transformer-based world models. Our method (TWM) outperforms previous model-free and model-based methods in terms of human normalized score on the 26 games of the Atari 100k benchmark. By using the transformer only during training, we were able to keep the computational costs low during inference, i.e., when running the learned policy in a real environment. We show how feeding back the predicted rewards into the transformer is beneficial for learning the world model. Furthermore, we introduce the balanced cross-entropy loss for finer control over the trade-off between the entropy and cross-entropy terms in the loss functions of the world model. A new thresholded entropy loss effectively stabilizes the entropy of the policy. Finally, our novel balanced sampling procedure corrects issues of naive uniform sampling of past experience.   1. We provide more example trajectories in Figure 9.</p>
<ol>
<li>We present more attention plots in Figures 10 and 11. All attention maps are generated using the attention rollout method by Abnar &amp; Zuidema (2020). Note that we had to modify the method slightly, in order to take the causal masks into account.</li>
</ol>
<p>Sample Efficiency:</p>
<p>We provide the scores of our main experiments after different amounts of interactions with the environment in Table 2. After 50K interactions, our method already has a higher mean normalized score than previous sample-efficient methods. Our mean normalized score is higher than DER, CURL, and SimPLe after 25K interactions. This demonstrates the high sample efficiency of our approach.</p>
<p>Stochasticity:</p>
<p>The stochastic prediction of the next state allows the world model to sample a variety of trajectories, even from the same starting state, as can be seen in Figure 12.</p>
<ol>
<li>
<p>Long Sequence Imagination: The world model is trained using sequences of length = 16, however, it generalizes well to very long trajectories, as shown in Figure 13.</p>
</li>
<li>
<p>Frame Stacking: In Figure 14 we visualize the learned stacks of frames. This shows that the world model encodes and predicts the motion of objects.</p>
</li>
</ol>
<p>Additional Ablation Studies:</p>
<ol>
<li>Thresholded Entropy Loss: In Figure 15 we compare (i) our thresholded entropy loss for the policy (see Section 2.2) with (ii) the usual entropy penalty. For (i) we use the same hyperparameters as in our main experiments, i.e., η = 0.01 and Γ = 0.1. For (ii) we set η = 0.001 and Γ = 1.0, which effectively disables the threshold. Without a threshold, the entropy is more likely to either collapse or diverge. When the threshold is used, the score is higher as well, probably because the entropy is in a more sensible range for the explorationexploitation trade-off. This cannot be solved by adjusting the penalty coefficient η alone, since it would increase or decrease the entropy in all games. 2. History Length: We trained our world model with a shorter history and set = 4 instead of = 16. This has a negative impact on the score, as can be seen in Figure 16, demonstrating that more time steps into the past are important. 3. Choice of Policy Input: In Section 2.2 we explained why the input to the policy is only the latent state, i.e., x = z. In Figure 17 we show that using x = [z, h] can result in lower final scores. We hypothesize that the policy network has a hard time keeping up with the changes of the space of h during training and cannot ignore this additional information. 4. Increasing the Sample Efficiency: To find out whether we can further increase the sample efficiency shown in Table 2, we train a random subset of games again on 10K, 25K, and 50K interactions with the full training budget that we used for the 100K interactions. In Figure 18 we see that this can lead to significant improvements in some cases, which could mean that the policy benefits from more training on imagined trajectories, but can even lead to worse performance in other cases, which could possibly be caused by overfitting of the world model. When the performance stays the same even with longer training, this could mean that better exploration in the real environment is required to get further improvements. Wall-Clock Times: For each run, we give the agent a total training and evaluation budget of roughly 10 hours on a single NVIDIA A100 GPU. The time can vary slightly, since the budget is based on the number of updates. An NVIDIA GeForce RTX 3090 requires 12-13 hours for the same amount of training and evaluation. When using a vanilla transformer, which does not use the memory mechanism of the Transformer-XL architecture (Dai et al., 2019), the runtime is roughly 15.5 hours on an NVIDIA A100 GPU, i.e., 1.5 times higher.</li>
</ol>
<p>We compare the runtime of our method with previous methods in Table 3. Our method is more than 20 times faster than SimPLe, but slower than model-free methods. However, our method should be as fast as other model-free methods during inference. In Table 2 we have shown that our method achieves a higher human normalized score than previous sample-efficient methods after 50K interactions. This suggests that our method could potentially outperform previous methods with shorter training, which would take less than 23.3 hours.</p>
<p>To determine how time-consuming the individual parts of our method are, we investigate the throughput of the models, with the batch sizes of our main experiments. The Transformer-XL version is almost twice as fast, which again shows the importance of this design choice. The throughputs were measured on an NVIDIA A100 GPU and are given in (approximate) samples per second:</p>
<p>•  Hafner et al. (2021) propose to use a balanced KL divergence loss to jointly optimize the observation encoder q θ and state predictor p θ with shared parameters θ, i.e., λ D KL (sg(q θ ) p θ ) + (1 − λ) D KL (q θ sg(p θ )),</p>
<p>where sg(·) denotes the stop-gradient operation and λ ∈ [0, 1] controls how much the state predictor adapts to the observation encoder and vice versa. We use the identity D KL (q p) = H(q, p)−H(q), where H(q, p) is the cross-entropy of distribution p relative to distribution q, and show that our loss functions lead to the same gradients as the balanced KL objective, but with finer control over the individual components:
∇ θ [λ D KL (sg(q θ ) p θ ) + (1 − λ) D KL (q θ sg(p θ ))] (8) = ∇ θ [λ (H(sg(q θ ), p θ ) − H(sg(q θ ))) + (1 − λ) (H(q θ , sg(p θ )) − H(q θ ))] (9) = ∇ θ [λ 1 H(sg(q θ ), p θ ) + λ 2 H(q θ , sg(p θ )) − λ 3 H(q θ )],(10)
since ∇ θ H(sg(q θ )) = 0 and by defining λ 1 = λ and λ 2 = 1 − λ and λ 3 = 1 − λ. In this form, we have control over the cross-entropy of the state predictor relative to the observation encoder and vice versa. Moreover, we explicitly penalize the entropy of the observation encoder, instead of being entangled inside of the KL divergence.</p>
<p>As common in the literature, we define the loss function by omitting the gradient in Equation (10), so that automatic differentiation computes this gradient. For our world model, we split the objective into two loss functions, as the observation encoder and state predictor have separate parameters, yielding Equations (3) and (4). s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a  s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a r s a    Figure 15: Effect of disabling the proposed thresholded entropy loss (by setting Γ = 1) on the performance and the entropy in a random subset of games. The thresholded version stabilizes the entropy and leads to a better score in Breakout and Pong, while the entropy behaves unfavorably without a threshold.   Figure 18: Scores on a random subset of games when we train with a lower number of interactions but the same training budget. This only leads to a significant improvement for UpNDown, where the final score is higher with only 50K interactions.  Figure 19: Comparison of the proposed balanced sampling procedure with uniform sampling. It shows the development of the dynamics loss from Equation (4), which is lower at the end of training in all cases.</p>
<p>A.3 ADDITIONAL TRAINING DETAILS</p>
<p>In Algorithm 1 we present pseudocode for training the world model and the actor-critic agent. We use the SiLU activation function (Elfwing et al., 2018) for all models. In Table 4 we summarize all hyperparameters that we used in our experiments. In Table 5 we provide the number of parameters of our models.</p>
<p>Pretraining for Better Initialization: During training we need to correctly balance the amount of world model training and policy training, since the policy has to keep up with the distributional shift of the latent space. However, we can spend some extra training time on the world model with pre-collected data (included in the 100K interactions) at the beginning of training in order to obtain a reasonable initialization for the latent states.</p>
<p>Algorithm 1 Training the world model and the actor-critic agent.   </p>
<p>Figure 1 :
1Our world model architecture.</p>
<p>Figure 2 :
2Comparing our balanced dataset sampling procedure (see Equation</p>
<p>(Figure 4
4Kaiser et al., 2020) implements a world model as an action-conditional video prediction model and trains a policy with PPO(Schulman et al., 2017), (ii) DER (van Hasselt et al., 2019) is a variant of Rainbow(Hessel et al., 2018) fine-tuned for sample efficiency, (iii) CURL(Laskin et al., 2020b) improves representations using contrastive learning as an auxiliary task and is combined with DER, (iv) DrQ(Yarats et al., 2021)  improves DQN by averaging Q-value estimates over multiple data augmentations of observations, and (v) SPR forces representations to be consistent across multiple time steps and data augmentations by extending Rainbow with a self-</p>
<p>Boxing. The player (white) presses fire, hits the opponent, and gets a reward. Freeway. The player moves up and bumps into a car. The world model correctly pushes the player down, although up is still pressed. The movement of the cars is modeled correctly.</p>
<p>Figure 5 :
5Trajectories imagined by our world model. Above each frame we show the performed action and the produced reward.</p>
<p>Figure 6 :
6Attention map of the learned transformer for the current hidden state h, computed on an imagined trajectory for the game Assault. The x-axis corresponds to the input sequence with the three modalities (states, actions, rewards), where the two rightmost columns are the current state and action. The y-axis corresponds to the layer of the transformer.</p>
<p>Boxing. The player (white) presses fire, misses the opponent, gets no reward, and retreats. Freeway. The player moves up, steps back because of an approaching car, and continues to move up. Jamesbond. The player presses fire to fire a missile, and gets a reward when it hits. Gopher. The player (farmer) closes a hole created by the opponent (gopher). The gopher keeps moving independent from the selected actions, indicating that the world model has correctly learned the correlations between the player and the actions. Note that the gopher briefly disappears and reappears on the other side.</p>
<p>Figure 9 :
9Additional example trajectories generated by our world model.Additional Analysis:</p>
<p>world model focuses on previous actions, indicating that the effect of actions can last longer than a single time step. This world model attends to all three modalities in the recent past. This world model attends to states at all time steps, probably because of the complexity of this 3D game.</p>
<p>This world model mainly focuses on four states at specific time steps.</p>
<p>Figure 10 :
10Average attention maps of the transformer, computed over many time steps. They show how different games require a different focus on modalities and time steps.</p>
<p>Figure 11 :Figure 12 :Figure 13 :
111213Attention map for Freeway for a single time step. At this point the player hits a car and gets pushed back (see alsoFigure 5b) and the world model puts more attention to past states and rewards, compared with the average attention at other time steps, as shown inFigure 10e. The world model has learned to handle this situation separately. Three trajectories for the game KungFuMaster generated by our world model, using the same starting state. Because of its stochastic nature, the world model is able to generate three different situations (one opponent, two opponents, one other type of opponent). Note that we only show every third frame to cover more time steps. A long trajectory imagined by our world model for the game Hero. The player traverses five different rooms and the world model is able to correctly predict the state and reward dynamics. Note that we only show every fifth frame to cover more time steps (the rewards lying in-between are summed up). The total number of time steps is 230.</p>
<p>Figure 14 :
14Visualization of frame stacks reconstructed from predicted statesẑ t . Each frame in the stack is visualized by a different color. The world model is able to encode and predict movements.</p>
<p>Figure 16 :
16Comparison of the history length = 16 used in our main experiments with = 4 on a random subset of games. We observe a lower human normalized score for = 4.</p>
<p>Table 1 :
1Mean scores on the Atari 100k benchmark per game as well as the aggregated human normalized mean and median. We perform 5 runs per game and compute the average over 100 episodes at the end of training for each run. Bold numbers indicate the best scores.Model-free 
Imagination </p>
<p>Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998-6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels.In 9th International Conference on Learning Repre-
sentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL 
https://openreview.net/forum?id=GY6-6sTvGaf. </p>
<p>Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games 
with limited data. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, 
and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: 
Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 
6-14, 2021, virtual, pp. 25476-25488, 2021. URL https://proceedings.neurips.cc/ 
paper/2021/hash/d5eca8dc3820cad9fe56a3bafda65ca1-Abstract.html. </p>
<p>Table 2 :
2Performance of our method at different stages of training compared with final scores of previous methods. We show individual game scores and mean human normalized scores. The normalized mean of our method is higher than SimPLe after only 25K interactions, and higher than previous methods after 50K interactions.TWM (ours) </p>
<p>Table 3 :
3Approximate runtime (i.e., training and evaluation time for a single run) of our method 
compared with previous methods that also evaluate on the Atari 100k benchmark. Runtimes of 
previous methods are taken from Schwarzer et al. (2021). They used an improved version of DER 
(van Hasselt et al., 2019), which is roughly equivalent to DrQ (Yarats et al., 2021), so the specified 
runtime might differ from the original DER implementation. There are data augmented versions for 
SPR and DER. All runtimes are measured on a single NVIDIA P100 GPU. </p>
<p>Method 
Model-based Runtime in hours </p>
<p>SimPLe </p>
<p>500 
TWM (ours) </p>
<p>23.3 
SPR (with aug.) </p>
<p>4.6 
SPR (w/o aug.) </p>
<p>3.0 
DER/DrQ (with aug.) </p>
<p>2.1 
DER/DrQ (w/o aug.) </p>
<p>1.4 </p>
<p>UpNDownSteps Human Normalized Scorex = z x = [z, h]Figure 17: Conditioning the policy on [z, h] compared with the usual z. In some cases the performance can be better during training, but the final score is lower or equal.0.2 
0.4 
0.6 
0.8 
1.0 
1e5 </p>
<p>0.000 </p>
<p>0.250 </p>
<p>0.500 </p>
<p>0.750 </p>
<p>1.000 </p>
<p>1.250 </p>
<p>BankHeist </p>
<p>0.2 
0.4 
0.6 
0.8 
1.0 
1e5 </p>
<p>0.000 </p>
<p>0.200 </p>
<p>0.400 </p>
<p>0.600 </p>
<p>0.800 </p>
<p>Breakout </p>
<p>0.2 
0.4 
0.6 
0.8 
1.0 
1e5 </p>
<p>0.000 </p>
<p>0.250 </p>
<p>0.500 </p>
<p>0.750 </p>
<p>1.000 </p>
<p>1.250 </p>
<p>1.500 </p>
<p>KungFuMaster </p>
<p>0.2 
0.4 
0.6 
0.8 
1.0 
1e5 </p>
<p>0.000 </p>
<p>1.000 </p>
<p>2.000 </p>
<p>3.000 </p>
<p>4.000 </p>
<p>5.000 </p>
<p>0.0 
0.2 
0.4 
0.6 
0.8 
1.0 
1e5 </p>
<p>0.000 </p>
<p>2.000 </p>
<p>4.000 </p>
<p>6.000 </p>
<p>Boxing </p>
<p>0.0 
0.2 
0.4 
0.6 
0.8 
1.0 
1e5 </p>
<p>0.000 </p>
<p>1.000 </p>
<p>2.000 </p>
<p>3.000 </p>
<p>CrazyClimber </p>
<p>0.0 
0.2 
0.4 
0.6 
0.8 
1.0 
1e5 </p>
<p>0.000 </p>
<p>0.005 </p>
<p>0.010 </p>
<p>0.015 </p>
<p>0.020 </p>
<p>Seaquest </p>
<p>0.0 
0.2 
0.4 
0.6 
0.8 
1.0 
1e5 </p>
<p>0.000 </p>
<p>1.000 </p>
<p>2.000 </p>
<p>3.000 </p>
<p>4.000 </p>
<p>5.000 </p>
<p>UpNDown </p>
<p>Steps 
Human Normalized Score </p>
<p>100K 
50K 
25K 
10K </p>
<p>// optimize world model via // self-supervised learning optim_observation(o,z,o_hat,z_hat) optim_dynamics(r,d,z,r_hat,d_hat,z_hat)    // z will be used for imagination return z function train_actor_critic(z) // imagine trajectories of states, // rewards, actions and discounts; // use z as starting point imag = [z] for t = 0 until H dofunction train_world_model( ) 
// sample sequences of observations, 
// rewards, actions and discounts 
o,a,r,d = sample_from_dataset() 
z = encode(o) 
o_hat = decode(z) 
h = transformer(z,a,r) 
r_hat,d_hat,z_hat = predict(h) </p>
<p>a = actor(z) 
imag.append(a) 
h = transformer(imag) 
r,d,z = predict(h) 
imag.extend([r,d,z]) </p>
<p>// optimize actor-critic via 
// reinforcement learning 
optim_actor_critic(imag) </p>
<p>Table 4 :
4Hyperparameters used in our experiments.Description 
Symbol 
Value </p>
<p>Dataset sampling temperature 
τ 
20 
Discount factor 
γ 
0.99 
GAE parameter 
λ 
0.95 
World model batch size 
N 
100 
History length 
16 
Imagination batch size 
M 
400 
Imagination horizon 
H 
15 
Encoder entropy coefficient 
α1 
5.0 
Consistency loss coefficient 
α2 
0.01 
Reward coefficient 
β1 
10.0 
Discount coefficient 
β2 
50.0 
Actor entropy coefficient 
η 
0.01 
Actor entropy threshold 
Γ 
0.1 </p>
<h2>Environment steps</h2>
<p>100K 
Frame skip 
-
4 
Frame down-sampling 
-
64 × 64 
Frame gray-scaling 
-
Yes 
Frame stack 
-
4 
Terminate on live loss 
-
Yes 
Max frames per episode 
-
108K 
Max no-ops 
-
30 </p>
<h2>Observation learning rate</h2>
<p>0.0001 
Dynamics learning rate 
-
0.0001 
Actor learning rate 
-
0.0001 
Critic learning rate 
-
0.00001 </p>
<h2>Transformer embedding size</h2>
<p>256 
Transformer layers 
-
10 
Transformer heads 
-
4 × 64 
Transformer feedforward size 
-
1024 
Latent state predictor units 
-
4 × 512 
Reward predictor units 
-
4 × 256 
Discount predictor units 
-
4 × 256 
Actor units 
-
4 × 512 
Critic units 
-
4 × 512 
Activation function 
-
SiLU </p>
<p>Table 5 :
5Number of parameters of our models.Model 
Symbol # Parameters </p>
<p>Observation model 
φ 
8.2M 
Dynamics model 
ψ 
10.8M 
Actor 
θ 
1.3M 
Critic 
ξ 
1.3M </p>
<h2>World model</h2>
<p>19M 
Actor-critic 
-
2.6M </p>
<h2>Total</h2>
<p>21.6M </p>
<p>Encoder + actor 
(at inference time) 
-
4.4M </p>
<p>Quantifying attention flow in transformers. Samira Abnar, Willem H Zuidema, 10.18653/v1/2020.acl-main.385Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreaultthe 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational Linguistics20202020Samira Abnar and Willem H. Zuidema. Quantifying attention flow in transformers. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 4190- 4197. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.385. URL https://doi.org/10.18653/v1/2020.acl-main.385.</p>
<p>Deep reinforcement learning at the edge of the statistical precipice. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, Marc G Bellemare, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021. Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman VaughanRishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C. Courville, and Marc G. Belle- mare. Deep reinforcement learning at the edge of the statistical precipice. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neu- ral Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 29304-29320, 2021. URL https://proceedings.neurips.cc/paper/2021/ hash/f514cec81cb148559cf475e7426eed5e-Abstract.html.</p>
<p>The arcade learning environment: An evaluation platform for general agents. G Marc, Yavar Bellemare, Joel Naddaf, Michael Veness, Bowling, 10.1613/jair.3912J. Artif. Intell. Res. 47Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning envi- ronment: An evaluation platform for general agents. J. Artif. Intell. Res., 47:253-279, 2013. doi: 10.1613/jair.3912. URL https://doi.org/10.1613/jair.3912.</p>
<p>Transdreamer: Reinforcement learning with transformer world models. CoRR, abs/2202.09481, 2022. Chang Chen, Yi-Fu Wu, Jaesik Yoon, Sungjin Ahn, Chang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. Transdreamer: Reinforcement learning with transformer world models. CoRR, abs/2202.09481, 2022. URL https://arxiv.org/ abs/2202.09481.</p>
<p>Decision transformer: Reinforcement learning via sequence modeling. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021. Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman VaughanLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Rein- forcement learning via sequence modeling. In Marc'Aurelio Ranzato, Alina Beygelz- imer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Informa- tion Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 15084- 15097, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ 7f489f642a0ddb10272b5c31057f0663-Abstract.html.</p>
<p>Efficient selectivity and backup operators in monte-carlo tree search. Rémi Coulom, 10.1007/978-3-540-75538-8_7Computers and Games, 5th International Conference. H. Jaap van den Herik, Paolo Ciancarini, and H. H. L. M. DonkersTurin, ItalySpringer4630Revised PapersRémi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In H. Jaap van den Herik, Paolo Ciancarini, and H. H. L. M. Donkers (eds.), Computers and Games, 5th In- ternational Conference, CG 2006, Turin, Italy, May 29-31, 2006. Revised Papers, volume 4630 of Lecture Notes in Computer Science, pp. 72-83. Springer, 2006. doi: 10.1007/978-3-540-75538-8\ _7. URL https://doi.org/10.1007/978-3-540-75538-8_7.</p>
<p>Transformer-xl: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Viet Le, Ruslan Salakhutdinov, 10.18653/v1/p19-1285Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Anna Korhonen, David R. Traum, and Lluís Màrquezthe 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyAssociation for Computational Linguistics1Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhut- dinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Anna Ko- rhonen, David R. Traum, and Lluís Màrquez (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-August 2, 2019, Volume 1: Long Papers, pp. 2978-2988. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1285. URL https://doi.org/10.18653/v1/p19-1285.</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko- reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum? id=YicbFdNTTy.</p>
<p>Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Stefan Elfwing, Eiji Uchibe, Kenji Doya, 10.1016/j.neunet.2017.12.012doi: 10. 1016/j.neunet.2017.12.012Neural Networks. 107Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 107:3-11, 2018. doi: 10. 1016/j.neunet.2017.12.012. URL https://doi.org/10.1016/j.neunet.2017.12.</p>
<p>Recurrent world models facilitate policy evolution. David Ha, Jürgen Schmidhuber, ; Hanna, M Wallach, Hugo Larochelle, Kristen Grauman, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. Nicolò Cesa-Bianchi, and Roman GarnettNeurIPS; Montréal, CanadaSamy Bengio,David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Ro- man Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Confer- ence on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pp. 2455-2467, 2018. URL https://proceedings.neurips.cc/ paper/2018/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html.</p>
<p>Learning latent dynamics for planning from pixels. Danijar Hafner, Timothy P Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, James Davidson, PMLRProceedings of the 36th International Conference on Machine Learning, ICML 2019. Kamalika Chaudhuri and Ruslan Salakhutdinovthe 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USA97Danijar Hafner, Timothy P. Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 2555-2565. PMLR, 2019. URL http://proceedings. mlr.press/v97/hafner19a.html.</p>
<p>Dream to control: Learning behaviors by latent imagination. Danijar Hafner, Timothy P Lillicrap, Jimmy Ba, Mohammad Norouzi, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Danijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In 8th International Conference on Learning Repre- sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=S1lOTC4tDS.</p>
<p>Mastering atari with discrete world models. Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, Jimmy Ba, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaDanijar Hafner, Timothy P. Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview. net/forum?id=0oabwyZbOu.</p>
<p>Rainbow: Combining improvements in deep reinforcement learning. Matteo Hessel, Joseph Modayil, Tom Hado Van Hasselt, Georg Schaul, Will Ostrovski, Dan Dabney, Bilal Horgan, Mohammad Gheshlaghi Piot, David Azar, Silver, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). Sheila A. McIlraith and Kilian Q. Weinbergerthe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)New Orleans, Louisiana, USAAAAI PressMatteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Gheshlaghi Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.), Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pp. 3215-3222. AAAI Press, 2018. URL https://www.aaai.org/ ocs/index.php/AAAI/AAAI18/paper/view/17204.</p>
<p>Offline reinforcement learning as one big sequence modeling problem. Michael Janner, Qiyang Li, Sergey Levine, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021. Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman VaughanMichael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 1273-1286, 2021. URL https://proceedings.neurips.cc/ paper/2021/hash/099fe6b0b444c23836c4a5d07346082b-Abstract.html.</p>
<p>Model based reinforcement learning for atari. Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, Henryk Michalewski, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H. Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, and Henryk Michalewski. Model based reinforcement learning for atari. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/ forum?id=S1xCPJHtDB.</p>
<p>Auto-encoding variational bayes. P Diederik, Max Kingma, Welling, 2nd International Conference on Learning Representations. Yoshua Bengio and Yann LeCunBanff, AB, CanadaConference Track ProceedingsDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/ abs/1312.6114.</p>
<p>Reinforcement learning with augmented data. Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, Aravind Srinivas, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin2020Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Rein- forcement learning with augmented data. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Had- sell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Process- ing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020a. URL https://proceedings.neurips.cc/ paper/2020/hash/e615c82aba461681ade82da2da38004a-Abstract.html.</p>
<p>CURL: contrastive unsupervised representations for reinforcement learning. Michael Laskin, Aravind Srinivas, Pieter Abbeel, PMLRProceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine Learning2020Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: contrastive unsupervised representa- tions for reinforcement learning. In Proceedings of the 37th International Conference on Ma- chine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 5639-5650. PMLR, 2020b. URL http://proceedings. mlr.press/v119/laskin20a.html.</p>
<p>Transformers are sample efficient world models. CoRR. Vincent Micheli, Eloi Alonso, François Fleuret, 10.48550/arXiv.2209.005882022Vincent Micheli, Eloi Alonso, and François Fleuret. Transformers are sample efficient world models. CoRR, abs/2209.00588, 2022. doi: 10.48550/arXiv.2209.00588. URL https://doi.org/ 10.48550/arXiv.2209.00588.</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin A Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, 10.1038/nature14236Nat. 5187540Ioannis Antonoglou. and Demis HassabisVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle- mare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier- stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nat., 518(7540):529-533, 2015. doi: 10.1038/nature14236. URL https://doi.org/10. 1038/nature14236.</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, Proceedings of the 33nd International Conference on Machine Learning. Maria-Florina Balcan and Kilian Q. Weinbergerthe 33nd International Conference on Machine LearningNew York City, NY, USA48Workshop and Conference ProceedingsVolodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Maria-Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19- 24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pp. 1928-1937. JMLR.org, 2016. URL http://proceedings.mlr.press/v48/mniha16.html.</p>
<p>Regularizing neural networks by penalizing confident output distributions. Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, Geoffrey E Hinton, 5th International Conference on Learning Representations. Toulon, FranceWorkshop Track Proceedings. OpenReview.netGabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey E. Hinton. Regu- larizing neural networks by penalizing confident output distributions. In 5th International Con- ference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id= HyhbYrGYe.</p>
<p>Smaller world models for reinforcement learning. Jan Robine, Tobias Uelwer, Stefan Harmeling, abs/2010.05767Jan Robine, Tobias Uelwer, and Stefan Harmeling. Smaller world models for reinforcement learning. CoRR, abs/2010.05767, 2020. URL https://arxiv.org/abs/2010.05767.</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P Lillicrap, David Silver, abs/1911.08265CoRRJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap, and David Silver. Mastering atari, go, chess and shogi by planning with a learned model. CoRR, abs/1911.08265, 2019. URL http://arxiv.org/abs/1911.08265.</p>
<p>Highdimensional continuous control using generalized advantage estimation. John Schulman, Philipp Moritz, Sergey Levine, Michael I Jordan, Pieter Abbeel, 4th International Conference on Learning Representations, ICLR 2016. Yoshua Bengio and Yann LeCunSan Juan, Puerto RicoConference Track ProceedingsJohn Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High- dimensional continuous control using generalized advantage estimation. In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http: //arxiv.org/abs/1506.02438.</p>
<p>Proximal policy optimization algorithms. CoRR, abs/1707.06347. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/ 1707.06347.</p>
<p>Data-efficient reinforcement learning with self-predictive representations. Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron C Courville, Philip Bachman, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaMax Schwarzer, Ankesh Anand, Rishab Goel, R. Devon Hjelm, Aaron C. Courville, and Philip Bachman. Data-efficient reinforcement learning with self-predictive representations. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id= uCQfPZwRaUu.</p>
<p>an integrated architecture for learning, planning, and reacting. S Richard, Sutton, Dyna, 10.1145/122344.122377SIGART Bull. 24Richard S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. SIGART Bull., 2(4):160-163, 1991. doi: 10.1145/122344.122377. URL https://doi.org/10. 1145/122344.122377.</p>
<p>When to use parametric models in reinforcement learning. Matteo Hado Van Hasselt, John Hessel, Aslanides, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, and Roman GarnettNeurIPS; Vancouver, BC, CanadaHado van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in reinforcement learning? In Hanna M. Wallach, Hugo Larochelle, Alina Beygelz- imer, Florence d'Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neu- ral Information Processing Systems 32: Annual Conference on Neural Information Pro- cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 14322-14333, 2019. URL https://proceedings.neurips.cc/paper/2019/ hash/1b742ae215adf18b75449c6e272fd92d-Abstract.html.</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Isabelle Guyon, Ulrike vonAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von</p>            </div>
        </div>

    </div>
</body>
</html>
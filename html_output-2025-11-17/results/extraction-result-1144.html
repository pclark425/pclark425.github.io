<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1144 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1144</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1144</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-2907083</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1103.5708v1.pdf" target="_blank">Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments</a></p>
                <p><strong>Paper Abstract:</strong> To maximize its success, an AGI typically needs to explore its initially unknown world. Is there an optimal way of doing so? Here we derive an affirmative answer for a broad class of environments.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1144.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1144.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Optimal Bayesian Exploration (DP approx.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Optimal Bayesian Exploration via Dynamic-Programming Approximation (curiosity Q-value DP approximation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive exploration agent that maximizes cumulative expected information gain (mutual information between environment parameters and future observations) by computing curiosity Q-values and approximating the optimal policy with dynamic programming / policy iteration on the current Bayesian posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DP approximation of optimal Bayesian exploration</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Maintains a Bayesian posterior (Dirichlet per state-action transition) over unknown MDP dynamics; computes expected immediate information gain g(·) for each state-action; uses policy iteration / dynamic programming on the current posterior treating g as a reward and transition predictive probabilities from the posterior to compute a policy that (approximately) maximizes expected cumulative information gain (the curiosity Q-values).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian experimental design / information-gain maximization (curiosity-driven exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>After each transition the agent updates Dirichlet posterior counts for the visited state-action; it computes g(α_{s,a}) = expected mutual information of one more observation for each state-action, then solves a Bellman-like recursion q = g + γ E_{s'~predictive} max_a' q(s',a') (policy iteration) to obtain a policy that trades off immediate and long-term expected information gain; actions are chosen greedily w.r.t. the computed curiosity Q-values.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Custom two-clique + corridor finite MDP (experiment in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Finite Markovian MDP with unknown transition probabilities (agent fully observes states but environment dynamics unknown), stochastic transitions inside cliques, deterministic transitions along corridor in experiment; reversible connectivity between regions; agent assumes Dirichlet priors over each state-action transition.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>60 discrete states (two cliques of 5 states each plus a corridor of 50 states), 2 actions in experiment, experiment length reported as 4000 steps; generally defined for finite S and A (n_s=60, n_a=2 in main experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Qualitative: achieves the highest cumulative information gain among compared methods (DP approximation outperforms random, greedy, and a Q-learning baseline), particularly in the early phase of exploration; exact numerical values not reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitative: more sample-efficient than greedy, Q-learning and random in the reported MDP: reaches substantially higher cumulative information gain earlier during the 4000-step run; no exact sample counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicitly balances immediate vs long-term information gain via the curiosity Q-value recursion q_tau(h,a) = g(a|h) + γ E_{o|ha}[ v_{τ-1}(hao) ], where γ is a discount; optimization (policy iteration) yields actions that trade off immediate mutual-information reward and future opportunities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Random exploration (uniform actions), Q-learning using immediate information gain as reward, Greedy myopic information-gain maximization (choose argmax_a g(a|h)).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Algorithmic: defines curiosity Q-value (expected cumulative information gain) and proves finite-horizon optimality; shows a DP/policy-iteration approximation (using current posterior predictive dynamics and immediate expected information gain as reward) is a practical approximation; provides uniform convergence bounds and shows approximation error |q_alpha - q^alpha| = O(c_alpha^{-2}) where c_alpha = min α_{s,a} (i.e., error shrinks as counts grow); empirically, DP approximation outperforms baselines on a 60-state MDP, especially early in exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Computational complexity: exact planning cost scales exponentially with lookahead τ (O((n_o n_a)^τ)); DP approximation requires computing expectations over finite observation/action spaces and performing policy iteration (may be expensive for large S,A); needs finite discrete spaces for exact expectations; convergence/meaningful limits require bounded total information content (counterexamples show divergence without further constraints); performance depends on quality of predictive model p(o|ha;θ) and prior; no exact numerical performance metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments', 'publication_date_yy_mm': '2011-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1144.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1144.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Greedy Info-Gain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy (myopic) information-gain maximization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A myopic adaptive exploration policy that at each step chooses the action maximizing the immediate expected information gain (mutual information) about the environment parameters, without planning for long-term information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Greedy exploration</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent computes g(a|h) = I(Θ; O | h,a) for each candidate action given current posterior and selects the action with maximum immediate expected information gain; updates Bayesian posterior (Dirichlet counts) after observing the outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Active learning / information-gain maximization (myopic Bayesian experimental design)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by Bayesian posterior updates after each observation and re-evaluating immediate expected information gain g(a|h); selects action that currently maximizes g, no explicit lookahead or planning beyond the current step.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same custom two-clique + corridor finite MDP (experiment in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Finite Markovian MDP with unknown transition probabilities; agent fully observes states but dynamics unknown; stochastic transitions in cliques and deterministic corridor in the experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>60 discrete states, 2 actions in the reported experiment, 4000-step run.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Qualitative: traverses between the two cliques in the experiment and achieves reasonable information gain, but is outperformed by the DP approximation in cumulative information gain (DP performs best especially early); exact numbers not given.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitative: more sample-efficient than random in the reported environment but less efficient than DP approximation which plans ahead.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Purely exploratory (myopic) — maximizes immediate expected information gain without considering long-term consequences, so may revisit high-gain local areas unless long-term structure is accounted for.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to DP approximation, Q-learning with immediate information reward, and random exploration in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Greedy immediate information maximization performs well at driving exploration and can move between regions, but lacks the long-term planning component that allows DP approximation to collect more cumulative information early; demonstrates importance of planning (balancing immediate vs future info) in dynamic environments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Myopic strategy may waste samples in locally informative regions and fail to trade off exploration paths yielding larger long-term information; no guarantees for global optimality; performance inferior to DP approx. in experiments; may be suboptimal in environments where immediate gain is misleading about long-term benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments', 'publication_date_yy_mm': '2011-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1144.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1144.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q-learning (info reward)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Q-learning using immediate information gain as reward</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning baseline that treats immediate information gain g(a,o|h) as the reward signal and learns action-values with Q-learning, without explicit Bayesian planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Q-learning with immediate information gain reward</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Standard Q-learning algorithm where the per-step reward is the immediate information gain (one-step KL-based mutual information) observed after executing an action and receiving an observation; updates Q(s,a) via temporal-difference learning.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Reinforcement learning with intrinsic reward (information-gain as intrinsic reward)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Learns Q-values from interactions using the immediate information gain as reward; adapts policy as Q estimates change, implicitly preferring actions that historically produced high immediate information gain, but does not perform Bayesian lookahead planning.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same custom two-clique + corridor finite MDP (experiment in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Finite Markovian MDP with unknown transition dynamics; in the experiment corridor transitions are deterministic and clique transitions stochastic; states fully observed but dynamics unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>60 states, 2 actions in experiment, 4000-step run.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Qualitative: in the reported experiment Q-learning becomes stuck in the initial clique and fails to efficiently explore other regions; cumulative information gain is worse than DP and greedy in this environment; exact numeric results not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Poor in experiment — tends to be trapped in regions where immediate information rewards decrease slowly (e.g., clique) and therefore fails to collect diverse informative samples.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balancing via learned Q-values and any exploration policy (e.g., ε-greedy), but because the reward signal is immediate info gain (which can quickly diminish for certain actions), learned Q-values may undervalue actions that are needed to reach informative regions (temporal credit assignment problem).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against DP approximation, greedy info-gain, and random exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Shows that naively inserting immediate information gain as an RL reward can fail: Q-learning got stuck in initial clique in the experiment because jumping into the corridor produced low immediate information gain after a few tries, so corridors were undervalued; highlights necessity of planning (curiosity Q-value) rather than only treating info-gain as standard additive reward.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fails in environments where informative transitions require multi-step planning and immediate information reward is small or transient (e.g., deterministic corridor where initial attempts reduce immediate information to nearly zero); lacks principled Bayesian lookahead and thus can learn misleading Q-values.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments', 'publication_date_yy_mm': '2011-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1144.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1144.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random exploration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uniform random action baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-adaptive baseline that selects actions uniformly at random (no learning-based adaptation), used to compare sample efficiency and cumulative information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Random exploration</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>At each time step selects among available actions uniformly at random; does not maintain or use Bayesian posterior or expected information metrics for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>none (non-adaptive baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>No adaptation; actions chosen i.i.d. uniformly each step.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same custom two-clique + corridor finite MDP (experiment in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Finite Markovian MDP with unknown transitions; random walk behavior can make crossing long corridors slow.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>60 states, 2 actions in experiment, 4000-step run.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Qualitative: performs worse than adaptive methods; in experiment random exploration has difficulty moving between the two cliques due to slow random-walk behavior in the long corridor, yielding lower cumulative information gain than DP and greedy.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Low sample efficiency in the reported MDP — requires many more steps to visit remote regions (corridor traversal) compared to DP and greedy.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>No explicit tradeoff; purely exploratory but inefficient.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Serves as a baseline compared to DP approximation, greedy, and Q-learning with info reward.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Random exploration is outperformed by adaptive strategies; highlights value of directed (information-seeking) exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Inefficient for environments with long traversal sequences or where targeted exploration is needed to reach informative regions (e.g., long corridor connecting clusters).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments', 'publication_date_yy_mm': '2011-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reinforcement driven information acquisition in non-deterministic environments <em>(Rating: 2)</em></li>
                <li>Bayesian experimental design: A review <em>(Rating: 2)</em></li>
                <li>Intrinsically motivated reinforcement learning <em>(Rating: 2)</em></li>
                <li>Curious model-building control systems <em>(Rating: 1)</em></li>
                <li>Active learning literature survey <em>(Rating: 1)</em></li>
                <li>Bayesian surprise attracts human attention <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1144",
    "paper_id": "paper-2907083",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "Optimal Bayesian Exploration (DP approx.)",
            "name_full": "Optimal Bayesian Exploration via Dynamic-Programming Approximation (curiosity Q-value DP approximation)",
            "brief_description": "An adaptive exploration agent that maximizes cumulative expected information gain (mutual information between environment parameters and future observations) by computing curiosity Q-values and approximating the optimal policy with dynamic programming / policy iteration on the current Bayesian posterior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "DP approximation of optimal Bayesian exploration",
            "agent_description": "Maintains a Bayesian posterior (Dirichlet per state-action transition) over unknown MDP dynamics; computes expected immediate information gain g(·) for each state-action; uses policy iteration / dynamic programming on the current posterior treating g as a reward and transition predictive probabilities from the posterior to compute a policy that (approximately) maximizes expected cumulative information gain (the curiosity Q-values).",
            "adaptive_design_method": "Bayesian experimental design / information-gain maximization (curiosity-driven exploration)",
            "adaptation_strategy_description": "After each transition the agent updates Dirichlet posterior counts for the visited state-action; it computes g(α_{s,a}) = expected mutual information of one more observation for each state-action, then solves a Bellman-like recursion q = g + γ E_{s'~predictive} max_a' q(s',a') (policy iteration) to obtain a policy that trades off immediate and long-term expected information gain; actions are chosen greedily w.r.t. the computed curiosity Q-values.",
            "environment_name": "Custom two-clique + corridor finite MDP (experiment in paper)",
            "environment_characteristics": "Finite Markovian MDP with unknown transition probabilities (agent fully observes states but environment dynamics unknown), stochastic transitions inside cliques, deterministic transitions along corridor in experiment; reversible connectivity between regions; agent assumes Dirichlet priors over each state-action transition.",
            "environment_complexity": "60 discrete states (two cliques of 5 states each plus a corridor of 50 states), 2 actions in experiment, experiment length reported as 4000 steps; generally defined for finite S and A (n_s=60, n_a=2 in main experiment).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Qualitative: achieves the highest cumulative information gain among compared methods (DP approximation outperforms random, greedy, and a Q-learning baseline), particularly in the early phase of exploration; exact numerical values not reported in the paper.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Qualitative: more sample-efficient than greedy, Q-learning and random in the reported MDP: reaches substantially higher cumulative information gain earlier during the 4000-step run; no exact sample counts reported.",
            "exploration_exploitation_tradeoff": "Explicitly balances immediate vs long-term information gain via the curiosity Q-value recursion q_tau(h,a) = g(a|h) + γ E_{o|ha}[ v_{τ-1}(hao) ], where γ is a discount; optimization (policy iteration) yields actions that trade off immediate mutual-information reward and future opportunities.",
            "comparison_methods": "Random exploration (uniform actions), Q-learning using immediate information gain as reward, Greedy myopic information-gain maximization (choose argmax_a g(a|h)).",
            "key_results": "Algorithmic: defines curiosity Q-value (expected cumulative information gain) and proves finite-horizon optimality; shows a DP/policy-iteration approximation (using current posterior predictive dynamics and immediate expected information gain as reward) is a practical approximation; provides uniform convergence bounds and shows approximation error |q_alpha - q^alpha| = O(c_alpha^{-2}) where c_alpha = min α_{s,a} (i.e., error shrinks as counts grow); empirically, DP approximation outperforms baselines on a 60-state MDP, especially early in exploration.",
            "limitations_or_failures": "Computational complexity: exact planning cost scales exponentially with lookahead τ (O((n_o n_a)^τ)); DP approximation requires computing expectations over finite observation/action spaces and performing policy iteration (may be expensive for large S,A); needs finite discrete spaces for exact expectations; convergence/meaningful limits require bounded total information content (counterexamples show divergence without further constraints); performance depends on quality of predictive model p(o|ha;θ) and prior; no exact numerical performance metrics provided.",
            "uuid": "e1144.0",
            "source_info": {
                "paper_title": "Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments",
                "publication_date_yy_mm": "2011-03"
            }
        },
        {
            "name_short": "Greedy Info-Gain",
            "name_full": "Greedy (myopic) information-gain maximization",
            "brief_description": "A myopic adaptive exploration policy that at each step chooses the action maximizing the immediate expected information gain (mutual information) about the environment parameters, without planning for long-term information gain.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Greedy exploration",
            "agent_description": "Agent computes g(a|h) = I(Θ; O | h,a) for each candidate action given current posterior and selects the action with maximum immediate expected information gain; updates Bayesian posterior (Dirichlet counts) after observing the outcome.",
            "adaptive_design_method": "Active learning / information-gain maximization (myopic Bayesian experimental design)",
            "adaptation_strategy_description": "Adapts by Bayesian posterior updates after each observation and re-evaluating immediate expected information gain g(a|h); selects action that currently maximizes g, no explicit lookahead or planning beyond the current step.",
            "environment_name": "Same custom two-clique + corridor finite MDP (experiment in paper)",
            "environment_characteristics": "Finite Markovian MDP with unknown transition probabilities; agent fully observes states but dynamics unknown; stochastic transitions in cliques and deterministic corridor in the experiment.",
            "environment_complexity": "60 discrete states, 2 actions in the reported experiment, 4000-step run.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Qualitative: traverses between the two cliques in the experiment and achieves reasonable information gain, but is outperformed by the DP approximation in cumulative information gain (DP performs best especially early); exact numbers not given.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Qualitative: more sample-efficient than random in the reported environment but less efficient than DP approximation which plans ahead.",
            "exploration_exploitation_tradeoff": "Purely exploratory (myopic) — maximizes immediate expected information gain without considering long-term consequences, so may revisit high-gain local areas unless long-term structure is accounted for.",
            "comparison_methods": "Compared to DP approximation, Q-learning with immediate information reward, and random exploration in experiments.",
            "key_results": "Greedy immediate information maximization performs well at driving exploration and can move between regions, but lacks the long-term planning component that allows DP approximation to collect more cumulative information early; demonstrates importance of planning (balancing immediate vs future info) in dynamic environments.",
            "limitations_or_failures": "Myopic strategy may waste samples in locally informative regions and fail to trade off exploration paths yielding larger long-term information; no guarantees for global optimality; performance inferior to DP approx. in experiments; may be suboptimal in environments where immediate gain is misleading about long-term benefit.",
            "uuid": "e1144.1",
            "source_info": {
                "paper_title": "Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments",
                "publication_date_yy_mm": "2011-03"
            }
        },
        {
            "name_short": "Q-learning (info reward)",
            "name_full": "Q-learning using immediate information gain as reward",
            "brief_description": "A reinforcement-learning baseline that treats immediate information gain g(a,o|h) as the reward signal and learns action-values with Q-learning, without explicit Bayesian planning.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Q-learning with immediate information gain reward",
            "agent_description": "Standard Q-learning algorithm where the per-step reward is the immediate information gain (one-step KL-based mutual information) observed after executing an action and receiving an observation; updates Q(s,a) via temporal-difference learning.",
            "adaptive_design_method": "Reinforcement learning with intrinsic reward (information-gain as intrinsic reward)",
            "adaptation_strategy_description": "Learns Q-values from interactions using the immediate information gain as reward; adapts policy as Q estimates change, implicitly preferring actions that historically produced high immediate information gain, but does not perform Bayesian lookahead planning.",
            "environment_name": "Same custom two-clique + corridor finite MDP (experiment in paper)",
            "environment_characteristics": "Finite Markovian MDP with unknown transition dynamics; in the experiment corridor transitions are deterministic and clique transitions stochastic; states fully observed but dynamics unknown.",
            "environment_complexity": "60 states, 2 actions in experiment, 4000-step run.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Qualitative: in the reported experiment Q-learning becomes stuck in the initial clique and fails to efficiently explore other regions; cumulative information gain is worse than DP and greedy in this environment; exact numeric results not provided.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Poor in experiment — tends to be trapped in regions where immediate information rewards decrease slowly (e.g., clique) and therefore fails to collect diverse informative samples.",
            "exploration_exploitation_tradeoff": "Balancing via learned Q-values and any exploration policy (e.g., ε-greedy), but because the reward signal is immediate info gain (which can quickly diminish for certain actions), learned Q-values may undervalue actions that are needed to reach informative regions (temporal credit assignment problem).",
            "comparison_methods": "Compared against DP approximation, greedy info-gain, and random exploration.",
            "key_results": "Shows that naively inserting immediate information gain as an RL reward can fail: Q-learning got stuck in initial clique in the experiment because jumping into the corridor produced low immediate information gain after a few tries, so corridors were undervalued; highlights necessity of planning (curiosity Q-value) rather than only treating info-gain as standard additive reward.",
            "limitations_or_failures": "Fails in environments where informative transitions require multi-step planning and immediate information reward is small or transient (e.g., deterministic corridor where initial attempts reduce immediate information to nearly zero); lacks principled Bayesian lookahead and thus can learn misleading Q-values.",
            "uuid": "e1144.2",
            "source_info": {
                "paper_title": "Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments",
                "publication_date_yy_mm": "2011-03"
            }
        },
        {
            "name_short": "Random exploration",
            "name_full": "Uniform random action baseline",
            "brief_description": "A non-adaptive baseline that selects actions uniformly at random (no learning-based adaptation), used to compare sample efficiency and cumulative information gain.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Random exploration",
            "agent_description": "At each time step selects among available actions uniformly at random; does not maintain or use Bayesian posterior or expected information metrics for action selection.",
            "adaptive_design_method": "none (non-adaptive baseline)",
            "adaptation_strategy_description": "No adaptation; actions chosen i.i.d. uniformly each step.",
            "environment_name": "Same custom two-clique + corridor finite MDP (experiment in paper)",
            "environment_characteristics": "Finite Markovian MDP with unknown transitions; random walk behavior can make crossing long corridors slow.",
            "environment_complexity": "60 states, 2 actions in experiment, 4000-step run.",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "Qualitative: performs worse than adaptive methods; in experiment random exploration has difficulty moving between the two cliques due to slow random-walk behavior in the long corridor, yielding lower cumulative information gain than DP and greedy.",
            "sample_efficiency": "Low sample efficiency in the reported MDP — requires many more steps to visit remote regions (corridor traversal) compared to DP and greedy.",
            "exploration_exploitation_tradeoff": "No explicit tradeoff; purely exploratory but inefficient.",
            "comparison_methods": "Serves as a baseline compared to DP approximation, greedy, and Q-learning with info reward.",
            "key_results": "Random exploration is outperformed by adaptive strategies; highlights value of directed (information-seeking) exploration.",
            "limitations_or_failures": "Inefficient for environments with long traversal sequences or where targeted exploration is needed to reach informative regions (e.g., long corridor connecting clusters).",
            "uuid": "e1144.3",
            "source_info": {
                "paper_title": "Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments",
                "publication_date_yy_mm": "2011-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reinforcement driven information acquisition in non-deterministic environments",
            "rating": 2,
            "sanitized_title": "reinforcement_driven_information_acquisition_in_nondeterministic_environments"
        },
        {
            "paper_title": "Bayesian experimental design: A review",
            "rating": 2,
            "sanitized_title": "bayesian_experimental_design_a_review"
        },
        {
            "paper_title": "Intrinsically motivated reinforcement learning",
            "rating": 2,
            "sanitized_title": "intrinsically_motivated_reinforcement_learning"
        },
        {
            "paper_title": "Curious model-building control systems",
            "rating": 1,
            "sanitized_title": "curious_modelbuilding_control_systems"
        },
        {
            "paper_title": "Active learning literature survey",
            "rating": 1,
            "sanitized_title": "active_learning_literature_survey"
        },
        {
            "paper_title": "Bayesian surprise attracts human attention",
            "rating": 1,
            "sanitized_title": "bayesian_surprise_attracts_human_attention"
        }
    ],
    "cost": 0.013655999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments</p>
<p>Yi Sun 
Faustino Gomez 
Jürgen Schmidhuber 
Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments
646996A16A85AF0788FA78F258AF1E23
To maximize its success, an AGI typically needs to explore its initially unknown world.Is there an optimal way of doing so?Here we derive an affirmative answer for a broad class of environments.</p>
<p>Introduction</p>
<p>An intelligent agent is sent to explore an unknown environment.Over the course of its mission, the agent makes observations, carries out actions, and incrementally builds up a model of the environment from this interaction.Since the way in which the agent selects actions may greatly affect the efficiency of the exploration, the following question naturally arises:</p>
<p>How should the agent choose the actions such that the knowledge about the environment accumulates as quickly as possible?</p>
<p>In this paper, this question is addressed under a classical framework, in which the agent improves its model of the environment through probabilistic inference, and learning progress is measured in terms of Shannon information gain.We show that the agent can, at least in principle, optimally choose actions based on previous experiences, such that the cumulative expected information gain is maximized.We then consider a special case, namely exploration in finite MDPs, where we demonstrate, both in theory and through experiment, that the optimal Bayesian exploration strategy can be effectively approximated by solving a sequence of dynamic programming problems.</p>
<p>The rest of the paper is organized as follows: Section 2 reviews the basic concepts and establishes the terminology; Section 3 elaborates the principle of optimal Bayesian exploration; Section 4 focuses on exploration in finite MDP; Section 5 presents a simple experiment; The related works are briefly reviewed in Section 6; Section 7 concludes the paper.</p>
<p>Preliminaries</p>
<p>Suppose that the agent interacts with the environment in discrete time cycles t = 1, 2, . ... In each cycle, the agent performs an action a, then receives a sensory input o.A history h is either the empty string ∅ or a string of the form a 1 o 1 • • • a t o t for some t, and ha and hao refer to the strings resulting from appending a and ao to h, respectively.arXiv:1103.5708v1[cs.AI] 29 Mar 2011</p>
<p>Learning from Sequential Interactions</p>
<p>To facilitate the subsequent discussion under a probabilistic framework, we make the following assumptions: Assumption I.The models of the environment under consideration are fully described by a random element Θ which depends solely on the environment.Moreover, the agent's initial knowledge about Θ is summarized by a prior density p (θ). Assumption II.The agent is equipped with a conditional predictor p (o|ha; θ), i.e. the agent is capable of refining its prediction in the light of information about Θ.</p>
<p>Using p (θ) and p (o|ha; θ) as building blocks, it is straightforward to formulate learning in terms of probabilistic inference.From Assumption I, given the history h, the agent's knowledge about Θ is fully summarized by p (θ|h).According to Bayes rule, p (θ|hao) = p(θ|ha)p(o|ha;θ) p(o|ha)</p>
<p>, with p (o|ha) = p (o|ha, θ) p (θ|h) dθ.The term p (θ|ha) represents the agent's current knowledge about Θ given history h and an additional action a.Since Θ depends solely on the environment, and, importantly, knowing the action without subsequent observations cannot change the agent's state of knowledge about Θ, p (θ|ha) = p (θ|h), hence the knowledge about Θ can be updated using
p (θ|hao) = p (θ|h) • p (o|ha; θ) p (o|ha) .(1)
It is worth pointing out that p (o|ha; θ) is chosen a priori.It is not required that they match the true dynamics of the environment, but the effectiveness of the learning certainly depends on the choices of p (o|ha; θ).For example, if Θ ∈ R, and p (o|ha; θ) depends on θ only through its sign, then no knowledge other than the sign of Θ can be learned.</p>
<p>Information Gain as Learning Progress</p>
<p>Let h and h be two histories such that h is a prefix of h .The respective posterior of Θ are p (θ|h) and p (θ|h ).Using h as a reference point, the amount of information gained when the history grows to h , can be measured using the KL divergence between p (θ|h) and p (θ|h ).This information gain from h to h is defined as g(h h) = KL (p (θ|h ) p (θ|h)) = p (θ|h ) log p (θ|h ) p (θ|h) dθ.</p>
<p>As a special case, if h = ∅, then g (h ) = g (h ∅) is the cumulative information gain with respect to the prior p (θ).We also write g (ao h) for g (hao h), which denotes the information gained from an additional action-observation pair.</p>
<p>From an information theoretic point of view, the KL divergence between two distributions p and q represents the additional number of bits required to encode elements sampled from p, using optimal coding strategy designed for q.This can be interpreted as the degree of 'unexpectedness' or 'surprise' caused by observing samples from p when expecting samples from q.</p>
<p>The key property information gain for the treatment below is the following decomposition: Let h be a prefix of h and h be a prefix of h , then
E h |h g (h h) = E h |h p (θ|h ) log p (θ|h ) p (θ|h) dθ = E h |h p (θ|h ) log p (θ|h ) p (θ|h ) + log p (θ|h ) p (θ|h) dθ = E h |h p (θ|h ) log p (θ|h ) p (θ|h ) dθ + E h |h p (θ|h ) log p (θ|h ) p (θ|h) dθ = E h |h g (h h ) + E h |h p (θ|h ) log p (θ|h ) p (θ|h) dθ.
From updating formula Eq.1,
o p (o|ha) p (θ|hao) = o p (θ|h) p (o|ha, θ) = p (θ|h) o p (o|ha, θ) = p (θ|h) .
Using this relation recursively,
E h |h p (θ|h ) = a1 o1 • • • at ot p (θ|h a 1 o 1 • • • a t o t ) = a1 o1 • • • at−1 ot−1 p (θ|h a 1 o 1 • • • a t−1 o t−1 ) = • • • = p (θ|h ) , therefore E h |h g (h h) = g (h h) + E h |h g (h h ) . (2)
That is, the information gain is additive in expectation.</p>
<p>Having defined the information gain from trajectories ending with observations, one may proceed to define the expected information gain of performing action a, before observing the outcome o.Formally, the expected information gain of performing a with respect to the current history h is given by g (a h) = E o|ha g (ao h).A simple derivation gives
g (a h) = o p (o|ha) p (θ|hao) log p (θ|hao) p (θ|h) dθ = o p (o, θ|ha) log p (θ|hao) p (o|ha) p (θ|h) p (o|ha) dθ = o p (o, θ|ha) log p (o, θ|ha) p (θ|ha) p (o|ha) dθ = I (O; Θ|ha) ,
which means that g (a h) is the mutual information between Θ and the random variable O representing the unknown observation, conditioned on the history h and action a. 1</p>
<p>Optimal Bayesian Exploration</p>
<p>In this section, the general principle of optimal Bayesian exploration in dynamic environments is presented.We first give results obtained by assuming a fixed limited life span for our agent, then discuss a condition required to extend this to infinite time horizons.</p>
<p>Results for Finite Time Horizon</p>
<p>Suppose that the agent has experienced history h, and is about to choose τ more actions in the future.Let π be a policy mapping the set of histories to the set of actions, such that the agent performs a with probability π (a|h) given h.Define the curiosity Q-value q τ π (h, a) as the expected information gained from the additional τ actions, assuming that the agent performs a in the next step and follows policy π in the remaining τ − 1 steps.Formally, for τ = 1,
q 1 π (h, a) = E o|ha g (ao h) = g (a h) ,
and for τ &gt; 1,
q τ π (h, a) = E o|ha E a1|hao E o1|haoa1 • • • E oτ−1|h•••aτ−1 g (haoa 1 o 1 • • • a τ −1 o τ −1 h) = E o|ha E a1o1•••aτ−1oτ−1|hao g (haoa 1 o 1 • • • a τ −1 o τ −1 h) .
The curiosity Q-value can be defined recursively.Applying Eq. 2 for τ = 2,
q τ π (h, a) = E o|ha E a1o1|hao g (haoa 1 o 1 h) = E o|ha g (ao h) + E a1o1|hao g (a 1 o 1 hao) = g (a h) + E o|ha E a |hao q 1 π (hao, a ) .
And for τ &gt; 2,
q τ π (h, a) = E o|ha E a1o1•••aτ−1oτ−1|hao g (haoa 1 o 1 • • • a τ −1 o τ −1 h) = E o|ha g (ao h) + E a1o1•••aτ−1oτ−1 g (haoa 1 o 1 • • • a τ −1 o τ −1 hao) = g (a h) + E o|ha E a |hao q τ −1 π (hao, a ) .(3)
Noting that Eq.3 bears great resemblance to the definition of state-action values (Q(s, a)) in reinforcement learning, one can similarly define the curiosity value of a particular history as v τ π (h) = E a|h q τ π (h, a), analogous to state values (V (s)), which can also be iteratively defined as v 1 π (h) = E a|h g (a h), and
v τ π (h) = E a|h g (a h) + E o|ha v τ −1 π (hao) .
The curiosity value v τ π (h) is the expected information gain of performing the additional τ steps, assuming that the agent follows policy π.The two notations can be combined to write
q τ π (h, a) = g (a h) + E o|ha v τ −1 π (hao) . (4)
This equation has an interesting interpretation: since the agent is operating in a dynamic environment, it has to take into account not only the immediate expected information gain of performing the current action, i.e., g (a h), but also the expected curiosity value of the situation in which the agent ends up due to the action, i.e., v τ −1 π (hao).As a consequence, the agent needs to choose actions that balance the two factors in order to improve its total expected information gain.Now we show that there is a optimal policy π * , which leads to the maximum cumulative expected information gain given any history h.To obtain the optimal policy, one may work backwards in τ , taking greedy actions with respect to the curiosity Q-values at each time step.Namely, for τ = 1, let
q 1 (h, a) = g (a h) , π 1 * (h) = arg max a g (a h) , and v 1 (h) = max a g (a h) , such that v 1 (h) = q 1 h, π 1 (h)
, and for τ &gt; 1, let
q τ (h, a) = g (a h) + E o|ha max a q τ −1 (a |hao) = g (a h) + E o|ha v τ −1 (hao) ,
with π τ * (h) = arg max a q τ (h, a) and v τ (h) = max a q τ (h, a).We show that π τ * (h) is indeed the optimal policy for any given τ and h in the sense that the curiosity value, when following π τ * , is maximized.To see this, take any other strategy π, first notice that
v 1 (h) = max a g (a h) ≥ E a|h g (a h) = v 1 π (h) . Moreover, assuming v τ (h) ≥ v τ π (h), v τ +1 (h) = max a g (a h) + E o|ha v τ (hao) ≥ max a g (a h) + E o|ha v τ π (hao) ≥ E a|h g (a h) + E o|ha v τ π (hao) = v τ +1 π (h) .
Therefore v τ (h) ≥ v τ π (h) holds for arbitrary τ , h, and π.The same can be shown for curiosity Q-values, namely, q τ (h, a) ≥ q τ π (h, a), for all τ , h, a, and π.It may be beneficial to write q τ in explicit forms, namely,
q τ (h, a) = E o|ha max a1 E o1|haoa1 • • • max aτ−1 E oτ−1|h•••aτ−1 g (haoa 1 o 1 • • • a τ −1 o τ −1 h) .
Now consider that the agent has a fixed life span T .It can be seen that at time t, the agent has to perform π T −t * (h t−1 ) to maximize the expected information gain in the remaining T − t steps.Here
h t−1 = a 1 o 1 • • • a t−1 o t−1
is the history at time t.However, from Eq.2,
E h T |ht−1 g (h T ) = g (h t−1 ) + E h T |ht−1 g (h T h t−1 ) .
Note that at time t, g (h t−1 ) is a constant, thus maximizing the cumulative expected information gain in the remaining time steps is equivalent to maximizing the expected information gain of the whole trajectory with respect to the prior.The result is summarized in the following proposition: Proposition 1.Let q 1 (h, a) = ḡ (a h), v 1 (h) = max a q 1 (h, a), and
q τ (h, a) = ḡ (a h) + E o|ha v τ −1 (hao) , v τ (h) = max a q τ (h, a) ,
then the policy π * τ (h) = arg max a q τ (h, a) is optimal in the sense that v τ (h) ≥ v π τ (h), q τ (h, a) ≥ q π τ (h, a) for any π, τ , h and a.In particular, for an agent with fixed life span T , following π * T −t (h t−1 ) at time t = 1, . . ., T is optimal in the sense that the expected cumulative information gain with respect to the prior is maximized.</p>
<p>Non-triviality of the Result</p>
<p>Intuitively, the interpretation of the recursive definition of the curiosity (Q) value is simple, and bears clear resemblance to their counterparts in reinforcement learning.It might be tempting to think that the result is nothing more than solving the finite horizon reinforcement learning problem using g (a h) or g (ao h) as the reward signals.However, this is not the case.</p>
<p>First, note that the decomposition Eq.2 is a direct consequence of the formulation of the KL divergence.The decomposition does not necessarily hold if g (h) is replaced with other types of measures of information gain.</p>
<p>Second, it is worth pointing out that g (ao h) and g (a h) behave differently from normal reward signals in the sense that they are additive only in expectation, while in the reinforcement learning setup, the reward signals are usually assumed to be additive, i.e., adding reward signals together is always meaningful.Consider a simple problem with only two actions.If g (ao h) is a plain reward function, then g (ao h) + g (a o hao) should be meaningful, no matter if a and o is known or not.But this is not the case, since the sum does not have a valid information theoretic interpretation.On the other hand, the sum is meaningful in expectation.Namely, when o has not been observed, from Eq.2,
g (ao h) + E o |haoa g (a o hao) = E o |haoa g (aoa o h) ,
the sum can be interpreted as the expectation of the information gained from h to haoa o .This result shows that g (a h) or g (ao h) can be treated as additive reward signals only when one is planning ahead.</p>
<p>To emphasize the difference further, note that all immediate information gains g (ao h) are non-negative since they are essentially KL divergence.A natural assumption would be that the information gain g (h), which is the sum of all g (ao h) in expectation, grows monotonically when the length of the history increases.However, this is not the case, see Figure 1 for example.Although g (ao h) is always non-negative, some of the gain may pull θ closer to its prior density p (θ), resulting in a decrease of KL divergence between p (θ|h) and p (θ).This is never the case if one considers the normal reward signals in reinforcement learning, where the accumulated reward would never decrease if all rewards are non-negative.</p>
<p>The Algorithm</p>
<p>The definition of the optimal exploration policy is constructive, which means that it can be readily implemented, provided that the number of actions and possible observations is finite so that the expectation and maximization can be computed exactly.</p>
<p>The following two algorithms computes the maximum curiosity value v τ (h) and the maximum curiosity Q-value q τ (h, a), respectively, assuming that the expected immediate gain g (a h) can be computed.Illustration of the difference between the sum of one-step information gain and the cumulative information gain with respect to the prior.In this case, 1000 independent samples are generated from a distribution over finite sample space {1, 2, 3}, with p (x = 1) = 0.1, p (x = 2) = 0.5, and p (x = 3) = 0.4.The task of learning is to recover the mass function from the samples, assuming a Dirichlet prior Dir 50 3 , 50 3 , 50 3 .The KL divergence between two Dirichlet distributions are computed according to [6].It is clear from the graph that the cumulative information gain fluctuates when the number of samples increases, while the sum of the one-step information gain increases monotonically.</p>
<p>It also shows that the difference between the two quantities can be large.
CuriosityValue(h, τ ) Input: history h, look-ahead τ Output: curiosity value v τ (h) 1 v ← 0 2 for all possible a 3 v ← max (v, CuriosityQValue (h, a, τ )) 4 end for 5 return v CuriosityQValue(h, a, τ ) Input: history h, action a, look-ahead τ Output: curiosity Q-value q τ (h, a) 1 q ← g (a h) 2 if τ = 0 3 for all possible o 4 q ← q + p (o|ha) •CuriosityValue(hao, τ − 1) 5 end for 6 end if 7 return q
The complexity of both CuriosityValue and CuriosityQValue are O ((n o n a )</p>
<p>τ ), where n o and n a are the number of possible observations and actions, respectively.Since the cost is exponential on τ , planning with large number of look ahead steps is infeasible, and approximation heuristics must be used in practice.</p>
<p>Extending to Infinite Horizon</p>
<p>Having to restrict the maximum life span of the agent is rather inconvenient.It is tempting to define the curiosity Q-value in the infinite time horizon case as the limit of curiosity Q-values with increasing life spans, T → ∞.However, this cannot be achieved without additional technical constraints.For example, consider simple coin tossing.Assuming a Beta (1, 1) over the probability of seeing heads, then the expected cumulative information gain for the next T flips is given by v
T (h 1 ) = I (Θ; X 1 , . . . , X T ) ∼ log T . With increasing T , v T (h 1 ) → ∞.
A frequently used approach to simplifying the math is to introduce a discount factor γ, as used in reinforcement learning.Assume that the agent has a maximum τ actions left, but before finishing the τ actions it may be forced to leave the environment with probability 1 − γ (0 ≤ γ &lt; 1) at each time step.In this case, the curiosity Q-value becomes q 1,γ π (h, a) = g (a h), and
q τ,γ π (h, a) = (1 − γ) g (a h) + γ g (a h) + E o|ha E a |hao q τ −1,γ π (hao, a ) = g (a h) + γE o|ha E a |hao q τ −1,γ π (hao, a ) .
One may also interpret q τ,γ π (h, a) as a linear combination of curiosity Q-values without the discount,
q τ,γ π (h, a) = (1 − γ) τ t=1 γ t−1 q t π (h, a) + γ τ q τ π (h, a) ,
Note that curiosity Q-values with larger look-ahead steps are weighed exponentially less.The optimal policy in the discounted case is given by
q 1,γ (h, a) = g (a h) , v 1,γ (h) = max a q 1,γ (h, a) ,andq τ,γ (h, a) = g (a h) + γE o|ha v τ −1,γ (hao) , v τ,γ (h) = max a q τ,γ (h, a) .
The optimal actions are given by π τ,γ * (h) = arg max a q τ,γ (h, a).The proof that π τ,γ * is optimal is similar to the one for no-discount case and thus is omitted here.</p>
<p>Adding the discount enables one to define the curiosity Q-value in infinite time horizon in a number of cases.However, it is still possible to construct scenarios where such discount fails.Consider a infinite list of bandits.For bandit n, there are n possible outcomes with Dirichlet prior Dir 1 n , . . ., 1 n .The expected information gain of pulling bandit n for the first time is then given by
log n − ψ (2) + log 1 + 1 n ∼ log n.
Assume at time t, only the first e e 2t bandits are available, thus the curiosity Qvalue in finite time horizon is always finite.However, since the largest expected information gain grows at speed e t 2 , for any given γ &gt; 0, q τ,γ goes to infinity with increasing τ .This example gives the intuition that to make the curiosity Q-value meaningful, the 'total information content' of the environment (or its growing speed) must be bounded.</p>
<p>The following two Lemmas are useful for later discussion.
Lemma 1. q τ +1,γ π (h, a) − q τ,γ π (h, a) = γ τ E oa1•••oτ−1aτ |ha g (a τ h • • • o τ −1 ).
Proof.Expand q τ,γ π and q τ +1,γ π ,
q τ +1,γ π − q τ,γ π = (1 − γ) τ +1 t=1 γ t−1 q t π + γ τ +1 q τ +1 π − (1 − γ) τ t=1 γ t−1 q t π − γ τ q τ π = γ τ q τ +1 π − q τ π .
By definition,
q τ +1 π − q τ π = E o|ha E a1o1•••aτ oτ |hao g (haoa 1 o 1 • • • a τ o τ h) − E o|ha E a1o1•••aτ−1oτ−1|hao g (haoa 1 o 1 • • • a τ −1 o τ −1 h) = E o|ha E a1o1•••aτ−1oτ−1|hao E aτ oτ |h•••oτ−1 g (haoa 1 o 1 • • • a τ o τ h) − g (haoa 1 o 1 • • • a τ −1 o τ −1 h) .
Using Eq.2,
E aτ oτ |h•••oτ−1 g (haoa 1 o 1 • • • a τ o τ h) − g (haoa 1 o 1 • • • a τ −1 o τ −1 h) = E aτ |h•••oτ−1 g (a τ h • • • o τ −1 ) , thus q τ +1,γ π − q τ,γ π = γ τ E o|ha E a1o1•••aτ−1oτ−1|hao E aτ |h•••oτ−1 g (a τ h • • • o τ −1 ) = γ τ E oa1•••oτ−1aτ |ha g (a τ h • • • o τ −1 ) . Lemma 2. q τ +1,γ (h, a)−q τ,γ (h, a) ≤ γ τ E o|ha max a1 E o1|haoa1 • • • max aτ g (a τ h • • • o τ −1 ).
Proof.Expand q τ,γ and q τ +1,γ , and note that max
X − max Y ≤ max |X − Y |, then q τ +1,γ (h, a) − q τ,γ (h, a) = E o|ha max a1 E o1|haoa1 • • • max aτ [g (a h) + γg (a 1 hao) + • • • + γ τ g (a τ h • • • o τ −1 )] − E o|ha max a1 E o1|haoa1 • • • max aτ−1 g (a h) + γg (a 1 hao) + • • • + γ τ −1 g (a τ −1 h • • • o τ −2 ) ≤ E o|ha max a1 {E o1|haoa1 • • • max aτ [g (a h) + γg (a 1 hao) + • • • + γ τ g (a τ h • • • o τ −1 )] − E o1|haoa1 • • • max aτ−1 g (a h) + γg (a 1 hao) + • • • + γ τ −1 g (a τ −1 h • • • o τ −2 ) } ≤ • • • ≤ γ τ E o|ha max a1 E o1|haoa1 • • • max aτ g (a τ h • • • o τ −1 ) .
It can be seen that if
E oa1•••oτ−1aτ |ha g (a τ h • • • o τ −1
) is bounded, then both q γ,τ π and q γ,τ are a Cauchy sequences with respect to τ .</p>
<p>Exploration in Finite Markovian Environment with Dirichlet Priors</p>
<p>In this section we restrict the discussion to a simple case, where the possible actions and sensory inputs are finite, and the agent assumes that the environment is Markovian, namely, the current sensory input and action is sufficient for determining the (probabilities of the) next sensory inputs.Formally, let S = {1, • • • , S} and A = {1, • • • , A} be the space of possible sensory inputs, to which we referred as 'states', and actions.The dynamics of the environment is fully determined by the transition probability p (s |s, a).</p>
<p>The agent tries to learn the transition probabilities.Initially, it assumes for each s, a a Dirichlet prior over the random variable Θ s,a corresponding to the distribution p (•|s, a).Through time, the agent observes the transitions when performing a at s, and updates its estimate of Θ s,a through probabilistic inference.Since the Dirichlet distribution is conjugate with multinomial distribution, the posterior is still a Dirichlet distribution over Θ s,a .Therefore, at any time, the agent's knowledge about the environment can be fully summarized by a three dimensional array α (s, a, s ), such that Dir (α s,a,1 , • • • , α s,a,S ) is the current (prior or posterior) density of Θ s,a , and the definition of the optimal curiosity Q-value can be written as2
q γ,τ α (s, a) = g (α s,a ) + γ s p α (s |s, a) max a q γ,τ −1 α s,a,s (s , a ) = g (α s,a ) + γ s α s,a,s α s,a max a q γ,τ −1 α s,a,s (s , a ) .
Here g (α s,a ) is the expected immediate information gain for Θ s,a given the current parameterization of the Dirichlet distribution.By definition, g (α s,a ) is also the mutual information between Θ and an additional observation.According to [6], the precise form of g is given by
g ([n 1 , • • • , n S ]) = log n * − ψ (n * + 1) − S s=1 n s n * [log n s − ψ (n s + 1)] ,
where n * = s n s , and ψ (•) is the standard digamma function.By marginalizing out Θ s,a , the predictive probability is given by p α (s |s, a) = α s,a,s αs,a , and is the operator3 such that α s, a, s is the same as α, except that the entry indexed by s, a, s is increased by 1.Similarly, for a given policy π, the curiosity Q-value can be written as
q γ,τ π,α (s, a) = g (α s,a ) + γ s α s,a,s α s,a a π α (a |s ) q γ,τ −1 π,α s,a,s (s , a ) .</p>
<p>Curiosity Q-value in Infinite Time Horizon</p>
<p>In this subsection we extend the definition of curiosity Q-value to infinite horizon.We show that a) the limit lim τ →∞ q γ,τ π,α exists, b) the limit lim τ →∞ q γ,τ α exists, and c) the limit is the solution of the infinite recursion.</p>
<p>Proposition 2. q γ π,α (s, a) = lim τ →∞ q γ,τ π,α (s, a) exists for any π, α, s, a, and γ ∈ [0, 1).Moreover, the convergence is uniform with respect to s, a in the sense that 0 ≤ q γ π,α (s, a) − q γ,τ π,α (s, a) ≤
g α 1 − γ γ τ , ∀s, a
where g α = max s max a g (α s,a ).</p>
<p>Proof.Rewrite the result in Lemma.1 in this context:
q γ,τ +1 π,α (s, a) − q γ,τ π,α (s, a) = γ τ E oa1•••oτ−1aτ |ha g (a τ h • • • o τ −1 ) = γ τ E s1a2s2•••sτ aτ+1|ha g α sτ aτ+1
, where
α = α s, a, s 1 s 1 , a 2 , s 2 • • • s τ −1 , a τ , s τ .
Because g α s,a depends only on the transitions when performing a at s, and all such transitions are exchangeable since they are assumed to be i.i.d.when Θ s,a is given, one can rewrite the expectation in the following form:
E s1a2s2•••sτ aτ+1|ha g α sτ aτ+1 = E s E a E n E x1 • • • E xn g α s,a .
The first and second expectations are taken over the possible final state-action pairs s τ , a τ +1 , from which g α sτ aτ+1 is computed.Once s, a is fixed, the third expectation is taken over the time n that s, a -pair appears in the trajectory sas 1 a 2 • • • s τ , i.e., the time that transitions starting from s with action a occurs.The rest of the expectations are over the n destinations of the transitions, denoted as x 1 , • • • , x n .By definition, Dir α s,a is the posterior distribution after seeing x 1 , • • • , x n , and g α s,a is the expected information gain of seeing the outcome of the (n + 1)-th transition, which we denote x n+1 , thus
g α s,a = I (Θ s,a ; X n+1 |x 1 , • • • , x n ) ,
and
E x1 • • • E xn g α s,a = I (Θ s,a ; X n+1 |X 1 , • • • , X n ) .
Note that X 1 , • • • , X n+1 are i.i.d.given Θ s,a , therefore
I (Θ s,a ; X n+1 |X 1 , • • • , X n ) = I (Θ s,a ; X 1 , • • • , X n+1 ) − I (Θ s,a ; X 1 , • • • , X n ) = H (X 1 , • • • , X n+1 ) − n+1 i=1 H (X i |Θ) − H (X 1 , • • • , X n ) + n i=1 H (X i |Θ) = H (X n+1 |X 1 , • • • , X n ) − H (X n+1 |Θ) ≤ H (X n+1 ) − H (X n+1 |Θ) = I (Θ; X n+1 ) = I (Θ; X 1 ) .
This means that I (Θ s,a ;
X n+1 |X 1 , • • • , X n ) is upper bounded by I (Θ; X 1 )
, which is the expected information gain of seeing the outcome of the transition for the first time.By definition I (Θ; X 1 ) = g (α s,a ), and it follows that
E n E x1 • • • E xn g α s,a ≤ g (α s,a ) .
Therefore,
q γ,τ +1 π,α (s, a) − q γ,τ π,α (s, a) = γ τ E s1a2s2•••sτ aτ+1|ha g α sτ aτ+1 = γ τ E s E a E n E x1 • • • E xn g α s,a ≤ γ τ E s E a g (α s,a ) ≤ γ τ max s max a g (α s,a ) ≤ γ τ g α .
Since g α depends on α only, for any T
q γ,τ +T π,α (s, a) − q γ,τ π,α (s, a) ≤ g α 1 − γ γ τ .
This means that q γ,τ π,α (s, a) is a Cauchy sequence with respect to τ , thus lim τ →∞ q γ,τ π,α (s, a) exists.Also note that the convergence is uniform since g α does not depend on s, a .Proposition 3. q γ α (s, a) = lim τ →∞ q γ,τ α (s, a) exists for any α, s, a, and γ ∈ [0, 1).Also the convergence is uniform in the sense that
0 ≤ q γ (s, a) − q γ,τ α (s, a) ≤ g α 1 − γ γ τ .
Proof.Rewrite the result in Lemma.2,
q γ,τ +1 α (s, a) − q γ,τ α (s, a) ≤ γ τ E s1|sa max a2 E s2|s1a2,α s,a,s1 • • • E sτ |sτ−1aτ ,α sas1•••sτ−1 max aτ+1 g α sτ ,ατ+1 .
Since the max operator is only over actions, the proof in the previous proposition still holds, so q γ,τ +1 α (s, a) − q γ,τ α (s, a) ≤ γ τ g α , and the result follows.</p>
<p>The next proposition shows that q γ α is the solution to the infinite recursion.Proposition 4. q γ α is the solution to the recursion
q γ α (s, a) = g (α s,a ) + γ s α s,a,s α s,a max a q γ α s,a,s (s , a ) ,
and for any other policy π, q γ α (s, a) ≥ q γ π,α (s, a).</p>
<p>Proof.To see that q γ α is the solution, taking any ε &gt; 0, one can find a τ such that q γ,τ +1 α − q γ α &lt; ε 2 , and q γ,τ α s,a,s − q γ α s,a,s &lt; ε 2 for any s, a, s , thanks to the fact that there are only finite number of s, a, s triples, and the convergence from q γ,τ +1 α (s, a) to q γ α (s, a) is uniform.It follows that
g (α s,a ) + γ s α s,a,s α s,a max a q γ α s,a,s (s , a ) − q γ α (s, a) ≤ q γ,τ +1 α − q γ α + γ q γ,τ α s,a,s − q γ α s,a,s &lt; ε.
Since ε is chosen arbitrary, q γ α (s, a) must be the solution of the infinite recursion.The fact that q γ α (s, a) ≥ q γ π,α (s, a) follows from the fact that q γ,τ α and q γ,τ π,α are monotonically increasing on τ (by Lemma.1), and q γ,τ α ≥ q γ,τ π,α for any given τ and π.</p>
<p>The propositions above guarantees the existence and optimality of q γ α , and the following discussions would focus on q γ α .We drop the super-script γ in the rest of this section.</p>
<p>Approximation through Dynamic Programming</p>
<p>The optimal curiosity Q-value is given by the infinite recursion A surprising fact is that when α is large, qα is in fact a very good approximation of q α , which is the central result in this section.We start by investigating the properties of the gain g α t s,a .
q α (s, a) = g (α s,a ) + γ s α s,a,s α s,a max a q α s,a,s (s , a ) .(5)</p>
<p>Properties of Expected Information Gain in Dirichlet Case</p>
<p>The expected information gain of a Dirichlet distribution Dir (n 1 , • • • , n S ) is given by
g (n) = log (n) − ψ (n + 1) − s n s n [log (n s ) − ψ (n s + 1)] . Define f (x) = x [ψ (x + 1) − log x] = 1 − x [log x − ψ (x)] . then g (n) = 1 n s f (n s ) − f (n) .
The following important properties has been proved by Alzer in [1].</p>
<p>Theorem 1. f has the following properties4 :
a) lim x→0 f (x) = 0, lim x→∞ f (x) = 12
b) f is strictly completely monotonic, in the sense that
(−1) n+1 d n f (x) dx n &gt; 0.
In particular, Theorem.1 shows that f is strictly monotonically increasing, and also strictly concave.The following Lemma summarizes the properties about f used in this paper.
Lemma 3. Define δ m (x) = f (x + m) − f (x) for m &gt; 0. Then a) f is sub-additive, i.e., f (x) + f (y) &gt; f (x + y) for x, y &gt; 0 b) δ m (x) is monotonically decreasing on (0, ∞). c) 0 &lt; xδ m (x) &lt; 1−e −1 2 m for x ∈ (0, ∞).
Proof.a) Note that g (n) is mutual information, and the unknown observation depends on the parameters of the distribution, therefore g (n) &gt; 0, and
0 &lt; g ([x, y]) = 1 x + y [f (x) + f (y) − f (x + y)] . b) Note that δ m (x) = m 0 f (x + s) ds,
and the result follows from f (x) &lt; 0. c) Clearly, xδ m (x) &gt; 0 because f is strictly increasing.From Intermediate Value Theorem, there some δ ∈ (0, m), such that
xδ m (x) = x [f (x + m) − f (x)] = mxf (x + δ) = mxf (x) + mx [f (x + δ) − f (x)] &lt; mxf (x) .
The inequality is because f is strictly concave.</p>
<p>From [1],
f (x) = 1 − x ∞ 0 φ (t) e −tx dt, where φ (t) = 1 1 − e −t − 1 t
is strictly increasing, with lim t→0 φ (t) = 1  2 and lim t→∞ φ (t) = 1.Therefore,
xf (x) = x ∞ 0 φ (t) e −tx (xt − 1) dt = x 2 1 x 0 φ (t) e −tx t − 1 x dt + x 2 ∞ 1 x φ (t) e −tx t − 1 x dt &lt; x 2 φ (0) 1 x 0 e −tx t − 1 x dt + x 2 φ (∞) ∞ 1 x e −tx t − 1 x dt = x 2 2 ∞ 0 e −tx t − 1 x dt + x 2 2 ∞ 1 x e −tx tdt − x 2 ∞ 1 x e −tx dt &lt; x 2 2 ∞ 0 e −tx t − 1 x dt + x 2 2 ∞ 0 e −tx tdt − x 2 ∞ 1 x
e −tx dt.</p>
<p>Note that
∞ 0 e −tx tdt = 1 x 2 , ∞ 0 e −tx dt = 1 x , and x ∞ 1 x e −tx dt = e −1 , it follows that xf (x) &lt; 1 − e −1 2 .
The properties of f guarantee that g (n) decreases at the rate of 1 n .The result is formulated in the following Lemma.Lemma 4. Let Dir n 0 1 , • • • , n 0 S and Dir (n t 1 , • • • , n t S ) be the prior and the posterior distribution, such that n t = n 0 + t.Let s * = arg max s n 0 s .Then
s =s * f n 0 s − f s =s * n 0 s 2n &lt; g n t &lt; S − 1 2n . Proof. The upper bound is because 0 &lt; f (x) &lt; 1 2 and f is increasing, thus s f n t s − f n t = f n t 1 − f n t + s =1 f n t s &lt; S − 1 2 .
The lower bound follows from the fact that f (x + m) − f (x) is decreasing.We show that the trajectory minimizing g (n t ) is the one such that all t observations equal to s * .To see this, let m s be the number of times observing s = s * , then
f n 0 s + m s + f n 0 s * + m s * = f n 0 s + f n 0 s * + m s * + m s + f n 0 s + m s − f n 0 s − f n 0 s * + m s * + m s − f n 0 s * + m s * .
Note that n 0 s * + m s * ≥ n 0 s , so
f n 0 s + m s + f n 0 s * + m s * ≥ f n 0 s + f n 0 s * + m s * + m s .
Now assume the observations are all s * , from sub-additivity,
s f n t s − f n t = s =s * f n 0 s + f n 0 s * + t − f n 0 + t = s =s * f n 0 s − f   s =s * n 0 s   +   f   s =s * n 0 s   + f n 0 s * + t − f n 0 + t   &gt; s =s * f n 0 s − f   s =s * n 0 s   .
A little remark: The bounds hold irrespective of the data generating process, namely, it holds for any sequences of observations, including sequences with zero probabilities.</p>
<p>The following Lemma bound the variation of the expected information gain, when one single observation is added.
Lemma 5. Let n = [n 1 , • • • , n S ] and n = [n 1 , • • • , n s−1 , n s + 1, n s+1 , • • • , n S ], then n s n |g (n ) − g (n)| ≤ S 2n 2 , ∀n s &gt; 0.
Proof.Without loss of generality let s = 1.Note that
n 1 n [g (n ) − g (n)] = n 1 n    1 n + 1   s =1 f (n s ) + f (n 1 + 1) − f (n + 1)   − 1 n s f (n s ) − f (n)    = n 1 n s f (n s ) n + 1 − f (n + 1) n + 1 + f (n 1 + 1) − f (n 1 ) n + 1 + f (n) n − s f (n s ) n = n 1 n − f (n + 1) n + 1 − s f (n s ) n (n + 1) + f (n 1 + 1) − f (n 1 ) n + 1 + f (n) n = n 1 n − f (n + 1) n + 1 − ng (n) + f (n) n (n + 1) + f (n 1 + 1) − f (n 1 ) n + 1 + f (n) n = n 1 n (n + 1) {−δ 1 (n) + δ 1 (n 1 ) − g (n)} = 1 n (n + 1) n 1 δ 1 (n 1 ) − n 1 n • nδ 1 (n) − n 1 n s f (n s ) − f (n)
From the previous Lemma,
0 &lt; xδ 1 (x) &lt; 1 2 , 0 &lt; f (x) &lt; 1 2 , so − S 2n 2 &lt; − n 1 n S 2n 2 &lt; n 1 n [g (n ) − g (n)] &lt; 1 2n 2 , thus n 1 n |g (n ) − g (n)| &lt; S 2n 2 .</p>
<p>Bounding the Difference Between q α and qα</p>
<p>In this subsection we present the result bounding the difference between q α and qα , without making any assumptions to the environment.Let c α = min s min a α s,a , the main conclusion of this subsection is that
|q α (s, a) − qα (s, a)| ∼ 1 c 2 α . Lemma 6. q α (s, a) ≤ S−1 2(1−γ)cα .
Proof.From Lemma.4,write K 0 = S−1 2 , then
g (α s,a ) &lt; K 0 α s,a &lt; K 0 c α , ∀s, a
By definition,
q γ,2 α (s, a) = g (α s,a ) + γ s α s,a,s α s,a max a q γ,1 α s,a,s (s , a ) &lt; K 0 1 c α + γ 1 c α s,a,s ≤ K 0 c α (1 + γ) , since c α s,a,s ≥ c α .
Repeat the process, it follows that for any τ ,
q γ,τ α (s, a) ≤ K 0 c α 1 + γ + • • • + γ τ −1 &lt; K 0 (1 − γ) c α = S − 1 2 (1 − γ) c α , thus q γ α (s, a) = lim τ →∞ q γ,τ α (s, a) ≤ K 0 (1 − γ) c α . Lemma 7. Let n = [n 1 , • • • , n S ], and n = [n 1 + 1, n 2 , • • • , n S ]. Let x 1 , • • , x S
be S non-negative numbers.Define p s = ns n , p 1 = n1+1 n+1 and p s = ns n+1 for s = 2, • • • , S. Then
p 1 s (p s − p s ) x s ≤ 1 n s p s x s .
Proof.Simple derivation gives
p 1 s (p s − p s ) x s = s p s x s • p 1 s (p s − p s ) x s s p s x s ≤ s p s x s max s p 1 • |p s − p s | p s . If s = 1, p 1 • |p 1 − p 1 | p 1 = n 1 n • n1+1 n+1 − n1 n n1 n = n − n 1 n (n + 1) ≤ 1 n . If s = 1, p 1 • |p s − p s | p s = n 1 n • ns n − ns n+1 ns n = n 1 n (n + 1) ≤ 1 n . Therefore, max s p 1 • |p s − p s | p s ≤ 1 n ,andp 1 s (p s − p s ) x s ≤ 1 n s p s x s .
Lemma 8.For any α, s † , a † , there is some constant K depending on S and γ only, such that
s α s † ,a † ,s α s † ,a † max a q α s † ,a † ,s (s, a) − q α (s, a) ≤ K c 2 α . Proof. First change the notations. Let s 0 = s † , a 0 = a † . Also let α 1 = α, β 1 = α s † , a † , s . The result to prove becomes s1 α 1 s0a0s1 α 1 s0a0 max a1 q β 1 (s 1 , a 1 ) − q α 1 (s 1 , a 1 ) ≤ K c 2 α .
Consider the finite time horizon approximations of q β 1 and q α 1 , namely q γ,τ β 1 and q γ,τ α 1 .With a little abuse of notation, we drop the superscript γ in this proof.Note that this shall not be confused with the finite time horizon curiosity Q-values without discount.</p>
<p>For τ = 2, consider the following inequality:
α 1 s0a0s1 α 1 s0a0 max a1 q 2 β 1 (s 1 , a 1 ) − q 2 α 1 (s 1 , a 1 ) ≤ α 1 s0a0s1 α 1 s0a0 max a1 g β 1 s1,a1 − g α 1 s1,a1 + γ α 1 s0a0s1 α 1 s0a0 max a1 s2 β 1 s1a1s2 β 1 s1a1 − α 1 s1a1s2 α 1 s1a1 max a2 q 1 β 2 (s 2 , a 2 ) + γ α 1 s0a0s1 α 1 s0a0 max a1 s2 α 1 s1a1s2 α 1 s1a1 max a2 q 1 β 2 (s 2 , a 2 ) − q 1 α 2 (s 2 , a 2 ) .
Here
β 2 = β 1 s 1 a 1 s 2 , α 2 = α 1 s 1 a 1 s 2 .
Note that the error between q 2 β 1 and q 2 α 1 has been decomposed into three terms.The first term captures the difference between the immediate information gain, the second term captures the error between transition probabilities, and the third term is of the same form as the left side of the inequality, except τ is decreased by 1.To simplify the notation, let F t be the operator
F t [• • • ] = st α t−1 st−1at−1st α t−1 st−1at−1 max at [• • • ] .
For fixed τ , let
δ t = g β t st,at − g α t st,at + γ st+1 β t statst+1 β t stat − α t statst+1 α t stat max at+1 q τ −t β t+1 (s t+1 , a t+1 ) ,
and
φ t = q τ +1−t β t (s t , a t ) − q τ +1−t α t (s t , a t ) .
One can write
F 1 φ 1 ≤ F 1 δ 1 + γF 1 F 2 φ 2 .
Repeat this process for general τ , it follows that
F 1 φ 1 ≤ F 1 δ 1 + γF 1 F 2 φ 2 ≤ F 1 δ 1 + γF 1 F 2 δ 2 + γF 2 F 3 φ 3 = F 1 δ 1 + γF 1 F 2 δ 2 + γ 2 F 1 F 2 F 3 φ 3 = • • • = F 1 δ 1 + γF 1 F 2 δ 2 + • • • + γ t−1 F 1 • • • F t δ t + • • • + γ τ −2 F 1 • • • F τ −1 δ τ −1 + γ τ −1 F 1 • • • F τ φ τ .
Now look at a particular term in the inequality above, for example,
F 1 • • • F t δ t = s1 α 1 s0a0s1 α 1 s0a0 max a1 • • • st α t−1 st−1at−1st α t−1 st−1at−1 max at δ t .
Note that if s t , a t = s † , a † then δ t = 0, since β t and α t differ only in the entry indexed by s † , a † , s 1 .The following discussion assumes that s t , a t = s † , a † .From Lemma.5, let K 1 = S 2 , then
g β t st,at − g α t st,at ≤ K 1 α t st,at α t st,at,s1.
From Lemma.6 and 7, there is some K 2 depends only on S and γ, such that
st+1 β t statst+1 β t stat − α t statst+1 α t stat max at+1 q τ −t β t+1 (s t+1 , a t+1 ) ≤ 1 α t s0,a0,s1 st+1 α t statst+1 α t stat max at+1 q τ −t β t+1 (s t+1 , a t+1 ) ≤ K 2 α t s0,a0,s1 c α t
, where c α t = min s min a α t s,a .In combination, there is some K 0 such that
δ t ≤ K 0 c α t α t s † ,a † ,s1
. The next step is tricky: Assume that the policy is given, say, it is already the policy maximize F 1 • • • F t δ t , so that each a is a deterministic function of the prior α 1 and the previous history.Consider a trajectory s 0 a 0 s 1 a 1 • • • s t a t , the predictive probability of seeing such a trajectory is given by
p (s 1 a 1 • • • s t a t |s 0 a 0 ) = α 1 s0a0s1 α 1 s0a0 α 2 s1a1s2 α 2 s1a1 • • • α t−1 st−1at−1st α t−1 st−1at−1 . Again, if s t , a t = s † , a † , then p (s 1 a 1 • • • s t a t |s 0 a 0 ) δ t = 0. Otherwise, p (s 1 a 1 • • • s t a t |s 0 a 0 ) δ t = α 2 s1a1s2 α 2 s1a1 • • • α t−1 st−1at−1st α t−1 st−1at−1 • α 1 s0a0s1 α 1 s0a0 δ t ≤ α 2 s1a1s2 α 2 s1a1 • • • α t−1 st−1at−1st α t−1 st−1at−1 • K 0 c α t α 1 s0a0 • α 1 s0a0s1 α t s0,a0,s1 ≤ α 2 s1a1s2 α 2 s1a1 • • • α t−1 st−1at−1st α t−1 st−1at−1 • K 0 c α t α 1 s0a0 ≤ α 2 s1a1s2 α 2 s1a1 • • • α t−1 st−1at−1st α t−1 st−1at−1 • K 0 c 2 α 1 . Note that α 2 s1a1s2 α 2 s1a1 • • • α t−1 st−1at−1st α t−1 st−1at−1 = p (s 2 a 2 • • • s t a t |s 1 a 1 )
is the probability of seeing the trajectory s 2 a 2 • • • s t a t , when the agent assumes prior α 1 s 0 , a 0 , s 1 = α 2 , and follows the same policy starting from s 1 , a 1 .Clearly,
s2 • • • st p (s 2 a 2 • • • s t a t |s 1 a 1 ) = 1,
which leads to
F 1 • • • F t δ t = s1 s2 • • • st p (s 1 a 1 • • • s t a t |s 0 a 0 ) δ t ≤ s1 K 0 c 2 α 1 s2 • • • st p (s 2 a 2 • • • s t a t |s 1 a 1 ) ≤ SK 0 c 2 α 1 .
Putting the equation back, and note that c 2 α 1 = c 2 α is a constant on α, one has
F 1 φ 1 ≤ F 1 δ 1 + γF 1 F 2 δ 2 + • • • + γ t−1 F 1 • • • F t δ t + • • • + γ τ −2 F 1 • • • F τ −1 δ τ −1 + γ τ −1 F 1 • • • F τ φ τ ≤ SK 0 c 2 α 1 + γ + • • • + γ τ −2 + γ τ −1 F 1 • • • F τ φ τ ≤ SK 0 1 − γ 1 c α + γ τ −1 F 1 • • • F τ φ τ .
From Lemma.6, since the curiosity Q-values are bounded, there is some K 3 such that
φ τ = q 1 β t (s t , a t ) − q 1 α t (s t , a t ) ≤ q 1 β t (s t , a t ) + q 1 α t (s t , a t ) ≤ |q β t (s t , a t )| + |q α t (s t , a t )| ≤ K 3 c α , thus F 1 φ 1 ≤ SK 0 1 − γ 1 c α + γ τ −1 K 3 c α . Let τ → ∞, one has s1 α 1 s0a0s1 α 1 s0a0 max a1 q β 1 (s 1 , a 1 ) − q α 1 (s 1 , a 1 ) ≤ K c 2 α ,
where K = SK0 1−γ is a constant depending on S and γ only.</p>
<p>The central result in this subsection is given by the following Proposition.</p>
<p>Proposition 5.There is some K &gt; 0 depending on S and γ only, such that
|q α (s, a) − qα (s, a)| ≤ K c 2 α .
Proof.Write Eq.5 into the following form
q α (s, a) = g (α s,a ) + γ s α s,a,s α s,a max a q α (s , a ) + γ s α s,a,s α s,a max a q α s,a,s (s , a ) − max a q α (s , a ) .
The last term is bounded by
δ = s α s,a,s α s,a max a q α s,a,s (s , a ) − max a q α (s , a ) ≤ s α s,a,s α s,a max a q α s,a,s (s , a ) − q α (s , a ) .
Apply Lemma.7, it follows that there is some constant K 0 depending on S and γ only, such that s α s,a,s α s,a max a q α s,a,s (s , a Letting K = γK0 1−γ completes the proof.
) − q α (s , a ) ≤ K 0 c 2 α . Therefore δ ≤ K0</p>
<p>Quality of the Approximation in Connected Markovian Environment</p>
<p>Proposition.5 guarantees that the difference between q α and qα decreases at the rate of c −2 α .However, this alone is not enough to guarantee that qα converges to q α when the agent operates in the environment.For example, consider the environment consists of two connected components.In this case, c α is upper bounded since that in one of the connected component α s,a never increases.</p>
<p>Here we make the following assumption: The first half of the assumption ensures that α s,a,s αs,a converges to p (s |s, a) when α s,a goes to infinity by Law of Large Numbers.The second half of the assumption implies that it is always possible to navigate from one state to another with positive probability of success.Therefore, if some g (α s,a ) is large, the information is guaranteed to propagate to all the states.Under this assumption, we prove in this section that when t → ∞, q α (s, a) qα (s, a)
Assumption
− 1 → 0, namely, the curiosity Q-value and the DP approximation are getting arbitrarily closer along time.</p>
<p>The proof is unwrapped in three steps.</p>
<p>Lemma 9. Assume IV), and the agent chooses the action greedily with respect to qα t , where α t is the posterior after t time steps.Then for any s, a, lim t→∞ α t s,a = ∞, a.s.</p>
<p>Proof.Note that α t s,a,s is non-decreasing, and can only increase by one if increasing.Therefore, lim t→∞ α t s,a &lt; ∞ implies that there is some T s,a and c s,a such that for all t &gt; T , α t s,a = c s,a .The complement of lim t→∞ α t s,a = ∞ for all s, a is that ∃Λ ⊂ S × A, Λ = ∅, and ∃T s,a , c s,a for all s, a ∈ Λ, such that α t s,a = c s,a for all t &gt; T s,a .Since there are only finitely many s, a , this can be simplified to ∃Λ = ∅, ∃T , ∃c s,a , such that α t s,a = c s,a for all t &gt; T and s, a ∈ Λ.</p>
<p>Fix Λ = ∅, T and c s,a , we show that the event α t s,a = c s,a for t &gt; T and s, a ∈ Λ is a null event.Let Λ = S × A\Λ, by definition, α t s,a → ∞ for all s, a ∈ Λ.Clearly, Λ is not empty.Define S I = {s ∈ S : ∃a, a such that s, a ∈ Λ, s, a / ∈ Λ} .</p>
<p>Namely, S I is the 'boundary' between Λ and Λ.The first step is to show S I = ∅ if Λ = ∅, or more precisely, the event S I = ∅ and Λ = ∅ is null.Assume S I = ∅ and Λ = ∅, then Λ must satisfy that if s, a ∈ Λ for some a, then s, a ∈ Λ for all a.Let S Λ ⊂ S be the set of s such that s, a ∈ Λ.Clearly, once reaching s ∈ S Λ , any action chosen would cause Λ be visited, which can only happen for finitely many times.This implies that for any s ∈ S Λ , any state action pair s , a such that p (s|s , a ) &gt; 0 can only be visited finite number of times almost surely, because the probability of sampling from p (•|s , a ) for infinitely many times but only getting finite number of s is zero.From Assumption IV), for any S Λ = S, there is always some s , a such that s / ∈ S Λ and p (s|s , a ) &gt; 0, so s , a can only be visited finitely many times, by definition s , a ∈ Λ, which contradicts with the fact that s / ∈ S Λ .Next we show that at least for one s ∈ S I , following the optimal strategy leads to some s, a ∈ Λ being visited.For t &gt; T , Define q (s, a) = r (s, a) + γ , if s, a ∈ Λ , and
r (s, a) = 0, if s, a / ∈ Λ g α t s,a , if s, a ∈ Λ .
Clearly, p and r do not depend on t, and q is the unique optimal solution.Now let s † , a † ∈ Λ be the pair such that s ∈ S I , and q s † , a † = max s,a ∈Λ,s∈S I q (s, a) .</p>
<p>It can be seen that for any a such that s † , a / ∈ Λ, q s † , a ≤ γ q s † , a † .The reason is the following: Performing a leads to zero immediate reward since s † , a / ∈ Λ.Let s be the result of the transition, then either s ∈ S I , so max a q (s , a ) ≤ q s † , a † , or s is some other state such that s , a / ∈ Λ for all a .(Note that s cannot be a state such that s , a ∈ Λ for all a .)In the latter case, since s is only connected to states in Λ through S I , it must be that max a q (s , a ) ≤ γ q s † , a † , since at least one more step must be made to reach S I first.Taking into account the discount, it follows that q s † , a † − q s † , a ≥ (1 − γ) q s † , a † .Replace q with qα t leads to
qα t s † , a † − qα t s † , a ≥ (1 − γ) q s † , a † + qα t s † , a † − q s † , a † + qα t s † , a − q s † , a
From the initial assumption, when t &gt; T , s † , a † is never visited, also, the action is chosen greedily with respect to qα t .This implies that at least for one a such that s † , a / ∈ Λ, From Lemma.4,
qα t s † , a † − qα t s † , a ≤ 0, or 0 ≥ (1 − γ) q s † , a † + qα t s † , a † − q s † , a † + qα t s † , a − q s † , a ≥ (1 − γ) q s † , a † − 2 max s max a |q α t (s, a) − q (s, a)| , which leads to max s max a |q α t (s, a) − q (s, a)| ≥ 1 − γ 2 q s † , a † .g α t s,a − r (s, a) = max s,a / ∈Λ g α t s,a &lt; S − 1 2α t s,a → 0.
Therefore, there is some T such that g α t s,a − r (s, a) &lt; 1−γ 4 q s † , a † for all s, a / ∈ Λ.Also note that does not converge to p (s |s, a), which is a null event because it contradicts the Strong Law of Large Numbers.This in turn implies that for fixed Λ, T and c s,a , the event ∃Λ = ∅, ∃T , ∃c s,a , such that α t s,a = c s,a for all t &gt; T and s, a ∈ Λ is null.As the last step, notice that there are only countably many such events, and since the union of countably many null events is still null, one can conclude that lim t→∞ α t s,a = ∞ for all s, a holds almost surely.
qα t (s , a ) ≤ S − 1 2 (1 − γ) c α , where c α = min s,a ∈Λ c s,a . Let K = γ(S−1) 2(1−γ) 2 cα , then K max s max a s α t s,a,s α t s,a − p (s |s, a) + 1 − γ 4 q s † , a † ≥ γ 1 − γ max s max a s α t s,a,s α t s,a − p (s |s, a) max a qα t (s , a ) + 1 1 − γ g α t s,a − r (s, a) ≥ max s max a |q α t (s, a) − q (s, a)| ≥ 1 − γ 2 q s † ,
The next step is to show that all qα t (s, a) decreases at the rate lower bounded by c −1 α .Let q s,a be the Q-value of performing a at state s, assuming the reward is 1 at all states.Namely, q s,a is the solution of the following Bellman equation q s,a = 1 + γ s p (s |s, a) a q s ,a .Clearly, q s,a &gt; 0 from Assumption IV).Define q = min s,a q s,a .Also, let Combining Lemma.9 and 10 produces the following proposition.
u s,a = s =s * f α 0 s,a,s − f s =s * α 0 s,a,s2
Proposition 6. Assume IV), and that the agent acts greedily with respect to qα , then lim t→∞ q α t qα t − 1 = 0, a.s. and there is some K depending only on the dynamics and γ, such that
lim sup t→∞ c α q α t qα t − 1 ≤ K. Proof. Note that q α qα − 1 ≤ K c α • 1 c α qα ,
where K is given in Proposition.5.Use Lemma.9, 10, the result follows trivially.</p>
<p>Experiment</p>
<p>The idea presented in the previous section is illustrated through a simple experiment.The environment is an MDP consisting of two groups of densely connected states (cliques) linked by a long corridor.The agent has two actions allowing it to move along the corridor deterministically, whereas the transition probabilities inside each clique are randomly generated.The agent assumes Dirichlet priors over all transition probabilities, and the goal is to learn the transition model of the MDP.In the experiment, each clique consists of 5 states, (states 1 to 5 and states 56 to 60), and the corridor is of length 50 (states 6 to 55).The prior over each transition probability is Dir 1 60 , . . ., 1 60 .We compare four different algorithms: i) random exploration, where the agent selects each of the two actions with equal probability at each time step; ii) Qlearning with the immediate information gain g (ao h) as the reward; iii) greedy exploration, where the agent chooses at each time step the action maximizing g (a h); and iv) a dynamic-programming (DP) approximation of the optimal Bayesian exploration, where at each time step the agent follows a policy which is computed using policy iteration, assuming that the dynamics of the MDP is given by the current posterior, and the reward is the expected information gain g (a h).</p>
<p>Fig. 2 shows the typical behavior of the four algorithms.The upper four plots show how the agent moves in the MDP starting from one clique.Both greedy exploration and DP approximation move back and forth between the two cliques.Random exploration has difficulty moving between the two cliques due to the random walk behavior in the corridor.Q-learning exploration, however, gets stuck in the initial clique.The reason for is that since the jump on the corridor is deterministic, the information gain decreases to virtually zero after only several attempts, therefore the Q-value of jumping into the corridor becomes much lower than the Q-value of jumping inside the clique.The bottom plot shows how the cumulative information gain grows over time, and how the DP approximation clearly outperforms the other algorithms, particularly in the early phase of exploration.</p>
<p>Related Work</p>
<p>The idea of actively selecting queries to accelerate learning process has a long history [2,3,8], and received a lot of attention in recent decades, primarily in the context of active learning [9] and artificial curiosity [7].In particular, measuring learning progress using KL divergence dates back to the 50's [5,3].In 1995 this was combined with reinforcement learning, with the goal of optimizing future expected information gain [11].Others renamed this Bayesian surprise [4].</p>
<p>Our work differs from most previous work in two main points: First, like in [11], we consider the problem of exploring a dynamic environment, where actions change the environmental state, while most work on active learning and Bayesian experiment design focuses on queries that do not affect the environment [9].Second, our result is theoretically sound and directly derived from first principles, in contrast to the more heuristic application [11] of traditional reinforcement learning to the problem of maximizing expected information gain.We formulated the concept of curiosity (Q) value, and highlighted the necessity of balancing immediate information gain and long-term expected information gain (see Eq.4).In particular, we pointed out a previously neglected subtlety of using KL divergence as learning progress.</p>
<p>Conceptually, however, our work is closely connected to artificial curiosity and intrinsically motivated reinforcement learning [7,10,8] for agents that actively explore the environment without external reward signal.In fact, the very definition of the curiosity (Q) value permits a firm connection between pure exploration and reinforcement learning.</p>
<p>Conclusion</p>
<p>We have presented the principle of optimal Bayesian exploration in dynamic environments, centered around the concept of the curiosity (Q) value.Our work provides a theoretically sound foundation for designing more effective exploration strategies.Based on this result, we establish the optimality of the DP approximation of the optimal Bayesian exploration in the MDP case.Fig. 2. The exploration process of a typical run of 4000 steps.The upper four plots shows the position of the agent between state 1 (the lowest) and 60 (the highest).The states at the top and the bottom correspond to the two cliques, and the states in the middle correspond to the corridor.The lowest plot is the cumulative information gain with respect to the prior.</p>
<p>Fig.1.Illustration of the difference between the sum of one-step information gain and the cumulative information gain with respect to the prior.In this case, 1000 independent samples are generated from a distribution over finite sample space {1, 2, 3}, with p (x = 1) = 0.1, p (x = 2) = 0.5, and p (x = 3) = 0.4.The task of learning is to recover the mass function from the samples, assuming a Dirichlet prior Dir 50 3 , 50 3 , 50 3 .The KL divergence between two Dirichlet distributions are computed according to[6].It is clear from the graph that the cumulative information gain fluctuates when the number of samples increases, while the sum of the one-step information gain increases monotonically.It also shows that the difference between the two quantities can be large.</p>
<p>c 2 α.
2
Now compare q α and qα : max s max a |q α (s, a) − qα (s, a)|</p>
<p>III The environment is finite Markovian with dynamics p (s |s, a), and the Markov chain with transition kernel p (s |s) = 1 A a∈A p (s |s, a) is irreducible.</p>
<p>s p (s |s, a) max a q (s , a ) , with p (s |s, a) = p (s |s, a) , if s, a / ∈ Λ</p>
<p>−−</p>
<p>Note that |q α t (s, a) − q (s, a)| ≤ g α t s,a − r (s, a) p (s |s, a) max a qα t (s , a ) + γ s p (s |s, a) max a |q α t (s , a ) − q (s, a)| , so max s max a |q α t (s, a) − q (s, p (s |s, a) max a qα t (s , a ) .</p>
<p>, where α 0 is the initial α representing the agent's prior, and s * = arg max s α 0 s,a,s as defined in Lemma.4.Define u = min s,a u s,a .Lemma 10.Assume IV), and let c α t = min s min a α t s,a , then lim inf t→∞ c α t qα t (s, a) ≥ uq, a.s.Proof.Let qα t be the solution to the following Bellman equation:
qα t (s, a) = g α t s,a + γClearly, for any s, a ,g α t s,a ≥u s,a α t s,a≥u c α t.and because qα t is optimal,qα t (s, a) ≥u c α tq s,a ≥uq c α γ(S−1) 2(1−γ) 2 , thenmax smax asα t s,a,s α t s,a− p (s |s, a) ≥c α Kmax≥uqε Kholds for infinitely many t, which contradicts again with the Law of Large Num-bers.Let ε n = 1 n , then the union of the countably many eventslim infis again a null event, thereforelim inf
s p (s |s, a) max a qα t (s , a ) .t , ∀s, a, or c α t qα t (s, a) ≥ uq.Fix an ε &gt; 0, we show that lim inf t→∞ c α t qα t (s, a) ≤ uq (1 − ε) , ∀s, a is a null event.Assuming lim inf t→∞ c α t qα t ≤ uq (1 − ε), and following similar procedure as in the proof of Lemma.9, let K = s max a |q α t (s, a) − q (s, a)| t→∞ c α t qα t (s, a) ≤ uq (1 − ε n ) , ∀s, a t→∞ c α t qα t (s, a) ≥ uq, a.s.</p>
<p>Side note: To generalize the discussion, concepts from algorithmic information theory, such as compression distance, may also be used here. However, restricting the discussion under a probabilistic framework greatly simplifies the matter.
We use αs,a both for the vector [αs,a,1, • • • , αs,a,S] and the number s α s,a,s . The meaning should be clear from the context.
Assume that the operator is right associative, so one can write α s1, a2, s2 s2, a3, s3 • • • , or simply α s1a2s2a3s3 • • • .
Alzer's original paper considers the function x (log x − ψ (x)) = 1 − f (x). Here the statements are modified accordingly.</p>
<p>On some inequalities for the gamma and psi functions. Horst Alzer, Mathematics of Computation. 662171997</p>
<p>Bayesian experimental design: A review. Kathryn Chaloner, Isabella Verdinelli, Statistical Science. 101995</p>
<p>Theory of optimal experiments. V V Fedorov, 1972Academic Press</p>
<p>Bayesian surprise attracts human attention. L Itti, P F Baldi, NIPS'05. 2006</p>
<p>On a measure of the information provided by an experiment. D V Lindley, Annals of Mathematical Statistics. 2741956</p>
<p>Kullback-liebler divergences of normal, gamma, dirichlet and wishart densities. W D Penny, 2001Wellcome Department of Cognitive Neurology, University College LondonTechnical report</p>
<p>Curious model-building control systems. Jürgen Schmidhuber, IJCNN'91. 19912</p>
<p>Formal theory of creativity, fun, and intrinsic motivation. Jürgen Schmidhuber, Autonomous Mental Development. 1990-2010. 9 20102</p>
<p>Active learning literature survey. Burr Settles, 2010Technical report</p>
<p>Intrinsically motivated reinforcement learning. S Singh, Ag Barto, N Chentanez, NIPS'04. 2004</p>
<p>Reinforcement driven information acquisition in non-deterministic environments. Jan Storck, Sepp Hochreiter, Jürgen Schmidhuber, ICANN'95. 1995</p>            </div>
        </div>

    </div>
</body>
</html>
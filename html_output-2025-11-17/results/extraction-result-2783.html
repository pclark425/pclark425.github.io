<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2783 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2783</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2783</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-3b61bc41dff751edbead03aab5e4a1da1aafcc06</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3b61bc41dff751edbead03aab5e4a1da1aafcc06" target="_blank">Multi-Stage Episodic Control for Strategic Exploration in Text Games</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work proposes to tackle the explore-vs-exploit dilemma using a multi-stage approach that explicitly disentangles these two strategies within each episode, and significantly outperforms prior approaches by 27% and 11% average normalized score over 12 games from the Jericho benchmark.</p>
                <p><strong>Paper Abstract:</strong> Text adventure games present unique challenges to reinforcement learning methods due to their combinatorially large action spaces and sparse rewards. The interplay of these two factors is particularly demanding because large action spaces require extensive exploration, while sparse rewards provide limited feedback. This work proposes to tackle the explore-vs-exploit dilemma using a multi-stage approach that explicitly disentangles these two strategies within each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins each episode using an exploitation policy that imitates a set of promising trajectories from the past, and then switches over to an exploration policy aimed at discovering novel actions that lead to unseen state spaces. This policy decomposition allows us to combine global decisions about which parts of the game space to return to with curiosity-based local exploration in that space, motivated by how a human may approach these games. Our method significantly outperforms prior approaches by 27% and 11% average normalized score over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in both deterministic and stochastic settings, respectively. On the game of Zork1, in particular, XTX obtains a score of 103, more than a 2x improvement over prior methods, and pushes past several known bottlenecks in the game that have plagued previous state-of-the-art methods.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2783.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2783.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XTX-IL (GPT-2 context)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XTX exploitation imitation policy (Transformer / GPT-2 conditioned on short context)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The exploitation component of XTX: a Transformer (GPT-2 architecture) trained by self‑imitation on sampled past trajectories; it conditions on a short context (two previous actions + current observation) to predict the next action and is used to return the agent to promising frontiers before local exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>pi_il (exploitation / imitation policy)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A Transformer model based on GPT-2 that takes context c = [a_{t-2}; a_{t-1}; o_t] and is trained with a language-modeling/cross-entropy loss to predict the next action token; used as the exploitation policy in XTX.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>A benchmark of human-created text-adventure games (interactive fiction) with large, dynamic natural-language action spaces, partial observability, and sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term contextual memory + episodic trajectory buffer (self-imitation)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>concatenated token context for Transformer (two past actions + current observation) together with an external trajectory buffer (set of sampled trajectories) used for training the policy cover</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>two most recent past actions and the current observation as immediate context; training buffer B contains full past trajectories (observations, actions, rewards, terminals) sampled from experience replay D</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>unspecified buffer capacity N for replay (Algorithm 1); trajectory sample size k=10 (k trajectories sampled to form B); number of optimization passes empirically ~40</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>relevance-based sampling of trajectories: sample a score u from seen unique scores (biased with temperature β1), then sample a trajectory with that score biased toward shorter length (temperature β2); then learn from the resulting trajectory buffer B</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Transitions (c_t, a_t, r_t, o_{t+1}, terminal) are appended to the prioritized replay buffer D at every time step; every n episodes k trajectories are sampled from D to form B and π_il is retrained (multiple passes) on B</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Bring the agent back to promising parts of the explored state space (the frontier) by imitating high-quality past trajectories (exploitation phase); provide 'roll-in' behavior so local exploration starts from useful states</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Included as part of XTX: the full XTX agent (which uses π_il in Phase 1) achieves large gains over baselines (e.g., average normalized score 56.3% across 12 deterministic games; ZORK1 average episode score 103.4 and normalized improvements reported in the paper). The paper reports XTX outperforms prior approaches by ~27% (deterministic) and ~11% (stochastic) average normalized score.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Ablation 'Pure imitation learning (λ=0)' (i.e., using only π_il without exploration) performs poorly: the paper states this model 'performs quite badly' and is unlikely to reach deep states. Exact numeric scores for the pure-IL ablation are not reported in the text but shown in ablation plots (worse than full XTX).</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Memory (the imitation-trained policy cover built from past trajectories) is effective for returning the agent to promising frontiers and enables deeper downstream exploration when combined with a strategic exploration policy; however, pure reliance on imitation (only memory-driven behavior) is ineffective as it fails to reach deep states and can get stuck in local minima—mixing with exploration (nonzero λ during Phase 1 and switching to λ=1 in Phase 2) is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>As a stand-alone strategy, imitation from stored trajectories can cause the agent to remain in local minima and not discover novel actions; the paper notes the need to train π_il on a mixture of trajectories (different scores/lengths) to mitigate falling into local minima. Exact capacity/scale limits of the buffer are not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Train π_il on a trajectory buffer B sampled with score bias (β1=1) and strong bias for shorter trajectories (β2 large), use k=10 trajectories and multiple optimization passes (≈40) every n episodes; maintain a small non-zero exploration mixture λ during Phase 1 (λ=1/(2*T) in experiments) and switch to λ=1 for Phase 2.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Stage Episodic Control for Strategic Exploration in Text Games', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2783.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2783.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experience Replay (prioritized)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prioritized experience replay buffer D used in XTX</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An external prioritized replay memory storing transition tuples (context, action, reward, next observation, terminal) used both to train the inverse-dynamics Q-policy and to sample trajectories for imitation learning; sampling prioritizes high-reward (max-score) transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>experience replay (training memory component used by XTX's π_inv-dy and to construct B)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A prioritized experience replay buffer with priority fraction ρ (set to 0.5 in experiments) that stores transitions collected online and is sampled to train the Q-based exploration policy (INV-DY) and to assemble the trajectory buffer for π_il.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Same as above; the buffer stores interactions from game episodes in Jericho.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external episodic replay memory (prioritized replay)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>sequential transition store with prioritized sampling; used to form both batches for TD/inverse dynamics updates and to select k trajectories for the trajectory buffer B</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>transition tuples (c_t, a_t, r_t, o_{t+1}, terminal) and stored trajectories (sequences of such tuples); prioritized subset contains transitions from trajectories that achieve the maximum score seen so far</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>capacity N (initialized in Algorithm 1) — capacity numeric value is not specified in the paper (left as N); priority fraction ρ=0.5</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>prioritized sampling for training π_inv-dy (ρ fraction prioritized); when sampling trajectories for π_il, the procedure first samples a score u from seen unique scores with bias β1 and then samples a trajectory with that score biased toward shorter length (β2); transitions for TD updates are sampled from the replay buffer</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Every timestep the agent stores the current transition into D; π_inv-dy is updated within episodes at every step using batches sampled from D; π_il is updated every n episodes using trajectories sampled from D</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Train Q-function and inverse dynamics bonuses for strategic exploration (π_inv-dy) and provide the experience pool from which high-quality trajectories are sampled to build an imitation policy cover (π_il) for exploitation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>The prioritized replay is a core part of XTX's training pipeline; the paper attributes part of XTX's gains to prioritized sampling (they set ρ=0.5 and prioritize transitions from max-score trajectories). The full XTX agent (which uses this replay) achieves the performance improvements reported above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Prioritizing transitions from high-scoring trajectories helps create a useful trajectory buffer for imitation and improves exploitation; biased trajectory sampling (favoring shorter trajectories for a given score) helps avoid wasting time on noisy/meaningless steps.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Paper does not report direct empirical limits of the buffer, but capacity N is unspecified; potential instability of Q-learning with novelty bonuses is discussed—XTX mitigates it by restricting Q-learning (with intrinsic bonuses) to the shorter-horizon exploration phase.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Use prioritized replay with priority fraction ρ=0.5, prioritize transitions from trajectories that achieved max score, sample k=10 trajectories with β1=1 (score bias) and large β2 (bias toward shortest length), and retrain π_il every n episodes using the resulting buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Stage Episodic Control for Strategic Exploration in Text Games', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2783.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2783.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Policy cover / Trajectory buffer (self-imitation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Policy cover constructed by sampling trajectories and self-imitation learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small set of k past trajectories sampled from experience replay (biased by score and short length) that are used to train an imitation policy (π_il) to build a 'policy cover' — a mechanism to globally return to promising regions of the state space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>trajectory buffer / policy cover (component of XTX)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A collection of k trajectories sampled (with replacement) from the replay buffer using a two-step scoring/length-biased sampling; these trajectories form B and are used to train the GPT-2 imitation policy by minimizing cross-entropy over (context, action) pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Interactive fiction games with sparse rewards and dynamic action sets; the trajectory buffer stores sequences of observations/actions from such games.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic trajectory memory / policy cover</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>buffer B of k sampled full trajectories (sequence of transitions); the imitation model ingests (a_{t-2}, a_{t-1}, o_t) contexts derived from these trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>full past trajectories: sequences of observations, actions, and rewards; used to construct single-step (context, action) training pairs for imitation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>k trajectories (k=10 in experiments); sampling is with replacement from replay D</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>two-stage sampling: (1) sample a score u from the set of unique scores seen so far (probability biased by β1), (2) sample a trajectory among those with score u biased toward shorter length (probability biased by β2); repeat k times</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>At every n episodes the trajectory buffer B is re-formed by sampling k trajectories from the continually updated replay D; π_il is retrained on B until convergence (≈40 passes reported empirically)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Create a policy cover so the agent can reliably return to promising frontiers (global exploitation) before switching to local exploration; helps reduce derailment and recover promising roll-in states</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>When incorporated into XTX, policy-cover based imitation contributes to empirical gains: XTX outperforms baselines across 12 games (average normalized score 56.3% deterministic). Ablation shows that pure imitation alone is insufficient, but the policy cover is necessary when combined with strategic exploration to achieve top performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>The XTX-Uniform ablation (which still uses a policy cover but uses uniform random exploration in Phase 2) performs worse than full XTX in several games — showing that the policy cover alone is not sufficient and must be paired with strategic local exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>A policy cover built from a mixture of trajectories (different scores and preferably shorter lengths for a given score) helps the agent return to useful states and avoid wasted actions; however, imitation must be mixed with exploration (non-zero λ during Phase 1) to avoid local minima and bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>If π_il is trained only on high-scoring homogeneous trajectories, the agent can fall into local minima (insufficient diversity) and fail to explore novel actions; the paper uses explicit sampling biases and mixing to mitigate this.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Sample k=10 trajectories with score bias β1=1 and strong bias toward shorter trajectories (β2 large ~10k), retrain π_il with multiple passes (~40) every n episodes, and keep a small exploration mixture (λ>0) during Phase 1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Stage Episodic Control for Strategic Exploration in Text Games', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2783.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2783.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge-graph memories (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning dynamic knowledge graphs to generalize on text-based games / Graph-constrained RL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned prior approaches that maintain a structured (graph) memory representing objects, relations, and state information across trajectories to mitigate partial observability; cited as focusing on language/state tracking but still facing large action-space/sparse-reward challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dynamic knowledge graphs to generalize on text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>knowledge-graph based agents (prior work cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Prior agents maintain dynamic knowledge graphs (nodes for objects/rooms, edges for relations) to track state across time and assist planning/action selection; cited works include Ammanabrolu & Hausknecht (2020) and Adhikari et al. (2020).</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho (and other text-game environments)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Same family of interactive fiction games; these methods aim to address partial observability by building structured memories of discovered entities and relations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory / knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>dynamic graph (nodes and labeled edges) representing discovered objects, locations, and relations; may be updated per observation/action</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>discovered objects, room descriptions, relations (e.g., location contains object), likely prior observed facts about the world</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>not specified in this paper (referenced work implements graph query/attention over graph nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>not specified here; in cited works graphs are updated as new observations are made</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Track persistent state to mitigate partial observability, assist in planning and avoid repeating fruitless actions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Cited as useful for partial observability and language semantics but reported in this paper to still suffer from the large/dynamic action space and sparse rewards in Jericho; no direct numeric comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Paper notes that knowledge-graph approaches help with partial observability/language semantics but do not by themselves solve the exploration-exploitation challenge posed by dynamic large action spaces and sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Still suffer from large dynamic action spaces and sparse rewards; paper implies additional mechanisms for exploration/exploitation are needed beyond state-tracking graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Stage Episodic Control for Strategic Exploration in Text Games', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2783.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2783.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Go-Explore archives (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>First return, then explore (Go-Explore) archive-based exploration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced multi-stage approach (Go-Explore) that stores archives of visited states/trajectories and explicitly returns to archived states before exploring; XTX contrasts with Go-Explore by not requiring such global archives and by using a learned imitation policy instead.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>First return, then explore</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Go-Explore (archive-based exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An exploration algorithm that (in the original formulation) stores an archive of visited states or cells, returns to promising archived states, and explores from them; often requires determinism or access to simulator restart functionality to reliably return to states.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Designed for hard-exploration RL problems (originally in Atari and other deterministic environments); stores and revisits archived states.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external archive of visited states / trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>archive (database) of stored states/trajectories, often with metadata about score and reachability</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>visited states, corresponding trajectories, and metadata (scores, actions to reach them)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>archive lookup to select a stored state/trajectory to return to (various heuristics); often deterministic restart to that state</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>append newly discovered states/trajectories to archive during exploration</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Enable reliable return to promising states so exploration can continue from those states (addresses derailment), effective especially in deterministic domains</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Go-Explore has produced state-of-the-art results in several hard-exploration deterministic domains; the paper contrasts XTX by noting Go-Explore requires archives and is limited in deterministic settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Archive-based returning is powerful in deterministic domains, but XTX argues that maintaining large archives and requiring deterministic resets is impractical for the dynamic, stochastic text-game setting; XTX achieves comparable benefits without explicit archives by learning imitation roll-in policies.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Requires determinism or simulator restart capability to reliably return to archived states; maintaining large archives can be cumbersome and not directly applicable to stochastic/text-game environments without extra assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Stage Episodic Control for Strategic Exploration in Text Games', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reading and acting while blindfolded: The need for semantics in text game agents <em>(Rating: 2)</em></li>
                <li>Keep CALM and explore: Language models for action generation in text-based games <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Learning dynamic knowledge graphs to generalize on text-based games <em>(Rating: 2)</em></li>
                <li>First return, then explore <em>(Rating: 2)</em></li>
                <li>Self-imitation learning <em>(Rating: 1)</em></li>
                <li>Exploration based language learning for text-based games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2783",
    "paper_id": "paper-3b61bc41dff751edbead03aab5e4a1da1aafcc06",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "XTX-IL (GPT-2 context)",
            "name_full": "XTX exploitation imitation policy (Transformer / GPT-2 conditioned on short context)",
            "brief_description": "The exploitation component of XTX: a Transformer (GPT-2 architecture) trained by self‑imitation on sampled past trajectories; it conditions on a short context (two previous actions + current observation) to predict the next action and is used to return the agent to promising frontiers before local exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "pi_il (exploitation / imitation policy)",
            "agent_description": "A Transformer model based on GPT-2 that takes context c = [a_{t-2}; a_{t-1}; o_t] and is trained with a language-modeling/cross-entropy loss to predict the next action token; used as the exploitation policy in XTX.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho",
            "game_description": "A benchmark of human-created text-adventure games (interactive fiction) with large, dynamic natural-language action spaces, partial observability, and sparse rewards.",
            "uses_memory": true,
            "memory_type": "short-term contextual memory + episodic trajectory buffer (self-imitation)",
            "memory_structure": "concatenated token context for Transformer (two past actions + current observation) together with an external trajectory buffer (set of sampled trajectories) used for training the policy cover",
            "memory_content": "two most recent past actions and the current observation as immediate context; training buffer B contains full past trajectories (observations, actions, rewards, terminals) sampled from experience replay D",
            "memory_capacity": "unspecified buffer capacity N for replay (Algorithm 1); trajectory sample size k=10 (k trajectories sampled to form B); number of optimization passes empirically ~40",
            "memory_retrieval_strategy": "relevance-based sampling of trajectories: sample a score u from seen unique scores (biased with temperature β1), then sample a trajectory with that score biased toward shorter length (temperature β2); then learn from the resulting trajectory buffer B",
            "memory_update_strategy": "Transitions (c_t, a_t, r_t, o_{t+1}, terminal) are appended to the prioritized replay buffer D at every time step; every n episodes k trajectories are sampled from D to form B and π_il is retrained (multiple passes) on B",
            "memory_usage_purpose": "Bring the agent back to promising parts of the explored state space (the frontier) by imitating high-quality past trajectories (exploitation phase); provide 'roll-in' behavior so local exploration starts from useful states",
            "performance_with_memory": "Included as part of XTX: the full XTX agent (which uses π_il in Phase 1) achieves large gains over baselines (e.g., average normalized score 56.3% across 12 deterministic games; ZORK1 average episode score 103.4 and normalized improvements reported in the paper). The paper reports XTX outperforms prior approaches by ~27% (deterministic) and ~11% (stochastic) average normalized score.",
            "performance_without_memory": "Ablation 'Pure imitation learning (λ=0)' (i.e., using only π_il without exploration) performs poorly: the paper states this model 'performs quite badly' and is unlikely to reach deep states. Exact numeric scores for the pure-IL ablation are not reported in the text but shown in ablation plots (worse than full XTX).",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Memory (the imitation-trained policy cover built from past trajectories) is effective for returning the agent to promising frontiers and enables deeper downstream exploration when combined with a strategic exploration policy; however, pure reliance on imitation (only memory-driven behavior) is ineffective as it fails to reach deep states and can get stuck in local minima—mixing with exploration (nonzero λ during Phase 1 and switching to λ=1 in Phase 2) is critical.",
            "memory_limitations": "As a stand-alone strategy, imitation from stored trajectories can cause the agent to remain in local minima and not discover novel actions; the paper notes the need to train π_il on a mixture of trajectories (different scores/lengths) to mitigate falling into local minima. Exact capacity/scale limits of the buffer are not specified.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": "Train π_il on a trajectory buffer B sampled with score bias (β1=1) and strong bias for shorter trajectories (β2 large), use k=10 trajectories and multiple optimization passes (≈40) every n episodes; maintain a small non-zero exploration mixture λ during Phase 1 (λ=1/(2*T) in experiments) and switch to λ=1 for Phase 2.",
            "uuid": "e2783.0",
            "source_info": {
                "paper_title": "Multi-Stage Episodic Control for Strategic Exploration in Text Games",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "Experience Replay (prioritized)",
            "name_full": "Prioritized experience replay buffer D used in XTX",
            "brief_description": "An external prioritized replay memory storing transition tuples (context, action, reward, next observation, terminal) used both to train the inverse-dynamics Q-policy and to sample trajectories for imitation learning; sampling prioritizes high-reward (max-score) transitions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "experience replay (training memory component used by XTX's π_inv-dy and to construct B)",
            "agent_description": "A prioritized experience replay buffer with priority fraction ρ (set to 0.5 in experiments) that stores transitions collected online and is sampled to train the Q-based exploration policy (INV-DY) and to assemble the trajectory buffer for π_il.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho",
            "game_description": "Same as above; the buffer stores interactions from game episodes in Jericho.",
            "uses_memory": true,
            "memory_type": "external episodic replay memory (prioritized replay)",
            "memory_structure": "sequential transition store with prioritized sampling; used to form both batches for TD/inverse dynamics updates and to select k trajectories for the trajectory buffer B",
            "memory_content": "transition tuples (c_t, a_t, r_t, o_{t+1}, terminal) and stored trajectories (sequences of such tuples); prioritized subset contains transitions from trajectories that achieve the maximum score seen so far",
            "memory_capacity": "capacity N (initialized in Algorithm 1) — capacity numeric value is not specified in the paper (left as N); priority fraction ρ=0.5",
            "memory_retrieval_strategy": "prioritized sampling for training π_inv-dy (ρ fraction prioritized); when sampling trajectories for π_il, the procedure first samples a score u from seen unique scores with bias β1 and then samples a trajectory with that score biased toward shorter length (β2); transitions for TD updates are sampled from the replay buffer",
            "memory_update_strategy": "Every timestep the agent stores the current transition into D; π_inv-dy is updated within episodes at every step using batches sampled from D; π_il is updated every n episodes using trajectories sampled from D",
            "memory_usage_purpose": "Train Q-function and inverse dynamics bonuses for strategic exploration (π_inv-dy) and provide the experience pool from which high-quality trajectories are sampled to build an imitation policy cover (π_il) for exploitation",
            "performance_with_memory": "The prioritized replay is a core part of XTX's training pipeline; the paper attributes part of XTX's gains to prioritized sampling (they set ρ=0.5 and prioritize transitions from max-score trajectories). The full XTX agent (which uses this replay) achieves the performance improvements reported above.",
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Prioritizing transitions from high-scoring trajectories helps create a useful trajectory buffer for imitation and improves exploitation; biased trajectory sampling (favoring shorter trajectories for a given score) helps avoid wasting time on noisy/meaningless steps.",
            "memory_limitations": "Paper does not report direct empirical limits of the buffer, but capacity N is unspecified; potential instability of Q-learning with novelty bonuses is discussed—XTX mitigates it by restricting Q-learning (with intrinsic bonuses) to the shorter-horizon exploration phase.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": "Use prioritized replay with priority fraction ρ=0.5, prioritize transitions from trajectories that achieved max score, sample k=10 trajectories with β1=1 (score bias) and large β2 (bias toward shortest length), and retrain π_il every n episodes using the resulting buffer.",
            "uuid": "e2783.1",
            "source_info": {
                "paper_title": "Multi-Stage Episodic Control for Strategic Exploration in Text Games",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "Policy cover / Trajectory buffer (self-imitation)",
            "name_full": "Policy cover constructed by sampling trajectories and self-imitation learning",
            "brief_description": "A small set of k past trajectories sampled from experience replay (biased by score and short length) that are used to train an imitation policy (π_il) to build a 'policy cover' — a mechanism to globally return to promising regions of the state space.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "trajectory buffer / policy cover (component of XTX)",
            "agent_description": "A collection of k trajectories sampled (with replacement) from the replay buffer using a two-step scoring/length-biased sampling; these trajectories form B and are used to train the GPT-2 imitation policy by minimizing cross-entropy over (context, action) pairs.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho",
            "game_description": "Interactive fiction games with sparse rewards and dynamic action sets; the trajectory buffer stores sequences of observations/actions from such games.",
            "uses_memory": true,
            "memory_type": "episodic trajectory memory / policy cover",
            "memory_structure": "buffer B of k sampled full trajectories (sequence of transitions); the imitation model ingests (a_{t-2}, a_{t-1}, o_t) contexts derived from these trajectories",
            "memory_content": "full past trajectories: sequences of observations, actions, and rewards; used to construct single-step (context, action) training pairs for imitation",
            "memory_capacity": "k trajectories (k=10 in experiments); sampling is with replacement from replay D",
            "memory_retrieval_strategy": "two-stage sampling: (1) sample a score u from the set of unique scores seen so far (probability biased by β1), (2) sample a trajectory among those with score u biased toward shorter length (probability biased by β2); repeat k times",
            "memory_update_strategy": "At every n episodes the trajectory buffer B is re-formed by sampling k trajectories from the continually updated replay D; π_il is retrained on B until convergence (≈40 passes reported empirically)",
            "memory_usage_purpose": "Create a policy cover so the agent can reliably return to promising frontiers (global exploitation) before switching to local exploration; helps reduce derailment and recover promising roll-in states",
            "performance_with_memory": "When incorporated into XTX, policy-cover based imitation contributes to empirical gains: XTX outperforms baselines across 12 games (average normalized score 56.3% deterministic). Ablation shows that pure imitation alone is insufficient, but the policy cover is necessary when combined with strategic exploration to achieve top performance.",
            "performance_without_memory": "The XTX-Uniform ablation (which still uses a policy cover but uses uniform random exploration in Phase 2) performs worse than full XTX in several games — showing that the policy cover alone is not sufficient and must be paired with strategic local exploration.",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "A policy cover built from a mixture of trajectories (different scores and preferably shorter lengths for a given score) helps the agent return to useful states and avoid wasted actions; however, imitation must be mixed with exploration (non-zero λ during Phase 1) to avoid local minima and bottlenecks.",
            "memory_limitations": "If π_il is trained only on high-scoring homogeneous trajectories, the agent can fall into local minima (insufficient diversity) and fail to explore novel actions; the paper uses explicit sampling biases and mixing to mitigate this.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": "Sample k=10 trajectories with score bias β1=1 and strong bias toward shorter trajectories (β2 large ~10k), retrain π_il with multiple passes (~40) every n episodes, and keep a small exploration mixture (λ&gt;0) during Phase 1.",
            "uuid": "e2783.2",
            "source_info": {
                "paper_title": "Multi-Stage Episodic Control for Strategic Exploration in Text Games",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "Knowledge-graph memories (prior work)",
            "name_full": "Learning dynamic knowledge graphs to generalize on text-based games / Graph-constrained RL",
            "brief_description": "Mentioned prior approaches that maintain a structured (graph) memory representing objects, relations, and state information across trajectories to mitigate partial observability; cited as focusing on language/state tracking but still facing large action-space/sparse-reward challenges.",
            "citation_title": "Learning dynamic knowledge graphs to generalize on text-based games",
            "mention_or_use": "mention",
            "agent_name": "knowledge-graph based agents (prior work cited)",
            "agent_description": "Prior agents maintain dynamic knowledge graphs (nodes for objects/rooms, edges for relations) to track state across time and assist planning/action selection; cited works include Ammanabrolu & Hausknecht (2020) and Adhikari et al. (2020).",
            "base_model_size": null,
            "game_benchmark_name": "Jericho (and other text-game environments)",
            "game_description": "Same family of interactive fiction games; these methods aim to address partial observability by building structured memories of discovered entities and relations.",
            "uses_memory": true,
            "memory_type": "graph-based memory / knowledge graph",
            "memory_structure": "dynamic graph (nodes and labeled edges) representing discovered objects, locations, and relations; may be updated per observation/action",
            "memory_content": "discovered objects, room descriptions, relations (e.g., location contains object), likely prior observed facts about the world",
            "memory_capacity": null,
            "memory_retrieval_strategy": "not specified in this paper (referenced work implements graph query/attention over graph nodes)",
            "memory_update_strategy": "not specified here; in cited works graphs are updated as new observations are made",
            "memory_usage_purpose": "Track persistent state to mitigate partial observability, assist in planning and avoid repeating fruitless actions",
            "performance_with_memory": "Cited as useful for partial observability and language semantics but reported in this paper to still suffer from the large/dynamic action space and sparse rewards in Jericho; no direct numeric comparison in this paper.",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Paper notes that knowledge-graph approaches help with partial observability/language semantics but do not by themselves solve the exploration-exploitation challenge posed by dynamic large action spaces and sparse rewards.",
            "memory_limitations": "Still suffer from large dynamic action spaces and sparse rewards; paper implies additional mechanisms for exploration/exploitation are needed beyond state-tracking graphs.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2783.3",
            "source_info": {
                "paper_title": "Multi-Stage Episodic Control for Strategic Exploration in Text Games",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "Go-Explore archives (prior work)",
            "name_full": "First return, then explore (Go-Explore) archive-based exploration",
            "brief_description": "Referenced multi-stage approach (Go-Explore) that stores archives of visited states/trajectories and explicitly returns to archived states before exploring; XTX contrasts with Go-Explore by not requiring such global archives and by using a learned imitation policy instead.",
            "citation_title": "First return, then explore",
            "mention_or_use": "mention",
            "agent_name": "Go-Explore (archive-based exploration)",
            "agent_description": "An exploration algorithm that (in the original formulation) stores an archive of visited states or cells, returns to promising archived states, and explores from them; often requires determinism or access to simulator restart functionality to reliably return to states.",
            "base_model_size": null,
            "game_benchmark_name": null,
            "game_description": "Designed for hard-exploration RL problems (originally in Atari and other deterministic environments); stores and revisits archived states.",
            "uses_memory": true,
            "memory_type": "external archive of visited states / trajectories",
            "memory_structure": "archive (database) of stored states/trajectories, often with metadata about score and reachability",
            "memory_content": "visited states, corresponding trajectories, and metadata (scores, actions to reach them)",
            "memory_capacity": null,
            "memory_retrieval_strategy": "archive lookup to select a stored state/trajectory to return to (various heuristics); often deterministic restart to that state",
            "memory_update_strategy": "append newly discovered states/trajectories to archive during exploration",
            "memory_usage_purpose": "Enable reliable return to promising states so exploration can continue from those states (addresses derailment), effective especially in deterministic domains",
            "performance_with_memory": "Go-Explore has produced state-of-the-art results in several hard-exploration deterministic domains; the paper contrasts XTX by noting Go-Explore requires archives and is limited in deterministic settings.",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Archive-based returning is powerful in deterministic domains, but XTX argues that maintaining large archives and requiring deterministic resets is impractical for the dynamic, stochastic text-game setting; XTX achieves comparable benefits without explicit archives by learning imitation roll-in policies.",
            "memory_limitations": "Requires determinism or simulator restart capability to reliably return to archived states; maintaining large archives can be cumbersome and not directly applicable to stochastic/text-game environments without extra assumptions.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2783.4",
            "source_info": {
                "paper_title": "Multi-Stage Episodic Control for Strategic Exploration in Text Games",
                "publication_date_yy_mm": "2022-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reading and acting while blindfolded: The need for semantics in text game agents",
            "rating": 2
        },
        {
            "paper_title": "Keep CALM and explore: Language models for action generation in text-based games",
            "rating": 2
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2
        },
        {
            "paper_title": "Learning dynamic knowledge graphs to generalize on text-based games",
            "rating": 2
        },
        {
            "paper_title": "First return, then explore",
            "rating": 2
        },
        {
            "paper_title": "Self-imitation learning",
            "rating": 1
        },
        {
            "paper_title": "Exploration based language learning for text-based games",
            "rating": 1
        }
    ],
    "cost": 0.019998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Multi-stage Episodic Control for Strategic EXPLORATION IN TEXT GAMES</h1>
<p>Jens Tuyls ${ }^{1}$, Shunyu Yao ${ }^{1}$, Sham Kakade ${ }^{2}$ \&amp; Karthik Narasimhan ${ }^{1}$<br>${ }^{1}$ Department of Computer Science, Princeton University<br>${ }^{2}$ John A. Paulson School of Engineering and Applied Sciences, Harvard University<br>{jtuyls, shunyuy, karthikn}@princeton.edu, sham@seas.harvard.edu</p>
<h4>Abstract</h4>
<p>Text adventure games present unique challenges to reinforcement learning methods due to their combinatorially large action spaces and sparse rewards. The interplay of these two factors is particularly demanding because large action spaces require extensive exploration, while sparse rewards provide limited feedback. This work proposes to tackle the explore-vs-exploit dilemma using a multi-stage approach that explicitly disentangles these two strategies within each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins each episode using an exploitation policy that imitates a set of promising trajectories from the past, and then switches over to an exploration policy aimed at discovering novel actions that lead to unseen state spaces. This policy decomposition allows us to combine global decisions about which parts of the game space to return to with curiosity-based local exploration in that space, motivated by how a human may approach these games. Our method significantly outperforms prior approaches by $27 \%$ and $11 \%$ average normalized score over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in both deterministic and stochastic settings, respectively. On the game of Zork1, in particular, XTX obtains a score of 103, more than a 2 x improvement over prior methods, and pushes past several known bottlenecks in the game that have plagued previous state-of-the-art methods. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>Text adventure games provide a unique test-bed for algorithms that integrate reinforcement learning (RL) with natural language understanding. Aside from the linguistic ingredient, a key challenge in these games is the combination of very large action spaces with sparse rewards, which calls for a delicate balance between exploration and exploitation. For instance, the game of ZORK1 can contain up to fifty valid action commands per state ${ }^{2}$ to choose from. Importantly, unlike other RL environments (Bellemare et al., 2013; Todorov et al., 2012), the set of valid action choices does not remain constant across states, with unseen actions frequently appearing in later states of the game. For example, Figure 1 shows several states from ZORK1 where a player has to issue unique action commands like 'kill troll with sword', 'echo' or 'odysseus' to progress further in the game. This requires a game-playing agent to perform extensive exploration to determine the appropriateness of actions, which is hard to bootstrap from previous experience. On the other hand, since rewards are sparse, the agent only gets a few high-scoring trajectories to learn from, requiring vigorous exploitation in order to get back to the furthest point of the game and make progress thereon. Prior approaches to solving these games (He et al., 2016a; Hausknecht et al., 2020; Ammanabrolu \&amp; Hausknecht, 2020; Guo et al., 2020) usually employ a single policy and action selection strategy, making it difficult to strike the right balance between exploration and exploitation.</p>
<p>In this paper, we propose eXploit-Then-eXplore (XTX), an algorithm for multi-stage control to explicitly decompose the exploitation and exploration phases within each episode. In the first phase, the agent selects actions according to an exploitation policy which is trained using self-imitation</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Sample game paths and state observations from ZORK1. Starting from the leftmost state ('West of House'), the agent encounters several novel and unique valid actions (e.g Odysseus, Echo) (in brown) across different states in the game. In order to make progress, our algorithm (XTX) strategically re-visits different frontiers in the state space (red and blue circles) and performs strategic local exploration to overcome bottleneck states (e.g. 'Troll Room') and dead-ends (e.g. 'Cellar'). Solid borders indicate visited states, dotted ones indicate potential future states.
learning on a mixture of promising trajectories from its past experience sampled using a combination of factors such as episodic scores and path length. This policy allows the agent to return to a state at the frontier of the state space it has explored so far. Importantly, we ensure that this policy is trained on a mixture of trajectories with different scores, in order to prevent the agent from falling into a local minimum in the state space (e.g. red space in Figure 1). In the second phase, an exploration policy takes over and the agent chooses actions using a value function that is trained using a combination of a temporal difference (TD) loss and an auxiliary inverse dynamics loss (Pathak et al., 2017). This allows the agent to perform strategic exploration around the frontier by reusing values of previously seen actions while exploring novel ones in order to find rewards and make progress in the game. To allow for more fine-grained control, we use a mixture of policies for both exploration and exploitation, and only change a single interpolation parameter to switch between phases.</p>
<p>The two-stage approach to gameplay in XTX allows an agent to combine global decisions about which parts of the game space to advance, followed by local exploration of sub-strategies in that space. This is similar to how humans tackle these games: if a player were to lose to a troll in the dungeon, they would immediately head back to the dungeon after the game restarts and explore strategies thereon to try and defeat the troll. XTX's multi-stage episodic control differs from prior approaches that add exploration biases to a single policy through curiosity bonuses (Pathak et al., 2017; Tang et al., 2017) or use different reward functions to train a separate exploration policy (Colas et al., 2018; Schäfer et al., 2021; Whitney et al., 2021). Moreover, in contrast to methods like GoExplore (Ecoffet et al., 2021; Madotto et al., 2020), XTX does not have global phases of random exploration followed by learning - instead, both our policies are continuously updated with new experience, allowing XTX to adapt and scale as the agent goes deeper into the game. XTX also does not make any assumptions about the environment being deterministic, and does not require access to underlying game simulator or additional memory archives to keep track of game trees.</p>
<p>We evaluate XTX on a set of games from the Jericho benchmark (Hausknecht et al., 2020), considering both deterministic and stochastic variants of the games. XTX outperforms competitive baselines on all 12 games, and achieves an average improvement of $5.8 \%$ in terms of normalized scores across all games. For instance, on Zork1, our method obtains a score of 103 in the deterministic setting and 67 in the stochastic setting - substantial improvements over baseline scores of 44 and 41, respectively. We also perform ablation studies to demonstrate the importance of the multi-stage approach, as well as several key design choices in our exploitation and exploration policies.</p>
<h1>2 Related Work</h1>
<p>Reinforcement learning for text-based games Prior work on building autonomous agents for text adventure games has explored several variants of reinforcement learning (RL) agents equipped with a language understanding module (see Osborne et al. (2021) for a detailed survey). Innovations on</p>
<p>the language representation side include using deep neural networks for handling text sequences trained using RL (Narasimhan et al., 2015; He et al., 2016a), knowledge graphs to track states across trajectories (Ammanabrolu \&amp; Hausknecht, 2020; Adhikari et al., 2020; Xu et al., 2020), and incorporating question answering or reading comprehension modules (Ammanabrolu et al., 2020; Guo et al., 2020). While these approaches focus mainly on the issues of partial observability and language semantics, they all suffer from challenges due to the large action space and sparse rewards found in games from benchmarks like Jericho (Hausknecht et al., 2020). Some approaches aim to navigate the large action space by filtering inadmissible actions (Zahavy et al., 2018; Jain et al., 2020), leveraging pre-trained language models for action selection (Yao et al., 2020; Jang et al., 2020) or word embeddings for affordance detection (Fulda et al., 2017). Recent work has also explored tackling sparse rewards by employing hierarchical policies (Xu et al., 2021).</p>
<p>Navigating the exploration-exploitation trade-off in RL The trade-off between exploration and exploitation is a well-known issue in RL (Sutton \&amp; Barto, 2018; François-Lavet et al., 2018; Kearns \&amp; Singh, 2002; Brafman \&amp; Tennenholtz, 2002). In this respect, we can broadly categorize prior techniques into two types. The first type includes methods with mixed objectives that balance exploration with exploitation. Oh et al. (2018) introduced the idea of self-imitation learning on highscoring episodes to exploit good trajectories, as an auxiliary objective to standard actor-critic methods. Prior work has also explored the addition of curiosity bonuses to encourage exploration (Pathak et al., 2017; Tang et al., 2017; Li et al., 2020; Bellemare et al., 2016; Machado et al., 2020; Taiga et al., 2021). While we leverage self-imitation learning for exploitation and inverse dynamics bonuses for exploration, we use a multi-stage mixed policy. Other works learn a mixture of policies for decoupling exploration and exploitation, either by using a conditional architecture with shared weights (Badia et al., 2020), pre-defining an exploration mechanism for restricted policy optimization (Shani et al., 2019), or learning separate task and exploration policies to maximize different reward functions (Colas et al., 2018; Schäfer et al., 2021; Whitney et al., 2021). While we also train multiple policies, our multi-stage algorithm performs distinct exploitation and exploration phases within each episode, not requiring pre-defined exploration policies or phases. Further, we consider environments with significantly larger action spaces that evolve dynamically as the game progresses.
The second class of algorithms explicitly separate exploitation and exploration in each episode. Methods like $E^{3}$ (Kearns \&amp; Singh, 2002; Henaff, 2019) maintain a set of dynamics models to encourage exploration. Policy-based Go-Explore (Ecoffet et al., 2021) uses self-imitation learning to 'exploit' high-reward trajectories, but requires choosing intermediate sub-goals for the agent to condition its policy on. PC-PG (Agarwal et al., 2020) uses a policy cover to globally choose state spaces to return to, followed by random exploration. Compared to these approaches, we perform more strategic local exploration due to the use of a Q-function with inverse dynamics bonus and do not require any assumptions about determinism or linearity of the MDP. We provide a more technical discussion on the novelty of our approach at the end of Section 3.</p>
<p>Directed exploration in text-based games As previously mentioned, the large dynamic action space in text games warrant specific strategies for directed exploration. Ammanabrolu et al. (2020) used a knowledge-graph based intrinsic motivation reward to encourage exploration. Jang et al. (2020) incorporated language semantics into action selection for planning using MCTS. Both methods utilize the determinism of the game or require access to a simulator to restart the game from specific states. Madotto et al. (2020) modified the Go-Explore algorithm to test generalization in the CoinCollector (Yuan et al., 2018) and Cooking world domains (Côté et al., 2018). Their method has two phases - the agent first randomly explores and collects trajectories and then a policy is learned through imitation of the best trajectories in the experience replay buffer. In contrast, our algorithm provides for better exploration of new, unseen actions in later stages of the game through the use of an inverse dynamics module and performs multiple rounds of imitation learning for continuous scaling to deeper trajectories in the game. Recently, Yao et al. (2021) used inverse dynamics to improve exploration and Yao et al. (2020) used a language model to generate action candidates that guide exploration. However, both approaches did not employ a two-stage rollout like our work, and the latter considers a different setup without any valid action handicap.</p>
<h1>3 Method</h1>
<p>Background Text-adventure games can be formalized as a Partially Observable Markov Decision Process (POMDP) $\langle S, T, A, O, R, \gamma\rangle$. The underlying state space $S$ contains all configurations of the game state within the simulator, which is unobserved by the agent. The agent receives observations from $O$ from which it has to infer the underlying state $s \in S$. The action set $A$ consists of short phrases from the game vocabulary, $T\left(s^{\prime} \mid s, a\right)$ is the transition function which determines the probability of moving to the next state $s^{\prime}$ given the agent has taken action $a$ in state $s, R(s, a)$ determines the instantaneous reward, and $\gamma$ is the reward discount factor.</p>
<p>Existing RL approaches that tackle these games usually learn a value function using game rewards. One example is the Deep Reinforcement Relevance Network (DRRN) (He et al., 2016b) which trains a deep neural network with parameters $\phi$ to approximate $Q_{\phi}(o, a)$. This model encodes each observation $o$ and action candidate $a$ using two recurrent networks $f_{o}$ and $f_{a}$ and aggregates the representations to derive the Q-value through an MLP $q: Q_{\phi}(o, a)=q\left(f_{o}(o), f_{a}(a)\right)$. The parameters $\phi$ of the model are trained by minimizing the temporal difference (TD) loss on tuples $\left(o, a, r, o^{\prime}\right)$ of observation, action, reward and the next observation sampled from an experience replay buffer:</p>
<p>$$
\mathcal{L}<em a_prime="a^{\prime">{\mathrm{TD}}(\phi)=\left(r+\gamma \max </em>
$$} \in A} Q_{\phi}\left(o^{\prime}, a^{\prime}\right)-Q_{\phi}(o, a)\right)^{2</p>
<p>The agent samples actions using a softmax exploration policy $\pi(a \mid o ; \phi) \propto \exp \left(Q_{\phi}(o, a)\right)$.
Challenges There are two unique aspects of text adventure games that make them challenging. First, the action space is combinatorially large - usually games can accept action commands of 1-4 words with vocabularies of up to 2257 words. This means an agent potentially has to choose between $\mathcal{O}\left(2257^{4}\right)=2.6 * 10^{13}$ actions at each state of the game. To make this problem more tractable, benchmarks like Jericho (Hausknecht et al., 2020) provide a valid action detector that filters out the set of inadmissible commands (i.e. commands that are either unrecognized by the game engine or do not change the underlying state of the game). However, this still results in the issue of dynamic action spaces for the agent that change with the state. For example, the action 'echo' is unique to the Loud Room (see Figure 1). Second, these games have very sparse rewards (see Appendix A.1) and several bottleneck states (Ammanabrolu et al., 2020), making learning difficult for RL agents that use the same policy for exploration and exploitation (He et al., 2016a; Hausknecht et al., 2020; Ammanabrolu \&amp; Riedl, 2019). This results in issues of derailment (Ecoffet et al., 2021), with agents unable to return to promising parts of the state space, resulting in a substantial gap between average episode performance of the agent and the maximum score it sees in the game (Yao et al., 2020).</p>
<h3>3.1 OUR ALGORITHM: EXPLOIT-THEN-EXPLORE (XTX)</h3>
<p>We tackle the two challenges outlined above using a multi-stage control policy that allows an agent to globally pick promising states of the game to visit while allowing for strategic local exploration thereafter. To this end, we develop eXploit-Then-eXplore, where an agent performs the following two distinct phases of action selection in each episode.</p>
<p>PHASE 1: Exploitation In the exploitation phase, the agent makes a global decision about revisiting promising states of the game it has seen in its past episodes. We sample $k$ trajectories from the experience replay $\mathcal{D}$ using a combination of factors such as game score and trajectory length. These trajectories are then used to learn a policy cover $\pi_{\text {exploit }}$ using self-imitation learning (Oh et al., 2018) (see Section 3.2 for details). The agent then samples actions from $\pi_{\text {exploit }}$ until it has reached either (1) the maximum score seen during training or (2) a number of steps in the episode equal to the longest of the $k$ sampled trajectories. The second condition ensures the agent can always return to the longest of the $k$ sampled trajectories ${ }^{3}$. Once either of the above conditions is satisfied, the agent transitions to the exploration phase, adjusting its policy.</p>
<p>PHASE 2: Exploration In the exploration phase, the agent uses a different policy $\pi_{\text {explore }}$ trained using both a temporal difference loss and an auxiliary inverse dynamics bonus (Pathak et al., 2017; Yao et al., 2021) (see Section 3.3 for details). The intuition here is that the exploitation policy $\pi_{\text {exploit }}$ in phase 1 has brought the agent to the game frontier, which is under-explored and may</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>contain a combination of common (e.g. "open door") and novel (e.g. "kill troll with sword") actions. Therefore, using a combination of Q-values and inverse dynamics bonus enables the agent to perform strategic, local exploration to expand the frontier and discover new rewarding states. The agent continues sampling from $\pi_{\text {explore }}$ until a terminal state or episode limit is reached.</p>
<h1>3.1.1 Mixture of POLICIES FOR FINE-GRAINED CONTROL</h1>
<p>While one could employ two completely disjoint policies for the exploitation and exploration phases, we choose a more general approach of having a policy mixture, with a single parameter $\lambda$ that can be varied to provide more fine-grained control during the two phases. Specifically, we define:</p>
<p>$$
\pi_{\lambda}(a \mid c, o ; \theta, \phi, \xi)=\lambda \pi_{\mathrm{inv}-\mathrm{dy}}(a \mid o ; \theta, \phi)+(1-\lambda) \pi_{\mathrm{il}}(a \mid c ; \xi)
$$</p>
<p>Here, $\pi_{\text {inv-dy }}$ refers to an exploration-inducing policy trained using TD loss with an inversedynamics bonus (Section 3.3) and is parameterized by $\theta$ and $\phi . \pi_{\mathrm{il}}$ refers to an exploitation-inducing policy trained through self-imitation learning (Oh et al., 2018) (Section 3.2) and is parameterized by $\xi$. Note that the action distribution over actions $a$ induced by $\pi_{\text {inv-dy }}$ is conditioned only on the current observation $o$, while the one induced by $\pi_{\mathrm{il}}$ is conditioned on context $c$ which is an augmentation of $o$ with past information. We can observe that $\lambda$ provides a trade-off between exploration (high $\lambda$ ) and exploitation (low $\lambda$ ). In our experiments, we choose a small, dynamic value, $\lambda=\frac{1}{2 \tau \tau}$ for exploitation (where $T$ is episode limit) and $\lambda=1$ for exploration. As we demonstrate later (Section 4.2), the non-zero $\lambda$ in exploitation is critical for the agent to avoid getting stuck in regions of local minima (e.g. Zork1). We now describe the individual components of the mixture.</p>
<h3>3.2 IMITATION LEARNING FOR BUILDING A GLOBAL POLICY COVER $\left(\pi_{\mathrm{il}}\right)$</h3>
<p>We parameterize the imitation policy $\pi_{\mathrm{il}}$ using a Transformer model (Vaswani et al., 2017) based on the GPT-2 architecture (Radford et al., 2019) that takes in a context $c=\left[a_{t-2} ; a_{t-1} ; o_{t}\right]$, i.e. the concatenation of two most recent past actions along with the current observation separated by [SEP] tokens, and outputs a sequence of hidden representations $h_{0}, \ldots, h_{m}$ where $m$ is the number of tokens in the sequence. $h_{m}$ is then projected to vocabulary size by multiplication with the output embedding matrix, after which softmax is applied to get probabilities for the next action token. Inspired by (Yao et al., 2020), the GPT-2 model is trained to predict the next action $a_{t}$ given the context $c$ using a language modeling objective (Equation 5). The training data consists of $k$ trajectories sampled from an experience replay memory $\mathcal{D}$ which stores transition tuples $\left(c_{t}, a_{t}, r_{t}, o_{t+1}\right.$, terminal $)$.
Sampling trajectories Let us define a trajectory $\tau$ as a sequence of observations, actions, and rewards, i.e. $\tau=o_{1}, a_{1}, r_{1}, o_{2}, a_{2}, r_{2} \ldots, o_{l+1}$, where $l_{\tau}$ denotes the trajectory length (i.e. number of actions) and thus $l \leq T$ where $T$ is the episode limit. We sample a trajectory from $\mathcal{D}$ using a two-step process. First, we sample a score $u$ from a categorical distribution:</p>
<p>$$
P(u) \propto \exp \left(\beta_{1}\left(u-\mu_{\mathcal{U}}\right) / \sigma_{\mathcal{U}}\right), \quad u \in \mathcal{U}
$$</p>
<p>where $\mathcal{U}$ is the set of all unique scores encountered in the game so far, $\mu_{\mathcal{U}}$ is the mean of the set $\mathcal{U}$, and $\sigma_{\mathcal{U}}$ is its standard deviation. $\beta_{1}$ is the temperature and determines how biased the sampling process is towards high scores. The second step collects all trajectories with the sampled score $u$ and samples a trajectory $\tau$ based on the trajectory length $l_{\tau}$ :</p>
<p>$$
P(\tau \mid u) \propto \exp \left(-\beta_{2}\left(l_{\tau}-\mu_{\mathbb{L}<em _mathbb_L="\mathbb{L">{u}}\right) / \sigma</em><em _tau="\tau">{u}}\right), \quad l</em>
$$} \in \mathbb{L}_{u</p>
<p>where $\mathbb{L}<em _tau="\tau">{u}$ is the multiset of trajectory lengths $l</em>}$ with score $u, \mu_{\mathbb{L<em u="u">{u}}$ is the mean of the elements in $\mathbb{L}</em>}$, and $\sigma_{\mathbb{L<em 2="2">{u}}$ is its standard deviation. $\beta</em>$ on which we perform imitation learning. This allows the agent to globally explore the game space by emulating promising experiences from its past, with a bias towards trajectories with high game score and shorter lengths. The motivation for sampling shorter length trajectories among the ones that reach the same score is because those tend to be the ones that waste less time performing meaningless actions (e.g. "pick up sword", "drop sword", etc.).
Learning from trajectories Given the trajectory buffer $\mathcal{B}$ containing single-step ( $c$, a) pairs of context $c$ and actions $a$ from the trajectories sampled in the previous step, we train $\pi_{\mathrm{il}}$ by minimizing the cross-entropy loss over action commands (Yao et al., 2020):}$ defines the temperature and determines the strength of the bias towards shorter length trajectories. We perform this sampling procedure $k$ times (with replacement) to obtain a trajectory buffer $\mathcal{B</p>
<p>$$
\mathcal{L}(\xi)=-\mathbb{E}<em _mathrm_il="\mathrm{il">{(c, a) \sim \mathcal{B}} \log \pi</em>(a \mid c ; \xi)
$$}</p>
<p>where $c$ defines the context of past observations and actions as before, and $\xi$ defines the parameters of the GPT-2 model. We perform several passes of optimization through the trajectory buffer $\mathcal{B}$ until convergence ${ }^{4}$ and periodically perform this optimization every $n$ epochs in gameplay to update $\pi_{il}$. Furthermore, $\pi_{i l}$ is renormalized over the valid action set $A_{v}$ during gameplay. Note that while $\pi_{i l}$ is trained similarly to the GPT-2 model in (Yao et al., 2020), their model generates action candidates.</p>
<h1>3.3 EFFICIENT LOCAL EXPLORATION WITH INVERSE DYNAMICS $\left(\pi_{\text {inv-dy }}\right)$</h1>
<p>In the second phase of our algorithm, we would like to use a policy that tackles (1) the large action space and (2) the dynamic nature of the action set at every step in the game, which makes it crucial to keep trying under-explored actions and is difficult for the Q network alone to generalize over. To this end, we use the inverse dynamics model (INV-DY) from (Yao et al., 2021). INV-DY is a Qbased policy $\pi_{\text {inv-dy }}$ similar to DRRN (He et al., 2016a), optimized with the standard TD loss (see Background of Section 3). In addition, it adds an auxiliary loss $\mathcal{L}<em _game="{game" _text="\text">{\text {inv }}$ capturing an inverse dynamics prediction error (Pathak et al., 2017), which is added as an intrinsic reward to the game reward ( $r=r</em>}}+\alpha_{1} * \mathcal{L<em _inv="{inv" _text="\text">{\text {inv }}$ ) and hence incorporated into the TD loss. Formally, $\mathcal{L}</em>$ is defined as:}</p>
<p>$$
\mathcal{L}<em d="d">{\mathrm{inv}}(\theta, \phi)=-\log p</em>\right)\right)\right)\right.
$$}\left(a \mid g_{\mathrm{inv}}\left(\operatorname{concat}\left(f_{o}(o), f_{o}\left(o^{\prime</p>
<p>where $\theta$ denotes the parameters for the recurrent decoder $d$ and the MLP $g_{i n v}$ (neither of which are used in $\pi_{\text {inv-dy }}$ during gameplay to score the actions), and $f_{o}$ is the encoder defined in Section 3. This loss is optimized together with the TD loss as well as with an action decoding loss $\mathcal{L}<em _inv-dy="{inv-dy" _text="\text">{\text {dec }}$ to obtain the following overall objective that is used to train $\pi</em>$ :}</p>
<p>$$
\mathcal{L}(\phi, \theta)=\mathcal{L}<em 2="2">{\mathrm{TD}}+\alpha</em>} \mathcal{L<em 3="3">{\mathrm{inv}}(\phi, \theta)+\alpha</em>(\phi, \theta)
$$} \mathcal{L}_{\mathrm{dec}</p>
<p>where $\mathcal{L}<em d="d">{\text {dec }}(\phi, \theta)=-\log p</em>$ (Schaul et al., 2015) and performing stochastic gradient descent. Inverse dynamics ameliorates the challenges (1) and (2) mentioned above by rewarding underexplored actions (i.e. a high loss in Equation 6) and by generalizing over novel action commands. Specifically, the INV-DY network might generalize through learning of past bonuses to what new actions would look like and hence identify novel actions before having tried them once.}\left(a \mid f_{a}(a)\right)$. Here, $f_{a}$ is a recurrent network (see Section 3). Please refer to Yao et al. (2021) for details. We train the model by sampling batches of transitions from a prioritized experience replay buffer $\mathcal{D</p>
<h3>3.4 EPISODIC ROLLOUTS WITH XTX (ALGORITHM 1)</h3>
<p>We now describe how XTX operates in a single episode. The agent starts in phase 1 , where actions $a_{t}$ are sampled from $\pi_{\text {exploit }}$. Following this exploitation policy brings the agent to the game frontier, which we estimate to happen when either (1) the current episode score $\geq M$, the maximum score in the trajectory buffer $\mathcal{B}$ or when (2) the current time step $t&gt;l_{\max }$, the length of the longest trajectory in $\mathcal{B}$. The agent then enters phase 2 and switches its strategy to sample actions only from $\pi_{\text {inv-dy }}$ by setting $\lambda=1$. At every time step $t$ during all phases, a transition tuple $(c_{t}, a_{t}, r_{t}, o_{t+1}$, terminal $)$ is added to the replay buffer $\mathcal{D}$. The policy $\pi_{\text {exploit }}$ is updated every $n$ episodes using the process in Section 3.2, while $\pi_{\text {explore }}$ is updated within episodes at every step using the TD loss in equation 7, sampling high rewarding trajectories with priority fraction $\rho$, similar to (Guo et al., 2020) ${ }^{5}$.</p>
<h3>3.5 NOVELTY IN COMPARISON TO PRIOR ALGORITHMS</h3>
<p>We now more explicitly discuss comparisons to a few other approaches. The most closely related approaches are multi-stage algorithms, including Go-Explore (Ecoffet et al., 2021) and PC-PG (Agarwal et al., 2020) (and somewhat the $E^{3}$ algorithm (Kearns \&amp; Singh, 2002; Henaff, 2019)). Both of these algorithms can be viewed as approaches which explicitly use a "roll-in" policy, with the goal of visiting a novel region of the state-action space. Go-Explore is limited to deterministic MDPs (where it is easy to re-visit any state in the past), while PC-PG (applicable to more general MDPs with provable guarantees under certain linearity assumptions) more explicitly builds a set of policies ('policy cover') capable of visiting different regions of the state space. However, in both of these</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 The eXploit-Then-eXplore (XTX) algorithm
    Initialize prioritized replay memory \(\mathcal{D}\) to capacity N with priority fraction \(\rho\).
    Initialize \(\pi_{\text {exploit }}\) (with parameters \(\xi\) ) and \(\pi_{\text {explore }}\) (with parameters \(\theta\) and \(\phi\) ).
    Set max score \(M\), max length \(l_{\max }\) in \(\mathcal{B}\) to 0 .
    Exploration steps \(R=50\); episode limit \(T=50\)
    for \(\operatorname{episode} \leftarrow 1, \ldots, E\) do
        for \(t \leftarrow 1, \ldots, T\) do
            Receive observation \(o_{t}\) and valid action set \(A_{v} \subset A\) for current state.
            if current episode score \(&lt;M\) and \(t&lt;T-R\) then \(\triangleright\) PHASE 1
                \(\lambda \leftarrow \frac{1}{2 * T}\)
            else
                \(\lambda \leftarrow 1\)
            end if
            Sample an action \(a_{t}\) from policy \(\pi_{\lambda}\left(a_{t} \mid o_{t}, a_{t-1}, a_{t-2} ; \phi, \theta, \xi\right) . \quad \triangleright\) Equation 2
            Step with \(a_{t}\) and receive \(\left(r_{t}, o_{t+1}\right.\), terminal \()\) from game engine.
            Store transition tuple \(\left(c_{t}, a_{t}, r_{t}, o_{t+1}\right.\), terminal \()\) in \(\mathcal{D}\).
            Update \(\pi_{\mathrm{inv}-\mathrm{dy}}\) using TD loss with inverse dynamics. \(\triangleright\) Equation 7
        end for
        if \(n\) episodes have passed then
            Sample \(k\) trajectories from \(\mathcal{D}\) to form the new trajectory buffer \(\mathcal{B}\). \(\triangleright\) Section 3.2
            Update \(\pi_{\mathrm{il}}\) with cross-entropy loss. \(\triangleright\) Equation 5
            Update \(M, l_{\max}\) and set \(T \leftarrow l_{\max }+R\).
        end if
    end for
</code></pre></div>

<p>approaches, once the agent reaches a novel part of the state-space, the agent acts randomly. A key distinction in our approach is that once the agent reaches a novel part of the state space, it uses an exploration with novelty bonuses, which may more effectively select promising actions over a random behavioral policy in large action spaces.</p>
<p>The other broad class of approaches that handle exploration use novelty bonuses, with either a policy gradient approach or in conjunction with $Q$-learning (see Section 2). The difficulty with the former class of algorithms is the "catastrophic forgetting" effect (see Agarwal et al. (2020) for discussion). The difficulty with $Q$-learning approaches (with a novelty bonus) is that bootstrapping approaches, with function approximation, can be unstable in long planning horizon problems (sometimes referred to as the "deadly triad" (Jiang et al., 2021)). While XTX also uses $Q$-learning (with a novelty bonus), we only use this policy ( $\pi_{\text {inv }-d y}$ ) in the second phase of the algorithm in contrast to (Yao et al., 2021) where $\pi_{\text {inv-dy }}$ is used throughout the entire episode; our hope is that this instability can be alleviated since we are effectively using $Q$-learning to solve a shorter horizon exploration problem (as opposed to globally using $Q$-learning, with a novelty bonus).</p>
<h1>4 EXPERIMENTS</h1>
<p>Environments We evaluate on 12 human-created games across several genres from the Jericho benchmark (Hausknecht et al., 2020). They provide a variety of challenges such as darkness, nonstandard actions, inventory management, and dialog (Hausknecht et al., 2020). At every step $t$, the observation from the Jericho game engine contains a description of the state, which is augmented with location and inventory information (by issuing "look" and "inventory" commands) to form $o_{t}$ (Hausknecht et al., 2019). In addition, we use of the valid action handicap provided by Jericho, which filters actions to remove those that do not change the underlying game state. We found this action handicap to be imperfect for some games (marked with * in Table 1), and manually added some actions required for agent progress from game walkthroughs to the game engine's grammar.</p>
<p>Evaluation We evaluate agents under two settings: (a) a deterministic setting where the transition dynamics $T\left(s^{\prime} \mid s, a\right)$ is a one-hot vector over all the next states $s^{\prime}$ and (b) a stochastic setting ${ }^{6}$ where the $T\left(s^{\prime} \mid s, a\right)$ defines a distribution over next states $s^{\prime}$, and the observations $o$ can be perturbed with</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Games</th>
<th style="text-align: center;">DRRN</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">INV-DY</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RC-DQN</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">XTX-Uniform</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">XTX (ours)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\Delta(\%)$</th>
<th style="text-align: center;">Game Max</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ZORK1</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">105.0</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">103.4</td>
<td style="text-align: center;">152.7</td>
<td style="text-align: center;">$+17 \%$</td>
<td style="text-align: center;">350</td>
</tr>
<tr>
<td style="text-align: center;">INHUMANE*</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">$+5 \%$</td>
<td style="text-align: center;">90</td>
</tr>
<tr>
<td style="text-align: center;">LUDICORP*</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">$+8 \%$</td>
<td style="text-align: center;">150</td>
</tr>
<tr>
<td style="text-align: center;">ZORK3*</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">$+6 \%$</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">PENTARI*</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">$+6 \%$</td>
<td style="text-align: center;">70</td>
</tr>
<tr>
<td style="text-align: center;">DETECTIVE</td>
<td style="text-align: center;">289.9</td>
<td style="text-align: center;">320.0</td>
<td style="text-align: center;">289.5</td>
<td style="text-align: center;">323.3</td>
<td style="text-align: center;">269.3</td>
<td style="text-align: center;">346.7</td>
<td style="text-align: center;">296.0</td>
<td style="text-align: center;">336.7</td>
<td style="text-align: center;">312.2</td>
<td style="text-align: center;">340.0</td>
<td style="text-align: center;">$+4 \%$</td>
<td style="text-align: center;">360</td>
</tr>
<tr>
<td style="text-align: center;">BALANCES*</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">$+4 \%$</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">LIBRARY*</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">$+8 \%$</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;">DEEPHOME*</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">$+6 \%$</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: center;">ENCHANTER*</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">$+2 \%$</td>
<td style="text-align: center;">400</td>
</tr>
<tr>
<td style="text-align: center;">DRAGON $^{7}$</td>
<td style="text-align: center;">$-3.7$</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">$-2.3$</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">126.0</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">127.0</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: center;">OMNIQUEST</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">$+3 \%$</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Avg. Norm Score</td>
<td style="text-align: center;">29.5\%</td>
<td style="text-align: center;">48.8\%</td>
<td style="text-align: center;">28.4\%</td>
<td style="text-align: center;">51.8\%</td>
<td style="text-align: center;">29.7\%</td>
<td style="text-align: center;">44.5\%</td>
<td style="text-align: center;">49.2\%</td>
<td style="text-align: center;">58.6\%</td>
<td style="text-align: center;">56.3\%</td>
<td style="text-align: center;">64.0\%</td>
<td style="text-align: center;">5.8\%</td>
<td style="text-align: center;">100\%</td>
</tr>
</tbody>
</table>
<p>Table 1: Results on deterministic games for the best XTX model, where the inverse dynamics scaling coefficient $\alpha_{1}$ was tuned per game. We outperform the baselines on all 12 games, achieving an average normalized game score of $56 \%$. * indicates actions were added to the game grammar. $\Delta$ indicates the absolute performance difference between XTX and the best baseline on Avg scores. Scores are averaged across 3 seeds. Baselines were rerun with the latest Jericho version.
irrelevant sentences such as "you hear in the distance the chirping of a song bird". We report both the episode score average (Avg) over the last 100 episodes at the end of training, as well as the maximum score (Max) seen in any episode during training.</p>
<p>Baselines We consider four baselines. 1) DRRN (He et al., 2016a): This model uses a Q-based softmax policy, i.e. $\pi \propto \exp (Q(o, a ; \phi))$, parameterized using GRU encoders and decoders, and trained using the TD loss (Equation 1). 2) INV-DY (Yao et al., 2021): Refer to Section 3.3. 3) RC-DQN (Guo et al., 2020): This is a state-of-the-art model that uses an object-centric reading comprehension (RC) module to encode observations and output actions. The training loss is that of DRRN above, and during gameplay, the agent uses an $\epsilon$-greedy strategy. 4) XTX-Uniform ( $\sim$ GoExplore): Here, we replace $\pi_{\text {inv-dy }}$ with a policy that samples actions uniformly during Phase 2, keeping all other factors of our algorithm the same. This is closest to a version of the Go-Explore algorithm (Ecoffet et al., 2021) that returns to promising states and performs random exploration. However, while conceptually similar to Go-Explore, XTX-Uniform does not make use of any additional memory archives, and avoids training a goal-based policy. See Appendix A. 2 for implementation details and hyperparameters.</p>
<h1>4.1 RESULTS</h1>
<p>Deterministic games We report results in Table 1 (refer to Appendix A. 5 and A. 6 for more details). Overall, XTX outperforms DRRN, INV-DY, RC-DQN, and XTX-Uniform by $27 \%, 28 \%, 27 \%$, and $7 \%$ respectively, in terms of average normalized game score (i.e. average episode score divided by max score, averaged over all the games). Our algorithm performs particularly well on Zork1, achieving a $17 \%$ absolute improvement in average episode score and a $14 \%$ improvement in average maximum score compared to the best baseline. In particular, XTX manages to advance past several documented bottlenecks like the dark Cellar (see Figure 1) which have proved to be very challenging for existing methods (Ammanabrolu et al., 2020). While performance with XTX-Uniform is sometimes close, exploration with inverse dynamics instead of random exploration pushes past several bottlenecks present in Zork1 and leads to significant gains on Deephome, Enchanter, Omniquest, and Ludicorp, showing the potential usefulness of strategic exploration at the game frontier.
Stochastic games To show the robustness of XTX to stochasticity, we evaluate our agent in the stochastic setting (Table 2). XTX outperforms the baselines on 4 out of 5 games, and pushes past the maximum scores of XTX-Uniform on the same fraction. Especially impressive is the performance on Zork1, which is still higher than the state-of-the-art score in the deterministic setting.</p>
<h3>4.2 Ablation Studies</h3>
<p>In order to evaluate the importance of the various components in XTX, we perform several ablations on a subset of the games as described below and shown in Figure 2 (more in Appendix A. 3 and A.4).</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Games</th>
<th style="text-align: center;">DRRN</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">INV-DY</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RC-DQN</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">XTX-Uniform</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">XTX (ours)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\Delta(\%)$</th>
<th style="text-align: center;">Game Max</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ZORK1</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">143.0</td>
<td style="text-align: center;">$+8 \%$</td>
<td style="text-align: center;">350</td>
</tr>
<tr>
<td style="text-align: center;">ZORK3*</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">$-1 \%$</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">PENTARI*</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">$+12 \%$</td>
<td style="text-align: center;">70</td>
</tr>
<tr>
<td style="text-align: center;">DEEPHOME*</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">$+4 \%$</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: center;">ENCHANTER*</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">$+1 \%$</td>
<td style="text-align: center;">400</td>
</tr>
<tr>
<td style="text-align: center;">Avg. Norm Score</td>
<td style="text-align: center;">$18.9 \%$</td>
<td style="text-align: center;">$39.0 \%$</td>
<td style="text-align: center;">$19.7 \%$</td>
<td style="text-align: center;">$41.5 \%$</td>
<td style="text-align: center;">$20.9 \%$</td>
<td style="text-align: center;">$30.5 \%$</td>
<td style="text-align: center;">$24.3 \%$</td>
<td style="text-align: center;">$39.1 \%$</td>
<td style="text-align: center;">$31.8 \%$</td>
<td style="text-align: center;">$48.9 \%$</td>
<td style="text-align: center;">$4.8 \%$</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Results on stochastic games. We outperform baselines on 4 out of 5 games, with an average normalized game score of $32 \%$. Scores are averaged across 3 seeds. Baselines were rerun with the latest Jericho version.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Average episode scores for 4 ablation models across 6 games. Overall, we find both the strategic inverse dynamics policy and the explicit exploitation policy to be key for our algorithm.</p>
<p>Pure imitation learning $(\lambda=0)$ This ablation sets $\lambda=0$ in equation 2, meaning the agent will always use the imitation learning policy $\pi_{\mathrm{il}}$. As expected, this model performs quite badly since it is based on pure exploitation and is hence unlikely to reach deep states in the game.</p>
<p>Pure inverse dynamics $(\lambda=1)$ This ablation sets $\lambda=1$ in equation 2, hence always using the inverse dynamics exploration policy $\pi_{\text {inv-dy }}$, resulting in the model proposed in (Yao et al., 2021). While this model can sometimes achieve high maximum scores, it is unable to learn from these and hence its average episode score remains quite low, consistent with findings in (Yao et al., 2021).</p>
<p>Mixing exploration and exploitation $(\lambda=0.5)$ By setting $\lambda=0.5$, this ablation constantly alternates between exploitation and exploration, never committing to one or the other. This causes the agent to suffer from issues of both the $\lambda=0$ and $\lambda=1$ models, resulting in weak results.</p>
<p>Pure separation of exploitation and exploration (XTX no-mix) In this ablation, we examine the importance of having a mixture policy in Phase 1 of the algorithm instead of setting $\lambda=0$ in Phase 1 and to 1 in Phase 2. This explicitly separated model, denoted as XTX (no-mix), performs a bit better in the games of Inhumane and Zork3, but sometimes fails to push past certain stages in Ludicorp and completely gets stuck in the game of Zork1. This shows it is crucial to have a mixture policy in Phase 1 in order to get past bottleneck states in difficult games.</p>
<h1>5 CONCLUSION</h1>
<p>We have proposed XTX, an algorithm for multi-stage episodic control in text adventure games. XTX explicitly disentangles exploitation and exploration into different policies, which are used by the agent for action selection in different phases of the same episode. Decomposing the policy allows the agent to combine global decisions on which state spaces in the environment to (re-)explore, followed by strategic local exploration that can handle novel, unseen actions - aspects that help tackle the challenges of sparse rewards and dynamic action spaces in these games. Our method significantly outperforms prior methods on the Jericho benchmark (Hausknecht et al., 2020) under both deterministic and stochastic settings, and even surpasses several challenging bottlenecks in games like Zork1 (Ammanabrolu et al., 2020). Future work can integrate our algorithm with approaches that better leverage linguistic signals to achieve further progress in these games.</p>
<h1>ACKNOWLEDGEMENTS</h1>
<p>We thank the members of the Princeton NLP group and the anonymous reviewers for their valuable feedback. JT was supported by a graduate fellowship at Princeton University. We are grateful to the Google Cloud Research program for computational support in running our experiments. We would also like to thank Matthew Hausknecht for all the help regarding the Jericho environment.</p>
<h2>ETHICAL CONSIDERATIONS</h2>
<p>This work focuses on building better agents for text-adventure games and hence does not have immediate direct ethical concerns. However, the techniques introduced in this paper may be generally useful for other autonomous agents that combine sequential decision making with language understanding (e.g. dialog systems). As such agents become more capable and influential in our lives, it is important to make sure their objectives align with those of humans, and that they are free of bias.</p>
<h2>REPRODUCIBILITY</h2>
<p>Our code is publicly available here https://github.com/princeton-nlp/XTX. We provide all implementation details such as hyperparameters, model architectures and training regimes in Appendix A.2. We used Weights \&amp; Biases for experiment tracking and visualizations to develop insights for this paper.</p>
<h2>REFERENCES</h2>
<p>Ashutosh Adhikari, Xingdi (Eric) Yuan, Marc-Alexandre Côté, Mikulas Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L. Hamilton . Learning dynamic knowledge graphs to generalize on text-based games. In NeurIPS 2020, 2020.</p>
<p>Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. Pc-pg: Policy cover directed exploration for provable policy gradient learning. arXiv preprint arXiv:2007.08459, 2020.</p>
<p>Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances in Neural Information Processing Systems, 34, 2021.</p>
<p>Prithviraj Ammanabrolu and Matthew J. Hausknecht. Graph constrained reinforcement learning for natural language action spaces. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https: //openreview.net/forum?id=Blx6w0EtwH.</p>
<p>Prithviraj Ammanabrolu and Mark Riedl. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 3557-3565, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1358. URL https://www.aclweb.org/anthology/ N19-1358.</p>
<p>Prithviraj Ammanabrolu, Ethan Tien, Zhaochen Luo, and Mark O Riedl. How to avoid being eaten by a grue: Exploration strategies for text-adventure agents. arXiv preprint arXiv:2002.08795, 2020.</p>
<p>Adrià Puigdomènech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven Kapturowski, Olivier Tieleman, Martin Arjovsky, Alexander Pritzel, Andrew Bolt, et al. Never give up: Learning directed exploration strategies. In International Conference on Learning Representations, 2020.</p>
<p>Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. Advances in neural information processing systems, 29:1471-1479, 2016.</p>
<p>Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 2013.</p>
<p>Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for nearoptimal reinforcement learning. Journal of Machine Learning Research, 3(Oct):213-231, 2002.</p>
<p>Cédric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. GEP-PG: Decoupling exploration and exploitation in deep reinforcement learning algorithms. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 1039-1048. PMLR, 10-15 Jul 2018. URL https://proceedings.mlr.press/v80/colas18a.html.</p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. Textworld: A learning environment for text-based games. CoRR, 2018.</p>
<p>Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. First return, then explore. Nature, 590(7847):580-586, 2021.</p>
<p>Vincent François-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare, and Joelle Pineau. An introduction to deep reinforcement learning. arXiv preprint arXiv:1811.12560, 2018.</p>
<p>Nancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. What can you do with a rock? affordance extraction via word embeddings. In Carles Sierra (ed.), Proceedings of the TwentySixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017, pp. 1039-1045. ijcai.org, 2017. doi: 10.24963/ijcai.2017/144. URL https: //doi.org/10.24963/ijcai.2017/144.</p>
<p>Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, and Shiyu Chang. Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 7755-7765, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.624. URL https://www.aclweb.org/anthology/ 2020.emnlp-main. 624.</p>
<p>Matthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D Williams. Nail: A general interactive fiction agent. arXiv preprint arXiv:1902.04259, 2019.</p>
<p>Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. Interactive fiction games: A colossal adventure. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7903-7910, Apr. 2020. doi: 10.1609/aaai.v34i05.6297. URL https: //ojs.aaai.org/index.php/AAAI/article/view/6297.</p>
<p>Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep reinforcement learning with a natural language action space. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16211630, Berlin, Germany, August 2016a. Association for Computational Linguistics. doi: 10.18653/ v1/P16-1153. URL https://aclanthology.org/P16-1153.</p>
<p>Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep reinforcement learning with a natural language action space. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16211630, Berlin, Germany, 2016b. Association for Computational Linguistics. doi: 10.18653/v1/ P16-1153. URL https://www.aclweb.org/anthology/P16-1153.</p>
<p>Mikael Henaff. Explicit explore-exploit algorithms in continuous state spaces. arXiv preprint arXiv:1911.00617, 2019.</p>
<p>Vishal Jain, William Fedus, Hugo Larochelle, Doina Precup, and Marc G. Bellemare. Algorithmic improvements for deep reinforcement learning applied to interactive fiction. In The Thirty-Fourth</p>
<p>AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 4328-4336. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/ article/view/5857.</p>
<p>Youngsoo Jang, Seokin Seo, Jongmin Lee, and Kee-Eung Kim. Monte-carlo planning and learning with language action value estimates. In International Conference on Learning Representations, 2020.</p>
<p>Ray Jiang, Tom Zahavy, Zhongwen Xu, Adam White, Matteo Hessel, Charles Blundell, and Hado Van Hasselt. Emphatic algorithms for deep reinforcement learning. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139. PMLR, 2021.</p>
<p>Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine learning, 49(2):209-232, 2002.</p>
<p>Jing Li, Xinxin Shi, Jiehao Li, Xin Zhang, and Junzheng Wang. Random curiosity-driven exploration in deep reinforcement learning. Neurocomputing, 418:139-147, 2020.</p>
<p>Marlos C. Machado, Marc G. Bellemare, and Michael Bowling. Count-based exploration with the successor representation. Proceedings of the AAAI Conference on Artificial Intelligence, 34(04): 5125-5133, Apr. 2020. doi: 10.1609/aaai.v34i04.5955. URL https://ojs.aaai.org/ index.php/AAAI/article/view/5955.</p>
<p>Andrea Madotto, Mahdi Namazifar, Joost Huizinga, Piero Molino, Adrien Ecoffet, Huaixiu Zheng, Alexandros Papangelis, Dian Yu, Chandra Khatri, and Gökhan Tür. Exploration based language learning for text-based games. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, pp. 1488-1494. ijcai.org, 2020. doi: 10.24963/ijcai.2020/207. URL https://doi.org/10.24963/ijcai.2020/207.</p>
<p>Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for text-based games using deep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1-11, Lisbon, Portugal, 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1001. URL https://www.aclweb. org/anthology/D15-1001.</p>
<p>Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. In International Conference on Machine Learning, pp. 3878-3887. PMLR, 2018.</p>
<p>Philip Osborne, Heido Nõmm, and Andre Freitas. A survey of text games for reinforcement learning informed by natural language. arXiv preprint arXiv:2109.09478, 2021.</p>
<p>Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning, pp. 2778-2787. PMLR, 2017.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019.</p>
<p>Lukas Schäfer, Filippos Christianos, Josiah Hanna, and Stefano V. Albrecht. Decoupling exploration and exploitation in reinforcement learning. CoRR, abs/2107.08966, 2021. URL https:// arxiv.org/abs/2107.08966.</p>
<p>Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.</p>
<p>Lior Shani, Yonathan Efroni, and Shie Mannor. Exploration conscious reinforcement learning revisited. In International Conference on Machine Learning, pp. 5680-5689. PMLR, 2019.</p>
<p>Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.</p>
<p>Adrien Ali Taiga, William Fedus, Marlos C Machado, Aaron Courville, and Marc G Bellemare. On bonus-based exploration methods in the arcade learning environment. arXiv preprint arXiv:2109.11052, 2021.</p>
<p>Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep reinforcement learning. In 31st Conference on Neural Information Processing Systems (NIPS), volume 30, pp. 1-18, 2017.</p>
<p>Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033. IEEE, 2012.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998-6008, 2017.</p>
<p>William F. Whitney, Michael Bloesch, Jost Tobias Springenberg, Abbas Abdolmaleki, and Martin A. Riedmiller. Decoupled exploration and exploitation policies for sample-efficient reinforcement learning. CoRR, abs/2101.09458, 2021. URL https://arxiv.org/abs/2101.09458.</p>
<p>Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, and Chengqi Zhang. Deep reinforcement learning with stacked hierarchical attention for text-based games. Advances in Neural Information Processing Systems, 33, 2020.</p>
<p>Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, and Chengqi Zhang. Generalization in text-based games via hierarchical reinforcement learning. arXiv preprint arXiv:2109.09968, 2021.</p>
<p>Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore: Language models for action generation in text-based games. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8736-8754, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main. 704. URL https://aclanthology.org/2020.emnlp-main. 704.</p>
<p>Shunyu Yao, Karthik Narasimhan, and Matthew Hausknecht. Reading and acting while blindfolded: The need for semantics in text game agents. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 3097-3102, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.247. URL https://aclanthology.org/2021. naacl-main. 247 .</p>
<p>Xingdi Yuan, Marc-Alexandre Côté, Alessandro Sordoni, Romain Laroche, Remi Tachet des Combes, Matthew J. Hausknecht, and Adam Trischler. Counting to explore and generalize in text-based games. CoRR, 2018.</p>
<p>Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J. Mankowitz, and Shie Mannor. Learn what not to learn: Action elimination with deep reinforcement learning. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pp. 3566-3577, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ 645098b086d2f9ele0e939c27f9f2d6f-Abstract.html.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 GAME STATISTICS</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Game</th>
<th style="text-align: center;">ZORK1</th>
<th style="text-align: center;">INHUMANE</th>
<th style="text-align: center;">LUDICORP</th>
<th style="text-align: center;">LIBRARY</th>
<th style="text-align: center;">ZORK3</th>
<th style="text-align: center;">PENTARI</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Avg./Max</td>
<td style="text-align: center;">$9 / 51$</td>
<td style="text-align: center;">$14 / 28$</td>
<td style="text-align: center;">$4 / 45$</td>
<td style="text-align: center;">$5 / 6$</td>
<td style="text-align: center;">$39 / 41$</td>
<td style="text-align: center;">$5 / 16$</td>
</tr>
<tr>
<td style="text-align: left;">Game</td>
<td style="text-align: center;">DETECTIVE</td>
<td style="text-align: center;">BALANCES</td>
<td style="text-align: center;">DEEPHOME</td>
<td style="text-align: center;">ENCHANTER</td>
<td style="text-align: center;">DRAGON</td>
<td style="text-align: center;">OMNIQUEST</td>
</tr>
<tr>
<td style="text-align: left;">Avg./Max</td>
<td style="text-align: center;">$2 / 5$</td>
<td style="text-align: center;">$12 / 54$</td>
<td style="text-align: center;">$6 / 53$</td>
<td style="text-align: center;">$15 / 40$</td>
<td style="text-align: center;">$9 / 24$</td>
<td style="text-align: center;">$13 / 26$</td>
</tr>
</tbody>
</table>
<p>Table 3: Average and maximum number of steps between rewards for games in Jericho (based on human walkthroughs). Several games have long sequences of actions without reward.</p>
<p>Table 3 contains the average and maximum number of steps between rewards in these games, showcasing their challenging nature.</p>
<h2>A. 2 IMPLEMENTATION DETAILS</h2>
<p>We use a learning rate of $10^{-3}$ and $10^{-4}$ for $\pi_{\mathrm{il}}$ and $\pi_{\mathrm{inv}-\mathrm{dy}}$, respectively. Both policies are trained on batches of size 64 , with hidden dimensions of size 128 . The scaling coefficient $\alpha_{1}$ for the inverse dynamics intrinsic reward is set to 1 for all games except for Deephome ( $\alpha_{1}=0.1$ ), Enchanter $\left(\alpha_{1}=0.5\right)$, Omniquest $\left(\alpha_{1}=2\right)$, Ludicorp $\left(\alpha_{1}=0.5\right)$, Detective $\left(\alpha_{1}=2\right)$, and Pentari $\left(\alpha_{1}=2\right)$. The Transformer $\pi_{\mathrm{il}}$ has 3 layers and 4 attention heads. $\beta_{1}$ in equation 3 is set to $1, \beta_{2}$ in equation 4 is set to 10 k to encourage picking the shortest length trajectory, and $k$ is set to 10 . In equation 6 , $\alpha_{1}=\alpha_{2}=1$. The priority fraction $\rho$ is set to 0.5 . Every time $\pi_{\mathrm{il}}$ is trained, we also scale the episode length $T$ to have at least $R$ remaining steps of exploration by setting $T=l_{\max }+R$, where $l_{\max }$ is the length of the longest trajectory in the trajectory buffer $\mathcal{B}$. In practice, $R=50$, and hence the agent will be guaranteed at least 50 steps of exploration each episode. XTX and DRRN are run for 800 k interaction steps, while RC-DQN which is run for 100 k interaction steps following Guo et al. (2020).</p>
<h2>A. 3 FULL SET OF ABLATIONS</h2>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Average episode scores for 4 ablation models across 12 games. Overall, we find both the strategic inverse dynamics policy and the explicit exploitation policy to be key for our algorithm. Scores for dragon were clipped to be between 0 and 1.</p>
<h1>A. 4 Ablation Training Plots</h1>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Average episode score throughout training for all ablations on Zork1. Shaded areas indicate one standard deviation.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Average episode score throughout training for all ablations on Inhumane. Shaded areas indicate one standard deviation.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Average episode score throughout training for all ablations on Zork3. Shaded areas indicate one standard deviation.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Average episode score throughout training for all ablations on Ludicorp. Shaded areas indicate one standard deviation.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Average episode score throughout training for all ablations on Balances. Shaded areas indicate one standard deviation.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Average episode score throughout training for all ablations on Deephome. Shaded areas indicate one standard deviation.</p>
<p>Avg. Episode Score</p>
<ul>
<li>XTX (full model) - Pure IL $(\lambda=0)$ - Mix $(\lambda=0.5)$</li>
<li>Pure Inv Dy $(\lambda=1)$ - XTX (no-mix)
<img alt="img-9.jpeg" src="img-9.jpeg" /></li>
</ul>
<p>Figure 10: Average episode score throughout training for all ablations on Detective. Shaded areas indicate one standard deviation.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Average episode score throughout training for all ablations on Enchanter. Shaded areas indicate one standard deviation.</p>
<p>Avg. Episode Score</p>
<ul>
<li>XTX (full model) - Pure IL $(\lambda=0)$ - Mix $(\lambda=0.5)$</li>
<li>Pure Inv Dy $(\lambda=1)$ - XTX (no-mix)
<img alt="img-11.jpeg" src="img-11.jpeg" /></li>
</ul>
<p>Figure 12: Average episode score throughout training for all ablations on Omniquest. Shaded areas indicate one standard deviation.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Average episode score throughout training for all ablations on Pentari. Shaded areas indicate one standard deviation.</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: Average episode score throughout training for all ablations on Dragon. Shaded areas indicate one standard deviation.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15: Average episode score throughout training for all ablations on Library. Shaded areas indicate one standard deviation.</p>
<h1>A. 5 Full Deterministic and Stochastic Results</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Games</th>
<th style="text-align: center;">DRRN</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">INV-DY</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RC-DQN</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">XTX-Uniform</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">XTX (ours)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\Delta(\%)$</th>
<th style="text-align: center;">Game Max</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zone1</td>
<td style="text-align: center;">40.3 (2.2)</td>
<td style="text-align: center;">33.0 (6.4)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">44.1 (12.0)</td>
<td style="text-align: center;">103.0 (19.9)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">43.7 (0.6)</td>
<td style="text-align: center;">33.0 (0.8)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">39.3 (1.5)</td>
<td style="text-align: center;">32.3 (2.8)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">103.4 (10.9)</td>
<td style="text-align: center;">132.7 (1.7)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+170$</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Intermediate</td>
<td style="text-align: center;">31.0 (1.0)</td>
<td style="text-align: center;">56.7 (4.7)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">28.1 (3.6)</td>
<td style="text-align: center;">60.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">31.8 (1.4)</td>
<td style="text-align: center;">63.3 (4.7)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">44.9 (8.9)</td>
<td style="text-align: center;">43.3 (9.4)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10.5 (5.9)</td>
<td style="text-align: center;">70.9 (18.1)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: center;">Intermediate*</td>
<td style="text-align: center;">14.8 (3.9)</td>
<td style="text-align: center;">56.7 (4.7)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">27.7 (5.3)</td>
<td style="text-align: center;">63.3 (4.7)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">29.8 (2.3)</td>
<td style="text-align: center;">53.3 (6.7)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">59.2 (1.2)</td>
<td style="text-align: center;">76.7 (9.4)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">64.0 (7.7)</td>
<td style="text-align: center;">76.7 (9.4)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+50$</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: center;">L1oncom</td>
<td style="text-align: center;">15.6 (0.1)</td>
<td style="text-align: center;">23.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">15.6 (0.2)</td>
<td style="text-align: center;">23.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">12.4 (1.1)</td>
<td style="text-align: center;">21.0 (2.2)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">19.9 (0.4)</td>
<td style="text-align: center;">23.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">20.9 (0.1)</td>
<td style="text-align: center;">23.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+10$</td>
<td style="text-align: center;">150</td>
</tr>
<tr>
<td style="text-align: center;">L1oncom**</td>
<td style="text-align: center;">13.3 (1.7)</td>
<td style="text-align: center;">48.7 (2.1)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">19.6 (5.5)</td>
<td style="text-align: center;">49.3 (16.2)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10.9 (1.7)</td>
<td style="text-align: center;">40.7 (2.5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">67.3 (4.2)</td>
<td style="text-align: center;">66.0 (2.6)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">76.0 (5.1)</td>
<td style="text-align: center;">81.0 (3.6)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+80$</td>
<td style="text-align: center;">150</td>
</tr>
<tr>
<td style="text-align: center;">Zone2</td>
<td style="text-align: center;">0.3 (0.0)</td>
<td style="text-align: center;">4.7 (0.5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.4 (0.0)</td>
<td style="text-align: center;">5.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.2 (0.5)</td>
<td style="text-align: center;">5.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.7 (0.2)</td>
<td style="text-align: center;">4.7 (0.5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4.2 (0.0)</td>
<td style="text-align: center;">5.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+70$</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">Zone1**</td>
<td style="text-align: center;">0.3 (0.0)</td>
<td style="text-align: center;">4.3 (0.5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.4 (0.1)</td>
<td style="text-align: center;">5.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.0 (0.2)</td>
<td style="text-align: center;">5.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.9 (0.4)</td>
<td style="text-align: center;">4.3 (0.5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4.3 (0.1)</td>
<td style="text-align: center;">5.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+60$</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">Poststat1</td>
<td style="text-align: center;">43.4 (4.5)</td>
<td style="text-align: center;">58.3 (2.4)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">55.8 (14.1)</td>
<td style="text-align: center;">46.7 (6.2)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">37.4 (7.0)</td>
<td style="text-align: center;">40.7 (11.8)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">43.4 (1.7)</td>
<td style="text-align: center;">48.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">49.3 (4.1)</td>
<td style="text-align: center;">48.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+50$</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Poststat2*</td>
<td style="text-align: center;">45.6 (1.9)</td>
<td style="text-align: center;">58.3 (2.4)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">54.5 (7.5)</td>
<td style="text-align: center;">53.3 (6.2)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">33.4 (6.9)</td>
<td style="text-align: center;">46.7 (6.2)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">43.4 (0.4)</td>
<td style="text-align: center;">48.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">49.6 (1.3)</td>
<td style="text-align: center;">60.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+60$</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Intermediate*</td>
<td style="text-align: center;">16.8 (0.6)</td>
<td style="text-align: center;">28.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">20.9 (0.4)</td>
<td style="text-align: center;">23.3 (4.7)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">20.9 (14.8)</td>
<td style="text-align: center;">34.7 (4.7)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">29.0 (0.0)</td>
<td style="text-align: center;">33.7 (12.5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">33.2 (10.3)</td>
<td style="text-align: center;">10.0 (0.2)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+40$</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: center;">Recurrent</td>
<td style="text-align: center;">18.0 (0.0)</td>
<td style="text-align: center;">18.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">9.9 (0.0)</td>
<td style="text-align: center;">18.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">18.0 (0.0)</td>
<td style="text-align: center;">18.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">9.6 (0.1)</td>
<td style="text-align: center;">18.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">18.0 (0.0)</td>
<td style="text-align: center;">18.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">00</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">Recurrent**</td>
<td style="text-align: center;">14.1 (0.6)</td>
<td style="text-align: center;">23.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">13.5 (1.6)</td>
<td style="text-align: center;">21.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10.0 (0.1)</td>
<td style="text-align: center;">14.1 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">13.5 (0.0)</td>
<td style="text-align: center;">14.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">13.5 (0.0)</td>
<td style="text-align: center;">14.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">00</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">L1most*</td>
<td style="text-align: center;">17.3 (0.7)</td>
<td style="text-align: center;">21.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17.0 (0.2)</td>
<td style="text-align: center;">21.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">16.2 (1.4)</td>
<td style="text-align: center;">21.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">18.8 (0.4)</td>
<td style="text-align: center;">21.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">19.7 (0.5)</td>
<td style="text-align: center;">21.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+50$</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;">L1most*</td>
<td style="text-align: center;">24.8 (0.6)</td>
<td style="text-align: center;">30.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">24.7 (0.4)</td>
<td style="text-align: center;">30.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">24.2 (1.4)</td>
<td style="text-align: center;">30.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">26.1 (0.4)</td>
<td style="text-align: center;">30.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">28.3 (0.3)</td>
<td style="text-align: center;">30.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+60$</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;">Recurrent</td>
<td style="text-align: center;">37.9 (0.4)</td>
<td style="text-align: center;">44.7 (0.5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">44.8 (19.9)</td>
<td style="text-align: center;">76.0 (5.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.0 (0.0)</td>
<td style="text-align: center;">1.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">46.2 (9.0)</td>
<td style="text-align: center;">46.7 (13.2)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">72.7 (5.0)</td>
<td style="text-align: center;">83.7 (5.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+60$</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: center;">Recurrent**</td>
<td style="text-align: center;">58.8 (0.1)</td>
<td style="text-align: center;">68.0 (0.8)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">58.9 (0.2)</td>
<td style="text-align: center;">72.7 (3.8)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.0 (0.0)</td>
<td style="text-align: center;">1.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">52.6 (0.4)</td>
<td style="text-align: center;">50.0 (0.8)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">77.7 (2.1)</td>
<td style="text-align: center;">92.3 (2.3)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+60$</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: center;">Recurrent**</td>
<td style="text-align: center;">46.3 (11.1)</td>
<td style="text-align: center;">70.0 (21.2)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">46.0 (3.6)</td>
<td style="text-align: center;">73.3 (8.5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">25.8 (8.5)</td>
<td style="text-align: center;">56.7 (14.3)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">43.4 (10.9)</td>
<td style="text-align: center;">53.3 (23.6)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">54.7 (21.2)</td>
<td style="text-align: center;">56.7 (20.6)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">00</td>
<td style="text-align: center;">400</td>
</tr>
<tr>
<td style="text-align: center;">Recurrent**</td>
<td style="text-align: center;">42.0 (1.2)</td>
<td style="text-align: center;">44.7 (2.4)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">44.2 (18.3)</td>
<td style="text-align: center;">63.3 (10.6)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">26.8 (1.9)</td>
<td style="text-align: center;">58.3 (4.7)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">24.3 (10.8)</td>
<td style="text-align: center;">28.3 (11.8)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">32.6 (23.1)</td>
<td style="text-align: center;">66.7 (10.9)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+20$</td>
<td style="text-align: center;">400</td>
</tr>
<tr>
<td style="text-align: center;">Recurrent</td>
<td style="text-align: center;">3.7 (0.0)</td>
<td style="text-align: center;">8.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.3 (0.2)</td>
<td style="text-align: center;">5.7 (1.7)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.1 (1.8)</td>
<td style="text-align: center;">8.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">40.7 (0.0)</td>
<td style="text-align: center;">50.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">56.7 (1.1)</td>
<td style="text-align: center;">57.3 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">00</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Deteriorant*</td>
<td style="text-align: center;">4.2 (0.1)</td>
<td style="text-align: center;">10.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">9.0 (0.0)</td>
<td style="text-align: center;">13.3 (2.4)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">9.3 (0.7)</td>
<td style="text-align: center;">10.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8.6 (0.1)</td>
<td style="text-align: center;">10.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">11.6 (1.3)</td>
<td style="text-align: center;">13.3 (2.4)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+70$</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;">Avg. Norm Score</td>
<td style="text-align: center;">29.5\% (29.8)</td>
<td style="text-align: center;">38.8\% (28.8)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">26.4\% (27.2)</td>
<td style="text-align: center;">51.8\% (27.3)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">29.7\% (25.6)</td>
<td style="text-align: center;">44.5\% (32.1)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">49.2\% (30.4)</td>
<td style="text-align: center;">51.6\% (33.6)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">56.3\% (28.1)</td>
<td style="text-align: center;">64.0\% (28.6)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.8\% (4.1)</td>
<td style="text-align: center;">100\%</td>
</tr>
</tbody>
</table>
<p>Table 4: Full Deterministic Results. Standard deviations are in parentheses. Scores are averaged across 3 seeds. Note that the average normalized scores only take into account the games listed in Table 1. Baselines were rerun with the latest Jericho version.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Games</th>
<th style="text-align: center;">DRRN</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">INV-DY</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RC-DQN</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">XTX-Uniform</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">XTX (ours)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\Delta(\%)$</th>
<th style="text-align: center;">Game Max</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zone1</td>
<td style="text-align: center;">41.3 (3.2)</td>
<td style="text-align: center;">55.7 (3.3)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">56.9 (2.4)</td>
<td style="text-align: center;">85.7 (14.8)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">40.3 (1.6)</td>
<td style="text-align: center;">53.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">31.3 (1.1)</td>
<td style="text-align: center;">48.0 (5.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">47.7 (8.0)</td>
<td style="text-align: center;">143.0 (10.7)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+80$</td>
<td style="text-align: center;">350</td>
</tr>
<tr>
<td style="text-align: center;">Zone2</td>
<td style="text-align: center;">0.2 (0.0)</td>
<td style="text-align: center;">4.3 (0.5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.7 (0.2)</td>
<td style="text-align: center;">5.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.7 (0.0)</td>
<td style="text-align: center;">5.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.8 (0.1)</td>
<td style="text-align: center;">6.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.7 (0.4)</td>
<td style="text-align: center;">5.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">00</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">Zone3*</td>
<td style="text-align: center;">0.2 (0.0)</td>
<td style="text-align: center;">4.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.4 (0.3)</td>
<td style="text-align: center;">4.7 (0.5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.7 (0.1)</td>
<td style="text-align: center;">4.7 (0.5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.3 (0.5)</td>
<td style="text-align: center;">4.3 (0.5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.4 (0.6)</td>
<td style="text-align: center;">5.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$-10$</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">Poststat1</td>
<td style="text-align: center;">42.0 (0.0)</td>
<td style="text-align: center;">40.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">39.9 (0.5)</td>
<td style="text-align: center;">45.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">33.2 (3.0)</td>
<td style="text-align: center;">45.1 (11.8)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">28.6 (1.3)</td>
<td style="text-align: center;">38.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">38.1 (0.4)</td>
<td style="text-align: center;">38.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+60$</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Poststat2*</td>
<td style="text-align: center;">58.2 (3.6)</td>
<td style="text-align: center;">60.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">57.5 (0.0)</td>
<td style="text-align: center;">55.0 (7.1)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">33.3 (6.0)</td>
<td style="text-align: center;">43.7 (10.3)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">38.8 (0.4)</td>
<td style="text-align: center;">40.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">47.2 (0.4)</td>
<td style="text-align: center;">48.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+120$</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Recurrent</td>
<td style="text-align: center;">58.2 (0.6)</td>
<td style="text-align: center;">72.0 (5.7)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">58.2 (0.5)</td>
<td style="text-align: center;">72.7 (2.5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.0 (0.0)</td>
<td style="text-align: center;">1.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">48.0 (10.1)</td>
<td style="text-align: center;">62.6 (14.2)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">73.9 (4.3)</td>
<td style="text-align: center;">99.3 (13.9)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+50$</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: center;">Recurrent**</td>
<td style="text-align: center;">43.0 (20.0)</td>
<td style="text-align: center;">65.7 (3.3)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">58.4 (0.5)</td>
<td style="text-align: center;">73.0 (1.4)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.0 (0.0)</td>
<td style="text-align: center;">1.0 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50.7 (2.3)</td>
<td style="text-align: center;">69.3 (0.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">70.9 (2.7)</td>
<td style="text-align: center;">96.0 (7.8)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+40$</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: center;">Recurrent**</td>
<td style="text-align: center;">41.0 (0.0)</td>
<td style="text-align: center;">71.7 (0.4)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">58.9 (14.5)</td>
<td style="text-align: center;">63.3 (10.6)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">25.0 (4.0)</td>
<td style="text-align: center;">50.0 (7.1)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">32.1 (10.9)</td>
<td style="text-align: center;">53.3 (23.6)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">46.2 (18.9)</td>
<td style="text-align: center;">51.7 (22.5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+10$</td>
<td style="text-align: center;">400</td>
</tr>
<tr>
<td style="text-align: center;">Recurrent**</td>
<td style="text-align: center;">42.0 (18.8)</td>
<td style="text-align: center;">56.7 (27.2)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">54.5 (10.9)</td>
<td style="text-align: center;">53.3 (23.6)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">27.1 (2.7)</td>
<td style="text-align: center;">53.3 (8.5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">30.2 (9.1)</td>
<td style="text-align: center;">45.0 (28.4)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">41.0 (19.4)</td>
<td style="text-align: center;">52.3 (27.8)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+10$</td>
<td style="text-align: center;">400</td>
</tr>
<tr>
<td style="text-align: center;">Avg. Norm Score</td>
<td style="text-align: center;">18.9\% (18.2)</td>
<td style="text-align: center;">19.0\% (28.1)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">19.7\% (17.5)</td>
<td style="text-align: center;">41.5\% (26.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">20.9\% (18.6)</td>
<td style="text-align: center;">10.5\% (17.1)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">24.3\% (17.9)</td>
<td style="text-align: center;">39.1\% (29.6)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">31.8\% (19.8)</td>
<td style="text-align: center;">48.9\% (26.0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4.8\% (4.7)</td>
<td style="text-align: center;">100\%</td>
</tr>
</tbody>
</table>
<p>Table 5: Full Stochastic Results. Standard deviations are in parentheses. Scores are averaged across 3 seeds. Note that the average normalized scores only take into account the games listed in Table 2. Baselines were rerun with the latest Jericho version.</p>
<h1>A. 6 Aggregate Metrics \&amp; Performance Profiles</h1>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 17: Aggregate metrics with $95 \%$ CIs for the deterministic games listed in Table 1, following Agarwal et al. (2021). The CIs use percentile bootstrap with stratified sampling.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ Interestingly, XTX manages to achieve a very high score on Dragon by exploiting an integer underflow bug.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4371 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4371</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4371</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-280536622</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.04612v1.pdf" target="_blank">A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature</a></p>
                <p><strong>Paper Abstract:</strong> The accelerating pace of research on autoregressive generative models has produced thousands of papers, making manual literature surveys and reproduction studies increasingly impractical. We present a fully open-source, reproducible pipeline that automatically retrieves candidate documents from public repositories, filters them for relevance, extracts metadata, hyper-parameters and reported results, clusters topics, produces retrieval-augmented summaries and generates containerised scripts for re-running selected experiments. Quantitative evaluation on 50 manually-annotated papers shows F1 scores above 0.85 for relevance classification, hyper-parameter extraction and citation identification. Experiments on corpora of up to 1000 papers demonstrate near-linear scalability with eight CPU workers. Three case studies -- AWD-LSTM on WikiText-2, Transformer-XL on WikiText-103 and an autoregressive music model on the Lakh MIDI dataset -- confirm that the extracted settings support faithful reproduction, achieving test perplexities within 1--3% of the original reports.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4371.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4371.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automated literature synthesis pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end, containerised pipeline that retrieves, parses, filters, extracts structured facts from, clusters, and summarises corpora of papers about autoregressive generative models, and generates executable reproduction scripts; it uses retrieval-augmented LLM summarisation constrained to extracted sentences to produce cluster-level summaries with explicit citations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Automated literature synthesis pipeline (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A six-stage pipeline: (1) retrieve candidate papers from APIs (arXiv, Semantic Scholar); (2) parallel PDF parsing/text extraction; (3) relevance filtering via keyword matching + classifier; (4) information extraction using regex/heuristics, rule-based hyperparameter patterns and a lightweight NER trained on hyperparameter mentions, numeric result extraction and citation linking; (5) topic clustering using TF–IDF embeddings and k-means (k chosen by silhouette score) and retrieval-augmented LLM summarisation restricted to retrieved sentences with required explicit citations; (6) script generation and optional automatic reproduction of experiments. All stages write JSON records to a thread-safe knowledge base; pipeline is orchestrated by Python scripts and containerised with Docker to ensure reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLaMA-2 or GPT-4 (models referenced; specific sizes not provided in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>PDF-to-text parsing followed by regex/heuristic patterns for metadata and hyperparameters, numeric scanning for metrics, citation-marker recognition, and a lightweight NER trained on annotated hyperparameter mentions to capture varied phrasings.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Retrieval-augmented LLM summarisation applied per cluster: retrieve sentences from extracted text for each topic cluster, prompt LLM constrained to those sentences to produce summaries with explicit in-text citations; aggregation into knowledge base and generation of runnable scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Experiments on corpora from 100 to 1,500 papers; demonstrated scaling to >1,000 papers (examples given for 1,000 papers).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Autoregressive generative models (language modelling, Transformer-based models, autoregressive music generation); pipeline is designed for AR literature but portable to other ML subdomains.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured JSON records (metadata, hyperparameters, results, citations), topic-clustered retrieval-augmented summaries appended to a report, aggregated tables (model vs dataset vs metric), and containerised executable training scripts for reproduction experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Extraction precision, recall, F1 (computed per Algorithm 2); processing time and peak memory for scalability; reproduction evaluation via task metrics (test perplexity) and human preference tests for music generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Extraction F1-scores: relevance filtering F1=0.90 (P=0.92, R=0.88), hyperparameter extraction F1=0.88 (P=0.87, R=0.89), citation identification F1=0.86 (P=0.84, R=0.88), result extraction F1=0.83 (P=0.81, R=0.86). Scalability: empirical models T(n)≈0.04n+1 minutes and M(n)≈0.01n+1 GB (e.g., T(1000)≈41 min, M(1000)≈11 GB). Reproduction case results: AWD-LSTM reproduced test perplexity 66.5 vs reported 65.8; Transformer-XL reproduced 19.5 vs reported 18.3; autoregressive music model per-event perplexity 70.3 vs reported 69.7.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against ablated versions of the pipeline (no parallel parsing, no relevance classifier, no rule-based patterns, no LLM summarisation) and discussed relative to prior domain-agnostic LLM pipelines (PROMPTHEUS, modular summarisation frameworks, OpenScholar) in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Ablations: removing parallel parsing increased runtime by ~3x; removing relevance classifier decreased precision by ~11 percentage points; removing rule-based hyperparameter patterns dramatically reduced recall; removing LLM summarisation had little effect on extraction F1 but affects report summaries (time decreased slightly). Compared to prior domain-agnostic pipelines, this pipeline claims stronger domain-specific extraction and integrated script generation (qualitative comparison in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Tight integration of rule-based extraction, a lightweight NER, and retrieval-augmented LLM summarisation yields high extraction accuracy (F1>0.85) and produces the hyperparameters and scripts necessary for faithful reproduction; parallel parsing yields near-linear speedups; constraining LLMs to retrieved sentences and requiring explicit citations mitigates hallucination in summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Hallucination risk if retrieved sentences lack key facts; failures for PDFs with unusual encodings or heavily mathematical notation causing false positives; idiosyncratic or unconventional phrasing can elude rule-based extractors despite the NER; reproducing massive models remains computationally infeasible for many researchers.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Empirical near-linear scaling in processing time and modest linear memory growth for n in [100,1500] with the formulas T(n)≈0.04n+1 minutes and M(n)≈0.01n+1 GB; demonstrated processing of 1,000 papers in ≈40 minutes on an 8-core CPU with 32 GB RAM using 8 workers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4371.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4371.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PROMPTHEUS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PROMPTHEUS: A human-centered pipeline to streamline systematic literature reviews with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-centered pipeline that integrates large language models to accelerate search, screening, and summarisation steps of systematic literature reviews, reported to reduce manual workload and achieve high precision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PROMPTHEUS: A human-centered pipeline to streamline systematic literature reviews with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PROMPTHEUS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A domain-agnostic pipeline that leverages LLMs to assist stages of systematic literature reviews including search, screening of candidate papers, and summarisation; emphasises human-centered interaction to streamline workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper (referenced as an LLM-based pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-assisted search and screening (likely prompting-based workflows), combined with traditional IR for candidate retrieval (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-based summarisation of selected documents to produce review outputs; integrates human oversight (human-in-the-loop).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Systematic literature reviews across domains (domain-agnostic).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Screened candidate lists and LLM-generated summaries to support literature reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported 'high precision' for screening; exact metrics not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Described as significantly reducing manual workload and achieving high precision, but no numerical metrics are provided in this paper's description.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Implicitly compared to manual systematic review workflows and earlier, rule-based or keyword-based screening pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported improvements in workflow efficiency and precision relative to manual processes (no quantitative numbers given here).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can substantially reduce manual effort in search/screening/summarisation when embedded into a human-centered pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not detailed in this paper's discussion beyond general limitations of domain-agnostic approaches noted by the authors (e.g., separation of extraction from experimental validation).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4371.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4371.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Modular summarisation pipeline (Achkar et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A modular pipeline for scientific literature summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular framework combining retrieval and question-generation components to produce multi-document scientific summaries, designed to accelerate literature synthesis in a domain-agnostic way.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A modular pipeline for scientific literature summarization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Modular summarisation pipeline (Achkar et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A domain-agnostic modular pipeline that integrates document retrieval with question-generation modules to perform multi-document summarisation for scientific literature; emphasizes modular components that can be combined or swapped.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval to gather relevant documents and question-generation to surface salient information for summarisation (details abstracted in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Multi-document summarisation using retrieval plus generated question-answer information to compose summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature summarisation (domain-agnostic).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Multi-document summaries of scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper's summary (referenced as a prior modular summarisation framework).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not specified in this paper's description.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Prior domain-agnostic automated summarisation systems and manual summarisation workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not quantified in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining retrieval and question-generation in modular fashion facilitates flexible multi-document summarisation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Prior frameworks are domain-agnostic and often separate extraction from experimental validation, limiting direct reproducibility of extracted experimental details.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4371.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4371.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenScholar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenScholar: Synthesizing scientific literature with retrieval-augmented language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that uses retrieval-augmented generation with large (billion-parameter) language models to answer queries about scientific literature and synthesise information across documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenScholar: Synthesizing scientific literature with retrieval-augmented language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenScholar</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A retrieval-augmented generation system that combines document retrieval with large language models (billions of parameters) to synthesize and answer queries over scientific literature, enabling multi-document question answering and summarisation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Described as 'billions of parameters' retrieval-augmented language models; specific model names and sizes not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval of relevant passages followed by LLM generation conditioned on retrieved evidence (retrieval-augmented generation).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Retrieval-augmented generation to answer user queries by synthesizing information across retrieved documents.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Query answers about scientific literature and synthesized summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not quantified in this paper's description; presented as a strong-capacity retrieval-augmented approach.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this paper's summary; implicitly against non-retrieval or smaller-model summarisation approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not quantified in this paper's description.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval-augmented LLMs with large parameter counts can effectively synthesize answers about scientific literature when conditioned on retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Potential for hallucination if retrieval fails to return relevant sentences; computational cost associated with large LLMs; details not expanded in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PROMPTHEUS: A human-centered pipeline to streamline systematic literature reviews with large language models <em>(Rating: 2)</em></li>
                <li>A modular pipeline for scientific literature summarization <em>(Rating: 2)</em></li>
                <li>OpenScholar: Synthesizing scientific literature with retrieval-augmented language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4371",
    "paper_id": "paper-280536622",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "Automated literature synthesis pipeline",
            "name_full": "A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature",
            "brief_description": "An end-to-end, containerised pipeline that retrieves, parses, filters, extracts structured facts from, clusters, and summarises corpora of papers about autoregressive generative models, and generates executable reproduction scripts; it uses retrieval-augmented LLM summarisation constrained to extracted sentences to produce cluster-level summaries with explicit citations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Automated literature synthesis pipeline (this work)",
            "system_description": "A six-stage pipeline: (1) retrieve candidate papers from APIs (arXiv, Semantic Scholar); (2) parallel PDF parsing/text extraction; (3) relevance filtering via keyword matching + classifier; (4) information extraction using regex/heuristics, rule-based hyperparameter patterns and a lightweight NER trained on hyperparameter mentions, numeric result extraction and citation linking; (5) topic clustering using TF–IDF embeddings and k-means (k chosen by silhouette score) and retrieval-augmented LLM summarisation restricted to retrieved sentences with required explicit citations; (6) script generation and optional automatic reproduction of experiments. All stages write JSON records to a thread-safe knowledge base; pipeline is orchestrated by Python scripts and containerised with Docker to ensure reproducibility.",
            "llm_model_used": "LLaMA-2 or GPT-4 (models referenced; specific sizes not provided in paper)",
            "extraction_technique": "PDF-to-text parsing followed by regex/heuristic patterns for metadata and hyperparameters, numeric scanning for metrics, citation-marker recognition, and a lightweight NER trained on annotated hyperparameter mentions to capture varied phrasings.",
            "synthesis_technique": "Retrieval-augmented LLM summarisation applied per cluster: retrieve sentences from extracted text for each topic cluster, prompt LLM constrained to those sentences to produce summaries with explicit in-text citations; aggregation into knowledge base and generation of runnable scripts.",
            "number_of_papers": "Experiments on corpora from 100 to 1,500 papers; demonstrated scaling to &gt;1,000 papers (examples given for 1,000 papers).",
            "domain_or_topic": "Autoregressive generative models (language modelling, Transformer-based models, autoregressive music generation); pipeline is designed for AR literature but portable to other ML subdomains.",
            "output_type": "Structured JSON records (metadata, hyperparameters, results, citations), topic-clustered retrieval-augmented summaries appended to a report, aggregated tables (model vs dataset vs metric), and containerised executable training scripts for reproduction experiments.",
            "evaluation_metrics": "Extraction precision, recall, F1 (computed per Algorithm 2); processing time and peak memory for scalability; reproduction evaluation via task metrics (test perplexity) and human preference tests for music generation.",
            "performance_results": "Extraction F1-scores: relevance filtering F1=0.90 (P=0.92, R=0.88), hyperparameter extraction F1=0.88 (P=0.87, R=0.89), citation identification F1=0.86 (P=0.84, R=0.88), result extraction F1=0.83 (P=0.81, R=0.86). Scalability: empirical models T(n)≈0.04n+1 minutes and M(n)≈0.01n+1 GB (e.g., T(1000)≈41 min, M(1000)≈11 GB). Reproduction case results: AWD-LSTM reproduced test perplexity 66.5 vs reported 65.8; Transformer-XL reproduced 19.5 vs reported 18.3; autoregressive music model per-event perplexity 70.3 vs reported 69.7.",
            "comparison_baseline": "Compared against ablated versions of the pipeline (no parallel parsing, no relevance classifier, no rule-based patterns, no LLM summarisation) and discussed relative to prior domain-agnostic LLM pipelines (PROMPTHEUS, modular summarisation frameworks, OpenScholar) in related work.",
            "performance_vs_baseline": "Ablations: removing parallel parsing increased runtime by ~3x; removing relevance classifier decreased precision by ~11 percentage points; removing rule-based hyperparameter patterns dramatically reduced recall; removing LLM summarisation had little effect on extraction F1 but affects report summaries (time decreased slightly). Compared to prior domain-agnostic pipelines, this pipeline claims stronger domain-specific extraction and integrated script generation (qualitative comparison in paper).",
            "key_findings": "Tight integration of rule-based extraction, a lightweight NER, and retrieval-augmented LLM summarisation yields high extraction accuracy (F1&gt;0.85) and produces the hyperparameters and scripts necessary for faithful reproduction; parallel parsing yields near-linear speedups; constraining LLMs to retrieved sentences and requiring explicit citations mitigates hallucination in summaries.",
            "limitations_challenges": "Hallucination risk if retrieved sentences lack key facts; failures for PDFs with unusual encodings or heavily mathematical notation causing false positives; idiosyncratic or unconventional phrasing can elude rule-based extractors despite the NER; reproducing massive models remains computationally infeasible for many researchers.",
            "scaling_behavior": "Empirical near-linear scaling in processing time and modest linear memory growth for n in [100,1500] with the formulas T(n)≈0.04n+1 minutes and M(n)≈0.01n+1 GB; demonstrated processing of 1,000 papers in ≈40 minutes on an 8-core CPU with 32 GB RAM using 8 workers.",
            "uuid": "e4371.0",
            "source_info": {
                "paper_title": "A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "PROMPTHEUS",
            "name_full": "PROMPTHEUS: A human-centered pipeline to streamline systematic literature reviews with large language models",
            "brief_description": "A human-centered pipeline that integrates large language models to accelerate search, screening, and summarisation steps of systematic literature reviews, reported to reduce manual workload and achieve high precision.",
            "citation_title": "PROMPTHEUS: A human-centered pipeline to streamline systematic literature reviews with large language models",
            "mention_or_use": "mention",
            "system_name": "PROMPTHEUS",
            "system_description": "A domain-agnostic pipeline that leverages LLMs to assist stages of systematic literature reviews including search, screening of candidate papers, and summarisation; emphasises human-centered interaction to streamline workflows.",
            "llm_model_used": "Not specified in this paper (referenced as an LLM-based pipeline).",
            "extraction_technique": "LLM-assisted search and screening (likely prompting-based workflows), combined with traditional IR for candidate retrieval (details not provided in this paper).",
            "synthesis_technique": "LLM-based summarisation of selected documents to produce review outputs; integrates human oversight (human-in-the-loop).",
            "number_of_papers": "Not specified in this paper.",
            "domain_or_topic": "Systematic literature reviews across domains (domain-agnostic).",
            "output_type": "Screened candidate lists and LLM-generated summaries to support literature reviews.",
            "evaluation_metrics": "Reported 'high precision' for screening; exact metrics not reported in this paper.",
            "performance_results": "Described as significantly reducing manual workload and achieving high precision, but no numerical metrics are provided in this paper's description.",
            "comparison_baseline": "Implicitly compared to manual systematic review workflows and earlier, rule-based or keyword-based screening pipelines.",
            "performance_vs_baseline": "Reported improvements in workflow efficiency and precision relative to manual processes (no quantitative numbers given here).",
            "key_findings": "LLMs can substantially reduce manual effort in search/screening/summarisation when embedded into a human-centered pipeline.",
            "limitations_challenges": "Not detailed in this paper's discussion beyond general limitations of domain-agnostic approaches noted by the authors (e.g., separation of extraction from experimental validation).",
            "scaling_behavior": "Not specified in this paper.",
            "uuid": "e4371.1",
            "source_info": {
                "paper_title": "A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Modular summarisation pipeline (Achkar et al.)",
            "name_full": "A modular pipeline for scientific literature summarization",
            "brief_description": "A modular framework combining retrieval and question-generation components to produce multi-document scientific summaries, designed to accelerate literature synthesis in a domain-agnostic way.",
            "citation_title": "A modular pipeline for scientific literature summarization",
            "mention_or_use": "mention",
            "system_name": "Modular summarisation pipeline (Achkar et al.)",
            "system_description": "A domain-agnostic modular pipeline that integrates document retrieval with question-generation modules to perform multi-document summarisation for scientific literature; emphasizes modular components that can be combined or swapped.",
            "llm_model_used": "Not specified in this paper.",
            "extraction_technique": "Retrieval to gather relevant documents and question-generation to surface salient information for summarisation (details abstracted in this paper).",
            "synthesis_technique": "Multi-document summarisation using retrieval plus generated question-answer information to compose summaries.",
            "number_of_papers": "Not specified in this paper.",
            "domain_or_topic": "Scientific literature summarisation (domain-agnostic).",
            "output_type": "Multi-document summaries of scientific literature.",
            "evaluation_metrics": "Not specified in this paper's summary (referenced as a prior modular summarisation framework).",
            "performance_results": "Not specified in this paper's description.",
            "comparison_baseline": "Prior domain-agnostic automated summarisation systems and manual summarisation workflows.",
            "performance_vs_baseline": "Not quantified in this paper's summary.",
            "key_findings": "Combining retrieval and question-generation in modular fashion facilitates flexible multi-document summarisation pipelines.",
            "limitations_challenges": "Prior frameworks are domain-agnostic and often separate extraction from experimental validation, limiting direct reproducibility of extracted experimental details.",
            "scaling_behavior": "Not specified in this paper.",
            "uuid": "e4371.2",
            "source_info": {
                "paper_title": "A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "OpenScholar",
            "name_full": "OpenScholar: Synthesizing scientific literature with retrieval-augmented language models",
            "brief_description": "A system that uses retrieval-augmented generation with large (billion-parameter) language models to answer queries about scientific literature and synthesise information across documents.",
            "citation_title": "OpenScholar: Synthesizing scientific literature with retrieval-augmented language models",
            "mention_or_use": "mention",
            "system_name": "OpenScholar",
            "system_description": "A retrieval-augmented generation system that combines document retrieval with large language models (billions of parameters) to synthesize and answer queries over scientific literature, enabling multi-document question answering and summarisation.",
            "llm_model_used": "Described as 'billions of parameters' retrieval-augmented language models; specific model names and sizes not provided in this paper.",
            "extraction_technique": "Embedding-based retrieval of relevant passages followed by LLM generation conditioned on retrieved evidence (retrieval-augmented generation).",
            "synthesis_technique": "Retrieval-augmented generation to answer user queries by synthesizing information across retrieved documents.",
            "number_of_papers": "Not specified in this paper.",
            "domain_or_topic": "General scientific literature.",
            "output_type": "Query answers about scientific literature and synthesized summaries.",
            "evaluation_metrics": "Not specified in this paper's summary.",
            "performance_results": "Not quantified in this paper's description; presented as a strong-capacity retrieval-augmented approach.",
            "comparison_baseline": "Not specified in this paper's summary; implicitly against non-retrieval or smaller-model summarisation approaches.",
            "performance_vs_baseline": "Not quantified in this paper's description.",
            "key_findings": "Retrieval-augmented LLMs with large parameter counts can effectively synthesize answers about scientific literature when conditioned on retrieved evidence.",
            "limitations_challenges": "Potential for hallucination if retrieval fails to return relevant sentences; computational cost associated with large LLMs; details not expanded in this paper.",
            "scaling_behavior": "Not specified in this paper.",
            "uuid": "e4371.3",
            "source_info": {
                "paper_title": "A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature",
                "publication_date_yy_mm": "2025-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PROMPTHEUS: A human-centered pipeline to streamline systematic literature reviews with large language models",
            "rating": 2,
            "sanitized_title": "promptheus_a_humancentered_pipeline_to_streamline_systematic_literature_reviews_with_large_language_models"
        },
        {
            "paper_title": "A modular pipeline for scientific literature summarization",
            "rating": 2,
            "sanitized_title": "a_modular_pipeline_for_scientific_literature_summarization"
        },
        {
            "paper_title": "OpenScholar: Synthesizing scientific literature with retrieval-augmented language models",
            "rating": 2,
            "sanitized_title": "openscholar_synthesizing_scientific_literature_with_retrievalaugmented_language_models"
        }
    ],
    "cost": 0.011935,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature
August 6, 2025</p>
<p>Faruk Alpay alpay@lightcap.ai 
AI Research Lab
Lightcap Institute</p>
<p>Bugra Kilictas 
Department of Engineering
Bahcesehir University</p>
<p>Hamdi Alakkad 
Department of Engineering
Bahcesehir University</p>
<p>A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature
August 6, 20252C49B2FF948ABAAA05FDE89E3939745CarXiv:2508.04612v1[cs.IR]
The rapid acceleration of research in autoregressive (AR) generative modelling has produced a deluge of publications, making it increasingly difficult for researchers to stay current and reproduce results.Building upon prior survey pipelines, we present a comprehensive and scalable computational pipeline that automatically retrieves, parses, filters, and synthesises literature on AR models.Emphasis is placed on integrating extraction modules with runnable scripts and on rigorous evaluation of each component.We report quantitative precision/recall measurements for relevance filtering, hyperparameter extraction, and citation identification (F1 &gt; 0.85), and we demonstrate the pipeline's portability to new domains with case studies on language modelling, Transformer-based models, and autoregressive music generation.Three reproduction experiments-AWD-LSTM on WikiText-2, Transformer-XL on WikiText-103, and an autoregressive music model trained on the Lakh MIDI dataset-illustrate how the pipeline's outputs support faithful reimplementation.Ablation studies, scalability experiments on more than 1,000 papers, and failure mode analyses highlight the pipeline's robustness and limitations.Detailed pseudocode, mathematical formulations, containerised execution scripts, and hardware specifications are provided to facilitate reproducibility.Overall, our study shows that automatic literature synthesis can underpin living surveys and reproducible research across fast-moving subfields of machine learning.</p>
<p>Introduction</p>
<p>The number of publications on generative modelling has grown exponentially over the last decade, with dozens of new papers on large language models and autoregressive (AR) techniques appearing each week.This deluge renders manual literature reviews impractical and hampers reproducibility.Systematic literature review (SLR) pipelines such as PROMPTHEUS (Torres et al., 2024) and modular summarisation frameworks (Achkar et al., 2024) have shown that automation can reduce the burden on researchers; however, they are domain-agnostic and often separate extraction from experimental validation.Our goal is to advance this line of work by delivering a fully integrated pipeline focused on AR models that not only summarises research but also extracts the hyperparameters, architectures, and metrics needed to reproduce experiments.</p>
<p>The challenges motivating our work are threefold.First, the "literature overload" problem means that even experts struggle to keep up with emergent models and techniques.Second, reproducibility remains an open concern in machine learning: a lack of transparent reporting of code and hyperparameters has led to irreproducible claims (Kapoor and Narayanan, 2022).Initiatives such as the NeurIPS reproducibility checklist encourage authors to document training settings and datasets (Pineau et al., 2021), yet many papers still omit critical information.Third, AR models themselves are evolving rapidly, from recurrent architectures such as LSTMs (Merity et al., 2017;Bengio et al., 2003) to Transformer-based systems (Vaswani et al., 2017) and emerging large language models (Touvron et al., 2023).</p>
<p>In response, we propose a scalable pipeline that automatically retrieves AR-related papers from public repositories, parallelises document parsing, extracts structured facts, performs topic analysis and summarisation with retrieval-augmented language models, and generates executable scripts for reproducing selected experiments.Our contributions include:</p>
<p>• Integrated extraction and execution.The pipeline seamlessly links text mining and information extraction to runnable scripts for training models, enabling end-to-end validation of literature claims without manual intervention.</p>
<p>• Quantitative evaluation of extraction modules.We benchmark the precision, recall and F1-score of relevance filtering, hyperparameter extraction, and citation identification on annotated subsets of papers, achieving F1 &gt; 0.85 across tasks.</p>
<p>• Portability and case studies.We demonstrate the pipeline on three reproduction tasks: (i) an AWD-LSTM language model on WikiText-2 (Merity et al., 2017), (ii) a Transformer-XL model on WikiText-103 (Dai et al., 2019), and (iii) an autoregressive music model trained on the Lakh MIDI dataset (Thickstun et al., 2024).These case studies show that our synthesis outputs support faithful reproduction across domains, including music.</p>
<p>• Scalability and ablation analysis.Experiments on corpora exceeding 1,000 papers reveal nearlinear speedups with parallel parsing.We provide CPU time and memory curves (our scaling analysis) and ablation studies isolating each pipeline component.</p>
<p>• Reproducible research artefacts.Pseudocode, mathematical formulations, Docker-based execution scripts, fixed random seeds, and hardware specifications are provided to ensure that the pipeline and reproduction studies can be replicated by others.</p>
<p>By moving beyond manual SLRs and emphasising rigorous evaluation and reproducibility, our work serves as a foundation for living surveys of AR modelling that can adapt as the field evolves.</p>
<p>2 Background and Related Work</p>
<p>Automated Literature Analysis</p>
<p>Early attempts at automating literature surveys leveraged rule-based filtering and keyword matching.Recent approaches employ large language models (LLMs) to accelerate systematic reviews.PROMPTHEUS (Torres et al., 2024) integrates LLMs for search, screening and summarisation, significantly reducing manual workload and achieving high precision.Achkar et al. (Achkar et al., 2024) introduced a modular pipeline combining retrieval and question-generation for multi-document summarisation.OpenScholar (Asai et al., 2024) uses retrieval-augmented generation with billions of parameters to answer queries about scientific literature.Our pipeline differs from these systems in three respects: (i) we focus on AR generative models and integrate domain-specific extraction rules (for example, recognising hyperparameter settings such as learning rates and sequence lengths); (ii) we couple extraction with automatic generation of executable training scripts; and (iii) we emphasise reproducibility through containerisation and controlled randomness.</p>
<p>Scalability is an important consideration for literature pipelines.Distributed frameworks such as Apache Spark enable horizontal scaling of data ingestion and processing; we adopt similar principles by parallelising document parsing and using in-memory data stores.Our pipeline can process hundreds of PDFs in minutes and scales to thousands of documents (Section 4.2).The design draws inspiration from best practices in scalable machine learning systems (Bohg et al., 2017).</p>
<p>Autoregressive Models</p>
<p>Autoregressive models factorise the joint probability of a sequence (x 1 , . . ., x T ) as
P (x 1 , . . . , x T ) = T t=1 P (x t | x 1:t−1 ),(1)
turning sequence generation into a series of conditional predictions (Bengio et al., 2003).Recurrent neural networks (RNNs) and their gated variants (LSTM, GRU) dominated AR text generation through the 2010s.Merity et al. (Merity et al., 2017) introduced the AWD-LSTM, combining variational dropout and weight tying to achieve state-of-the-art perplexity on WikiText-2.The Transformer architecture (Vaswani et al., 2017) replaced recurrence with self-attention, enabling parallel computation and scaling to billions of parameters.Transformer-XL (Dai et al., 2019) extended Transformers with a segment-level recurrence; the authors reported perplexity 18.3 on WikiText-103 and 54.5 on the Penn Treebank, improving state-of-the-art results .Large language models such as LLaMA 2 (Touvron et al., 2023) and GPT-3 (Brown et al., 2020) further scale AR modelling, demonstrating emergent capabilities in few-shot learning.Autoregressive modelling is also applied to images (van den Oord et al., 2016a), audio (van den Oord et al., 2016b), and music.Automated music generation often employs autoregressive sequence models.Anticipatory Music Transformer (Thickstun et al., 2024) trains AR and infilling models on the Lakh MIDI dataset; Table 1 of their paper reports per-event perplexities on Lakh MIDI across model sizes and training schedules, with larger models achieving perplexities below 70.MuseGAN (Dong et al., 2018) uses generative adversarial networks on multi-track piano-rolls derived from the Lakh Pianoroll dataset; the dataset contains 174,154 unique multitrack piano-rolls and a cleansed subset of 21,425 sequences satisfying 4/4 time and other constraints.These works demonstrate the breadth of AR modelling across modalities, motivating the need for an up-to-date synthesis.</p>
<p>Reproducibility and Research Transparency</p>
<p>Reproducibility is critical for scientific progress, yet many machine learning papers omit code, hyperparameters or random seeds, leading to irreproducible results (Kapoor and Narayanan, 2022).The NeurIPS reproducibility checklist encourages authors to disclose experimental details such as datasets, model parameters, and evaluation procedures.Community initiatives like Papers with Code emphasise sharing implementations and benchmarks.Raff (Raff, 2019) argues that transparent reporting is inseparable from reproducible research.Our pipeline supports reproducibility by extracting configuration details automatically, documenting them in a knowledge base, and providing containerised scripts with fixed seeds.</p>
<p>Pipeline Design</p>
<p>Overview</p>
<p>Figure 1 illustrates the architecture of our pipeline (summarised in Algorithm 1).The input is a topic definition (for example, "autoregressive generative models") and optional date range.The pipeline consists of six stages: (1) retrieval of candidate papers from APIs (arXiv, Semantic Scholar); (2) parallel PDF parsing and text extraction; (3) relevance filtering using keyword matching and a classifier; (4) information extraction via rule-based and NLP methods; (5) topic clustering and summarisation using retrieval-augmented LLMs; and (6) script generation and reproduction.All stages write structured records to a knowledge base.The pipeline is orchestrated via Python scripts and containerised with Docker to ensure consistent execution.</p>
<p>Algorithm 1 Automated literature synthesis pipeline (corresponding to Figure 1) Require: Topic q, years [y min , y max ], number of workers N Ensure: Report R, knowledge base K, optional reproduction results E 1: P ← search api(q, [y min , y max ]) ▷ Retrieve candidate papers 2: P ← filter by relevance(P)</p>
<p>▷ Keyword matching and classifier 3: Initialise shared database K 4: parallel for p ∈ P using N workers:
5: t ← pdf</p>
<p>Parallel Parsing and Extraction</p>
<p>Parallelism is crucial for efficiency.We assign each PDF to a worker process that downloads the file, converts it to text, and performs extraction.The shared database K uses thread-safe append operations.For corpora of 1,000 papers, eight workers processed the set in under 60 minutes on an 8-core CPU (Section 4.2).Extraction modules are evaluated quantitatively (Section 4).</p>
<p>Information Extraction</p>
<p>We target four categories of information: metadata (title, authors, year, venue), hyperparameters (architecture type, layer counts, hidden sizes, learning rate, optimiser, dropout rates), results (metrics and numerical values), and citations.Metadata extraction uses regex patterns and heuristics.Hyperparameter extraction relies on rule-based matching of patterns such as "learning rate 0.001" or "3-layer LSTM".For results, we scan for tokens near metric names ("perplexity", "accuracy") and extract numbers.Citation identification recognises citation markers (such as author-year keys enclosed in brackets or braces) and links statements to the corresponding bibliographic entries.Extracted entries are stored in JSON format and aggregated into tables (for example, model vs. dataset vs. perplexity).</p>
<p>Summarisation and Knowledge Base</p>
<p>Topic analysis employs TF-IDF embeddings of paper abstracts and k-means clustering with k selected via the silhouette score.For each cluster, we compile key findings and prompt an LLM (LLaMA-2 or GPT-4) to summarise the group.The LLM is restricted to sentences from the extracted text to prevent hallucination, and we require explicit citations for every factual claim.The resulting summary is appended to the report with in-text citations.The knowledge base exposes a query interface enabling questions such as "What learning rates are most common on WikiText-2?" or "Which papers report perplexity below 40 on WikiText-103?".</p>
<p>Evaluation</p>
<p>We evaluate three aspects of the pipeline: (i) extraction accuracy, (ii) scalability and ablation, and (iii) reproduction case studies.</p>
<p>Extraction Accuracy</p>
<p>To measure extraction quality, we manually annotated 50 papers with ground-truth labels for relevance, hyperparameters, and citation links.The evaluation metrics are precision (P), recall (R) and F1-score (F1).Table 1 summarises results.Hyperparameter extraction achieved an F1-score of 0.88, relevance filtering 0.90, and citation identification 0.86.The high precision indicates that the rule-based patterns rarely over-extract, while the recall shows that most relevant details are captured.</p>
<p>Table 1: Extraction accuracy on a labelled sample of 50 papers.Each task is evaluated using precision (P), recall (R) and F1-score (F1</p>
<p>Scalability and Ablation Studies</p>
<p>We benchmarked the pipeline on datasets ranging from 100 to 1,500 papers.Each experiment measured total processing time and peak memory usage on an 8-core CPU with 32 GB RAM.our scaling analysis shows that processing time increases approximately linearly with the number of papers, and memory consumption grows moderately.For example, 1,000 papers required roughly 40 minutes and 12 GB of RAM.The near-linear scaling demonstrates the effectiveness of parallel parsing.</p>
<p>We also conducted ablation studies by disabling individual components of the pipeline.Table 2 reports the impact on extraction F1-score and processing time.Removing parallelisation increased runtime by almost 3x, while omitting the relevance classifier decreased precision by 11 percentage points.Eliminating rule-based patterns for hyperparameter extraction reduced recall dramatically.These results underscore the necessity of each component.</p>
<p>In lieu of a plot, we analytically characterise scalability.Let n denote the number of processed papers, T (n) the processing time in minutes and M (n) the peak memory usage in gigabytes.Empirically we observe approximately linear relationships
T (n) ≈ 0.04 n + 1, M (n) ≈ 0.01 n + 1,
for n between 100 and 1,500.Thus T (1000) = 41 min and M (1000) = 11 GB.These formulae capture the behaviour previously illustrated graphically and emphasise the near-linear scaling of time and memory with respect to corpus size.Failure Cases.Despite high overall accuracy, the pipeline occasionally fails.Some PDFs use unusual encodings that defeat our text extractor, leading to missing results; we flag these for manual review.Highly mathematical papers with many symbols sometimes yield false positives in hyperparameter extraction.Finally, summarisation quality depends on the retrieval set: if relevant sentences are absent, the LLM may produce generic statements.We mitigate this by expanding retrieval windows and by allowing manual inspection of flagged summaries.</p>
<p>Reproduction Case Studies</p>
<p>We demonstrate the pipeline's practical value through three reproduction studies.All experiments were conducted on a single NVIDIA V100 GPU with random seed 42.The pipeline is containerised to encapsulate dependencies and scripts.Following NeurIPS reproducibility guidelines, we provide dataset links, hyperparameters, and evaluation procedures.</p>
<p>AWD-LSTM on WikiText-2</p>
<p>Our first case reproduces the AWD-LSTM baseline from Merity et al. (2017).The pipeline extracted the architecture (three LSTM layers with hidden sizes 1150, 1150, and 400), variational dropout rates (0.4 on embeddings and 0.3-0.5 on LSTM layers), weight tying, an SGD optimiser with initial learning rate 30, gradient clipping at 0.25, and the training schedule (learning-rate decay on validation plateau).Using these settings, we trained an AWD-LSTM for 500 epochs on the WikiText-2 dataset.The reproduced model achieved test perplexity 66.5, closely matching the reported 65.8.This demonstrates that the extracted hyperparameters suffice for faithful reproduction.Pseudocode for training appears in Algorithm 3, and conceptual reproduction guidelines are provided after the algorithm.M .train()
5:
for each minibatch (x, y) in D train do 6:</p>
<p>Zero optimiser gradients 7:</p>
<p>(ŷ, h) ← M (x) ▷ Forward pass with truncated BPTT 8:</p>
<p>Compute loss L = CrossEntropy(ŷ, y) Reproduction guidelines.To replicate this study in a framework-agnostic manner, researchers should follow a series of general steps rather than relying on a specific codebase.First, obtain the dataset used in the target paper (here, WikiText-2) and preprocess it as described by the authors (for example, tokenise text and construct the vocabulary).Second, initialise the model architecture with the hyperparameters extracted by the pipeline, including the number of layers, hidden sizes, dropout rates, optimiser type, learning-rate schedule, and gradient clipping threshold.Third, train the model for the number of epochs or steps reported in the original work, monitoring validation perplexity and reducing the learning rate when improvements plateau.Finally, evaluate the trained model on the held-out test set using the same metric (test perplexity) and compare it to the published baseline.To ensure comparability, fix the random seed (e.g., 42) and document the computing hardware (e.g., GPU model and memory).These conceptual instructions enable reproduction regardless of implementation details or environment.</p>
<p>Transformer-XL on WikiText-103</p>
<p>For our second study we reproduced Transformer-XL on the larger WikiText-103 dataset.The pipeline recovered the architecture (18-layer Transformer with hidden size 1024, 16 attention heads), recurrence length 150, adaptive softmax, Adam optimiser with learning rate 2 × 10 −4 , and training schedule.Using these hyperparameters, we trained a model for 200K steps.Our reproduced model achieved test perplexity 19.5, close to the 18.3 reported by Dai et al. (2019) on the same dataset.The slight gap stems from computational constraints (we used a smaller batch size and fewer context segments) but illustrates that the extracted settings yield competitive results.</p>
<p>Autoregressive Music Model on Lakh MIDI</p>
<p>The third study tests the pipeline's portability to a different modality.We selected the autoregressive arrivaltime transformer from the Anticipatory Music Transformer work of Thickstun et al. (2024), which models Lakh MIDI events as a temporal point process.The pipeline identified key hyperparameters: vocabulary size 512, model sizes 128M-780M parameters, training steps up to 800K, and nucleus sampling for generation.We reproduced the "Medium 360M arrival" model (row 8 of Table 1 in their paper) with 360 million parameters and 800K training steps; we achieved per-event perplexity 70.3 on the Lakh MIDI test set, comparable to the 69.7 reported by the authors.Human preference evaluation, following their methodology, showed no significant difference between our reproduced model and the baseline FIGARO Music Transformer.This demonstrates that our pipeline can extract hyperparameters and reproduce complex AR models beyond language.</p>
<p>Discussion</p>
<p>Significance and Portability</p>
<p>Our results show that automated literature synthesis can support reproducible research across domains.By combining rule-based extraction, retrieval-augmented summarisation, and script generation, the pipeline produces living surveys that evolve as new papers appear.These surveys go beyond summarisation: they supply the hyperparameters and configurations needed to verify results.The reproduction case studies confirm that extracted settings translate into near-baseline performance across models and domains.Importantly, the pipeline is not limited to AR text models; the music case study and the potential to extend to diffusion models and reinforcement learning highlight its portability.For diffusion models, the same extraction and script-generation approach can harvest denoising schedules and model architectures, while RL pipelines could benefit from automated extraction of environment settings, reward structures and algorithm hyperparameters.</p>
<p>Limitations and Future Work</p>
<p>Despite promising results, challenges remain.Early versions of our information extractor relied exclusively on hand-written heuristics, which could miss unconventional descriptions of hyperparameters.To mitigate this, we extended the extractor with a lightweight named-entity-recognition (NER) component trained on annotated hyperparameter mentions.The NER model recognises parameter names and values in a variety of phrasings (for example, "the dropout was set to 40%"), greatly reducing the number of missed entries.Nevertheless, some idiosyncratic descriptions may still elude detection.LLM summarisation occasionally produces generic text if key sentences are absent from the retrieval set.The pipeline currently uses a fixed clustering algorithm; adaptive topic modelling like BERTopic (Grootendorst, 2022) could improve thematic grouping.Extending the pipeline to cover diffusion models will require new extraction templates (for example, beta schedules).For reinforcement learning, extracting environment configurations and reward functions will be more complex.Finally, while our reproduction experiments use moderate compute (one GPU), faithfully reproducing massive models (for example, GPT-3) remains infeasible for most researchers; nevertheless, the pipeline can still summarise their configurations.</p>
<p>Conclusion</p>
<p>We have presented a comprehensive, scalable pipeline for synthesising the literature on autoregressive generative models.By tightly integrating data retrieval, parallel parsing, information extraction, summarisation, and script generation, the pipeline delivers reproducible surveys that bridge the gap between literature and implementation.Quantitative evaluation shows high extraction accuracy, and scalability experiments demonstrate that the pipeline handles large corpora efficiently.Reproduction studies on language and music models validate the practical utility of the extracted information.Our work provides a template for future automated surveys that can keep pace with fast-moving research fields and promote reproducibility.We release our code and data to foster adoption and encourage the community to extend the pipeline to other domains such as diffusion models and reinforcement learning.</p>
<p>Figure 1 :
1
Figure 1: Schematic of the automated literature synthesis pipeline.Each box corresponds to a stage in Algorithm 1.</p>
<p>Algorithm 3
3
Training AWD-LSTM on WikiText-2 Require: Architecture parameters (layers = 3, hidden sizes = [1150, 1150, 400], embed = 400), dropout rates (d emb , d hid , d out ), optimiser (SGD with ℓr 0 = 30), gradient clip c, epochs E Require: Dataset (D train , D val , D test ) 1: Initialise model M with parameters; tie input and output embeddings 2: Set optimiser with learning rate ℓr = ℓr 0 3: for e = 1 to E do 4:</p>
<p>to text(p) Build tables of metrics, datasets, etc. 13: (T , labels) ← cluster topics({d.title| d ∈ K}) 14: for each topic t in T do
6:d ← extract metadata(t)7:h ← extract hyperparams(t)8:r ← extract results(t)9:c ← extract citations(t)10:Append (d, h, r, c) to K11: end parallel12: K ← aggregate(K) ▷ 15: S t ← summarise(K restricted to t)▷ LLM with retrieval16:Append S t to report R17: end for18: Optional: E ← reproduce experiments(K, q)Retrieve papersParallel parsingRelevance filteringInformation extraction(APIs)&amp; text extraction(keywords + classifier)(metadata, hyperparameters, results)Topic clusteringScript generation&amp; summarisation&amp; reproduction
19: return (R, K, E)</p>
<p>).
TaskPRF1Relevance filtering0.92 0.88 0.90Hyperparameter extraction 0.87 0.89 0.88Citation identification0.84 0.88 0.86Result extraction0.81 0.86 0.83Algorithm 2 Precision, Recall and F1 CalculationRequire: Extracted set E, ground-truth set GEnsure: Precision P, recall R, F1-score F11: tp ← |E ∩ G|▷ True positives2: P ← tp/|E|3: R ← tp/|G|4: F1 ← 2 × P × R/(P + R)5: return (P, R, F1)
Algorithm for Metric Computation.Precision and recall are computed as follows.Let E denote the set of extracted items and G the ground truth.Precision is P = |E ∩ G|/|E|, recall is R = |E ∩ G|/|G|, and F1 = 2PR/(P + R).When computing citation correctness, we treat a citation as correct if the extracted reference corresponds to the correct paper.Algorithm 2 outlines this computation.</p>
<p>Table 2 :
2
Ablation study.Each row disables one component of the pipeline while keeping others intact.We report extraction F1-score and processing time (minutes) on a corpus of 500 papers.
ConfigurationF1-score Time (min)Full pipeline0.8820No parallel parsing0.8858No relevance classifier0.7921No rule-based patterns0.6221No LLM summarisation0.8818</p>
<p>Evaluate test perplexity on D test
9:Backpropagate: L.backward()10:Clip gradients: clip grad norm(M, c)11:Optimiser step12:end for13:Evaluate validation perplexity ppl val14:if ppl val did not improve for 5 epochs then15:ℓr ← ℓr/416:end if17: end for18:</p>
<p>A neural probabilistic language model. Y Bengio, R Ducharme, P Vincent, C Jauvin, Journal of Machine Learning Research. 32003</p>
<p>Interactive perception and robot learning. J Bohg, K Hausman, B Sankaran, O Brock, D Kragic, S Schaal, G Sukhatme, IEEE Robotics and Automation Magazine. 2432017</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, Advances in Neural Information Processing Systems. 202033</p>
<p>Transformer-XL: Attentive language models beyond a fixed-length context. Z Dai, Z Yang, Y Yang, J Carbonell, Q Le, R Salakhutdinov, Proceedings of ACL. ACL2019</p>
<p>MuseGAN: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment. H.-W Dong, W.-Y Hsiao, L.-C Yang, Y.-H Yang, Proceedings of AAAI. AAAI2018</p>
<p>BERTopic: Neural topic modeling with a class-based TF-IDF procedure. M Grootendorst, arXiv:2203.057942022arXiv preprint</p>
<p>Leakage and the reproducibility crisis in machine learning-based science. S Kapoor, A Narayanan, arXiv:2207.070482022arXiv preprint</p>
<p>S Merity, N S Keskar, R Socher, arXiv:1708.02182Regularizing and optimizing LSTM language models. 2017arXiv preprint</p>
<p>Improving reproducibility in machine learning research: A report from the NeurIPS reproducibility program. J Pineau, P Vincent-Lamarre, I Belghazi, E Alché-Buc, Journal of Machine Learning Research. 2212021</p>
<p>Inside good research practices: Transparent research is reproducible research. Papers with Code initiative. E Raff, 2019. 6 Aug 2025</p>
<p>PROMPTHEUS: A human-centered pipeline to streamline systematic literature reviews with large language models. H Torres, F Melo, G Brissaud, N Corsetti, E Medina, M Sano, A Díaz, M Ribeiro, Information. 1654202024</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, arXiv:2307.092882023arXiv preprint</p>
<p>Anticipatory music transformer: Controllable music generation via temporal point processes. J Thickstun, D Hall, C Donahue, P Liang, arXiv:2306.086202024arXiv preprint</p>
<p>A Van Den Oord, N Kalchbrenner, K Kavukcuoglu, arXiv:1601.06759Pixel recurrent neural networks. 2016aarXiv preprint</p>
<p>A Van Den Oord, S Dieleman, H Zen, K Simonyan, O Vinyals, A Graves, N Kalchbrenner, A Senior, K Kavukcuoglu, arXiv:1609.03499WaveNet: A generative model for raw audio. 2016barXiv preprint</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A Gomez, L Kaiser, I Polosukhin, Advances in Neural Information Processing Systems. 201730</p>
<p>A modular pipeline for scientific literature summarization. H Achkar, R Hy, C Grecos, S Kiritchenko, arXiv:2505.163492024arXiv preprint</p>
<p>OpenScholar: Synthesizing scientific literature with retrieval-augmented language models. A Asai, E Chen, K Chen, J Luo, X Qiu, H Peng, M Tan, M Yasunaga, P Liang, L Dong, arXiv:2411.141992024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
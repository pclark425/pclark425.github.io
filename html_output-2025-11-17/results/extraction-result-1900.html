<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1900 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1900</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1900</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-41.html">extraction-schema-41</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <p><strong>Paper ID:</strong> paper-277467976</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.00614v2.pdf" target="_blank">Learning Bipedal Locomotion on Gear-Driven Humanoid Robot Using Foot-Mounted IMUs</a></p>
                <p><strong>Paper Abstract:</strong> Sim-to-real reinforcement learning (RL) for humanoid robots with high-gear ratio actuators remains challenging due to complex actuator dynamics and the absence of torque sensors. To address this, we propose a novel RL framework leveraging foot-mounted inertial measurement units (IMUs). Instead of pursuing detailed actuator modeling and system identification, we utilize foot-mounted IMU measurements to enhance rapid stabilization capabilities over challenging terrains. Additionally, we propose symmetric data augmentation dedicated to the proposed observation space and random network distillation to enhance bipedal locomotion learning over rough terrain. We validate our approach through hardware experiments on a miniature-sized humanoid EVAL-03 over a variety of environments. The experimental results demonstrate that our method improves rapid stabilization capabilities over non-rigid surfaces and sudden environmental transitions.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1900",
    "paper_id": "paper-277467976",
    "extraction_schema_id": "extraction-schema-41",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.003705,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning Bipedal Locomotion on Gear-Driven Humanoid Robot Using Foot-Mounted IMUs</p>
<p>Sotaro Katayama 
Yuta Koda 
Norio Nagatsuka 
Masaya Kinoshita 
Learning Bipedal Locomotion on Gear-Driven Humanoid Robot Using Foot-Mounted IMUs
830054FBB648FC0840B90AFC89FAFE31
Sim-to-real reinforcement learning (RL) for humanoid robots with high-gear ratio actuators remains challenging due to complex actuator dynamics and the absence of torque sensors.To address this, we propose a novel RL framework leveraging foot-mounted inertial measurement units (IMUs).Instead of pursuing detailed actuator modeling and system identification, we utilize foot-mounted IMU measurements to enhance rapid stabilization capabilities over challenging terrains.Additionally, we propose symmetric data augmentation dedicated to the proposed observation space and random network distillation to enhance bipedal locomotion learning over rough terrain.We validate our approach through hardware experiments on a miniature-sized humanoid EVAL-03 over a variety of environments.The experimental results demonstrate that our method improves rapid stabilization capabilities over non-rigid surfaces and sudden environmental transitions.</p>
<p>I. INTRODUCTION</p>
<p>Bipedal and humanoid robots have fascinated people for decades.One of their anticipated roles is to replace human workers.Humanoid robots, which have morphologies similar to humans, are expected to navigate environments accessible to humans and perform tasks that humans can accomplish.Another significant application is in entertainment.A pioneering example of entertainment robotics is the AIBO series [1], a dog-like quadrupedal robot developed to interact with people.QRIO [2] is a small-sized humanoid robot that followed the same approach as AIBO but with biepdal locomotion.BD-X [3], a bipedal robot with a unique, characterlike design, has demonstrated its entertainment applications in the real world.EVAL-03, depicted in Fig. 1, was developed by Sony Interactive Entertainment to further explore the potential of robotics in entertainment.In this work, we focus on enhancing the locomotion capabilities of EVAL-03, which has been limited to upper body movements, static posing, and walking on a flat plane without disturbances [4].</p>
<p>Reinforcement learning (RL) has demonstrated robust, dynamic, and natural locomotion capabilities [5], [6], [7], [8].A key enabler of these advancements is zero-shot sim-to-real transfer, where training is conducted entirely in massively parallelized physics simulation frameworks [9], [10], [11], and the learned policies are deployed directly on real hardware without fine-tuning.However, the success of this approach largely depends on mitigating the simto-real gap.One effective strategy for reducing this gap involves embedding an actuator network [12] within the simulation.This network, trained on real-world data, infers joint torques based on historical joint measurements in the simulation.However, this method requires actuators equipped with torque sensors, which are often prohibitively expensive.An alternative approach is the use of direct or quasi-direct drive actuators [13], [14], which enable accurate modeling of PD-controlled actuators within simulations.Nonetheless, challenges persist in applying RL to robots with high-gear ratio actuators that lack torque sensors.This is particularly relevant for low-cost, miniature-sized humanoid robots.Such robots typically require high-gear ratios to amplify the capabilities of small-sized, low-power motors, often at the expense of increased backlash and joint friction, which leads to nonlinear torque-current relationships.Moreover, these low-cost actuators generally do not have torque sensors, rendering actuator networks unavailable.The ROBOTIS-OP3 [15] exemplifies such a low-cost, miniature-sized humanoid robot and has been utilized in RL studies [7], [16].In [7], to mitigate the sim-to-real gap, the high-gain position control mode is employed while subsequently actuator parameters are identified.Although the authors demonstrated agile soccer motions of the robot, these movements were limited to flat ground.For effective locomotion over varied terrain, compliant joint control, i.e., low-gain joint position control, can be a crucial factor, as it aids in estimating contact states and terrain features, as reported in [17].To enable arXiv:2504.00614v2[cs.RO] 11 Apr 2025 the ROBOTIS-OP3 to operate with compliant joint control, [16] employed a more detailed actuator model identification.Unfortunately, the results only demonstrated slow walking over a tilted plane, with no success on rough terrain or steps, despite the necessity for careful real data collection.In a similar context, but for human-sized humanoid robots, [18] incorporates current feedback to account for torquetracking errors of the actuators.However, this approach still necessitates the identification of accurate motor parameters, such as motor armature and friction characteristics.Upon these studies, we pose the question: can we improve sim-toreal transfer by introducing additional sensor observations?</p>
<p>In this paper, we propose the use of foot-mounted inertial measurement units (IMUs) for learning bipedal locomotion on a gear-driven humanoid robot.As illustrated in Fig. 1, we utilize sensor measurements (linear accelerations and angular velocities) from foot-mounted IMUs as well as the base-mounted IMU within the blind locomotion learning framework [17].Additionally, we introduce symmetric data augmentation [19] dedicated to the proposed observation space and random network distillation [20], [21] to enhance the learning of bipedal locomotion over rough terrains.We conducted hardware experiments on the gear-driven, miniature-sized humanoid robot EVAL-03 over a variety of environments including non-rigid surfaces and sudden environmental transitions.Through the hardware experiments, we demonstrated that the proposed method improves rapid stabilization capabilities by leveraging the feet states measured by foot-mounted IMUs instead of employing detailed and careful system identification of the actuators as in [16].</p>
<p>II. RELATED WORKS A. Sim-to-Real Transfer</p>
<p>Massively parallelized physical simulation frameworks [9], [10], [11] have enabled efficient collection of large amounts of training data.However, when deploying policies trained solely in simulation to the real world, the sim-toreal gap-the discrepancy between the simulation model and real environment-can significantly affect the policy's performance.A common approach to mitigate this gap is to employ domain randomization [22] for parameters such as inertial properties (e.g., masses and centers of mass) and actuator characteristics (e.g., PD gains and friction).For robots equipped with direct or quasi-direct drive actuators [13], [14], simple domain randomization has proven effective, as these actuators can be accurately modeled using basic PD controllers in physical simulators.However, when the actuator model deviates significantly from simple PD control-for instance, with substantial nonlinearity in the current-torque relationship-more detailed parameter identification becomes necessary [23], [24], [16].This is particularly relevant for high-gear ratio actuators that exhibit backlash, joint friction, which leads to nonlinear torquecurrent relationships.An alternative approach involves training neural networks to imitate real-world data [12].However, this method is only viable when actuators are equipped with torque sensors, which are often prohibitively expensive.Moreover, both detailed parameter identification and actuator networks require careful collection of real-world data to ensure sufficient coverage of possible observations.</p>
<p>B. Leveraging Foot-Mounted IMUs</p>
<p>Foot-mounted IMUs have been utilized in human motion analysis [25].However, in legged robotics, their application has been limited to a few studies [26], [27].In [26] and [27], foot-mounted IMUs are employed to enhance state estimation in humanoid and quadrupedal robots, respectively.The demonstrated effectiveness of foot-mounted IMU measurements in state estimation has inspired us to leverage them in RL-based locomotion control, as (partially observable) RL can encompass state estimation [17], [28], [29].</p>
<p>III. METHOD</p>
<p>A. Reinforcement Learning of Bipedal Locomotion with Foot-Mounted IMUs</p>
<p>Our method is based on Legged Gym [10], a model-free RL framework leveraging massively parallelized physical simulation [9].The policy is conditioned on velocity commands comprising longitudinal, lateral, and yaw velocities (v x,cmd , v y,cmd , and w z,cmd , respectively).As provided by Legged Gym, the policy is trained across various terrains, including slopes, rough surfaces, upward stairs, downward stairs, and discrete steps.Each of these terrains is generated with 10 different difficulty levels, and we employ a constant curriculum similar to [12].</p>
<p>The observation space of the proposed method is detailed in Table I, and the observation noise for sim-to-real transfer is listed in Table II.Notably, our observations include linear accelerations and angular velocities from IMUs mounted on the left and right feet.Additionally, we incorporate the linear acceleration of the base IMU, which is absent in some existing studies on RL for locomotion.We hypothesize that   The action space consists of target joint positions for the low-level PD controller.Since we often employ high-gain PD control for small-sized and low-cost actuators such as those in EVAL-03, the target joint position command must be smooth to avoid hardware damage.To achieve this, following [7], we employ a low-pass filter:
q J,cmd (t) = 0.8 q J,cmd (t ‚àí 1) + 0.2 a(t),(1)
where q J,cmd (t) is the target joint position at time t and a(t) is the latest scaled action at time t.The PD controller with the low pass filter update (1) run at 1000 Hz and and policy inference operate at 100 Hz in both simulation and real-robot scenarios.</p>
<p>In defining the reward function, we primarily follow the default reward structure provided by Legged Gym [10], while tuning the hyperparameters, particularly those regarding the robot's size.We also introduce dedicated rewards for bipedal locomotion introduced in [30]: gait reward, swing-foot clearance reward, and penalties for self-collisions between left and right feet, while also adapting hyperparameters to the miniature-sized humanoid robot.Our reward function terms are summarized in Table III.</p>
<p>B. Sim-to-Real Considerations</p>
<p>To enhance sim-to-real transfer, we employ domain randomizations such as additional base mass, center of mass   IV.</p>
<p>It should be noted that, because the IMUs equipped on EVAL-03 are not high-grade, their accelerometer range is limited.To close the sim-to-real gap, we replicate this limited sensor range in the simulation by clipping the linear acceleration observation terms.</p>
<p>Additionally, due to the low-cost motors, the actuators do not have an interface to provide joint velocities.Therefore, in the hardware, we estimate joint velocities using finite differences of joint positions:
qJ (t) ‚âÉ (q J (t) ‚àí q J (t ‚àí 1))/‚àÜt,(2)
at each 1000 Hz control loop, that is, ‚àÜt = 1 ms.To close the sim-to-real gap, we also replicate this joint velocity estimation (2) in simulation as the joint velocity observation instead of using actual joint velocities from the simulator.</p>
<p>C. Symmetric Data Augmentation</p>
<p>We observe that naive RL can result in asymmetric and inefficient bipedal motions due to specific hardware design.Specifically, in training the RL policy for EVAL-03 with our reward settings, naive RL tends to excessively avoid selfcollisions between the left and right feet, which are very close even in the default joint position, as shown in Fig. 1.The resultant motion can produce undesired yaw velocities due to the asymmetricity.To enforce a symmetric policy with respect to the body center, we employ symmetric data augmentation [19].To generate symmetric observations, we mirror the given observations with respect to the body center.To make the mirroring straightforward, most observation terms are expressed in the base local coordinate (e.g., velocity commands, projected gravity, base-mounted IMU measurements) or joint quantities.However, we must perform coordinate frame transformations to mirror observations from the feet IMUs that are expressed in the local coordinate of each IMU.These transformations are illustrated in Fig. 2.</p>
<p>D. Teacher-Student Training with Fine-tuning</p>
<p>We adopt the teacher-student training framework for blind locomotion over rough terrains [17], as illustrated in Fig. 3. Initially, we train the teacher policy using a short observation history (o t‚àí4 , . . ., o t ) and a short privileged observation history (p t‚àí4 , . . ., p t ) using the Proximal Policy Optimization (PPO) algorithm [31], with modifications for symmetric data augmentation [19].The privileged observation terms are included in Table I.</p>
<p>Subsequently, we train the student policy through supervised learning to imitate the teacher's actions and reconstruct privileged information.The student policy employs a temporal convolutional network (TCN) [32] as a belief encoder to estimate the privileged information from the long observation history (o t‚àí99 , . . ., o t ).During the supervised learning phase, we also leverage symmetric data augmentation: we collect data from the simulation using the student policy in an on-policy fashion, similar to DAgger, and augment its symmetric counterpart to the batch.</p>
<p>After the supervised learning of the student policy, we further fine-tune the student policy via RL using PPO.In this phase, we employ an asymmetric actor-critic approach, providing privileged observations to the critic while withholding them from the actor.In contrast to [33], we train the entire student policy, as this approach enhances performance in our problem settings compared to fine-tuning only the base policy MLP.</p>
<p>E. Random Network Distillation</p>
<p>During the RL training of the teacher policy, we utilize random network distillation (RND) [20] to enhance explo-ration.We observe that, without RND, the teacher policy tends to exhibit minimal swing-foot clearance to overly avoid risks of falling, even we have the reward function to promote swing-foot clearance as listed in Table III.Following [21], instead of using the full observations o t in RND exploration, we define the so-called curiosity state s independent of the observations:
s := Ô£Æ Ô£∞ r left r right HeightScan Ô£π Ô£ª ,(3)
where r left , r right ‚àà R 3 denote the positions of the left and right feet expressed in the base local coordinate frame, respectively.The curiosity state defined in (3) aims to encourage exploration of various foot positions for each given terrain observation.We choose MLPs with hidden sizes of (64, 64, 16) for the target network and (64, 32, 16) for the predictor network, respectively.During training of a teacher policy, we add the intrinsic reward [20] whose weight was set to 2.0 while updates the predictor network to reduce the difference between the outputs of the predictor and target networks.</p>
<p>IV. EXPERIMENTAL SETUP</p>
<p>To evaluate the effectiveness of the proposed method, we have compared the following three policies in the hardware experiments:</p>
<p>1) Policy observing linear accelerations and angular velocities of base-mounted IMU and foot-mounted IMUs (w/ Feet IMUs) 2) Policy observing linear accelerations and angular velocities of base-mounted IMU (w/o Feet IMUs 1) 3) Policy observing angular velocities of base-mounted IMU (w/o Feet IMUs 2) The first method represents our proposed approach, while the latter two represent existing methods.Through hardware experiments, we investigate how the additional feet IMU observations can mitigate sim-to-real gaps and enhance stability on real hardware.</p>
<p>A. Hardware Details</p>
<p>We use the gear-driven, miniature-sized humanoid robot EVAL-03, which is depicted in Fig. 1, throughout the experiments.It stands approximately 240 mm tall from ground to the head link when standing at the default joint posture.The total weight is around 1.73 kg.The robot has 27 degrees of freedom (DOFs) in total: 6 DOFs in each leg, 3 DOFs in the torso, 4 DOFs in each arm, and 3 DOFs in the head.However, in this paper, we treat the joints in the upper body as fixed joints for simplicity, reducing the total active DOFs to 12.</p>
<p>Consistent with its compact size, the motors are also small.Consequently, each actuator employs a high-gear ratio to compensate for the low-power motors while lacking a torque sensor.To facilitate smooth sim-to-real transfer under these specifications, we employ high-gain PD control with the low pass filter (1) as introduced in [7].The control architecture operates at multiple frequencies.The policy runs at 100 Hz, while the low-level PD controller and orientation filter [34] operate at 1000 Hz.The orientation filter estimates the base rotation, expressed as a quaternion, from the base IMU observations, which is then converted to the projected gravity.</p>
<p>B. Training Details</p>
<p>For each policy, we trained multiple seeds and selected the best one for comparison through a three-stage process.First, we trained six teacher policies with different seeds and selected the best two policies based on motion quality and reward performance.Then, we trained four student policies with different seeds for each of the two selected teacher policies.Finally, we selected the best one from the eight student policies based on sim-to-real transfer performance on the real hardware, rather than simulation reward values.</p>
<p>1) Training Teacher Policy: For teacher policy training, we collected trajectories using 4096 parallelized environments.We implemented PPO with modifications for symmetric data augmentation [19].The batch size was 196608 with symmetric data augmentation, and the number of minibatches was 6.We utilized the adaptive learning rate as described in [10].The teacher policy was trained for 20000 learning iterations.</p>
<p>2) Training Student Policy: For student policy training, we collected trajectories using 2048 parallelized environments.The batch size was 98304 with symmetric data augmentation, and the number of minibatches was 6.We employed a fixed learning rate of 5.0 √ó 10 ‚àí4 .The student policy was trained for 15000 learning iterations.</p>
<p>3) Finetuning Student Policy: For student policy finetuning, we collected trajectories using 2048 parallelized environments.The batch size was 98304 with symmetric data augmentation, and the number of minibatches was 6.We utilized the adaptive learning rate as described in [10].The finetuning process continued for 25000 learning iterations.</p>
<p>V. EXPERIMENTAL RESULTS</p>
<p>A. Walking on Floor</p>
<p>First, we examined the performances of three policies on the floor to evaluate their velocity tracking capabilities.We evaluated the performances with forward command (v x,cmd = 0.05 m/s), turn command (w z,cmd = 0.5 rad/s), and fast turn command (w z,cmd = 1.0 rad/s).</p>
<p>Table V shows the average walking speed of each policy for given velocity commands.While the yaw velocity (w z )  was directly measured from the base-mounted IMU, the forward velocity of the robot (v x ) was estimated from videos because we could not measure it from equipped sensors.As shown in Table V, the proposed method tracked the velocity commands better than the other methods in terms of average speed comparison.Notably, the proposed method could track the fast turn command while the other methods fell down by losing balance to track the fast yaw velocity command.Fig. 4 shows the plot of yaw velocity w z of each policy during tracking the turn command w z,cmd = 0.5, which also illustrates that the proposed method resulted in less deviation between w z from w z,cmd than the other two policies.Fig. 5 shows snapshots of EVAL-03 walking on the floor using the policy with foot-mounted IMU observations, including the reactive motion against an external push disturbance.In the following experiments, we further compare such robustness among the three policies.</p>
<p>B. Walking over a Variety of Terrains</p>
<p>Second, we examined the performance of the policies in walking over various terrains: floor, turf (thin), turf (thick), cushion (pet), cushion (human), bubble wrap, and uneven urethane sheet.These terrains are depicted in Fig. 6.We commanded forward walking with v x,cmd = 0.05 m/s, v y,cmd = 0 mm/s, and w z,cmd = 0 rad/s.During the experiments, we measured two metrics: the success rate in traversing the terrain and the walking speed relative to the policy's performance on the floor.Note that the walking speeds were only evaluated from successful cases.</p>
<p>Table VI shows the success rates and walking speed rates of the three policies, while Fig. 7 shows snapshots of EVAL-03 walking over challenging terrains using the proposed method (w/ Feet IMUs).As shown in Table VI, the feet IMU observations enhanced stability on uneven terrains.Notably, for cushion (pet) and uneven urethane sheet, the proposed method achieved significantly higher success rates while the other policies failed to maintain balance and fell.Additionally, the proposed method maintained consistent walking speeds even in the challenging cushion (pet) case comparable to floor walking, while other policies struggled and became stuck on soft terrains.</p>
<p>C. Descending Steps</p>
<p>Third, we evaluated the policies' performance in descending steps of varying heights (10 mm, 20 mm, and 25 mm).We commanded forward walking with v x,cmd = 0.05 m/s, v y,cmd = 0 m/s, and w z,cmd = 0 rad/s.We measured the success rate, defined as the percentage of successful step descents without falling.Table VII presents the success rates for step descent, while Fig. 8 shows snapshots of EVAL-03 descending various steps using the proposed method (w/ Feet IMUs).As shown in Table VII, the proposed method successfully navigated steps where other methods consistently failed, demonstrating its enhanced robustness in sudden terrain transitions.</p>
<p>D. Walking with Payloads</p>
<p>Lastly, we evaluated the policies' performance while carrying unexpected payloads (0.33 kg and 0.55 kg).We commanded forward walking with v x,cmd = 0.05 m/s, v y,cmd = 0 m/s, and w z,cmd = 0 rad/s and estimated the walking speed from the videos.</p>
<p>Table VIII presents the walking speed with unexpected payloads, while Fig. 9 shows snapshots of EVAL-03 walking with payloads using the proposed method (w/ Feet IMUs).As shown in Table VIII, with a relatively light 0.33 kg payload (19 % of the total mass), the proposed method maintained consistent walking speeds comparable to its unloaded performance, while other methods exhibited significant speed degradation when carrying payloads.However, with a relatively heavy 0.55 kg payload (32 % of the total mass), all three methods resulted in slow walking speeds.</p>
<p>E. Discussion and Limitation</p>
<p>The experimental results demonstrate that the proposed method exhibited rapid stabilization capabilities over challenging terrains, including non-rigid surfaces (cushion (pet)    VI) and sudden environmental transitions (step descent in Table VII).We hypothesize that foot-mounted IMUs enable direct and rapid measurement of feet states, which helps the policy cope with balance challenges arising from contacts with various environments.</p>
<p>However, several limitations remain.The proposed method failed to maintain balance in more challenging scenarios (e.g., human-sized cushion in Fig. 6).Furthermore, the proposed method was unable to climb even modest obstacles, such as a 5 mm step, as well as the other two policies.This limitation suggests that terrain feature estimation solely through foot-mounted IMUs may be insufficient within our current learning framework despite utilizing a long observation history of up to 100 time steps (1.0 s).Alternative approaches, such as utilizing joint position tracking errors with low-gain PD control [17], still remain promising for addressing these challenges.</p>
<p>VI. CONCLUSIONS</p>
<p>This paper presented novel approach to learning bipedal locomotion on gear-driven humanoid robots using footmounted IMUs.Rather than pursuing complex actuator modeling or system identification, we introduced linear acceleration and angular velocity measurements from footmounted IMUs as well as the base-mounted IMU within the blind locomotion learning framework.We also intro-duced symmetric data augmentation and random network distillation to enhance bipedal locomotion learning with the proposed framework.Through hardware experiments on EVAL-03 with a variety of settings, we showed that the proposed method improved stability on non-rigid surfaces and during sudden environmental transitions, such as step descents.However, limitations remain, particularly in upward step navigation, which suggests directions for future research to introduce compliant joint control with lower PD gains.</p>
<p>Fig. 1 :
1
Fig. 1: Upper: A photo (left) and kinematic model (right) of the gear-driven, miniature-sized humanoid robot EVAL-03.Lower: An overview of the proposed method.In the kinematic model, three IMUs mounted on the body, left foot, and right foot are illustrated as red spheres.</p>
<p>2 J- 5 .
25
. tracking exp(‚àí50 * (wz ‚àí w z,cmd ) 2 ) min &lt; z swing &lt; zmax) [30] 0.2 Stance feet slip v 2 x,y + w 2 z -0.1 Feet distance exp(min(d feet ‚àí 0.05, 0)) -2.0 Knee distance exp(min(d knee ‚àí 0.05, 0)) -2.0 Foot-knee distance exp(min(d foot‚àíknee ‚àí 0.05, 0)) -2.0 Joint positions |q J ‚àí q J,default | -0.1 Joint velocities q2 J -5.0e-4 Joint accelerations q2 J -1.0e-7 Joint torques œÑ 0e-5 Action rate (at ‚àí a t‚àí1 ) 2 -0.01 Action smoothness (at ‚àí 2a t‚àí1 ‚àí a t‚àí2 ) 2 -0.01 Termination 1 termination -200 the foot-mounted IMUs enable direct and rapid measurement of feet states, which can improve capabilities of motion over a variety of terrains.</p>
<p>Fig. 2 :
2
Fig. 2: Coordinate frame transformations in mirroring leftfoot IMU observations to right-foot IMU observations for symmetric data augmentation.</p>
<p>1Fig. 3 :
3
Fig. 3: Teacher-student training with fine-tuning</p>
<p>Fig. 4 :
4
Fig. 4: Plots of yaw velocities w z in tracking the turn command w z,cmd = 0.5 rad/s.</p>
<p>Fig. 5 :
5
Fig. 5: Snapshots of EVAL-03 walking on floor using the foot-mounted IMU observations.</p>
<p>Fig. 6 :
6
Fig. 6: Photos of terrains used in the hardware experiments: floort, turf (thin), turf (thick), cushion (pet), cushion (human), babble wrap, and uneven urethane sheet.</p>
<p>(a) Cushion (pet).(b) Uneven urethane sheet.</p>
<p>Fig. 7 :
7
Fig. 7: Snapshots of EVAL-03 walking over challenging terrains using the foot-mounted IMU observations.</p>
<p>(a) Small step (10 mm).(b) Medium step (20 mm).(c) Large step (25 mm).</p>
<p>Fig. 8 :
8
Fig. 8: Snapshots of EVAL-03 stepping down from small (10 mm), medium (20 mm), and large (25 mm) steps using the foot-mounted IMU observations.</p>
<p>Fig. 9 :
9
Fig. 9: Snapshots of EVAL-03 walking with unexpected 0.55 kg payload using the foot-mounted IMU observations.</p>
<p>TABLE I :
I
List of observation terms
InputObs. Privileged obs. Dim.Base velocity command‚úì‚úì3Base IMU linear acceleration‚úì‚úì3Base IMU angular velocity‚úì‚úì3Base projected gravity‚úì‚úì3Joint positions‚úì‚úì12Joint velocities‚úì‚úì12Last actions‚úì‚úì12Feet IMU accelerations‚úì‚úì6Feet IMU angular velocities‚úì‚úì6Noiseless joint positions‚úì12Noiseless joint velocities‚úì12Base linear velocity‚úì3Noiseless base projected gravity‚úì3Base push force‚úì3Base push torque‚úì3Feet contact forces‚úì6Feet contact flags‚úì2Target feet contact flags‚úì2Added base mass‚úì1COM displacement‚úì3Friction coefficient‚úì1Restituition coefficient‚úì1Height scan‚úì117</p>
<p>TABLE II :
II
List of observation noise scales
InputNoise scaleBase velocity command-Base/feet IMU acceleration4.0Base/feet IMU angular velocity0.1Base projected gravity0.05Joint positions0.05Joint velocities1.0Last actions-</p>
<p>TABLE III :
III
List of reward function terms</p>
<p>Reward Term Expression WeightLin.vel.tracking exp(‚àí1000 * (vxy ‚àí v xy,cmd</p>
<p>TABLE IV :
IV
List of domain randomizations
ParameterUnitRangeOperatorJoint position encoder offsetrad[-0.01, 0.01]AdditiveIMU accerarometer offsetm/s 2[-0.1, 0.1]AdditiveIMU gyroscope sensor offsetrad/s[-0.005, 0.005]AdditiveAdded base masskg[0.0, 0.2]AdditiveCOM displacementm[-0.05, 0.05]AdditiveFriction coefficient-[0.1, 1.0]ScalingRestituition coefficient-[0.0, 0.1]AdditiveKp factor-[0.9, 1.1]ScalingK d factor-[0.5, 1.5]ScalingSystem delayms[0, 10]-ùë£ùë£ ùë•ùë•ùë£ùë£ ùë•ùë•Mirroring linear term:ùë£ùë£ ùë¶ùë¶‚Üí‚àíùë£ùë£ ùë¶ùë¶ùë£ùë£ ùëßùëßùë£ùë£ ùëßùëßùë§ùë§ ùë•ùë•‚àíùë§ùë§ ùë•ùë•Mirroring angular term:ùë§ùë§ ùë¶ùë¶‚Üíùë§ùë§ ùë¶ùë¶ùë§ùë§ ùëßùëß‚àíùë§ùë§ ùëßùëßBase localBase localRight-footLeft-footlocallocalWorldWorld</p>
<p>TABLE V :
V
Average walking speed for a given input velocity command in wallking on floor Method v x,cmd = 0.05 w z,cmd = 0.5 w z,cmd = 1.0
(forward)(turn)(fast turn)w/ Feet IMUsvx = 0.043wz = 0.59wz = 1.23w/o Feet IMUs 1vx = 0.033wz = 0.71-w/o Feet IMUs 2vx = 0.03wz = 0.66-</p>
<p>TABLE VII :
VII
Success rates in descending steps
MethodSmall Medium Largew/ Feet IMUs1.01.00.6w/o Feet IMUs 10.00.20.0w/o Feet IMUs 20.20.00.0</p>
<p>TABLE VIII :
VIII
Forward walking speed x [m/s] with unexpected payloads
Method0.33 kg 0.55 kgw/ Feet IMUs0.040.017w/o Feet IMUs 10.0080.012w/o Feet IMUs 20.0240.014
Sony Group Corporation, Minato-ku, Tokyo, Japan, 108-0075 sotaro.katayama@sony.com
Sony Interactive Entertainment Inc., Sony City 1-7-1, Konan, Minatoku, Tokyo, 108-0075 Japan
TABLE VI: Success rates and speed rates in walking over a variety of terrains.The speed rates are computed by dividing the average moving speed over the terrain by the walking speed over the floor for each policy.(c) External push.
Aibo. Sony, 2025</p>
<p>Stories of qrio and pino, and beyond: Lessons learned from small humanoid projects from r&amp;d to business. M Fujita, Y Kawanami, K Miyazawa, M Kinoshita, K Sawai, F Yamasaki, T Matsui, K Endo, S Ishiguro, H Kitano, International Journal of Humanoid Robotics. 210123500272024</p>
<p>Design and control of a bipedal robotic character. R Grandia, E Knoop, M Hopkins, A , G Wiedebach, J Bishop, S Pickles, D Muller, M Bacher, Robotics: Science and Systems (RSS). 20242024</p>
<p>Learning bipedal robot locomotion from human movement. M Taylor, S Bashkirov, J F Rico, I Toriyama, N Miyada, H Yanagisawa, K Ishizuka, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Reinforcement learning for versatile, dynamic, and robust bipedal locomotion control. Z Li, X B Peng, P Abbeel, S Levine, G Berseth, K Sreenath, The International Journal of Robotics Research. 2024</p>
<p>Humanoid parkour learning. Z Zhuang, S Yao, H Zhao, 8th Annual Conference on Robot Learning. 2024</p>
<p>Learning agile soccer skills for a bipedal robot with deep reinforcement learning. T Haarnoja, B Moran, G Lever, S H Huang, D Tirumala, J Humplik, M Wulfmeier, S Tunyasuvunakool, N Y Siegel, R Hafner, Science Robotics. 98980222024</p>
<p>Unitree g1 bionic: Agile upgrade. U Robotics, 2025Accessed: (Use the date of access)</p>
<p>V Makoviychuk, L Wawrzyniak, Y Guo, M Lu, K Storey, M Macklin, D Hoeller, N Rudin, A Allshire, A Handa, arXiv:2108.10470Isaac gym: High performance gpu-based physics simulation for robot learning. 2021arXiv preprint</p>
<p>Learning to walk in minutes using massively parallel deep reinforcement learning. N Rudin, D Hoeller, P Reist, M Hutter, Conference on Robot Learning. 2022</p>
<p>Orbit: A unified simulation framework for interactive robot learning environments. M Mittal, C Yu, Q Yu, J Liu, N Rudin, D Hoeller, J L Yuan, R Singh, Y Guo, H Mazhar, IEEE Robotics and Automation Letters. 862023</p>
<p>Learning agile and dynamic motor skills for legged robots. J Hwangbo, J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, Science Robotics. 42658722019</p>
<p>Mini cheetah: A platform for pushing the limits of dynamic quadruped control. B Katz, J Di Carlo, S Kim, 2019 International Conference on Robotics and Automation (ICRA). 2019</p>
<p>Berkeley humanoid: A research platform for learning-based control. Q Liao, B Zhang, X Huang, X Huang, Z Li, K Sreenath, 2024</p>
<p>Robotis-op3. ROBOTIS. 2024</p>
<p>Sim-to-real transfer of compliant bipedal locomotion on torque sensor-less gear-driven humanoid. S Masuda, K Takahashi, 2023 IEEE-RAS 22nd International Conference on Humanoid Robots (Humanoids). 2023</p>
<p>Learning quadrupedal locomotion over challenging terrain. J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, Science robotics. 54759862020</p>
<p>Learning bipedal walking for humanoids with current feedback. Z Xie, P Gergondet, F Kanehiro, IEEE Access. 112023</p>
<p>Symmetry considerations for learning task symmetric robot policies. M Mittal, N Rudin, V Klemm, A Allshire, M Hutter, 2024 IEEE International Conference on Robotics and Automation. 2024ICRA 2024</p>
<p>Exploration by random network distillation. Y Burda, H Edwards, A Storkey, O Klimov, Seventh International Conference on Learning Representations. 2019</p>
<p>Curiosity-driven learning of joint locomotion and manipulation tasks. C Schwarke, V Klemm, M Van Der Boon, M Bjelonic, M Hutter, Proceedings of The 7th Conference on Robot Learning. The 7th Conference on Robot Learning2023229</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2017</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. J Tan, RSST Zhang, RSSE Coumans, RSSA Iscen, RSSY Bai, RSSD Hafner, RSSS Bohez, RSSV Vanhoucke, RSSRobotics: Science and Systems. 2018. 2018</p>
<p>Sim-to-real transfer for biped locomotion. W Yu, V C Kumar, G Turk, C K Liu, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2019</p>
<p>Real-time identification of gait events in impaired subjects using a single-imu foot-mounted device. J C Perez-Ibarra, A A Siqueira, H I Krebs, IEEE Sensors Journal. 2052019</p>
<p>Multi-imu proprioceptive state estimator for humanoid robots. F E Xavier, G Burger, M P√©triaux, J.-E Deschaud, F Goulette, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 202310887</p>
<p>Multi-imu proprioceptive odometry for legged robots. S Yang, Z Zhang, B Bokser, Z Manchester, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2023</p>
<p>Rma: Rapid motor adaptation for legged robots. A Kumar, Z Fu, D Pathak, J Malik, 2021</p>
<p>Learning robust perceptive locomotion for quadrupedal robots in the wild. T Miki, J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, Science robotics. 76228222022</p>
<p>Humanoid-gym: Reinforcement learning for humanoid robot with zero-shot sim2real transfer. X Gu, Y.-J Wang, J Chen, arXiv:2404.056952024arXiv preprint</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Temporal convolutional networks for action segmentation and detection. C Lea, M D Flynn, R Vidal, A Reiter, G D Hager, proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2017</p>
<p>Adapting rapid motor adaptation for bipedal robots. A Kumar, Z Li, J Zeng, D Pathak, K Sreenath, J Malik, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2022</p>
<p>An efficient orientation filter for inertial and inertial/magnetic sensor arrays. S Madgwick, 201025-io and University of Bristol (UK)Report x</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1105 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1105</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1105</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-5472671</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1610.00366v1.pdf" target="_blank">Funneled Bayesian Optimization for Design, Tuning and Control of Autonomous Systems</a></p>
                <p><strong>Paper Abstract:</strong> In this paper, we tackle several problems that appear in robotics and autonomous systems: algorithm tuning, automatic control, and intelligent design. All those problems share in common that they can be mapped to global optimization problems where evaluations are expensive. Bayesian optimization (BO) has become a fundamental global optimization algorithm in many problems where sample efficiency is of paramount importance. BO uses a probabilistic surrogate model to learn the response function and reduce the number of samples required. Gaussian processes (GPs) have become a standard surrogate model for their flexibility to represent a distribution over functions. In a black-box settings, the common assumption is that the underlying function can be modeled with a stationary GP. In this paper, we present a novel kernel function specially designed for BO, that allows nonstationary behavior of the surrogate model in an adaptive local region. This kernel is able to reconstruct nonstationarity even with the irregular sampling distribution that arises from BO. Furthermore, in our experiments, we found that this new kernel results in an improved local search (exploitation), without penalizing the global search (exploration) in many applications. We provide extensive results in well-known optimization benchmarks, machine learning hyperparameter tuning, reinforcement learning, and control problems, and UAV wing optimization. The results show that the new method is able to outperform the state of the art in BO both in stationary and nonstationary problems.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1105.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1105.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SBO-MountainCar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spartan Bayesian Optimization (SBO) applied to Mountain Car policy search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active policy-search agent that uses Spartan Bayesian Optimization (a GP surrogate with an adaptive local+global 'funnel' kernel) and Expected Improvement acquisition to adaptively select policy parameters for the mountain car task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Spartan Bayesian Optimization (SBO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A Bayesian optimization agent using a Gaussian process surrogate with a Spartan (funnel) kernel: a weighted linear combination of a global and a local Matérn kernel (both ARD), with the local kernel center (θ_p) and other hyperparameters inferred by MCMC; next-policy selection is performed by maximizing Expected Improvement (EI) over policy parameter space.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization (active policy search) using an adaptive Spartan kernel + Expected Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The agent maintains a GP posterior (mixture over MCMC hyperparameter samples). The Spartan kernel combines a broad global kernel and an adaptive local kernel whose center θ_p is treated as a hyperparameter and sampled with MCMC; as data accumulates, the posterior over θ_p shifts toward densely sampled / high-performing regions (funnel moves), and EI is maximized to choose the next policy. Thus adaptation uses all past observations (GP posterior) and hyperparameter MCMC samples to focus sampling near promising optima while retaining global exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Mountain Car (policy search)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown dynamics (simulator-based evaluations); reward landscape with large flat/penalized regions and small informative regions (heteroscedastic / nonstationary w.r.t. policy), many policies yield near-zero or very low reward (failure/flat zones); continuous state and action representations used in the policy parameterization; evaluations are episodic (Monte Carlo returns).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Low-to-moderate policy dimensionality: 7 policy parameters for the perceptron policy used; episode length not explicitly quantified for mountain car in paper; continuous state (position, velocity) and continuous action (throttle) spaces; stochasticity not emphasized (simulator deterministic in typical mountain car).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>SBO achieved maximum performance in all trials after ~27 policy trials (reported as 17 BO iterations + 10 initial Latin-hypercube samples).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines (standard stationary BO and WARP input-warping BO) converged more slowly or got stuck in local maxima; quantitative baseline numbers not provided for mountain car beyond qualitative comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High: reached maximum performance reliably within 27 policy evaluations (including 10 initial samples), demonstrating very few-episode sample efficiency compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Managed by the Spartan kernel + EI: the global kernel maintains coarse long-range uncertainty (encouraging exploration), while the adaptive local kernel produces tight bounds and higher variance near inferred promising regions (encouraging fast local exploitation); EI balances mean and variance to pick next queries.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to standard Gaussian-process BO (stationary Matérn ν=5/2 ARD) and WARP (input warping BO); results are also discussed relative to classical RL solver SARSA (qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>SBO markedly improved sample efficiency on mountain car policy search, reaching maximal performance in all trials with ~27 policy evaluations while standard BO and WARP were slower or got stuck; the adaptive funnel kernel focuses modeling capacity near optima while preserving global exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No explicit failure on this task reported, but general SBO caveats apply: larger hyperparameter dimensionality (more MCMC cost) and potential transient suboptimal early behavior if the local kernel has not yet moved to a promising region.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Funneled Bayesian Optimization for Design, Tuning and Control of Autonomous Systems', 'publication_date_yy_mm': '2016-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1105.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1105.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SBO-Walker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spartan Bayesian Optimization (SBO) applied to three-limb walker policy search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active policy-search agent using SBO to tune an 8-parameter controller for a biped/three-limb walker to maximize walking speed while avoiding falls.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Spartan Bayesian Optimization (SBO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Bayesian optimization agent with a Gaussian process surrogate using the Spartan (funnel) kernel (local + global Matérn kernels, ARD), hyperparameters sampled via MCMC; acquisition is Expected Improvement over policy parameters; used to directly optimize episodic return of a parameterized walking controller.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization (active policy search) using an adaptive Spartan kernel + Expected Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adaptation proceeds by updating GP posterior with each episodic return; MCMC samples of kernel hyperparameters (including θ_p centers for local kernel) cause the local funnel to migrate toward regions with dense/high-performing samples; next policy selected by maximizing EI, thereby concentrating samples near promising policy regions while retaining exploration via the global kernel.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Three-limb walker (Westervelt-style walker)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown dynamics for the walker; episodic controller evaluation with penalty for falling (failure states) producing large negative/flat reward regions; reward is walking speed with penalties for losing upright posture; heteroscedastic and nonstationary reward surface w.r.t. controller parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Controller parameter dimensionality: 8 continuous parameters; state space of simulated walker (not fully enumerated in paper) but dynamics are nonlinear and can produce catastrophic failures (falls). Episodes are finite horizon; Monte Carlo used to estimate expected return.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>SBO achieved higher reward and improved sample efficiency compared to standard BO and WARP across experiments (qualitative improvement reported; specific numeric reward/time series not tabulated in the text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Standard BO and WARP underperform relative to SBO (often slower convergence and lower final reward); exact numeric baselines not provided in the paper for this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported qualitatively as more sample-efficient than BO and WARP; converges in fewer trials and to higher/ more repeatable rewards (no exact sample counts provided).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Handled by the funnel kernel together with EI: global kernel encourages exploration across parameter space, adaptive local kernel focuses exploitation on regions with promising episodic returns; the funnel center is updated by MCMC sampling driven by data density near optima.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against stationary GP-based BO and WARP (input warping). SBO results were reported to be comparable or superior to popular RL solvers like SARSA in sample-efficiency (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>SBO produced more efficient learning of walking policies, obtaining higher rewards and faster convergence than baseline BO and WARP in the three-limb walker benchmark; adaptive focusing of model capacity near promising policies was key to better exploitation in the presence of large flat/failure regions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Paper reports no outright failure on this benchmark, but notes general SBO limitations: increased hyperparameter space (3d for Matérn ARD local+global), extra MCMC computational cost, and the method can require careful initialization/initial design to sample non-crashing policies sufficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Funneled Bayesian Optimization for Design, Tuning and Control of Autonomous Systems', 'publication_date_yy_mm': '2016-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1105.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1105.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SBO-Helicopter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spartan Bayesian Optimization (SBO) applied to aerobatic helicopter hovering policy search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active policy-search agent that uses SBO to tune controllers for a high-dimensional, chaotic helicopter simulation (XCell Tempest) to maximize a quadratic reward penalizing state error and control effort while avoiding crashes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Spartan Bayesian Optimization (SBO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A Bayesian optimization approach with a Gaussian process surrogate using the Spartan funnel kernel (local + global Matérn ARD kernels) and Expected Improvement acquisition; hyperparameters (including local kernel center θ_p) are sampled by MCMC, and the funnel adapts toward promising policy regions as more episodic results are observed.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization (active policy search) using SBO (Spartan kernel) + Expected Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>SBO updates the GP posterior with each full-episode helicopter return (Monte Carlo over episodes), samples hyperparameters (including funnel center θ_p) with MCMC so the local kernel concentrates near dense/high-performing policy regions, and selects new policies by maximizing EI—this focuses sampling on safe/performant policies while keeping global exploration to discover non-crashing regions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>XCell Tempest aerobatic helicopter hovering simulator (RL-competition hovering task)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>High-dimensional, chaotic/complex dynamics with potentially stochastic wind perturbations in the simulator; terminal 'crash' states give large negative reward (creating very large flat/penalized areas); reward is quadratic penalizing state error and actions; episode length = 10 s simulated (6000 control steps); simulator learned from real helicopter data (apprenticeship learning).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>State-space dimensionality: 12 (position, orientation, translational & rotational velocities); action-space dimensionality: 4 continuous controls; each episode = 6000 control steps over 10 s; many initial policies lead to immediate crashes producing highly sparse/penalized reward signal; controller search spaces tested: weak baseline linear policy (12 parameters) and other parameterizations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>SBO achieved faster improvement and higher final performance than standard BO and WARP in the helicopter benchmark; SBO was able to exploit non-crashing policies and rapidly improve reward where BO and WARP converged slowly due to many crashing policies. Quantitative runtimes/samples: initial Sobol design of 40 points used to guarantee sampling of some non-crashing policies; exact numeric final rewards per method are shown in paper figures but not reported as explicit tabulated numbers in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Standard BO and WARP exhibited slow convergence and often poor performance because many sampled policies crash early, yielding very little gradient information; other literature methods require many more trials to reach similar performance (citations provided), but exact comparative sample counts are not tabulated in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported qualitatively as substantially more sample-efficient than competing BO variants: SBO exploited promising policies quickly after initial exploration (with an initial Sobol LHS of 40 samples), whereas BO/WARP required many more trials to find stable, high-reward policies. Exact sample-to-performance thresholds are presented in plots but not given as explicit numeric captions in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>SBO's funnel kernel provides tighter bounds near promising policies (local exploitation) while the global kernel maintains exploratory uncertainty elsewhere; EI then trades off mean and variance to select next policies, helping to find non-crashing, high-reward regions in a space dominated by crash-penalized flat regions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against stationary GP BO and WARP (input warping). The paper also qualitatively compares SBO performance to standard RL solvers like SARSA and references methods in the literature that required many more trials for similar tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>On the challenging helicopter hovering task (12D state, 4D actions, chaotic dynamics), SBO discovered and refined non-crashing, high-performing policies in far fewer trials than stationary BO and WARP; the adaptive local kernel that focuses modeling/uncertainty near promising regions enabled rapid exploitation despite many crash-dominated flat reward regions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires a larger initial design (the experiments used a Sobol initial design of 40 points) to ensure sampling of non-crashing policies; SBO increases hyperparameter dimensionality (higher MCMC cost); when most policies crash, initial selection is delicate—SBO relies on the initial design to sample some informative policies; computational cost (MCMC over more hyperparameters) is higher than stationary BO though still lower than WARP in CPU time for many experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Funneled Bayesian Optimization for Design, Tuning and Control of Autonomous Systems', 'publication_date_yy_mm': '2016-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Active policy learning for robot planning and exploration under uncertainty <em>(Rating: 2)</em></li>
                <li>A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning <em>(Rating: 2)</em></li>
                <li>Bayesian optimization for learning gaits under uncertainty <em>(Rating: 2)</em></li>
                <li>Robots that can adapt like animals <em>(Rating: 2)</em></li>
                <li>Practical Bayesian optimization of machine learning algorithms <em>(Rating: 1)</em></li>
                <li>Input warping for Bayesian optimization of non-stationary functions <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1105",
    "paper_id": "paper-5472671",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "SBO-MountainCar",
            "name_full": "Spartan Bayesian Optimization (SBO) applied to Mountain Car policy search",
            "brief_description": "An active policy-search agent that uses Spartan Bayesian Optimization (a GP surrogate with an adaptive local+global 'funnel' kernel) and Expected Improvement acquisition to adaptively select policy parameters for the mountain car task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Spartan Bayesian Optimization (SBO)",
            "agent_description": "A Bayesian optimization agent using a Gaussian process surrogate with a Spartan (funnel) kernel: a weighted linear combination of a global and a local Matérn kernel (both ARD), with the local kernel center (θ_p) and other hyperparameters inferred by MCMC; next-policy selection is performed by maximizing Expected Improvement (EI) over policy parameter space.",
            "adaptive_design_method": "Bayesian optimization (active policy search) using an adaptive Spartan kernel + Expected Improvement",
            "adaptation_strategy_description": "The agent maintains a GP posterior (mixture over MCMC hyperparameter samples). The Spartan kernel combines a broad global kernel and an adaptive local kernel whose center θ_p is treated as a hyperparameter and sampled with MCMC; as data accumulates, the posterior over θ_p shifts toward densely sampled / high-performing regions (funnel moves), and EI is maximized to choose the next policy. Thus adaptation uses all past observations (GP posterior) and hyperparameter MCMC samples to focus sampling near promising optima while retaining global exploration.",
            "environment_name": "Mountain Car (policy search)",
            "environment_characteristics": "Unknown dynamics (simulator-based evaluations); reward landscape with large flat/penalized regions and small informative regions (heteroscedastic / nonstationary w.r.t. policy), many policies yield near-zero or very low reward (failure/flat zones); continuous state and action representations used in the policy parameterization; evaluations are episodic (Monte Carlo returns).",
            "environment_complexity": "Low-to-moderate policy dimensionality: 7 policy parameters for the perceptron policy used; episode length not explicitly quantified for mountain car in paper; continuous state (position, velocity) and continuous action (throttle) spaces; stochasticity not emphasized (simulator deterministic in typical mountain car).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "SBO achieved maximum performance in all trials after ~27 policy trials (reported as 17 BO iterations + 10 initial Latin-hypercube samples).",
            "performance_without_adaptation": "Baselines (standard stationary BO and WARP input-warping BO) converged more slowly or got stuck in local maxima; quantitative baseline numbers not provided for mountain car beyond qualitative comparison.",
            "sample_efficiency": "High: reached maximum performance reliably within 27 policy evaluations (including 10 initial samples), demonstrating very few-episode sample efficiency compared to baselines.",
            "exploration_exploitation_tradeoff": "Managed by the Spartan kernel + EI: the global kernel maintains coarse long-range uncertainty (encouraging exploration), while the adaptive local kernel produces tight bounds and higher variance near inferred promising regions (encouraging fast local exploitation); EI balances mean and variance to pick next queries.",
            "comparison_methods": "Compared to standard Gaussian-process BO (stationary Matérn ν=5/2 ARD) and WARP (input warping BO); results are also discussed relative to classical RL solver SARSA (qualitatively).",
            "key_results": "SBO markedly improved sample efficiency on mountain car policy search, reaching maximal performance in all trials with ~27 policy evaluations while standard BO and WARP were slower or got stuck; the adaptive funnel kernel focuses modeling capacity near optima while preserving global exploration.",
            "limitations_or_failures": "No explicit failure on this task reported, but general SBO caveats apply: larger hyperparameter dimensionality (more MCMC cost) and potential transient suboptimal early behavior if the local kernel has not yet moved to a promising region.",
            "uuid": "e1105.0",
            "source_info": {
                "paper_title": "Funneled Bayesian Optimization for Design, Tuning and Control of Autonomous Systems",
                "publication_date_yy_mm": "2016-10"
            }
        },
        {
            "name_short": "SBO-Walker",
            "name_full": "Spartan Bayesian Optimization (SBO) applied to three-limb walker policy search",
            "brief_description": "An active policy-search agent using SBO to tune an 8-parameter controller for a biped/three-limb walker to maximize walking speed while avoiding falls.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Spartan Bayesian Optimization (SBO)",
            "agent_description": "Bayesian optimization agent with a Gaussian process surrogate using the Spartan (funnel) kernel (local + global Matérn kernels, ARD), hyperparameters sampled via MCMC; acquisition is Expected Improvement over policy parameters; used to directly optimize episodic return of a parameterized walking controller.",
            "adaptive_design_method": "Bayesian optimization (active policy search) using an adaptive Spartan kernel + Expected Improvement",
            "adaptation_strategy_description": "Adaptation proceeds by updating GP posterior with each episodic return; MCMC samples of kernel hyperparameters (including θ_p centers for local kernel) cause the local funnel to migrate toward regions with dense/high-performing samples; next policy selected by maximizing EI, thereby concentrating samples near promising policy regions while retaining exploration via the global kernel.",
            "environment_name": "Three-limb walker (Westervelt-style walker)",
            "environment_characteristics": "Unknown dynamics for the walker; episodic controller evaluation with penalty for falling (failure states) producing large negative/flat reward regions; reward is walking speed with penalties for losing upright posture; heteroscedastic and nonstationary reward surface w.r.t. controller parameters.",
            "environment_complexity": "Controller parameter dimensionality: 8 continuous parameters; state space of simulated walker (not fully enumerated in paper) but dynamics are nonlinear and can produce catastrophic failures (falls). Episodes are finite horizon; Monte Carlo used to estimate expected return.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "SBO achieved higher reward and improved sample efficiency compared to standard BO and WARP across experiments (qualitative improvement reported; specific numeric reward/time series not tabulated in the text).",
            "performance_without_adaptation": "Standard BO and WARP underperform relative to SBO (often slower convergence and lower final reward); exact numeric baselines not provided in the paper for this benchmark.",
            "sample_efficiency": "Reported qualitatively as more sample-efficient than BO and WARP; converges in fewer trials and to higher/ more repeatable rewards (no exact sample counts provided).",
            "exploration_exploitation_tradeoff": "Handled by the funnel kernel together with EI: global kernel encourages exploration across parameter space, adaptive local kernel focuses exploitation on regions with promising episodic returns; the funnel center is updated by MCMC sampling driven by data density near optima.",
            "comparison_methods": "Compared against stationary GP-based BO and WARP (input warping). SBO results were reported to be comparable or superior to popular RL solvers like SARSA in sample-efficiency (qualitative).",
            "key_results": "SBO produced more efficient learning of walking policies, obtaining higher rewards and faster convergence than baseline BO and WARP in the three-limb walker benchmark; adaptive focusing of model capacity near promising policies was key to better exploitation in the presence of large flat/failure regions.",
            "limitations_or_failures": "Paper reports no outright failure on this benchmark, but notes general SBO limitations: increased hyperparameter space (3d for Matérn ARD local+global), extra MCMC computational cost, and the method can require careful initialization/initial design to sample non-crashing policies sufficiently.",
            "uuid": "e1105.1",
            "source_info": {
                "paper_title": "Funneled Bayesian Optimization for Design, Tuning and Control of Autonomous Systems",
                "publication_date_yy_mm": "2016-10"
            }
        },
        {
            "name_short": "SBO-Helicopter",
            "name_full": "Spartan Bayesian Optimization (SBO) applied to aerobatic helicopter hovering policy search",
            "brief_description": "An active policy-search agent that uses SBO to tune controllers for a high-dimensional, chaotic helicopter simulation (XCell Tempest) to maximize a quadratic reward penalizing state error and control effort while avoiding crashes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Spartan Bayesian Optimization (SBO)",
            "agent_description": "A Bayesian optimization approach with a Gaussian process surrogate using the Spartan funnel kernel (local + global Matérn ARD kernels) and Expected Improvement acquisition; hyperparameters (including local kernel center θ_p) are sampled by MCMC, and the funnel adapts toward promising policy regions as more episodic results are observed.",
            "adaptive_design_method": "Bayesian optimization (active policy search) using SBO (Spartan kernel) + Expected Improvement",
            "adaptation_strategy_description": "SBO updates the GP posterior with each full-episode helicopter return (Monte Carlo over episodes), samples hyperparameters (including funnel center θ_p) with MCMC so the local kernel concentrates near dense/high-performing policy regions, and selects new policies by maximizing EI—this focuses sampling on safe/performant policies while keeping global exploration to discover non-crashing regions.",
            "environment_name": "XCell Tempest aerobatic helicopter hovering simulator (RL-competition hovering task)",
            "environment_characteristics": "High-dimensional, chaotic/complex dynamics with potentially stochastic wind perturbations in the simulator; terminal 'crash' states give large negative reward (creating very large flat/penalized areas); reward is quadratic penalizing state error and actions; episode length = 10 s simulated (6000 control steps); simulator learned from real helicopter data (apprenticeship learning).",
            "environment_complexity": "State-space dimensionality: 12 (position, orientation, translational & rotational velocities); action-space dimensionality: 4 continuous controls; each episode = 6000 control steps over 10 s; many initial policies lead to immediate crashes producing highly sparse/penalized reward signal; controller search spaces tested: weak baseline linear policy (12 parameters) and other parameterizations.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "SBO achieved faster improvement and higher final performance than standard BO and WARP in the helicopter benchmark; SBO was able to exploit non-crashing policies and rapidly improve reward where BO and WARP converged slowly due to many crashing policies. Quantitative runtimes/samples: initial Sobol design of 40 points used to guarantee sampling of some non-crashing policies; exact numeric final rewards per method are shown in paper figures but not reported as explicit tabulated numbers in text.",
            "performance_without_adaptation": "Standard BO and WARP exhibited slow convergence and often poor performance because many sampled policies crash early, yielding very little gradient information; other literature methods require many more trials to reach similar performance (citations provided), but exact comparative sample counts are not tabulated in-text.",
            "sample_efficiency": "Reported qualitatively as substantially more sample-efficient than competing BO variants: SBO exploited promising policies quickly after initial exploration (with an initial Sobol LHS of 40 samples), whereas BO/WARP required many more trials to find stable, high-reward policies. Exact sample-to-performance thresholds are presented in plots but not given as explicit numeric captions in the text.",
            "exploration_exploitation_tradeoff": "SBO's funnel kernel provides tighter bounds near promising policies (local exploitation) while the global kernel maintains exploratory uncertainty elsewhere; EI then trades off mean and variance to select next policies, helping to find non-crashing, high-reward regions in a space dominated by crash-penalized flat regions.",
            "comparison_methods": "Compared against stationary GP BO and WARP (input warping). The paper also qualitatively compares SBO performance to standard RL solvers like SARSA and references methods in the literature that required many more trials for similar tasks.",
            "key_results": "On the challenging helicopter hovering task (12D state, 4D actions, chaotic dynamics), SBO discovered and refined non-crashing, high-performing policies in far fewer trials than stationary BO and WARP; the adaptive local kernel that focuses modeling/uncertainty near promising regions enabled rapid exploitation despite many crash-dominated flat reward regions.",
            "limitations_or_failures": "Requires a larger initial design (the experiments used a Sobol initial design of 40 points) to ensure sampling of non-crashing policies; SBO increases hyperparameter dimensionality (higher MCMC cost); when most policies crash, initial selection is delicate—SBO relies on the initial design to sample some informative policies; computational cost (MCMC over more hyperparameters) is higher than stationary BO though still lower than WARP in CPU time for many experiments.",
            "uuid": "e1105.2",
            "source_info": {
                "paper_title": "Funneled Bayesian Optimization for Design, Tuning and Control of Autonomous Systems",
                "publication_date_yy_mm": "2016-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Active policy learning for robot planning and exploration under uncertainty",
            "rating": 2,
            "sanitized_title": "active_policy_learning_for_robot_planning_and_exploration_under_uncertainty"
        },
        {
            "paper_title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning",
            "rating": 2,
            "sanitized_title": "a_tutorial_on_bayesian_optimization_of_expensive_cost_functions_with_application_to_active_user_modeling_and_hierarchical_reinforcement_learning"
        },
        {
            "paper_title": "Bayesian optimization for learning gaits under uncertainty",
            "rating": 2,
            "sanitized_title": "bayesian_optimization_for_learning_gaits_under_uncertainty"
        },
        {
            "paper_title": "Robots that can adapt like animals",
            "rating": 2,
            "sanitized_title": "robots_that_can_adapt_like_animals"
        },
        {
            "paper_title": "Practical Bayesian optimization of machine learning algorithms",
            "rating": 1,
            "sanitized_title": "practical_bayesian_optimization_of_machine_learning_algorithms"
        },
        {
            "paper_title": "Input warping for Bayesian optimization of non-stationary functions",
            "rating": 2,
            "sanitized_title": "input_warping_for_bayesian_optimization_of_nonstationary_functions"
        }
    ],
    "cost": 0.01306225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Funneled Bayesian Optimization for Design, Tuning and Control of Autonomous Systems
2 Oct 2016</p>
<p>Ruben Martinez-Cantin rmcantin@unizar.es 
Centro Universitario de la Defensa
ZaragozaSpain</p>
<p>Funneled Bayesian Optimization for Design, Tuning and Control of Autonomous Systems
2 Oct 20165A023F24FCFA8EDCE3228E132031D953arXiv:1610.00366v1[cs.AI]
Bayesian optimization has become a fundamental global optimization algorithm in many problems where sample efficiency is of paramount importance.Recently, there has been proposed a large number of new applications in fields such as robotics, machine learning, experimental design, simulation, etc.In this paper, we focus on several problems that appear in robotics and autonomous systems: algorithm tuning, automatic control and intelligent design.All those problems can be mapped to global optimization problems.However, they become hard optimization problems.Bayesian optimization internally uses a probabilistic surrogate model to learn from the process and reduce the number of samples required.The de facto standard surrogate model is a Gaussian process due to its flexibility to represent a distribution over functions.In order to generalize to unknown functions in a black-box fashion, the common assumption is that the underlying function can be modeled with a stationary process.Nonstationary Gaussian process regression cannot generalize easily and it typically requires prior knowledge of the function.Some works have designed techniques to generalize Bayesian optimization to nonstationary functions in an indirect way, but using techniques originally designed for regression, where the objective is to improve the quality of the surrogate model everywhere.Instead optimization should focus on improving the surrogate model near the optimum.In this paper, we present a novel kernel function specially designed for Bayesian optimization, that allows nonstationary behavior of the surrogate model in an adaptive local region.In our experiments, we found that this new kernel results in an improved local search (exploitation), without penalizing the global search (exploration).We provide extensive results in well-known benchmarks and real applications to show that the new method is able to outperform the state of the art in Bayesian optimization both in stationary and nonstationary problems.</p>
<p>Introduction</p>
<p>Many problems in engineering, computer science, economics, etc., require to find the extremum of an unknown real valued function usign as few evaluations as possible.In many cases, those functions represent actual industrial processes, expensive trials or time consuming computations or simulations.The optimization process must consider the actual budget and limitations of gathering new evaluations.Then, sample efficiency becomes the key element.Furthermore, those functions might be highly multimodal, requiring a global solution.</p>
<p>Bayesian optimization, also found in the literature with the names of Bayesian Sampling [56], Efficient Global Optimization (EGO) [31], Sequential Kriging Optimization (SKO) [27], Sequential Model-Based Optimization (SMBO) [29] or Bayesian guided pattern search [59], is a classic optimization method [37,43] which has become quite popular recently for being a sample efficient method of global optimization [31].It has been applied with great success to autonomous algorithm tuning [39], specially for machine learning applications [52,16], robot planning [41], control [10], task optimization [36], reinforcement learning [42,13], structural design [17], sensor networks [55,20], simulation design [7], circuit design [59], ecology [63], biochemistry [12], dynamical modeling of biological systems [61], etc.Recent works have found connections between Bayesian optimization and the way biological systems adapt and search in nature, such as human active search [5] or animal adaptation to injuries [11].</p>
<p>Bayesian optimization uses a Bayesian surrogate model, that is, a distribution over target functions P (f ) to incorporate all available information during the optimization procedure.This distribution can capture both the prior information available and all the information from each observation.Therefore, the surrogate model provides a memory [45] that improves the sample efficiency of the method by considering the whole history of trials and evaluations during the optimization procedure.Besides, this model is sample efficient by using two principles.</p>
<p>First, it can be updated recursively as outcomes are available from the evaluated trials y i = f (x i ) P (f |X 1:i , y 1:i ) = P (x i , y i |f )P (f |X 1:i−1 , y 1:i−1 ) P (x i , y i ) ,</p>
<p>∀ i = 2 . . .n where X 1:i is a matrix with all the inputs {x j } i j=1 and y 1:i is a vector with all the outcomes {y j } i j=1 .By using this method, the information is always updated 1 .Second, it allows to compute the optimal decision/action a of selecting the next trial x n+1 by maximizing (minimizing) the expected utility (loss):
a BO = arg min a δ n (f, a) dP (f |x 1:n , y 1:n ) (2)
where δ n (f, a) is the optimality criterion or regret function that drives the optimization towards the optimum x * .For example, we can use the optimality gap δ n (f, a) = f (x n ) − f (x * ) to get the optimal outcome, the Euclidean distance error δ n (f, a) = x n − x * 2 to get the optimal input, or the relative entropy δ n (f, a) = H(x * |x 1:n−1 ) − H(x * |x 1:n ) to maximize the information about the optimum.</p>
<p>In summary, Bayesian optimization is the combination of two main components: a surrogate model which captures all prior and observed information and a decision process which performs the optimal action, i.e.: where to sample next, based on the previous model.This methodology in two steps of modeling and decision is connected to many fields.In the way points are selected, the optimization problem can be considered an active learning problem on the estimation of the optimum [25,14].Other authors have drawn connections between Bayesian optimization and some reinforcement learning setups such as multi-armed bandits [55], POMDPs [60] or even active reinforcement learning [42].These two components also hide extra computational cost.On one hand, we need to update the surrogate model and, on the other hand, optimize the criterion function However, this additional cost can be compensated by the reduced number of target function evaluations thanks to the sample efficiency of the method.Therefore, Bayesian optimization is specially suitable for expensive black-box functions and trial-anderror methodologies.</p>
<p>Under these conditions, the quality of the surrogate model is of paramount importance as it also affects to the optimality of the decision process.Earliest versions of Bayesian optimization used Wiener processes [37] or Wiener fields [44] as surrogate models.Similar methods used radial basis functions [24] or branch and bound with polynomials [51].It was the seminal paper of Jones et al. [31] that introduced the use of Gaussian processes, also called Kriging, as a Bayesian surrogate function.Jones also wrote an excellent review on this kind of surrogate models [30].Recently, other Bayesian models have become popular like Student's t processes [50,40], treed Gaussian processes [59,3], random forests [29], tree-structured Parzen estimators [4] or deep neural networks [53].In the case of discrete inputs, the Beta-Bernouilli bandit setting provides an equivalent framework [48].</p>
<p>However, the Gaussian process (GP) is still the most popular model due to its accuracy, robustness and flexibility, because Bayesian optimization is mainly used in black or grey-box scenarios.The range of applicability of a Gaussian process is defined by its kernel function, which sets the family of functions that is able to represent through the reproducing kernel Hilbert space (RKHS) [47].In fact, regret bounds for Bayesian optimization using Gaussian processes are always defined in terms of specific kernel functions [14,55,9].From a practical point of view, the standard procedure is to select a generic kernel function, such as the Gaussian (square exponential) or Matérn kernels, and estimate the kernel hyperparameters from data.One property of these kernels is that they are stationary.Although it might be a reasonable assumption in a black box setup, we show in Section 3 that this reduces the efficiency of Bayesian optimization in most situations.It also limits the potential range of applications.Furthermore, nonstationay methods usually require extra knowledge of the function (e.g.: the global trend or the space partition).Being global properties, gathering this knowledge from data requires global sampling, which is contrary to the Bayesian optimization methodology.</p>
<p>The main contribution of the paper is a new set of adaptive kernels for Gaussian processes that are specifically designed to model functions from nonstationary processes but focused on the region near the optimum.Thus, the new model maintains the global exploration/local exploitation trade off.This idea results in an improved efficiency and applicability of any Bayesian optimization based on Gaussian processes.We call this new method Spartan Bayesian Optimization (SBO).The algorithm has been extensively evaluated in many scenarios and applications.Besides some standard optimization benchmarks, the method has been evaluated in automatic algorithm tuning for machine learning applications, optimal policy learning in reinforcement learning scenarios and autonomous wing design of an airplane using realistic CFD simulations.In our results, we have found that SBO reaches its best performance in problems that are clearly nonstationary, where the local and global shape of the function are different.However, our tests have also shown that SBO can improve the results of Bayesian optimization in all those scenarios.From an optimization point of view, these new kernels result in an improved local search while maintaining global exploration capabilities, similar to other locally-biased global optimization algorithms [18].</p>
<p>The algorithm is called Spartan as it follows the same intuition and analogous strategy as the Greek forces at the end of the Battle of Thermopylae.The most likely theory claims that, the last day of the battle, a small group of forces led by spartan King Leonidas stood in the narrow pass of the Thermopylae to block the Persian cavalry, while the rest of the forces retreated to cover more terrain and avoid being surrounded by the Persian moving through a mountain path [38].This dual strategy of allocate global resources sparsely while maintaining a local dense vanguard at a strategic location is emphasized within Spartan Bayesian Optimization.</p>
<p>Bayesian optimization with Gaussian processes</p>
<p>We start describing the ingredients for a Bayesian optimization algorithm using Gaussian processes as surrogate model.Consider the problem of finding the minimum of an unknown real valued function f : X → R, where X is a compact space, X ⊂ R d , d ≥ 1.In order to find the minimum, the algorithm has a maximum budget of N evaluations of the target function f .The purpose of the algorithm is to select the best query points at each iteration such as the optimization gap or regret is minimum for the available budget.</p>
<p>Without loss of generality, for the remainder of the paper we are going to assume that the surrogate model P (f ) is a Gaussian process GP(x|µ, σ 2 , θ) with inputs x ∈ X, scalar outputs y ∈ R and an associate kernel or covariance function k(•, •) with hyperparameters θ.The hyperparameters are estimated using Monte Carlo Markov Chain (MCMC) resulting in m samples Θ = {θ i } m i=1 .Concretely, we use the slice sampling algorithm which has already been used in Bayesian optimization [52], although bootstraping could equally be used [33].One advantage of using Gaussian processes as a distributions over functions is that new observations of the target function (x i , y i ) can be easily used to update the distribution over functions.Furthermore, the posterior distribution is also a GP.Therefore, the posterior can be used as a prior for the next iteration in a recursive algorithm.In fact, being a non-parametric method, the algorithm of adding a new sample can be highly optimized provided that we do not update the hyperparameters every iteration [40].</p>
<p>Given a dataset at step n of query points X = {x 1:n } and its respective outcomes y = {y 1:n }, then the prediction of the Gaussian process at a new query point x q , with kernel k i conditioned on the i-th
hyperparameter sample k i = k(•, •|θ i ) is a normal distribution such as y q ∼ m i=1 N (µ i , σ 2 i |x q ) where: µ i (x q ) = k i (x q , X)K i (X, X) −1 y σ 2 i (x q ) = k i (x q , x q ) − k i (x q , X)K i (X, X) −1 k i (X, x q )(3)
being k i (x q , X) the corresponding cross-correlation vector of the query point x q with respect to the dataset X k i (x q , X) = [k i (x q , x 1 ), . . ., k i (x q , x n )]</p>
<p>T and K i (X, X) is the Gram matrix corresponding to kernel k i for the dataset
X K i (X, X) =    k i (x 1 , x 1 ) . . . k i (x 1 , x n ) . . . . . . . . . k i (x n , x 1 ) . . . k i (x n , x n )    + σ 2 n I
where σ 2 n is a noise or nugget term to represent stochastic functions [27] or surrogate missmodeling [23].Note that, because we use a sampling distribution of θ the predictive distribution at any point x is a mixture of Gaussians.</p>
<p>The problem with the regret functions that we presented for equation ( 2) is that all of them rely on the knowledge of the optimum x * .Thus, we cannot use any of those directly.Instead, the Bayesian optimization literature have developed proxies of those functions, called acquisition functions.To select the next point at each iteration, we use the expected improvement criterion [43] as a proxy of the optimality gap criterion.The expected improvement is defined as the expectation of the improvement function I(x) = max(0, ρ − f (x)).The improvement is defined over a incumbent target ρ, which in many applications is considered to be the best outcome until that iteration ρ = y best .Other incumbent values could be considered, specially in the presence of noise.Taking the expectation over the mixture of Gaussians of the predictive distribution, we can compute the expected improvement as:
EI(x) = E p(y|x,θ) [max(0, ρ − f (x))] = m i=1 <a href="4">(ρ − µ i ) Φ(z i ) + σ i φ(z i )</a>
where φ and Φ are the corresponding Gaussian probability density function (PDF) and cumulative density function (CDF), being z i = (ρ − µ i )/σ i .In this case, (µ i , σ 2 i ) is the prediction computed with Equation (3).</p>
<p>Assuming that the incumbent target is the best observation up to iteration n − 1, then at iteration n, we select the next query at the point that maximizes the corresponding expected improvement:
x n = arg max x m i=1 <a href="5">(y best − µ i ) Φ(z i ) + σ i φ(z i )</a>
Finally, in order to avoid bias and guarantee global optimality, we rely on an initial design of p points based on Latin Hypercube Sampling (LHS) following the recommendation in the literature [31,9,32].Algorithm 1 summarizes the basic steps in Bayesian optimization.</p>
<p>Algorithm 1 Bayesian optimization (BO)</p>
<p>1: Initial design of p points using any prior information (e.g.: LHS to maximize coverage) 2: X ← x 1:p y ← y 1:p 3: for n = p . . .N do Available budget of N queries 4:</p>
<p>for i = 1 . . .m do Iterate over MCMC samples 5:
µ i (x) ← k(x, X|θ i )K(X, X|θ i ) −1 y Predicted mean 6: σ 2 i (x) ← k(x, x|θ i ) − k(x, X|θ i )K(X, X|θ i ) −1 k(X, x|θ i ) Predicted variance 7:
end for 8:
Θ ← {θ i } m i=1
Update the hyperparameters using MCMC 9:</p>
<p>x n = arg max x EI(x) Expected improvement, see (5) 10:
y n ← f (x n ) X ← add(x n ) y ← add(y n ) 11: end for</p>
<p>Nonstationary Gaussian processes</p>
<p>Many applications of Gaussian process regression, including Bayesian optimization, are based on the assumption that the process is stationary.This is a reasonable assumption for black-box optimization as it does not assume any extra information on the evolution of the function in the space.For example, the use of the squared exponential (SE) kernel in GPs is quite frequent.Another popular kernel in GP regression is the Matérn kernel family:
k SE (x, x ) = exp − 1 2 r 2(6)k M atern (x, x ) = 2 1−ν Γ(ν) √ 2νr ν K v √ 2νr(7)
where r 2 = (x − x ) T Λ(x − x )) with Λ = θ −1 l I.For these kernels, θ l represents the length-scale hyperparameter that captures the smoothness or variability of the function.In the Matérn kernel, K ν is a modified Bessel function.The Matérn kernel is usually computed for values of ν that are half-integers ν = p + 1/2 where p is a non-negative integer, because the function becomes simpler.
k M,ν=1/2 (x, x ) = exp (−r) (8) k M,ν=3/2 (x, x ) = exp − √ 3r 1 + √ 3r (9) k M,ν=5/2 (x, x ) = exp − √ 5r 1 + √ 5r + 5 3 r 2(10)
The value of ν is related to the smoothness of the functions as it directly affects the kth-differentiability of the underlying process.The process g(x) defined by the Matérn kernel k(•, •) is k-times mean square differentiable if and only if k &gt; ν [47].Furthermore, for ν → ∞, we obtain the SE kernel from equation (6).</p>
<p>The analysis of the kernel length-scale is very important for optimization.Small values of θ l will be more suitable to capture signals with high frequency components; while large values of θ l result in a model for low frequency signals or flat functions.This property also holds for kernels with automatic relevance determination (ARD) [47], where θ l becomes a vector with a length-scale parameter per dimension.In Bayesian optimization, the Matérn kernel with ν = 5/2 and ARD is frequently used for its performance in many benchmarks.</p>
<p>This length-scale estimation results in an interesting behavior in Bayesian optimization.For the same distance between points, a kernel with smaller length-scale will result in higher predictive variance, therefore the exploration will be more aggressive.This idea was previously explored in Wang et al. [62] by forcing smaller scale parameters to improve the exploration.More formally, in order to achieve no-regret convergence to the minimum, the target function must be an element of the reproducing kernel Hilbert space (RKHS) characterized by the kernel k(•, •) [9,55].For a set of kernels like the SE or Matérn, it can be shown that: Proposition 1.Given two kernels k l and k s with large and small length scale hyperparameters respectively, any function f in the RKHS characterized by a kernel k l is also an element of the RKHS characterized by k s [62].</p>
<p>Thus, using k s instead of k l is safer in terms of guaranteeing convergence.However, if the small kernel is used everywhere, it might result in unnecessary sampling of smooth areas.</p>
<p>Definition 1.Let f : R d → R be a function and H k be the reproducing kernel Hilbert space generated by kernel k(•, •).
• We say that a function f (x) is stationary if ∃ k(x, x ) = k(τ ) where τ = x − x and f ∈ H k . • In contrast, we say that a function f (x) is nonstationary if k(x, x ) = k(τ ) where τ = x − x and f ∈ H k .
• Finally, we say that a function f (x) is local stationary if there is a subset X ⊂ R d so that the function is stationary ∀x ∈ X and nonstationary ∀x ∈ R d \ X .</p>
<p>According to the previous definition, most applications of Bayesian optimization are nonstationary or local stationary.Take for example the reinforcement learning problems of Section 5.4.In that scenario, an agent A is trying to find the optimal policy π * which produce the behavior that maximize a reward function R:</p>
<p>Example 1.Let us consider a biped robot (agent) trying the get the walking pattern (policy) that maximizes the walking speed (reward).In this setup, there are some policies that reach undesirable states or result in a failure condition, like the robot falling or losing the upright posture.Then, the system returns a null reward or arbitrary penalty.In cases where finding a stable policy is difficult, the reward function may end up being almost flat, except for a small region of successful policies where the reward is actually informative in order to maximize the speed.</p>
<p>Modeling these kind of functions with Gaussian processes require kernels with different length scales for the flat/non-flat regions or specially designed kernels to capture that behavior.</p>
<p>Furthermore, Bayesian optimization is inherently a local stationary process depending on the acquisition function.It has a dual behavior of global exploration and local exploitation.Ideally, both samples and uncertainty estimation end up being distributed unevenly, with many samples and small uncertainty near the local optima and sparse samples and large uncertainty everywhere else.</p>
<p>There has been several attempts to model nonstationary functions with Gaussian processes.For example, the use of specific nonstationary kernels [47], Bayesian treed GP models [22] or projecting (warping) the input space to a stationary latent space [49].The idea of treed GPs was used in Bayesian optimization combined with an auxiliary local optimizer [59].A version of the warping idea was applied to Bayesian optimization [54].Later, Assael et al. [3] built a treed GPs where the warping model was used in the leaves.These methods try to model the nonstationary property in a global way.However, as pointed out before, sampling in Bayesian optimization is uneven, thus the global model might end up being inaccurate.</p>
<p>Like many global optimization and bandit setups, Bayesian optimization requires to control the bounds of the function to drive exploration efficiently.As pointed out in Bubeck et al. [8], tight bounds are required only near its optimum for these setups.Then, milder conditions can be defined elsewhere.For example, they use the "weak Lipschitz" condition as a milder version of the Lipschitz condition.In probabilistic terms, the upper and lower bounds defined by the Gaussian process in Bayesian optimization play the same role as the Lipschitz constant in classical optimization [14].That is, high uncertainty represents loose bounds and viceversa.Our proposal exploits this idea by providing an adaptive kernel which allows tight bounds near the optimum and looser bounds everywhere else.As new information of the optimum location is available, the tight bounds are moved towards its location.</p>
<p>Spartan Bayesian Optimization</p>
<p>Our approach to nonstationarity is based on the model presented in Krause &amp; Guestrin [35] where the input space is partitioned in different regions such as the resulting GP is the linear combination of local GPs: ξ(x) = j λ j (x)ξ j (x).Each local GP has its own specific hyperparameters, making the final GP nonstationary even when the local GPs are stationary.In order to achieve smooth interpolation between regions, the authors suggest the use of a weighting function ω j (x) for each region, having the maximum in region j and decreasing its value with distance to region j [35].Then, we can set λ j (x) = ω j (x)/ p ω p (x).In practice, the mixed GP can be obtained by a combined kernel function of the form: k(x, x |θ) = j λ j (x)λ j (x )k j (x, x |θ).A related approach of additive GPs was used by Kandasamy et al. [32] for Bayesian optimization of high dimensional functions under the assumption that the actual function is a combination of lower dimensional functions.</p>
<p>For Bayesian optimization, we propose the combination of a local and a global kernels and with multivariate normal distributions as weighting functions.We have called this kernel, the Spartan kernel:
k(x, x |θ S ) = λ (g) (x)λ (g) (x )k (g) (x, x |θ g ) + M m=1 λ (l) m (x|θ p )λ (l) m (x |θ p )k (l) (x, x |θ l m )(11)
where the normalized local weights λ (l) m (x|θ p ) includes parameters to move the influence region.The unnormalized weights ω = are defined as:
ω (g) = N ψ, Iσ 2 g ω (l) m = N θ p , Iσ 2 m ∀ m = 1 . . . M (12)
where ψ and θ p can be seen as the centers of the influence region of each kernel while σ 2 g and σ 2 m represents the area of influence.Note that all the local kernels share the same mean value (center), but different variance (area).Thus, it generates a funnel-like structure.The Spartan kernel with a single local kernel is shown in Figure 1.In this case, the kernel is just a combination of a single local and a global kernel.Typically, the local and global kernels have a small and large length-scale respectively.The influence of each kernel is represented by the normalized weight at the bottom of the plot.Note how the kernel with small length-scale produce larger uncertainties which is an advantage for fast exploitation, but it can perform poorly for global exploration as it tends to sample equally everywhere.On the other hand, the kernel with large length-scale provides a better global estimate, but with is smoother with higher uncertainty which improves global exploration.</p>
<p>Global kernel parameters Unless we have prior knowledge of the function, the parameters of the global kernel are mostly irrelevant.In most applications, we can use a uniform distribution, which can be easily approximated with a large σ 2 g .For example, assuming a normalized input space X = [0, 1] d , we can set ψ = [0.5]d and σ 2 g = 10.</p>
<p>Local kernel parameters For the local kernels, we estimate the center of the funnel structure θ p based on the data gathered.We propose to consider θ p as part of the hyperparameters for the Spartan kernel, that also includes the parameters of the local and global kernels, that is,
θ S = [θ g , θ l 1 , . . . , θ l M , θ p ]
The area of influence of each local kernel could also be adapted including the terms σ 2 m ∀ m = 1 . . .M as part of the kernel hyperparameters θ S .However, in that case the problem becomes ill-posed, resulting in overfitting.Instead of adding regularization terms, we found simpler to fix the value of σ 2 m or define it in terms of numbers of samples inside.The second method has the advantage that, while doing exploitation, as the number of local samples increase, the funnel gets narrower, allowing better local refinement.</p>
<p>As commented in Section 2, when new data is available, all the parameters are updated using MCMC.Therefore, the position of the local kernel θ p is moved each iteration to represent the posterior.Due to the sampling behavior in Bayesian optimization, we found that it intrinsically moves more likely towards the more densely sampled areas in many problems, which corresponds to the location of the function minima.Furthermore, as we have m MCMC samples, there are m different positions for the local kernel Θ p = {θ p i } m i=1 .Algorithm 2 modifies algorithm 1 for Spartan Bayesian Optimization.As can be seen in Algorithms 2, Spartan Bayesian Optimization is simple to implement once we have an implementation of Algorithm 1.It is important to note that, although we have implemented SBO relying on Gaussian processes and expected improvement, the funnel kernel also works with other popular models such as Student-t processes [65,46], treed Gaussian processes [22] and other criteria such as upper confidence bound [55], relative entropy [26,25], etc.In summary, it requires to modify the kernel function and hyperparameter estimation.However, as we will see in Section 5, the results show a large gain in terms of convergence speed and sample efficiency for many problems.The intuition behind SBO is the same of many acquisition functions in Bayesian optimization: the aim of the surrogate model is not to approximate the target function precisely in every point, but to provide information about the Algorithm 2 Spartan Bayesian Optimization (SBO) 1: Initial design of p points using any prior information (e.g.: LHS to maximize coverage) 2: X ← x 1:p y ← y 1:p 3: for n = p . . .N do Available budget of N queries 4:</p>
<p>for i = 1 . . .m do Iterate over MCMC samples
5: k(x, x |θ i ) ← λ l (x|θ pos i )λ l (x |θ pos i )k l (x, x |θ l i ) + λ g (x)λ g (x )k g (x, x |θ g i ) 6: µ i (x) ← k(x, X|θ i )K(X, X|θ i ) −1 y Predicted mean 7: σ 2 i (x) ← k(x, x|θ i ) − k(x, X|θ i )K(X, X|θ i ) −1 k(X, x|θ i ) Predicted variance 8:
end for
9: Θ ← {θ g i , θ l i , θ p i } m i=1
Update the hyperparameters using MCMC 10:</p>
<p>x n = arg max x EI(x) Expected improvement, see (5) 11: location of the minimum.Many optimization problems are difficult due to the fact that the region near the minimum is heteroscedastic, i.e.: it has higher variability than the rest of the space, like the function in Figure 2. In this case, SBO greatly improves the performance of the state of the art in Bayesian optimization.
y n ← f (x n ) X ← add(x n ) y ← add(y n ) 12: end for</p>
<p>Evaluation and results</p>
<p>We have selected a variety of benchmarks from different applications in robotics to test our method.The purpose is twofold.First, we highly the potential applicability of Bayesian optimization in many ways in robotics, from design, control, software tuning, etc.Second, we show that in all those setups, our method outperforms the state of the art in Bayesian optimization.</p>
<ol>
<li>
<p>We have taken several well-known functions for testing global optimization algorithms.The set of functions includes both stationary and non-stationary problems.</p>
</li>
<li>
<p>For the algorithm tuning and perception problems, we have selected a set of machine learning tuning benchmarks.They include both image classification and large clustering problems with different algorithms.</p>
</li>
<li>
<p>For the control/reinforcement learning problems, we have selected some classic benchmarks and a highly complex benchmark to control a hovering aerobatic helicopter.</p>
</li>
<li>
<p>Finally, we address the intelligent design and embodiment problem with the optimal design of a wing profile for a UAV.This is a highly complex scenario, due to the chaotic nature of fluid dynamics.Thus, this problem is ubiquitous in global optimization and evolutionary computation literature.</p>
</li>
</ol>
<p>Implementation details</p>
<p>For evaluation purposes and to highlight the robustness of SBO, we have simplified the funnel structure to a single local and global kernel as in Figure 1.We also took the simpler approach to fix the variance of the ω l .We found that a single value of σ2 l = 0.05 was robust enough in all the experiments once the input space was normalized to the unit hypercube.</p>
<p>Although this method allows for any combination of local and global kernels, for the purpose of evaluation, we used the Matérn kernel from equation (10) with automatic relevance determination for both -local and global-kernels.Furthermore, the kernel hyperparameters were initialized with the same prior for the both kernels.Therefore, we let the data determine which kernel has smaller length-scale.We found that the typical result is the behavior from Figure 1.However, in some problems, the method may learn a model where the local kernel has a larger length-scale (i.e.: smoother and smaller variance) than the global kernel, which may also improve the convergence in plateau-like functions.Besides, if the target function is stationary, the system might end up learning a similar length-scale for both kernels, thus being equivalent to a single kernel.We can say that SBO generalizes the modeling capabilities of standard BO, that is, standard BO is a special case of SBO where the local and global kernels are the same.</p>
<p>Given that for a single Matérn kernel with ARD, the number of kernel hyperparameters is the dimensionality of the problem, d, the number of hyperparameters for the Spartan kernel in this setup is 3d.As we will see in the experiments, this is the only drawback of SBO compared to standard BO, as it requires running MCMC in a larger dimensional space, which results in higher computational cost.However, because SBO is more efficient, the extra computational cost can be easily compensated by a reduced number of samples.</p>
<p>We implemented 2 Spartan Bayesian Optimization (SBO) using the BayesOpt library [40] as codebase.This allowed us to evaluate the setup for many surrogates and criteria.For comparison, we also evaluated other nonstatioary Bayesian optimization models:</p>
<p>• Input warping (WARP) from Snoek et al. [54] is the closest to our method both in terms of properties, applicability and results.To our knowledge, this is the only Bayesian optimization algorithm that has deal with nonstationarity using Gaussian processes in a fully correlated way.However, as presented in Section 5.6, WARP is much more expensive than SBO in terms of computational cost (at least, one order of magnitude in CPU time).</p>
<p>• Random forests [29] are excellent for discrete or categorical spaces and their computational (CPU time) performance is excellent.However, for continuous functions, their sample performance is not as good as any GP-based BO, as it has been reported previously in the literature [40,15].In Appendix B we discuss how to combine those two techniques to exploit the best properties of each method.</p>
<p>• Treed GPs [59,3] can be quite expressive in terms of modeling.However, due to the reduced number of samples per leaf and reduced global correlation, we found that they require a larger nugget or noise term σ 2 n to avoid numerical instability.For deterministic functions or large signal to noise ratio, like those presented in the results, increasing the nugget reduces the accuracy of the results.Furthermore, as presented in Assael et al. [3], single nonstationary GPs like the input warping method or our Spartan kernel can be used as tree leaves.Thus, SBO can be considered as an enhancement of treed GPs, not a competitor.</p>
<p>For clarity in the plots, we have only included the results from WARP algorithm, as the current state of the art, and standard (stationary) BO, as a reference baseline.We did not include the random forests or treed GPs as its performance was worse than standard BO or WARP algorithms.For the experiments reported here we used: a Gaussian process with unit mean function like in [31].For BO and WARP we also used a Matérn kernel ν = 5/2 with ARD.The kernel hyperparameters, including θ p for SBO and (α, β) for the warping functions were estimated using MCMC (i.e.: slice sampling).Due to the computational burden of MCMC, we used a small number of samples (i.e.: 10), while trying to decorrelate every resample with large burn-in periods (i.e.: 100 samples) as in Snoek et al. [52].All experiments were repeated 20 times using common random numbers.As commented in Section 2, the number of function evaluations in each plot includes an initial design of 10 points using latin hypercube sampling.</p>
<p>Optimization benchmarks</p>
<p>We evaluated the algorithms on a set of well-known test functions for global optimization both smooth or with sharp drops.</p>
<p>Figure 3 shows the results for the optimization benchmarks for functions with sharp drops where local optimization is fundamental.First, the exponential 2D function from Gramacy [22]:
f (x) = x 1 exp −x 2 1 − x 2 2(13)
with x 1 , x 2 ∈ [−2, 18] 2 .Our method (SBO) provides excellent results, by reaching the optimum in less than 25 iterations (35 samples) for all tests.Because the function is nonstationary, the WARP method outperforms standard BO, but its convergence is much slower, Figure 3, also shows the results for the Michalewicz function.The function has a parameter to define the dimensionality d and the steepness m.The function is defined as:
f (x) = d i=1 sin(x i ) sin 2m ix 2 i π(14)
for x ∈ [0, π] d .This function is known to be a hard benchmarks in global optimization due to the many local minima (d!) and steep drops.We used d = 10 and m = 10, resulting in 3628800 minima with very steep edges.For this problem, SBO clearly outperforms the rest of the methods by a large margin.Figure 4 show the results functions with smooth or wide minima.Bayesian optimization is more suitable for these function and naive strategies preform well in general.Therefore, there is barely room from improvement.However, we show that, even in this situation, SBO is equal or better than stationary BO.There is no penalty for the extra complexity on the model.However, the WARP method may get slower convergence due to the extra complexity.First, we evaluated the Branin-Hoo function, which has become a standard benchmark in Bayesian optimization,
f (x) = x 2 − 5.1 4π 2 x 2 1 + 5 π x 1 − 6 2 + 10 1 − 1 8π cos(x 1 ) + 10(15)
with x 1 ∈ [−5, 10] and x 2 ∈ [0, 15].As can be seen in Figure 4, the differences between SBO and BO are insignificant.Meanwhile, the warping function in WARP introduces an exploration bias at early stages, resulting in slower convergence.Finally, we tested the Hartmann 6D function for x ∈ [0, 1] 6 :
f (x) = − 4 i=1 α i exp   − 6 j=1 A ij (x j − P ij ) 2  (16)
with α, A and P defined in Appendix A. In this case, the differences are small, which imply that the function is most likely stationary and simple to exploit.However, we can see in the variance of the plots that nonstationary methods are more robust during the final exploitation.</p>
<p>Benchmarks for machine learning hyperparameter tuning</p>
<p>Our next set of experiments was based on a set of problems for automatic tunning of machine learning algorithms.</p>
<p>The first problem consists on tuning the 4 parameters of a logistic regression classifier to recognize handwritten numbers from the MNIST dataset (see Figure 6).As can be seen in the results, it is an easy problem for Bayesian optimization.Even the standard BO method were able to reach the minimum in less than 50 function evaluations.In this case, the warped method was the fastest one, with almost 20 evaluations.The proposed method had similar performance in terms of convergence, but with one order of magnitude lower execution time (see Section 5.6).</p>
<p>The second problem is called online LDA.This problem is based on the setup defined in Snoek et al [52] for learning topics of Wikipedia articles using Latent Dirichlet Allocation in an online fashion.It requires to tune 3 parameters.Both the standard BO and the WARP method got stuck while our method was able escape from the local minimum and outperform other methods by a large margin.Again, SBO required much lower computational cost than WARP.</p>
<p>Finally, we evaluated the HP-NNET problem, based on a deep neural network written by Bergstra et al. [4] to classify a modified MNIST dataset.In this case, the handwritten numbers were arbitrarily rotated and with random background images as distractors (see Figure 6).The new database is called MRBI, for MNIST Rotated and with Background Images.In this case, due to the high dimensionality and heterogeneity of the input space (7 continuous + 7 categorical parameters) we tested two approaches.First, we applied a single fully-correlated model for all the variables.The categorical variables were mapped to integer values that were computed by rounding the query values.In this case, similarly to the Hartmann function, our method (SBO) is more precise and robust.</p>
<p>Second, we applied a hierarchical model (see Appendix B for the details), which splits the continuous and categorical variables in a two layer optimization process.In this case, the nonstationary algorithms (SBO or WARP) were only applied on the continuous variables (outer loop).For the inner loop we used a Hamming kernel for categorical variables [28,62].
k H (x, x |θ) = exp − θ 2 g (s(x), s(x )) 2 (17)
where s(•) is a function that maps continuous vectors to discrete vectors by scaling and rounding.The function g(•, •) is defined as the Hamming distance g(x, x ) = |{i : x i = x i }| so as not to impose an artificial ordering between the values of categorical parameters.</p>
<p>Note that the plots are with respect to target function evaluations.However, the results of the hierarchical model are based on only 20 iterations of the outer loop, as each iteration requires 10 function evaluations in the inner loop.At early stages, SBO was trying to find a good location for the local kernel and the performance was slightly worse.However, after some data was gathered, the local kernel jumped to a good spot and the convergence was faster.</p>
<p>For these benchmarks, due to the complexity of the machine learning algorithms and the size of the datasets a single evaluation can take hours or days of wall-time.In order to simplify the comparison and run more tests, we used the surrogate benchmarks presented in Eggensperger et al. [15].It uses surrogate functions build with real data and evaluations to predict the performance of the tuned algorithms.Evaluating the surrogate function can be done in seconds, compared to the actual training each machine learning algorithm.The authors provide different surrogates [15].We selected the Gradient Boosting as it provides the lowest prediction error (RMSE) with respect to the actual data from each problem.We explicitly rejected the Gaussian Process surrogate benchmark to avoid the potential advantage of perfectly modeling.The results are shown in Figure 5.A detailed description of the experiments can be found in the original paper [15].</p>
<p>Reinforcement learning benchmarks</p>
<p>We evaluated SBO with several reinforcement learning problems.Reinforcement learning deals with the problem of how artificial agents perform optimal behaviors.An agent is defined by a set of variables s t that capture the current state and configuration of the agent in the world.The agent can then perform an action a t that modifies the agent state s t+1 = T (s t , a t ).At each time step, the agent collects the reward signal associated with its current state and action R t .The actions are selected according to a policy function that represents the agent behavior a t+1 = π(s t ).The states, actions and transitions are modeled using probabilities to deal with uncertainty.Thus, the objective of reinforcement learning is to find the optimal policy π * that maximizes the expected reward for a defined time horizon
π * = arg max π E s 0:T ,a 1:T T t=1 r t(18)
In the problems we have selected, we assume a finite time horizon.The expectation is evaluated using Monte Carlo, by sampling several episodes for a given policy.</p>
<p>Reinforcement learning algorithms usually rely on variants of the Bellman equation to optimize the policy step by step considering each instantaneous reward r t separately.Some algorithms also rely on partial or total knowledge of the transition model s t+1 = T (s t , a t ) in advance.Other methods tackle the optimization problem directly, considering equation ( 18) as an stochastic optimization problem.Those methods are called direct policy search [66].The use of Bayesian optimization for reinforcement learning was previously called active policy search [42] for its connection with active learning and how samples are carefully selected based on current information.</p>
<p>The main advantage of using Bayesian optimization to compute the optimal policy is that it can be done with very little information.In fact, as soon as we are able to simulate scenarios and return the total reward T t=1 r t , we do not need to access the dynamics, the instantaneous reward or the current For the first problem, SBO is able to achieve higher reward, while other get stuck in a local maxima.For the mountain car, SBO is able to achieve maximum performance in all trials after just 27 policy trials (17 iterations + 10 initial samples).For the helicopter problem, BO and WARP have slow convergence, because many policies results in an early crash.Providing almost no information.However, SBO is able to exploit good policies and quickly improve the performance.state of the system.Furthermore, there is no need for space or action discretization, building complex features or tile coding [57].We found that for many problems, a simple, low dimensional, quase-linear controller is able to achieve state-of-the-art performance if properly optimized.</p>
<p>A frequent issue for applying general purpose optimization algorithms for policy search is that, in many problems, the occurrence of failure states or scenarios results in large discontinuities or flat regions due to large penalties.This is opposed to the behavior of the reward near the optimal policy where small variations on a suboptimal policy can considerably change the performance achieved.Therefore, the resulting reward function presents a nonstationary behavior with respect to the policy.</p>
<p>We have compared our method in three well-known benchmarks with different level of complexity.The first problem is learning the controller of a three limb robot walker presented in Westervelt et al. [64].The controller modulates the walking pattern of a simple biped robot.The desired behavior is a fast upright walking pattern, the reward is based on the walking speed with a penalty for not maintaining the upright position.The dynamic controller has 8 continuous parameters.The walker problem was already used as a Bayesian optimization benchmark [26].</p>
<p>The second problem is the mountain car problem represented in Figure 7.The state of the system is the car horizontal position.The action is the horizontal acceleration a ∈ [−1, 1].Contrary to the many solutions that discretize both the state and action space, we can directly deal with continuous states and actions.The policy is a perceptron model inspired by Brochu et al. [6] as can be seen in Figure 7.The potentially unbounded policy parameters w = {w i } 7 i=1 are computed as
w = tan (π − π ) w 01 − π 2
where w 01 are the policy parameters bounded in the [0, 1] 7 space.The term π was used to avoid w i → ∞.For the experiments, we used π = 10 −4 .</p>
<p>The third problem is the hovering helicopter from the RL-competition3 .This is one of the most challenging scenarios of the competition, being presented in all the editions.This problem is based on a simulator of the XCell Tempest aerobatic helicopter.The simulator model was learned based on actual data from the helicopter using apprenticeship learning [1].The model was used to learn a policy that was later used in the real robot.The simulator included several difficult wind conditions.The state space is 12D (position, orientation, translational velocity and rotational velocity) and the action is 4D (forward-backward cyclic pitch, lateral cyclic pitch, main collective pitch and tail collective pitch).The reward is a quadratic function that penalizes both the state error (inaccuracy) and the action (energy).Each episode is run during 10 seconds (6000 control steps).If the simulator enters a terminal state (crash), a large negative reward is given, corresponding to getting the most negative reward achievable for the remaining time.</p>
<p>We also used a weak baseline controller that was included with the helicopter model.This weak controller is a simple linear policy with 12 parameters (weights).In theory, this controller is enough to avoid crashing but is not very robust.We show how this policy can be easily improved with few iterations.In this case, initial exploration of the parameter space is specially important because the number of policies not crashing in few control steps is very small.For most policies, the reward is the most negative reward achievable.Thus, in this case, we have used Sobol sequences for the initial samples of Bayesian optimization.These samples are deterministic, therefore we guarantee that the same number of non-crashing policies are sampled for every trial and every algorithm.We also increased the number of initial points to 40.</p>
<p>Figure 8 the performance for the three limb walker presented, the mountain car and the helicopter problem.In all cases, the results obtained by SBO were more efficient in terms on number of trials and accuracy, with respect to standard BO and WARP.Furthermore, we found that the results of SBO were comparable to those obtained by popular reinforcement learning solvers like SARSA [57], but with much less information and prior knowledge about the problem.For the helicopter problem, other solutions found in the literature require a larger number of scenarios/trials to achieve similar performance [2,34].</p>
<p>Automatic wing design using a CDF software</p>
<p>Computational fluid dynamics (CDF) software is a powerful tool for the design of mechanical and structural elements subject to interaction with fluids, such as aerodynamics, hydrodynamics or heating systems.Compared to physical design and testing in wind tunnels or artificial channels, the cost of simulation is almost negligible.Because simulated redesign is simple, CDF methods have been used for autonomous design of mechanical elements following principles from experimental design.This simulation-based autonomous design is a special case of the design and analysis of computer experiments (DACE).Recent developments both in terms of algorithms and available software have reduced the computational cost and improved the accuracy of the results, which allows DACE methodologies to be used to find optimal designs based on trial and error.Nevertheless, the computational cost of an average CDF simulation can still take days or months of CPU time.Therefore, the use of a sample efficient methodology is mandatory.Bayesian optimization has an enormous potential in the field for autonomous design.</p>
<p>The experiment that we designed to highlight the potential of Bayesian optimization in this field was the autonomous design of an optimal wing for an airplane [17].We simulated a wind tunnel using the xFlow TM CDF software.The objective of the experiment was to find the shape of the wing that minimizes the drag while maintaining enough lift.First, as a common practice in this kind of problems, we assumed a 2D simulation of the fluid along the profile of the wing.This is a reasonable assumption for wings with large aspect ration (large span compared to the chord), and it considerably reduces the wall time of each simulation from days to hours.For the parametrization of the profile, there are many alternatives based on geometric or manufacturing principles.In our case, we used Bezier curves for its simplicity to generate the corresponding shape.However, note that Bayesian optimization is agnostic of the geometric parametrization and any other parametrization could also be used.The Bezier curve of the wing was based on 7 control points, which resulted in 14 parameters.However, adding some physical and manufacturing restrictions resulted in 5 free parameters.The problem of minimizing the drag directly is that the best solutions tends to generate flat wings that do not provide enough lift for the plane.Figure 9 shows comparison of a wing with no lift and the optimal design.Although there has been some recent work on Bayesian optimization with constraints [19,21] we decided to use a simpler approach of adding a penalty with two purposes.Firts, the input space remains unconstrained, improving the performance of the optimization of the acquisition function.Second, due to the kernel smoothing, each penalty points generates a safety area where the function is partly penalized.Besides the drops and edges due to penalties, fluid dynamics also have chaotic properties: small changes in the initial conditions may produce a large variability in the result.For example, the flow near the trailing edge can transition from laminar to turbulent regime due to a small change in the wing shape.Thus, the resulting forces are completely different, increasing the drag and reducing the lift.Under this conditions, Figure 10 shows how both BO and WARP fail to find the optimum wing shape.However, SBO finds a better wing shape.Furthermore, it does it in few iterations.</p>
<p>Computational cost</p>
<p>Table 1 shows the average CPU time of the different experiments for the total number of function evaluations.HPNN-h uses the hierarchical model, with 200 function evaluations but only 20 iterations of the optimization loop, thus being faster.Due to the extensive evaluation, we had to rely on different machines for running the experiments, although all the algorithms for a single experiment were compared on the same machine.Thus, CPU time of different experiments might not be directly comparable.</p>
<p>The main difference between the three methods in terms of the algorithm is within the kernel function k(•, •), which includes the evaluation of the weights in SBO and the evaluation of the warping function (the cumulative density function of a Beta distribution or Beta CDF) in WARP.The rest of the algorithm is equivalent.That is, we used the same code optimization techniques.For that reason, we decided to measure the CPU time as a direct metric of the computational cost.Besides, CPU time is more robust than wall-time.</p>
<p>After some profiling, we found that the time differences between the algorithms were mainly driven by the dimensionality of the hyperparameter space because MCMC was the main bottleneck.Furthermore, the shape of the posterior distribution of the kernel hyperparameters (which is sampled through MCMC) played an important role.The likelihood of the parameters for the Beta CDF was very narrow and the slice sampling algorithm spent many iterations before accepting a samples.Narrow likelihoods and largeflip sampling are well known issues for MCMC.There are methods that could be applied to alleviate the issues such as hybrid Monte Carlo or sequential Monte Carlo samplers, but that remains an open problem.Note that, for all algorithms and experiments, we used slice sampling as recommended by the authors of WARP [54].Finally, the evaluation of the Beta CDF was much more involved and computationally expensive than the evaluation of the Matérn kernel or the Gaussian weights.That extra cost became an important factor as the kernel function was being called billions of times for each Bayesian optimization run.</p>
<p>It is important to note that, although Bayesian optimization is intended for expensive functions and the cost per iteration is negligible in most applications (for example: CDF simulations, deep learning algorithm tuning, etc.), the difference between methods could mean hours of CPU-time for a single iteration, changing the range of potential applications and inexpensive function evaluations.</p>
<p>Conclusions</p>
<p>In this paper, we have presented a new algorithm called Spartan Bayesian Optimization (SBO) which combines a local and a global kernel in a single adaptive kernel to deal with the exploration/exploitation trade-off and the inherent nonstationarity in the search process during Bayesian optimization.We have shown that this new kernel increases the convergence speed and reduces the number of samples in many kind of problems.For nonstationary problems, the method provides excellent results compared to standard Bayesian optimization and the state of the art method to deal with nonstationarity.Furthermore, SBO also performs well in stationary problems by improving local refinement while retaining global exploration capabilities.We evaluated the algorithm extensively in standard optimization benchmarks, automatic wing design and machine learning applications, such as hyperparameter tuning problems and classic reinforcement learning scenarios.The results have shown that SBO outperforms the state of the art in Bayesian optimization for all the experiments and tests.It requires less samples or achieves smaller optimization gap.In addition to that, we have shown how SBO was much more efficient in terms of CPU usage than other nonstationary methods for Bayesian optimization.</p>
<p>The results in reinforcement learning also highlight the potential of the active policy search paradigm for reinforcement learning.Our method is specially suitable for that paradigm.This fact opens a new opportunity for Bayesian optimization as an efficient and competitive reinforcement learning solver, without relying on the dynamics of the system, instantaneous rewards or discretization of the different spaces.</p>
<p>A Parameters of the Hartmann 6D function</p>
<p>The Hartmann 6D function for x ∈ [0, 1] 6 is defined as:
f (x) = − 4 i=1 α i exp   − 6 j=1 A ij (x j − P ij ) 2  (19)</p>
<p>B Discussion on mixed input spaces in Bayesian optimization</p>
<p>Although Bayesian optimization started as a method to solve classic nonlinear optimization problems with box-bounded restrictions, its sample efficiency and the flexibility of the surrogate models have attracted the interest of other communities and expanded the potential applications of the method.In many current Bayesian optimization applications, like hyperparameter optimization, it is necessary to simultaneously optimize different kinds of input variables, for example: continuous, discrete, categorical, etc.While Gaussian processes are suitable for modeling those spaces by choosing a suitable kernel [58,28], Bayesian optimization can become quite involved as the acquisition function (criterion) must be optimized in the same mixed input space.Available implementations of Bayesian optimization like Spearmint [52] use grid sampling and rounding tricks to combine different input spaces.However, this reduces the quality of the final result compared to proper nonlinear optimization methods [40].Other authors have proposed some heuristics specially designed for criterion maximization in Bayesian optimization [14], but its applicability to mixed input spaces still remains an open question.</p>
<p>We propose a hierarchical Bayesian optimization model, where the input space X is partitioned between homogeneous variables, for example: continuous variables x c and discrete variables x d .That is:
X = X c ∪ X d . = {x = [x c , x d ] T : x c ∈ X c ∨ x d ∈ X d }(20)
Therefore, the evaluation of an element higher in the hierarchy implies the full optimization of the elements lower in the hierarchy.In principle, that would require many more function evaluations but, as the input space has been partitioned, the dimensionality of each separate problem is much lower.In practice, for the same number of function evaluations, the computational cost of the optimization algorithm is considerably reduced.We can also include conditional variables in the outer loop to select which computations to perform in the inner loop.An advantage of this approach is that we can combine different surrogate models for different levels of the hierarchy.For example, using Random Forests [29] or tree-structured Parzen estimators [4] could be more suitable as a surrogate model for certain discrete/categorical variables than Gaussian processes.We could also use specific kernels like the Hamming kernel as we used in the previous section.</p>
<p>In contrast, we loose the correlation among variables in the inner loop, which may be counterproductive in certain situations.A similar alternative in the case where the target function is actually a combination of lower spaces, could be to use additive models, such as additive GPs [32].</p>
<p>Figure 5 shows how the method requires more function evaluations to achieve similar results than the fully correlated approach.However, as can be seen in Table 1, the computations cost and number of iterations increases, which might open new applications.A similar approach has been recently proposed to deal with high dimensional problems where evaluations are not expensive [61].</p>
<p>Figure 1 :
1
Figure1: Representation of the Spartan kernel in SBO.In this case, the kernel is just a combination of a single local and a global kernel.Typically, the local and global kernels have a small and large length-scale respectively.The influence of each kernel is represented by the normalized weight at the bottom of the plot.Note how the kernel with small length-scale produce larger uncertainties which is an advantage for fast exploitation, but it can perform poorly for global exploration as it tends to sample equally everywhere.On the other hand, the kernel with large length-scale provides a better global estimate, but with is smoother with higher uncertainty which improves global exploration.</p>
<p>Figure 2 :
2
Figure2: Left: Exponential 2D function from Gramacy[22].The path bellow the surface represents the location of the local kernel as being sampled by MCMC.Clearly, it converges to the nonstationary section of the function.For visualization, the path is broken in colors by the order in the path (blue → black → green → red).</p>
<p>Figure 3 :
3
Figure3: Left: Exponential 2D function from[22].The proposed method SBO results in an outstanding convergence speed compared to the state of the art.SBO finds the minimum in about 30 function evaluations in all tests.Right: Michalewicz 10D function with m=10.The convergence is slow, but nonstationay methods clearly perform better with SBO being fastest.</p>
<p>Figure 4 :
4
Figure 4: Left: Minimum value for the Branin-Hoo function.BO and SBO are barely identical.Center: Hartmann 6D function.Evolution of the minimum value for each method.Right: Zoom of minimum value during the final iterations.We can see how both SBO and WARP produce more accurate results and with smaller uncertainty, meaning that the results are more repeatable.</p>
<p>Figure 5 :
5
Figure 5: Surrogate benchmarks by [15] based on real hyperparameter optimization of machine learning algorithms.From left to right: logistic regression (4D continuous), online LDA (3D continuous), deep neural network (HP-NNET with the mrbi dataset, 7D continuous, 7D categorical), and deep neural network using the hierarchical model of Appendix B. In all cases SBO performs better than BO.Only in the logistic regression problem, the WARP method outperforms SBO.</p>
<p>Figure 6 :
6
Figure 6: The classifications problems use images of handwritten numbers.The objective of the classifier is to identify the correct number.Left: MNIST dataset of handwritten numbers used for the logistic regression problem.Right: Modified MNIST dataset with Rotated numbers and Background Images (MRBI) to increase the difficulty of classification.It was used in the HP-NNET problem.</p>
<p>Figure 7 :Figure 8 :
78
Figure 7: Left: Mountain car scenario.The car is underactuated and cannot climb to the goal directly.Instead it requires to move backwards to get inertia.The line in the left is an inelastic wall.Right: Policy use to control the mountain car.The inputs are the horizontal position x t , velocity v t = x t − x t−1 and acceleration a t = v t − v t−1 of the car.The output is the car throttle bounded to [−1, 1].</p>
<p>Figure 9 :
9
Figure 9: Vorticity plots of two different wing shapes.The left wing barely affect the trajectory of the wind, resulting in not enough lift.Meanwhile the right wing is able to provide enough lift with minimum drag.</p>
<p>Figure 10 :
10
Figure 10: Results for the wing design optimization.Each plot is based on 10 runs.</p>
<p>Algorithm 3 1 : 3 : 5 : 6 : 7 :samples for x d 8 : 9 : 10 :| x c n 11 : 13 :
31356789101113
Hierarchical Bayesian Optimization Total budget N = N c • N d Discrete iterations times continuous iterations 2: Split the discrete and continuous components of input space x = [x c , x d ] T Initial samples for x c 4: for n = 1 . . .N c do Outer loop Update model (e.g.: Gaussian process) for x c Find continuous component of next query point x c n (e.g.: maximize EI) Initial for k = 1 . . .N d do Inner loop Update model (e.g.: Random forest) for x d | x c n Find discrete component of next query point x d kCombine queries and evaluate function:y k ← f ([x c n , x d k ])Return optimal discrete query for current continuous query: x * (d) n| x c n 14: end for 15: Return optimal continuous query and corresponding discrete match: x * = [x * (c) , x * (d) ] T</p>
<p>Table 1 :
1
Average total CPU time in seconds.
Time (s)Exp2BraninHart.Michal.WalkerMCarLogROnLDAHPNNHPNN-h#dims226108743147+7#evals60407021040405070110200 (20)BO1201714608 36047382811276320SBO2 4813 73210 415225 3134407977302 13128 442146WARP13 92928 474188 9424 445 85420 27118 9729 14921 299710 9742 853
For the remainder of the paper we use the colon notation v 1:T to represent a stacked vectorv 1:T = [v 1 , . . . , v T ] T or a stacked matrix V 1:T = [v 1 , . . . , v T ].
The code will be release as open source when the paper gets published.
http://www.rl-competition.org/</p>
<p>An application of reinforcement learning to aerobatic helicopter flight. Pieter Abbeel, Adam Coates, Morgan Quigley, Andrew Ng, Advances in Neural Information Processing Systems. 2006</p>
<p>Reinforcement learning competition: Helicopter hovering with controllability and kernel-based stochastic factorization. Anwarissa Asbah, M S André, Clement Barreto, Joelle Gehring, Doina Pineau, Precup, International Conference on Machine Learning (ICML), Reinforcement Learning Competition Workshop. 2013</p>
<p>Heteroscedastic treed bayesian optimisation. M John-Alexander, Ziyu Assael, Nando Wang, De Freitas, arXiv2014Technical report</p>
<p>Algorithms for hyper-parameter optimization. J Bergstra, R Bardenet, Y Bengio, B Kgl, NIPS. 2011</p>
<p>Bayesian optimization explains human active search. Ali Borji, Laurent Itti, Advances in Neural Information Processing Systems. C J C Burges, L Bottou, M Welling, Z Ghahramani, K Q Weinberger, Curran Associates, Inc201326</p>
<p>A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. E Brochu, V M Cora, N De Freitas, arXiv:1012.2599arXiv.orgDecember 2010eprint</p>
<p>A Bayesian interactive optimization approach to procedural animation design. Eric Brochu, Tyson Brochu, Nando De Freitas, ACM SIGGRAPH/Eurographics Symposium on Computer Animation. 2010</p>
<p>X-armed bandits. Sébastien Bubeck, Rémi Munos, Gilles Stoltz, Csaba Szepesvári, Journal of Machine Learning Research. 122011</p>
<p>Convergence rates of efficient global optimization algorithms. Adam D Bull, Journal of Machine Learning Research. 122011</p>
<p>Bayesian optimization for learning gaits under uncertainty. R Calandra, A Seyfarth, J Peters, M Deisenroth, Annals of Mathematics and Artificial Intelligence (AMAI), 1. 12015</p>
<p>Robots that can adapt like animals. Antoine Cully, Jeff Clune, Danesh Tarapore, Jean-Baptiste Mouret, Nature. 5215035072015</p>
<p>Robust optimization of svm hyperparameters in the classification of bioactive compounds. Wojciech M Czarnecki, Sabina Podlewska, Andrzej J Bojarski, Journal of cheminformatics. 712015</p>
<p>Active reward learning with a novel acquisition function. C Daniel, O Kroemer, M Viering, J Metz, J Peters, Autonomous Robots. 3932015</p>
<p>Exponential regret bounds for Gaussian process bandits with deterministic observations. Alex Nando De Freitas, Masrour Smola, Zoghi, International Conference on Machine Learning (ICML). 2012</p>
<p>Efficient benchmarking of hyperparameter optimizers via surrogates. K Eggensperger, F Hutter, H H Hoos, K Leyton-Brown, Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence. the Twenty-Ninth AAAI Conference on Artificial IntelligenceJanuary 2015</p>
<p>Efficient and robust automated machine learning. Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, Frank Hutter, Advances in Neural Information Processing Systems. 2015</p>
<p>Optimization using surrogate models and partially converged computational fluid dynamics simulations. Neil W Alexander Ij Forrester, Andy J Bressloff, Keane, Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences. 4622071. 2006</p>
<p>A locally-biased form of the DIRECT algorithm. M Joerg, Tim Gablonsky, Kelley, Journal of Global Optimization. 2112001</p>
<p>Bayesian optimization with inequality constraints. Jacob Gardner, Matt Kusner, Zhixiang Xu, Kilian Weinberger, John Cunningham, Proceedings of the 31st International Conference on Machine Learning (ICML-14). the 31st International Conference on Machine Learning (ICML-14)2014</p>
<p>Bayesian optimization for sensor set selection. Roman Garnett, Michael A Osborne, Stephen J Roberts, Information Processing in Sensor Networks (IPSN 2010). 2010</p>
<p>Bayesian optimization with unknown constraints. A Michael, Jasper Gelbart, Ryan P Snoek, Adams, Uncertainty in Artificial Intelligence (UAI). 2014</p>
<p>Bayesian treed Gaussian process models. Robert B Gramacy, 2005Santa ClaraUniversity of CaliforniaPhD thesis</p>
<p>Cases for the nugget in modeling computer experiments. Robert B Gramacy, K H Herbert, Lee, Statistics and Computing. 222012</p>
<p>A radial basis function method for global optimization. H-M Gutmann, Journal of Global Optimization. 1932001</p>
<p>Entropy search for information-efficient global optimization. Philipp Hennig, Christian J Schuler, Journal of Machine Learning Research. 132012</p>
<p>Predictive entropy search for efficient global optimization of black-box functions. José Miguel Hernández-Lobato, Matthew W Hoffman, Zoubin Ghahramani, Advances in Neural Information Processing Systems. Z Ghahramani, M Welling, C Cortes, N D Lawrence, K Q Weinberger, Curran Associates, Inc201427</p>
<p>Global optimization of stochastic black-box systems via sequential kriging meta-models. D Huang, T T Allen, W I Notz, N Zheng, Journal of Global Optimization. 3432006</p>
<p>Automated Configuration of Algorithms for Solving Hard Computational Problems. Frank Hutter, 2009Vancouver, CanadaUniversity of British ColumbiaPhD thesis</p>
<p>Sequential model-based optimization for general algorithm configuration. Frank Hutter, H Holger, Kevin Hoos, Leyton-Brown, LION-5. 2011507523</p>
<p>A taxonomy of global optimization methods based on response surfaces. R Donald, Jones, Journal of Global Optimization. 212001</p>
<p>Efficient global optimization of expensive black-box functions. Donald R Jones, Matthias Schonlau, William J Welch, Journal of Global Optimization. 1341998</p>
<p>High dimensional bayesian optimisation and bandits via additive models. Kirthevasan Kandasamy, Jeff Schneider, Barnabas Poczos, International Conference on Machine Learning (ICML). 2015</p>
<p>Expected improvement in efficient global optimization through bootstrapped kriging. Wim Jack Pc Kleijnen, Inneke Van Beers, Van Nieuwenhuyse, Journal of Global Optimization. 5412012</p>
<p>Neuroevolutionary reinforcement learning for generalized control of simulated helicopters. Rogier Koppejan, Shimon Whiteson, Evolutionary intelligence. 442011</p>
<p>Nonmyopic active learning of Gaussian processes: an exploration-exploitation approach. Andreas Krause, Carlos Guestrin, International Conference on Machine Learning (ICML), Corvallis. OregonJune 2007</p>
<p>Combining active learning and reactive control for robot grasping. Oliver Kroemer, Renaud Detry, Justus Piater, Jan Peters, Robotics and Autonomous Systems. 5892010</p>
<p>A new method of locating the maximum of an arbitrary multipeak curve in the presence of noise. J Harold, Kushner, Journal of Basic Engineering. 861964</p>
<p>The defence of Greece. John F Lazenby, 1993Aris &amp; Phillips</p>
<p>Adaptive MCMC with Bayesian optimization. Nimalan Mahendran, Ziyu Wang, Firas Hamze, Nando De Freitas, Journal of Machine Learning Research -Proceedings Track for Artificial Intelligence and Statistics (AISTATS). 222012</p>
<p>BayesOpt: A Bayesian optimization library for nonlinear optimization, experimental design and bandits. Ruben Martinez-Cantin, Journal of Machine Learning Research. 152014</p>
<p>A Bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided mobile robot. Ruben Martinez-Cantin, Nando De Freitas, Eric Brochu, Jose Castellanos, Arnoud Doucet, Autonomous Robots. 2732009</p>
<p>Active policy learning for robot planning and exploration under uncertainty. Ruben Martinez-Cantin, Nando De Freitas, Arnoud Doucet, Jose A Castellanos, Robotics: Science and Systems. 2007</p>
<p>Bayesian Approach to Global Optimization. Jonas Mockus, Mathematics and Its Applications. 371989Kluwer Academic Publishers</p>
<p>The application of Bayesian methods for seeking the extremum. Jonas Mockus, Vytautas Tiesis, Antanas Zilinskas, Towards Global Optimisation 2. L C W Dixon, G P Szego, Elsevier1978</p>
<p>Memory-based stochastic optimization. Andrew W Moore, Jeff Schneider, Advances in Neural Information Processing Systems. David S Touretzky, Michael C Mozer, Michael E Hasselmo, The MIT Press19968</p>
<p>Some Bayesian numerical analysis. O' Anthony, Hagan, Bayesian Statistics. 41992</p>
<p>Carl E Rasmussen, K I Christopher, Williams, Gaussian Processes for Machine Learning. Cambridge, MassachusettsThe MIT Press2006</p>
<p>Learning to optimize via posterior sampling. Daniel Russo, Benjamin Van Roy, Mathematics of Operations Research. 3942014</p>
<p>Nonparametric estimation of nonstationary spatial covariance structure. D Paul, Peter Sampson, Guttorp, Journal of the American Statistical Association. 874171992</p>
<p>Student-t processes as alternatives to Gaussian processes. Amar Shah, Andrew Gordon Wilson, Zoubin Ghahramani, AISTATS, JMLR Proceedings. JMLR.org. 2014</p>
<p>A pseudo-global optimization approach with application to the design of containerships. D Hanif, Vikram Sherali, Ganesan, Journal of Global Optimization. 2642003</p>
<p>Practical Bayesian optimization of machine learning algorithms. Jasper Snoek, Hugo Larochelle, Ryan Adams, NIPS. 2012</p>
<p>Scalable bayesian optimization using deep neural networks. Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Md Mostofa Ali Patwary, Ryan P Prabhat, Adams, International Conference on Machine Learning. 2015</p>
<p>Input warping for Bayesian optimization of non-stationary functions. Jasper Snoek, Kevin Swersky, Richard S Zemel, Ryan Prescott, Adams , International Conference on Machine Learning. 2014</p>
<p>Gaussian process optimization in the bandit setting: No regret and experimental design. Niranjan Srinivas, Andreas Krause, Sham Kakade, Matthias Seeger, Proc. International Conference on Machine Learning (ICML). International Conference on Machine Learning (ICML)2010</p>
<p>A comparison of bayesian/sampling global optimization techniques. E Bruce, Eric E Stuckman, Easom, IEEE Transactions on Systems, Man, and Cybernetics. 2251992</p>
<p>Reinforcement Learning: An Introduction. Richard S Sutton, Andrew G Barto, 1998The MIT Press</p>
<p>Raiders of the lost architecture: Kernels for bayesian optimization in conditional parameter spaces. Kevin Swersky, David Duvenaud, Jasper Snoek, Frank Hutter, Michael A Osborne, arXiv:1409.4011NIPS workshop on Bayesian Optimization. 2014arXiv preprint</p>
<p>Bayesian guided pattern search for robust local optimization. Matthew A Taddy, Genetha A Herbert Kh Lee, Joshua D Gray, Griffin, Technometrics. 5142009</p>
<p>The Bayesian search game. Marc Toussaint, Theory and Principled Methods for Designing Metaheuristics. Alberto Moraglio, Yossi Borenstein, Springer2014</p>
<p>Bayesian optimization with dimension scheduling: Application to biological systems. Doniyor Ulmasov, Caroline Baroukh, Benoit Chachuat, Marc Peter Deisenroth, Ruth Misener, 26th European Symposium on Computer Aided Process Engineering. 2016Computer-Aided Chemical Engineering</p>
<p>Bayesian optimization in high dimensions via random embeddings. Ziyu Wang, Masrour Zoghi, Frank Hutter, David Matheson, Nando De Freitas, International Joint Conferences on Artificial Intelligence (IJCAI) -Extended version. 2013</p>
<p>Automatic methods for long-term tracking and the detection and decoding of communication dances in honeybees. Fernando Wario, Benjamin Wild, Margaret Jane Couvillon, Raúl Rojas, Tim Landgraf, Frontiers in Ecology and Evolution. 31032015</p>
<p>Feedback control of dynamic bipedal robot locomotion. Jessy W Eric R Westervelt, Christine Grizzle, Jun Chevallereau, Benjamin Ho Choi, Morris, 2007CRC press28</p>
<p>Sequential design of computer experiments to minimize integrated response functions. Brian J Williams, Thomas J Santner, William I Notz, Statistica Sinica. 1042000</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. Williams Ronald, Machine learning. 83-41992</p>            </div>
        </div>

    </div>
</body>
</html>
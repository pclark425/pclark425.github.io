<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1481 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1481</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1481</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-259501912</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.03906v1.pdf" target="_blank">ScriptWorld: Text Based Environment For Learning Procedural Knowledge</a></p>
                <p><strong>Paper Abstract:</strong> Text-based games provide a framework for developing natural language understanding and commonsense knowledge about the world in reinforcement learning based agents. Existing text-based environments often rely on fictional situations and characters to create a gaming framework and are far from real-world scenarios. In this paper, we introduce ScriptWorld: a text-based environment for teaching agents about real-world daily chores and hence imparting commonsense knowledge. To the best of our knowledge, it is the first interactive text-based gaming framework that consists of daily real-world human activities designed using scripts dataset. We provide gaming environments for 10 daily activities and perform a detailed analysis of the proposed environment. We develop RL-based baseline models/agents to play the games in Scriptworld. To understand the role of language models in such environments, we leverage features obtained from pre-trained language models in the RL agents. Our experiments show that prior knowledge obtained from a pre-trained language model helps to solve real-world text-based gaming environments. We release the environment via Github: https://github.com/Exploration-Lab/ScriptWorld</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1481.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1481.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ScriptWorld</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ScriptWorld: Text Based Environment For Learning Procedural Knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A choice-based interactive text environment built from human-written script corpora (DeScript) that generates realistic daily‑life procedural tasks as scenario graphs with branching, parallel subpaths and dynamic choice sampling; used to train and evaluate RL agents augmented with pretrained language model embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RPPO (Recurrent PPO) + SBERT (baseline set: DQN, A2C, PPO, RPPO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents receive SBERT sentence embeddings for each presented textual choice; embeddings are concatenated and passed into standard RL algorithms. RPPO is PPO augmented with an LSTM (recurrent policy) and achieved the best overall performance; other baselines were DQN, A2C, and PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ScriptWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Choice-based text environment constructed from DeScript aligned event sequences. Each state shows a textual observation and a sampled list of textual choices (one correct sampled randomly from node actions; incorrect choices sampled from temporally distant nodes). Supports flexible settings: number of choices per state, backward hop distance on wrong actions, and a 'handicap' hint (textual clue) shown at every state (hints generated via GPT-2). Episodes terminate after 5 successive wrong actions; rewards: -1 per wrong action, 0 per correct step, +10 on task completion.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures (daily household / real-world tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Bath, Baking a Cake, Flying in an Airplane, Going Grocery Shopping, Repairing a Flat Bicycle Tire, Riding a Bus, Riding a Train, Planting a Tree, Getting a Haircut, Borrowing a Book from the Library</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Tasks are compositional: scenario graphs are constructed from aligned event clusters (compact graph) and expanded into scenario graphs where each abstract event can split into entry/exit nodes and multiple sub-step paths; tasks decompose into sequences of events with branching (parallel alternative subpaths), optional/skippable events and variable ordering across ESDs.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Scenario compact-node counts range ≈ 21–39 (Table 4); number of correct paths varies widely per scenario (computed via DFS plus parallel subpaths); difficulty is adjustable via number of choices per step (2–5 tested) and presence/absence of hints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>The paper evaluates generalization by training RPPO on one scenario and testing on others (100 runs): some transfer occurs between similar scenarios (e.g., Bus ↔ Train), but generalization is limited, especially without hints and with more choices.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained contextual language-model embeddings (SBERT) help RL agents operate in realistic procedural text environments; recurrent policies (RPPO + LSTM) handle long multi-step scripted procedures better than non-recurrent baselines; hints (handicaps) reduce variance and improve learning; increasing action branching (more choices) increases difficulty and induces temporary performance dips that correlate with exploration and graph coverage.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1481.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1481.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yin & May 2019 (map familiarization + curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and bandit feedback to learn families of text-based adventure games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work (cited in this paper) that combines map familiarization, curriculum learning, and bandit-style feedback to train agents on families of text‑based adventure games (task family: cooking/recipe-style tasks in new houses).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and bandit feedback to learn families of text-based adventure games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Families of text-based adventure games (new‑house recipe tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive text‑based adventure games organized as families (example: cooking a recipe in an unfamiliar house); the referenced work uses map familiarization and curriculum learning as part of the training regimen, plus bandit feedback to adapt learning across related games.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>household procedural tasks (recipe-style / adventure game tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Cooking a new recipe in a new house (map-familiarization tasks in text-adventure settings)</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>curriculum learning (combined with map familiarization and bandit feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>According to the citation, the approach integrates map familiarization and curriculum learning, augmented by bandit feedback, to sequence and adapt training across related text‑game tasks (paper title only; ScriptWorld cites this work as related prior art but provides no experimental details).</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>map familiarization / curriculum-driven ordering (exact principle not detailed in this paper's citation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a relevant approach combining map familiarization, curriculum learning and bandit feedback to learn families of text‑based adventure games; ScriptWorld cites it as an example of curriculum-style training in text-based RL but does not reproduce or analyze its results.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and bandit feedback to learn families of text-based adventure games <em>(Rating: 2)</em></li>
                <li>Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines <em>(Rating: 1)</em></li>
                <li>Learning Dynamic Belief Graphs to Generalize on Text-Based Games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1481",
    "paper_id": "paper-259501912",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "ScriptWorld",
            "name_full": "ScriptWorld: Text Based Environment For Learning Procedural Knowledge",
            "brief_description": "A choice-based interactive text environment built from human-written script corpora (DeScript) that generates realistic daily‑life procedural tasks as scenario graphs with branching, parallel subpaths and dynamic choice sampling; used to train and evaluate RL agents augmented with pretrained language model embeddings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RPPO (Recurrent PPO) + SBERT (baseline set: DQN, A2C, PPO, RPPO)",
            "agent_description": "Agents receive SBERT sentence embeddings for each presented textual choice; embeddings are concatenated and passed into standard RL algorithms. RPPO is PPO augmented with an LSTM (recurrent policy) and achieved the best overall performance; other baselines were DQN, A2C, and PPO.",
            "agent_size": null,
            "environment_name": "ScriptWorld",
            "environment_description": "Choice-based text environment constructed from DeScript aligned event sequences. Each state shows a textual observation and a sampled list of textual choices (one correct sampled randomly from node actions; incorrect choices sampled from temporally distant nodes). Supports flexible settings: number of choices per state, backward hop distance on wrong actions, and a 'handicap' hint (textual clue) shown at every state (hints generated via GPT-2). Episodes terminate after 5 successive wrong actions; rewards: -1 per wrong action, 0 per correct step, +10 on task completion.",
            "procedure_type": "commonsense procedures (daily household / real-world tasks)",
            "procedure_examples": "Bath, Baking a Cake, Flying in an Airplane, Going Grocery Shopping, Repairing a Flat Bicycle Tire, Riding a Bus, Riding a Train, Planting a Tree, Getting a Haircut, Borrowing a Book from the Library",
            "compositional_structure": "Tasks are compositional: scenario graphs are constructed from aligned event clusters (compact graph) and expanded into scenario graphs where each abstract event can split into entry/exit nodes and multiple sub-step paths; tasks decompose into sequences of events with branching (parallel alternative subpaths), optional/skippable events and variable ordering across ESDs.",
            "uses_curriculum": false,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": null,
            "task_complexity_range": "Scenario compact-node counts range ≈ 21–39 (Table 4); number of correct paths varies widely per scenario (computed via DFS plus parallel subpaths); difficulty is adjustable via number of choices per step (2–5 tested) and presence/absence of hints.",
            "performance_with_curriculum": null,
            "performance_without_curriculum": null,
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": "The paper evaluates generalization by training RPPO on one scenario and testing on others (100 runs): some transfer occurs between similar scenarios (e.g., Bus ↔ Train), but generalization is limited, especially without hints and with more choices.",
            "key_findings": "Pretrained contextual language-model embeddings (SBERT) help RL agents operate in realistic procedural text environments; recurrent policies (RPPO + LSTM) handle long multi-step scripted procedures better than non-recurrent baselines; hints (handicaps) reduce variance and improve learning; increasing action branching (more choices) increases difficulty and induces temporary performance dips that correlate with exploration and graph coverage.",
            "uuid": "e1481.0"
        },
        {
            "name_short": "Yin & May 2019 (map familiarization + curriculum)",
            "name_full": "Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and bandit feedback to learn families of text-based adventure games",
            "brief_description": "A prior work (cited in this paper) that combines map familiarization, curriculum learning, and bandit-style feedback to train agents on families of text‑based adventure games (task family: cooking/recipe-style tasks in new houses).",
            "citation_title": "Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and bandit feedback to learn families of text-based adventure games",
            "mention_or_use": "mention",
            "agent_name": null,
            "agent_description": null,
            "agent_size": null,
            "environment_name": "Families of text-based adventure games (new‑house recipe tasks)",
            "environment_description": "Interactive text‑based adventure games organized as families (example: cooking a recipe in an unfamiliar house); the referenced work uses map familiarization and curriculum learning as part of the training regimen, plus bandit feedback to adapt learning across related games.",
            "procedure_type": "household procedural tasks (recipe-style / adventure game tasks)",
            "procedure_examples": "Cooking a new recipe in a new house (map-familiarization tasks in text-adventure settings)",
            "compositional_structure": null,
            "uses_curriculum": true,
            "curriculum_name": "curriculum learning (combined with map familiarization and bandit feedback)",
            "curriculum_description": "According to the citation, the approach integrates map familiarization and curriculum learning, augmented by bandit feedback, to sequence and adapt training across related text‑game tasks (paper title only; ScriptWorld cites this work as related prior art but provides no experimental details).",
            "curriculum_ordering_principle": "map familiarization / curriculum-driven ordering (exact principle not detailed in this paper's citation)",
            "task_complexity_range": null,
            "performance_with_curriculum": null,
            "performance_without_curriculum": null,
            "has_curriculum_comparison": null,
            "alternative_curriculum_performance": null,
            "transfer_generalization": null,
            "key_findings": "Mentioned as a relevant approach combining map familiarization, curriculum learning and bandit feedback to learn families of text‑based adventure games; ScriptWorld cites it as an example of curriculum-style training in text-based RL but does not reproduce or analyze its results.",
            "uuid": "e1481.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and bandit feedback to learn families of text-based adventure games",
            "rating": 2,
            "sanitized_title": "learn_how_to_cook_a_new_recipe_in_a_new_house_using_map_familiarization_curriculum_learning_and_bandit_feedback_to_learn_families_of_textbased_adventure_games"
        },
        {
            "paper_title": "Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines",
            "rating": 1,
            "sanitized_title": "textbased_rl_agents_with_commonsense_knowledge_new_challenges_environments_and_baselines"
        },
        {
            "paper_title": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games",
            "rating": 1,
            "sanitized_title": "learning_dynamic_belief_graphs_to_generalize_on_textbased_games"
        }
    ],
    "cost": 0.012996499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ScriptWorld: Text Based Environment For Learning Procedural Knowledge</p>
<p>Abhinav Joshi 
Indian Institute of Technology Kanpur (IIT-K)</p>
<p>Areeb Ahmad 
Indian Institute of Technology Kanpur (IIT-K)</p>
<p>Umang Pandey umangp@iitk.ac.in 
Indian Institute of Technology Kanpur (IIT-K)</p>
<p>Ashutosh Modi ashutoshm@cse.iitk.ac.in 
Indian Institute of Technology Kanpur (IIT-K)</p>
<p>ScriptWorld: Text Based Environment For Learning Procedural Knowledge</p>
<p>Text-based games provide a framework for developing natural language understanding and commonsense knowledge about the world in reinforcement learning based agents. Existing text-based environments often rely on fictional situations and characters to create a gaming framework and are far from real-world scenarios. In this paper, we introduce ScriptWorld: a text-based environment for teaching agents about real-world daily chores and hence imparting commonsense knowledge. To the best of our knowledge, it is the first interactive text-based gaming framework that consists of daily real-world human activities designed using scripts dataset. We provide gaming environments for 10 daily activities and perform a detailed analysis of the proposed environment. We develop RLbased baseline models/agents to play the games in ScriptWorld. To understand the role of language models in such environments, we leverage features obtained from pre-trained language models in the RL agents. Our experiments show that prior knowledge obtained from a pre-trained language model helps to solve real-world text-based gaming environments.</p>
<p>Introduction</p>
<p>Text-based games in reinforcement learning have attracted research interests in recent years [Hausknecht et al., 2020;Küttler et al., 2020]. These games have been developed to impart Natural Language Understanding (NLU) and commonsense reasoning capabilities in Reinforcement Learning (RL) based agents. A typical text-based game consists of a textual description of states of an environment where the agent/player observes and understands the game state and context using text and interacts with the environment using textual commands (actions). For successfully solving a text-based game, in addition to language understanding, an agent needs complex decision-making abilities, memory, planning, questioning, and commonsense knowledge [Côté et al., 2018]. Existing text-based gaming frameworks (e. g., Jericho [Hausknecht et al., 2020]) provide a rich fictional setup (e.g., treasure hunt in a fantasy world) and require an agent to take complex decisions involving language and fantasy world knowledge. However, the existing text-based frameworks are created using a fixed prototype and are often distant from real-world scenarios involving daily human activities. Though these frameworks aim to provide a rich training bench for enhancing NLU in RL algorithms, the fictional concepts in these games are not well grounded in real-world scenarios, making the learned knowledge non-applicable to the real world. In contrast, for trained RL algorithms to be of practical utility, they should be trained in real-world scenarios that involve daily human activities. Humans carry out daily activities (e.g., making coffee, going for a bath) without much effort by making use of implicit Script Knowledge. Formally, Scripts are defined as sequences of actions describing stereotypical human activities, for example, cooking pasta, making coffee, etc. [Schank and Abelson, 1975]. Scripts entail knowledge about the world. For example, when someone talks about "Washing Dishes", there lies an implicit knowledge of fine-grained steps which would be present in the activity. By just saying, "I washed dishes on Thursday," a person conveys the implicit knowledge about the entire process ( Fig. 1). The detailed implicit understanding of a task not only helps to learn about an activity but also facilitates taking suitable actions depending on the environment and past choices. Moreover, for learning a new task, humans can quickly and effortlessly discover new skills for performing the task either by their knowledge about the world or by reading (a manual) about it. With the aim to promote similar learning behavior in RL agents, in this paper, we propose ScriptWorld, a new text-based game environment based on real-world scenarios involving script knowledge. The motivation for creating ScriptWorld environment is threefold. Firstly, ScriptWorld environment is based on the concept of scripts that encapsulates commonsense and procedural knowledge about the world. The environment is designed to enable agents to learn this knowledge while participating in the game. Scripts have non-linear structure [Wanzare et al., 2016]. A script scenario can be described in multiple ways with linguistic variation across different descriptions. Fig. 1 shows different descriptions for the washing dishes scenario. Moreover, at the level of execution, the order of events/actions within the script can vary across different descriptions of a scenario. For example, some events may be skipped, and the order of events might vary. Hence, learning script knowledge is challenging. Taking into account the variability in descriptions of a scenario, an agent needs to learn the prototypical order of events and needs to abstract out the meaning of different verbal descriptions of an action. Secondly, ScriptWorld being a text-based environment about everyday scenarios, provides an opportunity for grounded language learning and understanding. Language phenomena do not happen in isolation, but the semantics are grounded in the real world [Hill et al., 2017]; ScriptWorld provides the environment to establish and learn that grounding. Lastly, there have been extensive studies that have explored the cognitive basis of script knowledge in humans [Miikkulainen and Elman, 1993;Modi, 2017]. ScriptWorld involves the acquisition of script knowledge. Consequently, it provides an opportunity to compare the behavior of a trained RL agent with humans providing further insights into the cognitive aspects. In a nutshell, we make the following contributions:</p>
<p>• We introduce a new interactive text-based gaming environment, ScriptWorldconsisting of games based on script descriptions provided by human annotators for performing realistic daily chores. We perform a detailed analysis of the proposed environment and release the environment and agents  [Regneri et al., 2010;Frermann et al., 2014;Modi and Titov, 2014;Rudinger et al., 2015;Jans et al., 2012;Pichotta and Mooney, 2016;Modi et al., 2017]. A number of corpora have also been created, e.g., InScript   To the best of our knowledge, the proposed method is the first novel approach to create an environment (based on script knowledge) that could be useful for training RL agents. The ScriptWorld environment is created from scratch using Python. A typical game begins by providing a quest (goal) to the agent. The quest/goal is a one-line description of the scenario (e.g., plant a tree). The agent is also provided with initial observations (in English). Since it is a choice-based game, at each step in the game, the agent is also presented with a list of actions/choices (in English) that it could opt to advance towards the goal. Based on the action selected by the agent, it is awarded a zero/positive/negative reward at each step. Every correct action takes the agent closer to task completion, whereas every wrong action results in a deviated path. (also see App. A, the appendix is available at https://github.com/Exploration-Lab/ScriptWorld). ) in E W ash 2 are linked (clustered) together. Aligned events (from different ESDs) are used to create a graph having nodes as the event clusters (of aligned events) and directed edges representing the prototypical order of the events. In particular, a directed edge is drawn from node p to q if there is at least one event in node p that directly precedes an event in node q. We refer to the created event node graph as the compact graph (Fig. 3), compact graphs for other scenarios are in App. A. The alignment annotations in the DeScript also group multiple sets of actions that belong to the same event. For example, an event "go to the terrace" can be performed in two sets of sequenced steps by different annotators. 1) call the elevator → step in elevator → step out at the top floor, and 2) find stairs → climb stairs → reach top floor. We leverage the presence of such instances in the graph node to enrich the complexity of our environment. We split each event node in the compact graph into two nodes, the entry event node and the exit event node. Further, multiple action sequences result in parallel paths for reaching the exit node from the entry node (see also App. A). For instance, the above example will result in two parallel paths, where a player or an agent has to decide at the entry node to either take the elevator or the stairs. If players choose to take the stairs, they are expected to follow the next set of actions to reach the terrace. Moreover, all the sub-steps in this event now result in multiple graph nodes. We refer to this graph as the scenario graph (see App. A). This helps to capture the variability in performing daily chores, making the environment more realistic. Though the DeScript corpus provides clustered events for every scenario, after graph creation, we found that a few of the ESDs present in the corpus were inconsistent, not fitting the commonsense reasoning for a procedure. We also observed that some of the ESDs written by annotators are too small and describe the task in generic terms. Such ESDs, when considered in graph formation, result in direct paths to the final goal node, making the game less complex. We remove all such inconsistencies from the graph by manual inspection, making it more reliable for capturing script knowledge and keeping the realism intact for the environment. The compact graph serves as an initial starting point for creating the scenario graph. The agents are trained on a scenario graph. To quantitatively capture the complexity of scenarios in ScriptWorld, we calculate the total number of paths reaching the end node from the start node. We first compute the total number of paths in the compact graph using a depth-first traversal. Further, we extend the computation by adding the number of parallel paths present for each entry and exit event node in the scenario graph. TotalPaths =</p>
<p>Graph Formation. DeScript provides set of aligned
ESDs (E Si 1 , E Si 2 , . . . , E Si N ) for a scenario S i . Each ESD E i k consists of sequence of short event descriptions: e (E i k ) 1 , e (E i k ) 2 , . . . e (E i k ) n . GoldT p k =0 N i=1 n i (p k ) ,
where T is the total number of paths in a compact graph, N represents the total number of nodes in a path p k and n i (p k ) denotes the number of splits for the i th node. Table 1 shows the total number of paths. As evident from the table, the number of paths in each of the scenarios is enormous and demonstrates the highly complex nature of the environment. Overall, the scenario Flying in an Airplane turns out to be the most complex one in terms of the number of correct possible paths. This is possibly due to more variability in carrying out this activity.</p>
<p>Environment Creation. We create the game environment using scenario graphs. For each state in the environment, the agent is required to pick the correct action (choice) from the available options. Since the created scenario graph contains a wide variety of suitable actions grouped in a node, we sample the right choice from the available actions in a node. Note that sampling of correct actions happens randomly at every visit, making the environment highly dynamic. To create incorrect choices, we exploit the temporal nature of the scenario graphs. As a scenario graph contains the sequence of actions to perform a specific sub-task, all actions in nodes (both past as well as future nodes are considered) that are far from the current node become invalid for the current state. For selecting this node distance, we manually experiment with different node distances and find the different distances (d 1 , d 2 , . . . d 10 ) suitable for sampling the invalid actions, i.e., for a scenario i, we consider all nodes at a distance greater than d i hops from the current node (Table in App. A shows various distances chosen for each of the scenarios). This strategy of sampling the invalid choices makes the environment more complex as all the options are related to the same scenario, and an understanding of event order in a task is required to achieve the goal.</p>
<p>Rewards (Performance Scores):</p>
<p>For all the scenarios, every incorrect action choice results in a negative reward of -1, and every correct choice returns a 0 reward. For task completion, the agent gets a reward of 10, i.e., a player gets a maximum reward of 10 at the end of each game if they choose a correct sequence of actions. The choice of zero rewards for correct action helps RL algorithms explore multiple correct ways of performing a task, capturing the generalized procedural knowledge required for a specific task. The game terminates when an agent chooses 5 successive wrong actions.</p>
<p>Flexibility: To introduce flexibility in ScriptWorld, we consider two settings in a game. 1) Number of choices: At each step, the number of choices presented to an agent can be changed (1 correct choice and the rest all incorrect). As the number of options increases, it becomes more challenging for an agent to choose the right action. 2) Number of backward hops for wrong actions: We choose the number of backward   hops as another game setting that decides how many hops to displace whenever a wrong action is selected. When an agent selects an incorrect choice, its location is displaced by hopping it backward in the temporal domain, and this back-hop distance is another parameter in the environment. In our experiments, agents played with the environment with a backhop distance of 1. Due to the presence of parallel paths in the graph, an agent hops to a previous node in case of incorrect action and may not follow the same path again, which acts as a penalty. For the start node, since backward hop is not possible, the agent remains at the same position; however, both positive and negative choices are re-sampled, and consequently, observations change. These parameters introduce flexibility in our environment, giving the freedom to create a suitable test bench for RL algorithms.</p>
<p>Handicaps (Hints): Text-based games are often challenging for RL agents playing from scratch. To mitigate the complexity issue, we introduce a version of the game with hints (referred to as handicaps) for each state. The hint for a state provides a short textual clue for the next action to take at the current state. The presence of hints in the environment makes the gameplay relatively easier. Hints are generated automatically using GPT2 [Radford et al., 2019]. Scenario title concatenated with state node event description (separated by a full-stop) is given as the prompt to GPT2 for generating a large number of hints, and then a hint is sampled from them. We manually examined the hints to ensure they did not repeat (verbatim) any of the existing actions. To introduce variability, one could also stochastically decide to show a hint, e.g., by sampling from a Bernoulli distribution at each state. However, in this paper, we consider only the setting where hints are shown at every state. We leave this for future work.</p>
<p>Comparison with other text-based environments: ScriptWorld environment is different from the existing text-world-based environments (e.g., Text World, Jericho, TWC, QAit). The primary novelty of ScriptWorld comes from the inclusion of realistic scenarios made by leveraging ESDs written by human annotators, and this requires procedural knowledge to solve the game. The complexity (Table  1) of the ScriptWorld is much more than the existing environments, requiring the agent to remember past events and actions. We provide more details about ScriptWorld and compare it with other environments in App. A.</p>
<p>RL Baselines</p>
<p>In the ScriptWorld environment, for every state, the environment returns a sample of a possible set of choices.</p>
<p>Since these choices provide feedback related to the current state, the agent must keep track of all the observations received after a particular choice. This property typically resembles the Partially Observable Markov decision processes (POMDP) [Kaelbling et al., 1998], where the agent can never observe the complete state of the environment. Formally, ScriptWorld is defined by (S, A, Ω, R, γ), where S is the set of environment states (nodes in the scenario graph), and A is the set of all actions (choices), Ω is the set of observations, i.e., description of various actions, R is the reward obtained and γ is the discount parameter. The goal of an agent is to learn a policy π(a | s), i.e., a mapping from a set of observations to actions leading to an optimal choice in a particular  B. Some of the other existing works for language-based RL algorithms use knowledge-based agents. As these KBs do not directly adapt to our setting, we could not experiment with these approaches. In the future, we would explore how to make use of external knowledge to incorporate into the agent. an extensive set of experiments considering various combinations of language model embeddings and popular RL algorithms. Due to space limitations, we report the primary findings here, and the remaining are discussed in the App. D. Table 2 shows the performance of various RL algorithms in all the scenarios. The performance score is the score (total reward) achieved by an agent till the point of termination.</p>
<p>Experiments, Results and Analysis</p>
<p>As ScriptWorld was designed, keeping flexibility the primary feature, in Table 2, we report the performance of RL algorithms using multiple flexibility settings, i.e., with/without handicap and action choices = 2. The performance of algorithms with a handicapped version of the environment seems to be easier when compared to a non-handicapped version, depicting the choice of keeping the handicap feature to be useful. For settings without any handicap provided, we found the RPPO algorithm to beat other RL algorithms by a significant margin. Fig. 4 shows the performance of algorithms over multiple episodes, depicting the convergence rate. We observe that RPPO convergence is faster at a higher score, and DQN seems unstable during initial episodes. We also plot performance curves for all the scenarios in App. D. As our RL framework combines language embeddings with RL algorithms, we also highlight the effect of different language model embeddings. We choose RPPO for reporting performance with different language models, as in extensive experimentation, we found RPPO to perform better than other RL algorithms on multiple environment settings. Fig. 5   Performance on different choice settings: To benchmark the flexibility feature of choosing the number of actions in the environment setting, we also report the results for RPPO on various numbers of actions. Fig. 6 shows the training curves for settings with choices = 2, 3, 4, 5, highlighting the increasing difficulty level as the number of choices in the environment increases. We observe an interesting trend, the occurrence of a performance dip in all the scenarios for different episode numbers. Notice the performance dip in Fig. 6 for all the runs with varying numbers of choices. As can be observed, the episode for performance dip increases with the increasing number of choices in the environment. We study this behavior of RL algorithms in detail by analyzing the trajectory followed by the RL algorithms. Fig. 7 shows the percentage coverage of scenario graph nodes along with rewards. The point for a maximum dip (after which the algorithm starts improving the score) directly coincides with the increasing percentage of node coverage; we speculate that the algorithm begins developing a mapping for each node after the entire graph exploration and works on improving the node repre-Training Scenario</p>
<p>Performance on other Scenarios</p>
<p>Airplane Bath Bicycle Bus Cake grocery Haircut Library Train Tree -24.78 -15.07 -5.02 9.97 -23.39 -26.51 -20.85 -16.29 2.14 -21.78 Train - 11.31 -13.22 -9.52 5.44 -4.42 0.22 -10.97 -6.79 9.56 -0.59  sentation in the later episodes. Though the graph coverage percentage is higher, it still remains a difficult task to optimize for correct choice as the number of paths in the graph is huge, and the choices generated for each node are random, making each scenario node different at different time steps.</p>
<p>Bus</p>
<p>Discussion and Future Directions</p>
<p>ScriptWorld provides a suitable benchmark to test different settings as it provides flexibility to adjust the game's complexity. The environment has certain limitations. For example, currently, the environment provides actions available at any state in the form of choices and does not allow the agent to generate actions in free-form text. This limitation is also there in the current parser based text-games that restrict the vocabulary size and sentence constructions that an agent can use for interaction. Parsing and understanding free-form text is a non-trivial task for the current state-of-the-art NLP technologies. In the future, we plan to develop a parser-based version (allowing free-form text) of the game, making use of LLMs. ScriptWorld 's current version only has 10 scenarios. This is mainly due to limitations from the DeScript corpus. In future work, we will try to address this by including more daily scenarios. Experiments show that agents struggle in no handicap setting since they do not have any prior knowledge about the real world. It would be interesting to incorporate external knowledge into agents in the future and explore the possibility of including human feedback for learning a new scenario. Alternatively, another idea to explore would be to allow agents to gather information about a task from the internet via search or by probing large language models. Including multiple diverse scenarios in the proposed environment can facilitate the validation of generalization and language understanding capabilities in fields like continual learning, where a single algorithm learns various tasks without catastrophic forgetting [Nguyen et al., 2019].</p>
<p>Conclusion</p>
<p>In this paper, we present a novel approach to building a textbased game environment (ScriptWorld) involving different daily scenarios. This is a step towards training RL agents to develop NLU capabilities and commonsense knowledge about the real world. We perform an extensive set of experiments. Our experiments and analysis not only explore the environment in RL setting but also open up new ways in which the environment is helpful for the research community. Tables   4 Statistics of ScriptWorld environment . . 11 5 The table contains the number of nodes in compact graphs for different scenarios and negative sampling distance (minimum distance either forward or backward from where wrong actions will be sampled). . . . . . . . 11 6 The table shows a comparison between different existing text-based environments and ScriptWorld. . . . . . . . . . . . . . . . 11 A ScriptWorld Details</p>
<p>List of</p>
<p>A.1 ScriptWorld Statistics and Comparison with Other Environments</p>
<p>The statistics of the ScriptWorld environment are provided in Table 4. Table 5 shows the number of compact nodes in each scenario along with the negative sampling distance used for each scenario. The primary novelty of the created RL environment comes from the use of real-world collected scripts dataset in contrast to artificially made text-based games. We provide a brief feature comparison of the proposed ScriptWorld environment with existing text-based environments in Table 6.     "DeScript"-dataset. "DeScript" provides annotations in two aspects 1) the annotations group the actions belonging to a specific event together (compact graph's node in our case) and 2) creating the alignment of these events for multiple scripts. Hence, we used the provided alignments to create a compact graph first and later to consider the multiple sub-paths to enrich it further(calling it a scenario graph). Moreover, another use of compact graphs would be in future work, as the RL algorithms can be trained to learn specific sub-tasks. E.g., if multiple scenarios have the same sub-tasks(events or sequence of events), the same knowledge can be transferred across multiple scenarios like Riding a Bus or Riding a Train. Also, the environment's reward can be set in such a way that rewards more on learning the generic events rather than the scenario-specific events.</p>
<p>A.2 Compact to Scenario Graph Creation</p>
<p>A.3 Graphs for Scenarios</p>
<p>The compact graphs for different scenarios are shown in Figures 22,25,21,26,29,30,24,23,28, and 27. </p>
<p>A.4 ScriptWorld Game-play examples</p>
<p>B RL Algorithms Details</p>
<p>B.1 RL agent Details</p>
<p>We explore multiple popular RL algorithms for baseline experiments. We formulate the RL framework by combining SBERT with RL algorithms. The observation-specific features are generated using a pre-trained SBERT model and used as input to the RL algorithms. We stick to the official implementations of StableBaselines3 1 as our codebase and build upon it to benchmark the proposed ScriptWorld environment.  ., 2017] algorithm that combines recurrent policies with the help of LSTM layers keeping the rest of the algorithm the same. As the ScriptWorld environment requires a continuous flow of several subtasks/actions for completing the required tasks, the recurrent nature of RPPO helps perform better in the environment.</p>
<p>C Evaluation Metrics</p>
<p>We use standard reward/scores vs. episodes as evaluation metrics for comparing the performance of multiple RL algorithms. All the results are averaged over 5 training iterations, with mean and standard deviations plotted on the plots. For the generalization experiment 5 (Generalization across Scenarios), we train on one scenario and infer on all the scenarios by averaging performance over 100 runs. Fig. 14 shows the average performance with standard deviations plotted as an error plot. Fig. 15, Fig. 16, Fig. 17, and Fig. 18 shows average performance scores plotted in a heatmap for a clear representation.</p>
<p>D Additional Results And Analysis</p>
<p>D.1 Additional Experiments with Environment</p>
<p>To examine the performance of the best-performing RL algorithm RPPO on ScriptWorld environment, we report the results of RPPO in various settings. Fig. 9 and Fig. 10 show a comparison of learning curves for all the scenarios in ScriptWorldon with and without handicap settings, respectively. The setting without handicaps shows higher variance and poor performance than the one with the handicap.</p>
<p>We also report additional results on the effect of increasing the number of choices in environment settings for other RL algorithms. Fig. 11 shows the performance of the PPO algorithm in the scenario Repairing a Flat Bicycle Tire. The plot shows a similar trend as reported in the main paper for RPPO, with a high correlation between the increasing level of difficulty in the environment and the number of choices provided to the agent. We also observe a similar performance dip trend as observed in RPPO. (performance dip analysis and details in the Section 5)</p>
<p>D.2 Performance with various LMs</p>
<p>To explore the effect of various language models, we do an extensive set of experiments with multiple SBRT embeddings. Overall, language model embeddings perform better than the GloVe(non-contextual) embeddings. Fig. 5 and Fig. 12 compare the performance of RPPO and PPO for various LMs respectively, on the same scenario Repairing a Flat Bicycle Tire. We also compare the performance of PPO for another scenario Taking a Bath in Fig. 13 and observe similar trends. We also observe that among various LMs, the models trained with paraphrases perform better than the ones trained on sentences. We speculate that since the choices in the games have paraphrases like "pick up a dish," the embeddings generated by the paraphrases ones are a better choice for the environment.</p>
<p>D.3 Generalization among Scenarios</p>
<p>To provide a clear idea about the generalization of an RL algorithm among the scenarios, we perform a comprehensive set of experiments by considering all possible combinations. We train the best-performing RL algorithm RPPO on all the scenarios and validate its generalization by testing the performance on all the other scenarios. As done in RL experimentations, we consider multiple seeds for testing and report the average performance over 100 test runs. Fig. 14 shows the average performance with standard deviations plotted as an error plot. We report the generalization results on multiple settings of the environment. Fig. 15 shows the performance of the RPPO algorithm on setting (with handicap, choices=2). We observe that the algorithms show generalizations in a few of the similar environments, like Riding a Bus and Riding a Train for this setting. However, for other difficult settings with more number choices (Fig. 17) and without handicaps shown in Fig. 16 and Figure 18, the generalization results are less prominent.  -7 -13.22 Bath 0.22 Grocery 0 -0.59 -11.31 Airplane -12 -4.42 Cake 5.44 Bus 5 - 10.97  Test Scenarios 9.54 -10.38 -23.13 -24.78 -12.61 -13.79 -4.59 -21.98 -11.31 -13.86 -11.09 9.45 -16.42 -15.07 -14.88 -22.66 -8.15 -11.03 -13.22 -16.14 -18.19 -10.81 9.74 -5.02 0.35 -14.57 -6.09 -7.29 -9.52 -11.89 -0.76 -5.31 -12.86 9.97 -7.03 -4.70 -8.67 -10.10 5.44 -6.40 -13.06 -11.00 -7.39 -23.39 9.78 -15.49 -9.73 -17.58 -4.42 -21.07 -5.07 -13.03 -18.23 -26.51 -8.83 9.71 0.81 -6.66 0.22 -12.17 -4.14 -11.83 -13.66 -20.85 -9.98 -15.15 9.63 -7.87 -10.97 -15.11 0.86 -21.13 -9.43 -16.29 -3.56 -6.50 -1.55 9.89 -6.79 -22.16 -2.88 -7.07 -11.73 2.14 -6.26 -6.40 -12.40 -15.85   Test Scenarios 6.85 -20.26 -29.40 -14.21 -44.11 -40.35 -15.94 -25.86 -11.14 -19.61 -19.23 6.35 -20.38 -21.70 -15.67 -17.41 -13.60 -23.32 -14.19 -20.74 -24.36 -21.60 7.85 -22.58 -15.94 -9.43 -22.59 -20.21 -8.19 -16.00 -7.06 -5.55 -13.67 9.32 -10.99 -6.76 -40.39 -14.50 -1.45 -15.47 -33.12 -19.08 -8.89 -18.91 7.18 -14.10 -19.60 -11.15 -10.16 -9.79 -22.89 -26.42 -24.20 -11.52 -28.77 8.79 -16.53 -11.23 -14.30 -19.79 -27.15 -21.57 -14.10 -16.50 -22.97 -11.48 6.32 -11.43 -15.42 -18.31 -8.23 -13.37 -13.98 -6.52 -24.85 1.55 -36.87 8.41 -5.42 -19.80 -5.09 -48.82 -9.62 -29.17 -15.53 -11.58 -30.11 -22.39 8.19 -10.33 -27.93 -9.83 -11.45 -20.55 -17.09 -9.92 -20.03 -14.15 -6.80 8.88  Test Scenarios 7.54 -12.31 -25.16 -33.29 -16.40 -15.21 -13.90 -14.04 -48.89 -15.42 -35.34 6.24 -13.88 -20.07 -13.60 -16.00 -19.02 -12.90 -25.99 -15.66 -25.81 -14.90 8.86 -17.51 -23.40 -15.03 -18.44 -15.19 -26.75 -25.96 - 16.36 -11.79 -14.33 9.41 -26.75 -11.93 -12.80 -13.96 -27.41 -11.51 -45.96 -13.41 -19.64 -16.34 8.19 -16.61 -16.10 -27.17 -77.31 -32.33 -13.54 -18.64 -12.84 -16.28 -13.45 8.75 -15.63 -22.19 -18.06 -11.75 -17.19 -14.43 -12.02 -20.96 -16.22 -15.37 8.77 -22.33 -16.14 -12.29 -22.51 -16.19 -13.18 -17.41 -25.18 -11.44 -25.71 9.01 -14.39 -11.11 -19.12 -15.43 -12.27 -19.91 -18.12 -12.00 -10.64 -15.62 6.35 -10.67 -30.84 -15.93 -39.52 -15.42 -28.14 -37.49 -97.43 -19.09 -43.32  Test Scenarios 0.81 -11.13 -14.21 -32.91 -11.58 -11.85 -10.56 -16.65 -58.94 -10.26 -57.54 -5.24 -16.80 -15.92 -11.46 -12.40 -40.19 -13.96 -19.95 -14.63 -20.66 -11.47 7.12 -19.23 -13.98 -11.84 -16.31 -14.78 -30.14 -14.69 -14.85 -11.36 -12.63 9.22 -19.21 -11.58 -11.02 -15.73 -69.35 -10.60 -24.26 -12.27 -20.56 -16.62 5.17 -12.23 -17.39 -29.95 -112.97 -10.77 -13.53 -13.30 -14.69 -18.02 -10.75 8.03 -14.79 -27.29 -13.34 -11.94 -20.30 -11.75 -10.83 -19.70 -13.34 -12.26 4.61 -32.05 -18.04 -13.20 -13.82 -16.60 -10.77 -15.30 -15.67 -11.75 -12.48 8.48 -16.96 -10.52 -14.63 -14.20 -11.64 -25.70 -14.85 -13.74 -10.22 -14.10 2.02 -10.08 -23.74 -14.09 -32.73 -12.86 -12.83 -16.47 -142.28 -32.84 -77.26 8.82 </p>
<p>Library</p>
<p>Tree</p>
<p>Figure 1 :
1Different descriptions for the Washing Dishes script scenario.</p>
<p>Figure 2 :
2The figure shows a simplified version of the scenario, Get Medicine, and the process of creating an environment graph (right diag.) from the ESDs (left diag.) and aligned events (middle diag.) for the scenario. The green directed edges in the environment graph represent the correct paths, and the red edges denote the environment transition when a wrong option is selected.</p>
<p>Figure 3 :
3The figure shows the "compact graph" created for the scenario Going on a Train.</p>
<p>Figure 4 :
4The figure shows the performance comparison of multiple RL algorithms on scenario Repairing a Flat Bicycle Tire on setting (without handicap, choices=2). Paraphrase Albert Small V2 is used as the LM. The plot shows moving average of performance curves across various episodes state. In some algorithms (e.g., DQN: Deep Q-Network), instead of learning the policy, the agent learns q-values, which can reveal the policy. Formally, q-value (q-function) Q π (s, a) is the expected cumulative return if an agent starts from state s and takes action a and thereafter follows a policy π. Recent developments in RL have proposed an approximation of π(a | s)/q-value via a parameterized model that takes state (features) and actions (features) as input and produces the π(a | s)/q-value as the output[Sutton and Barto, 2018]. We follow the same approach. Recently, Language Models (LM) have shown promising results in almost all tasks in NLP (e.g., [Sancheti and Rudinger,  2022]). For the RL baselines for the ScriptWorld environment, we consider using pre-trained SBERT language models [Reimers and Gurevych, 2019] as a source of prior real-world knowledge, which could be used directly by an RL algorithm to solve the environment. We consider a generalized scheme where a pre-trained language model extracts information from observations, i.e., the features extracted (h i = LM(c i )) from the available set of choices c ∈ {c 1 , .. . , c n  }) is used by the RL algorithms as input features. The pre-trained language model generates embeddings (h i ) corresponding to each of the provided n options. The obtained embeddings are concatenated (O) and passed as input to the RL algorithm, i.e., c ∈ {c 1 , . . . , c n }; h i = LM(c i ) O = h 1 ⊕ h 2 ⊕ . . . ⊕ h n Subsequently, the RL framework generates π(a | s)/Q values for the available set of actions. With the help of this generalized architecture, we run a detailed set of experiments with combinations of multiple language models and different RL algorithms. In particular, we use DQN [Mnih et al., 2013], A2C [Mnih et al., 2016], PPO [Schulman et al., 2017], and RPPO: Recurrent PPO (PPO + LSTM). More details about RL agents, training, and other settings are provided in App.</p>
<p>RLFigure 5 :
5Agents Performances: To benchmark the performance of existing RL algorithms on ScriptWorld we perform The figure shows the performance of the RPPO algorithm with various language models on scenario Repairing a Flat Bicycle Tire on setting (with handicap, choices=2). (highlighting the importance of LMs (contextual embeddings) over GloVe (non-contextual)).</p>
<p>Figure 6 :Figure 7 :
67The figure shows the performance of RPPO algorithm on scenario Repairing a Flat Bicycle Tire (without handicap) on multiple choice settings, The figure shows the performance of RPPO algorithm on scenario Repairing a Flat Bicycle Tire (without handicap) on choices, 3 and, 5 with respective node coverages across learning. The increasing coverage slope (green) and the performance dip (blue) coincide in both settings highlighting the role of graph coverage in algorithm's learning.performance of RPPO on all scenarios trained on one scenario. We observe that the RPPO algorithm generalizes more across similar scenarios e.g., between Train and Bus (more details in App. D). Results obtained in this experiment also open up new research directions like test-time domain adaptation and continual learning.</p>
<p>Fig. 8 Figure 8 :
88shows an example of a compact graph with entry and exit nodes ( §3). The motivation for creating the compact graph was due to the type of annotations provided in the Figure shows a dummy example of a compact graph split into scenario graph.</p>
<p>Figures
19 and 20  show sample game play for the scenarion Going Grocery Shopping.</p>
<p>DQN: DQN algorithm [Mnih et al., 2013]  is trained to learn the action selection policies directly from the SBERT dimensional input. We use MLP (multilayer perceptron) variant in the DQN architecture and finetune the model parameters by manual inspections. A2C: A2C (Advantage Actor-Critic)[Mnih et al., 2016]  is a synchronous version of the A3C policy gradient method, which uses gradient descent for the optimization of feedforward parameters. A2C being synchronous, averages over all the actors for updates after the actors finish the segment. PPO: PPO (Proximal Policy Optimization)[Schulman et al.,  2017]  uses trust regions to improve actors' performance, split the optimizations to multiple workers, like in A2C, and introduces clipping to avoid large updates keeping updated policies to be closer to the old policies. R-PPO: RPPO (Recurrent-PPO) is the recurrent version of the PPO [Schulman et al</p>
<p>Figure 9 :Figure 10 :Figure 11 :Figure 12 :Figure 13 :
910111213The figure shows the performance of RPPO algorithm on all scenarios with setting (with handicap, choices=2). The figure shows the performance of RPPO algorithm on all scenarios with setting (without handicap, choices=2). The figure shows the performance of PPO algorithm on scenario Repairing a Flat Bicycle Tire (without handicap) on multiple-choice settings, 2, 3, 4, 5 respectively. The plot shows the increasing level of difficulty in the environment with an increase in the number of choices provided to the agent. The figure shows the performance of PPO algorithm with various language models on scenario Repairing a Flat Bicycle Tire on setting (with a handicap, choices=2). The figure shows the performance of PPO algorithm with various language models on scenario Taking a Bath on setting (with handicap, choices=2).</p>
<p>Haircut 10 Figure 14 :
1014The figure shows the performance of RPPO algorithm trained on scenario Going on a Train on other scenarios with setting (with handicap, choices=2).airplane bath bicycle bus cake grocery haircut library train</p>
<p>Figure 15 :
15The figure shows the performance of RPPO algorithm on setting (with handicap, choices=2), trained on one scenario and tested on all scenarios.airplane bath bicycle bus cake grocery haircut library train</p>
<p>Figure 16 :
16The figure shows the performance of RPPO algorithm on setting (without handicap, choices=2), trained on one scenario and tested on all scenarios.airplane bath bicycle bus cake grocery haircut library train</p>
<p>Figure 17 :
17The figure shows the performance of RPPO algorithm on setting (with handicap, choices=5), trained on one scenario and tested on all scenarios.airplane bath bicycle bus cake grocery haircut library train</p>
<p>Figure 18 :
18The figure shows the performance of RPPO algorithm on setting (without handicap, choices=5), trained on one scenario and tested on all scenarios.</p>
<p>Figure 19 :Figure 20 :Figure 21 :Figure 22 :Figure 23 :Figure 24 :Figure 25 :Figure 26 :Figure 27 :Figure 28 :Figure 29 :
1920212223242526272829The figure shows a sample game-play (with hint) for scenario Going Grocery Shopping. (the game-play sequences are left to right and top to bottom.) The figure shows a sample game-play (without hint) for scenario Going Grocery Shopping. (the game-play sequences are left to right and top to bottom.) The figure shows the compact graph created for the scenario Flying in an Airplane The figure shows the compact graph created for the scenario The figure shows the compact graph created for the scenario Repairing a Flat Bicycle Tire The figure shows the compact graph created for the scenario Riding on The figure shows the compact graph created for the scenario Baking a Cake The figure shows the compact graph created for the scenario Going Grocery Shopping The figure shows the compact graph created for the scenario Getting a Haircut The figure shows the compact graph created for the scenario Borrowing a Book from the Library The figure shows the compact graph created for the scenario Going on a Train</p>
<p>Figure 30 :
30The figure shows the compact graph created for the scenario Planting a Tree</p>
<p>Text Based[He et al., 2016]. The player issues a command in Parser-based games by typing in the input, and an inbuilt parser parses it. In Hypertext-based games, the player issues a command by selecting one of the Hyperlinks present in the prompt. In choice-based games, the player chooses the command from a list of options in ad-Other Text-based game frameworks have been proposed, such as TWC (TextWorld Commonsense)[Murugesan et al., 2020], and Question Answering with Interactive Text (QAit) [Yuan et al., 2019] build on TextWorld. Similarly, Hausknecht et al., 2020 have introduced a new framework called Jericho, which facilitates using man-made Interactive Fiction Games as learning environments for agents to train and learn.: https://github.com/ 
Exploration-Lab/ScriptWorld. 
• We propose and experiment with a battery of Rein-
forcement Learning (RL) agents based on pre-trained 
Language Models (LM) as baselines for solving the 
ScriptWorld environment. The experiments show 
that pre-trained LMs, when combined with RL agents, 
give reasonable performance, pointing towards scope for 
improvement and inclusion of prior knowledge. </p>
<p>2 Related Work </p>
<p>Text Based Games. Text-based games are divided into 
three main categories based on how an agent/player might </p>
<p>issue (take) commands (actions): Parser-based, Choice Base, 
and Hyper dition to the state description. Parser-based games are lim-
ited since these can only parse sentences that adhere to pre-
defined grammar and vocabulary. Giving flexibility for free-
form text suffers from the exponentially increasing action 
space. ScriptWorld uses choice-based approach (also see 
 §6). Moreover, in general, choice-based games are more 
popular among humans than parser-based games [He et al., 
2016]. Côté et al., 2018 have introduced TextWorld sand-
box environment, a Python-based framework in which the 
user can build parser-based game worlds of varying difficulty 
along with in-game objects and goal states while monitoring 
states and assigning rewards. Language diversity and com-
plexity of action space are limited in TextWorld. In con-
trast, ScriptWorld (created using human written texts) 
overcomes these issues by generating ample alternative path-
ways to complete a task. The complexity and variability in 
ScriptWorld help to develop better language understand-
ing capabilities in agents. Scripts. Scripts have been an active area of research for 
the last four decades. As evident from the definition ( §1), 
scripts encapsulate commonsense and procedural knowledge 
about the world and hence are an ideal source for training 
agents to learn about the world. Several computational mod-
els have developed for modeling script knowledge, inter alia, </p>
<p>Table 1 :
1The table compares graphs of different scenarios present in ScriptWorld. Deg. represents the average degree for the nodes in the scenario graph.of the game to equip the agent with language learning capa-
bilities as well as acquire real-world knowledge. Our baseline 
agents come close to Singh et al., 2022. </p>
<p>3 ScriptWorld Environment </p>
<p>ScriptWorld tries to bridge the gap between real-world 
scenarios (via Scripts) and text-based games for RL by creat-
ing a suitable environment. We take into consideration three 
design choices for developing the environment: 1) Complex-
ity: The game environment should be complex enough to 
test an RL algorithm's capacity to capture, understand and 
remember reasonable steps required for performing a daily 
chore. 2) Flexibility: For an environment to help develop 
and debug RL algorithms, it becomes imperative to consider 
flexibility as a feature. The environment should be flexible 
regarding difficulty levels and handicaps (hints) to provide 
a good test bench for reinforcement learning algorithms. 3) 
Relation to Real-World scenarios: The environment should 
consist of activities/tasks grounded in the real world and well 
understood among humans. </p>
<p>DeScript. Given the nature of Script knowledge, we use a 
scripts corpus referred to as DeScript [Wanzare et al., 2016] 
for creating ScriptWorld environment. DeScript is a cor-
pus having a telegram-style sequential description of a sce-
nario in English (e.g., baking a cake, taking a bath, etc.) De-
Script is created via crowd-sourcing. For a given scenario, 
crowd-workers write a point-wise and sequential short de-
scription of various events involved in executing the scenario 
(this one complete description is called an ESD (Event Se-
quence Description)). Fig 1 shows an example of 5 ESDs 
for the Washing Dishes scenario. DeScript collects data 
for 40 daily activities (scenarios), and 100 ESDs (written by 
different crowd-sourced workers) are collected for each sce-
nario. Additionally, for a given scenario, semantically similar 
events from different ESDs are manually aligned by human 
annotators (more details about data collection and annotations 
are present in Wanzare et al., 2016). The alignment annota-
tion is done for 10 scenarios (Table 1 gives the list of sce-
narios). In the present version of ScriptWorld, we only 
include these 10 scenarios with gold alignment. Another line 
of work can be to consider sequence alignment algorithms 
[Chatzou et al., 2016] to align sequences for the remaining 
30 scenarios. However, as observed in initial experiments, 
the error rate of alignment algorithms gets propagated to the 
graph formation leading to a less reliable environment. We </p>
<p>leave the automatic alignment of the remaining 30 scenarios 
for future work. The gold alignments in the DeScsript corpus 
contain cluster annotations of similar events across multiple 
ESDs into a single abstract, generalized event. For example, 
Fig. 2 depicts the scenario, Get Medicine, where similar 
events from ESDs written by different people are clustered to 
form generalized event categories. Further, the combined set 
of events and the relation between the ESDs is leveraged to 
construct a graph (as explained later) where each node rep-
resents an abstract event. </p>
<p>alignment in DeScript results in events in different ESDs that are semantically similar, getting linked to each other, i.e., clustered together. For example, for the Washing Dishes scenario, events "put dishesin sink" (e </p>
<p>(E W ash </p>
<p>1 </p>
<p>) 
1 </p>
<p>) in E W ash </p>
<p>1 </p>
<p>and "take dirty dishes to sink" </p>
<p>(e </p>
<p>(E W ash </p>
<p>2 </p>
<p>) 
1 </p>
<p>Table 2 :
2The table shows performance scores (averaged over multiple runs) of various agents for all the scenarios (number of choices = 2). The number in brackets shows the standard deviation of the score. Paraphrase Albert Small V2 is used as the LMdecide-where-to-go </p>
<p>other-pack </p>
<p>check-time-table </p>
<p>get-train-station </p>
<p>get-tickets </p>
<p>other-park-car </p>
<p>other-go-ticket-counter </p>
<p>other-wait-queue </p>
<p>enter-station </p>
<p>other-find-platform </p>
<p>wait </p>
<p>get-on </p>
<p>get-platform </p>
<p>other-wait-to-board </p>
<p>other-walk </p>
<p>other-get-out-car </p>
<p>train-arrives </p>
<p>other-stow-luggage </p>
<p>find-place </p>
<p>conductor-checks </p>
<p>spend-time-train </p>
<p>get-off </p>
<p>arrive-destination </p>
<p>Victory </p>
<p>other-leave-station </p>
<p>other-take-luggage </p>
<p>other-get-in-car </p>
<p>Table 3 :
3The table shows performance on RPPO algorithm trained one scenario and evaluated on all scenarios. RPPO trained on Bus performs better on Train and vice versa (highlighted in red), depicting the generalization across scenarios.</p>
<p>Table 4 :
4Statistics of ScriptWorld environmentScenario No. of compact Nodes Negative sampling distance </p>
<p>Bath 
32 
7 
Cake 
29 
6 
Airplane 
36 
8 
Shopping 
32 
8 
Train 
25 
6 
Tree 
24 
5 
Bus 
21 
5 
Bicycle 
28 
6 
Library 
22 
5 
Haircut 
39 
9 </p>
<p>Table 5 :
5The table contains the number of nodes in compact graphs for different scenarios and negative sampling distance (minimum distance either forward or backward from where wrong actions will be sampled).Environments 
Game Type (basis) 
Modalities </p>
<p>Text World 
Parser/ Choice 
Text 
TWC 
Parser 
Text 
Jericho 
Template 
Text 
Pyfiction 
Hypertext 
Text 
Nethack<em> 
Visual and Text 
Visual, Text 
3D World</em> 
Choice 
Visual, Text 
VizDoom* 
Choice 
Visual, Text 
QAit 
Parser 
Text 
Evennia 
Parser 
Text 
ScriptWorld (ours) 
Choice 
Text </p>
<p>Table 6 :
6The table shows a comparison between different existing text-based environments and ScriptWorld.
https://github.com/DLR-RM/stable-baselines3
Appendix. . . . . . . . . . . . . . . 12 D.2Performance with various LMs .. . . 12 D.3Generalization among Scenarios .. . 12List ofFigures
Bootstrapped Q-learning with Context Relevant Observation Pruning to Generalize in Text-based Games. [ References, Adhikari, 10.1093/bib/bbv099Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13). Marc-Alexandre Côté,Ákos Kádár, Xingdi Yuan, Ben A. Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew J. Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischlerthe Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)Matthew HausknechtAAAIReferences [Adhikari et al., 2020] Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikulas Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L. Hamilton. Learning Dy- namic Belief Graphs to Generalize on Text-Based Games. In NeurIPS, 2020. [Adolphs and Hofmann, 2020] Leonard Adolphs and Thomas Hofmann. LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games. In AAAI, 2020. [Ammanabrolu and Hausknecht, 2020] Prithviraj Am- manabrolu and Matthew Hausknecht. Graph Constrained Reinforcement Learning for Natural Language Ac- tion Spaces. In International Conference on Learning Representations, 2020. [Ammanabrolu and Riedl, 2019] Prithviraj Ammanabrolu and Mark Riedl. Transfer in Deep Reinforcement Learning Using Knowledge Graphs. In Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13), 2019. [Chatzou et al., 2016] Maria Chatzou, Cedrik Magis, Jia- Ming Chang, Carsten Kemena, Giovanni Bussotti, Ionas Erb, and Cedric Notredame. Multiple Sequence Align- ment Modeling: Methods and Applications. Briefings in Bioinformatics, 2016. [Chaudhury et al., 2020] Subhajit Chaudhury, Daiki Kimura, Kartik Talamadupula, Michiaki Tatsubori, Asim Munawar, and Ryuki Tachibana. Bootstrapped Q-learning with Context Relevant Observation Pruning to Generalize in Text-based Games. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. [Côté et al., 2018] Marc-Alexandre Côté,Ákos Kádár, Xingdi Yuan, Ben A. Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew J. Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. TextWorld: A Learning Environment for Text-based Games. In CGW@IJCAI, 2018. [Frermann et al., 2014] Lea Frermann, Ivan Titov, and Man- fred Pinkal. A Hierarchical Bayesian Model for Unsuper- vised Induction of Script Knowledge. In Proceedings of the 14th Conference of the European Chapter of the Asso- ciation for Computational Linguistics, 2014. [Hausknecht et al., 2020] Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. Interactive Fiction Games: A Colossal Adventure. In AAAI, 2020.</p>
<p>Deep Reinforcement Learning with a Natural Language Action Space. arXiv:1710.09867Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. 2017] Felix Hill, Karl Moritz Hermann, Phil Blunsom, and Stephen Clarkthe 54th Annual Meeting of the Association for Computational LinguisticsarXiv preprintUnderstanding Grounded Language Learning Agentset al., 2016] Ji He, Jianshu Chen, Xiaodong He, Jian- feng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep Reinforcement Learning with a Natural Language Action Space. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 2016. [Hill et al., 2017] Felix Hill, Karl Moritz Hermann, Phil Blunsom, and Stephen Clark. Understanding Grounded Language Learning Agents. arXiv preprint arXiv:1710.09867, 2017.</p>
<p>Subsymbolic Natural Language Processing: An Integrated Model of Scripts, Lexicon, and Memory. https:/dl.acm.org/doi/10.5555/3045390.3045594arXiv:1312.5602Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics. Mnih, Adria Puigdomenech Badia, Mehdi Mirzathe 13th Conference of the European Chapter of the Association for Computational LinguisticsDavid Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller; Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray KavukcuogluModi and TitovPlaying Atari with Deep Reinforcement Learning. arXiv preprintProceedings of the Eighteenth Conference on Computational Natural Language Learninget al., 2012] Bram Jans, Steven Bethard, Ivan Vulić, and Marie Francine Moens. Skip N-grams and Ranking Functions for Predicting Script Events. In Proceedings of the 13th Conference of the European Chapter of the Asso- ciation for Computational Linguistics, 2012. [Kaelbling et al., 1998] Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and Act- ing in Partially Observable Stochastic Domains. Artificial Intelligence, 1998. [Küttler et al., 2020] Heinrich Küttler, Nantas Nardelli, Alexander H. Miller, Roberta Raileanu, Marco Selvatici, Edward Grefenstette, and Tim Rocktäschel. The NetHack Learning Environment. In NeurIPS, 2020. [Miikkulainen and Elman, 1993] Risto Miikkulainen and Jeffrey Elman. Subsymbolic Natural Language Pro- cessing: An Integrated Model of Scripts, Lexicon, and Memory. MIT press, 1993. [Mnih et al., 2013] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with Deep Reinforcement Learning. arXiv preprint arXiv:1312.5602, 2013. [Mnih et al., 2016] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asyn- chronous Methods for Deep Reinforcement Learning. In International Conference on Machine Learning, 2016. [Modi and Titov, 2014] Ashutosh Modi and Ivan Titov. In- ducing Neural Models of Script Knowledge. In Proceed- ings of the Eighteenth Conference on Computational Nat- ural Language Learning, 2014.</p>
<p>InScript: Narrative texts annotated with script information. Modi, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). the Tenth International Conference on Language Resources and Evaluation (LREC'16)Modi et al., 2016] Ashutosh Modi, Tatjana Anikina, Simon Ostermann, and Manfred Pinkal. InScript: Narrative texts annotated with script information. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), 2016.</p>
<p>Vera Demberg, Asad Sayeed, and Manfred Pinkal. Modeling Semantic Expectation: Using Script Knowledge for Referent Prediction. Modi, Transactions of the Association for Computational Linguistics. Modi et al., 2017] Ashutosh Modi, Ivan Titov, Vera Dem- berg, Asad Sayeed, and Manfred Pinkal. Modeling Se- mantic Expectation: Using Script Knowledge for Referent Prediction. Transactions of the Association for Computa- tional Linguistics, 2017.</p>
<p>Event Embeddings for Semantic Script Modeling. Ashutosh Modi, Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning. the 20th SIGNLL Conference on Computational Natural Language Learning, 2016] Ashutosh Modi. Event Embeddings for Se- mantic Script Modeling. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, 2016.</p>
<p>Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines. Ashutosh Modi, ; Murugesan, arXiv:1908.01091abs/1908.01091Tal Hassner, Vijay Mahadevan, and Stefano Soatto. Toward Understanding Catastrophic Forgetting in Continual Learning. Simon Ostermann, Michael Roth, Ashutosh Modi, Stefan Thater, and Manfred Pinkal. SemEvalJeffrey Pennington, Richard11Saarland UniversityarXiv preprintProceedings of the 12th International Workshop on Semantic Evaluation. Pennington et al., 2014, 2017] Ashutosh Modi. Modeling Common Sense Knowledge via Scripts. PhD thesis, Saarland University, 2017. [Murugesan et al., 2020] Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrin- maya Sachan, and Murray Campbell. Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines. In AAAI Conference on Artificial Intelligence, 2020. [Narasimhan et al., 2015] Karthik Narasimhan, Tejas Kulka- rni, and Regina Barzilay. Language Understanding for Text-based Games using Deep Reinforcement Learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015. [Nguyen et al., 2019] Cuong V Nguyen, Alessandro Achille, Michael Lam, Tal Hassner, Vijay Mahadevan, and Stefano Soatto. Toward Understanding Catastrophic Forgetting in Continual Learning. arXiv preprint arXiv:1908.01091, abs/1908.01091, 2019. [Ostermann et al., 2018a] Simon Ostermann, Ashutosh Modi, Michael Roth, Stefan Thater, and Manfred Pinkal. MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge. In LREC, 2018. [Ostermann et al., 2018b] Simon Ostermann, Michael Roth, Ashutosh Modi, Stefan Thater, and Manfred Pinkal. SemEval-2018 Task 11: Machine Comprehension Using Commonsense Knowledge. In Proceedings of the 12th In- ternational Workshop on Semantic Evaluation, 2018. [Pennington et al., 2014] Jeffrey Pennington, Richard</p>
<p>Abhilasha Sancheti and Rachel Rudinger. What do Large Language Models Learn about Scripts?. Christopher Socher, Manning ; Karl, Raymond J Pichotta, Mooney, Radford, https:/dl.acm.org/doi/abs/10.5555/1624626.1624649arXiv:1707.06347Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. Keisuke Sakaguchi, Chandra Bhagavatula, Ronan Le Bras, Niket Tandon, Peter Clark, and Yejin Choithe 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing1arXiv preprintFilip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithmsSocher, and Christopher Manning. GloVe: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014. [Pichotta and Mooney, 2016] Karl Pichotta and Raymond J. Mooney. Using Sentence-Level LSTM Language Models for Script Inference. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 2016. [Radford et al., 2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [Regneri et al., 2010] Michaela Regneri, Alexander Koller, and Manfred Pinkal. Learning Script Knowledge with Web Experiments. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, 2010. [Reimers and Gurevych, 2019] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019. [Rudinger et al., 2015] Rachel Rudinger, Vera Demberg, Ashutosh Modi, Benjamin Van Durme, and Manfred Pinkal. Learning to predict script events from domain- specific text. In Proceedings of the Fourth Joint Confer- ence on Lexical and Computational Semantics, 2015. [Sakaguchi et al., 2021] Keisuke Sakaguchi, Chandra Bha- gavatula, Ronan Le Bras, Niket Tandon, Peter Clark, and Yejin Choi. proScript: Partially Ordered Scripts Gener- ation. In Findings of the Association for Computational Linguistics: EMNLP, 2021. [Sancheti and Rudinger, 2022] Abhilasha Sancheti and Rachel Rudinger. What do Large Language Models Learn about Scripts? In Proceedings of the 11th Joint Conference on Lexical and Computational Semantics, 2022. [Schank and Abelson, 1975] Roger C. Schank and Robert P. Abelson. Scripts, Plans, and Knowledge. In Proceedings of the 4th International Joint Conference on Artificial In- telligence, IJCAI, 1975. [Schulman et al., 2017] John Schulman, Filip Wolski, Pra- fulla Dhariwal, Alec Radford, and Oleg Klimov. Prox- imal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Pre-trained Language Models as Prior Knowledge for Playing Text-based Games. 21st International Conference on Autonomous Agents and Multiagent Systems. 2022et al., 2022] Ishika Singh, Gargi Singh, and Ashutosh Modi. Pre-trained Language Models as Prior Knowledge for Playing Text-based Games. In 21st International Con- ference on Autonomous Agents and Multiagent Systems, AAMAS, 2022.</p>
<p>A Crowdsourced Database of Event Sequence Descriptions for the Acquisition of High-quality Script Knowledge. ; Barto, S Richard, Andrew G Barto ; Sutton, Wanzare, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). the Tenth International Conference on Language Resources and Evaluation (LREC'16)MIT press2020Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processingand Barto, 2018] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press, 2018. [Wanzare et al., 2016] Lilian D. A. Wanzare, Alessandra Zarcone, Stefan Thater, and Manfred Pinkal. A Crowd- sourced Database of Event Sequence Descriptions for the Acquisition of High-quality Script Knowledge. In Pro- ceedings of the Tenth International Conference on Lan- guage Resources and Evaluation (LREC'16), 2016. [Yao et al., 2020] Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and Explore: Language Models for Action Generation in Text- based Games. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.</p>
<p>Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and bandit feedback to learn families of text-based adventure games. Xusen Yin, Jonathan May, ; Yuan, arXiv:1908.04777Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingarXiv preprintChris Pal, Yoshua Bengio, and Adam Trischler. Interactive Language Learning by Question Answeringand May, 2019] Xusen Yin and Jonathan May. Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and bandit feedback to learn families of text-based adventure games. arXiv preprint arXiv:1908.04777, 2019. [Yuan et al., 2019] Xingdi Yuan, Marc-Alexandre Côté, Jie Fu, Zhouhan Lin, Chris Pal, Yoshua Bengio, and Adam Trischler. Interactive Language Learning by Question An- swering. In Proceedings of the 2019 Conference on Em- pirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019.</p>
<p>11 9 RPPO All scenarios (handicap, choices=2). Compact to Scenario Graph creationCompact to Scenario Graph creation . . . . . 11 9 RPPO All scenarios (handicap, choices=2) . . 13</p>
<p>RPPO All scenarios (w/o handicap. 213RPPO All scenarios (w/o handicap, choices=2) 13</p>
<p>13 12 PPO diff. ) . . , LM embds. (handicap, choices=2) . 13 13 PPO diff. LM embeds. (handicap, choices=2). 13PPO (handicap, choices 2, 3, 4, 5PPO (handicap, choices 2, 3, 4, 5) . . . . . . 13 12 PPO diff. LM embds. (handicap, choices=2) . 13 13 PPO diff. LM embeds. (handicap, choices=2) 13</p>
<p>Generalization Performance (µ, σ). . . , 13Generalization Performance (µ, σ) . . . . . . 13</p>
<p>Heatmap RPPO (handicap, choices=2). 14Heatmap RPPO (handicap, choices=2) . . . . 14</p>
<p>. Rppo Heatmap, choices=2) . 14Heatmap RPPO (w/o handicap, choices=2) . 14</p>
<p>Heatmap RPPO (handicap, choices=5). 14Heatmap RPPO (handicap, choices=5) . . . . 14</p>
<p>. Rppo Heatmap, choices=5) . 14Heatmap RPPO (w/o handicap, choices=5) . 14</p>
<p>Gameplay "shopping" (handicap, choices=5). 15Gameplay "shopping" (handicap, choices=5) 15</p>
<p>16 21 Compact Graph "airplane. ) . . . . . . . . . . . . . . . . . ; &quot; . . . . . . . . . ; . . . . . . . . . . . ; . . . . . . . . . . ; . . . . . . . . . . . . ; . . . . . . . . . . . ; . . . . . . . . Gameplay ; Choices=5, 23 28 Compact Graph "library. 24 29 Compact Graph "train" . . . . . . . . . . . . 25 30 Compact Graph "tree" . . . . . . . . . . . . 26Gameplay "shopping" (w/o-handicap, choices=5) . . . . . . . . . . . . . . . . . . . 16 21 Compact Graph "airplane" . . . . . . . . . . 17 22 Compact Graph "bath" . . . . . . . . . . . . 18 23 Compact Graph "bicycle" . . . . . . . . . . . 19 24 Compact Graph "bus" . . . . . . . . . . . . . 20 25 Compact Graph "cake" . . . . . . . . . . . . 21 26 Compact Graph "shopping" . . . . . . . . . . 22 27 Compact Graph "haircut" . . . . . . . . . . . 23 28 Compact Graph "library" . . . . . . . . . . . 24 29 Compact Graph "train" . . . . . . . . . . . . 25 30 Compact Graph "tree" . . . . . . . . . . . . 26</p>            </div>
        </div>

    </div>
</body>
</html>
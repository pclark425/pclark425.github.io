<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-570 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-570</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-570</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-4a64ae536639324e314b831208b592dd988b54b9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4a64ae536639324e314b831208b592dd988b54b9" target="_blank">Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Computer Vision</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a simple approach to generate large annotated instance datasets with minimal effort and outperforms existing synthesis approaches and when combined with real images improves relative performance by more than 21% on benchmark datasets.</p>
                <p><strong>Paper Abstract:</strong> A major impediment in rapidly deploying object detection models for instance detection is the lack of large annotated datasets. For example, finding a large labeled dataset containing instances in a particular kitchen is unlikely. Each new environment with new instances requires expensive data collection and annotation. In this paper, we propose a simple approach to generate large annotated instance datasets with minimal effort. Our key insight is that ensuring only patch-level realism provides enough training signal for current object detector models. We automatically ‘cut’ object instances and ‘paste’ them on random backgrounds. A naive way to do this results in pixel artifacts which result in poor performance for trained models. We show how to make detectors ignore these artifacts during training and generate data that gives competitive performance on real data. Our method outperforms existing synthesis approaches and when combined with real images improves relative performance by more than 21% on benchmark datasets. In a cross-domain setting, our synthetic data combined with just 10% real data outperforms models trained on all real data.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e570.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e570.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Poisson blending</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Poisson image editing (Poisson blending)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An image compositing technique that solves a Poisson equation to seamlessly blend a source region into a target image, preserving gradient fields to reduce visible seams and lighting discontinuities at pasted object boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Poisson image editing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Poisson blending / Poisson image editing</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Poisson blending formulates compositing as a boundary-value problem: it copies the gradient field of the source object into the target region and solves a Poisson equation (with Dirichlet boundary conditions) to produce pixel values whose gradients match the source while agreeing with the target at the boundary. The result reduces hard seams and can adjust local lighting/gradients to better integrate the pasted object with the background.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>image processing / computational image editing</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>computer graphics / image processing</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>computer vision dataset synthesis for instance detection</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application as a blending mode within a new data-generation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Used Poisson blending as one of several blending modes when compositing real object masks onto real background scenes; not altered algorithmically, but integrated into a pipeline that generates multiple blended variants of the same scene (i.e., same object placement with different blending). No core mathematical change to Poisson blending was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful: as a single blending mode Poisson blending alone produced lower detection performance relative to some other blending modes (Table 1: Poisson mAP lower than 'No blending' or 'Gaussian blurring'), indicating it can introduce appearance changes detrimental by itself; however, when used as one member of an ensemble of blending methods and combined with multi-blend augmentation (i.e., rendering the same scene with multiple blending modes), it contributed to higher overall performance (All Blend and All Blend + same image rows improved mAP to 72.4 and 73.7 respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Poisson blending can alter local lighting/gradients in ways that create domain shifts from real images (e.g., unnatural lighting transitions), which may confuse detectors if used alone; subtle artifacts and distributional shifts at patch boundaries remain a concern.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of off-the-shelf Poisson blending implementations and the compatibility of the method with simple compositing pipelines; the model's reliance on local patch features meant local smoothing was directly relevant; combining multiple blending modes reduced reliance on any single artifact type.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires the pasted object mask and target background region; computational cost for solving Poisson equation per paste (but feasible at dataset-generation scale); no special sensor or capture equipment beyond the input RGB masks and backgrounds.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High within visual data synthesis tasks — Poisson blending can be applied to many image compositing problems (e.g., synthetic dataset generation for other detection/segmentation tasks), but its efficacy depends on how it is used (alone vs. in ensemble) and on the target model's sensitivity to local image statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>instrumental/technical skills (procedural application of an image-editing algorithm) and explicit procedural steps (how blending is integrated into data generation)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection', 'publication_date_yy_mm': '2017-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e570.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e570.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Denoising-inspired multi-blend augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Denoising autoencoder-inspired multi-blend augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-generation strategy inspired by denoising autoencoders: render the same composite scene multiple times with varying low-level artifacts (different blending modes) so the detector learns to be invariant to those artifacts and focus on object appearance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Extracting and composing robust features with denoising autoencoders</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Denoising-inspired multi-blend augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Borrowing the denoising principle, the method generates multiple noisy/variant versions of the same underlying signal (here: identical object placement) by changing blending parameters and blending algorithms; training on these variants forces the learning algorithm to ignore those low-level 'noise' artifacts and learn invariant features corresponding to object appearance, improving robustness to compositing artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data augmentation / training-data generation strategy (conceptual transfer from unsupervised learning)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>unsupervised representation learning / denoising autoencoders (machine learning)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>supervised dataset synthesis for object/instance detection (computer vision)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (conceptual transfer applied as a specific data-generation protocol)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>The denoising autoencoder idea (train on corrupted variants to learn invariances) was operationalized by generating multiple composite images that differ only in blending type/parameters (Gaussian blur, Poisson blending, simple alpha blending, etc.) and using those variants as training data for Faster R-CNN; this is a conceptual adaptation rather than a mathematical one.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful: empirically shown to improve detection performance substantially — Table 1 reports that rendering the same scene with different blending ('All Blend + same image') improved mAP by about 8 AP points compared to no blending (No blending mAP 65.9 vs All Blend + same image 73.7), demonstrating the efficacy of the denoising-inspired approach.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Need to choose a set of perturbations/blending modes that are realistic and diverse; some individual perturbations (e.g., Poisson alone) can hurt performance if used alone, so selecting and combining variants matters.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>The conceptual similarity between denoising (ignoring corruptions) and desensitizing detectors to compositing artifacts; easy-to-generate image variations (different blends) made implementation straightforward; modern detectors (region-based ConvNets) learn from many examples so multi-variant augmentation is effective.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Ability to programmatically generate multiple compositing variants of the same scene (compute resources); access to object masks and background images; a detector training pipeline to use the augmented data.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Likely generalizable to other visual tasks that suffer from synthetic-to-real artifacts (e.g., segmentation, pose estimation) where generating variant corruptions/noisy renderings can teach invariance; conceptually portable beyond vision where analogous 'corruption ensembles' can be constructed.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles (denoising/invariance) transferred as explicit procedural steps (how to generate variants) — hybrid of interpretive framework and procedural know-how</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection', 'publication_date_yy_mm': '2017-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e570.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e570.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Image compositing of real masks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Compositing real object masks onto real background images for dataset synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic-data-generation procedure that segments real object instances from photographs and pastes them onto other real background images to produce annotated images with bounding boxes, avoiding full 3D rendering by leveraging real object appearance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Synthetic data for text localisation in natural images</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Compositing real object masks onto real backgrounds</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Extract object foreground masks (via segmentation) from images of instances and paste these masks onto unrelated real background images with varying placements, scales, rotations, occlusions, truncations and blending modes to generate large numbers of annotated training images; bounding boxes are available automatically from the mask placement.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data-generation protocol / data augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>computer vision approaches to synthetic data and image compositing (previously applied, e.g., to synthetic text localization and some vision tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>instance detection training data generation (object detection in robotics/indoor scenes)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (applied compositing approaches used in other vision tasks to instance detection with additional augmentations and blending strategies)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Integrated segmentation via an FCN to obtain masks from turntable images (BigBIRD), added multiple blending modes, varied 2D and 3D rotations, occlusion and truncation, and added distractor objects; automated mask extraction pipeline and post-processing (bilateral solver) were introduced to make the method scalable and applicable to unseen objects.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful and practically effective: synthetic-only training produced competitive detection performance on GMU (Table 2: Synthetic mAP 76.2) relative to real-only training on GMU (Real mAP 86.3), and combining synthetic with real images improved performance beyond real-only (Synthetic + Real mAP 88.8), demonstrating complementary gains. On cross-domain Active Vision experiments, combining 10% real + synthetic yielded much higher mAP (43.2) than 10% real alone (15.8).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Naive compositing produced boundary artifacts that models exploited; global scene inconsistency (object placed in physically implausible locations) could be unrealistic though detectors cared more about local patches; need for good-quality masks to avoid artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of real object images with diverse viewpoints (BigBIRD) and real background image sets (UW Scenes); existing segmentation models and blending algorithms; detectors' reliance on local patch realism allowed success despite lack of global scene realism.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Datasets of object images (to extract masks) and background images; segmentation model to extract masks or ground-truth masks (depth-based); compute for generating many composites; detector training pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Broadly generalizable to other detection/recognition tasks where object appearance matters more than global scene layout; can be combined with methods that enforce global consistency for further gains.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and instrumental/technical skills (how to assemble and augment composites), plus some practical/tacit know-how (what augmentations and blending modes are effective)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection', 'publication_date_yy_mm': '2017-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e570.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e570.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FCN trained on depth masks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Foreground/background segmentation via Fully Convolutional Network trained using depth-derived masks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Train an FCN (based on VGG-16) to predict per-pixel foreground vs background labels using masks derived from a depth sensor as ground truth, then use the network to produce object masks for previously unseen instances for compositing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fully convolutional networks for semantic segmentation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>FCN-based foreground/background segmentation trained with depth-derived labels</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Use depth sensor-produced object masks from a set of training instances as supervisory labels to train a Fully Convolutional Network (initialized from a pre-trained segmentation model) to predict foreground (object) vs background for new RGB images; post-process FCN outputs with a fast bilateral solver to refine mask boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / image segmentation procedure</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>RGB-D perception and semantic segmentation (robotics/vision using depth sensors and FCNs)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>automated mask extraction for synthetic dataset generation (vision dataset construction)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (using depth-derived labels to supervise an RGB segmentation model used for a new data-generation purpose)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Trained the FCN on depth-derived masks from BigBIRD turntable captures (using instances not used in evaluation) and combined FCN output with the fast bilateral solver for post-processing; the trained model was used to segment novel object images including some transparent objects where depth alone fails.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful in practice (qualitative evidence): the FCN generalized to unseen objects with modest backgrounds and produced usable masks for compositing; authors note it generalized even to some transparent objects where depth sensors fail qualitatively (Figure 5). No quantitative segmentation metrics were provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Depth sensors fail on transparent objects and can be noisy, so raw depth masks are imperfect; had to use post-processing (bilateral solver) to clean results and careful selection of training instances to encourage generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of a dataset (BigBIRD) with depth-aligned masks for supervision, and pre-trained FCN models (VGG-16) from segmentation tasks; bilateral solver for fast mask cleanup.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to RGB-D captures or any source of pixelwise masks for training; compute for FCN training; post-processing tools for mask refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Method is generalizable where pixelwise labels can be cheaply derived (e.g., from depth or other sensors) to train RGB segmentation models for mask extraction in dataset synthesis pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps (training an FCN with depth-derived labels) and instrumental/technical skills (segmentation model training and mask post-processing)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection', 'publication_date_yy_mm': '2017-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e570.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e570.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-view / 3D rotation augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-view (3D) rotation augmentation using turntable-captured object images (BigBIRD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use turntable multi-view captures of objects to render or sample diverse 3D poses/viewpoints of objects when composing synthetic images, producing better coverage of appearance variations that may be scarce in hand-collected datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bigbird: A large-scale 3d database of object instances</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Multi-view 3D rotation augmentation from turntable dataset</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Exploit a multi-view object capture dataset (BigBIRD) that provides images of each object from many viewpoints; when composing synthetic scenes, sample different 3D viewpoints (and 2D rotations) from these captures to increase viewpoint diversity in the generated training set, including atypical rotations that are under-represented in hand-collected real datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data augmentation / dataset-construction protocol</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>robotics / 3D object dataset capture (turntable multi-view imaging)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>2D instance detection dataset synthesis and augmentation (computer vision)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application/adaptation (use of multi-view capture assets as augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Sampled 3D rotations from BigBIRD images to simulate viewpoints not commonly present in real annotated datasets; combined with other augmentations (occlusion, truncation, blending) to produce training images.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful: adding 3D rotation augmentation yielded measurable gains — Table 1 indicates that disabling 3D rotation reduced performance (No 3D Rotation mAP 68.3 vs All mAP 73.7), suggesting multi-view augmentation contributed >4 AP improvement in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Need for multi-view captures of each object (BigBIRD provided these); mapping between available views and desired continuous viewpoint coverage can be limited by dataset capture density.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of BigBIRD with many images per object across viewpoints and an automated compositing pipeline to place chosen views into scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Multi-view object images (or 3D models), system to sample viewpoints and place corresponding images into scenes, and compute to generate augmented dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Generalizable to other tasks needing viewpoint diversity (pose estimation, recognition) provided multi-view captures or 3D models exist; limited when such captures are unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and instrumental/technical skills (how to sample and incorporate multi-view images into a synthetic-data pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection', 'publication_date_yy_mm': '2017-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e570.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e570.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synthetic distractors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of synthetic distractor objects from existing datasets as clutter/negative examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Add additional object instances (distractors) into composed scenes to emulate real-world clutter and force detectors to discriminate target instances from distractors, improving robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Synthetic distractor insertion</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>While composing scenes, include extra objects (drawn from the same object-image pool) placed with partial overlaps and random positions to model clutter and distractor presence; this increases negative and hard-negative examples during detector training.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data augmentation / negative sampling protocol</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>dataset construction practices in computer vision (negative sampling / hard negative mining)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>instance detection dataset synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (applied as augmentation to composited images)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Used additional BigBIRD instances as distractors during compositing, controlled maximum IOU for occlusion, and combined with other augmentations; integrated distractors in synthetic scenes systematically.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful: presence of distractors improved mAP modestly (Table 1: All + Distractor mAP 76.2 vs All mAP 73.7 — approximately +2.5 AP), indicating improved robustness to clutter.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Need to manage sampling so distractors do not overwhelm scenes or create unrealistic compositions; potential for generating false negatives if distractors occlude targets excessively.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of many object instances (BigBIRD) to serve as distractors and the automated compositing pipeline to place them reproducibly.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Source pool of object images to use as distractors and controls on overlap/cluttering parameters during synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Generalizable across vision tasks where clutter and distractors are relevant (detection, segmentation), but effect size depends on realism and variety of distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>practical know-how (tactical dataset-design choices) and procedural steps (inserting distractors with constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection', 'publication_date_yy_mm': '2017-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Poisson image editing <em>(Rating: 2)</em></li>
                <li>Extracting and composing robust features with denoising autoencoders <em>(Rating: 2)</em></li>
                <li>Synthetic data for text localisation in natural images <em>(Rating: 2)</em></li>
                <li>Fully convolutional networks for semantic segmentation <em>(Rating: 2)</em></li>
                <li>Bigbird: A large-scale 3d database of object instances <em>(Rating: 2)</em></li>
                <li>The fast bilateral solver <em>(Rating: 1)</em></li>
                <li>Synthesizing training data for object detection in indoor scenes <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-570",
    "paper_id": "paper-4a64ae536639324e314b831208b592dd988b54b9",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "Poisson blending",
            "name_full": "Poisson image editing (Poisson blending)",
            "brief_description": "An image compositing technique that solves a Poisson equation to seamlessly blend a source region into a target image, preserving gradient fields to reduce visible seams and lighting discontinuities at pasted object boundaries.",
            "citation_title": "Poisson image editing",
            "mention_or_use": "use",
            "procedure_name": "Poisson blending / Poisson image editing",
            "procedure_description": "Poisson blending formulates compositing as a boundary-value problem: it copies the gradient field of the source object into the target region and solves a Poisson equation (with Dirichlet boundary conditions) to produce pixel values whose gradients match the source while agreeing with the target at the boundary. The result reduces hard seams and can adjust local lighting/gradients to better integrate the pasted object with the background.",
            "procedure_type": "image processing / computational image editing",
            "source_domain": "computer graphics / image processing",
            "target_domain": "computer vision dataset synthesis for instance detection",
            "transfer_type": "direct application as a blending mode within a new data-generation pipeline",
            "modifications_made": "Used Poisson blending as one of several blending modes when compositing real object masks onto real background scenes; not altered algorithmically, but integrated into a pipeline that generates multiple blended variants of the same scene (i.e., same object placement with different blending). No core mathematical change to Poisson blending was reported.",
            "transfer_success": "partially successful: as a single blending mode Poisson blending alone produced lower detection performance relative to some other blending modes (Table 1: Poisson mAP lower than 'No blending' or 'Gaussian blurring'), indicating it can introduce appearance changes detrimental by itself; however, when used as one member of an ensemble of blending methods and combined with multi-blend augmentation (i.e., rendering the same scene with multiple blending modes), it contributed to higher overall performance (All Blend and All Blend + same image rows improved mAP to 72.4 and 73.7 respectively).",
            "barriers_encountered": "Poisson blending can alter local lighting/gradients in ways that create domain shifts from real images (e.g., unnatural lighting transitions), which may confuse detectors if used alone; subtle artifacts and distributional shifts at patch boundaries remain a concern.",
            "facilitating_factors": "Availability of off-the-shelf Poisson blending implementations and the compatibility of the method with simple compositing pipelines; the model's reliance on local patch features meant local smoothing was directly relevant; combining multiple blending modes reduced reliance on any single artifact type.",
            "contextual_requirements": "Requires the pasted object mask and target background region; computational cost for solving Poisson equation per paste (but feasible at dataset-generation scale); no special sensor or capture equipment beyond the input RGB masks and backgrounds.",
            "generalizability": "High within visual data synthesis tasks — Poisson blending can be applied to many image compositing problems (e.g., synthetic dataset generation for other detection/segmentation tasks), but its efficacy depends on how it is used (alone vs. in ensemble) and on the target model's sensitivity to local image statistics.",
            "knowledge_type": "instrumental/technical skills (procedural application of an image-editing algorithm) and explicit procedural steps (how blending is integrated into data generation)",
            "uuid": "e570.0",
            "source_info": {
                "paper_title": "Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection",
                "publication_date_yy_mm": "2017-08"
            }
        },
        {
            "name_short": "Denoising-inspired multi-blend augmentation",
            "name_full": "Denoising autoencoder-inspired multi-blend augmentation",
            "brief_description": "A data-generation strategy inspired by denoising autoencoders: render the same composite scene multiple times with varying low-level artifacts (different blending modes) so the detector learns to be invariant to those artifacts and focus on object appearance.",
            "citation_title": "Extracting and composing robust features with denoising autoencoders",
            "mention_or_use": "use",
            "procedure_name": "Denoising-inspired multi-blend augmentation",
            "procedure_description": "Borrowing the denoising principle, the method generates multiple noisy/variant versions of the same underlying signal (here: identical object placement) by changing blending parameters and blending algorithms; training on these variants forces the learning algorithm to ignore those low-level 'noise' artifacts and learn invariant features corresponding to object appearance, improving robustness to compositing artifacts.",
            "procedure_type": "data augmentation / training-data generation strategy (conceptual transfer from unsupervised learning)",
            "source_domain": "unsupervised representation learning / denoising autoencoders (machine learning)",
            "target_domain": "supervised dataset synthesis for object/instance detection (computer vision)",
            "transfer_type": "adapted/modified for new context (conceptual transfer applied as a specific data-generation protocol)",
            "modifications_made": "The denoising autoencoder idea (train on corrupted variants to learn invariances) was operationalized by generating multiple composite images that differ only in blending type/parameters (Gaussian blur, Poisson blending, simple alpha blending, etc.) and using those variants as training data for Faster R-CNN; this is a conceptual adaptation rather than a mathematical one.",
            "transfer_success": "successful: empirically shown to improve detection performance substantially — Table 1 reports that rendering the same scene with different blending ('All Blend + same image') improved mAP by about 8 AP points compared to no blending (No blending mAP 65.9 vs All Blend + same image 73.7), demonstrating the efficacy of the denoising-inspired approach.",
            "barriers_encountered": "Need to choose a set of perturbations/blending modes that are realistic and diverse; some individual perturbations (e.g., Poisson alone) can hurt performance if used alone, so selecting and combining variants matters.",
            "facilitating_factors": "The conceptual similarity between denoising (ignoring corruptions) and desensitizing detectors to compositing artifacts; easy-to-generate image variations (different blends) made implementation straightforward; modern detectors (region-based ConvNets) learn from many examples so multi-variant augmentation is effective.",
            "contextual_requirements": "Ability to programmatically generate multiple compositing variants of the same scene (compute resources); access to object masks and background images; a detector training pipeline to use the augmented data.",
            "generalizability": "Likely generalizable to other visual tasks that suffer from synthetic-to-real artifacts (e.g., segmentation, pose estimation) where generating variant corruptions/noisy renderings can teach invariance; conceptually portable beyond vision where analogous 'corruption ensembles' can be constructed.",
            "knowledge_type": "theoretical principles (denoising/invariance) transferred as explicit procedural steps (how to generate variants) — hybrid of interpretive framework and procedural know-how",
            "uuid": "e570.1",
            "source_info": {
                "paper_title": "Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection",
                "publication_date_yy_mm": "2017-08"
            }
        },
        {
            "name_short": "Image compositing of real masks",
            "name_full": "Compositing real object masks onto real background images for dataset synthesis",
            "brief_description": "A synthetic-data-generation procedure that segments real object instances from photographs and pastes them onto other real background images to produce annotated images with bounding boxes, avoiding full 3D rendering by leveraging real object appearance.",
            "citation_title": "Synthetic data for text localisation in natural images",
            "mention_or_use": "use",
            "procedure_name": "Compositing real object masks onto real backgrounds",
            "procedure_description": "Extract object foreground masks (via segmentation) from images of instances and paste these masks onto unrelated real background images with varying placements, scales, rotations, occlusions, truncations and blending modes to generate large numbers of annotated training images; bounding boxes are available automatically from the mask placement.",
            "procedure_type": "data-generation protocol / data augmentation",
            "source_domain": "computer vision approaches to synthetic data and image compositing (previously applied, e.g., to synthetic text localization and some vision tasks)",
            "target_domain": "instance detection training data generation (object detection in robotics/indoor scenes)",
            "transfer_type": "adapted/modified for new context (applied compositing approaches used in other vision tasks to instance detection with additional augmentations and blending strategies)",
            "modifications_made": "Integrated segmentation via an FCN to obtain masks from turntable images (BigBIRD), added multiple blending modes, varied 2D and 3D rotations, occlusion and truncation, and added distractor objects; automated mask extraction pipeline and post-processing (bilateral solver) were introduced to make the method scalable and applicable to unseen objects.",
            "transfer_success": "successful and practically effective: synthetic-only training produced competitive detection performance on GMU (Table 2: Synthetic mAP 76.2) relative to real-only training on GMU (Real mAP 86.3), and combining synthetic with real images improved performance beyond real-only (Synthetic + Real mAP 88.8), demonstrating complementary gains. On cross-domain Active Vision experiments, combining 10% real + synthetic yielded much higher mAP (43.2) than 10% real alone (15.8).",
            "barriers_encountered": "Naive compositing produced boundary artifacts that models exploited; global scene inconsistency (object placed in physically implausible locations) could be unrealistic though detectors cared more about local patches; need for good-quality masks to avoid artifacts.",
            "facilitating_factors": "Availability of real object images with diverse viewpoints (BigBIRD) and real background image sets (UW Scenes); existing segmentation models and blending algorithms; detectors' reliance on local patch realism allowed success despite lack of global scene realism.",
            "contextual_requirements": "Datasets of object images (to extract masks) and background images; segmentation model to extract masks or ground-truth masks (depth-based); compute for generating many composites; detector training pipeline.",
            "generalizability": "Broadly generalizable to other detection/recognition tasks where object appearance matters more than global scene layout; can be combined with methods that enforce global consistency for further gains.",
            "knowledge_type": "explicit procedural steps and instrumental/technical skills (how to assemble and augment composites), plus some practical/tacit know-how (what augmentations and blending modes are effective)",
            "uuid": "e570.2",
            "source_info": {
                "paper_title": "Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection",
                "publication_date_yy_mm": "2017-08"
            }
        },
        {
            "name_short": "FCN trained on depth masks",
            "name_full": "Foreground/background segmentation via Fully Convolutional Network trained using depth-derived masks",
            "brief_description": "Train an FCN (based on VGG-16) to predict per-pixel foreground vs background labels using masks derived from a depth sensor as ground truth, then use the network to produce object masks for previously unseen instances for compositing.",
            "citation_title": "Fully convolutional networks for semantic segmentation",
            "mention_or_use": "use",
            "procedure_name": "FCN-based foreground/background segmentation trained with depth-derived labels",
            "procedure_description": "Use depth sensor-produced object masks from a set of training instances as supervisory labels to train a Fully Convolutional Network (initialized from a pre-trained segmentation model) to predict foreground (object) vs background for new RGB images; post-process FCN outputs with a fast bilateral solver to refine mask boundaries.",
            "procedure_type": "computational method / image segmentation procedure",
            "source_domain": "RGB-D perception and semantic segmentation (robotics/vision using depth sensors and FCNs)",
            "target_domain": "automated mask extraction for synthetic dataset generation (vision dataset construction)",
            "transfer_type": "adapted/modified for new context (using depth-derived labels to supervise an RGB segmentation model used for a new data-generation purpose)",
            "modifications_made": "Trained the FCN on depth-derived masks from BigBIRD turntable captures (using instances not used in evaluation) and combined FCN output with the fast bilateral solver for post-processing; the trained model was used to segment novel object images including some transparent objects where depth alone fails.",
            "transfer_success": "successful in practice (qualitative evidence): the FCN generalized to unseen objects with modest backgrounds and produced usable masks for compositing; authors note it generalized even to some transparent objects where depth sensors fail qualitatively (Figure 5). No quantitative segmentation metrics were provided in the paper.",
            "barriers_encountered": "Depth sensors fail on transparent objects and can be noisy, so raw depth masks are imperfect; had to use post-processing (bilateral solver) to clean results and careful selection of training instances to encourage generalization.",
            "facilitating_factors": "Availability of a dataset (BigBIRD) with depth-aligned masks for supervision, and pre-trained FCN models (VGG-16) from segmentation tasks; bilateral solver for fast mask cleanup.",
            "contextual_requirements": "Access to RGB-D captures or any source of pixelwise masks for training; compute for FCN training; post-processing tools for mask refinement.",
            "generalizability": "Method is generalizable where pixelwise labels can be cheaply derived (e.g., from depth or other sensors) to train RGB segmentation models for mask extraction in dataset synthesis pipelines.",
            "knowledge_type": "explicit procedural steps (training an FCN with depth-derived labels) and instrumental/technical skills (segmentation model training and mask post-processing)",
            "uuid": "e570.3",
            "source_info": {
                "paper_title": "Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection",
                "publication_date_yy_mm": "2017-08"
            }
        },
        {
            "name_short": "Multi-view / 3D rotation augmentation",
            "name_full": "Multi-view (3D) rotation augmentation using turntable-captured object images (BigBIRD)",
            "brief_description": "Use turntable multi-view captures of objects to render or sample diverse 3D poses/viewpoints of objects when composing synthetic images, producing better coverage of appearance variations that may be scarce in hand-collected datasets.",
            "citation_title": "Bigbird: A large-scale 3d database of object instances",
            "mention_or_use": "use",
            "procedure_name": "Multi-view 3D rotation augmentation from turntable dataset",
            "procedure_description": "Exploit a multi-view object capture dataset (BigBIRD) that provides images of each object from many viewpoints; when composing synthetic scenes, sample different 3D viewpoints (and 2D rotations) from these captures to increase viewpoint diversity in the generated training set, including atypical rotations that are under-represented in hand-collected real datasets.",
            "procedure_type": "data augmentation / dataset-construction protocol",
            "source_domain": "robotics / 3D object dataset capture (turntable multi-view imaging)",
            "target_domain": "2D instance detection dataset synthesis and augmentation (computer vision)",
            "transfer_type": "direct application/adaptation (use of multi-view capture assets as augmentation)",
            "modifications_made": "Sampled 3D rotations from BigBIRD images to simulate viewpoints not commonly present in real annotated datasets; combined with other augmentations (occlusion, truncation, blending) to produce training images.",
            "transfer_success": "successful: adding 3D rotation augmentation yielded measurable gains — Table 1 indicates that disabling 3D rotation reduced performance (No 3D Rotation mAP 68.3 vs All mAP 73.7), suggesting multi-view augmentation contributed &gt;4 AP improvement in experiments.",
            "barriers_encountered": "Need for multi-view captures of each object (BigBIRD provided these); mapping between available views and desired continuous viewpoint coverage can be limited by dataset capture density.",
            "facilitating_factors": "Availability of BigBIRD with many images per object across viewpoints and an automated compositing pipeline to place chosen views into scenes.",
            "contextual_requirements": "Multi-view object images (or 3D models), system to sample viewpoints and place corresponding images into scenes, and compute to generate augmented dataset.",
            "generalizability": "Generalizable to other tasks needing viewpoint diversity (pose estimation, recognition) provided multi-view captures or 3D models exist; limited when such captures are unavailable.",
            "knowledge_type": "explicit procedural steps and instrumental/technical skills (how to sample and incorporate multi-view images into a synthetic-data pipeline)",
            "uuid": "e570.4",
            "source_info": {
                "paper_title": "Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection",
                "publication_date_yy_mm": "2017-08"
            }
        },
        {
            "name_short": "Synthetic distractors",
            "name_full": "Use of synthetic distractor objects from existing datasets as clutter/negative examples",
            "brief_description": "Add additional object instances (distractors) into composed scenes to emulate real-world clutter and force detectors to discriminate target instances from distractors, improving robustness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Synthetic distractor insertion",
            "procedure_description": "While composing scenes, include extra objects (drawn from the same object-image pool) placed with partial overlaps and random positions to model clutter and distractor presence; this increases negative and hard-negative examples during detector training.",
            "procedure_type": "data augmentation / negative sampling protocol",
            "source_domain": "dataset construction practices in computer vision (negative sampling / hard negative mining)",
            "target_domain": "instance detection dataset synthesis",
            "transfer_type": "adapted/modified for new context (applied as augmentation to composited images)",
            "modifications_made": "Used additional BigBIRD instances as distractors during compositing, controlled maximum IOU for occlusion, and combined with other augmentations; integrated distractors in synthetic scenes systematically.",
            "transfer_success": "successful: presence of distractors improved mAP modestly (Table 1: All + Distractor mAP 76.2 vs All mAP 73.7 — approximately +2.5 AP), indicating improved robustness to clutter.",
            "barriers_encountered": "Need to manage sampling so distractors do not overwhelm scenes or create unrealistic compositions; potential for generating false negatives if distractors occlude targets excessively.",
            "facilitating_factors": "Availability of many object instances (BigBIRD) to serve as distractors and the automated compositing pipeline to place them reproducibly.",
            "contextual_requirements": "Source pool of object images to use as distractors and controls on overlap/cluttering parameters during synthesis.",
            "generalizability": "Generalizable across vision tasks where clutter and distractors are relevant (detection, segmentation), but effect size depends on realism and variety of distractors.",
            "knowledge_type": "practical know-how (tactical dataset-design choices) and procedural steps (inserting distractors with constraints)",
            "uuid": "e570.5",
            "source_info": {
                "paper_title": "Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection",
                "publication_date_yy_mm": "2017-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Poisson image editing",
            "rating": 2
        },
        {
            "paper_title": "Extracting and composing robust features with denoising autoencoders",
            "rating": 2
        },
        {
            "paper_title": "Synthetic data for text localisation in natural images",
            "rating": 2
        },
        {
            "paper_title": "Fully convolutional networks for semantic segmentation",
            "rating": 2
        },
        {
            "paper_title": "Bigbird: A large-scale 3d database of object instances",
            "rating": 2
        },
        {
            "paper_title": "The fast bilateral solver",
            "rating": 1
        },
        {
            "paper_title": "Synthesizing training data for object detection in indoor scenes",
            "rating": 2
        }
    ],
    "cost": 0.01649475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection</h1>
<p>Debidatta Dwibedi Ishan Misra Martial Hebert<br>The Robotics Institute, Carnegie Mellon University<br>debidatta@cmu.edu, {imisra, hebert}@cs.cmu.edu</p>
<h4>Abstract</h4>
<p>A major impediment in rapidly deploying object detection models for instance detection is the lack of large annotated datasets. For example, finding a large labeled dataset containing instances in a particular kitchen is unlikely. Each new environment with new instances requires expensive data collection and annotation. In this paper, we propose a simple approach to generate large annotated instance datasets with minimal effort. Our key insight is that ensuring only patch-level realism provides enough training signal for current object detector models. We automatically 'cut' object instances and 'paste' them on random backgrounds. A naive way to do this results in pixel artifacts which result in poor performance for trained models. We show how to make detectors ignore these artifacts during training and generate data that gives competitive performance on real data. Our method outperforms existing synthesis approaches and when combined with real images improves relative performance by more than $21 \%$ on benchmark datasets. In a cross-domain setting, our synthetic data combined with just $10 \%$ real data outperforms models trained on all real data.</p>
<h2>1. Introduction</h2>
<p>Imagine using an object detection system for an environment like your kitchen. Such a system needs to not only recognize different kinds of objects but also distinguish between many different instances of the same object category, e.g., your cup vs. my cup. With the tremendous progress that has been made in visual recognition, as documented on benchmark detection datasets, one may expect to easily take a state-of-the-art system and deploy it for such a setting.</p>
<p>However, one of the biggest drawbacks of using a state-of-the-art detection system is the amount of annotations needed to train it. For a new environment with new objects, we would likely need to curate thousands of diverse images with varied backgrounds and viewpoints, and annotate them with boxes. Traditionally, vision researchers have undertaken such a mammoth task [8, 26] for a few commonly occurring categories like man, cow, sheep etc., but
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We present a simple way to rapidly generate training images for instance detection with minimal human effort. We automatically extract object instance masks and render it on random background images to create realistic training images with bounding box labels. Our results show that such data is competitive with human curated datasets, and contains complementary information.
this approach is unlikely to scale to all possible categories, especially the instances in your kitchen. In a personalized setting we need annotations for instances like your cup. We believe that collecting such annotations is a major impediment for rapid deployment of detection systems in robotics or other personalized applications.</p>
<p>Recently, a successful research direction to overcome this annotation barrier, is to use synthetically rendered scenes and objects [22, 34, 47] to train a detection system. This approach requires a lot of effort to make the scenes and objects realistic, ensuring high quality global and local consistency. Moreover, models trained on such synthetic data</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: We present a simple approach to rapidly synthesize datasets for instance detection. We start with a set of images of the instances and background scenes. We then automatically extract the object mask and segment the object. We paste the objects on the scenes with different blending to ensure that local artifacts are ignored by the detection model. Our results show that this synthesized data is both competitive with real data and contains complementary information.</p>
<p>have trouble generalizing to real data because of the change in image statistics [5, 36]. To address this, an emerging theme of work [16] moves away from graphics based renderings to composing real images. The underlying theme is to 'paste' real object masks in real images, thus reducing the dependence on graphics renderings. Concurrent work [12] estimates scene geometry and layout and then synthetically places object masks in the scene to create realistic training images. However, the scene layout estimation step may not generalize to unseen scenes. In our paper, we show a simpler approach that does not require such scene geometry estimation to create training images.</p>
<p>Our key insight is that state-of-the art detection methods like Faster-RCNN [39] and even older approaches like DPM [9] <em>etc</em>. care more about <em>local</em> region-based features for detection than the <em>global</em> scene layout. As an example, a cup detector mostly cares about the visual appearance of the cup and its blending with the background, and not so much about where the cup occurs in the scene: the table-top or the ground. We believe that while global consistency is important, only ensuring <em>patch-level realism</em> while composing synthetic datasets should go a long way to train these detectors. We use the term <em>patch-level realism</em> to refer to the observation that the bounding box containing the pasted object <em>looks</em> realistic to the human eye.</p>
<p>However, naively placing object masks in scenes creates subtle pixel artifacts in the images. As these minor imperfections in the pixel space feed forward deeper into the layers of a ConvNet [25], they lead to noticeably different features and the training algorithm focuses on these discrepancies to detect objects, often ignoring to model their complex visual appearance. As our results show (Table 1), such models give reduced detection performance.</p>
<p>Since our main goal is to create training data that is useful for training detectors, we resolve these local imperfections and maintain patch level realism. Inspired from methods in data augmentation and denoising auto encoders [51], we generate data that forces the training algorithm to ignore these artifacts and focus only on the object appearance. We show how rendering the same scene with the same object placement and only varying the blending parameter settings (Section 5.2) makes the detector robust to these subtle pixel artifacts and improves training. Although these images do not respect global consistency or even obey scene factors such as lighting <em>etc</em>., training on them leads to high performance detectors with little effort. Our method is also complementary to existing work [12, 34, 47] that ensures global consistency and can be combined with them.</p>
<p>Data generated using our approach is surprisingly effective at training detection models. Our results suggest that curated instance recognition datasets suffer from poor coverage of the visual appearances of the objects. With our method, we are able to generate many such images with different viewpoints/scales, and get a good coverage of the visual appearance of the object with minimal effort. Thus, our performance gain is particularly noticeable when the test scenes are different from the training scenes, and thus the objects occur in different viewpoints/scales.</p>
<h2>2 Related Work</h2>
<p>Instance detection is a well studied problem in computer vision. [55] provides a comprehensive overview of the popular methods in this field. Early approaches, such as [6], heavily depend on extracting local features such as SIFT [30], SURF [3], MSER [32] and matching them to retrieve instances [29, 48]. These approaches do not work well for objects which are not 'feature-rich', where shape-based methods [10, 19, 21] are more successful.</p>
<p>Modern detection methods [14, 15, 39] based on learned ConvNet features [23, 25, 44] generalize across feature rich and feature poor objects [43]. With the availability of powerful commodity GPUs, and fast detection al-</p>
<p>gorithms [27, 38], these methods are suitable for realtime object detection required in robotics. More recently, deep learning based approaches in computer vision are being adopted for the task of pose estimation of specific objects[33, 53, 54]. Improving instance detection and pose estimation in warehouses will be signifcantly useful for the perception pipeline in systems trying to solve the Amazon Picking Challenge[7].</p>
<p>The use of these powerful methods for object and instance detection requires large amounts of annotated data. This requirement is both impractical and expensive for rapidly deploying detection systems. Sythesizing data is one way to address this issue. [36, 47] use rendered images of objects to do both object detection and pose estimation. They render 3D models of objects from different viewpoints and place them against randomly sampled backgrounds. [34] also highlight the importance of using photorealsitic models in training CNNs.</p>
<p>There is a wide spectrum of work where rendered datasets are used for computer vision tasks. At one end, we have datasets with images of single objects on random backgrounds [34–36, 47]. On the other end, there are datasets where the entire scene is rendered [11, 17, 40]. On that spectrum our work lies in between as we do not render the whole world but use real images of both objects and backgrounds to compose new scenes. In this sense, our work closely related to contemporary work from [16] which generates synthetic data for localizing text in scenes.</p>
<p>Sedaghat et al. [42] show how an annotated dataset can be created for the task of object pose estimation by taking videos by walking around the object. [18] uses synthetic data from [4] for multi-view instance recognition. [31] use real and synthetic images for 2D-3D alignment.</p>
<p>Similarly, [5, 50] render 3D humans in scenes and use this data for pose estimation. Tasks requiring dense annotation, such as segmentation, tracking etc. have also shown to benefit by using such approaches [11, 17, 40, 41]. [46] shows a novel approach for collecting data of objects in a closed domain setting. [1, 13, 24] annotate 3D points belonging to an object in the point cloud reconstruction of a scene and propagate the label to all frames where the object is visible. As synthetic data can be significantly different from real images, [49] shows a domain adaptation approach to overcome this issue. In contrast, our work composes training scenes using real object images as well as real background images.</p>
<p>The existing approaches to sythesizing datasets focus largely on ensuring global consistency and realism [5, 12, 22, 50]. While global consistency is important, we believe that local features matter more for training detection systems. Our approach ensures that when we train our detection model it is invariant to local discrepancies.</p>
<h2>Object Detection</h2>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Object vs Instance Detection. Instance detection involves fine-grained recognition within the same 'object category'(as shown by the visually similar cups) while also detecting the same instance from different viewpoints(depicted by the different views of the granola bars). In this example, instance recognition must distinguish amongst 6 classes: 2 types of granola bars and 4 types of coffee cups. Object detection would distinguish only amongst 2 classes: coffee cups and granola bars.</p>
<h2>3. Background</h2>
<p>Instance Detection: Instance detection requires accurate localization of a particular object, e.g. a particular brand of cereal, a particular cup etc. In contrast, generic object detection detects an entire generic category like a cereal box or a cup (see Figure 3). In fact, in the instance detection scenario correctly localizing a cereal box of some other brand is counted as a mistake. Instance detection occurs commonly in robotics, AR/VR etc., and can also be viewed as fine-grained recognition.</p>
<p>Traditional Dataset Collection: Building detection datasets involves a data curation step and an annotation step. Typically, data curation involves collecting internet images for object detection datasets [8, 26]. However, this fails for instance datasets as finding internet images of particular instances is not easy. For instance detection [45] data curation involves placing the instances in varied backgrounds and manually collecting the images. Manually collecting these images requires one to pay attention to ensure diversity in images by placing the object in different backgrounds and collecting different viewpoints. The annotation step is generally crowd sourced. Depending on the type of data, human annotations can be augmented with object tracking or 3D sensor information [1, 13, 24, 46, 52].</p>
<p>Unfortunately, both these steps are not suitable for rapidly gathering instance annotations. Firstly, as we show in our experiments, even if we limit ourselves to the same type of scene, e.g., kitchens, the curation step can lack diversity and create biases that do not hold in the test setting. Secondly, as the number of images and instances increase, manual annotation requires additional time and expense.</p>
<h2>4. Approach Overview</h2>
<p>We propose a simple approach to rapidly collect data for instance detection. Our results show that our approach is competitive with the manual curation process, while requiring little time and no human annotation.</p>
<p>Ideally, we want to capture all of the visual diversity of an instance. Figures 1 and 3 show how a single instance appears different when seen from different views, scales, orientation and lighting conditions. Thus, distinguishing between such instances requires the dataset to have good coverage of viewpoints and scales of the object. Also, as the number of classes increases rapidly with newer instances, the long-tail distribution of data affects instance recognition problems. With synthetic data, we can ensure that the data has good coverage of both instances and viewpoints. Figure 2 shows the main steps of our method:</p>
<ol>
<li>Collect object instance images: Our approach is agnostic to the way the data is collected. We assume that we have access to object images which cover diverse viewpoints and have a modest background.</li>
<li>Collect scene images: These images will serve as background images in our training dataset. If the test scenes are known beforehand (like in the case of a smarthome or a warehouse) one can collect images from those scenes. As we do not compute any scene statistics like geometry or layout, our approach can readily deal with new scenes.</li>
<li>Predict foreground mask for the object: We predict a foreground mask which separates the instance pixels from the background pixels. This gives us the object mask which can be placed in the scenes.</li>
<li>Paste object instances in scenes: Paste the extracted objects on a randomly chosen background image. We ensure invariance to local artifacts while placing the objects so that the training algorithm does not focus on subpixel discrepancies at the boundaries. We add various modes of blending and synthesize the exact same scene with different blending to make the algorithm robust to these artifacts. We also add data augmentation to ensure a diverse viewpoint/scale coverage.</li>
</ol>
<h2>5. Approach Details and Analysis</h2>
<p>We now present additional details of our approach and provide empirical analysis of our design choices.</p>
<h3>5.1. Collecting images</h3>
<p>We first describe how we collect object/background images, and extract object masks without human effort.
Images of objects from different viewpoints: We choose the objects present in Big Berkeley Instance Recognition Dataset (BigBIRD) [45] to conduct our experiments. Each object has 600 images, captured by five cameras with different viewpoints. Each image also has a corresponding depth image captured by an IR camera.
Background images of indoor scenes: We place the extracted objects from the BigBIRD images on randomly sampled background images from the UW Scenes dataset [24]. There are 1548 images in the backgrounds dataset.
Foreground/Background segmentation: Once we have collected images of the instances, we need to determine the pixels that belong to the instance vs. the background. We automate this by training a model for foreground/background classification. We train a FCN network [28] (based on VGG-16 [44] pre-trained on PASCAL VOC [8] image segmentation) to classify each image pixel into foreground/background. The object masks from the depth sensor are used as ground truth for training this model. We train this model using images of instances which are not present in our final detection evaluation. We use [2] as a post-processing step to clean these results and obtain an object mask. Figure 5 shows some of these results. In practice, we found this combination to generalize to images of unseen objects with modest backgrounds and give good quality object masks from input images. It also generalizes to transparent objects, e.g., coca cola bottle, where the depth sensor does not work well.</p>
<h3>5.2. Adding Objects to Images</h3>
<p>After automatically extracting the object masks from input images, we paste them on real background images. Naïvely pasting objects on scenes results in artifacts which the training algorithm focuses on, ignoring the object's visual appearance. In this section, we present steps to generate data that forces the training algorithm to ignore these artifacts and focus only on the object appearance. To evaluate these steps empirically, we train a detection model on our synthesized images and evaluate it on a benchmark instance detection dataset (real images).
Detection Model: We use the Faster R-CNN [39] method and initialize the model from a VGG-16 [44] model pretrained on object detection on the MSCOCO [26] dataset.
Benchmarking Dataset: After training the detection model on our synthetic images, we use the GMU Kitchen dataset [13] for evaluation. There are 9 scenes in this dataset. Three dataset splits with 6 scenes for training and 3 for testing have been provided in [13] to conduct experiments on the GMU Kitchen dataset. We follow these splits for train/test and report the average over them. No images or statistics from this dataset are used for either dataset synthesis or training the detector. We report Mean Average Precision (mAP) at IOU of 0.5 [8] in all our experiments.</p>
<h3>5.2.1 Blending</h3>
<p>Directly pasting objects on background images creates boundary artifacts. Figure 6 shows some examples of such</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: A few randomly chosen samples from our synthesized images. We describe the details of our approach in Section 5.</p>
<p><strong>Table 1:</strong> We analyze the effect of various factors in synthesizing data by generating data with different settings and training a detector [39]. We evaluate the trained model on the GMU Dataset [13]. As we describe in Section 5, these factors greatly improve the quality of the synthesized data.</p>
<table>
<thead>
<tr>
<th></th>
<th>2D Rot.</th>
<th>3D Rot.</th>
<th>Trunc.</th>
<th>Occl.</th>
<th>coca</th>
<th>coffee</th>
<th>honey</th>
<th>hunt's</th>
<th>mahatma</th>
<th>nature</th>
<th>nature</th>
<th>palmolive</th>
<th>pop</th>
<th>pringles</th>
<th>red</th>
<th>mAP</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>cola</td>
<td>mate</td>
<td>bunches</td>
<td>sauce</td>
<td>rice</td>
<td>v1</td>
<td>v2</td>
<td>orange</td>
<td>secret</td>
<td>bbq</td>
<td>bull</td>
</tr>
<tr>
<td>Blending (Sec 5.2.1)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>No blending</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>65.7</td>
<td>91.1</td>
<td>83.2</td>
<td>59.8</td>
<td>57.7</td>
<td>92.1</td>
<td>84.4</td>
<td>61.4</td>
<td>59.0</td>
<td>38.7</td>
<td>31.9</td>
<td>65.9</td>
</tr>
<tr>
<td>Gaussian Blurring</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>65.3</td>
<td>88.1</td>
<td>80.8</td>
<td>67.5</td>
<td>63.7</td>
<td>90.8</td>
<td>79.4</td>
<td>57.9</td>
<td>58.9</td>
<td>65.7</td>
<td>40.2</td>
<td>68.9</td>
</tr>
<tr>
<td>Poisson [37]</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>62.9</td>
<td>82.9</td>
<td>63.9</td>
<td>59.4</td>
<td>20.7</td>
<td>84.6</td>
<td>67.9</td>
<td>60.9</td>
<td>73.5</td>
<td>41.0</td>
<td>25.1</td>
<td>58.4</td>
</tr>
<tr>
<td>All Blend</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>76.0</td>
<td>90.3</td>
<td>79.9</td>
<td>65.4</td>
<td>67.3</td>
<td>93.4</td>
<td>86.6</td>
<td>64.5</td>
<td>73.2</td>
<td>60.4</td>
<td>39.8</td>
<td>72.4</td>
</tr>
<tr>
<td>All Blend + same image</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>78.4</td>
<td>92.7</td>
<td>81.8</td>
<td>66.2</td>
<td>69.8</td>
<td>93.0</td>
<td>82.9</td>
<td>65.7</td>
<td>76.0</td>
<td>62.9</td>
<td>41.2</td>
<td>73.7</td>
</tr>
<tr>
<td>Data Aug. (Sec 5.2.2)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>No 2D Rotation</td>
<td></td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>63.3</td>
<td>90.4</td>
<td>81.4</td>
<td>63.7</td>
<td>54.2</td>
<td>91.8</td>
<td>82.3</td>
<td>59.2</td>
<td>71.3</td>
<td>68.2</td>
<td>41.4</td>
<td>69.7</td>
</tr>
<tr>
<td>No 3D Rotation</td>
<td>✓</td>
<td></td>
<td>✓</td>
<td>✓</td>
<td>73.1</td>
<td>90.6</td>
<td>83.2</td>
<td>63.3</td>
<td>55.8</td>
<td>93.4</td>
<td>82.1</td>
<td>65.9</td>
<td>64.5</td>
<td>45.7</td>
<td>33.6</td>
<td>68.3</td>
</tr>
<tr>
<td>No Trunc.</td>
<td>✓</td>
<td>✓</td>
<td></td>
<td>✓</td>
<td>73.4</td>
<td>92.1</td>
<td>77.8</td>
<td>59.9</td>
<td>64.6</td>
<td>92.4</td>
<td>84.6</td>
<td>62.0</td>
<td>74.2</td>
<td>67.4</td>
<td>41.7</td>
<td>71.8</td>
</tr>
<tr>
<td>No Occlusion</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td></td>
<td>63.1</td>
<td>84.9</td>
<td>74.4</td>
<td>64.5</td>
<td>50.8</td>
<td>76.9</td>
<td>67.6</td>
<td>55.7</td>
<td>69.0</td>
<td>58.7</td>
<td>28.1</td>
<td>63.1</td>
</tr>
<tr>
<td>All</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>78.4</td>
<td>92.7</td>
<td>81.8</td>
<td>66.2</td>
<td>69.8</td>
<td>93.0</td>
<td>82.9</td>
<td>65.7</td>
<td>76.0</td>
<td>62.9</td>
<td>41.2</td>
<td>73.7</td>
</tr>
<tr>
<td>All + Distractor</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>81.0</td>
<td>93.3</td>
<td>85.6</td>
<td>55.6</td>
<td>73.8</td>
<td>94.9</td>
<td>87.1</td>
<td>68.7</td>
<td>79.5</td>
<td>77.1</td>
<td>42.0</td>
<td>76.2</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Given an image of a new unseen object instance, we use a ConvNet to predict foreground/background pixels. Using these predictions we automatically obtain an object mask. This method generalizes to transparent surfaces where traditional methods relying on depth sensors for segmentation fail (second row).</p>
<p>artifacts. Although these artifacts seem subtle, when such images are used to train detection algorithms, they give poor performance as seen in Table 1. As current detection methods [39] strongly depend on local region-based features, boundary artifacts substantially degrade their performance.</p>
<p>The blending step 'smoothens' out the boundary artifacts between the pasted object and the background. Figure 6 shows some examples of blending. Each of these modes add different image variations, <em>e.g</em>., Poisson blending [37] smooths edges and adds lighting variations. Although these blending methods do not yield visually 'perfect' results, they improve performance of the trained detectors. Table 1 lists these blending methods and shows the improvement in performance after training on blended images.</p>
<p>To make the training algorithm further ignore the effects of blending, we synthesize the exact same scene with the same object placement, and only vary the type of blending used. We denote this by 'All Blend + same image' in Table 1. Training on multiple such images where only the blending factor changes makes the training algorithm invariant to these blending factors and improves performance by 8 AP points over not using any form of blending.</p>
<h3>5.2.2 Data Augmentation</h3>
<p>While pasting the objects on background, we also add the following modes of data augmentation:</p>
<p>2D Rotation: The objects are rotated at uniformly sampled</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Different blending modes used while generating datasets. These modes help the model in ignoring artifacts arising from pasting objects on background scenes. More details in Section 5.2.1</p>
<p>Random angles in between 30 to −30 degrees to account for camera/object rotation changes. Table 1 shows a gain of 3 AP points by adding this augmentation.</p>
<p><strong>3D Rotation:</strong> As we can control this step, we have many images containing atypical 3D rotations of the instances which is hard to find in real data. Table 1 shows a gain of more than 4 AP points because of this augmentation. In Section 6.2 and Figure 7, we show examples of how a model trained on human collected data consistently fails to detect instances from certain viewpoints because the training data has poor viewpoint coverage and different biases from the test set. This result shows the value of being able to synthesize data with diverse viewpoints.</p>
<p><strong>Occlusion and Truncation:</strong> Occlusion and truncation naturally appear in images. They refer to partially visible objects (such as those in Figure 2). We place objects at the boundaries of the images to model truncation, ensuring at least 0.25 of the object box is in the image. To add occlusion, we paste the objects with partial overlap with each other (max IOU of 0.75). Like other modes of augmentation, we can easily vary the amount of truncation/occlusion. As Table 1 shows, adding truncation/occlusion improves the result by as much as 10 AP points.</p>
<p><strong>Distractor Objects:</strong> We add distractor objects in the scenes. This models real-world scenarios with multiple distractor objects. We use additional objects from the BigBIRD dataset as distractors. Presence of synthetic distractors also encourages the learning algorithm to not only latch on to boundary artifacts when detecting objects but also improves performance by 3 AP points.</p>
<h3>6. Experiments</h3>
<p>We now compare the effectiveness of our synthesized data against human annotated data on two benchmark datasets. We first describe our common experimental setup.</p>
<p><strong>Synthesized Data:</strong> We analyze our design choices in Section 5 to pick the best performing ones. We use a total of 33 object instances from the BigBIRD Dataset [45] overlap-</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Missed detections on the Active Vision Dataset [1] for a model trained on the hand-annotated GMU Dataset [13]. The model consistently fails to detect certain viewpoints as the training data has poor viewpoint coverage and has biases different from the test set. Each row shows a single instance.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Examples of false positives from the UNC dataset by the detector trained on the hand-annotated bounding boxes from the GMU dataset. Object detectors trained on hand annotated scenes also need new negatives to be able to perform well in newer scenes.</p>
<p>ping with the 11 instances from GMU Kitchen Dataset [13] and the 33 instances from Active Vision Dataset [1]. We use a foreground/background ConvNet (Section 5.1) to extract the foreground masks from the images. The foreground/background ConvNet is <em>not</em> trained on instances we</p>
<p>Table 2: We compute the performance of training a model on synthetic data and compare it against training on real data. We evaluate on the test split of the GMU Kitchen Dataset <em>[13]</em>.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>coca</th>
<th>coffee</th>
<th>honey</th>
<th>hunt’s</th>
<th>mahatma</th>
<th>nature</th>
<th>nature</th>
<th>palmolive</th>
<th>pop</th>
<th>pringles</th>
<th>red</th>
<th>mAP</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>cola</td>
<td>mate</td>
<td>bunches</td>
<td>sauce</td>
<td>rice</td>
<td>v1</td>
<td>v2</td>
<td>orange</td>
<td>secret</td>
<td>bbq</td>
<td>bull</td>
<td></td>
</tr>
<tr>
<td>Real Images from GMU</td>
<td>81.9</td>
<td>95.3</td>
<td>92.0</td>
<td>87.3</td>
<td>86.5</td>
<td>96.8</td>
<td>88.9</td>
<td>80.5</td>
<td>92.3</td>
<td>88.9</td>
<td>58.6</td>
<td>86.3</td>
</tr>
<tr>
<td>SP-BL-SS<em>[12]</em></td>
<td>55.5</td>
<td>67.9</td>
<td>71.2</td>
<td>34.6</td>
<td>30.6</td>
<td>82.9</td>
<td>66.2</td>
<td>33.1</td>
<td>54.3</td>
<td>54.8</td>
<td>17.7</td>
<td>51.7</td>
</tr>
<tr>
<td>(Ours) Synthetic Images</td>
<td>81.0</td>
<td>93.3</td>
<td>85.6</td>
<td>55.6</td>
<td>73.8</td>
<td>94.9</td>
<td>87.1</td>
<td>68.7</td>
<td>79.5</td>
<td>77.1</td>
<td>42.0</td>
<td>76.2</td>
</tr>
<tr>
<td>SP-BL-SS + Real Images<em>[12]</em></td>
<td>82.6</td>
<td>92.9</td>
<td>91.4</td>
<td>85.5</td>
<td>81.9</td>
<td>95.5</td>
<td>88.6</td>
<td>78.5</td>
<td>93.6</td>
<td>90.2</td>
<td>54.1</td>
<td>85.0</td>
</tr>
<tr>
<td>(Ours) Synthetic + Real Images</td>
<td>88.5</td>
<td>95.5</td>
<td>94.1</td>
<td>88.1</td>
<td>90.3</td>
<td>97.2</td>
<td>91.8</td>
<td>80.1</td>
<td>94.0</td>
<td>92.2</td>
<td>65.4</td>
<td>88.8</td>
</tr>
</tbody>
</table>
<p>use to evaluate detection. As in Section 5, we use backgrounds from the UW Scenes Dataset <em>[24]</em> We generate a synthetic dataset with approximately 6000 images using all modes of data augmentation from Section 5. We sample scale, rotation, position and the background randomly. Each background appears roughly 4 times in the generated dataset with different objects. To model occlusions we allow a maximum IOU of 0.75 between objects. For truncations, we allow at least 25% of the object box to be in the image. For each scene we have three versions produced with different blending modes as described in Section 5.2.1. Figure 4 shows samples of generated images. We use this synthetic data for all our experiments. The code used for generating scenes is available at: https://goo.gl/imXRt7.
Model: We use a Faster R-CNN model <em>[39]</em> based on the VGG-16 <em>[44]</em> pre-trained weights on the MSCOCO <em>[26]</em> detection task. We initialize both the RPN trunk and the object classifier trunk of the network in this way. We fine-tune on different datasets (both real and synthetic) and evaluate the model’s performance. We fine-tune all models for 25K iterations using SGD+momentum with a learning rate of 0.001, momentum 0.9, and reduce the learning rate by a factor of 10 after 15K iterations. We also use weight decay of 0.0005 and dropout of 0.5 on the fully-connected layers. We set the value of all the loss weights (both RPN and classification) as 1.0 in our experiments. We ensure that the model hyperparameters and random seed do not change across datasets/experiments for consistency.
Evaluation: We report Average Precision (AP) at IOU of 0.5 in all our experiments for the task of instance localization. Following <em>[1]</em>, we consider boxes of size at least $50\times 30$ pixels in the images for evaluation.</p>
<h3>6.1 Training and Evaluation on the GMU Dataset</h3>
<p>Similar to Section 5, we use the GMU Kitchen Dataset<em>[13]</em> which contains 9 kitchen scenes with 6, 728 images. We evaluate on the 11 objects present in the dataset overlapping with the BigBIRD <em>[45]</em> objects. We additionally report results from <em>[12]</em>. Their method synthesizes images by accounting for global scene structure when placing Table 3: Evaluation on the entire Active Vision dataset by varying the amount of real data from the GMU Kitchen Scenes train dataset</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>coca</th>
<th>honey</th>
<th>hunt’s</th>
<th>mahatma</th>
<th>nature</th>
<th>red</th>
<th>mAP</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>cola</td>
<td>bunches</td>
<td>sauce</td>
<td>rice</td>
<td>v2</td>
<td>bull</td>
<td></td>
</tr>
<tr>
<td>Real Images</td>
<td>57.7</td>
<td>34.4</td>
<td>48.0</td>
<td>39.9</td>
<td>24.6</td>
<td>46.6</td>
<td>41.9</td>
</tr>
<tr>
<td>Synthetic</td>
<td>63.0</td>
<td>29.3</td>
<td>34.2</td>
<td>20.5</td>
<td>49.0</td>
<td>23.0</td>
<td>36.5</td>
</tr>
<tr>
<td>Synthetic + Real Images</td>
<td>69.9</td>
<td>44.2</td>
<td>51.0</td>
<td>41.8</td>
<td>48.7</td>
<td>50.9</td>
<td>51.1</td>
</tr>
<tr>
<td>10% Real</td>
<td>15.3</td>
<td>19.1</td>
<td>31.6</td>
<td>11.2</td>
<td>6.1</td>
<td>11.7</td>
<td>15.8</td>
</tr>
<tr>
<td>10% Real + Syn</td>
<td>66.1</td>
<td>36.5</td>
<td>44.0</td>
<td>26.4</td>
<td>48.9</td>
<td>37.6</td>
<td>43.2</td>
</tr>
<tr>
<td>40% Real</td>
<td>55.8</td>
<td>31.6</td>
<td>47.3</td>
<td>27.4</td>
<td>24.8</td>
<td>41.9</td>
<td>38.2</td>
</tr>
<tr>
<td>40% Real + Syn</td>
<td>69.8</td>
<td>41.0</td>
<td>55.7</td>
<td>38.3</td>
<td>52.8</td>
<td>47.0</td>
<td>50.8</td>
</tr>
<tr>
<td>70% Real</td>
<td>55.3</td>
<td>30.6</td>
<td>47.9</td>
<td>36.4</td>
<td>25.0</td>
<td>41.2</td>
<td>39.4</td>
</tr>
<tr>
<td>70% Real + Syn</td>
<td>67.5</td>
<td>42.0</td>
<td>50.9</td>
<td>43.0</td>
<td>48.5</td>
<td>51.8</td>
<td>50.6</td>
</tr>
</tbody>
</table>
<p>objects in scenes, e.g., ensure that cups lie on flat surfaces like table tops. In contrast, our method does not use take into account such global structure, but focuses on patch-level realism. We note that their method <em>[12]</em> uses a different background scenes dataset for their synthesis.</p>
<p>Table 2 shows the evaluation results. We see that training on the synthetic data is competitive with training on real images (rows 1 vs 3) and also outperforms the synthetic dataset from <em>[12]</em> (rows 2 vs 3). Combining synthetic data with the real data shows a further improvement for all synthetic image datasets (rows 4, 5). These results show that the data generated by our approach is not only competitive with both real data and existing synthetic data, but also provides complementary information. Figure 9 shows qualitative examples illustrating this point.</p>
<h3>6.2 Evaluation on the Active Vision Dataset</h3>
<p>To test generalization across datasets, we now present experiments where we train on either our synthetic data or the GMU Dataset <em>[13]</em>, and evaluate on the Active Vision Dataset <em>[1]</em>. The Active Vision Dataset<em>[1]</em> has 9 scenes and 17,556 images. It has 33 objects in total and 6 objects in overlap with the GMU Kitchen Scenes. We use these 6 objects for our analysis. We do not use this dataset for training.</p>
<p>We train a model trained on all the images from the GMU Dataset (Section 6.1). This model serves as a base-</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: We show qualitative detection results and mark true positives in green, false positives in red and arrows to highlight regions. The top two rows are from the GMU Kitchen Scenes [13] and the bottom two rows from the Active Vision Dataset [1]. (<em>a</em>), (<em>b</em>): Model trained on real data misses objects which are heavily occluded (<em>a</em>) or stops detecting objects as viewpoint changes from <em>a</em> to <em>b</em>. (<em>c</em>), (<em>d</em>): Model trained on synthetic data detects occluded and truncated objects. (<em>e</em>): Combining synthetic data removes false positives due to training only on real data. (<em>g</em>), (<em>h</em>): Combining real data removes false positives due to training only on synthetic data. (<em>f</em>), (<em>g</em>): Viewpoint changes cause false negatives. (Best viewed electronically)</p>
<p>Line for our model trained on synthetic data. As Table 3 shows, by collecting just 10% images and adding our synthetically generated images, we are able to get more MAP than using the real images in the dataset without the synthetic images. This highlights how useful our approach of dataset generation is in scenarios where there is a dearth of labeled images. Also, the performance gap between these datasets is smaller than in Section 6.1.</p>
<p><strong>Failure modes of real data:</strong> Upon inspecting the errors [20] made by the GMU model, we see that a common error mode of the detector is its <strong>failure to recognize certain views</strong> in the test-set (see Figure 7). These viewpoints were sparsely present in the human annotated training data. In contrast, our synthetic training data has a diverse viewpoint coverage. The model trained on the synthesized images drastically reduces these errors. Combining the synthesized images with the real images from GMU gives a further improvement of <strong>10 AP points</strong> suggesting that synthesized images do provide complementary information.</p>
<p><strong>Varying Real Data:</strong> We investigate the effect of varying the number of real images combined with the synthesized data. We randomly sample different amounts of real images from the GMU Dataset and combine them with the synthetic data to train the detector. As a baseline we also train the model on varying fractions of the real data. Table 3 shows that by adding synthetic images to just 10% of the real images we get a boost of <strong>10 AP points</strong> over just using real images. This performance is also tantalizingly close to the performance of combining larger fractions of real data. This result reinforces the effectiveness and complementary nature of our approach. In the supplementary material, we present additional such results.</p>
<h1>7. Discussion and Future Work</h1>
<p>We presented a simple technique to synthesize annotated training images for instance detection. Our key insights were to leverage randomization for blending objects into scenes and to ensure a diverse coverage of instance viewpoints and scales. We showed that patch-based realism is sufficient for training region-proposal based object detectors. Our method performs favorably to existing hand curated datasets and captures complementary information. In a realistic cross-domain setting we show that by combining just 10% of the available real annotations with our synthesized data, our model performs better than using all the real annotations. From a practical standpoint our technique affords the possibility of generating scenes with non-uniform distributions over object viewpoints and scales without additional data collection effort.</p>
<p>We believe our work can be combined with existing approaches [12] that focus on global consistency for placing objects and [22] which model realism. Future work should focus on a combination of such approaches.</p>
<p><strong>Acknowledgements</strong>: The authors are grateful to Georgios Georgakis and Phil Ammirato for their help with the datasets and discussions. This work was supported in part by NSF Grant CNS1518865.</p>
<h2>References</h2>
<p>[1] P. Ammirato, P. Poirson, E. Park, J. Kosecka, and A. C. Berg. A dataset for developing and benchmarking active vision. In 2017 IEEE International Conference on Robotics and Automation (ICRA), 2017. 3, 6, 7, 8
[2] J. T. Barron and B. Poole. The fast bilateral solver. In European Conference on Computer Vision, pages 617-632. Springer International Publishing, 2016. 4
[3] H. Bay, A. Ess, T. Tuytelaars, and L. Van Gool. Speeded-up robust features (surf). Computer vision and image understanding, 110(3):346-359, 2008. 2
[4] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 3
[5] W. Chen, H. Wang, Y. Li, H. Su, Z. Wang, C. Tu, D. Lischinski, D. Cohen-Or, and B. Chen. Synthesizing training images for boosting human 3d pose estimation. In 3D Vision (3DV), 2016 Fourth International Conference on, pages 479-488. IEEE, 2016. 2, 3
[6] A. Collet, M. Martinez, and S. S. Srinivasa. The moped framework: Object recognition and pose estimation for manipulation. The International Journal of Robotics Research, 30(10):1284-1306, 2011. 2
[7] N. Correll, K. E. Bekris, D. Berenson, O. Brock, A. Causo, K. Hauser, K. Okada, A. Rodriguez, J. M. Romano, and P. R. Wurman. Lessons from the amazon picking challenge. arXiv preprint arXiv:1601.05484, 2016. 3
[8] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2007 (VOC2007). 1, 3, 4
[9] P. Felzenszwalb, D. McAllester, and D. Ramanan. A discriminatively trained, multiscale, deformable part model. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1-8. IEEE, 2008. 2
[10] V. Ferrari, T. Tuytelaars, and L. Van Gool. Object detection by contour segment networks. In European conference on computer vision, pages 14-28. Springer, 2006. 2
[11] A. Gaidon, Q. Wang, Y. Cabon, and E. Vig. Virtual worlds as proxy for multi-object tracking analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4340-4349, 2016. 3
[12] G. Georgakis, A. Mousavian, A. C. Berg, and J. Kosecka. Synthesizing training data for object detection in indoor scenes. arXiv preprint arXiv:1702.07836, 2017. 2, 3, 7, 8
[13] G. Georgakis, M. A. Reza, A. Mousavian, P.-H. Le, and J. Košecká. Multiview rgb-d dataset for object instance detection. In 3D Vision (3DV), 2016 Fourth International Conference on, pages 426-434. IEEE, 2016. 3, 4, 5, 6, 7, 8
[14] R. Girshick. Fast r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, pages 1440-1448, 2015. 2
[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580-587, 2014. 2
[16] A. Gupta, A. Vedaldi, and A. Zisserman. Synthetic data for text localisation in natural images. In Proceedings of the</p>
<p>IEEE Conference on Computer Vision and Pattern Recognition, pages 2315-2324, 2016. 2, 3
[17] A. Handa, V. Patraucean, V. Badrinarayanan, S. Stent, and R. Cipolla. Scenenet: Understanding real world indoor scenes with synthetic data. arXiv preprint arXiv:1511.07041, 2015. 3
[18] D. Held, S. Thrun, and S. Savarese. Robust single-view instance recognition. In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 2152-2159. IEEE, 2016. 3
[19] S. Hinterstoisser, C. Cagniart, S. Ilic, P. Sturm, N. Navab, P. Fua, and V. Lepetit. Gradient response maps for realtime detection of textureless objects. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(5):876-888, 2012. 2
[20] D. Hoiem, Y. Chodpathumwan, and Q. Dai. Diagnosing error in object detectors. In European conference on computer vision, pages 340-353. Springer, 2012. 8
[21] E. Hsiao, A. Collet, and M. Hebert. Making specific features less discriminative to improve point-based 3d object recognition. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 2653-2660. IEEE, 2010. 2
[22] K. Karsch, V. Hedau, D. Forsyth, and D. Hoiem. Rendering synthetic objects into legacy photographs. In ACM Transactions on Graphics (TOG), volume 30, page 157. ACM, 2011. $1,3,8$
[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097-1105, 2012. 2
[24] K. Lai, L. Bo, X. Ren, and D. Fox. A large-scale hierarchical multi-view rgb-d object dataset. In Robotics and Automation (ICRA), 2011 IEEE International Conference on, pages 1817-1824. IEEE, 2011. 3, 4, 7
[25] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1, 1989. 2
[26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft coco: Common objects in context. In ECCV. 2014. 1, 3, 4, 7
[27] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.Y. Fu, and A. C. Berg. Ssd: Single shot multibox detector. In European Conference on Computer Vision, pages 21-37. Springer, 2016. 3
[28] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3431-3440, 2015. 4
[29] D. G. Lowe. Local feature view clustering for 3d object recognition. In Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on, volume 1, pages I-I. IEEE, 2001. 2
[30] D. G. Lowe. Distinctive image features from scaleinvariant keypoints. International journal of computer vision, 60(2):91-110, 2004. 2
[31] F. Massa, B. C. Russell, and M. Aubry. Deep exemplar 2d-3d detection by adapting from real to rendered views. In Proceedings of the IEEE Conference on Computer Vision and</p>
<p>Pattern Recognition, pages 6024-6033, 2016. 3
[32] K. Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisserman, J. Matas, F. Schaffalitzky, T. Kadir, and L. Van Gool. A comparison of affine region detectors. International journal of computer vision, 65(1-2):43-72, 2005. 2
[33] C. Mitash, K. E. Bekris, and A. Boularias. A self-supervised learning system for object detection using physics simulation and multi-view pose estimation. arXiv preprint arXiv:1703.03347, 2017. 3
[34] Y. Movshovitz-Attias, T. Kanade, and Y. Sheikh. How useful is photo-realistic rendering for visual learning? In Computer Vision-ECCV 2016 Workshops, pages 202-217. Springer International Publishing, 2016. 1, 2, 3
[35] D. Park and D. Ramanan. Articulated pose estimation with tiny synthetic videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 58-66, 2015.
[36] X. Peng, B. Sun, K. Ali, and K. Saenko. Learning deep object detectors from 3d models. In Proceedings of the IEEE International Conference on Computer Vision, pages 12781286, 2015. 2, 3
[37] P. Pérez, M. Gangnet, and A. Blake. Poisson image editing. In ACM Transactions on Graphics (TOG), volume 22, pages 313-318. ACM, 2003. 5
[38] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You only look once: Unified, real-time object detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. 3
[39] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91-99, 2015. 2, 4, 5, 7
[40] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In European Conference on Computer Vision, pages 102-118. Springer, 2016. 3
[41] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. 3
[42] N. Sedaghat and T. Brox. Unsupervised generation of a viewpoint annotated car dataset from videos. In Proceedings of the IEEE International Conference on Computer Vision, pages 1314-1322, 2015. 3
[43] A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carlsson. Cnn features off-the-shelf: an astounding baseline for recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 806813, 2014. 2
[44] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 2, 4, 7
[45] A. Singh, J. Sha, K. S. Narayan, T. Achim, and P. Abbeel. Bigbird: A large-scale 3d database of object instances. In Robotics and Automation (ICRA), 2014 IEEE International Conference on, pages 509-516. IEEE, 2014. 3, 4, 6, 7
[46] S. Song, L. Zhang, and J. Xiao. Robot in a room: Toward perfect object recognition in closed environments. 3
[47] H. Su, C. R. Qi, Y. Li, and L. J. Guibas. Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views. In Proceedings of the IEEE International Conference on Computer Vision, pages 2686-2694, 2015. 1, 2, 3
[48] G. Tolias, R. Sicre, and H. Jégou. Particular object retrieval with integral max-pooling of cnn activations. arXiv preprint arXiv:1511.05879, 2015. 2
[49] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE International Conference on Computer Vision, pages 4068-4076, 2015. 3
[50] G. Varol, J. Romero, X. Martin, N. Mahmood, M. Black, I. Laptev, and C. Schmid. Learning from synthetic humans. arXiv preprint arXiv:1701.01370, 2017. 3
[51] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096-1103. ACM, 2008. 2
[52] C. Vondrick, D. Patterson, and D. Ramanan. Efficiently scaling up crowdsourced video annotation. International Journal of Computer Vision, 101(1):184-204, 2013. 3
[53] J. M. Wong, V. Kee, T. Le, S. Wagner, G.-L. Mariottini, A. Schneider, L. Hamilton, R. Chipalkatty, M. Hebert, D. Johnson, et al. Segicp: Integrated deep semantic segmentation and pose estimation. arXiv preprint arXiv:1703.01661, 2017. 3
[54] A. Zeng, K.-T. Yu, S. Song, D. Suo, E. Walker Jr, A. Rodriguez, and J. Xiao. Multi-view self-supervised deep learning for 6d pose estimation in the amazon picking challenge. arXiv preprint arXiv:1609.09475, 2016. 3
[55] L. Zheng, Y. Yang, and Q. Tian. Sift meets cnn: a decade survey of instance retrieval. arXiv preprint arXiv:1608.01807, 2016. 2</p>
<h1>Appendix</h1>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: A few randomly chosen samples from our synthesized images.</p>            </div>
        </div>

    </div>
</body>
</html>
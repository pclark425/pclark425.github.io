<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3153 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3153</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3153</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-8e68b5d1808349fd53c372c0d952b6ebea9d7b9e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8e68b5d1808349fd53c372c0d952b6ebea9d7b9e" target="_blank">Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This paper leverages the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics, they can be prompted to autoformalize informal mathematical statements into formal Isabelle code -- which provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting -- the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes. The code can be found at https://github.com/jinpz/dtv.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3153.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3153.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DTV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Don't Trust: Verify</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intervention that autoformalizes informal LLM solutions and problem statements into Isabelle, generates formal solution sketches, and uses automated theorem provers to verify solutions, then aggregates only verified answers; proposed and evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Don't Trust: Verify (DTV) — method using LLMs + Isabelle ATP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A verification pipeline rather than a single neural model: (1) informal solutions are generated by LLMs (Minerva variants or GPT-3.5), (2) statements and (sketched) solutions are autoformalized (few-shot prompting), (3) Isabelle/HOL + Sledgehammer and tactics are used to fill low-level steps and verify proofs; includes symbolic vacuity filter and a neural self-critique filter.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Grade-school and contest-style word problems: GSM8K, MultiArith, and MATH subsets (Prealgebra, Algebra, Number Theory); tasks include multi-step arithmetic, algebraic solving, digit manipulation, modular arithmetic, and word-problem reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Instead of changing how LLMs compute arithmetic internally, DTV grounds LLM outputs by translating them into formal representations that are then checked by a symbolic ATP; hypothesized origin of LLM formalization ability is exposure to code/formal math in training data, enabling pattern-based translation rather than explicit algorithmic arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical evaluation: DTV (with Minerva 62B formalizer) raises average solve rate from majority voting 61.5% to 65.0%; using GPT-3.5 for statement formalization increases average to 68.2%. Ablations show formal-solution sketches and filters improve solve rates; verified informal-solution rate (percentage of correct-answer solutions among those kept) increases substantially (e.g., from 24.3% to 50.5% on Number Theory with Minerva formalizer).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct probe of internal arithmetic representations; failure modes arise from incorrect/unfaithful formalizations and domain limits of ATP (geometry/probability), showing that verification success depends on fidelity of translation rather than LLM performing provably-correct arithmetic internally.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Autoformalization + symbolic verification intervention: few-shot prompting for statement and solution formalization, generation of formal solution sketches, vacuous-statement filter (symbolic), neural self-critique filter, and ATP (Isabelle + Sledgehammer + tactics) to close low-level steps. Also uses sampling (n=64) and majority voting restricted to verified solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improves problem-level accuracy vs. baselines: e.g., GSM8K solve rate increased from 67.2% (majority voting, Minerva 62B) to 71.4% (DTV with Minerva 62B) and to 79.4% (DTV with GPT-3.5 for formalization). DTV increases proportion of correct-answer solutions retained by >20 percentage points in evaluated categories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples from paper (informal-solution generation with Minerva 62B, 64 samples): Single-sample: average 42.8%; Majority voting: average 61.5%; DTV (Minerva 62B formalizer): average 65.0%; DTV (GPT-3.5 formalizer): average 68.2%. GSM8K: majority 67.2% -> DTV Minerva 71.4% -> DTV GPT-3.5 79.4%. Number Theory: majority 23.7% -> DTV Minerva 31.9% -> DTV GPT-3.5 36.1%. Correct-answer solution rates (before/after autoformalization) e.g., Number Theory: 24.3% -> 50.5% (Minerva formalizer) -> 62.3% (GPT-3.5 formalizer).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Unfaithful translations (formalization changes meaning), vacuous formal statements (contradictory assumptions prove anything), omitted constraints (e.g., sign/type errors), ATP domain limitations (geometry/probability), LLM arithmetic/formatting quirks (e.g. nat type negative semantics), and formalizer failures leading to false rejections or false verifications.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>DTV explicitly compares LLM informal reasoning to symbolic verification: Isabelle+Sledgehammer is used as a ground-truth consistency oracle. The paper argues DTV makes LLM outputs closer to symbolic certainty, and that LLMs alone make calculation and logical errors that symbolic provers do not, but success depends on faithful formalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization", 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3153.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3153.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Minerva</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Minerva (family of models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of large transformer language models pretrained with additional mathematical data to improve quantitative reasoning; variants used include 8B, 62B, and 540B parameter versions evaluated as informal-solution generators and (in one size) as a formalizer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Solving quantitative reasoning problems with language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Minerva (8B, 62B, 540B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLM family trained or finetuned on natural-language mathematics and scientific text; evaluated sizes in paper: 8B, 62B, 540B. Minerva 62B is used to generate informal solutions (and in some experiments to autoformalize statements/solutions).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Grade-school arithmetic word problems (GSM8K, MultiArith) and contest-style math (MATH subsets: Prealgebra, Algebra, Number Theory) involving multi-step arithmetic, algebraic solving, modular arithmetic, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Produces chain-of-thought style reasoning that often contains correct high-level steps but can include calculation errors and omitted low-level derivations; capability to autoformalize arises from exposure to code/formal math in training data (conjectured).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Scaling improves performance: single-sample accuracy increases with model size (e.g., GSM8K single-sample: Minerva 8B 15.2% -> 62B 48.6% -> 540B 54.1%), and majority voting likewise improves; Minerva-generated solutions when filtered/verified by DTV yield higher verified-correct solution rates.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Despite chain-of-thought outputs, Minerva still makes arithmetic mistakes and unjustified logical leaps; formalization errors (e.g., typing/nat semantics) show the model's outputs are not reliably symbolic or formally precise.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompting (few-shot chain-of-thought), temperature sampling (T=0.6, nucleus p=0.95) to produce many solution samples; also used as a formalizer in some experiments and used in self-critique filter and solution formalization pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Chain-of-thought and sampling + majority voting significantly improve accuracy (e.g., average 42.8% -> 61.5%); applying DTV to Minerva outputs further improves solve rates by several percentage points; larger Minerva sizes yield higher baseline and DTV performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Selected metrics from paper: Single sample (Minerva 62B) average 42.8%; Majority voting (Minerva 62B) average 61.5%; DTV with Minerva 62B average 65.0%. Per-dataset examples: GSM8K single 48.6%, majority 67.2%, DTV 71.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Simple calculation errors, unjustified logical leaps, omission of low-level algebraic steps needed for formal proof, fragile formalization (type/format errors), and dependence on prompt demonstrations for formalization quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Compared against formal provers via DTV pipeline: Minerva's informal reasoning is often plausible but lacks the formal rigor of symbolic provers, which either succeed or fail deterministically on formalized statements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization", 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3153.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3153.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (formalizer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (OpenAI) used for statement formalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of large transformer LLMs from OpenAI; in this paper GPT-3.5 is used (via API) as a statement-formalization model and yields better formalizations than Minerva 62B in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (as statement formalizer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based LLM (OpenAI GPT-3.5 family), accessed via API in experiments for autoformalizing natural-language problem statements into Isabelle/HOL; used selectively due to inference cost/time.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same datasets as Minerva (GSM8K, MATH subsets, MultiArith) — statement formalization for word problems and contest math.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Stronger pattern-matching and translation ability to produce faithful formal statements from informal statements (likely due to general coding/formal-text training), enabling more reliable grounding to symbolic ATP.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical improvement when GPT-3.5 is used for statement formalization: overall average solve rate increases from 65.0% (DTV with Minerva 62B formalizer) to 68.2% (DTV with GPT-3.5); GSM8K DTV goes from 71.4% -> 79.4% when GPT-3.5 used for statement formalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No internal-probing evidence; formalization can still be unfaithful and requires filtering; cost and latency limit usage in large-scale experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Used as an improved autoformalizer in the DTV pipeline for statement formalization; combined with Minerva-generated informal solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improved formalization faithfulness and downstream verified-solve rates; e.g., Number Theory DTV: 31.9% (Minerva formalizer) -> 36.1% (GPT-3.5 formalizer).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>DTV performance examples when GPT-3.5 used for statement formalization: average 68.2%; GSM8K 79.4%; Number Theory 36.1%; also increases rates of correct-answer solutions kept (e.g., Number Theory 62.3% with GPT-3.5 formalizer).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Still produces unfaithful translations in some cases; can be computationally costly; may still miss subtle constraints leading to wrong formal statements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>GPT-3.5 formalizations better match Isabelle's expectations than Minerva in these experiments, but final correctness depends on ATP verification; GPT-3.5 does not replace symbolic proving but serves as a more accurate translator.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization", 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3153.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3153.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Majority voting / Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Majority voting (self-consistency sampling aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An aggregation baseline that samples multiple chain-of-thought solutions from an LLM and selects the most frequent final answer; shown to substantially boost accuracy versus single greedy sample.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Majority voting / Self-consistency (applied to LLM chain-of-thought outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sampling-based aggregation: draw many (e.g., n=64) solutions with temperature sampling (T=0.6, nucleus p=0.95), extract final answers, and pick the modal answer as prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Applied to same arithmetic/word-problem datasets (GSM8K, MultiArith, MATH subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Reduces variance of stochastic decoding and leverages consensus among multiple sampled rationales to boost likely-correct answers; relies on LLM sampling diversity to surface correct reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Majority voting baseline substantially improves over single sample (paper reports average 42.8% -> 61.5% with Minerva 62B), demonstrating efficacy of sampling+aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Majority answer can still be wrong; DTV shows cases where the correct answer is not the modal answer but is recoverable via formal verification; majority voting does not filter logically inconsistent or vacuous solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Sampling+aggregation (n=64) over chain-of-thought outputs; compared as baseline to DTV.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Large accuracy gains over single-sample decoding but inferior to DTV in experiments (DTV further improves by verifying solutions and discarding unverified ones).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Majority voting (Minerva 62B) reported per-table: GSM8K 67.2%, Number Theory 23.7%, Algebra 60.8%, Prealgebra 59.2%, MultiArith 96.6%, Average 61.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Can amplify common systematic errors; fails when many sampled chains share the same incorrect reasoning; cannot detect logical inconsistency or vacuity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Majority voting is purely statistical and does not provide formal guarantees like ATP verification; DTV demonstrates combining majority voting with verification yields better reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization", 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3153.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3153.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autoformalization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autoformalization with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The process of translating informal natural-language mathematical statements and solutions into formal theorem-prover code (Isabelle/Lean/Coq), performed here via few-shot prompting of LLMs to enable automatic symbolic verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autoformalization with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Autoformalization via few-shot prompted LLMs (Minerva, GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM-based translation pipeline: few-shot examples (10 per prompt for statements, 3 for solution sketches) are used to prompt LLMs to emit Isabelle/HOL theorem statements and formal solution sketches; multiple formalization attempts per informal solution are performed to increase chance of faithful translation.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used to formalize arithmetic, algebra, number-theory, and combinatorial word problems for symbolic verification by an ATP.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>LLMs map linguistic patterns to formal syntactic templates learned from training on code/formal mathematics; this mapping can be robust enough to produce faithful formal statements for many problems but can also introduce subtle semantic/type errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Numerous successful formalizations given as examples (Figure 2, Appendix) and quantitative gains when using better formalizers (GPT-3.5) in DTV; prior work (cited) shows feasibility of neural autoformalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Formalization can be unfaithful (modify formulas, omit constraints), produce vacuous/contradictory hypotheses, or misuse types (e.g., nat negative semantics), requiring symbolic and neural filters; success is sensitive to few-shot examples and prompt quality.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Few-shot prompting for statement/solution formalization, multiple attempts per example (10 attempts), plus post-filters (vacuous test, self-critique).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Enables symbolic verification and thereby improves selection of correct answers; using a stronger autoformalizer (GPT-3.5) yields measurable downstream accuracy gains in DTV.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports that with Minerva 62B formalizer DTV average = 65.0%; replacing statement formalizer with GPT-3.5 increases average to 68.2%; many example problems successfully formalized and verified (case studies shown).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Unfaithful or vacuous translations, subtle type/format mismatches (e.g., nat negative handling), and dependence on few-shot examples cause both false positives (wrong informal solution verified because translation changed problem) and false negatives (faithful statements rejected).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Autoformalization is an imperfect translator between human informal proofs and symbolic provers; it enables using symbolic provers as oracles but cannot replace formal human-crafted formalizations when domain support is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization", 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3153.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3153.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Isabelle + Sledgehammer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Isabelle/HOL theorem prover with Sledgehammer and tactics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive theorem proving environment (Isabelle/HOL) augmented with Sledgehammer (automation bridging to ATPs) and a set of tactics used to fill low-level proof steps and automatically attempt proofs of formalized statements and solution sketches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Isabelle/hol: A proof assistant for higher-order logic</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Isabelle/HOL + Sledgehammer (ATP pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Isabelle/HOL is a higher-order logic proof assistant; Sledgehammer connects Isabelle to external automated theorem provers and the pipeline uses tactics (auto, simp, blast, fastforce, force, eval, presburger, sos, arith, linarith, auto simp: field_simps) to discharge goals automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Symbolically verifies arithmetic, algebra, divisibility, modular arithmetic, combinatorial counting, and other formalized mathematical goals derived from word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Performs exact symbolic reasoning and closure of low-level steps; ATPs can automatically simplify and discharge goals that are 'close' to assumptions (e.g., arithmetic simplification, algebraic rearrangement).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Used as the verification oracle in the DTV pipeline; can directly prove many translated statements without step-by-step solutions and can fill low-level gaps left by formal solution sketches, enabling acceptance of correct informal solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Limited domain coverage (harder for geometry/probability requiring measure theory), and cannot prove statements if formalization is incorrect or lacking required lemmas; failures often arise from translation errors rather than ATP capability.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Symbolic verification tool used downstream of autoformalization; also used in vacuity filter by attempting to prove 'False' to detect contradictory hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Acts as a near-oracle filter that increases reliability of selected answers and discards many incorrect informal solutions once their formal counterparts fail to verify; central to DTV's accuracy improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports overall problem-solve improvements attributable to ATP verification within DTV (e.g., increases in solved rates as cited in DTV performance metrics); vacuity filter and ATP success used as binary signals to accept/reject formalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Cannot handle domains not well-supported in library; sensitive to type/formulation errors from the formalizer; may fail to close proof obligations that require human-level lemma invention.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Provides deterministic symbolic correctness guarantees not available from LLM outputs; used to turn probabilistic LLM outputs into formally verified conclusions when formalization is faithful.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization", 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3153.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3153.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Formal solution sketches</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Formal solution sketches (sketch+ATP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High-level formal proof steps automatically generated from informal solutions that leave low-level details to ATP; intended to bridge human-style informal steps and machine-verifiable formal proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Draft, Sketch, and Prove: Guiding formal theorem provers with informal proofs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Formal solution sketches generated by LLMs (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM-generated sequences of higher-level formal proof steps (sketch) that correspond to natural-language solution steps; Sledgehammer and tactics are then used to fill in omitted low-level reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step arithmetic and algebraic proofs where direct ATP proof may need intermediate justifications (e.g., application of lemmas, algebraic manipulations).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Sketches capture high-level structure of a proof (theorem-decomposition) while delegating routine algebraic/arithmetical manipulation to ATPs, combining human-style reasoning with symbolic exactness.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation: removing formal solution sketches slightly decreases performance (e.g., Number Theory from 31.9% -> 29.2% in DTV Minerva 62B), and case studies show sketches are necessary when ATP cannot directly prove the statement from assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Not strictly necessary for many problems that ATP can prove directly from correct formal statements; generating correct sketches depends on faithful translation of informal steps.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>LLM-generated formal sketch followed by ATP tactics to discharge remaining goals.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improves verification success on problems requiring multi-step reasoning that ATP alone would not find; helps translate informal reasoning into a form ATPs can complete.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation numbers: DTV (Minerva 62B) Number Theory 31.9% with sketches vs 29.2% without; similar small improvements in other categories reported.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Sketches can omit necessary lemmas or include incorrect high-level steps; success depends on ATP library support and sketch fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Sketches emulate the role of a human-written high-level proof outline while relying on symbolic systems for routine derivations, combining strengths of both.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization", 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3153.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3153.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Filters (vacuous & self-critique)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vacuous-statement filter and neural self-critique filter</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two complementary filters to remove unfaithful formalizations: a symbolic vacuity test that replaces the goal with False and checks provability, and an LLM-based self-critique that scores faithfulness of the formalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Symbolic vacuity filter + LLM self-critique (applied in DTV)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vacuous filter: syntactically replace 'show ...' with 'show False' and run ATP to detect contradictory assumptions; Self-critique: few-shot prompted LLM judges whether a generated formal statement faithfully matches the informal statement (yes/no with justification).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Applied to any formalized arithmetic/word-problem statements produced in the pipeline to reduce false verifications due to translation mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Vacuity filter detects contradictory/hypothesis-based trivializations; self-critique leverages LLM's linguistic understanding to flag semantic mismatches not detectable symbolically.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablations show both filters help: removing vacuous filter or self-critique lowers solve rates slightly (Table 3): e.g., Number Theory DTV from 31.9% -> 30.7% (no vacuous) and -> 30.0% (no self-critique).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Self-critique can both falsely reject faithful formalizations and miss unfaithful ones; vacuity detection only catches contradictory cases not other semantic translation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Post-autoformalization filtering: symbolic ATP check for vacuity and LLM binary faithfulness judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Reduces incorrect acceptances due to vacuous/unfaithful formalizations, modestly improving overall verified-solve rates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation numbers: removing vacuous filter or self-critique reduces DTV solve rates by ~0.3–2.6 percentage points across categories in Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Self-critique reliability depends on few-shot examples; vacuity filter cannot detect non-contradictory but semantically altered formalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Combines symbolic checks (which humans performing formalization would also catch) with LLM judgment to emulate human proofreading of formalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization", 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autoformalization with large language models <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Draft, Sketch, and Prove: Guiding formal theorem provers with informal proofs <em>(Rating: 2)</em></li>
                <li>Towards a mathematics formalisation assistant using large language models <em>(Rating: 1)</em></li>
                <li>Three years of experience with sledgehammer, a practical link between automatic and interactive theorem provers <em>(Rating: 1)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3153",
    "paper_id": "paper-8e68b5d1808349fd53c372c0d952b6ebea9d7b9e",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "DTV",
            "name_full": "Don't Trust: Verify",
            "brief_description": "An intervention that autoformalizes informal LLM solutions and problem statements into Isabelle, generates formal solution sketches, and uses automated theorem provers to verify solutions, then aggregates only verified answers; proposed and evaluated in this paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Don't Trust: Verify (DTV) — method using LLMs + Isabelle ATP",
            "model_description": "A verification pipeline rather than a single neural model: (1) informal solutions are generated by LLMs (Minerva variants or GPT-3.5), (2) statements and (sketched) solutions are autoformalized (few-shot prompting), (3) Isabelle/HOL + Sledgehammer and tactics are used to fill low-level steps and verify proofs; includes symbolic vacuity filter and a neural self-critique filter.",
            "arithmetic_task_type": "Grade-school and contest-style word problems: GSM8K, MultiArith, and MATH subsets (Prealgebra, Algebra, Number Theory); tasks include multi-step arithmetic, algebraic solving, digit manipulation, modular arithmetic, and word-problem reasoning.",
            "reported_mechanism": "Instead of changing how LLMs compute arithmetic internally, DTV grounds LLM outputs by translating them into formal representations that are then checked by a symbolic ATP; hypothesized origin of LLM formalization ability is exposure to code/formal math in training data, enabling pattern-based translation rather than explicit algorithmic arithmetic.",
            "evidence_for_mechanism": "Empirical evaluation: DTV (with Minerva 62B formalizer) raises average solve rate from majority voting 61.5% to 65.0%; using GPT-3.5 for statement formalization increases average to 68.2%. Ablations show formal-solution sketches and filters improve solve rates; verified informal-solution rate (percentage of correct-answer solutions among those kept) increases substantially (e.g., from 24.3% to 50.5% on Number Theory with Minerva formalizer).",
            "evidence_against_mechanism": "No direct probe of internal arithmetic representations; failure modes arise from incorrect/unfaithful formalizations and domain limits of ATP (geometry/probability), showing that verification success depends on fidelity of translation rather than LLM performing provably-correct arithmetic internally.",
            "intervention_type": "Autoformalization + symbolic verification intervention: few-shot prompting for statement and solution formalization, generation of formal solution sketches, vacuous-statement filter (symbolic), neural self-critique filter, and ATP (Isabelle + Sledgehammer + tactics) to close low-level steps. Also uses sampling (n=64) and majority voting restricted to verified solutions.",
            "effect_of_intervention": "Improves problem-level accuracy vs. baselines: e.g., GSM8K solve rate increased from 67.2% (majority voting, Minerva 62B) to 71.4% (DTV with Minerva 62B) and to 79.4% (DTV with GPT-3.5 for formalization). DTV increases proportion of correct-answer solutions retained by &gt;20 percentage points in evaluated categories.",
            "performance_metrics": "Examples from paper (informal-solution generation with Minerva 62B, 64 samples): Single-sample: average 42.8%; Majority voting: average 61.5%; DTV (Minerva 62B formalizer): average 65.0%; DTV (GPT-3.5 formalizer): average 68.2%. GSM8K: majority 67.2% -&gt; DTV Minerva 71.4% -&gt; DTV GPT-3.5 79.4%. Number Theory: majority 23.7% -&gt; DTV Minerva 31.9% -&gt; DTV GPT-3.5 36.1%. Correct-answer solution rates (before/after autoformalization) e.g., Number Theory: 24.3% -&gt; 50.5% (Minerva formalizer) -&gt; 62.3% (GPT-3.5 formalizer).",
            "notable_failure_modes": "Unfaithful translations (formalization changes meaning), vacuous formal statements (contradictory assumptions prove anything), omitted constraints (e.g., sign/type errors), ATP domain limitations (geometry/probability), LLM arithmetic/formatting quirks (e.g. nat type negative semantics), and formalizer failures leading to false rejections or false verifications.",
            "comparison_to_humans_or_symbolic": "DTV explicitly compares LLM informal reasoning to symbolic verification: Isabelle+Sledgehammer is used as a ground-truth consistency oracle. The paper argues DTV makes LLM outputs closer to symbolic certainty, and that LLMs alone make calculation and logical errors that symbolic provers do not, but success depends on faithful formalization.",
            "uuid": "e3153.0",
            "source_info": {
                "paper_title": "Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Minerva",
            "name_full": "Minerva (family of models)",
            "brief_description": "A set of large transformer language models pretrained with additional mathematical data to improve quantitative reasoning; variants used include 8B, 62B, and 540B parameter versions evaluated as informal-solution generators and (in one size) as a formalizer.",
            "citation_title": "Solving quantitative reasoning problems with language models",
            "mention_or_use": "use",
            "model_name": "Minerva (8B, 62B, 540B)",
            "model_description": "Transformer-based LLM family trained or finetuned on natural-language mathematics and scientific text; evaluated sizes in paper: 8B, 62B, 540B. Minerva 62B is used to generate informal solutions (and in some experiments to autoformalize statements/solutions).",
            "arithmetic_task_type": "Grade-school arithmetic word problems (GSM8K, MultiArith) and contest-style math (MATH subsets: Prealgebra, Algebra, Number Theory) involving multi-step arithmetic, algebraic solving, modular arithmetic, etc.",
            "reported_mechanism": "Produces chain-of-thought style reasoning that often contains correct high-level steps but can include calculation errors and omitted low-level derivations; capability to autoformalize arises from exposure to code/formal math in training data (conjectured).",
            "evidence_for_mechanism": "Scaling improves performance: single-sample accuracy increases with model size (e.g., GSM8K single-sample: Minerva 8B 15.2% -&gt; 62B 48.6% -&gt; 540B 54.1%), and majority voting likewise improves; Minerva-generated solutions when filtered/verified by DTV yield higher verified-correct solution rates.",
            "evidence_against_mechanism": "Despite chain-of-thought outputs, Minerva still makes arithmetic mistakes and unjustified logical leaps; formalization errors (e.g., typing/nat semantics) show the model's outputs are not reliably symbolic or formally precise.",
            "intervention_type": "Prompting (few-shot chain-of-thought), temperature sampling (T=0.6, nucleus p=0.95) to produce many solution samples; also used as a formalizer in some experiments and used in self-critique filter and solution formalization pipeline.",
            "effect_of_intervention": "Chain-of-thought and sampling + majority voting significantly improve accuracy (e.g., average 42.8% -&gt; 61.5%); applying DTV to Minerva outputs further improves solve rates by several percentage points; larger Minerva sizes yield higher baseline and DTV performance.",
            "performance_metrics": "Selected metrics from paper: Single sample (Minerva 62B) average 42.8%; Majority voting (Minerva 62B) average 61.5%; DTV with Minerva 62B average 65.0%. Per-dataset examples: GSM8K single 48.6%, majority 67.2%, DTV 71.4%.",
            "notable_failure_modes": "Simple calculation errors, unjustified logical leaps, omission of low-level algebraic steps needed for formal proof, fragile formalization (type/format errors), and dependence on prompt demonstrations for formalization quality.",
            "comparison_to_humans_or_symbolic": "Compared against formal provers via DTV pipeline: Minerva's informal reasoning is often plausible but lacks the formal rigor of symbolic provers, which either succeed or fail deterministically on formalized statements.",
            "uuid": "e3153.1",
            "source_info": {
                "paper_title": "Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GPT-3.5 (formalizer)",
            "name_full": "GPT-3.5 (OpenAI) used for statement formalization",
            "brief_description": "A family of large transformer LLMs from OpenAI; in this paper GPT-3.5 is used (via API) as a statement-formalization model and yields better formalizations than Minerva 62B in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (as statement formalizer)",
            "model_description": "Large transformer-based LLM (OpenAI GPT-3.5 family), accessed via API in experiments for autoformalizing natural-language problem statements into Isabelle/HOL; used selectively due to inference cost/time.",
            "arithmetic_task_type": "Same datasets as Minerva (GSM8K, MATH subsets, MultiArith) — statement formalization for word problems and contest math.",
            "reported_mechanism": "Stronger pattern-matching and translation ability to produce faithful formal statements from informal statements (likely due to general coding/formal-text training), enabling more reliable grounding to symbolic ATP.",
            "evidence_for_mechanism": "Empirical improvement when GPT-3.5 is used for statement formalization: overall average solve rate increases from 65.0% (DTV with Minerva 62B formalizer) to 68.2% (DTV with GPT-3.5); GSM8K DTV goes from 71.4% -&gt; 79.4% when GPT-3.5 used for statement formalization.",
            "evidence_against_mechanism": "No internal-probing evidence; formalization can still be unfaithful and requires filtering; cost and latency limit usage in large-scale experiments.",
            "intervention_type": "Used as an improved autoformalizer in the DTV pipeline for statement formalization; combined with Minerva-generated informal solutions.",
            "effect_of_intervention": "Improved formalization faithfulness and downstream verified-solve rates; e.g., Number Theory DTV: 31.9% (Minerva formalizer) -&gt; 36.1% (GPT-3.5 formalizer).",
            "performance_metrics": "DTV performance examples when GPT-3.5 used for statement formalization: average 68.2%; GSM8K 79.4%; Number Theory 36.1%; also increases rates of correct-answer solutions kept (e.g., Number Theory 62.3% with GPT-3.5 formalizer).",
            "notable_failure_modes": "Still produces unfaithful translations in some cases; can be computationally costly; may still miss subtle constraints leading to wrong formal statements.",
            "comparison_to_humans_or_symbolic": "GPT-3.5 formalizations better match Isabelle's expectations than Minerva in these experiments, but final correctness depends on ATP verification; GPT-3.5 does not replace symbolic proving but serves as a more accurate translator.",
            "uuid": "e3153.2",
            "source_info": {
                "paper_title": "Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Majority voting / Self-consistency",
            "name_full": "Majority voting (self-consistency sampling aggregation)",
            "brief_description": "An aggregation baseline that samples multiple chain-of-thought solutions from an LLM and selects the most frequent final answer; shown to substantially boost accuracy versus single greedy sample.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "Majority voting / Self-consistency (applied to LLM chain-of-thought outputs)",
            "model_description": "Sampling-based aggregation: draw many (e.g., n=64) solutions with temperature sampling (T=0.6, nucleus p=0.95), extract final answers, and pick the modal answer as prediction.",
            "arithmetic_task_type": "Applied to same arithmetic/word-problem datasets (GSM8K, MultiArith, MATH subsets).",
            "reported_mechanism": "Reduces variance of stochastic decoding and leverages consensus among multiple sampled rationales to boost likely-correct answers; relies on LLM sampling diversity to surface correct reasoning chains.",
            "evidence_for_mechanism": "Majority voting baseline substantially improves over single sample (paper reports average 42.8% -&gt; 61.5% with Minerva 62B), demonstrating efficacy of sampling+aggregation.",
            "evidence_against_mechanism": "Majority answer can still be wrong; DTV shows cases where the correct answer is not the modal answer but is recoverable via formal verification; majority voting does not filter logically inconsistent or vacuous solutions.",
            "intervention_type": "Sampling+aggregation (n=64) over chain-of-thought outputs; compared as baseline to DTV.",
            "effect_of_intervention": "Large accuracy gains over single-sample decoding but inferior to DTV in experiments (DTV further improves by verifying solutions and discarding unverified ones).",
            "performance_metrics": "Majority voting (Minerva 62B) reported per-table: GSM8K 67.2%, Number Theory 23.7%, Algebra 60.8%, Prealgebra 59.2%, MultiArith 96.6%, Average 61.5%.",
            "notable_failure_modes": "Can amplify common systematic errors; fails when many sampled chains share the same incorrect reasoning; cannot detect logical inconsistency or vacuity.",
            "comparison_to_humans_or_symbolic": "Majority voting is purely statistical and does not provide formal guarantees like ATP verification; DTV demonstrates combining majority voting with verification yields better reliability.",
            "uuid": "e3153.3",
            "source_info": {
                "paper_title": "Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Autoformalization",
            "name_full": "Autoformalization with LLMs",
            "brief_description": "The process of translating informal natural-language mathematical statements and solutions into formal theorem-prover code (Isabelle/Lean/Coq), performed here via few-shot prompting of LLMs to enable automatic symbolic verification.",
            "citation_title": "Autoformalization with large language models",
            "mention_or_use": "use",
            "model_name": "Autoformalization via few-shot prompted LLMs (Minerva, GPT-3.5)",
            "model_description": "LLM-based translation pipeline: few-shot examples (10 per prompt for statements, 3 for solution sketches) are used to prompt LLMs to emit Isabelle/HOL theorem statements and formal solution sketches; multiple formalization attempts per informal solution are performed to increase chance of faithful translation.",
            "arithmetic_task_type": "Used to formalize arithmetic, algebra, number-theory, and combinatorial word problems for symbolic verification by an ATP.",
            "reported_mechanism": "LLMs map linguistic patterns to formal syntactic templates learned from training on code/formal mathematics; this mapping can be robust enough to produce faithful formal statements for many problems but can also introduce subtle semantic/type errors.",
            "evidence_for_mechanism": "Numerous successful formalizations given as examples (Figure 2, Appendix) and quantitative gains when using better formalizers (GPT-3.5) in DTV; prior work (cited) shows feasibility of neural autoformalization.",
            "evidence_against_mechanism": "Formalization can be unfaithful (modify formulas, omit constraints), produce vacuous/contradictory hypotheses, or misuse types (e.g., nat negative semantics), requiring symbolic and neural filters; success is sensitive to few-shot examples and prompt quality.",
            "intervention_type": "Few-shot prompting for statement/solution formalization, multiple attempts per example (10 attempts), plus post-filters (vacuous test, self-critique).",
            "effect_of_intervention": "Enables symbolic verification and thereby improves selection of correct answers; using a stronger autoformalizer (GPT-3.5) yields measurable downstream accuracy gains in DTV.",
            "performance_metrics": "Paper reports that with Minerva 62B formalizer DTV average = 65.0%; replacing statement formalizer with GPT-3.5 increases average to 68.2%; many example problems successfully formalized and verified (case studies shown).",
            "notable_failure_modes": "Unfaithful or vacuous translations, subtle type/format mismatches (e.g., nat negative handling), and dependence on few-shot examples cause both false positives (wrong informal solution verified because translation changed problem) and false negatives (faithful statements rejected).",
            "comparison_to_humans_or_symbolic": "Autoformalization is an imperfect translator between human informal proofs and symbolic provers; it enables using symbolic provers as oracles but cannot replace formal human-crafted formalizations when domain support is limited.",
            "uuid": "e3153.4",
            "source_info": {
                "paper_title": "Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Isabelle + Sledgehammer",
            "name_full": "Isabelle/HOL theorem prover with Sledgehammer and tactics",
            "brief_description": "An interactive theorem proving environment (Isabelle/HOL) augmented with Sledgehammer (automation bridging to ATPs) and a set of tactics used to fill low-level proof steps and automatically attempt proofs of formalized statements and solution sketches.",
            "citation_title": "Isabelle/hol: A proof assistant for higher-order logic",
            "mention_or_use": "use",
            "model_name": "Isabelle/HOL + Sledgehammer (ATP pipeline)",
            "model_description": "Isabelle/HOL is a higher-order logic proof assistant; Sledgehammer connects Isabelle to external automated theorem provers and the pipeline uses tactics (auto, simp, blast, fastforce, force, eval, presburger, sos, arith, linarith, auto simp: field_simps) to discharge goals automatically.",
            "arithmetic_task_type": "Symbolically verifies arithmetic, algebra, divisibility, modular arithmetic, combinatorial counting, and other formalized mathematical goals derived from word problems.",
            "reported_mechanism": "Performs exact symbolic reasoning and closure of low-level steps; ATPs can automatically simplify and discharge goals that are 'close' to assumptions (e.g., arithmetic simplification, algebraic rearrangement).",
            "evidence_for_mechanism": "Used as the verification oracle in the DTV pipeline; can directly prove many translated statements without step-by-step solutions and can fill low-level gaps left by formal solution sketches, enabling acceptance of correct informal solutions.",
            "evidence_against_mechanism": "Limited domain coverage (harder for geometry/probability requiring measure theory), and cannot prove statements if formalization is incorrect or lacking required lemmas; failures often arise from translation errors rather than ATP capability.",
            "intervention_type": "Symbolic verification tool used downstream of autoformalization; also used in vacuity filter by attempting to prove 'False' to detect contradictory hypotheses.",
            "effect_of_intervention": "Acts as a near-oracle filter that increases reliability of selected answers and discards many incorrect informal solutions once their formal counterparts fail to verify; central to DTV's accuracy improvement.",
            "performance_metrics": "Paper reports overall problem-solve improvements attributable to ATP verification within DTV (e.g., increases in solved rates as cited in DTV performance metrics); vacuity filter and ATP success used as binary signals to accept/reject formalizations.",
            "notable_failure_modes": "Cannot handle domains not well-supported in library; sensitive to type/formulation errors from the formalizer; may fail to close proof obligations that require human-level lemma invention.",
            "comparison_to_humans_or_symbolic": "Provides deterministic symbolic correctness guarantees not available from LLM outputs; used to turn probabilistic LLM outputs into formally verified conclusions when formalization is faithful.",
            "uuid": "e3153.5",
            "source_info": {
                "paper_title": "Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Formal solution sketches",
            "name_full": "Formal solution sketches (sketch+ATP)",
            "brief_description": "High-level formal proof steps automatically generated from informal solutions that leave low-level details to ATP; intended to bridge human-style informal steps and machine-verifiable formal proofs.",
            "citation_title": "Draft, Sketch, and Prove: Guiding formal theorem provers with informal proofs",
            "mention_or_use": "use",
            "model_name": "Formal solution sketches generated by LLMs (few-shot)",
            "model_description": "LLM-generated sequences of higher-level formal proof steps (sketch) that correspond to natural-language solution steps; Sledgehammer and tactics are then used to fill in omitted low-level reasoning.",
            "arithmetic_task_type": "Multi-step arithmetic and algebraic proofs where direct ATP proof may need intermediate justifications (e.g., application of lemmas, algebraic manipulations).",
            "reported_mechanism": "Sketches capture high-level structure of a proof (theorem-decomposition) while delegating routine algebraic/arithmetical manipulation to ATPs, combining human-style reasoning with symbolic exactness.",
            "evidence_for_mechanism": "Ablation: removing formal solution sketches slightly decreases performance (e.g., Number Theory from 31.9% -&gt; 29.2% in DTV Minerva 62B), and case studies show sketches are necessary when ATP cannot directly prove the statement from assumptions.",
            "evidence_against_mechanism": "Not strictly necessary for many problems that ATP can prove directly from correct formal statements; generating correct sketches depends on faithful translation of informal steps.",
            "intervention_type": "LLM-generated formal sketch followed by ATP tactics to discharge remaining goals.",
            "effect_of_intervention": "Improves verification success on problems requiring multi-step reasoning that ATP alone would not find; helps translate informal reasoning into a form ATPs can complete.",
            "performance_metrics": "Ablation numbers: DTV (Minerva 62B) Number Theory 31.9% with sketches vs 29.2% without; similar small improvements in other categories reported.",
            "notable_failure_modes": "Sketches can omit necessary lemmas or include incorrect high-level steps; success depends on ATP library support and sketch fidelity.",
            "comparison_to_humans_or_symbolic": "Sketches emulate the role of a human-written high-level proof outline while relying on symbolic systems for routine derivations, combining strengths of both.",
            "uuid": "e3153.6",
            "source_info": {
                "paper_title": "Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Filters (vacuous & self-critique)",
            "name_full": "Vacuous-statement filter and neural self-critique filter",
            "brief_description": "Two complementary filters to remove unfaithful formalizations: a symbolic vacuity test that replaces the goal with False and checks provability, and an LLM-based self-critique that scores faithfulness of the formalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Symbolic vacuity filter + LLM self-critique (applied in DTV)",
            "model_description": "Vacuous filter: syntactically replace 'show ...' with 'show False' and run ATP to detect contradictory assumptions; Self-critique: few-shot prompted LLM judges whether a generated formal statement faithfully matches the informal statement (yes/no with justification).",
            "arithmetic_task_type": "Applied to any formalized arithmetic/word-problem statements produced in the pipeline to reduce false verifications due to translation mistakes.",
            "reported_mechanism": "Vacuity filter detects contradictory/hypothesis-based trivializations; self-critique leverages LLM's linguistic understanding to flag semantic mismatches not detectable symbolically.",
            "evidence_for_mechanism": "Ablations show both filters help: removing vacuous filter or self-critique lowers solve rates slightly (Table 3): e.g., Number Theory DTV from 31.9% -&gt; 30.7% (no vacuous) and -&gt; 30.0% (no self-critique).",
            "evidence_against_mechanism": "Self-critique can both falsely reject faithful formalizations and miss unfaithful ones; vacuity detection only catches contradictory cases not other semantic translation errors.",
            "intervention_type": "Post-autoformalization filtering: symbolic ATP check for vacuity and LLM binary faithfulness judgment.",
            "effect_of_intervention": "Reduces incorrect acceptances due to vacuous/unfaithful formalizations, modestly improving overall verified-solve rates.",
            "performance_metrics": "Ablation numbers: removing vacuous filter or self-critique reduces DTV solve rates by ~0.3–2.6 percentage points across categories in Table 3.",
            "notable_failure_modes": "Self-critique reliability depends on few-shot examples; vacuity filter cannot detect non-contradictory but semantically altered formalizations.",
            "comparison_to_humans_or_symbolic": "Combines symbolic checks (which humans performing formalization would also catch) with LLM judgment to emulate human proofreading of formalizations.",
            "uuid": "e3153.7",
            "source_info": {
                "paper_title": "Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autoformalization with large language models",
            "rating": 2
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Draft, Sketch, and Prove: Guiding formal theorem provers with informal proofs",
            "rating": 2
        },
        {
            "paper_title": "Towards a mathematics formalisation assistant using large language models",
            "rating": 1
        },
        {
            "paper_title": "Three years of experience with sledgehammer, a practical link between automatic and interactive theorem provers",
            "rating": 1
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1
        }
    ],
    "cost": 0.01949675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Don’t Trust: Verify - Grounding LLM Quantitative ReASONing With Autoformalization</h1>
<p>Jin Peng Zhou*<br>Cornell University</p>
<p>Kilian Q. Weinberger<br>Cornell University</p>
<p>Charles Staats ${ }^{\dagger}$<br>Wenda Li<br>University of Edinburgh<br>Christian Szegedy ${ }^{\dagger}$<br>xAI<br>Yuhuai $\mathbf{W u}^{\dagger}$<br>xAI</p>
<h4>Abstract</h4>
<p>Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code - which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting - the previously best method to identify correct answers, by more than $12 \%$ on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes. The code can be found at https://github.com/jinpz/dtv.</p>
<h2>1 INTRODUCTION</h2>
<p>Recently, language models (Devlin et al., 2018; Brown et al., 2020; Chowdhery et al., 2022) have advanced significantly in many natural language processing tasks such as machine translation, question answering, summarization, etc. More recent large language models (LLMs) such as Minerva (Lewkowycz et al., 2022), GPT3.5 (OpenAI) and GPT4 (OpenAI, 2023) have also become increasingly capable of solving quantitative reasoning problems, ranging from middle school math word problems (Cobbe et al., 2021) to challenging high school mathematical competition problems (Hendrycks et al., 2021). By training or finetuning the model on high-quality natural language mathematical and scientific text, these LLMs can generate self-contained step-by-step solutions to quantitative reasoning problems without relying on external tools. However, just like human beings, the solutions LLMs generate are prone to simple calculation errors and unjustified logical leaps.</p>
<p>Following Wang et al. (2022); Lewkowycz et al. (2022), one can sample many proposed solutions, extract the final answer from each, and select the most common answer. While aggregating answers like this improves performance at the problem level, the most common answer is sometimes wrong. Ideally, we would like a better heuristic to identify the correct answer. In fact, there are well-known techniques for computers to verify mathematical reasoning using formalization (Wiedijk, 2008). Those methods involve translating the problem and sometimes the solution into a formal language. Unfortunately this translation is difficult and time-intensive enough that formalization methods are less frequently leveraged.</p>
<p>Recently it was discovered that large language models, with few-shot prompting, can automatically formalize natural mathematical language into formal languages (Agrawal et al., 2022; Azerbayev</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A pictorial illustration of Don’t Trust: Verify. Left: starting from an informal statement, an informal solution is generated by a large language model. The informal statement and solution are then translated into their formal counterparts. Multiple informal solutions for each problem are generated with temperature sampling. Right: An automated theorem prover in the formal environment is used to verify formal solutions against formal statements step by step (indicated by [ATP]). The final answer is chosen using majority voting over only the verified solutions. In this example, Formal Solution #1 successfully proves Formal Statement #1. Formal Solution #2, however, fails to prove the "only if" direction of Formal Statement #2.</p>
<p>et al., 2023; Wu et al., 2022; Jiang et al., 2023; Wang et al., 2020). It is conjectured this capability arises because the training data for these LLMs contains sufficiently many examples of computer code and/or formal mathematics. While the translation is far from perfect and can fail on many complex statements, the correctness of the statements produced can be checked using formal theorem proving systems such as Isabelle (Nipkow et al., 2002) and Lean (de Moura et al., 2015).</p>
<p>In this paper, we demonstrate that in spite of its issues, autoformalization is already capable enough to identify correct answers for many quantitative reasoning problems. We call our method <em>Don’t Trust: Verify</em> (DTV) and provide an overview in Figure 1. Intuitively, instead of naively taking majority voting of all generated natural language solutions, we only aggregate solutions that can be verified with autoformalization. To carry out verification, both a formal statement and solution are needed. Therefore, we attempt to translate the plain text problem and solution into formal language using large language models. However, because language models can erroneously translate incorrect statements into correct ones, we develop both symbolic and neural filters to improve the statement translation reliability. Additionally, even with a correct formal statement, a formal solution directly translated from informal solution can fail to prove it. In particular, correct informal solutions (whether generated by humans or LLMs) skip low-level steps that are necessary for formal reasoning. To this end, we instead generate a formal solution sketch following Wiedijk (2004); Jiang et al. (2023) and employ an automated theorem prover (ATP) to fill in the low-level gaps.</p>
<p>We evaluate DTV on GSM8K (Cobbe et al., 2021), three subsets of MATH (Hendrycks et al., 2021) following prior work (Zheng et al., 2021), and MultiArith (Roy &amp; Roth, 2016) datasets. The results show that our method consistently outperforms vanilla majority voting (Wang et al., 2022; Lewkowycz et al., 2022), with a more than 12% improvement on GSM8K. We demonstrate that DTV improves performance across various model sizes and categories. Additionally, we provide case studies on our method identifying correct answers as well as informal solutions. Finally, we discuss the limitations of our approach inherited from LLM and theorem proving environments.</p>
<h2>2 BACKGROUND AND RELATED WORK</h2>
<p><strong>Informal quantitative reasoning with language models.</strong> A number of large language models such as PaLM (Chowdhery et al., 2022), Minerva (Lewkowycz et al., 2022), GPT3.5 (OpenAI) and GPT4 (OpenAI, 2023) have demonstrated their impressive quantitative reasoning abilities by pretraining or finetuning on mathematical and science data. To improve informal reasoning</p>
<p>performance, chain-of-thought prompting (Wei et al., 2022) is typically used to encourage language models outputting intermediate reasoning steps before arriving at an answer. A diverse set of prompting methods have since been proposed for informal reasoning that tries to find better examples for prompting (Fu et al., 2023; Zhang et al., 2022) or better strategies for decoding from the model such as conditioning on references, multi-step decoding (Creswell et al., 2022; Zheng et al., 2023; Welleck et al., 2022; Khot et al., 2023). Cobbe et al. (2021) explores training informal verifiers to judge the correctness of reasoning. Additionally, to alleviate erroneous reasoning with language models, techniques that explore multi-sample consistency in the informal reasoning (Wang et al., 2022; Jung et al., 2022) have also been proposed. Our approach does not require training and is complementary to these methods since we rely on the consistency of a formal theorem proving system to identify correct reasoning paths and improve the reliability of large language model reasoning.</p>
<p>Augmented language models. Besides elucidating rationales via chain-of-thought prompting, recent research has also been devoted to augmenting LLMs with external tools such as web search engine (Nakano et al., 2021; Lazaridou et al., 2022; Schick et al., 2023), external memory retrieval (Shi et al., 2023; Izacard et al., 2022) and programming-based calculators (Chen et al., 2022; Gao et al., 2022; Imani et al., 2023) to bolster their downstream performance and reduce hallucination. Our work augments language models with a formal theorem proving environment that goes beyond simple arithmetic (Chen et al., 2022; Imani et al., 2023) and Boolean logic (Jung et al., 2022).</p>
<p>Interactive theorem proving and autoformalization. Modern interactive theorem provers such as Isabelle (Nipkow et al., 2002), Coq (Barras et al., 1997), and Lean (de Moura et al., 2015), provide an interactive environment to encode and mechanically verify mathematical proofs. Success stories with handcrafted proofs include the verification of industrial software systems (Klein et al., 2009) and research-level mathematics (Castelvecchi et al., 2021). Szegedy (2020) argues for automatically obtaining formal mathematics from their informal counterparts by translation i.e. autoformalization. Since then, researchers have shown the feasibility of autoformalization using neural networks in particular large language models (Agrawal et al., 2022; Azerbayev et al., 2023; Wu et al., 2022; Jiang et al., 2023; Wang et al., 2020). Our work which aims to ground LLM reasoning with formal theorem proving environments is distinct from Jiang et al. (2023) that improves theorem proving performance with LLM. DTV assumes a much more difficult but realistic setting where only natural language data is available. We are the first to demonstrate it is possible to automatically verify correct answer and informal solutions with autoformalization.</p>
<h1>3 Don’t Trust: Verify</h1>
<p>We describe our approach Don't Trust: Verify (DTV). Given a mathematical question and several solutions to it phrased in natural language, we assume the question has a well-defined answer and each solution contains an answer that may or may not be correct. Our goal is to understand which informal solutions are more likely to be correct. An overview of our method can be found in Figure 1.</p>
<h3>3.1 Statement Formalization</h3>
<p>We begin by finding a formal statement that corresponds to the description of the informal problem statement. In many mainstream formal theorem proving environments such as Isabelle (Nipkow et al., 2002), Lean (de Moura et al., 2015), and Coq (Barras et al., 1997), a formal statement needs to be an assertion rather than a question. For example, a formal statement cannot be in the form of Is $\pi$ irrational? but rather Show that $\pi$ is irrational. Since quantitative reasoning problems typically ask for an answer rather than a proof, we first extract the answer from each proposed informal solution. The formal statement is then generated conditioned on both the plain text statement and the extracted answer. Following Wu et al. (2022), we leverage the few-shot learning capability of large language models to generate formal representation of the informal statements. Specifically, we provide a few informal-formal statement translation pairs and prompt the language model to complete the subsequent problem statement formalization.</p>
<h3>3.2 Solution Formalization and Verification</h3>
<p>The formalized statement itself does not tell us its correctness without a formal solution or proof. Because of this, we seek to generate a piece of formal solution to verify the statement correctness.</p>
<p>Ideally, formal solution steps could be directly translated from natural language solution steps by sharing a similar level of abstraction. However, this could fail to prove the statement since formal reasoning steps require additional low-level justification than their natural language counterparts either written by human or LLMs. To address this issue, we generate a formal solution sketch following Wiedijk (2004); Jiang et al. (2023) (see Figure 1 (left)). The formal solution sketch contains high-level steps that are based on natural language counterparts and leaves low-level justifications to an automated theorem prover (indicated as [ATP] in Figure 1). Similar to statement formalization, we few-shot prompt a large language model to automatically generate such formal solution sketches.</p>
<p>To perform verification, we attempt to prove the formal statement with individual solution steps in a formal theorem proving environment. We leverage the consistency of the formal environment and its automated theorem prover to sequentially verify the steps. At any step, if the automated theorem prover fails to close the gap, we consider the formal statement not verified and hence incorrect. Besides, if there are still remaining goals to be proved after verifying every step, the formal statement is also treated as unverified. For example, in Figure 1 (right) Formal Solution #2, although both steps are correct, the formal statement cannot be proved since the original statement is an if and only if statement, and the right-to-left direction is clearly false. If a formal statement is considered verified, the corresponding informal solution and answer are also considered verified. To arrive at the final answer to the problem, we take the most common answer among the verified informal solutions. If no solution can be verified, we fallback to majority voting across all unverified solutions.</p>
<h1>3.3 Filtering Unfaithful Statement Formalizations</h1>
<p>For DTV to achieve good performance, it is crucial that informal and formal statements match precisely with each other. This is because an answer that is definitely incorrect to one problem could still be correct to an altered problem. For example, in the problem statement of Figure 1, if the constraint of $x$ being positive is omitted, the answer $x= \pm 2$ would be correct instead. This is detrimental as the erroneously translated formal statements with incorrect answers can get verified by DTV. We call such statements unfaithful. To mitigate this issue, we propose to employ two types of filters to discard formal statements that are potentially unfaithful.</p>
<p>Vacuous statement filter. In our preliminary experiments, we found that vacuous formal statements form one common category of verified but unfaithful statement translations. By vacuous we mean that the formalized statement's hypotheses are contradictory, and can thus be used to prove anything by contradiction. The contradictory hypotheses are usually due to language model translation mistakes rather than the original problem itself being contradictory. For instance, a translated statement could constrain a variable $x$ to simultaneously satisfy $x&gt;0$ and $x&lt;0$, leading to a contradiction. To address this issue, we replace the formal statement goal with a simple statement of False and apply the automated theorem prover. Any formal statement that can be proved after this substitution is vacuous. We assume it is very unlikely that the natural language question is legitimately vacuous and we discard such vacuous formalizations.</p>
<p>Self-critique filter. There are other categories of unfaithful translations we find harder to identify symbolically with automated theorem provers. Formalization attempts generated by LLMs can sometimes be irrelevant to the problem or will even modify a formula to make the statement correct. Inspired by the fact that large language models are capable of critiquing their own outputs (Saunders et al., 2022), we explicitly ask the language model to decide whether the formal statement is a faithful translation of the informal statement. The likelihood of a yes versus no followed by a justification is compared. If the likelihood of no outweighs yes, the formal statement is considered unfaithful and discarded. It is worth noting that the self-critique filter can reject faithful formal statements as well as miss unfaithful formal statements. We empirically verify its effectiveness in the experiment section.</p>
<h2>4 EXPERIMENTS</h2>
<h3>4.1 DataSet and Formal Environment</h3>
<p>We evaluate DTV on three quantitative reasoning datasets: GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021) and MultiArith (Roy \&amp; Roth, 2016). These datasets have been used extensively to evaluate the reasoning capability of large language models (Cobbe et al., 2021; Wei et al., 2022; Lewkowycz et al., 2022; Zhang et al., 2022; OpenAI, 2023). GSM8K and MultiArith</p>
<p>Informal Statement from Prealgebra: At Annville Junior High School, $30 \%$ of the students in the Math Club are in the Science Club, and $80 \%$ of the students in the Science Club are in the Math Club. There are 15 students in the Science Club. How many students are in the Math Club? Show that it is 40.</p>
<h1>Generated Formal Statement:</h1>
<h2>theorem</h2>
<p>fixes math_club science_club math_and_science : : nat
assumes "math_and_science $=0.3 <em>$ math_club"
and "math_and_science $=0.8 </em>$ science_club"
and "science_club = 15"
shows "math_club = 40"
Informal Statement from Number Theory: What is the hundreds digit in the following product: $5 \times 6 \times 7 \times 8 \times 9 \times 10$ ? Show that it is 2 .</p>
<h2>Generated Formal Statement:</h2>
<h2>theorem</h2>
<p>"((5::nat) * $6 * 7 * 8 * 9 * 10)$ div $100 \bmod 10=2$ "</p>
<p>Informal Statement from GSM8K: A herd consists of camels and dromedaries. There are 180 heads and 304 bumps. How many dromedaries are there if camels have two humps each and dromedaries have one hump each? Show that it is 56 .</p>
<h2>Generated Formal Statement:</h2>
<h2>theorem</h2>
<p>fixes camels dromedaries : : nat
assumes "camels + dromedaries = 180"
and "2 * camels + dromedaries = 304"
shows "dromedaries = 56"
Informal Statement from Algebra: A parabola $a x^{2}+b x+c$ contains the points $(-1,0),(0,5)$, and $(5,0)$. Find the value $100 a+10 b+c$. Show that it is -55 .</p>
<h2>Generated Formal Statement:</h2>
<h2>theorem</h2>
<p>fixes a b c : : real
assumes h0 : "a * $(-1)^{\wedge} 2+b *(-1)+c=0 "$
and h1 : "a * $0^{\wedge} 2+b * 0+c=5 "$
and h2 : "a * $5^{\wedge} 2+b * 5+c=0 "$
shows "100 * a + 10 * b + c = -55"
Figure 2: Examples of faithful formal statements translated from informal statements with correct answers by DTV. Majority voting failed to solve all four problems but DTV solves them successfully. The model is capable of translating complex informal statements precisely. Note that the sentence "Show that it is [answer]" is appended to the original informal problem statement by first extracting the answer from an informal solution. We provide more examples in Appendix A.1.
datasets contain grade-school arithmetic word problems. MATH dataset consists of high school mathematical competition problems drawn from AMC 10, AMC 12, AIME, etc that are more challenging. The problems in MATH have also been grouped into 7 categories: Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus. Following prior work (Zheng et al., 2021) which only draws problems from algebra and number theory categories, we evaluate on Prealgebra, Algebra and Number Theory subsets of the MATH dataset, which are most applicable in current theorem proving environments.
Similar to Wu et al. (2022); Jiang et al. (2023), we use Isabelle (Nipkow et al., 2002) as the formal theorem proving environment for the experiments since it has one of the largest formal corpora that enables formalization of challenging problems as well as powerful automated theorem provers to close low-level details in the formal solution sketches. We do not see fundamental limitations of our</p>
<p>DTV framework applying to other formal languages such as Lean and Mizar and we leave this to future work. We adopt the Portal-to-ISAbelle (Jiang et al., 2021) interface to interact with Isabelle.</p>
<h1>4.2 Baselines and Evaluation Protocol</h1>
<p>We compare DTV with two baselines: single sample and multiple samples with majority voting (Wang et al., 2022; Lewkowycz et al., 2022). In single sample, for each problem, only one informal solution is sampled greedily with $T=0$. For the latter, multiple informal solutions are generated with temperature sampling. To evaluate the performance, we consider a problem being solved correctly if the most common answer matches the ground truth answer. For single sample, the most common answer is the answer extracted from the solution whereas for multiple samples with majority voting, a grouping of solutions based on the answer is performed and the most frequent answer is chosen.</p>
<h3>4.3 EXPERIMENTAL SETUP</h3>
<p>Informal solution generation. We few-shot prompt large language models to generate $n=64$ informal solutions per problem conditioned on the informal problem statement. We experiment with the 8B, 62B and 540B Minerva models (Lewkowycz et al., 2022). We use the default sampling configuration ( $T=0.6$, nucleus sampling (Holtzman et al., 2019) $p=0.95$ ) reported in Lewkowycz et al. (2022) for Minerva models.</p>
<p>Statement formalization. We prepare 25 paired (informal statement, Isabelle formal statement) examples for each of the three categories from MATH dataset, GSM8K and MultiAirth datasets as the candidates for few-shot demonstrations. When prompting the model to formalize an informal statement, we randomly draw and permute 10 examples to form the few-shot prompt. We always ensure the problem to be translated does not appear in the few-shot prompt. To reduce randomness of statement formalization process, for each informal solution, we perform 10 statement formalization attempts. All few-shot demonstration examples can be found in the supplementary materials.</p>
<p>Solution formalization and verification. We further select 10 problems from each dataset and manually write complete formalization examples including both statements and solutions in the form of (informal statement, formal statement, informal solution, formal solution sketch) as examples for solution formalization. We randomly select 3 examples for each few-shot prompt. We query the language model once for solution formalization for each formal statement sample so that each informal solution is in total formalized $10 \times 1$ times. We then use Isabelle to verify the formal solution sketch against the generated formal statement. Sledgehammer (Paulsson \&amp; Blanchette, 2012) along with 11 common tactics (auto, simp, blast, fastforce, force, eval, presburger, sos, arith, linarith, and auto simp: field_simps) are used to close low-level open conjectures in the formal solution sketch. We consider the formal solution and consequently the corresponding original informal solution correct if Sledgehammer and tactics succeed. An informal solution is kept if any of the 10 formalization attempts is verified successfully.</p>
<p>Unfaithful statement filters. For the vacuous statement filter, we change the goal to be proved in generated formal statement to false. This is accomplished by replacing text after show clause to show false and checking whether Sledgehammer and tactics can prove the modified formal statement. Statements proved this way are discarded due to being vacuous. To create the self-critique filter, we take 5 faithful and 5 unfaithful statement formalization examples generated by the model in the preliminary experiments as few-shot demonstrations. The unfaithful formalization examples are also accompanied by what errors it have. This few shot demonstration is kept fixed throughout the experiments. Generated formal statements are discarded if the likelihood of an answer of yes is lower than no for the language model.</p>
<h3>4.4 EXPERIMENTAL RESULTS</h3>
<p>Table 1 shows the percentage of problems solved for baselines and DTV. The informal solutions are generated with Minerva 62B model (Lewkowycz et al., 2022). We consider two autoformalization model choices: Minerva 62B and GPT3.5 (OpenAI). Due to the API inference time and cost, we only use GPT3.5 to generate the formal statements, which is the most crucial component in our approach. Minerva 62B is queried for both solution formalization and self-critique filter.</p>
<p>It can be seen that majority voting is a strong baseline that significantly improves the language model performance over single sample generation from an average of $42.8 \%$ to $61.5 \%$, matching the observations in <em>Wang et al. (2022); Lewkowycz et al. (2022)</em>. With autoformalization and verification in the formal environment, DTV outperforms majority voting baseline and achieves an average performance of $65.0 \%$ using the same Minerva 62B without any finetuning, suggesting the effectiveness of autoformalization. Since Minerva was mainly trained on natural language math content, its autoformalization capability could be limited. To this end, with the same informal solutions generated by Minerva, we switch the statement formalization model to GPT3.5. This leads to an even larger improvement of average solve rate to $68.2 \%$. We observe that the boost in performance is consistent across all datasets and categories of problems. A larger improvement could potentially be obtained by switching solution formalization and self-critique filter to GPT3.5.</p>
<p>Table 1: Comparison of problem solve rate on GSM8K, MATH Number Theory, Algebra, Prealgebra and MultiArith evaluation sets. DTV consistently outperforms the two baselines that do not perform autoformalization. *To reduce the time and cost of repeatedly calling OpenAI APIs, GPT3.5 is only used to generate formal statements.</p>
<table>
<thead>
<tr>
<th>Problem Solve Rate</th>
<th>GSM8K</th>
<th>Number Theory</th>
<th>Algebra</th>
<th>Prealgebra</th>
<th>MultiArith</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baselines</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Single Sample with Minerva 62B</td>
<td>$48.6 \%$</td>
<td>$12.2 \%$</td>
<td>$33.3 \%$</td>
<td>$34.1 \%$</td>
<td>$85.7 \%$</td>
<td>$42.8 \%$</td>
</tr>
<tr>
<td>Majority Voting with Minerva 62B</td>
<td>$67.2 \%$</td>
<td>$23.7 \%$</td>
<td>$60.8 \%$</td>
<td>$59.2 \%$</td>
<td>$96.6 \%$</td>
<td>$61.5 \%$</td>
</tr>
<tr>
<td>Don't Trust: Verify (DTV)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DTV Formalization with Minerva 62B</td>
<td>$71.4 \%$</td>
<td>$31.9 \%$</td>
<td>$61.8 \%$</td>
<td>$61.0 \%$</td>
<td>$98.8 \%$</td>
<td>$65.0 \%$</td>
</tr>
<tr>
<td>DTV Formalization with GPT3.5*</td>
<td>$\mathbf{7 9 . 4 \%}$</td>
<td>$\mathbf{3 6 . 1 \%}$</td>
<td>$\mathbf{6 3 . 2 \%}$</td>
<td>$\mathbf{6 3 . 4 \%}$</td>
<td>$\mathbf{9 9 . 0 \%}$</td>
<td>$\mathbf{6 8 . 2 \%}$</td>
</tr>
</tbody>
</table>
<h1>4.5 Ablation</h1>
<p>Size of informal solution models. Table 2 shows how baselines and DTV performs when varying the size of model that generates informal solutions. Specifically, we experiment with all three sizes of the Minerva model: 8B, 62B and 540B on Number theory, GSM8K and MultiArith problems. Not surprisingly, as the informal solution model scales up, both single sample and majority voting baselines improve their performance, suggesting larger models have generally stronger quantitative reasoning capability than smaller models. For DTV, we keep the formalization model the same as in Table 1 and do not vary its size. DTV consistently outperforms the baselines by a large margin across all three informal reasoning model sizes on all three datasets, indicating the scalability of DTV. The improvement is particularly significant on Minerva 8B, almost doubling its problem solve rate for GSM8K and number theory. Interestingly, the results also demonstrate that it is beneficial to autoformalize informal solutions from a larger model (Minerva 540B) even with a weaker model such as Minerva 62B, which opens up the possibility of reducing autoformalization runtime and cost.</p>
<p>Table 2: Problem solve rate on MATH Number Theory, GSM8K, MultiArith datasets. Each column represents the model that generates informal solutions, ranging from Minerva 8B to Minerva 540B. Baselines and DTV are shown on each row. DTV consistently outperforms baselines at different model sizes. *To reduce the time and cost of repeatedly calling OpenAI APIs, GPT3.5 is only used to generate formal statements.</p>
<table>
<thead>
<tr>
<th>Problem Solve Rate on Number Theory</th>
<th>Minerva 8B</th>
<th>Minerva 62B</th>
<th>Minerva 540B</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baselines</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Single Sample</td>
<td>$7.1 \%$</td>
<td>$12.2 \%$</td>
<td>$19.1 \%$</td>
</tr>
<tr>
<td>Majority Voting</td>
<td>$13.5 \%$</td>
<td>$23.7 \%$</td>
<td>$36.1 \%$</td>
</tr>
<tr>
<td>Don't Trust: Verify (DTV)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DTV Formalization with Minerva 62B</td>
<td>$23.0 \%$</td>
<td>$31.9 \%$</td>
<td>$40.2 \%$</td>
</tr>
<tr>
<td>DTV Formalization with GPT3.5*</td>
<td>$\mathbf{2 6 . 1 \%}$</td>
<td>$\mathbf{3 6 . 1 \%}$</td>
<td>$\mathbf{4 4 . 4 \%}$</td>
</tr>
<tr>
<td>Problem Solve Rate on GSM8K</td>
<td>Minerva 8B</td>
<td>Minerva 62B</td>
<td>Minerva 540B</td>
</tr>
<tr>
<td>Baselines</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Single Sample</td>
<td>$15.2 \%$</td>
<td>$48.6 \%$</td>
<td>$54.1 \%$</td>
</tr>
<tr>
<td>Majority Voting</td>
<td>$27.7 \%$</td>
<td>$67.2 \%$</td>
<td>$75.6 \%$</td>
</tr>
<tr>
<td>Don't Trust: Verify (DTV)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DTV Formalization with Minerva 62B</td>
<td>$\mathbf{4 8 . 4 \%}$</td>
<td>$\mathbf{7 1 . 4 \%}$</td>
<td>$\mathbf{7 8 . 8 \%}$</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Problem Solve Rate on MultiArith</th>
<th>Minerva 8B</th>
<th>Minerva 540B</th>
<th>Minerva 58B</th>
<th>Minerva 58B</th>
<th>Minerva 58B</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Single Sample</td>
<td>$52.7 \%$</td>
<td>$65.0 \%$</td>
<td>$95.0 \%$</td>
<td>$98.0 \%$</td>
<td>$99.3 \%$</td>
</tr>
<tr>
<td>Majority Voting</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Don't Trust: Verify (DTV)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DTV Formalization with Minerva 62B</td>
<td>$\mathbf{9 8 . 8 \%}$</td>
<td>$\mathbf{9 9 . 8 \%}$</td>
<td>$\mathbf{9 9 . 8 \%}$</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Effect of solution formalization and filters. For quantitative reasoning problems, it is possible to directly prove a translated formal statement with the automated theorem prover in absence of a step-by-step solution sketch. This is because once the statement has been formalized correctly, the proposition to be proved is not "far" from the assumption. Proving the proposition could just involve simplification and evaluation that the automated theorem prover can be capable of. We observe that by not asking DTV to formalize informal solutions, we can still achieve a strong problem solve rate (see Table 3). However, in this case only the informal solution answer is checked by DTV and correct formal statements that require elaborate formal solution steps will not be proved successfully. As shown in Table 3, both filters that are used to detect unfaithful formalization are beneficial for improving the problem solve rate.</p>
<p>Table 3: Problem solve rate on MATH Number theory, Algebra and Prealgebra categories with three types of ablation. The results suggest that solution formalization and two statement filters are beneficial for improving performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Problem Solve Rate</th>
<th style="text-align: center;">Number Theory</th>
<th style="text-align: center;">Algebra</th>
<th style="text-align: center;">Prealgebra</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DTV Formalization with Minerva 62B</td>
<td style="text-align: center;">$31.9 \%$</td>
<td style="text-align: center;">$61.8 \%$</td>
<td style="text-align: center;">$61.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">- Formal Solution Sketch</td>
<td style="text-align: center;">$29.2 \%$</td>
<td style="text-align: center;">$60.7 \%$</td>
<td style="text-align: center;">$60.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">- Vacuous Statement Filter</td>
<td style="text-align: center;">$30.7 \%$</td>
<td style="text-align: center;">$61.4 \%$</td>
<td style="text-align: center;">$60.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">- Self-Critique Filter</td>
<td style="text-align: center;">$30.0 \%$</td>
<td style="text-align: center;">$60.9 \%$</td>
<td style="text-align: center;">$58.4 \%$</td>
</tr>
</tbody>
</table>
<h1>4.6 Qualitative Analysis</h1>
<p>DTV translates problems that majority voting fails. In Figure 2, we show four case study examples that majority voting fails to solve due to the correct answer not being the most common answer. DTV, however, successfully formalizes the informal statement and uses automated theorem prover to prove these propositions, making the correct answer as the majority answer. Each example statement formalized require clear and precise understanding of the informal statement and the language model is capable of formalizing diverse types of problems. For example, in the first example, the model needs to understand inclusion-exclusion relationship between different types of students and precisely translates them into the formal environment. In Appendix A.1, we provide more case study examples of statement formalization.</p>
<p>DTV solves additional problems leveraging correct informal solution steps. In Figure 3, we showcase a number theory problem whose statement and informal solution are both formalized correctly by the model. The formal statement itself is beyond the capability of automated theorem prover and requires formal solution steps to solve it. It can be seen that by translating individual informal solution steps correctly, the problem is successfully solved. Additionally, we observe that for this problem, there are low-quality informal solutions that simply output a final answer without justification and their corresponding formal solutions cannot prove the proposition. This suggests that autoformalization could potentially be used to identify high-quality informal solutions. We provide more examples in Appendix A.2.</p>
<h3>4.7 Limitations and Future Work</h3>
<p>One limitation of our approach is that it requires the theorem proving environment to support the problem domain in question. Currently, most environments are still limited in scope, a shortcoming that DTV naturally inherits. For example, Isabelle has limited support for formalizing geometric and probabilistic reasoning. Statements involving such arguments usually need to be built upon measure theory and advanced calculus, which is far beyond the math required to solve these high school problems. This is also the reason why these subsets in the MATH dataset have been excluded by prior work (Zheng et al., 2021) and us. As more support in such areas becomes available, we expect DTV to exhibit similar gains but a thorough evaluation is currently left as future work. Until then, for practical purposes, one could imagine training a simple classifier that predicts if a particular problem belongs to the domain that is well suited for DTV.
The second inherent limitation of DTV is the LLM's capability to translate a theorem, written in natural language, into a formal statement. Mistakes can be subtle. For example, $1-2+3$ could be erroneously formalized to (1::nat) $-2+3$, which evaluates to 3 instead of 2 in Isabelle because a variable of the type nat cannot be negative (i.e. (1::nat) $-2=0$ ). Future</p>
<p>Informal Statement from Number Theory: The least common multiple of two numbers is 3720, and their greatest common divisor is 8 . Given that one of the numbers is 120 , what is the other number? Show that it is 248 .</p>
<h1>Generated Informal Solution:</h1>
<p>The product of the two numbers is the product of their LCM and their GCD:</p>
<p>$$
3720 \cdot 8=29760
$$</p>
<p>Since one of the numbers is 120 , we can divide this product by 120 to obtain the other number:</p>
<p>$$
\frac{29760}{120}=\overline{248}
$$</p>
<p>Final Answer: The final answer is 248 . I hope it is correct.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Generated</span><span class="w"> </span><span class="n">Formal</span><span class="w"> </span><span class="k">Statement</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Formal</span><span class="w"> </span><span class="nl">Solution</span><span class="p">:</span>
<span class="n">theorem</span>
<span class="w">    </span><span class="n">fixes</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">x</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">nat</span>
<span class="w">    </span><span class="n">assumes</span><span class="w"> </span><span class="ss">&quot;lcm x y \(=3720 &quot;</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="ow">and</span><span class="w"> </span><span class="ss">&quot;gcd x y = 8&quot;</span>
<span class="w">    </span><span class="ow">and</span><span class="w"> </span><span class="ss">&quot;x = 120&quot;</span>
<span class="w">    </span><span class="n">shows</span><span class="w"> </span><span class="ss">&quot;y = 248&quot;</span>
<span class="n">proof</span><span class="w"> </span><span class="o">-</span>
<span class="p">(</span><span class="o">*</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">product</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">numbers</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">product</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">their</span><span class="w"> </span><span class="n">LCM</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">their</span><span class="w"> </span><span class="nl">GCD</span><span class="p">:</span>
<span class="mi">3720</span><span class="err">\</span><span class="n">cdot29760</span><span class="p">.</span><span class="w"> </span><span class="o">*</span><span class="p">)</span>
<span class="w">    </span><span class="n">have</span><span class="w"> </span><span class="ss">&quot;lcm x y * gcd x y = x * y&quot;</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="n">auto</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="w">    </span><span class="k">then</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="ss">&quot;3720 * 8 = 120 * y&quot;</span><span class="w"> </span><span class="k">using</span><span class="w"> </span><span class="n">assms</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="n">fastforce</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="p">(</span><span class="o">*</span><span class="w"> </span><span class="n">Since</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">numbers</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">120</span><span class="p">,</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">divide</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">product</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="mi">120</span>
<span class="k">to</span><span class="w"> </span><span class="n">obtain</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">other</span><span class="w"> </span><span class="nl">number</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="n">frac</span><span class="err">{</span><span class="mi">29760</span><span class="err">}{</span><span class="mi">120</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="n">bmod</span><span class="p">(</span><span class="mi">248</span><span class="p">).</span><span class="w"> </span><span class="o">*</span><span class="p">)</span>
<span class="w">    </span><span class="k">then</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="ss">&quot;y = 29760/120&quot;</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="n">auto</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="w">    </span><span class="k">then</span><span class="w"> </span><span class="n">show</span><span class="w"> </span><span class="vm">?</span><span class="n">thesis</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="n">auto</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="n">qed</span>
</code></pre></div>

<p>Figure 3: An example number theory problem of which both the informal statement and solution are translated correctly by DTV. The generated formal statement cannot be directly solved by automated theorem prover. By translating informal solution step by step, the formal statement is proved and the answer is verified. The steps that end with ${A T P}$ are generated by the automated theorem prover.
directions could explore employing more effective filters and leveraging reinforcement learning from human feedback (RLHF) (Bai et al., 2022) to further finetune language models on aligned mathematical formalizations directly.</p>
<h2>5 CONCLUSION</h2>
<p>In this paper, we show that by leveraging the autoformalization capability of large language models through few-shot prompting, we can identify the correct answer among many informal solution samples generated by the same large language models. Our approach, Don't Trust: Verify, utilizes the internal consistency of formal theorem proving environments to check for correct answers. DTV is lightweight with no training or finetuning required. We demonstrate the feasibility and effectiveness of DTV by reaching state-of-the-art performance on GSM8K, subsets of MATH and MultiArith datasets. DTV consistently outperforms vanilla majority voting, the best previous approach, and leads to improvement across different model sizes from 8B, 64B and 540B. DTV is also complementary to different prompting methods such as Zheng et al. (2023); Fu et al. (2023) that only process reasoning in informal domain. We seek to combine these approaches with DTV for future work.</p>
<h1>6 ACKNOWLEDGEMENT</h1>
<p>We would like to thank Albert Jiang for his help and support in Isabelle ${ }^{1}$. JPZ is supported by grant from the Natural Sciences and Engineering Research Council of Canada (NSERC) (567916). WL was supported by the ERC Advanced Grant ALEXANDRIA (Project GA 742178).</p>
<h2>REFERENCES</h2>
<p>Ayush Agrawal, Siddhartha Gadgil, Navin Goyal, Ashvni Narayanan, and Anand Tadipatri. Towards a mathematics formalisation assistant using large language models. arXiv preprint arXiv:2211.07524, 2022.</p>
<p>Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W Ayers, Dragomir Radev, and Jeremy Avigad. Proofnet: Autoformalizing and formally proving undergraduate-level mathematics. arXiv preprint arXiv:2302.12433, 2023.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.</p>
<p>Bruno Barras, Samuel Boutin, Cristina Cornes, Judicaël Courant, Jean-Christophe Filliatre, Eduardo Gimenez, Hugo Herbelin, Gerard Huet, Cesar Munoz, Chetan Murthy, et al. The Coq proof assistant reference manual: Version 6.1. PhD thesis, Inria, 1997.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Davide Castelvecchi et al. Mathematicians welcome computer-assisted proof in 'grand unification'theory. Nature, 595(7865):18-19, 2021.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks, 2022.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning, 2022.</p>
<p>Leonardo de Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, and Jakob von Raumer. The lean theorem prover (system description). In Automated Deduction-CADE-25: 25th International Conference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25, pp. 378-388. Springer, 2015.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning, 2023.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019.</p>
<p>Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398, 2023.</p>
<p>Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. arXiv preprint arXiv, 2208, 2022.</p>
<p>Albert Qiaochu Jiang, Wenda Li, Jesse Michael Han, and Yuhuai Wu. Lisa: Language models of isabelle proofs. In 6th Conference on Artificial Intelligence and Theorem Proving, pp. 378-392, 2021.</p>
<p>Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timoth'ee Lacroix, Yuhuai Wu, and Guillaume Lample. Draft, Sketch, and Prove: Guiding formal theorem provers with informal proofs. In International Conference on Learning Representations, 2023. URL https://doi.org/10.48550/arXiv.2210.12283.</p>
<p>Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. Maieutic prompting: Logically consistent reasoning with recursive explanations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1266-1279, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main. 82.</p>
<p>Aishwarya Kamath and Rajarshi Das. A survey on semantic parsing. arXiv preprint arXiv:1812.00978, 2018.</p>
<p>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks, 2023.</p>
<p>Gerwin Klein, Kevin Elphinstone, Gernot Heiser, June Andronick, David Cock, Philip Derrin, Dhammika Elkaduwe, Kai Engelhardt, Rafal Kolanski, Michael Norrish, et al. sel4: Formal verification of an os kernel. In Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles, pp. 207-220, 2009.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: $22199-22213,2022$.</p>
<p>Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internetaugmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115, 2022.</p>
<p>Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=IFXTZERXdM7.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.</p>
<p>Tobias Nipkow, Markus Wenzel, and Lawrence Charles Paulson. Isabelle/hol: A proof assistant for higher-order logic. 2002.</p>
<p>OpenAI. Introducing chatgpt. URL https://openai.com/blog/chatgpt.
OpenAI. Gpt-4 technical report, 2023.</p>
<p>Lawrence C Paulsson and Jasmin C Blanchette. Three years of experience with sledgehammer, a practical link between automatic and interactive theorem provers. In Proceedings of the 8th International Workshop on the Implementation of Logics (IWIL-2010), Yogyakarta, Indonesia. EPiC, volume 2, 2012.</p>
<p>Subhro Roy and Dan Roth. Solving general arithmetic word problems. arXiv preprint arXiv:1608.01413, 2016.</p>
<p>William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023.</p>
<p>Christian Szegedy. A promising path towards autoformalization and general artificial intelligence. In Intelligent Computer Mathematics: 13th International Conference, CICM 2020, Bertinoro, Italy, July 26-31, 2020, Proceedings 13, pp. 3-20. Springer, 2020.</p>
<p>Qingxiang Wang, Chad Brown, Cezary Kaliszyk, and Josef Urban. Exploration of neural machine translation in autoformalization of mathematics in mizar. In Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs, pp. 85-98, 2020.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum? id=_VjQlMeSB_J.</p>
<p>Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded mathematical proof generation with language models, 2022.</p>
<p>Freek Wiedijk. Formal proof sketches. Lecture notes in computer science, 3085:378-393, 2004.
Freek Wiedijk. Formal proof-getting started. 2008.
Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy. Autoformalization with large language models. Advances in Neural Information Processing Systems, 35:32353-32368, 2022.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022.</p>
<p>Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves reasoning in large language models, 2023.</p>
<p>Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: a cross-system benchmark for formal olympiad-level mathematics. arXiv preprint arXiv:2109.00110, 2021.</p>
<h1>APPENDIX</h1>
<h2>A Additional Case Study Examples</h2>
<h2>A. 1 Faithful Statement Formalization</h2>
<p>Informal Statement: Gina has five pairs of white socks, three pairs of black socks, and two pairs of red socks. What percent of her socks are red? Show that it is $20 \%$.</p>
<h2>Generated Formal Statement:</h2>
<div class="codehilite"><pre><span></span><code>theorem
assumes &quot;card whites = 10&quot;
and &quot;card blacks = 6&quot;
and &quot;card reds = 4&quot;
shows &quot;100 * (card reds) / (card whites + card blacks + card reds) = 20&quot;
</code></pre></div>

<p>Informal Statement: What is the sum of the units digits of all the multiples of 3 between 0 and 50 ? Show that it is 78 .</p>
<h2>Generated Formal Statement:</h2>
<h2>theorem</h2>
<p>"sum_list [x mod 10 . x \<leftarrow> [0..50], 3 dvd x] = (78::nat) "</p>
<p>Informal Statement: Blake and Jenny each took four 100-point tests. Blake averaged 78 on the four tests. Jenny scored 10 points higher than Blake on the first test, 10 points lower than him on the second test, and 20 points higher on both the third and fourth tests. What is the difference between Jenny's average and Blake's average on these four tests? Show that it is 10 .</p>
<h2>Generated Formal Statement:</h2>
<h2>theorem</h2>
<p>fixes b1 b2 b3 b4 j1 j2 j3 j4 : : real
assumes "b1 + b2 + b3 + b4 = 4 * 78"
and "j1 = b1 + 10"
and "j2 = b2 - 10"
and "j3 = b3 + 20"
and "j4 = b4 + 20"
shows " $(j 1+j 2+j 3+j 4) / 4-(b 1+b 2+b 3+b 4) / 4=10 "$</p>
<p>Informal Statement: What is the perimeter, in units, of a rhombus if its area is 120 square units and one diagonal is 10 units? Show that it is 52 .</p>
<h1>Generated Formal Statement:</h1>
<div class="codehilite"><pre><span></span><code>theorem
    fixes dl d2 :: real
    assumes &quot;d1 &gt; 0&quot;
    and &quot;d2 &gt; 0&quot;
    and &quot;d1 = 10&quot;
    and &quot;d1 <span class="gs">* d2 / 2 = 120&quot;</span>
<span class="gs">    shows &quot;2 *</span> sqrt ((d1/2)^2+(d2/2)^2)*2=52&quot;
</code></pre></div>

<p>Informal Statement: Sam is carrying a 2 gallon bucket of milk to his house from the barn, which is 3 miles away from his house. However, the bucket has a leak in it. For each mile he walks, there is $\frac{2}{3}$ as much milk in the bucket as at the beginning of the mile. How many gallons of milk will be in the bucket when Sam gets home? Show that it is $\frac{10}{27}$.</p>
<h2>Generated Formal Statement:</h2>
<h2>theorem</h2>
<p>fixes $x$ : : real
assumes "x = (2/3)^3 * 2"
shows "x = 16/27"</p>
<p>Informal Statement: Expand $\left(2 x^{5}+3 x^{2}\right)\left(x^{4}-4 x^{2}+3 x-8\right)$. Show that it is $2 x^{9}-8 x^{7}+9 x^{6}-$ $16 x^{5}-12 x^{4}+9 x^{3}-24 x^{2}$.</p>
<h2>Generated Formal Statement:</h2>
<h2>theorem</h2>
<p>fixes $x$ : : real
shows " $(2 * x^{\wedge} 5+3 * x^{\wedge} 2) *(x^{\wedge} 4-4 * x^{\wedge} 2+3 * x-8)$
$=2 * x^{\wedge} 9-8 * x^{\wedge} 7+9 * x^{\wedge} 6-16 * x^{\wedge} 5$
$-12 * x^{\wedge} 4+9 * x^{\wedge} 3-24 * x^{\wedge} 2$ "</p>
<p>Informal Statement: The function $f(n)=n^{2}+n+17$ for $0 \leq n \leq 15$ generates prime numbers. What is the value of $f(10)-f(9)$ ? Show that it is 20 .</p>
<h2>Generated Formal Statement:</h2>
<h2>theorem</h2>
<p>fixes n : : nat
assumes "\<forall> n \<in> ${0 . .15}$. prime $\left(n^{\wedge} 2+n+17\right)$ "
shows " $((10:$ :nat) ${ }^{\wedge} 2+10+17)-((9:$ :nat $)^{\wedge} 2+9+17)=20$ "</p>
<p>Informal Statement: Reversing the digits in John's age results in his father's age. The positive difference between their ages is 27 years. If the sum of the digits in each age is 5 , what is the number of years in John's father's age? Show that it is 41 .</p>
<h2>Generated Formal Statement:</h2>
<h2>theorem</h2>
<p>fixes j f jl fl : : nat
assumes "j = 10 * jl + fl"
and "f = 10 * fl + jl"
and "f - j = 27"
and "jl + fl = 5"
shows "f = 41"</p>
<p>Informal Statement: It took Lara five days to read a novel. Each day after the first day, Lara read half as many pages as the day before. If the novel was 248 pages long, how many pages did she read on the first day? Show that it is 128 .</p>
<h1>Generated Formal Statement:</h1>
<h2>theorem</h2>
<p>fixes p1 p2 p3 p4 p5 : : nat
assumes "p1 + p2 + p3 + p4 + p5 = 248"
and "p2 = p1 / 2"
and "p3 = p2 / 2"
and "p4 = p3 / 2"
and "p5 = p4 / 2"
shows "p1 = 128"</p>
<p>Informal Statement: Alex, Bob, Camille, and Danielle's mothers are comparing their children's ages. They observe that the sum of Alex, Bob, and Danielle's ages is fourteen times Camille's age. They also note that the sum of Alex and Bob's ages is six times Camille's age, and Bob's age is two years less than the difference in ages of Danielle and Alex. How old is Camille? Show that it is 1.</p>
<h2>Generated Formal Statement:</h2>
<h2>theorem</h2>
<p>fixes a b c d : : nat
assumes "a &gt; 0"
and "b &gt; 0"
and "c &gt; 0"
and "d &gt; 0"
and "a + b + d = 14 * c"
and "a + b = 6 * c"
and "b = (d - a) - 2"
shows "c = 1"</p>
<p>Figure 4: Additional examples of faithful formal statements translated from informal statements with correct answers by DTV. Majority voting failed to solve these problems but DTV solves them successfully. The model is capable of translating complex informal statements precisely. Note that the "Show that it is [answer]" is appended to the original informal problem statement by first extracting the answer from informal solution.</p>
<h1>A. 2 Faithful Solution Formalization</h1>
<p>Informal Statement: Kailin's favorite restaurant has a special offer on a three-course meal. A meal consists of an appetizer, an entree and a dessert. The menu features five appetizers, eight entrees and four desserts. How many different three-course meals can be formed with these choices? Show that it is 160 .</p>
<h2>Generated Informal Solution:</h2>
<p>We have to choose one of the five appetizers, one of the eight entrees, and one of the four desserts. We have 5 choices for the appetizers, 8 choices for the entrees, and 4 choices for the dessert. Therefore, the number of possible meals is $5 \cdot 8 \cdot 4=160$. Final Answer: The final answer is 160 . I hope it is correct.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Generated</span><span class="w"> </span><span class="n">Formal</span><span class="w"> </span><span class="k">Statement</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Formal</span><span class="w"> </span><span class="nl">Solution</span><span class="p">:</span>
<span class="n">theorem</span>
<span class="w">    </span><span class="n">assumes</span><span class="w"> </span><span class="ss">&quot;card appetizers = 5&quot;</span>
<span class="w">    </span><span class="ow">and</span><span class="w"> </span><span class="ss">&quot;card entrees = 8&quot;</span>
<span class="w">    </span><span class="ow">and</span><span class="w"> </span><span class="ss">&quot;card desserts = 4&quot;</span>
<span class="w">    </span><span class="n">shows</span><span class="w"> </span><span class="ss">&quot;card (appetizers \&lt;times&gt; entrees \&lt;times&gt; desserts) = 160&quot;</span>
<span class="n">proof</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="w"> </span><span class="n">We</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="nf">choose</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">five</span><span class="w"> </span><span class="n">appetizers</span><span class="p">,</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">eight</span><span class="w"> </span><span class="n">entrees</span><span class="p">,</span>
<span class="ow">and</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">four</span><span class="w"> </span><span class="n">desserts</span><span class="p">.</span><span class="w"> </span><span class="o">*</span><span class="p">)</span>
<span class="w">    </span><span class="n">have</span><span class="w"> </span><span class="ss">&quot;card (appetizers \&lt;times&gt; entrees \&lt;times&gt; desserts)</span>
<span class="ss">        = card appetizers * card entrees * card desserts&quot;</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="p">(</span><span class="n">simp</span><span class="w"> </span><span class="k">add</span><span class="err">:</span><span class="w"> </span><span class="n">card_cartesian_product</span><span class="p">)</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="w"> </span><span class="n">Therefore</span><span class="p">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">possible</span><span class="w"> </span><span class="n">meals</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="err">\</span><span class="n">cdot8</span><span class="err">\</span><span class="n">cdot4</span><span class="o">=</span><span class="mf">160.</span><span class="w"> </span><span class="o">*</span><span class="p">)</span>
<span class="w">    </span><span class="k">then</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="ss">&quot;card (appetizers \&lt;times&gt; entrees \&lt;times&gt; desserts)</span>
<span class="ss">        = 5 * 8 * 4&quot;</span><span class="w"> </span><span class="k">using</span><span class="w"> </span><span class="n">assms</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="n">auto</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="w">    </span><span class="n">also</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="ss">&quot;\&lt;dots&gt; = 160&quot;</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="n">simp</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="w"> </span><span class="n">Final</span><span class="w"> </span><span class="nl">Answer</span><span class="p">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">final</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mf">160.</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">hope</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">correct</span><span class="p">.</span><span class="w"> </span><span class="o">*</span><span class="p">)</span>
<span class="w">    </span><span class="n">finally</span><span class="w"> </span><span class="n">show</span><span class="w"> </span><span class="vm">?</span><span class="n">thesis</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="n">auto</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="n">qed</span>
</code></pre></div>

<p>Informal Statement: Simplify $\sqrt{192}$. Show that it is $8 \sqrt{3}$.</p>
<h1>Generated Informal Solution:</h1>
<p>We can factor out a perfect square from 192: $192=64 \cdot 3=8^{2} \cdot 3$. Thus, we have $\sqrt{192}=\sqrt{8^{2} \cdot 3}=$ $8 \sqrt{3}$. Final Answer: The final answer is $8 \sqrt{3}$. I hope it is correct.</p>
<h2>Generated Formal Statement and Formal Solution:</h2>
<div class="codehilite"><pre><span></span><code><span class="n">theorem</span>
<span class="w">    </span><span class="n">fixes</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">::</span><span class="w"> </span><span class="nc">real</span>
<span class="w">    </span><span class="n">assumes</span><span class="w"> </span><span class="ss">&quot;x = 192&quot;</span>
<span class="w">    </span><span class="n">shows</span><span class="w"> </span><span class="ss">&quot;sqrt x = 8 * sqrt (3::real)&quot;</span>
<span class="n">proof</span><span class="w"> </span><span class="o">-</span>
<span class="p">(</span><span class="o">*</span><span class="w"> </span><span class="n">We</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">factor</span><span class="w"> </span><span class="k">out</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">perfect</span><span class="w"> </span><span class="nf">square</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="mi">192</span><span class="err">:</span><span class="w"> </span><span class="mi">192</span><span class="o">=</span><span class="mi">64</span><span class="err">\</span><span class="n">cdot3</span><span class="o">=</span><span class="mi">8</span><span class="err">}\</span><span class="n">mp</span><span class="nv">@subsup</span><span class="err">{}{}{</span><span class="mi">2</span><span class="err">}{}\</span><span class="n">cdot</span>
<span class="n">Thus</span><span class="p">,</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">have</span><span class="err">\</span><span class="nf">sqrt</span><span class="err">{</span><span class="mi">192</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nf">sqrt</span><span class="err">{</span><span class="mi">8</span><span class="o">^</span><span class="err">{</span><span class="mi">2</span><span class="err">}}\</span><span class="n">cdot</span><span class="err">\</span><span class="n">overline</span><span class="err">{</span><span class="mi">3</span><span class="err">}</span><span class="o">=</span><span class="mi">8</span><span class="err">\</span><span class="nf">sqrt</span><span class="err">{</span><span class="mi">3</span><span class="err">}</span><span class="p">.</span><span class="o">*</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="n">have</span><span class="w"> </span><span class="ss">&quot;x = 64 * 3&quot;</span><span class="w"> </span><span class="n">unfolding</span><span class="w"> </span><span class="n">assms</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="n">simp</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="w">    </span><span class="k">then</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="ss">&quot;sqrt x = sqrt (64 * 3)&quot;</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="n">simp</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="w">    </span><span class="n">also</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="ss">&quot;\&lt;dots&gt; = sqrt 64 * sqrt 3&quot;</span>
<span class="w">        </span><span class="k">using</span><span class="w"> </span><span class="n">real_sqrt_mult</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">blast</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="w">    </span><span class="n">also</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="ss">&quot;\&lt;dots&gt; = 8 * sqrt 3&quot;</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="n">simp</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="w">    </span><span class="n">finally</span><span class="w"> </span><span class="n">show</span><span class="w"> </span><span class="vm">?</span><span class="n">thesis</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="n">simp</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="n">ged</span>
</code></pre></div>

<p>Informal Statement: What is $3^{-4} \cdot 3^{-8} \cdot 3^{14}$ ? Show that it is 9 .</p>
<h2>Generated Informal Solution:</h2>
<p>By the product rule for exponents, we have that</p>
<p>$$
3^{-4} \cdot 3^{-8} \cdot 3^{14}=3^{(-4)+(-8)+14}=3^{2}=\boxed{9}
$$</p>
<p>Final Answer: The final answer is 9. I hope it is correct.</p>
<h2>Generated Formal Statement and Formal Solution:</h2>
<div class="codehilite"><pre><span></span><code><span class="n">theorem</span>
<span class="w">    </span><span class="n">fixes</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="n">z</span><span class="w"> </span><span class="o">::</span><span class="w"> </span><span class="nc">real</span>
<span class="w">    </span><span class="n">assumes</span><span class="w"> </span><span class="ss">&quot;x = 3 powr (-4)&quot;</span>
<span class="w">    </span><span class="ow">and</span><span class="w"> </span><span class="ss">&quot;y = 3 powr (-8)&quot;</span>
<span class="w">    </span><span class="ow">and</span><span class="w"> </span><span class="ss">&quot;z = 3 powr 14&quot;</span>
<span class="w">    </span><span class="n">shows</span><span class="w"> </span><span class="ss">&quot;x * y * z = 9&quot;</span>
<span class="n">proof</span><span class="w"> </span><span class="o">-</span>
<span class="p">(</span><span class="o">*</span><span class="w"> </span><span class="k">By</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">product</span><span class="w"> </span><span class="k">rule</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">exponents</span><span class="p">,</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="o">*</span><span class="p">)</span>
<span class="w">    </span><span class="n">have</span><span class="w"> </span><span class="ss">&quot;x * y * z = 3 powr (-4) * 3 powr (-8) * 3 powr 14&quot;</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="p">(</span><span class="n">smt</span><span class="w"> </span><span class="p">(</span><span class="n">z3</span><span class="p">)</span><span class="w"> </span><span class="n">assms</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="n">assms</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="n">assms</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="p">(</span><span class="o">*</span><span class="w"> </span><span class="mi">3</span><span class="o">^</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">)</span><span class="err">\</span><span class="n">cdot</span><span class="w"> </span><span class="mi">3</span><span class="o">^</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">)</span><span class="err">\</span><span class="n">cdot</span><span class="w"> </span><span class="mi">3</span><span class="o">^</span><span class="p">(</span><span class="mi">14</span><span class="p">)</span><span class="o">=</span><span class="mi">3</span><span class="o">^</span><span class="p">((</span><span class="o">-</span><span class="mi">4</span><span class="p">)</span><span class="o">+</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">)</span><span class="o">+</span><span class="mi">14</span><span class="p">)</span><span class="o">=</span><span class="mi">3</span><span class="o">^</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">=</span><span class="err">\</span><span class="n">boxed</span><span class="p">(</span><span class="mi">9</span><span class="p">).</span><span class="o">*</span><span class="p">)</span>
<span class="w">    </span><span class="n">also</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="ss">&quot;\&lt;dots&gt; = 3 powr (-4 + -8 + 14)&quot;</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="n">eval</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="w">    </span><span class="n">also</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="ss">&quot;\&lt;dots&gt; = 3 powr 2&quot;</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="n">auto</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="w">    </span><span class="n">also</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="ss">&quot;\&lt;dots&gt; = 9&quot;</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="n">auto</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="p">(</span><span class="o">*</span><span class="w"> </span><span class="n">Final</span><span class="w"> </span><span class="nl">Answer</span><span class="p">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">final</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mf">9.</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">hope</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">correct</span><span class="p">.</span><span class="w"> </span><span class="o">*</span><span class="p">)</span>
<span class="w">    </span><span class="n">finally</span><span class="w"> </span><span class="n">show</span><span class="w"> </span><span class="vm">?</span><span class="n">thesis</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="n">auto</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="n">ged</span>
</code></pre></div>

<p>Informal Statement: Simplify: $3!(2^{3}+\sqrt{9}) \div 2$. Show that it is 33 .</p>
<h1>Generated Informal Solution:</h1>
<p>First, we simplify the expressions inside the parentheses and the square root.</p>
<p>$$
2^{3}+\sqrt{9}=8+3=11
$$</p>
<p>Next, we simplify the factorial.</p>
<p>$$
3!=3 \cdot 2 \cdot 1=6
$$</p>
<p>Now, we can combine the three expressions.</p>
<p>$$
3!(2^{3}+\sqrt{9}) \div 2=6 \cdot 11 \div 2=\overline{33}
$$</p>
<p>Final Answer: The final answer is 33 . I hope it is correct.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Generated</span><span class="w"> </span><span class="n">Formal</span><span class="w"> </span><span class="k">Statement</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Formal</span><span class="w"> </span><span class="nl">Solution</span><span class="p">:</span>
<span class="n">theorem</span>
<span class="w">    </span><span class="ss">&quot;fact 3* (2^3 + sqrt 9) / 2 = 33&quot;</span>
<span class="n">proof</span><span class="w"> </span><span class="o">-</span>
<span class="p">(</span><span class="o">*</span><span class="w"> </span><span class="k">First</span><span class="p">,</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">simplify</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">expressions</span><span class="w"> </span><span class="n">inside</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">parentheses</span>
<span class="ow">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nf">square</span><span class="w"> </span><span class="n">root</span><span class="p">.</span><span class="w"> </span><span class="o">*</span><span class="p">)</span>
<span class="w">    </span><span class="n">have</span><span class="w"> </span><span class="ss">&quot;2^3+ sqrt 9 = 8 + 3&quot;</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="n">auto</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="w"> </span><span class="k">Next</span><span class="p">,</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">simplify</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">factorial</span><span class="p">.</span><span class="w"> </span><span class="o">*</span><span class="p">)</span>
<span class="w">    </span><span class="n">have</span><span class="w"> </span><span class="ss">&quot;fact 3 = 3*2&quot;</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="p">(</span><span class="n">smt</span><span class="w"> </span><span class="p">(</span><span class="n">z3</span><span class="p">)</span><span class="w"> </span><span class="n">fact_2</span><span class="w"> </span><span class="n">fact_numeral</span><span class="w"> </span><span class="n">pred_numeral_simps</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="w"> </span><span class="n">Final</span><span class="w"> </span><span class="nl">Answer</span><span class="p">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">final</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mf">33.</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">hope</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">correct</span><span class="p">.</span><span class="w"> </span><span class="o">*</span><span class="p">)</span>
<span class="w">    </span><span class="k">then</span><span class="w"> </span><span class="n">show</span><span class="w"> </span><span class="mi">7</span><span class="n">thesis</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="n">auto</span><span class="w"> </span><span class="o">[</span><span class="n">ATP</span><span class="o">]</span>
<span class="n">qed</span>
</code></pre></div>

<p>Figure 5: Additional example problems of which both the informal statement and solution are translated correctly by DTV. The generated formal statement cannot be directly solved by an automated theorem prover. By translating the informal solution step by step, the formal statement is proved and the answer is verified. The steps that end with ${A T P}$ are generated by the automated theorem prover.</p>
<h1>B Few-shot Prompt Data</h1>
<p>We provide few-shot prompts used in the experiments in the supplementary material zip file. Below is a few-shot prompt example for statement formalization.</p>
<p>Informal Statement: Evaluate $\log _{2} 64$. Show that it is 6 .
Formal Statement:
theorem
shows "log $264=6$ "
Informal Statement: What is the distance between the points with coordinates $(-5,5)$ and $(5,-5)$ ? Express your answer in simplest radical form. Show that it is $10 \sqrt{2}$.
Formal Statement:
theorem
fixes x1 x2 y1 y2 : : real
assumes " $(x 1, y 1)=(-5,5)$ " and " $(x 2, y 2)=(5,-5)$ "
shows "sqrt $((y 2-y 1)^{\wedge} 2+(x 2-x 1)^{\wedge} 2)=10 * \operatorname{sqrt}(2)$ "
Informal Statement: If three flicks are equivalent to eight flecks, and six flocks are equivalent to four flecks, how many flocks are equivalent to 12 flicks? Show that it is 48.
Formal Statement:
theorem
fixes flick fleck flock : : real
assumes "flick &gt; 0" and "flock &gt; 0" and "fleck &gt; 0"
assumes "3 * flick = 8 * fleck"
and "6 * flock = 4 * fleck"
shows "48 * flock = 12 * flick"
...
$&lt;$ Informal statement to be formalized here $&gt;$</p>
<h2>C ADDITIONAL DISCUSSION</h2>
<p>DTV with GPT3.5. In Table 4, we additionally evaluate the performance of DTV when the informal solutions and their formalization are both generated from GPT3.5. Due to the cost of OpenAI API, we evaluate only on the MATH Number Theory category. It can be seen that by leveraging stronger models like GPT3.5, the performance of DTV can be further improved.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Problem Solve Rate</th>
<th style="text-align: center;">Minerva 62B</th>
<th style="text-align: center;">GPT-3.5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Single Sample</td>
<td style="text-align: center;">$12.2 \%$</td>
<td style="text-align: center;">$25.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Majority Voting</td>
<td style="text-align: center;">$23.7 \%$</td>
<td style="text-align: center;">$41.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">DTV Formalization with Minerva 62B</td>
<td style="text-align: center;">$31.9 \%$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DTV Formalization with GPT-3.5</td>
<td style="text-align: center;">$\mathbf{3 6 . 1 \%}$</td>
<td style="text-align: center;">$\mathbf{4 9 . 4 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of problem solve rates
DTV performance with number of samples. In the experiment section, we use 64 samples per problem for majority voting and DTV. In Figure 6, we plot the performance of majority voting and DTV against number of samples per problem on Number Theory. It can be seen that both majority voting and DTV improve as number of samples increase and DTV significantly outperform majority voting when the number of samples is around 20.
Informal answer extraction. To facilitate answer extraction, we include the following answer format in the few-shot prompt examples: The final answer is [placeholder]. and use regular expressions to extract the answer. In certain scenarios, extraction might be more challenging and not only restricted to the final answer. In this case, we could leverage various semantic parsing methods (Kamath \&amp; Das,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 6: Comparison of majority voting and DTV.
2018) and possibly use a language model to extract the informal reasoning as seen in Kojima et al. (2022) as well.</p>
<p>Potential improvement of filters. In the main text, we discuss and experiment with one symbolic filter that filters out vacuous formal statements. The filter can be leveraged to detect unfaithful statement translations. It is also possible to expand this into a set of filters that attempt to detect trivial formal statements, which are very likely to be unfaithful translations. One type of trivial statements could have the same goal appearing in the assumption already, which makes it provable immediately following the assumption. Another type could have the left-hand side of the goal being the same as that of right-hand side. Implementing these filters could potentially further improve the performance at a diminishing rate.</p>
<p>DTV helps filter incorrect answer solutions. Besides problem solve rate, we study to what extent DTV can reject incorrect answer solutions present in the informal solution samples. As seen in Table 5, without autoformalization, the portion of correct answer solutions from Minerva 62B is between $24.3 \%$ and $33.9 \%$ for different categories. By performing autoformalization and only considering informal solutions whose formal counterparts are proved, the percentage of correct answer solutions increase by more than $20 \%$, indicating the effectiveness of our approach in identifying correct answer solutions. GPT3.5 is better than Minerva 62B at this, which also supports the fact that GPT3.5 leads to higher problem solve rate in Table 1.</p>
<p>Table 5: Proportion of correct answer solutions generated by Minerva 62B before and after performing autoformalization. DTV is effective in discarding incorrect answer solutions and lead to better performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Correct Answer Rate</th>
<th style="text-align: center;">Number Theory</th>
<th style="text-align: center;">Algebra</th>
<th style="text-align: center;">Prealgebra</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">No autoformalization</td>
<td style="text-align: center;">$24.3 \%$</td>
<td style="text-align: center;">$31.0 \%$</td>
<td style="text-align: center;">$33.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">DTV Formalization with Minerva 62B</td>
<td style="text-align: center;">$50.5 \%$</td>
<td style="text-align: center;">$45.6 \%$</td>
<td style="text-align: center;">$45.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">DTV Formalization with GPT3.5</td>
<td style="text-align: center;">$62.3 \%$</td>
<td style="text-align: center;">$56.1 \%$</td>
<td style="text-align: center;">$54.9 \%$</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/albertqjiang/Portal-to-ISAbelle&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4654 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4654</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4654</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-104.html">extraction-schema-104</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <p><strong>Paper ID:</strong> paper-267028086</p>
                <p><strong>Paper Title:</strong> <a href="https://ojs.aaai.org/index.php/AAAI/article/download/29754/31298" target="_blank">Large Language Models Are Neurosymbolic Reasoners</a></p>
                <p><strong>Paper Abstract:</strong> A wide range of real-world applications is characterized by their symbolic nature, necessitating a strong capability for symbolic reasoning. This paper investigates the potential application of Large Language Models (LLMs) as symbolic reasoners. We focus on text-based games, significant benchmarks for agents with natural language capabilities, particularly in symbolic tasks like math, map reading, sorting, and applying common sense in text-based worlds. To facilitate these agents, we propose an LLM agent designed to tackle symbolic challenges and achieve in-game objectives. We begin by initializing the LLM agent and informing it of its role. The agent then receives observations and a set of valid actions from the text-based games, along with a specific symbolic module. With these inputs, the LLM agent chooses an action and interacts with the game environments. Our experimental results demonstrate that our method significantly enhances the capability of LLMs as automated agents for symbolic reasoning, and our LLM agent is effective in text-based games involving symbolic tasks, achieving an average performance of 88% across all tasks.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4654.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4654.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM agent (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompted LLM agent with external symbolic modules</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot, prompt-driven agent that uses an LLM (GPT-3.5-turbo, with experiments on GPT-4) as a policy model; at each timestep the agent is prompted with role initialization, current observation, inventory state and a constrained valid action set and may call external symbolic modules (calculator, sorter, navigator, knowledge-base) as actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A prompting-based policy using GPT-3.5-turbo (primary) that is initialized with a role and then issued an Action Query each timestep containing the current observation, inventory, score and a constrained valid-action set; the model selects one action (either an environment action or a symbolic-module action). Symbolic modules (Calculation, Sorting, Navigation, Knowledge Base) are callable as actions and return textual observations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3.5-turbo (primary); GPT-4 evaluated in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>TextWorldExpress text-based games (Arithmetic, MapReader, Sorting, Text World Common Sense (TWC))</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Four symbolic tasks: Arithmetic (solve math then pick/place matching quantity), MapReader (navigation + pick-and-place using map-derived routes), Sorting (sort objects by quantity and place them sequentially), TWC (collect objects and place them in commonsense locations using a KB module).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>No external memory beyond the LLM context; uses in-context memory via prompt (current observation, inventory state and symbolic-module outputs) rather than an explicit external memory store.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw textual observations, inventory contents, valid-action set and symbolic module responses are placed in the prompt each timestep; no separate structured key-value or embedding memory is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>The prompt is updated each timestep with the latest observation, inventory state, score and valid-action set; symbolic-module outputs are incorporated into subsequent prompts. There is no separate persistent memory update mechanism beyond this per-timestep prompt update.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Implicit retrieval by including necessary information in the prompt (i.e. the agent 'remembers' only what is fed back in the current prompt). No explicit retrieval-augmented generation, attention over a memory store, or indexing mechanism is described.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No formal ablation comparing explicit memory mechanisms. Analyses reported: (1) constrained prompts that include inventory, valid-action set and action constraints improve score across tasks and reduce number of steps (average score with constrained prompts = 0.887 on Test, without = 0.678 as reported); (2) GPT-4 outperforms GPT-3.5 on MapReader and Sorting but not on TWC; (3) qualitative failure modes linked to 'forgetting' — e.g., MapReader inefficiency arises because agent tends to forget navigation routes returned by the navigation module and must re-query routes, increasing steps; Sorting failures are tied to limited ability to remember the full ascending/descending order across multiple items.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>The paper reports limitations tied to memory-like behavior: (a) the agent 'forgets' routes obtained from the navigation module, causing repeated queries and inefficiency (more steps); (b) limited memory capacity of the prompt/LLM hampers the agent's ability to hold the full sort order in Sorting tasks; (c) the approach relies on in-context prompt information rather than an explicit persistent memory, making long-horizon planning and multi-step state retention fragile; (d) authors note uncertainty and error-proneness despite symbolic-module access.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Authors recommend stronger prompt engineering and constrained/detailed prompts (include task description, inventory, valid-action set, and action constraints) to improve performance and reduce steps; limit the number of valid actions presented; explicitly include inventory and symbolic-module outputs in the prompt; for future work integrate more sophisticated symbolic modules and richer memory/dynamic memory mechanisms (authors point to adding more detailed prompts and extending to domains beyond simple text games). They also show empirically that using more capable LLMs (GPT-4) improves some memory-demanding tasks (MapReader, Sorting).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Neurosymbolic Reasoners', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: an autonomous agent with dynamic memory and selfreflection <em>(Rating: 2)</em></li>
                <li>Behavior Cloned Transformers are Neurosymbolic Reasoners <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 1)</em></li>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 1)</em></li>
                <li>TextWorld: A learning environment for text-based games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4654",
    "paper_id": "paper-267028086",
    "extraction_schema_id": "extraction-schema-104",
    "extracted_data": [
        {
            "name_short": "LLM agent (this paper)",
            "name_full": "Prompted LLM agent with external symbolic modules",
            "brief_description": "A zero-shot, prompt-driven agent that uses an LLM (GPT-3.5-turbo, with experiments on GPT-4) as a policy model; at each timestep the agent is prompted with role initialization, current observation, inventory state and a constrained valid action set and may call external symbolic modules (calculator, sorter, navigator, knowledge-base) as actions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LLM agent",
            "agent_description": "A prompting-based policy using GPT-3.5-turbo (primary) that is initialized with a role and then issued an Action Query each timestep containing the current observation, inventory, score and a constrained valid-action set; the model selects one action (either an environment action or a symbolic-module action). Symbolic modules (Calculation, Sorting, Navigation, Knowledge Base) are callable as actions and return textual observations.",
            "llm_model_name": "GPT-3.5-turbo (primary); GPT-4 evaluated in experiments",
            "game_or_benchmark_name": "TextWorldExpress text-based games (Arithmetic, MapReader, Sorting, Text World Common Sense (TWC))",
            "task_description": "Four symbolic tasks: Arithmetic (solve math then pick/place matching quantity), MapReader (navigation + pick-and-place using map-derived routes), Sorting (sort objects by quantity and place them sequentially), TWC (collect objects and place them in commonsense locations using a KB module).",
            "memory_used": false,
            "memory_type": "No external memory beyond the LLM context; uses in-context memory via prompt (current observation, inventory state and symbolic-module outputs) rather than an explicit external memory store.",
            "memory_representation": "Raw textual observations, inventory contents, valid-action set and symbolic module responses are placed in the prompt each timestep; no separate structured key-value or embedding memory is reported.",
            "memory_update_mechanism": "The prompt is updated each timestep with the latest observation, inventory state, score and valid-action set; symbolic-module outputs are incorporated into subsequent prompts. There is no separate persistent memory update mechanism beyond this per-timestep prompt update.",
            "memory_retrieval_mechanism": "Implicit retrieval by including necessary information in the prompt (i.e. the agent 'remembers' only what is fed back in the current prompt). No explicit retrieval-augmented generation, attention over a memory store, or indexing mechanism is described.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "No formal ablation comparing explicit memory mechanisms. Analyses reported: (1) constrained prompts that include inventory, valid-action set and action constraints improve score across tasks and reduce number of steps (average score with constrained prompts = 0.887 on Test, without = 0.678 as reported); (2) GPT-4 outperforms GPT-3.5 on MapReader and Sorting but not on TWC; (3) qualitative failure modes linked to 'forgetting' — e.g., MapReader inefficiency arises because agent tends to forget navigation routes returned by the navigation module and must re-query routes, increasing steps; Sorting failures are tied to limited ability to remember the full ascending/descending order across multiple items.",
            "challenges_or_limitations": "The paper reports limitations tied to memory-like behavior: (a) the agent 'forgets' routes obtained from the navigation module, causing repeated queries and inefficiency (more steps); (b) limited memory capacity of the prompt/LLM hampers the agent's ability to hold the full sort order in Sorting tasks; (c) the approach relies on in-context prompt information rather than an explicit persistent memory, making long-horizon planning and multi-step state retention fragile; (d) authors note uncertainty and error-proneness despite symbolic-module access.",
            "best_practices_or_recommendations": "Authors recommend stronger prompt engineering and constrained/detailed prompts (include task description, inventory, valid-action set, and action constraints) to improve performance and reduce steps; limit the number of valid actions presented; explicitly include inventory and symbolic-module outputs in the prompt; for future work integrate more sophisticated symbolic modules and richer memory/dynamic memory mechanisms (authors point to adding more detailed prompts and extending to domains beyond simple text games). They also show empirically that using more capable LLMs (GPT-4) improves some memory-demanding tasks (MapReader, Sorting).",
            "uuid": "e4654.0",
            "source_info": {
                "paper_title": "Large Language Models Are Neurosymbolic Reasoners",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and selfreflection",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "Behavior Cloned Transformers are Neurosymbolic Reasoners",
            "rating": 2,
            "sanitized_title": "behavior_cloned_transformers_are_neurosymbolic_reasoners"
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 1,
            "sanitized_title": "inner_monologue_embodied_reasoning_through_planning_with_language_models"
        },
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 1,
            "sanitized_title": "language_models_as_zeroshot_planners_extracting_actionable_knowledge_for_embodied_agents"
        },
        {
            "paper_title": "TextWorld: A learning environment for text-based games",
            "rating": 1,
            "sanitized_title": "textworld_a_learning_environment_for_textbased_games"
        }
    ],
    "cost": 0.009561749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models Are Neurosymbolic Reasoners</p>
<p>Meng Fang meng.fang@liverpool.ac.uk 
University of Liverpool
United Kingdom</p>
<p>Eindhoven University of Technology
Netherlands</p>
<p>Shilong Deng shilong.deng@liverpool.ac.uk 
University of Liverpool
United Kingdom</p>
<p>Yudi Zhang y.zhang5@tue.nl 
Eindhoven University of Technology
Netherlands</p>
<p>Zijing Shi zijing.shi@student.uts.edu.au 
University of Technology Sydney
Australia</p>
<p>Ling Chen ling.chen@uts.edu.au 
University of Technology Sydney
Australia</p>
<p>Mykola Pechenizkiy m.pechenizkiy@tue.nl 
Eindhoven University of Technology
Netherlands</p>
<p>Jun Wang j.wang@cs.ucl.ac.uk 
University College London
United Kingdom</p>
<p>Large Language Models Are Neurosymbolic Reasoners
682140E380B72F329A1846E8B9FDF624
A wide range of real-world applications is characterized by their symbolic nature, necessitating a strong capability for symbolic reasoning.This paper investigates the potential application of Large Language Models (LLMs) as symbolic reasoners.We focus on text-based games, significant benchmarks for agents with natural language capabilities, particularly in symbolic tasks like math, map reading, sorting, and applying common sense in text-based worlds.To facilitate these agents, we propose an LLM agent designed to tackle symbolic challenges and achieve in-game objectives.We begin by initializing the LLM agent and informing it of its role.The agent then receives observations and a set of valid actions from the text-based games, along with a specific symbolic module.With these inputs, the LLM agent chooses an action and interacts with the game environments.Our experimental results demonstrate that our method significantly enhances the capability of LLMs as automated agents for symbolic reasoning, and our LLM agent is effective in text-based games involving symbolic tasks, achieving an average performance of 88% across all tasks.</p>
<p>Introduction</p>
<p>The ability to perform reasoning is crucial for AI due to its significant impact on various real-world tasks.The widespread adoption of large language models (LLMs), such as ChatGPT and GPT-4 (OpenAI 2023), has led to a series of remarkable successes in reasoning tasks, ranging from question &amp; answering to solving math problems.Among these challenges, text-based games serve as important benchmarks for agents with natural language capabilities and have garnered significant attention in the realm of language-centric machine learning research (Narasimhan, Kulkarni, and Barzilay 2015;Côté et al. 2018;Xu et al. 2020;Ryu et al. 2022;Shi et al. 2022).In these games, an agent uses language to interpret various scenarios and make decisions.The complexity of such games arises from the need for language comprehension, common sense, managing action spaces with combinatorial complexity, and the crucial importance of long-term memory and planning (Côté et al. 2018;Wang et al. 2022a).The challenges escalate in text-based games that involve symbolic tasks (Wang et al. 2022b).For instance, contemporary agents might be tasked with a scenario where they are required to solve a mathematical problem and simultaneously gather a specified amount of fruits, with the quantity needed being the solution to the math problem.</p>
<p>Using symbolic modules or external tools for arithmetic, navigation, sorting, and knowledge-base lookup is crucial for language agents, especially in complex text-based games (Lample and Charton 2020; Poesia, Dong, and Goodman 2021; Wang et al. 2022b;Qian et al. 2023).However, effectively integrating these aspects into language agents remains a relatively unaddressed challenge.Solving such textbased games requires interactive multi-step reasoning, and agents have most commonly been modeled using reinforcement learning (Xu et al. 2020;Yao et al. 2020;Xu et al. 2021).These methods, however, face challenges such as delayed rewards and difficulty in exploring large action spaces.Recently, there has been an exploration of imitation learning approaches, which utilize human play data (Wang et al. 2022b).While Behavior Cloning (BC) shows potential in effectively addressing these challenges, it often necessitates substantial effort and resources.This is primarily due to the need for acquiring expert data.</p>
<p>Recently, large language models (LLMs) have demonstrated notable in-context generalization capabilities, suggesting the potential to elicit reasoning abilities by prompting these models (Brown et al. 2020;Min et al. 2022).However, the application of LLMs in performing symbolic reasoning remains an under-explored area.Models like GPT-3.5 and GPT-4 have shown the ability to encode extensive information (OpenAI 2023).A significant example of this is their acquisition of substantial knowledge during training, enabling them to approach human-level performance across a wide range of tasks (OpenAI 2023).This indicates the feasibility of utilizing LLMs as neurosymbolic reasoners without relying on labeled gold training data.However, there is currently limited research on utilizing these models for reasoning tasks that involve logic, graphs, or symbolic formulas.The exploration and development of methods that leverage LLMs for symbolic reasoning is highly intriguing and Figure 1: The LLM agent is capable of interacting with the game environment, leveraging its reasoning abilities to determine the most suitable actions.These actions alter the environment's state and contribute to achieving the given objective.The environment, along with its corresponding symbolic modules, offers a valid set of actions to the LLM agent.The agent's responsibility is to select an action from this set.</p>
<p>The chosen action will then dictate how the agent interacts with either the game environment or the symbolic module.</p>
<p>holds significant potential impact.</p>
<p>In this paper, our aim is to investigate the role of Large Language Models (LLMs) in symbolic reasoning within the context of text-based games.When engaging in games that involve symbolic tasks, our LLM agent generates the most rational actions based on the observed game state in a zeroshot manner, assisted by external symbolic modules such as calculators or navigators, as illustrated in Figure 1.The LLM agent employs both the text-based game environment and symbolic modules to generate a list of valid actions.These valid actions, along with the current observation, are integrated into the prompt to direct the LLM agent in selecting an appropriate action.Subsequently, the LLM agent executes this action, interacting with both the game environment and symbolic modules to complete the task.</p>
<p>In summary, our contributions include:</p>
<p>• We introduce the use of LLMs for symbolic reasoning and provide a framework for employing the LLM agent as a neurosymbolic reasoner.This achievement underscores the potential of LLMs, with the support of external modules, to function as neurosymbolic reasoners, capable of successfully completing complex tasks.</p>
<p>• We have developed the LLM agent with tailored prompts, enabling it to effectively utilize symbolic modules and enhance its performance in text-based games that involve symbolic tasks.</p>
<p>Related Work</p>
<p>Large Language Models for Decision Making.LLMs have demonstrated notable capabilities, enabling their application in tasks that extend beyond language generation (OpenAI 2023).Furthermore, they are increasingly being grounded as policy models for decision-making in interactive contexts (Yang et al. 2023).Current studies focus on enhancing the decision-making capacity of LLMs through techniques such as prompting and in-context learning.For instance, Wei et al. (2022)  Text-based Game.Text-based games can be formally characterized as partially observable Markov decision processes (POMDPs) (Côté et al. 2018).In recent years, there has been a notable increase in the design of reinforcement learning (RL) agents to solve these games (Liu et al. 2021;Hendrycks et al. 2021;Osborne, Nõmm, and Freitas 2022) More recently, with the advancement of LLMs, research has shifted towards using prompts to enable LLMs to solve textbased games (Yao et al. 2022;Shinn, Labash, and Gopinath 2023).However, these efforts have primarily focused on the LLMs' capability for in-context learning, while the exploration of their potential in symbolic reasoning has been rel-atively overlooked.</p>
<p>Neurosymbolic Reasoning.The field of neurosymbolic reasoning combines the capabilities of deep neural networks with symbolic reasoning, significantly reducing the search space associated with symbolic techniques.This approach has been used to tackle various complex multi-step inference challenges, including tasks like multi-hop question answering (Weber et al. 2019)</p>
<p>Preliminaries</p>
<p>Text-based Games as POMDPs.Text-based games can be formally defined as partially observable Markov decision processes (POMDPs), considering that the agent only observes partial information about the environment at each turn (Sutton and Barto 2018).In games with symbolic modules, at each discrete time step t, the agent is provided with an observation denoted as o t and is given a task description denoted as d.The symbolic module then produces a collection of valid actions, denoted as A t,SyM , while the text game environment concurrently establishes its own set of proper actions, denoted as A t,Env .Consequently, the set of acceptable actions at time step t is the union of these two sets, denoted as A t = A t,Env ∪ A t,SyM .The agent's goal is to select an action a t from the set of valid actions A t , given the observation o t and the task description d.If a t belongs to the set A t,SyM , the symbolic module generates the next observation o t+1 .Conversely, if a t is not part of A t,SyM , the text-based game environment processes a t and produces both the subsequent observation o t+1 and the reward r t .</p>
<p>Symbolic Tasks.There are four distinct tasks within textbased games, namely Arithmetic, MapReader, Sorting and Text World Common Sense (TWC) (Wang et al. 2022b).</p>
<p>Each task is equipped with its own symbolic modules designed to assist agents in successfully accomplishing the task.</p>
<p>Methodology</p>
<p>We introduce an LLM agent, namely a language agent, for employing LLMs2 to engage in text-based games by leveraging symbolic modules in a zero-shot manner.We begin with an overview of playing games using symbolic modules, followed by a detailed description of the key design features of our language agent, including its prompting mechanism.</p>
<p>Interaction Action Query</p>
<p>Role: Your first task is to solve the math problem.Then, pick up the item with the same quantity as the math problem answer, and place it in the box.</p>
<p>Role Initialization</p>
<p>Step 2:</p>
<p>Step 3:</p>
<p>LLM Agent</p>
<p>Step 1:</p>
<p>Action: read math problem</p>
<p>Game Symbolic Module</p>
<p>Observation: You are in the laundry room.In one part of the room you see a bench that has 23 peas, 936 squashes on it.There is also a math problem.You also see a box, that is empty.</p>
<p>Playing Games with Symbolic Tasks</p>
<p>We describe the process of playing games that involve symbolic tasks, using the LLM agent in conjunction with external symbolic modules.</p>
<p>Symbolic Modules.Symbolic modules play a crucial role in maximizing the reasoning capabilities of LLMs.For example, as shown in Figure 2, consider a scenario where a mathematical problem is presented, and a calculator is available.In such cases, the LLM's reasoning can effectively use the calculator to complete the task in a zero-shot manner.Furthermore, symbolic modules are adept at their functions, as employing an external tool like a calculator is considered an action in itself.</p>
<p>The scenarios include four distinct symbolic modules: the Calculation Module, Sorting Module, Knowledge Base Module, and Navigation Module.Table 1 shows examples of how these symbolic modules are utilized.The observation produced by a symbolic module indicates the current state of the game, while the action selected by the LLM agent serves as the input.Additionally, the Navigation Module requires the previous observation as input to accurately determine Your task is to pick up objects, then place them in their usual locations in the environment.</p>
<p>INPUT: query clean brown shirt RESPONSE: Clean brown shirt is expected to be located at wardrobe.</p>
<p>Table 1: Text-based games with symbolic tasks and their corresponding symbolic modules.INPUT refers to the current action that is sent to the symbolic modules.RESPONSE denotes the responses generated by the symbolic modules at the present time.the player's current position.For instance, in a mathematical task, the LLM agent may select a computational action such as "multiply 8 by 7" (mul 8 7).This action triggers the symbolic module to calculate the product, and the resulting observation, "Multiplying 8 and 7 results in 56," is then returned.</p>
<p>The process of engaging in text-based games with LLMs involves multiple stages.The specifics of these steps are detailed in Figure 2. As mentioned earlier, the comprehensive environment, comprising both the symbolic modules and the text-based game environment, presents the LLM agent with a list of allowable actions.Upon receiving an observation, the LLM agent uses its symbolic reasoning to select an action from this list.If the chosen action involves the symbolic module, the module provides the next observation; otherwise, the text-based game environment supplies the subsequent observation.</p>
<p>LLM as the Neurosymbolic Reasoner</p>
<p>We investigate whether the accumulated world knowledge of LLMs can aid in making accurate decisions for downstream symbolic tasks.To ground LLMs in text-based games, we employ a prompting approach, which eliminates the need for costly additional training.Therefore, we construct prompts in a way that incorporates external context, enabling the LLM agent to generate reasonable actions.</p>
<p>We describe the role of the agent, incorporating the observation, valid actions, and the constraints of executing the action in the prompt, as it is not easy for the LLM agent to understand the underlying rules of the environment through interacting with game environments.The key components of our approach include:</p>
<p>• Role initialization: We initialize the agent by providing them with task descriptions and action constraints.</p>
<p>• Action Query: This step is repeated at each timestep.We prompt the LLM agent with the current observation, inventory state, valid action set, and a question.</p>
<p>Role</p>
<p>Initialization</p>
<p>You are a robot.{TASK DESC}\n You are required to choose action from the valid action set to complete the task step by step.\nTo take action, respond with an action in the valid action set.\n Action Query {OBS}\n {INV STATE}\n Your current score is: {SCORE}\n The valid action set contains: {VALID ACT SET}.\nPlease choose one action from the valid action set to finish the task step by step.\nDo NOT respond with any other text, and you cannot decline to take an action.</p>
<p>Table 2: The prompting format for role initialization and action query for each time step.{TASK DESC} is the task description.{OBS} is the current observation.{INV STATE} describes the items in your inventory.{SCORE} is the obtained reward.{VALID ACT SET} is a set of valid actions at the current time step.</p>
<p>• Answer by the LLM agent: The LLM agent chooses an action from the valid action set to complete the task.</p>
<p>Role Initialization.We initialize the role and provide instructions for a functional agent assigned to a task.This process informs the agent about its role, the task description, and the actions it can take, along with their explanations and constraints.These actions are necessary for interacting with text-based games or calling the symbolic module.The agent is instructed to choose from a valid set of actions, such as reading the map, getting paths to specific locations, and recalling the task.Additionally, the agent is advised to utilize the external symbolic module and to avoid unnecessary actions during the task.</p>
<p>Action Query.At each timestep, we inform the LLM agent of the current game state, as outlined in To sort the items one by one, please follow the instruction:\n 1) choose 'sort ascending' or 'sort descending' to know which one should be sort next.\n2) take the items.\n3) put the items in box.\nTWC 1) When you take the item, you will get positive score.\n2) When you put the item in the right place, you will get higher positive score.Otherwise you get 0.\n 3) You are supposed to get as much score as possible.\nTable 3: The prompting format for adding constraints on the actions of an agent.</p>
<p>the inventory, the reward, and the valid action set.The inventory state refers to the current possessions of the agent.For instance, in mathematical tasks, the inventory state may consist of a mathematical problem, while in the MapReader task, it could include a map.Additionally, the inventory state can encompass tangible objects, such as toothpaste or a quantity of 18 avocados, acquired by the agent within the environment.The LLM agent is then tasked with selecting one action from the valid action set to continue with the task.</p>
<p>It is important to note that the LLM agent is not allowed to decline or provide any text beyond the prescribed response.</p>
<p>We also limit the number of valid actions provided by the symbolic module.</p>
<p>In addition, it is essential to develop appropriate prompts that effectively restrict the agent's actions according to the information provided in Table 3.It is not feasible for the agent to acquire knowledge and infer the rules within trajectories solely through its interaction with the environment.In all tasks, there is typically a specific order of events, where the object is first taken and then placed in a designated lo-cation.This strategy is adopted to prevent scenarios where the object is placed before it is acquired, which would be considered unacceptable in the given context.</p>
<p>Experiments</p>
<p>We demonstrate the potential of LLMs in serving as neurosymbolic reasoners for text-based games.In particular, we present experimental results on four text-based games that involve different symbolic tasks.In these tasks, we observe that LLMs can effectively function as symbolic reasoners.</p>
<p>Setup</p>
<p>We follow the evaluation framework and game environments in Wang et al. (2022b).These games are developed using the TextWorldExpress game engine (Jansen and Cote 2023).For our LLM agent, we use GPT-3.5-turbo.The LLM agent can interact with game environments and symbolic modules.The task descriptions and examples of how the symbolic modules are called are provided in Table 1.The evaluation includes four text-based games involving symbolic tasks.Each task is divided into "Train", "Dev", and "Test" sets.All evaluations are conducted on the "Test" set.</p>
<p>The evaluation metric is based on two factors: the average score achieved at the end of each game, and the average number of steps taken within a single episode.</p>
<p>Environments</p>
<p>We use four text-based game benchmark environments (Wang et al. 2022b):</p>
<p>Arithmetic.The task at hand involves a mathematical component, wherein an agent is required to read and solve a mathematical problem.This process determines the specific object from a given set of objects that they should select and place.The arithmetic game includes a calculator module equipped with the capability to perform basic mathematical operations, including addition, subtraction, multiplication, and division.</p>
<p>MapReader.A pick-and-place game with a navigation theme, similar to the Coin Collector game (Yuan et al. 2018).The agent is equipped with a map that may be exploited to optimize route planning.The map provides information on the connections between rooms, such as the lounge connecting to the cookery and supermarket.The navigation symbolic module has the capability to extract location information from the observation space.This includes specific information relating to the present location and geographical features leading to the intended destination.For instance, the instructions sent to the agent might indicate that in order to get from the cooking area to the recreation zone, one must pass through the bar, steam room, library, and finally reach the recreation zone.</p>
<p>Sorting.This game involves an agent initially situated in a room containing a variable number of objects, ranging from three to five.The agent's task is to sequentially place these objects into a designated box, adhering to a specific sorting criterion based on increasing quantity.In this game, units related to volume, mass, or length are used, as exemplified by items such as 25g of oak, 12ml of marble, and 6cm of cedar.The sorting game includes a module capable of extracting information from the observation space.This module is specifically designed to identify items that include quantities and can arrange these objects in either ascending or descending order, following the user's instructions.</p>
<p>Text World Common Sense (TWC).The challenges provided in this game serve as a baseline for evaluating common sense reasoning abilities (Murugesan et al. 2021).In this game, agents are required to gather objects from their surroundings, such as a clean brown shirt, and subsequently place these objects in their appropriate and commonly recognized locations, like a wardrobe.The incorporation of a symbolic module within this game enables agents to engage in knowledge-based queries.For instance, it allows them to deduce that a clean brown shirt is typically found in a wardrobe.</p>
<p>Baselines</p>
<p>We also compare our LLM agent with two baselines, namely the Deep Reinforcement Relevance Network (DRRN) (He et al. 2016) and the T5-based Behavior Cloned Transformer (Raffel et al. 2020;Wang et al. 2022b), as follows:</p>
<p>• DRRN: The primary concept of the DRRN is based on Q-learning.The candidate action with the highest anticipated Q-value is chosen as the next action, based on the current observation.The DRRN employs a Deep Q-Network (Mnih et al. 2013) to estimate the Q-value for each observation-action pair.Xu et al. (2020) note that the DRRN is a fast and robust reinforcement learning baseline, frequently used to produce near state-of-the-art performance in a variety of text-based games.</p>
<p>• Behavior Cloned Transformer: This method adopts an imitation learning approach, conceptualizing reinforcement learning as a sequence-to-sequence problem, similar to the Decision Transformer (Chen et al. 2021).It predicts the subsequent action based on a sequence of previous observations.This baseline aligns with the approach described in Ammanabrolu et al. (2021), where the model input at timestep t includes the task description, current state observation, previous action, and previous state observation.Symbolic modules are utilized in the demonstrations, specifically employing gold trajectories.</p>
<p>Following Wang et al. (2022b), both baseline models include two variants: one with symbolic modules and one without.When using symbolic modules, we inject actions from these modules into the action space of each game for the baseline models.</p>
<p>Results</p>
<p>Based on the results presented in Table 4, it is evident that the use of the symbolic module in conjunction with the LLM agent yields a favorable average performance compared to other baseline approaches.When comparing the outcomes of the Behavior Cloned Transformer with a symbolic module to those of the LLM agent, the performance of the LLM agent is observed to be slightly lower.However, the LLM agent demonstrates a similar level of competency in interacting with the game environment.Furthermore, unlike the Behavior Cloned Transformer models, the LLM agent does not require extensive training with a large volume of expert data.As a result, this approach saves significant training resources.</p>
<p>Table 5 demonstrates that the LLM agent possesses a robust capacity for reasoning, enabling effective handling of tasks involving symbolic tasks.It shows exceptional performance, particularly in mathematics.In the MapReader benchmark, the agent achieves commendable scores, though it requires a considerable number of steps to complete the task.This inefficiency is mainly due to the agent's tendency to forget the route obtained from the symbolic module, leading to the risk of reaching incorrect locations and necessitating repeated route queries.The complexity of map logic, which involves determining one's current location and desired destination, adds to the probabilistic nature of this task.In contrast, the Sorting task reveals suboptimal performance, as the LLM agent's understanding of sorting logic is not fully developed.This issue is largely attributed to the agent's limited memory capacity, hindering its ability to remember the ascending order of all objects.</p>
<p>In Table 6, it compares the performance of the model with constrained prompts to that of the model without constrained prompts.The results indicate that when the LLM agent is provided with the prompts outlined in Table 3, there is an improvement in performance across all tasks.Additionally, a reduction in the average number of steps required to interact with the game environment is observed.This demonstrates the effectiveness of our constrained prompts in these tasks.Furthermore, experimental results using GPT-4, as shown in  3.</p>
<p>Table 7, reveal that it significantly outperforms the GPT-3.5 agent in the MapReader and Sorting tasks, while showing weaker performance in the TWC task.</p>
<p>Discussion.Our results demonstrate that the incorporation of external symbolic modules by the LLM agent leads to enhanced average accuracy compared to other baselines.This capability is achieved by leveraging the underlying patterns present in the training data.Instead of relying on symbolic thinking or explicit rules, this approach acquires knowledge by recognizing patterns and associations from the extensive corpus of text to which it has been exposed during its training phase, as exemplified by GPT-3.5 and GPT-4 (OpenAI 2023).Although the LLM agent has the capability to connect with a symbolic module for specific tasks, it still exhibits uncertainty and is prone to making mistakes.</p>
<p>Conclusion</p>
<p>This paper has demonstrated the effective application of Large Language Models (LLMs) in complex text-based games involving symbolic tasks.Utilizing a prompting approach, we have guided the LLM agent to efficiently engage with symbolic modules within these games.The efficacy of our method, leveraging LLMs, has shown superior performance compared to alternative benchmarks, highlighting the potential of LLMs to enhance training procedures in text-based games.Consequently, it can be posited that Large Language Models can be considered as Neurosymbolic Reasoners, possessing significant potential for performing sym- The performance of the LLM agent using GPT-3.5 and GPT-4 on the "Test" set.</p>
<p>bolic tasks in real-world applications.</p>
<p>Limitations</p>
<p>The addition of more detailed prompts could offer greater control over the actions of the LLM agent.This would be particularly beneficial in tasks like Sorting, where providing essential information beforehand is advantageous.Acknowledging and addressing these limitations could significantly enhance the system's performance.For future progress, it is crucial to extend the model's application to more complex domains, going beyond the scope of straightforward textbased games.Integrating more sophisticated symbolic modules would be necessary to tackle the complexities of diverse scenarios, thereby facilitating a more efficient problemsolving approach.</p>
<p>, language contextualization (Zellers et al. 2021), and semantic analysis (Cambria et al. 2022).Text-based games that involve symbolic tasks serve as a valuable test-bed for addressing such challenges.Previous approaches have employed traditional optimization techniques or reinforcement learning agents.For example, Kimura et al. (2021a) decompose text-based games into collections of logical rules, which are then integrated with deep reinforcement learning.Basu et al. (2022) use Integer Linear Programming (ILP) to substantially improve agent performance, providing an interpretable framework for understanding agents' selection of specific actions.</p>
<p>Your task is to solve the following math problem: multiply 26 and 36 .Then, pick up the item with the same quantity as the answer, and place it in the box.</p>
<p>Figure 2 :
2
Figure 2: An overview of how an LLM agent plays textbased games with external symbolic modules.The following procedural steps are involved in utilizing the LLM agent for engaging in a text-based game.Initially, the LLM agent is provided with a role initialization prompt.The first observation received by the LLM agent comes from the text game environment.As depicted in the diagram, the selection of actions, determined by the LLM's reasoning, activates the symbolic module.Subsequently, the symbolic module provides output, including observations related to the module.Then the next action chosen by the LLM agent is influenced by the outcome from the symbolic module.This process is executed repeatedly until the end of the game.</p>
<p>Your first task is to solve the math problem.Then, pick up the item with the same quantity as the math problem answer, and place it in the box.Your task is to take the coin located in the pantry, and put it into the box found in the chamber.A map is provided, that you may find helpful.
Task(SymbolicDescriptionSymbolic ModuleModule)Arithmetic (Calculation Module)INPUT: mul 8 7 RESPONSE: Multiplying 8 and 7 results in 56.MapReader (Navigation Module)INPUT: next step to pantry RESPONSE: The next location to go to is canteen. If you want to go to pantry from chamber, you need go through canteen, pantry.Sorting (Sorting Module)Your task is to sort objects by quantity. First, place the ob-ject with the smallest quantity in the box. Then, place the objects with the next smallest quantity in the box, and re-peat until all objects have been placed in the box.INPUT: sort ascending RESPONSE: The observed items, sorted in order of in-creasing quantity, are: 25 g of oak, 47 g of brick, 15 kg of cedar, 21 kg of marble.TWC(KnowledgeBase Module)</p>
<p>Table 2
2. This</p>
<p>Table 4 :
4
The average performance of the model across a set of 100 games in the unseen test set."+symbolic module" indicates the utilization of symbolic modules within the action space of the models.
DRRNBehavior Cloned TransformerLLM AgentBaseline+symbolic moduleBaseline+symbolic moduleBenchmark Score Steps ScoreStepsScore Steps ScoreStepsScore StepsArithmetic0.17100.1470.5651.0051.004MapReader 0.02500.02500.71271.00100.8615Sorting0.03210.03180.7270.9880.717TWC0.57270.37340.9060.9730.944Average0.20270.14270.72110.9970.887</p>
<p>Table 6 :
6
The performance of the LLM agent with and without constrained prompts on the "Test" set.The constrained prompts are shown in Table
TaskTrain Score Steps Score Steps Score Steps Dev TestArithmetic1.0030.9541.004MapReader 0.84150.84140.8615Sorting0.7070.6360.717TWC0.9340.835 50.944Average0.8770.8170.887Table 5: The performance of the LLM agent on differentsets of the game, including "Train", "Dev", and "Test". Thescores are subjected to normalization, resulting in valuesranging from 0 to 1, with higher values indicating greaterperformance. On the other hand, the steps quantify the num-ber of actions taken by an agent inside the environment, withlower values indicating more efficient behavior.Taskw/ Constraints w/o Constrains Score Steps Score StepsArithmetic 1.0040.963MapReader 0.86150.6412Sorting0.7170.3510TWC0.9440.737Average0.8870.678</p>
<p>Table 7 :
7Taskw/ GPT-3.5 Score Steps Score Steps w/ GPT-4Arithmetic1.0041.004MapReader 0.86150.997Sorting0.7170.938TWC0.9440.7116Average0.8870.918
The Thirty-Eighth AAAI Conference on Artificial Intelligence 
We utilize LLMs from OpenAI: https://chat.openai.com/.</p>
<p>Learning dynamic belief graphs to generalize on text-based games. A Adhikari, X Yuan, M.-A Côté, M Zelinka, M.-A Rondeau, R Laroche, P Poupart, J Tang, A Trischler, W Hamilton, Advances in Neural Information Processing Systems. 2020</p>
<p>Graph constrained reinforcement learning for natural language action spaces. P Ammanabrolu, M Hausknecht, International Conference on Learning Representations. 2020</p>
<p>How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds. P Ammanabrolu, J Urbanek, M Li, A Szlam, T Rocktäschel, J Weston, K Basu, K Murugesan, M Atzeni, P Kapanipathi, K Talamadupula, T Klinger, M Campbell, M Sachan, G Gupta, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2021. 2022Combining Learning and Reasoning: Programming Languages, Formalisms, and Representations</p>
<p>E Brooks, L Walls, R L Lewis, S Singh, arXiv:2210.03821context policy iteration. 2022arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in Neural Information Processing Systems. 2020</p>
<p>SenticNet 7: A commonsense-based neurosymbolic AI framework for explainable sentiment analysis. E Cambria, Q Liu, S Decherchi, F Xing, K Kwok, Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation Conference2022</p>
<p>Decision transformer: Reinforcement learning via sequence modeling. L Chen, K Lu, A Rajeswaran, K Lee, A Grover, M Laskin, P Abbeel, A Srinivas, I Mordatch, Advances in Neural Information Processing Systems. 2021</p>
<p>Textworld: A learning environment for textbased games. M.-A Côté, A Kádár, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, M Hausknecht, L El Asri, M Adada, Computer Games: 7th Workshop. 2018. 2018Held in Conjunction with the 27th International Conference on Artificial Intelligence</p>
<p>Deep Reinforcement Learning with a Combinatorial Action Space for Predicting Popular Reddit Threads. J He, M Ostendorf, X He, J Chen, J Gao, L Li, L Deng, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language Processing2016</p>
<p>What would jiminy cricket do? Towards agents that behave morally. D Hendrycks, M Mazeika, A Zou, S Patel, C Zhu, J Navarro, D Song, B Li, J Steinhardt, Advances in Neural Information Processing Systems. 2021</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, International Conference on Machine Learning. 2022a</p>
<p>W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, arXiv:2207.05608Inner monologue: Embodied reasoning through planning with language models. 2022barXiv preprint</p>
<p>TextWorldExpress: Simulating Text Games at One Million Steps Per Second. P Jansen, M Cote, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations. the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations2023</p>
<p>LOA: Logical Optimal Actions for Text-based Interaction Games. D Kimura, S Chaudhury, M Ono, M Tatsubori, D J Agravante, A Munawar, A Wachi, R Kohita, A Gray, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations2021a</p>
<p>Neuro-Symbolic Reinforcement Learning with First-Order Logic. D Kimura, M Ono, S Chaudhury, R Kohita, A Wachi, D J Agravante, M Tatsubori, A Munawar, A Gray, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021b</p>
<p>Reward design with language models. M Kwon, S M Xie, K Bullard, D Sadigh, International Conference on Learning Representations. Lample, G.; and Charton, F. 2020. Deep Learning For Symbolic Mathematics. 2023International Conference on Learning Representations</p>
<p>Code as policies: Language model programs for embodied control. J Liang, W Huang, F Xia, P Xu, K Hausman, B Ichter, P Florence, A Zeng, 2023 IEEE International Conference on Robotics and Automation. 2023</p>
<p>Learning object-oriented dynamics for planning from text. G Liu, A Adhikari, A.-M Farahmand, P Poupart, International Conference on Learning Representations. 2021</p>
<p>Self-refine: Iterative refinement with selffeedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, arXiv:2303.176512023arXiv preprint</p>
<p>MetaICL: Learning to Learn In Context. S Min, M Lewis, L Zettlemoyer, H Hajishirzi, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, arXiv:1312.5602Playing atari with deep reinforcement learning. 2013arXiv preprint</p>
<p>Text-based rl agents with commonsense knowledge: New challenges, environments and baselines. K Murugesan, M Atzeni, P Kapanipathi, P Shukla, S Kumaravel, G Tesauro, K Talamadupula, M Sachan, M Campbell, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Language Understanding for Text-based Games using Deep Reinforcement Learning. K Narasimhan, T Kulkarni, R Barzilay, ArXiv, abs/2303.08774Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 1-11. OpenAI. 2023. GPT-4 Technical Report. the 2015 Conference on Empirical Methods in Natural Language Processing, 1-11. OpenAI. 2023. GPT-4 Technical Report2015</p>
<p>A survey of text games for reinforcement learning informed by natural language. P Osborne, H Nõmm, A Freitas, Transactions of the Association for Computational Linguistics. 2022</p>
<p>Contrastive reinforcement learning of symbolic reasoning domains. G Poesia, W Dong, N Goodman, Advances in Neural Information Processing Systems. 2021</p>
<p>Limitations of Language Models in Arithmetic and Symbolic Induction. J Qian, H Wang, Z Li, S Li, X Yan, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of machine learning Research. 2112020</p>
<p>Sword Cuts: Commonsense Inductive Bias for Exploration in Text-based Games. D Ryu, E Shareghi, M Fang, Y Xu, S Pan, R Haf, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Short Papers. the 60th Annual Meeting of the Association for Computational Linguistics20222</p>
<p>Stay moral and explore: Learn to behave morally in text-based games. Z Shi, M Fang, Y Xu, L Chen, Y Du, International Conference on Learning Representations. 2022</p>
<p>Reflexion: an autonomous agent with dynamic memory and selfreflection. N Shinn, B Labash, A Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>Progprompt: Generating situated robot task plans using large language models. I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, IEEE. 2023. 2023</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 2018MIT press</p>
<p>ChatGPT for Robotics: Design Principles and Model Abilities. S Vemprala, R Bonatti, A Bucker, A Kapoor, R Wang, P Jansen, M.-A Côté, P Ammanabrolu, MSR-TR-2023-8Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingMicrosoft2023. 2022aTechnical ReportScienceWorld: Is your Agent Smarter than a 5th Grader?</p>
<p>Behavior Cloned Transformers are Neurosymbolic Reasoners. R Wang, P A Jansen, M.-A Côté, P Ammanabrolu, Conference of the European Chapter. the Association for Computational Linguistics2022b</p>
<p>NLProlog: Reasoning with Weak Unification for Question Answering in Natural Language. L Weber, P Minervini, J Münchmeyer, U Leser, T Rocktäschel, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Generalization in Text-based Games via Hierarchical Reinforcement Learning. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E H Chi, Q V Le, D Zhou, Findings of the Association for Computational Linguistics: EMNLP 2021. Xu, Y2022. 2021Advances in Neural Information Processing Systems</p>
<p>Perceiving the World: Question-guided Reinforcement Learning for Text-based Games. Y Xu, M Fang, L Chen, Y Du, J Zhou, C Zhang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational Linguistics2022</p>
<p>Deep reinforcement learning with stacked hierarchical attention for text-based games. Y Xu, M Fang, L Chen, Y Du, J T Zhou, C Zhang, Advances in Neural Information Processing Systems. 2020</p>
<p>Keep CALM and Explore: Language Models for Action Generation in Text-based Games. S Yang, O Nachum, Y Du, J Wei, P Abbeel, D Schuurmans, S Yao, R Rao, M Hausknecht, K Narasimhan, arXiv:2303.04129Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language Processing2023. 2020arXiv preprintFoundation models for decision making: Problems, methods, and opportunities</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, X Yin, J May, arXiv:2210.03629IEEE Conference on Games. 2022. 2019arXiv preprintComprehensible context-driven text game playing</p>
<p>PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World. X Yuan, M.-A Côté, A Sordoni, R Laroche, R T D Combes, M Hausknecht, A Trischler, R Zellers, A Holtzman, M Peters, R Mottaghi, A Kembhavi, A Farhadi, Y Choi, arXiv:1806.11525Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing2018. 20211arXiv preprintCounting to explore and generalize in text-based games</p>            </div>
        </div>

    </div>
</body>
</html>
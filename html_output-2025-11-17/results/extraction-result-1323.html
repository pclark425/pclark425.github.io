<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1323 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1323</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1323</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-273375004</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.12690v1.pdf" target="_blank">Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators</a></p>
                <p><strong>Paper Abstract:</strong> A critical bottleneck for scientific progress is the costly nature of computer simulations for complex systems. Surrogate models provide an appealing solution: such models are trained on simulator evaluations, then used to emulate and quantify uncertainty on the expensive simulator at unexplored inputs. In many applications, one often has available data on related systems. For example, in designing a new jet turbine, there may be existing studies on turbines with similar configurations. A key question is how information from such"source"systems can be transferred for effective surrogate training on the"target"system of interest. We thus propose a new LOcal transfer Learning Gaussian Process (LOL-GP) model, which leverages a carefully-designed Gaussian process to transfer such information for surrogate modeling. The key novelty of the LOL-GP is a latent regularization model, which identifies regions where transfer should be performed and regions where it should be avoided. This"local transfer"property is desirable in scientific systems: at certain parameters, such systems may behave similarly and thus transfer is beneficial; at other parameters, they may behave differently and thus transfer is detrimental. By accounting for local transfer, the LOL-GP can rectify a critical limitation of"negative transfer"in existing transfer learning models, where the transfer of information worsens predictive performance. We derive a Gibbs sampling algorithm for efficient posterior predictive sampling on the LOL-GP, for both the multi-source and multi-fidelity transfer settings. We then show, via a suite of numerical experiments and an application for jet turbine design, the improved surrogate performance of the LOL-GP over existing methods.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1323.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1323.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Jet-turbine FEA (MATLAB module)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MATLAB finite-element-analysis (FEA) module for turbine blade stress</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A MATLAB-based FEA simulator (cited as [51] in the paper) used to compute steady-state thermal + structural stress fields of a turbine blade; used as the expensive high-fidelity simulator in the jet-turbine surrogate experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MATLAB FEA module (turbine stress simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Finite-element-analysis simulator coupling static-structural mechanics with steady-state thermal expansion to compute stress distributions on a turbine blade; solves governing PDEs via mesh discretization and returns scalar responses (maximum stress at blade root).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / structural stress analysis (thermomechanics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High-fidelity (FEA with fine mesh); in experiments a high-fidelity target used mesh size 0.02 with lower-fidelity variants at coarser mesh sizes (0.04, 0.06).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Fidelity differences realized by mesh density: finer mesh (0.02) resolves more geometric/thermal gradients and stress concentrations; coarser meshes (0.04, 0.06) are computationally cheaper but smooth or miss local high gradients; physics (thermal expansion + static structural equations) are present at all fidelities but spatial resolution differs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LOL-GP (and benchmarks: GP, KO, BKO, TLNet)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Probabilistic surrogate (Local transfer Learning Gaussian Process - LOL-GP) trained on simulator evaluations, with comparative baselines including standard GP, Kennedy–O'Hagan (KO), Bayesian KO (BKO), and a neural-network TLNet.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Surrogate modeling to predict maximum stress at blade root as a function of operating conditions (pressure, temperature) and to provide calibrated uncertainty for downstream decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Transfer from source data (different blade material) or from lower-fidelity FEA meshes to the high-fidelity target FEA (i.e., emulate high-fidelity stress outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Qualitative: LOL-GP yielded the best RMSE and CRPS among compared methods and mitigated cases where other transfer methods produced negative transfer; exact numeric performance for the jet-turbine experiment is reported in the paper but is not unambiguously tabulated in the PDF text extract provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Multi-fidelity experiments used nested meshes (0.06, 0.04, 0.02). Transfer from coarser meshes to the fine-mesh high-fidelity target was effective when local similarity existed; LOL-GP outperformed KO/BKO/TLNet by identifying regions where mesh-based low-fidelity data were informative and suppressing transfer where they were not.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>No explicit minimal-fidelity threshold is given; authors emphasize that local similarity between fidelities (i.e., where low- and high-fidelity outputs are close) is necessary for beneficial transfer, and that coarse meshes may be sufficient in regions where physics are well-resolved but not where local gradients dominate.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Other transfer approaches (KO, BKO, TLNet) sometimes exhibited negative transfer (worse RMSE/CRPS than no-transfer GP) when lower-fidelity meshes misrepresented local stress behavior; the paper reports that TLNet in particular experienced negative transfer in some settings and that KO/BKO can fail when transfer is applied in regions where physics differ.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1323.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1323.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-fidelity mesh simulators (Forrester/Branin experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synthetic multi-fidelity simulators constructed from prototypical test functions (Forrester, Branin) with low/high-fidelity variants</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Synthetic low-, medium-, and high-fidelity simulators created by perturbing canonical test functions (Forrester 1D, Branin 2D) to produce locally differing behavior; used to benchmark multi-fidelity transfer performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Synthetic multi-fidelity Forrester / Branin variants (f1, f2, f3)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Artificial simulator functions: the high-fidelity function is the canonical test function (Forrester or Branin); lower-fidelity versions are formed by adding localized perturbation/discrepancy terms so that source and target are similar in some regions and different in others (creating 'local transfer' structure).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>methodology / surrogate modeling (toy numerical experiments) — not real physics but intended to mimic properties of physical simulators</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Multiple fidelity levels synthetically defined: high-fidelity = canonical function; low/medium fidelity = canonical function plus localized additive terms (thus lower fidelity in regions with added discrepancies).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Differences are realized by additive localized discrepancy functions (piecewise functions, oscillatory terms) that make low-fidelity outputs deviate from high-fidelity in particular regions; all other aspects are exact (deterministic functions with no numerical noise).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LOL-GP (and benchmarks GP, KO, BKO, TLNet)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Surrogate models (Gaussian-process-based LOL-GP with latent ReLU-regularized transfer weights; baselines include standard GP, KO, Bayesian KO, and a neural-network TLNet).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Predict high-fidelity outputs (emulation) across the domain given limited high-fidelity data and abundant lower-fidelity / source data; identify where transfer is beneficial (local transfer).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>High-fidelity canonical function (held-out test points) — i.e., transfer from lower-fidelity variants to emulate/predict high-fidelity outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Numeric results (test performance after training reported in paper): For multi-fidelity Forrester (1D) LOL-GP RMSE 0.127, CRPS 0.062 (best among compared); for multi-fidelity Branin (2D) LOL-GP RMSE 0.064, CRPS 0.034 (best among compared). Other methods sometimes showed worse RMSE/CRPS or negative transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Comparisons show that naively transferring information from low-fidelity variants can harm performance when discrepancies are localized; methods that adaptively select/suppress transfer (LOL-GP) outperform constant-transfer (KO) and fully flexible but unregularized transfer (BKO) in presence of local mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>No explicit minimal-fidelity threshold is specified; conclusion is qualitative: low-fidelity data help only in regions where their outputs closely match high-fidelity outputs — otherwise transfer must be suppressed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>KO (constant transfer) and BKO (over-flexible transfer) suffered negative transfer in some experiments: KO injected biased transfer across entire domain, harming predictions in regions of mismatch; BKO sometimes overfit and induced high variance, also harming predictive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1323.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1323.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Forrester synthetic target/source</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Forrester 1D test function and perturbed source variants</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Canonical Forrester 1D function used as a high-fidelity target; source variants were constructed by adding piecewise and oscillatory perturbations to create regions of local similarity and dissimilarity for transfer experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Forrester function (and perturbed source variants)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Deterministic analytic 1D function f_T(x) = 0.2(6x − 2)^2 sin(12x − 4) + 0.5 used as target; source functions are f_S(x) = f_T(x) + localized additive terms to emulate local transfer scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>methodology / surrogate modeling (toy/benchmark numerical function)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High-fidelity target is the canonical Forrester function; source variants act as 'related systems' rather than explicit fidelity levels.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Synthetic discrepancies introduced that make source and target nearly identical in some intervals (good-transfer regions) and markedly different in others (bad-transfer regions).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LOL-GP, KO, BKO, GP, TLNet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Surrogate/transfer models fitted to simulated training data from the analytic functions; LOL-GP uses latent ReLU-regularized transfer weights to enable local transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Emulation (predictive surrogate) of the Forrester target function using limited target data and additional source evaluations; identify regions where source is helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Prediction of target Forrester values at held-out test inputs (i.e., transfer from source function to target predictions).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Numeric (from paper): In the multi-source Forrester experiment, LOL-GP achieved RMSE = 0.096 and CRPS = 0.047 (best); KO experienced negative transfer (worse than GP), BKO improved in some regions but still worse than LOL-GP overall.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Not applicable beyond noting that local similarity (not global fidelity) determines whether transfer helps.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>KO (constant transfer) produced negative transfer in regions where the source differed from the target (x < 0.5). BKO sometimes overfit (high variance) with limited target data, hurting performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1323.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1323.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Friedman 5D synthetic sources</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Friedman 5-dimensional test function with perturbed source variants</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>5D Friedman benchmark used as the high-fidelity target with two synthetic source functions that match the target in complementary subregions (creating local transfer structure).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Friedman 5D function (and two perturbed source variants)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Deterministic analytic 5D function (standard Friedman formulation) used as target; two source variants created by adding structured, localized multiplicative/additive perturbations on one input dimension to emulate local transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>methodology / surrogate modeling (toy/benchmark numerical function)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Single high-fidelity canonical target; sources are related but not fidelity-ordered—each source is informative in different subregions.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Perturbations in source functions vary with one coordinate (x3) to produce regions of good/bad transfer; no numerical discretization fidelity differences (analytic functions).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LOL-GP, KO, BKO, GP, TLNet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gaussian-process-based surrogates (LOL-GP with local ReLU regularization) and baselines; TLNet is a shallow composite neural network used as a point-prediction baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Emulate the 5D Friedman target from limited target data plus source data and identify which source to transfer from in which regions (local transfer).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>High-fidelity Friedman target values on test set (transfer from two source functions).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Numeric (from paper): In the 5D multi-source Friedman experiment, LOL-GP achieved RMSE = 0.272 and CRPS = 0.152 (best among compared); BKO experienced negative transfer (worse than GP).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Not applicable; emphasis on local similarity across input subspaces rather than global fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>BKO (highly flexible input-dependent transfer without ReLU regularization) experienced negative transfer due to overfitting/variance with limited target data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1323.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1323.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Particle / heavy-ion collision simulators (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>High-energy physics particle simulators / heavy-ion collision simulators (general reference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High-cost particle collision simulators (heavy-ion collision codes) referenced as examples of expensive scientific simulators where transfer learning and surrogate modeling are valuable; these typically require thousands of CPU hours per run.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Heavy-ion collision / particle physics simulators (general)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Domain-specific Monte Carlo and transport/hydrodynamic simulators used in high-energy physics to emulate particle collisions and quark-gluon plasma dynamics; described as extremely expensive per run (thousands of CPU hours).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>high-energy physics / computational particle physics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High-fidelity, physics-rich simulators (detailed microphysics and many model components); referenced as costly targets for emulation; lower-fidelity surrogate or related-system simulators may exist for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Include detailed physics of collisions and medium evolution; expensive because of fine discretization, multiphysics stages, event-by-event sampling; lower-fidelity approximations may omit stages or use coarser discretizations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Surrogate/emulation models in cited literature (e.g., transfer-learning-enabled GP surrogates like in Liyanage et al. [33] and other referenced works)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gaussian-process-based and other surrogate/emulator models used to approximate outputs of particle simulators; transfer learning approaches have been applied to accelerate emulation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Emulation of expensive particle-collision outputs and inference of physics parameters via surrogate-based calibration and uncertainty quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Emulation of high-cost particle simulators and transfer from related particle systems or lower-fidelity simulators to accelerate inference and prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Only briefly discussed as motivating examples; no detailed minimal-fidelity conclusions in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not detailed in this paper; heavy-ion simulation domain is mentioned as an application area where transfer can be beneficial but also where local mismatch can cause negative transfer if not modeled carefully.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Efficient emulation of relativistic heavy ion collisions with transfer learning <em>(Rating: 2)</em></li>
                <li>A graphical multi-fidelity Gaussian process model, with application to emulation of heavy-ion collisions <em>(Rating: 2)</em></li>
                <li>Conglomerate multi-fidelity Gaussian process modeling, with application to heavy-ion collisions <em>(Rating: 2)</em></li>
                <li>Bayesian analysis of multifidelity computer models with local features and nonnested experimental designs: application to the WRF model <em>(Rating: 2)</em></li>
                <li>Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling <em>(Rating: 2)</em></li>
                <li>A composite neural network that learns from multi-fidelity data: Application to function approximation and inverse PDE problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1323",
    "paper_id": "paper-273375004",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "Jet-turbine FEA (MATLAB module)",
            "name_full": "MATLAB finite-element-analysis (FEA) module for turbine blade stress",
            "brief_description": "A MATLAB-based FEA simulator (cited as [51] in the paper) used to compute steady-state thermal + structural stress fields of a turbine blade; used as the expensive high-fidelity simulator in the jet-turbine surrogate experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "MATLAB FEA module (turbine stress simulator)",
            "simulator_description": "Finite-element-analysis simulator coupling static-structural mechanics with steady-state thermal expansion to compute stress distributions on a turbine blade; solves governing PDEs via mesh discretization and returns scalar responses (maximum stress at blade root).",
            "scientific_domain": "mechanics / structural stress analysis (thermomechanics)",
            "fidelity_level": "High-fidelity (FEA with fine mesh); in experiments a high-fidelity target used mesh size 0.02 with lower-fidelity variants at coarser mesh sizes (0.04, 0.06).",
            "fidelity_characteristics": "Fidelity differences realized by mesh density: finer mesh (0.02) resolves more geometric/thermal gradients and stress concentrations; coarser meshes (0.04, 0.06) are computationally cheaper but smooth or miss local high gradients; physics (thermal expansion + static structural equations) are present at all fidelities but spatial resolution differs.",
            "model_or_agent_name": "LOL-GP (and benchmarks: GP, KO, BKO, TLNet)",
            "model_description": "Probabilistic surrogate (Local transfer Learning Gaussian Process - LOL-GP) trained on simulator evaluations, with comparative baselines including standard GP, Kennedy–O'Hagan (KO), Bayesian KO (BKO), and a neural-network TLNet.",
            "reasoning_task": "Surrogate modeling to predict maximum stress at blade root as a function of operating conditions (pressure, temperature) and to provide calibrated uncertainty for downstream decisions.",
            "training_performance": null,
            "transfer_target": "Transfer from source data (different blade material) or from lower-fidelity FEA meshes to the high-fidelity target FEA (i.e., emulate high-fidelity stress outputs).",
            "transfer_performance": "Qualitative: LOL-GP yielded the best RMSE and CRPS among compared methods and mitigated cases where other transfer methods produced negative transfer; exact numeric performance for the jet-turbine experiment is reported in the paper but is not unambiguously tabulated in the PDF text extract provided here.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Multi-fidelity experiments used nested meshes (0.06, 0.04, 0.02). Transfer from coarser meshes to the fine-mesh high-fidelity target was effective when local similarity existed; LOL-GP outperformed KO/BKO/TLNet by identifying regions where mesh-based low-fidelity data were informative and suppressing transfer where they were not.",
            "minimal_fidelity_discussion": "No explicit minimal-fidelity threshold is given; authors emphasize that local similarity between fidelities (i.e., where low- and high-fidelity outputs are close) is necessary for beneficial transfer, and that coarse meshes may be sufficient in regions where physics are well-resolved but not where local gradients dominate.",
            "failure_cases": "Other transfer approaches (KO, BKO, TLNet) sometimes exhibited negative transfer (worse RMSE/CRPS than no-transfer GP) when lower-fidelity meshes misrepresented local stress behavior; the paper reports that TLNet in particular experienced negative transfer in some settings and that KO/BKO can fail when transfer is applied in regions where physics differ.",
            "uuid": "e1323.0",
            "source_info": {
                "paper_title": "Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Multi-fidelity mesh simulators (Forrester/Branin experiments)",
            "name_full": "Synthetic multi-fidelity simulators constructed from prototypical test functions (Forrester, Branin) with low/high-fidelity variants",
            "brief_description": "Synthetic low-, medium-, and high-fidelity simulators created by perturbing canonical test functions (Forrester 1D, Branin 2D) to produce locally differing behavior; used to benchmark multi-fidelity transfer performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "Synthetic multi-fidelity Forrester / Branin variants (f1, f2, f3)",
            "simulator_description": "Artificial simulator functions: the high-fidelity function is the canonical test function (Forrester or Branin); lower-fidelity versions are formed by adding localized perturbation/discrepancy terms so that source and target are similar in some regions and different in others (creating 'local transfer' structure).",
            "scientific_domain": "methodology / surrogate modeling (toy numerical experiments) — not real physics but intended to mimic properties of physical simulators",
            "fidelity_level": "Multiple fidelity levels synthetically defined: high-fidelity = canonical function; low/medium fidelity = canonical function plus localized additive terms (thus lower fidelity in regions with added discrepancies).",
            "fidelity_characteristics": "Differences are realized by additive localized discrepancy functions (piecewise functions, oscillatory terms) that make low-fidelity outputs deviate from high-fidelity in particular regions; all other aspects are exact (deterministic functions with no numerical noise).",
            "model_or_agent_name": "LOL-GP (and benchmarks GP, KO, BKO, TLNet)",
            "model_description": "Surrogate models (Gaussian-process-based LOL-GP with latent ReLU-regularized transfer weights; baselines include standard GP, KO, Bayesian KO, and a neural-network TLNet).",
            "reasoning_task": "Predict high-fidelity outputs (emulation) across the domain given limited high-fidelity data and abundant lower-fidelity / source data; identify where transfer is beneficial (local transfer).",
            "training_performance": null,
            "transfer_target": "High-fidelity canonical function (held-out test points) — i.e., transfer from lower-fidelity variants to emulate/predict high-fidelity outputs.",
            "transfer_performance": "Numeric results (test performance after training reported in paper): For multi-fidelity Forrester (1D) LOL-GP RMSE 0.127, CRPS 0.062 (best among compared); for multi-fidelity Branin (2D) LOL-GP RMSE 0.064, CRPS 0.034 (best among compared). Other methods sometimes showed worse RMSE/CRPS or negative transfer.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Comparisons show that naively transferring information from low-fidelity variants can harm performance when discrepancies are localized; methods that adaptively select/suppress transfer (LOL-GP) outperform constant-transfer (KO) and fully flexible but unregularized transfer (BKO) in presence of local mismatch.",
            "minimal_fidelity_discussion": "No explicit minimal-fidelity threshold is specified; conclusion is qualitative: low-fidelity data help only in regions where their outputs closely match high-fidelity outputs — otherwise transfer must be suppressed.",
            "failure_cases": "KO (constant transfer) and BKO (over-flexible transfer) suffered negative transfer in some experiments: KO injected biased transfer across entire domain, harming predictions in regions of mismatch; BKO sometimes overfit and induced high variance, also harming predictive performance.",
            "uuid": "e1323.1",
            "source_info": {
                "paper_title": "Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Forrester synthetic target/source",
            "name_full": "Forrester 1D test function and perturbed source variants",
            "brief_description": "Canonical Forrester 1D function used as a high-fidelity target; source variants were constructed by adding piecewise and oscillatory perturbations to create regions of local similarity and dissimilarity for transfer experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "Forrester function (and perturbed source variants)",
            "simulator_description": "Deterministic analytic 1D function f_T(x) = 0.2(6x − 2)^2 sin(12x − 4) + 0.5 used as target; source functions are f_S(x) = f_T(x) + localized additive terms to emulate local transfer scenarios.",
            "scientific_domain": "methodology / surrogate modeling (toy/benchmark numerical function)",
            "fidelity_level": "High-fidelity target is the canonical Forrester function; source variants act as 'related systems' rather than explicit fidelity levels.",
            "fidelity_characteristics": "Synthetic discrepancies introduced that make source and target nearly identical in some intervals (good-transfer regions) and markedly different in others (bad-transfer regions).",
            "model_or_agent_name": "LOL-GP, KO, BKO, GP, TLNet",
            "model_description": "Surrogate/transfer models fitted to simulated training data from the analytic functions; LOL-GP uses latent ReLU-regularized transfer weights to enable local transfer.",
            "reasoning_task": "Emulation (predictive surrogate) of the Forrester target function using limited target data and additional source evaluations; identify regions where source is helpful.",
            "training_performance": null,
            "transfer_target": "Prediction of target Forrester values at held-out test inputs (i.e., transfer from source function to target predictions).",
            "transfer_performance": "Numeric (from paper): In the multi-source Forrester experiment, LOL-GP achieved RMSE = 0.096 and CRPS = 0.047 (best); KO experienced negative transfer (worse than GP), BKO improved in some regions but still worse than LOL-GP overall.",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Not applicable beyond noting that local similarity (not global fidelity) determines whether transfer helps.",
            "failure_cases": "KO (constant transfer) produced negative transfer in regions where the source differed from the target (x &lt; 0.5). BKO sometimes overfit (high variance) with limited target data, hurting performance.",
            "uuid": "e1323.2",
            "source_info": {
                "paper_title": "Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Friedman 5D synthetic sources",
            "name_full": "Friedman 5-dimensional test function with perturbed source variants",
            "brief_description": "5D Friedman benchmark used as the high-fidelity target with two synthetic source functions that match the target in complementary subregions (creating local transfer structure).",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "Friedman 5D function (and two perturbed source variants)",
            "simulator_description": "Deterministic analytic 5D function (standard Friedman formulation) used as target; two source variants created by adding structured, localized multiplicative/additive perturbations on one input dimension to emulate local transfer.",
            "scientific_domain": "methodology / surrogate modeling (toy/benchmark numerical function)",
            "fidelity_level": "Single high-fidelity canonical target; sources are related but not fidelity-ordered—each source is informative in different subregions.",
            "fidelity_characteristics": "Perturbations in source functions vary with one coordinate (x3) to produce regions of good/bad transfer; no numerical discretization fidelity differences (analytic functions).",
            "model_or_agent_name": "LOL-GP, KO, BKO, GP, TLNet",
            "model_description": "Gaussian-process-based surrogates (LOL-GP with local ReLU regularization) and baselines; TLNet is a shallow composite neural network used as a point-prediction baseline.",
            "reasoning_task": "Emulate the 5D Friedman target from limited target data plus source data and identify which source to transfer from in which regions (local transfer).",
            "training_performance": null,
            "transfer_target": "High-fidelity Friedman target values on test set (transfer from two source functions).",
            "transfer_performance": "Numeric (from paper): In the 5D multi-source Friedman experiment, LOL-GP achieved RMSE = 0.272 and CRPS = 0.152 (best among compared); BKO experienced negative transfer (worse than GP).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Not applicable; emphasis on local similarity across input subspaces rather than global fidelity.",
            "failure_cases": "BKO (highly flexible input-dependent transfer without ReLU regularization) experienced negative transfer due to overfitting/variance with limited target data.",
            "uuid": "e1323.3",
            "source_info": {
                "paper_title": "Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Particle / heavy-ion collision simulators (mentioned)",
            "name_full": "High-energy physics particle simulators / heavy-ion collision simulators (general reference)",
            "brief_description": "High-cost particle collision simulators (heavy-ion collision codes) referenced as examples of expensive scientific simulators where transfer learning and surrogate modeling are valuable; these typically require thousands of CPU hours per run.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "Heavy-ion collision / particle physics simulators (general)",
            "simulator_description": "Domain-specific Monte Carlo and transport/hydrodynamic simulators used in high-energy physics to emulate particle collisions and quark-gluon plasma dynamics; described as extremely expensive per run (thousands of CPU hours).",
            "scientific_domain": "high-energy physics / computational particle physics",
            "fidelity_level": "High-fidelity, physics-rich simulators (detailed microphysics and many model components); referenced as costly targets for emulation; lower-fidelity surrogate or related-system simulators may exist for transfer.",
            "fidelity_characteristics": "Include detailed physics of collisions and medium evolution; expensive because of fine discretization, multiphysics stages, event-by-event sampling; lower-fidelity approximations may omit stages or use coarser discretizations.",
            "model_or_agent_name": "Surrogate/emulation models in cited literature (e.g., transfer-learning-enabled GP surrogates like in Liyanage et al. [33] and other referenced works)",
            "model_description": "Gaussian-process-based and other surrogate/emulator models used to approximate outputs of particle simulators; transfer learning approaches have been applied to accelerate emulation.",
            "reasoning_task": "Emulation of expensive particle-collision outputs and inference of physics parameters via surrogate-based calibration and uncertainty quantification.",
            "training_performance": null,
            "transfer_target": "Emulation of high-cost particle simulators and transfer from related particle systems or lower-fidelity simulators to accelerate inference and prediction.",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Only briefly discussed as motivating examples; no detailed minimal-fidelity conclusions in this paper.",
            "failure_cases": "Not detailed in this paper; heavy-ion simulation domain is mentioned as an application area where transfer can be beneficial but also where local mismatch can cause negative transfer if not modeled carefully.",
            "uuid": "e1323.4",
            "source_info": {
                "paper_title": "Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Efficient emulation of relativistic heavy ion collisions with transfer learning",
            "rating": 2,
            "sanitized_title": "efficient_emulation_of_relativistic_heavy_ion_collisions_with_transfer_learning"
        },
        {
            "paper_title": "A graphical multi-fidelity Gaussian process model, with application to emulation of heavy-ion collisions",
            "rating": 2,
            "sanitized_title": "a_graphical_multifidelity_gaussian_process_model_with_application_to_emulation_of_heavyion_collisions"
        },
        {
            "paper_title": "Conglomerate multi-fidelity Gaussian process modeling, with application to heavy-ion collisions",
            "rating": 2,
            "sanitized_title": "conglomerate_multifidelity_gaussian_process_modeling_with_application_to_heavyion_collisions"
        },
        {
            "paper_title": "Bayesian analysis of multifidelity computer models with local features and nonnested experimental designs: application to the WRF model",
            "rating": 2,
            "sanitized_title": "bayesian_analysis_of_multifidelity_computer_models_with_local_features_and_nonnested_experimental_designs_application_to_the_wrf_model"
        },
        {
            "paper_title": "Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling",
            "rating": 2,
            "sanitized_title": "nonlinear_information_fusion_algorithms_for_dataefficient_multifidelity_modelling"
        },
        {
            "paper_title": "A composite neural network that learns from multi-fidelity data: Application to function approximation and inverse PDE problems",
            "rating": 1,
            "sanitized_title": "a_composite_neural_network_that_learns_from_multifidelity_data_application_to_function_approximation_and_inverse_pde_problems"
        }
    ],
    "cost": 0.01827725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators
17 Oct 2024</p>
<p>Xinming Wang 
Department of Industrial Engineering and Management
Peking University</p>
<p>Simon Mak 
Department of Statistical Science
Duke University</p>
<p>John Miller 
Department of Statistical Science
Duke University</p>
<p>Jianguo Wu j.wu@pku.edu.cn 
Department of Industrial Engineering and Management
Peking University</p>
<p>Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators
17 Oct 202422AF1D01006E2EFD83664EB606A7F44FarXiv:2410.12690v2[stat.ML]Surrogate modelingGaussian processestransfer learningregularization AMS subject classifications. 68Q2568R1068U05
A critical bottleneck for scientific progress is the costly nature of computer simulations for complex systems.Surrogate models provide an appealing solution: such models are trained on simulator evaluations, then used to emulate and quantify uncertainty on the expensive simulator at unexplored inputs.In many applications, one often has available data on related systems.For example, in designing a new jet turbine, there may be existing studies on turbines with similar configurations.A key question is how information from such "source" systems can be transferred for effective surrogate training on the "target" system of interest.We thus propose a new LOcal transfer Learning Gaussian Process (LOL-GP) model, which leverages a carefully-designed Gaussian process to transfer such information for surrogate modeling.The key novelty of the LOL-GP is a latent regularization model, which identifies regions where transfer should be performed and regions where it should be avoided.This "local transfer" property is desirable in scientific systems: at certain parameters, such systems may behave similarly and thus transfer is beneficial; at other parameters, they may behave differently and thus transfer is detrimental.By accounting for local transfer, the LOL-GP can rectify a critical limitation of "negative transfer" in existing transfer learning models, where the transfer of information worsens predictive performance.We derive a Gibbs sampling algorithm for efficient posterior predictive sampling on the LOL-GP, for both the multi-source and multi-fidelity transfer settings.We then show, via a suite of numerical experiments and an application for jet turbine design, the improved surrogate performance of the LOL-GP over existing methods.</p>
<ol>
<li>Introduction.With fundamental advances in scientific modeling and computing, computer simulations are becoming an increasingly valuable tool for investigating complex physical phenomena.Such virtual simulations have been successfully employed in important scientific and engineering applications, including rocket design [35], climate science [56] and high-energy physics [31].One critical bottleneck, however, is that these simulations are typically computationally costly.For complex phenomena such as rocket propulsion, the evaluation of the simulator f (x) at an input x can easily require millions of CPU hours [58]!An effective solution in the literature is surrogate modeling [20]: one first performs simulations on a carefully selected set of design points, then uses simulation results as training data to fit a predictive model that efficiently "emulates" and quantifies uncertainty on the expensive simulator.In what follows, we focus on probabilistic surrogates that provide uncertainty quantification (UQ) on its predictions, as such probabilistic UQ is essential for many facets of scientific decision-making [20].</li>
</ol>
<p>A popular probabilistic surrogate model is the Gaussian process (GP; [55]), a flexible Bayesian nonparametric model widely used in statistics and machine learning.The primary appeal of GPs as surrogates is that, conditioned on simulated training data, its posterior predictive distribution can be evaluated as a closed-form expression.Such an expression not only permits efficient prediction and UQ on the expensive simulator f (•), but also facilitates an effective use of the trained surrogate for downstream objectives, including calibration [7,44], optimization [38] and sequential design [48,8].In recent years, there has been much work improving the expressiveness of GPs for broad applications, including deep GPs [48], neural network GPs [30] and shrinkage GPs [50].</p>
<p>Despite this body of work, an inherent bottleneck for surrogate modeling is the expensive (and thus limited) nature of simulation training data.This is exacerbated by the reality that, in scientific applications, systems may have high-dimensional input spaces and thus may require large training sample sizes for accurate surrogate modeling.A recent development in machine learning, called transfer learning [60], offers a promising solution.The key idea is to "transfer" knowledge from source systems with ample amounts of data, to target systems with limited data.For surrogate modeling of expensive simulators, the target system is the simulator of interest, and the source systems may be simulators from related prior studies.Despite successful applications of transfer learning in machine learning (e.g., its use in text classification [12] and spam filtering [6]), there is scant literature on its broad use for surrogate modeling.Recent work has explored transfer learning within neural network surrogates for thermodynamics [59] and reliability [10] applications.Such models, however, yield only point predictions and do not provide any quantification of uncertainties.Liyanage et al. [33] proposed an extension of the Kennedy-O'Hagan (KO) model [25] for transfer learning of GP surrogates, which we explore in detail later.</p>
<p>A critical challenge for transfer learning is the potential of negative transfer [60], where the transfer of information can considerably worsen performance on the target system.Such negative transfer can be disastrous for surrogate training, resulting in highly inaccurate surrogates with unreliable UQ.To mitigate this, recent machine learning works have explored various selection criteria for choosing which sources to include for transfer learning [57,53,54,32].For physical science applications, however, the key consideration is typically not which sources are selected, but rather how transfer is modeled.Consider, for example, our motivating application on the stress analysis of a jet engine turbine blade (Section 6).It is known [46] that different blade materials induce similar stress profiles at certain operating conditions, but considerably different profiles at other conditions.Transfer learning can thus be beneficial in the first case, but detrimental in the second.This highlights a so-called "local transfer" property, where systems may be related at certain parametrizations due to similar dominant physics, but considerably different at other parametrizations when such physics are less pronounced; see, e.g., an observation of this in high-energy physics [33].For such problems, the incorporation of local transfer can potentially mitigate negative transfer by learning local regions for transfer; existing methods, however, do not account for this.</p>
<p>We thus propose a new LOcal transfer Learning GP (LOL-GP), which accounts for local transfer via a flexible Bayesian non-parametric modeling framework.The key novelty of the LOL-GP is a latent regularization model, which identifies from data regions where transfer should be performed and regions where it should be avoided.By modeling this local transfer property, the LOL-GP can trade off between overly simple (i.e., highly biased) and overly complex (i.e., overfitted) transfer models, thus facilitating robust and reliable transfer learning for emulation.We derive an efficient Gibbs sampler for posterior predictive sampling from the LOL-GP, for both the multi-source and multi-fidelity transfer settings.We then show, in a suite of numerical experiments and an application to jet turbine design, the improved surrogate performance of the LOL-GP over existing methods, particularly in mitigating the risks of negative transfer.</p>
<p>It is worth noting that, while the general use of transfer learning for surrogates is rather limited, a specific instance has been explored under the topic of "multi-fidelity learning".Using transfer learning terminology, multi-fidelity learning aims to transfer knowledge from cheaper low-fidelity simulators (i.e., source systems) for emulating the costly high-fidelity simulator (i.e., target system).Beginning with the seminal KO model [25], there has been considerable work on multi-fidelity surrogates for scientific applications, including [45,52,43,24,23].However, as we show later, such models may similarly suffer from negative transfer, in that the use of low-fidelity data may worsen predictions on the high-fidelity simulator.A saving grace is that local transfer is also expected for many multi-fidelity systems.For example, in our earlier jet turbine application, the stress profiles simulated at low and high fidelity may be similar at certain operating conditions, but considerably different at others.We show later that the LOL-GP can similarly exploit such local structure for robust multi-fidelity learning, in a suite of numerical experiments and for our jet turbine application.</p>
<p>The paper is organized as follows.Section 2 provides background on GP modeling and outlines a motivating example demonstrating limitations of existing models.Section 3 presents the proposed LOL-GP model and its adaptation for multi-fidelity learning.Section 4 details the workflow for model training and prediction.Sections 5 and 6 compare the effectiveness of the proposed model with the state-of-the-art in a suite of numerical experiments and for our jet turbine application.Section 7 concludes the paper.</p>
<p>Background and Motivation.</p>
<p>We first provide a brief overview of GPs and related work on transfer learning surrogates, then present a motivating example outlining the risk of negative transfer for existing surrogate models.</p>
<p>2.1.Gaussian process modeling.Let x be the simulation inputs on the input space X = [0, 1] d (i.e., all inputs are normalized to be on a unit hypercube; see [20]), and let f (x) ∈ R be the simulated output at inputs x.A (zero-mean) GP places the following prior stochastic process on the black-box function f (•):
f (•) ∼ GP{0, k(•, •)}. (2.1)
Here, k(•, •) : X × X → R is its covariance function, which dictates the smoothness of sample paths from the GP.Popular choices for k include the squared-exponential and Matérn-ν kernels, which induce infinitely-differentiable and (⌊ν⌋ − 1)-differentiable sample paths from the GP, respectively (see [47]).</p>
<p>Suppose we then run the expensive simulator at design points
X = {x 1 , • • • , x n }, yielding outputs f = (f (x 1 ), • • • , f (x n )).
In what follows, we presume the simulator is deterministic, in that the same output f (x) is observed every time the simulator is run at inputs x; this can be extended in a straight-forward manner for simulators with Gaussian noise (see [2]).</p>
<p>Suppose we wish to predict f at a new input x new .Conditioning on training data f , the posterior predictive distribution of f (x new ) can be shown to be Gaussian, i.e.:
f (x new )|f ∼ N {µ(x new ), σ 2 (x new )} (2.2)
where its mean and variance are given by the closed-form expressions:
µ(x new ) = k T (X , x new )[K(X , X )] −1 f , σ 2 (x new ) = k(x new , x new ) − k T (X , x new )[K(X , X )] −1 k(X , x new ). (2.3) Here, K(X , X ) = [k(x, x ′ )] x,x ′
∈X is the covariance matrix at the simulated inputs in X , and k(X , x new ) = [k(x, x new )] x∈X is the covariance vector between the simulated inputs X and the new point x new .A key advantage of GPs is the closed-form nature of the predictive equations (2.2) and (2.3); this facilitates an effective use of the trained surrogate for downstream objectives, including design optimization [9], inverse problems [14] and control [39].</p>
<p>2.2.</p>
<p>Existing transfer learning surrogates.Next, we outline existing work on probabilistic surrogates that leverage transfer learning.As mentioned earlier, much of this work focuses specifically on multi-fidelity surrogates rather than the general problem of transfer learning.An early seminal work on multi-fidelity surrogates is the Kennedy-O'Hagan model [25], which aims to predict a high-fidelity simulator f T (•) by borrowing information from a sequence of L ≥ 1 lower fidelity simulators f L (•), • • • , f 1 (•), ranked by decreasing fidelities.To do this, the KO model presumes:
f l+1 (x) = ρf l (x) + δ l (x), l = 1, • • • , L,(2.4)
where f L+1 (•) := f T (•), and f 1 (•) and {δ l (•)} L l=1 follow independent GP priors.Here, ρ can be viewed as a "transfer" parameter that facilitates information transfer over fidelity levels, and δ l (x) models the "discrepancy" between fidelity levels l and l +1.There has been considerable extensions of the KO model, including with mesh densities [52,24], graphical models [23] and for Bayesian optimization [26].A recent work [33] adopted the KO model for transfer learning of surrogates in high-energy physics; here, transfer is from a related source system to the target system, where both systems are not related via fidelity.Letting f S (•) and f T (•) be the source and target simulators, respectively, the model in [33] takes the form:
(2.5) f T (x) = ρf S (x) + δ(x),
where f S (•) and δ(•) follow independent GP priors.Such a model was shown to be effective for transfer learning between different particle collision systems for surrogate modeling [33].</p>
<p>A potential limitation of the KO model (2.4) (and similarly of the transfer learning model (2.5)) is that the modeled transfer is too simplistic.One can view this as an overly "biased" model for transfer learning; such bias may inhibit transfer and thus induce negative transfer.For the multi-fidelity setting, [45] proposed the following model extending (2.4):
f l+1 (x) = ρ l (x)f l (x) + δ l (x), l = 1, • • • , L. (2.6)
Here, {ρ l (x)} l follow independent GPs, which facilitates a more flexible input-dependent transfer of information from low to high fidelity.Under this so-called Bayesian KO (BKO) model, [45] introduced an efficient Gibbs sampler for sampling the posterior predictive distribution of the high-fidelity code f T (•) = f L+1 (•).A similar extension can be used on (2.5) for transfer learning; we will investigate this model later.One can view such an extension as a way to reduce the aforementioned "bias" on the transfer learning model.However, by increasing model flexibility, the "variance" of the transfer model naturally increases.If this is too high, the transfer model may then be overfit and thus induce negative transfer; we shall see this later.</p>
<p>Finally, beyond GPs, there is recent literature on transfer learning using neural network surrogates.This includes the works of [10,49,59], which employs a "pre-training" of the network using source data, then a fine-tuning of such a model using target data.As mentioned in the Introduction, existing surrogates of this flavor are largely deterministic in nature, and do not provide the probabilistic UQ required in our applications.Furthermore, from a point prediction perspective, such transfer models suffer from similar issues of high variance and thus may experience negative transfer.We will show this later in numerical experiments.</p>
<p>A motivating example.</p>
<p>To explore these issues of bias and/or variance for existing methods, consider a motivating example using the one-dimensional Forrester function [15].This function, which we treat as the target, is given by:
(2.7) f T (x) = 0.2(6x − 2) 2 sin(12x − 4) + 0.5.
To mimic local transfer, where systems may be highly related at certain parameters but considerably different at others, we adopt the source function as:</p>
<p>(2.8) f S (x) = f T (x) + 0.6 I {x≥0.5}(x − 0.5) + 1.6 I {x&lt;0.5}(x − 0.5) sin(30x) √ 5x − 4.</p>
<p>Figure 1 visualizes the source and target functions.When x ≥ 0.5, we see that the source function is nearly identical to the target; when x &lt; 0.5, the source function becomes considerably different from the target.This thus captures the desired local transfer property expected in physical systems.To mitigate negative transfer, a good model should identify the first region (where source and target are similar) as appropriate for transfer, and the latter region (where source and target are different) as detrimental for transfer.We collect training data on n S = 24 equidistant design points for the source function, and data on n T = 8 equidistant design points on the target (see Figure 1).Figure 2 shows the fitted benchmark surrogates: the standard GP, the KO [25] and BKO models [45]; the first model is trained using just target data, whereas the latter are trained using both source and target data.We see the standard GP yields poor predictions with high uncertainty, which is unsurprising due to its small sample size of n T = 8; some transfer would thus be beneficial here.The KO model (2.5), which adopts constant transfer over the full domain, yields noticeably improved predictions over the region [0.5, 1.0], where transfer is desirable.However, over [0.0, 0.5], where the source and target are considerably different, the modeled transfer appears to hurt performance: the fitted surrogate yields poorer predictions with overly certain UQ compared to the standard GP.This highlights how the inflexible 0.0 0.2 0.4 0.6 0.8 1.0  "biased" KO model may induce negative transfer by failing to restrict transfer over [0.0, 0.5].</p>
<p>The BKO model instead adopts a highly flexible input-dependent transfer, where ρ(x) follows a GP.From Figure 2, the BKO yields slight improvements over the KO model, but still worse performance compared to the standard GP over [0.0, 0.5].This suggests that an overly flexible model (particularly with limited target data) may also induce negative transfer via an inflated predictive variance.For point predictions, we further compare with a transfer learning neural network benchmark called TLNet (details on its architecture discussed later in Section 5).</p>
<p>From Figure 2, TLNet appears to suffer from similar issues as the BKO: its point predictions are rather erratic throughout, due to potential overfitting (and hence high variance) with limited data.This general dichotomy of "bias" versus "variance" has of course been extensively explored in statistical learning [21].Simpler models that are misspecified may predict poorly due to its bias, whereas highly complex models that are overparametrized may also predict poorly due to its high variance.One way to strike a good trade-off is via careful regularization: the use of data to guide simplifications of the fitted model.Thus, to tackle limitations of existing transfer learning surrogates, we introduce next the LOL-GP, which leverages a local transfer regularization model to trade off between bias and variance for robust transfer learning.</p>
<ol>
<li>The LOL-GP Model.We first present the LOL-GP for the general multi-source transfer learning set-up, where there are L source systems and a single target system.We then outline a natural extension of the LOL-GP for the specific setting of multi-fidelity surrogates.</li>
</ol>
<p>3.1.Multi-source transfer.Consider first the general transfer learning set-up, where we have simulated data on L ≥ 1 related source systems and wish to predict the target simulator.For the l-th source system, suppose its simulator (denoted as f l (•)) is run at inputs
X l = {x l,1 , • • • , x l,n l } ⊆ R d , yielding outputs f l ∈ R n l . Further suppose the target simulator (denoted as f T (•)) is run at inputs X T = {x 1 , • • • , x n T } ⊆ R d , yielding outputs f T ∈ R n T .
The LOL-GP then adopts the following transfer learning model:
f T (x) = L l=1 ρ l (x)f l (x) + δ(x), f l (x) ∼ GP{0, k l (•, •)}, δ(x) ∼ GP{0, k δ (•, •)}, ρ l (x) = ReLU{ω l (x)}, ω l (x) ∼ GP{0, k ω l (•, •)}, l = 1, • • • , L.(3.1)
Here, all GP priors are specified independently.We investigate below each line of the above specification.The first line models the target simulator as a weighted sum of the source simulators, where the transfer weights ρ l (x) may depend on input parameters x.Here, δ(x) accounts for potential discrepancies between the target system and its weighted predictor using the L source systems.The second line in (3.1) places independent GP priors on the unknown response surfaces for source systems as well as the discrepancy term.The key novelty is the third line of (3.1), where the transfer function is fed through the rectified linear unit (ReLU) activation function ReLU(•) = max(0, •), which is widely used in neural network models [1].</p>
<p>Here, the ReLU activation provides an appealing framework for regularizing the desired local transfer behavior.When the latent function ω l (x) exceeds zero, transfer is permitted by setting the transfer weight as ρ l (x) = ω l (x) &gt; 0; when ω l (x) falls below zero, transfer is restricted by zeroing out its transfer weight.Further insight can be gleaned on the LOL-GP through the lens of the bias-variance tradeoff.Under the simplified setting ρ l (x) = ρ l , one recovers an analogous transfer framework to the KO model [25], which presumes constant (i.e., input-independent) transfer between source and target, and thus may be highly biased due to its overly simplified model.Conversely, without ReLU activation, one obtains a similar transfer framework to the BKO model [45], where the transfer function ρ l (x) follows a highly flexible GP prior without any regularization.In the presence of local transfer, where transfer may be detrimental in certain parts of the parameter space, such a model may have high variance due to overfitting.The LOL-GP (3.1) offers a compromise between these two extremes in the bias-variance trade-off, via the use of ReLU activations for regularizing the transfer model.Such regularization is driven by data: with independent GP priors on the latent functions {ω l (•)} L l=1 , the posterior distribution of such functions can then identify (with uncertainty) regions where transfer may or may not be beneficial.This can in turn mitigate the risk of negative transfer at either end of the bias-variance spectrum, and facilitate robust transfer learning for surrogate modeling.The next step is to investigate the posterior predictive distribution of f T (x new ), conditional on data from source and target systems, namely, f 1 , • • • , f L and f T , respectively.We will denote this as
[f T (x new )|f 1 , • • • , f L , f T ]
, where [X] denotes the distribution of a random variable X.Unfortunately, unlike the standard GP equations (2.2), this desired posterior distribution cannot be evaluated in closed form.The LOL-GP does, however, admit nice structure for efficient Gibbs sampling [18], a popular Markov-Chain Monte Carlo (MCMC) technique that cyclically updates model parameters via full conditional sampling.We show next that, with a careful selection of latent parameters Θ, such full conditional distributions can be derived in closed form, thus enabling efficient posterior sampling of f T (x new ).</p>
<p>Assume for now that the GP model parameters (i.e., its variance and length-scale parameters) are fixed; we discuss the estimation of such parameters later in Section 4.1.Define the set of latent parameters
Θ as Θ = {ω 1 (X T ), • • • , ω L (X T ), f 1 (X T ), • • • , f L (X T )}, where f l (X ) = [f l (x)]
x∈X is the vector of outputs for the l-th source system at design points X .For notational brevity, we presume that no design points are shared between source and target systems in the following derivation; Appendix SM1 provides analogous derivations for the setting with potentially shared design points.</p>
<p>Given data = {f 1 , • • • , f L , f T }, we can derive the following full conditional distributions1 on the latent weights in Θ:
[ω l (x i )|Θ − , data] ∼ π i,l N R + (µ i,l,+ , σ 2 i,l,+ ) + (1 − π i,l ) N R − (µ i,l,− , σ 2 i,l,− ), i = 1, • • • , n T , l = 1, • • • , L,(3.2)
Following notation from spike-and-slab priors [22], πΠ 1 + (1 − π)Π 2 denotes a two-mixture distribution with weight π on distribution Π 1 and weight 1 − π on distribution Π 2 .Here, N R + (µ, σ 2 ) denotes the normal distribution N (µ, σ 2 ) truncated on the non-negative reals R + , and N R − (µ, σ 2 ) denotes the same distribution truncated on the non-positive reals R − .The full conditional weights {π i,l } i,l in (3.2) can then be evaluated as:
(3.3) π i,l = Φ −µ i,l,− σ i,l,− Φ −µ i,l,− σ i,l,− + Φ µ i,l,+ σ i,l,+ ,
where the means {µ i,l,+ , µ i,l,− } i,l and variances {σ 2 i,l,+ , σ 2 i,l,− } i,l can be computed as:
µ i,l,− = k T ω l (X [−i] T , x i )K ω l (X [−i] T , X [−i] T ) −1 ω [−i],l , σ 2 i,l,− = k ω l (x i , x i ) − k T ω l (X [−i] T , x i )K ω l (X [−i] T , X [−i] T ) −1 k ω l (X [−i] T , x i ), µ i,l,+ = σ 2 i,l,+   µ i,l,− σ 2 i,l,− + e i,l − k T δ (X [−i] T , x i )K δ (X [−i] T , X [−i] T ) −1 f [−i],T − l ρ [−i] l ⊙ f [−i] l k δ (x i , x i ) − k T δ (X [−i] T , x i )K δ (X [−i] T , X [−i] T ) −1 k δ (X [−i] T , x i ) /f l (x i )   , σ 2 i,l,+ = 1 σ 2 i,l,− + [f l (x i )] 2 k δ (x i , x i ) − k T δ (X [−i] T , x i )K δ (X [−i] T , X [−i] T ) −1 k δ (X [−i] T , x i ) −1 . (3.4)
Here, X</p>
<p>[−i] T = X T \ x i denotes the set of target design points without the i-th point, and ω
[−i] l , ρ [−i] l and f [−i] l
are the vectors for w l (•), ρ l (•) and f l (•), respectively, at points X
[−i]
T .Finally, the scalar e i,l is defined as e
i,l = f T (x i ) − l ′ ̸ =l ρ l ′ (x i )f l ′ (x i ).
Similarly, we can derive the following full conditional distributions on the latent function values in Θ:
(3.5) [f l (x i )|Θ − , data] ∼ N {µ i,l , σ 2 i,l }, i = 1, • • • , n T , l = 1, • • • , L.
The posterior means {µ i,l } i,l and variances {σ 2 i,l } i,l take the closed form:
(3.6) µ i,l = k T l (X l∪T , x i )K −1 l∪T f l∪T , σ 2 i,l = k l (x i , x i ) − k T l (X l∪T , x i )K −1 l∪T k l (X l∪T , x i ), where X l∪T = X l ∪ X T , f l∪T = [f l , f T ], k l (x i , X l∪T ) = [k l (x i , X l ), ρ l (x i )k l (x i , X T )]
, and:
K l∪T = K l (X l , X l ) 1 n l ρ T l (X T ) ⊙ K l (X l , X T ) ρ l (X T )1 T n l ⊙ K l (X T , X l ) L l ′ =1 K l ′ (X T , X T ) + K δ (X T , X T
) is the covariance matrix for f l∪T .Here, ⊙ denotes the Hadamard (entry-wise) product.</p>
<p>While the above looks quite involved, the key appeal is that the closed-form full conditional distributions (3.2) and (3.5) can be sampled quickly, thus permitting efficient Gibbs sampling of the posterior distribution [Θ|data].Such a sampler proceeds as follows.First, the latent parameters in Θ are initialized in the sample chain.Next, one sequentially samples (and subsequently updates) each parameter θ ∈ Θ from the full conditional distributions (3.2) and (3.5).Each full conditional step can be sampled quickly here, as it involves sampling either a normal or a two-mixture normal distribution; further analysis of computational complexity is provided in Section 4.2.These steps are then repeated for B ≫ 1 iterations until the sample chain on Θ converges.One can show [17] that the stationary distribution of this MCMC sample chain is indeed the posterior distribution [Θ|data].Algorithm 3.1 summarizes these steps for our Gibbs sampler.</p>
<p>Finally, with the above MCMC samples (denoted
{Θ [b] } B b=1
) in hand, we can then sample from the desired posterior predictive distribution [f T (x new )|data].Note that:
(3.7) [f T (x new )|data] = [f T (x new )|Θ, data][Θ|data]dΘ =: A B dΘ.
Here, the distribution A can be sampled via Monte Carlo as follows.First (i), draw a sample from [{f l (x new )} L l=1 |Θ, data], where each entry [f l (x new )|Θ, data] follows independent normal distributions from the standard GP equations (2.2), conditional on f l (X l∪T ) and using kernel k l .Next (ii), draw a sample from [{ω l (x new )} L l=1 |Θ, data], where [ω l (x new )|Θ, data] follows independent normal distributions from the standard GP equations (2.2), conditional on ω l (X T ) and using kernel k ω l .Finally (iii), with
Ψ := ({f l (x new )} L l=1 , {ω l (x new )} L l=1 ) in hand, sample f T (x new ) from: [f T (x new )|Ψ, Θ, data] ∼ N L l=1 ρ l (x new )f l (x new ) + k T δ (X T , x new )K δ (X T , X T ) −1 f T (X T ) − L l=1 ρ l (X T ) ⊙ f l (X T ) , k δ (x new , x new ) − k T δ (X T , x new )K δ (X T , X T ) −1 k δ (X T , x new ) ,(3.8)
Algorithm 3.1 Multi-source LOL-GP: Gibbs sampling and posterior predictive sampling
Require: Source data {(X l , f l )} L l=1 , target data (X T , f T ), number of MCMC iterations B, initial MCMC parameters Θ [0] . Gibbs Sampling of [Θ|data]: • Initialize Θ ← Θ [0] in the MCMC chain. for b = 1, • • • , B do • Set Θ ← Θ [b−1] . for l = 1, • • • , L do for i = 1, • • • , n T do • Update ω l (x i ) in Θ by sampling from the full conditional distribution (3.2).
• Update f l (x i ) in Θ by sampling from the full conditional distribution (3.5).end for end for
• Update Θ [b] ← Θ. end for Return: MCMC samples {Θ [b] } B b=1 . Posterior Predictive Sampling of [f T (x new )|data]: • Denote {ω l,[b] } B b=1 , {f l,[b] } B b=1 and {f [b] } B b=1
as the posterior samples for w l (x new ), f l (x new ) and f T (x new ), respectively.
for b = 1, • • • , B do for l = 1, • • • , L do • Sample w l,[b] ∼ N k T ω l (X T , x new )K ω l (X T , X T ) −1 ω l (X T ), k ω l (x new , x new ) − k T ω l (X T , x new )K ω l (X T , X T ) −1 k ω l (X T , x new ) with ω l (X T ) taken from Θ [b] . • Sample f l,[b] ∼ N k T l (X l∪T , x new )K l (X l∪T , X l∪T ) −1 f l (X l∪T ), k l (x new , x new ) − k T l (X l∪T , x new )K l (X l∪T , X l∪T ) −1 k l (X l∪T , x new ) with f l (X T ) from Θ [b] . end for • Sample f [b] from the full conditional distribution (3.8) with {f l,[b] } l and {w l,[b] } l . end for Return: MCMC samples {f [b] } B b=1 on f T (x new ).
where ρ l (x new ) = ReLU{ω(x new )}.The distribution B has already been sampled via the MCMC chain
{Θ [b] } B b=1 . Thus, from (3.7), the desired predictive distribution [f T (x new )|data] can be sampled by taking a sample Θ [b] from the MCMC chain {Θ [b] } B b=1 , then performing steps (i)-(iii) with Θ = Θ [b]
to obtain a sample of f T (x new ).Algorithm 3.1 summarizes these steps for posterior predictive sampling.</p>
<p>Multi-fidelity transfer.</p>
<p>As alluded to in the Introduction, the LOL-GP can be adapted for robust transfer learning in multi-fidelity surrogate modeling.Here, the L source systems f 1 (•), • • • , f L (•) are ranked in terms of increasing fidelity levels, and the goal is to transfer learning from such lower-fidelity systems to a target high-fidelity system f T (•).</p>
<p>A natural adaptation of the LOL-GP for this multi-fidelity setting is the model:
f l+1 (x) = ρ l (x)f l (x) + δ l (x), f 1 (x) ∼ GP{0, k 1 (•, •)}, δ l (x) ∼ GP{0, k δ l (•, •)}, ρ l (x) = ReLU{ω l (x)}, ω l (x) ∼ GP{0, k ω l (•, •)}, l = 1, • • • , L.(3.9)
As before, f T (•) = f L+1 (•) is the highest-fidelity simulator, and all GP priors are specified independently.This can be viewed as an extension of the BKO model (2.6) that accounts for potential local transfer from low to high fidelity levels.As with the earlier multi-source transfer model, transfer from the l-th to (l + 1)-th fidelity level is permitted only when the latent function ω l (x) &gt; 0; the posterior distributions on {ω l (•)} L l=1 can then identify beneficial regions for multi-fidelity transfer.As we see in later numerical experiments, when this local transfer behavior is present in multi-fidelity systems, the integration of such behavior can facilitate robust multi-fidelity modeling, mitigating the risk of negative transfer for overly simple (i.e., biased) or complex (i.e., overfit) multi-fidelity surrogates.</p>
<p>Following Section 3.1, we derive next an efficient Gibbs sampler on [Θ|data] for this multifidelity transfer model.Let X l and f l = f l (X l ) be the design points and simulated outputs for the l-th fidelity system, respectively.Assume for now that the GP model parameters (i.e., its variance and length-scale parameters) are fixed; its estimation is discussed later in Section 4.1.Let X (l) = ∪ L+1 l ′ =l X l ′ denote the augmented design consisting of all design points with fidelity level l or higher.With this, redefine the set of latent parameters Θ as
Θ = {Θ 1 , • • • , Θ L }, where Θ l = {ω l (X (l+1) ), f l (X (l+1) )}.
As before, for notational brevity, we presume that no design points are shared between any systems; Appendix SM1 provides analogous expressions for the setting with potentially shared design points.</p>
<p>For the l-th fidelity level, given data = {f 1 , • • • , f L+1 }, we can derive the following full conditional distributions for the latent weight parameters in Θ l :
(3.10) [w l (x i )|Θ − , data] ∼ π i,l N R + {µ i,l,+ , σ 2 i,l,+ }+(1−π i,l ) N R − {µ i,l,− , σ 2 i,l,− }, x i ∈ X (l+1) .
Here, the full conditional weights {π i,l } i,l have the same form as (3.3), with its means and variances given by:
µ i,l,− = k T ω l (X [−i] (l+1) , x i )K ω l (X [−i] (l+1) , X [−i] (l+1) ) −1 ω [−i] l , σ 2 i,l,− = k ω l (x i , x i ) − k T ω l (X [−i] (l+1) , x i )K ω l (X [−i] (l+1) , X [−i] (l+1) ) −1 k ω l (X [−i] (l+1) , x i ), µ i,l,+ = σ 2 i,l,+   µ i,l,− σ 2 i,l,− + e i,l − k T δ l (X [−i] (l+1) , x i )K δ l (X [−i] (l+1) , X [−i] (l+1) ) −1 f [−i] l+1 − ρ [−i] l ⊙ f [−i] l k δ l (x i , x i ) − k T δ l (X [−i] (l+1) , x i )K δ l (X [−i] (l+1) , X [−i] (l+1) ) −1 k δ l (X [−i] (l+1) , x i ) /f l (x i )   , σ 2 i,l,+ =   1 σ 2 i,l,− + [f l (x i )] 2 k δ l (x i , x l ) − k T δ l (X [−i] (l+1) , x i )K δ l (X [−i] (l+1) , X [−i] (l+1) ) −1 k δ l (X [−i] (l+1) , x i )   −1 . (3.11)
Here, X
[−i]
(l+1) denotes the set of X (l+1) without the i-th point, and ω
[−i] l , ρ [−i] l , f [−i] l
and e i,l are as defined in Section 3.1.</p>
<p>Similarly, we can derive the following full conditional distributions on the latent function values in Θ l :
(3.12) [f l (x i )|Θ − , data] ∼ N {µ i,l , σ 2 i,l }, x i ∈ X (l+1) .
Here, the posterior means {µ i,l } i,l and variances {σ 2 i,l } i,l take the closed form expressions:
(3.13) µ i,l = c T l (x i )C(X [−i] aug ) −1 f [−i] aug , σ 2 i,l = c l,l (x i , x i ) − c T l (x i )C(X [−i] aug ) −1 c l (x i ),
where
f aug = [f l (X (l) )] L+1 l=1 and f [−i]
aug omits its i-th point.For brevity, the full expressions for kernel c l,l (•, •), kernel vector c l (•) and and kernel matrix C(X
[−i] aug ) are provided in Appendix SM2.
The key appeal is again that the above closed-form full conditionals (3.11) and (3.13) permit efficient Gibbs sampling of the posterior distribution [Θ|data]; see Algorithm 3.2.</p>
<p>With this Gibbs sampler for [Θ|data], we can then sequentially sample the posterior distribution [f l (x new )|data], from lowest fidelity (i.e., l = 1) to highest fidelity (i.e., l = L + 1).This is achieved by first rewriting the desired posterior distribution [f L+1 (x new )|data] as:
[f L+1 (x new )|data] = [f L+1 (x new )|f L (x new ), Θ, data] [f L (x new )|Θ, data][Θ|data] df L (x new ) dΘ. (3.14)
Note that the distribution [f L (x new )|Θ, data] above can be recursively decomposed via the following equation:
[f l+1 (x new )|Θ, data] = [f l+1 (x new )|f l (x new ), Θ, data][f l (x new )|Θ, data] df l (x new ) =: A l+1 [f l (x new )|Θ, data] df l (x new ), l = 1, • • • , L − 1. (3(3.16) [f L+1 (x new )|data] = A L+1 • • • A 1 B df L (x new ) • • • df 1 (x new ) dΘ,
where A 1 := [f 1 (x new )|Θ, data] and B := [Θ|data] denotes the posterior distribution of latent parameters.Equation (3.16) provides the roadmap for efficient posterior sampling from the desired predictive distribution [f L+1 (x new )|data].Note that distribution B has already sampled via our earlier Gibbs sampler using (3.10) and (3.12).The distribution A 1 follows the standard GP posterior distribution (2.2) on f 1 (•), conditioned on data f 1 (X 1 ) and latent observations f 1 (X (2) ) from Θ.Each of the distributions in { A l+1 } L l=1 can then be sampled via Monte Carlo in two steps.First (i), draw a sample from [ω l (x new )|Θ, data], which follows an independent normal distribution from the standard GP equations (2.2) for ω l (•) conditional on ω l (X (l+1) ).Next (ii), with f l (x new ) and ω l (x new ) in hand, sample f l+1 (x new ) from:
[f l+1 (x new )|f l (x new ), ω l (x new ), Θ, data] ∼ N ρ l (x new )f l (x new ) + k T δ l (X (l+1) , x new )K δ l (X (l+1) , X (l+1) ) −1 • f l+1 (X (l+1) ) − ρ l (X (l+1) ) ⊙ f l (X (l+1) ) , k δ l (x new , x new ) − k T δ l (X (l+1) , x new )K δ l (X (l+1) , X (l+1) ) −1 k δ l (X (l+1) , x new ) .
(3.17)</p>
<p>With this, we can finally construct the posterior sampler for
[f L+1 (x new )|data]. First, take a sample Θ [b] from the MCMC chain for [Θ|data]. Next, with Θ = Θ [b] , sample f 1 (x new ) from A 1 , sample f 2 (x new ) from A 2 ,
and so until f L+1 (x new ) is sampled from A L+1 .The resulting sample follows the desired posterior distribution [f L+1 (x new )|data] from (3.16).One can then repeat this procedure for all samples in the MCMC chain {Θ [b] } B b=1 to obtain corresponding MCMC samples for the high-fidelity output f L+1 (x new ).Algorithm 3.2 summarizes this posterior predictive sampler for the multi-fidelity LOL-GP.</p>
<ol>
<li>Methodological Developments.Next, we outline methodological developments on the LOL-GP for practical implementation.We first present an efficient optimization framework for estimating kernel hyperparameters, then discuss our model's computational complexity and how this can be sped up in the multi-fidelity setting via a nested experimental design.</li>
</ol>
<p>4.1.Estimation of kernel hyperparameters.In the previous section, we presumed the LOL-GP kernel hyperparameters (i.e., hyperparameters for kernels k δ , {k l } L l=1 and {k ω l } L l=1 in the multi-source case, and hyperparameters for kernels {k δ l } L l=1 , k 1 and {k ω l } L l=1 in the multi-fidelity case) are known when deriving the sampler for posterior prediction.Such hyperparameters are, however, unknown in practice, and need to be inferred from data.Given the potentially large number of hyperparameters, careful consideration is needed on how these hyperparameters can be efficiently fit from data.Here, a fully Bayesian inference approach can be costly, as such hyperparameters do not admit closed-form full conditional distributions and may require long sample chains using off-the-shelf MCMC samplers for sufficient mixing.Further, a standard maximum likelihood or maximum-a-posteriori (MAP) approach may also be challenging, as both the likelihood and the marginal likelihood do not admit a closed form due to the presence of the latent weight functions {ω l (•)} L l=1 .One thus requires numerical approximations of the objective function for maximization, which can again be costly.For efficient model training, we elect for an approximate MAP procedure (described below) using the inferred latent parameters Θ from Section 3.</p>
<p>Let Ξ = {Ξ δ , Ξ f , Ξ ω } denote the set of kernel hyperparameters to estimate.For the multisource case, Ξ δ , Ξ f and Ξ ω consist of hyperparameters for kernels k δ , {k l } L l=1 and {k ω l } L l=1 , respectively; for the multi-fidelity case, these consist of hyperparameters for kernels {k δ l } L l=1 , k 1 and {k ω l } L l=1 , respectively.Let [Ξ δ ], [Ξ f ] and [Ξ ω ] denote the respective prior distributions on parameters Ξ.With this, we employ the optimization formulation:
Ξ := arg max Ξ log[Ξ|data, Θ] = arg max Ξ log [data| Θ, Ξ] [ Θ|Ξ] [Ξ] . (4.1)
Algorithm 3.2 Multi-fidelity LOL-GP: Gibbs sampling and posterior predictive sampling
Require: Multi-fidelity data {(X l , f l )} L+1 l=1 , number of MCMC iterations B, initial MCMC parameters Θ [0] . Gibbs Sampling of [Θ|data]: • Initialize Θ ← Θ [0] in the MCMC chain. for b = 1, • • • , B do • Set Θ ← Θ [b−1] . for l = 1, • • • , L do for i = 1, • • • , L+1 l ′ =l+1 n l ′ do • For x i ∈ X (l+1)
, update ω l (x i ) in Θ l by sampling from the full conditional distribution (3.10).</p>
<p>• For x i ∈ X (l+1) , update f l (x i ) in Θ l by sampling from the full conditional distribution (3.12).end for end for
• Update Θ [b] ← Θ. end for Return: MCMC samples {Θ [b] } B b=1 . Posterior Predictive Sampling of [f L+1 (x new )|data]: • Denote {ω l,[b] } B b=1 and {f l,[b] } B b=1
as the posterior samples for w l (x new ) and f l (x new ).
for b = 1, • • • , B do • Set Θ ← Θ [b−1] . • Sample f 1,[b] ∼ N k T 1 (X (1) , x new )K 1 (X (1) , X (1) ) −1 f 1 (X (1) ), k 1 (x new , x new ) − k T 1 (X (1) , x new )K 1 (X (1) , X (1) ) −1 k 1 (X (1) , x new ) . end for for l = 1, • • • , L do for b = 1, • • • , B do • Set Θ ← Θ [b−1] . • Sample w l,[b] ∼ N k T ω l (X (l+1) , x new )K −1 ω l (X (l+1) , X (l+1) )ω l (X (l+1) ), k ω l (x new , x new ) − k T ω l (X (l+1) , x new )K −1 ω l (X (l+1) , X (l+1) )k ω l (X (l+1) , x new ) . • Sample f l+1,[b] from the full conditional distribution (3.17) with f l,[b] and w l,[b] . end for end for Return: MCMC samples {f L+1,[b] } B b=1 on f L+1 (x new ).
Here, Θ is a point estimate of the latent parameters Θ, which we take as the posterior mean of the MCMC samples {Θ [b] } B b=1 from Section 3. Ξ can thus be viewed as an approximate MAP estimator of the kernel parameters Ξ, using the plug-in estimator Θ for Θ.</p>
<p>The key benefit of using this estimator is that, under the LOL-GP, the terms [data|Ξ, Θ] and [ Θ|Ξ] in (4.1) both admit analytic closed-form expressions.In particular, such terms reduce to Gaussian likelihood expressions in both the multi-source and multi-fidelity cases; their full expressions are provided in Appendix SM3.With this, the objective function in (4.1) can be evaluated analytically, thus facilitating efficient optimization via state-of-theart nonlinear optimization algorithms.Such a closed-form objective further permits the use of automatic differentiation [5], which allows for quick computation of objective gradients without the need for deriving gradients by hand.In our later implementation, we made use of the L-BFGS algorithm [41] with automatic differentiation to optimize (4.1), with five random restarts for initializing the optimizer.To contrast, a standard maximum likelihood or MAP estimation of Ξ can be much more costly, due to the need for numerical approximation of the optimization objective.</p>
<p>4.2.Computational complexity.Next, we investigate the computational complexity for fitting the LOL-GP, for both Gibbs sampling and hyperparameter optimization, We present this for first the multi-source setting then the multi-fidelity setting.Table 1 summarizes this complexity analysis.</p>
<p>Consider first the multi-source transfer model from Section 3.1.For Gibbs sampling, note that the matrix inverses in Algorithm 3.1 can be pre-computed prior to sampling; this requires a total work of O n 3 T + L l=1 n 3 l using the decremental algorithm in [42].This same cost is incurred within each objective evaluation of (4.1) for hyperparameter estimation.With this computed, each Gibbs sampling step is dominated by the matrix operations in evaluating {µ T + n T L l=1 n l per Gibbs step.Thus, the bottleneck in model training lies within the earlier computation of matrix inverses, which is unsurprising given similar cubic bottlenecks for standard GPs [20].
i,l,− } n T i=1 L l=1 , {µ i,l,+ } n T
Consider next the multi-fidelity transfer model in Section 3.2.Here, the matrix inverses in Algorithm 3.2 can similarly be pre-computed prior to Gibbs sampling; this incurs a total work of O ( L+1 l=1 n (l) ) 3 using the decremental algorithm in [42], where n (l) is the size of X (l) .This same cost is again required for each objective evaluation of (4.1) for hyperparameter estimation.With this computed, each Gibbs sampling step is dominated by the matrix operations in evaluating {µ i,l,− } n l i=1 L l=1 , {µ i,l,+ } n l i=1 L l=1 and {µ i,l } n l i=1 L l=1 (see Equations (3.11) and (3.13)); this incurs a total complexity of O L l=1 2n 2 (l+1) + n (l+1) n aug per Gibbs step, where n aug = L+1 l=1 n (l) .Again, not surprisingly, the bottleneck for model training lies within the earlier computation of matrix inverses.Compared to the earlier multi-source setting, however, this cost of matrix inverses is considerably higher in the multi-fidelity setting; the former requires O n 3 T + L l=1 n 3 l work, whereas the latter requires O ( L+1 l=1 n l ) 3 work.Fitting the multi-fidelity LOL-GP can thus be costly when the sum of sample sizes over all fidelity levels grows large.We explore next a nested experimental design approach for reducing this training cost in the multi-fidelity setting.</p>
<p>Multi-source (Hyperparameter optimization)</p>
<p>O
n 3 T + L l=1 n 3 l O n 3 T + L l=1 n 3 l Multi-fidelity (Gibbs) O L l=1 2n 2 (l+1) + n (l+1) n aug O L l=1 2n 2 l+1
Multi-fidelity (Hyperparameter optimization)
O L+1 l=1 n l 3 O L+1 l=1 n 3 l
fidelity modeling via nested designs (see, e.g., [28,23]), which can be extended for our local transfer model.In what follows, we explore nested designs for only the multi-fidelity setting as its training cost is more burdensome; similar nested designs can be adapted for the multi-source setting, but this leads to only minor computational speed-ups (see Appendix SM4).We first introduce the notion of nested designs for the multi-fidelity LOL-GP.The lowerfidelity points X 1 , • • • , X L and the high-fidelity points X T = X L+1 are deemed nested if X L+1 ⊆ X L ⊆ • • • ⊆ X 1 , i.e., higher-fidelity design points are nested within lower-fidelity ones.Given this nested structure, consider the following recursive reformulation of the multi-fidelity LOL-GP model:
f l+1 (x) = ρ l (x) fl (x) + δ l (x), fl (x) ∼ N {μ l (x), σ2 l (x)}, l = 1, • • • , L, (4.2)
where ρ l (x) and δ l (x) follow the same priors from Equation (3.9).Here, the mean and variance terms μl (x) and σ2 l (x) are defined recursively as:
μl (x) = ρ l−1 (x)μ l−1 (x) + k T l (X l , x)K −1 l (X l , X l )[f l − ρ l−1 (X l ) ⊙ f l−1 (X l )], σ2 l (x) = ρ 2 l−1 (x)σ 2 l−1 (x) + k l (x, x) − k T l (X l , x)K −1 l (X l , X l )k l (X l , x), l = 1, • • • , L. (4.3)
The intuition behind this recursive formulation is as follows.Here, fl (•) can be interpreted as the GP model for the l-th fidelity level, conditional on simulated data at that level.The recursion arises from (4.3), where μl (x) and σ2 l (x) are defined in terms of their respective functions at the (l − 1)-th fidelity level.One can show (via an analogous argument from [28]) that, with nested design points, the posterior predictive distribution [f T (x new )|data] from the multi-fidelity LOL-GP (3.9) yields the same posterior predictive distribution from the recursive reformulation (4.2).This insight allows for quicker computation, as we show next.</p>
<p>The recursive form (4.2), with nested design points, permits two nice computational speedups for the multi-fidelity LOL-GP.First, for optimizing kernel hyperparameters (Section 4.1) it can be shown (see Appendix SM3) that the likelihood [data|Ξ, Θ] takes the form:
(4.4) [data|Ξ, Θ] = 1 |C 1:(L+1) | exp − 1 2 f T 1:(L+1) C −1 1:(L+1) f 1:(L+1) ,
where f 1:(L+1) is the data vector over all fidelity levels and C 1:(L+1) is its prior covariance matrix.Previously, each evaluation of this likelihood incurs O ( L+1 l=1 n l ) 3 work, which forms the main bottleneck for hyperparameter optimization.Using the recursive formulation with nested designs, this likelihood simplifies to:
[data|Ξ, Θ] ∝ L+1 l=1 1 |K l (X l , X l )| exp − 1 2 z T l K −1 l (X l , X l )z l , (4.5)
where 4.5) thus factorizes the objective function over fidelity levels, which allows for separate optimization of hyperparameters over each fidelity level.The matrix inverses {K l (X l , X l )} L l=1 can further be computed separately, which reduces the objective evaluation cost for hyperparameter optimization from O ( L+1 l=1 n l ) 3 to O L+1 l=1 n 3 l .A similar reduction holds for the precomputation of matrix inverses prior to Gibbs sampling.Second, within each Gibbs sampling step (Algorithm 3.2), one can bypass the sampling step for [f l (x i )|Θ − , data] in (3.12), as such points are known with certainty using nested design points.The complexity of each Gibbs sampling step then reduces to O L l=1 2n 2 l+1 .Table 1 summarizes the above computational speed-ups using nested designs.
z 1 = f 1 and z l = f l − ρ l−1 (X l ) ⊙ f l−1 (X l ) for l = 2, • • • , L + 1. Equation (</p>
<p>Numerical Experiments.</p>
<p>We now explore the proposed models in a suite of numerical experiments.The following existing methods will be used as benchmarks for comparison:</p>
<p>• GP: This is the standard GP model, using only data from the target system with no transfer learning.Here, the standard anisotropic squared-exponential kernel [20] is used, with kernel hyperparameters fitted via maximum likelihood.• KO: This is the Kennedy-O'Hagan model [25] with a constant correlation parameter ρ; its multi-fidelity form follows (2.4), and its multi-source form follows (3.1) with ρ l (x) = ρ.All GPs employ anisotropic squared-exponential kernels [20], with kernel hyperparameters and ρ fitted via maximum likelihood.</p>
<p>• BKO: This is the Bayesian Kennedy-O'Hagan model [45] with ρ(x) varying in x; its multifidelity form follows (2.6), and its multi-source form follows (3.1) with ρ l (x) = ω l (x).All GPs employ anisotropic squared-exponential kernels [20], with kernel hyperparameters fitted via maximum likelihood.• TLNet: A neural network baseline model for transfer learning.Here, source functions are modeled via a shallow fully-connected neural network [29], with the outputs from such models then fed as inputs into another neural network for target system prediction [37].The standard ReLU activation is used within all neural network layers.Appendix SM5 provides further details on this modeling architecture.Here, we emphasize that such a model does not provide the desired probabilistic UQ for surrogates (see Section 1); we thus use this solely as a machine learning benchmark for point prediction.To evaluate surrogate performance, we employ the following two metrics:</p>
<p>• Root-mean-squared-error (RMSE): The RMSE, defined as n −1 2 , measures point prediction accuracy on the target function f T (•).Here, x1 , • • • , xntest are test points, and fT ( x) is the point prediction at x.
test ntest i=1 [ fT ( xi ) − f T ( xi )]
• Continuous ranked probability score (CRPS; [19]): The CRPS is defined as the following scoring rule:</p>
<p>(5.1)
n −1 test ntest i=1 F Y T ( xi ) (u) − I {u≥f T ( xi )} 2 du.
Here, Y T ( x) is the probabilistic (i.e., random) predictor at test point x, with F Y ( x) (u) its cumulative distribution function.For both metrics, smaller values indicate better performance.A smaller RMSE suggests better point prediction accuracy, whereas a smaller CRPS suggests better probabilistic predictions.</p>
<p>5.1.Multi-source transfer experiments.We first consider numerical experiments for multi-source transfer with two source systems (f 1 and f 2 ) and one target system (f T ).The first experiment, which builds off of the earlier 1-d Forrester function [15], uses:
f 1 (x) = f T (x) + 0.4I {x&lt;0.5} (x − 0.5) + 1.6I {x≥0.5} (x − 0.5) cos(40x)(5x − 1), f 2 (x) = f T (x) + 0.4I {x≥0.5} (x − 0.5) + 1.6I {x&lt;0.5} (x − 0.5) cos(20x) √ 4 − 5x, f T (x) = 0.2(6x − 2) 2 sin(12x − 4) + 0.5,(5.2)
where f T is the original Forrester function in [15].Figure 3 visualizes these functions.Note that local transfer is implicitly captured here: the first source f 1 mimics the target function within the local region x ∈ [0, 0.5], whereas the second source f 2 mimics the target within x ∈ [0.5, 1.0].The second experiment, which builds off of the 5-d Friedman function [16], uses:
f 1 (x) = 2 sin(πx 1 x 2 ) + 2.2x 4 − x 5 + 8(x 3 − 0.5) 2 [1.2I {x 3 &lt;0.5} + I {x 3 ≥0.5} (1 + 2 sin(30(x 3 − 0.5)))], f 2 (x) = 2 sin(πx 1 x 2 ) + 2x 4 − 0.8x 5 + 8(x 3 − 0.5) 2 [I {x 3 ≥0.5} + 1.5I {x 3 &lt;0.5} (1 + sin(20(x 3 − 0.5) − 1.5))], f T (x) = 2 sin(πx 1 x 2 ) + 8(x 3 − 0.5) 2 + 2x 4 − 1x 5 ,
where f T is the original Friedman function in [16].As before, local transfer is captured here: the first source mimics the target within the local region x 3 ∈ [0, 0.5], whereas the second source mimics the target within x 3 ∈ [0.5, 1.0].We then generate training design points as follows.For the 1-d Forrester experiment, we employ equally-spaced design points over [0, 1], with n 1 = n 2 = 32 design points for source functions and n T = 7 design points for the target.Here, we use n test = 100 evenly-spaced test points on the target function.[36] over [0, 1] 5 , with n 1 = n 2 = 60 design points for each source and n T = 15 design points on the target.Here, we use n test = 500 randomly-sampled test points on the target.We note that, for both experiments, n T is intentionally set to be smaller than the usual ruleof-thumb sample size of 10d (see [34]), to mimic the cost-intensive nature of the target system and thus the need for transfer learning.Figure 4a visualizes the predictive performance of the compared methods for the 1-d experiment, with Table 2 reporting its predictive metrics.We see that, in the presence of local transfer, the proposed LOL-GP significantly outperforms existing models in terms of both point prediction (i.e., lower RMSE) and probabilistic prediction (i.e., lower CRPS).A careful inspection further reveals the potential for negative transfer when the transfer model is poorly specified.From Table 2, the KO model experiences considerable negative transfer: it yields worse performance than the standard GP (without transfer) for both metrics.Figure 4a shows that a potential reason may be high model bias: by presuming constant transfer over the full domain, the KO erroneously transfers information from source 1 over [0.5, 1], which considerably worsens predictions.The LOL-GP addresses this via a careful regularization of  the transfer functions {ρ l (x)} 2 l=1 , to identify regions where transfer is beneficial or detrimental for each source.Figure 5 (left) visualizes this via the posterior distributions of the latent functions {ω l (x)} 2 l=1 , and the posterior mean of its corresponding transfer functions {ρ l (x)} 2 l=1 ; see Equation (3.1).We see that the latent function for source 1 is positive only within [0, 0.5], which is desirable as transfer is beneficial only in this local region (see Figure 3).Conversely, the latent function for source 2 is largely positive only within [0.5, 1], which is desired as transfer is only beneficial in this region (see Figure 3).Thus, by identifying this underlying local transfer behavior from data, the LOL-GP can provide robust transfer learning by mitigating the earlier danger of negative transfer.</p>
<p>Next, Figure 4b visualizes the predictive performance for the 5-d experiment, with Table 2 reporting its predictive metrics.Again, in the presence of local transfer, we see that the LOL-GP yields considerably improved performance in terms of both point and probabilistic predictions.From Table 2, we now see that the BKO experiences significant negative transfer: it yields worse performance for both metrics over the standard GP (without transfer).Given previous discussions, one potential reason may be high model variance: by permitting high Multi-source (Forrester): ω Multi-source (Friedman): ρ flexibility for the transfer functions {ρ l (x)} 2 l=1 without regularization, the BKO transfers information within regions where transfer is detrimental, which in turn worsens performance.The LOL-GP addresses this via careful regularization of transfer functions to identify regions where transfer is beneficial or detrimental.Figure 5 (right) visualizes this via the posterior mean of these two transfer functions, projected onto the x 3 -space.We see that the transfer function for source 1 is largely positive only for x 3 ∈ [0, 0.5], which is desirable as transfer is beneficial only within this region (see Equation (5.1)).Similarly, the transfer function for source 2 is largely positive only for x 3 ∈ [0.5, 1], which is again desired as transfer is beneficial here.By incorporating such local transfer behavior, the LOL-GP provides robust transfer learning from sources to target, mitigating the earlier risk of negative transfer.</p>
<p>5.2.Multi-fidelity transfer experiments.Next, we consider numerical experiments for multi-fidelity transfer.The first experiment, which again builds off of the 1-d Forrester function [15], uses:
f 1 (x) = f 2 (x) + 0.2I {x≥0.5} (x − 0.5) + 1.6I {x&lt;0.5} (x − 0.5) cos(30x)(5x − 2) 2 , f 2 (x) = 0.2(6x − 2) 2 sin(12x − 4) + 0.5. (5.3)
Here, f 1 and f 2 are taken as the low-fidelity and high-fidelity functions, respectively.Figure 6a visualizes these functions.Similar to before, f 1 mimics f 2 within the local region x ∈ [0.5, 1] but not within x ∈ [0, 0.5], thus capturing local transfer from low to high fidelity.Note that this is the earlier motivating example in Section 2.  of the 2-d Branin function [11], uses:
f 1 (x) = f 3 (x) − 0.01(x 1 − x 2 ) + 0.5I {C 1 (x 1 ,x 2 )&lt;0} sin[0.1C 1 (x 1 , x 2 ) + 1], f 2 (x) = f 3 (x) − 0.01(x 1 − x 2 ) + 0.5I {C 2 (x 1 ,x 2 )&lt;0} sin[0.2C 2 (x 1 , x 2 ) + 1], f 3 (x) = 0.01 x 2 − 5.1 4π 2 x 2 1 + 5 π x 1 − 1 2 + 10 1 − 1 8π cos(x 2 1 ) + 10 ,(5.4)
where
C 1 (x 1 , x 2 ) = (x 1 − 10) 2 + (x 2 − 2.5) 2 − 8 2 and C 2 (x 1 , x 2 ) = (x 1 − 10) 2 + (x 2 − 2.5) 2 − 6 2 .
Here, f 1 , f 2 and f 3 are taken as the low-, medium-and high-fidelity functions, respectively.Figure 6b visualizes these functions.One sees that, aside from the local region on the right, f 1 and f 2 mimic the high-fidelity function f 3 , thus capturing local transfer.</p>
<p>We then generate design points as follows.For the 1-d experiment, we take equally-spaced design points on [0, 1], with n 1 = 16 and n 2 = 8; the high-fidelity (target) design are nested within the low-fidelity design.Here, n test = 100 equally-spaced points are used for testing.For the 2-d experiment, design points are selected via a sliced Latin hypercube design [3] (first generated on [0, 1] 2 , then rescaled to [−5, 10] 2 ), with sample sizes n 1 = 72, n 2 = 36 and n 3 = 18; such points are selected in a nested fashion.Here, n test = 400 grid points are used for testing.</p>
<p>Figure 7a visualizes the predictive performance for the 1-d experiment, with Table 2 reporting its predictive metrics.Again, in the presence of local transfer, the LOL-GP yields considerably better performance over existing methods for both point predictions (i.e., lower RMSE) and probabilistic predictions (i.e., lower CRPS).Here, while no existing methods experience negative transfer (i.e., its predictive metrics are better than that for the standard GP), Figure 7a shows some evidence of poor local predictions from high bias (e.g., KO) or overfit (e.g., BKO, TLNet) models; the LOL-GP addresses this via the modeling of local transfer.Figure 8 (left) shows this via the posterior distributions of the latent and transfer functions ω 1 (x) and ρ 1 (x) for the fitted LOL-GP.We see the estimated ρ 1 (x) is largely positive only within [0.5, 1], which is desirable as transfer is beneficial only within this region (see Figure 6a); integrating this within the LOL-GP thus greatly improves predictions.-5 2.5 10   Next, Figure 7b shows the predictive performance for the 2-d experiment, with Table 2 summarizing its metrics.The LOL-GP again significantly outperforms existing methods in the presence of local transfer.Here, all three existing models experience considerable negative transfer: their metrics are considerably worse than the standard GPs, both for RMSE and CRPS.This demonstrates the dangers of an overly-biased or overfit transfer model; at either end of the bias-variance trade-off, this transfer can greatly deteriorate performance.Figure 8 (right) shows the estimated transfer function ρ2 (x), which is largely positive for x 1 ∈ [0, 0.5]; this is desirable as transfer is largely beneficial within this region (see Figure 6b).By incorporating such local transfer structure, the LOL-GP carefully trades off between bias and variance, which in turn mitigates the negative transfer experienced by existing methods.</p>
<ol>
<li>Surrogate Modeling of Stress Analysis in Jet Engine Turbines.Jet engines are broadly used in commercial aviation applications [40], and its reliable performance is of upmost importance during operation.Such engines generate thrust as follows.A heated gas is sucked into and passed to a compressor, where it is subjected to high pressure and high temperatures.This gas is then discharged as a fast-moving jet through a which then generates the necessary thrust for propulsion.Here, careful stress analysis of turbine blades is essential for assessing engine reliability; these blades should have enough strength to withstand any stress and/or deformations in such a high-pressure (up to 0.7 MPa) and high-temperature (up to 1050 • C) environment.</li>
</ol>
<p>Figure 9 shows the blade schematic for the considered jet engine turbine in our study, following [51].For initial analysis, physical experiments can be prohibitively expensive, as it requires considerable costs for engine prototyping, manufacturing and subsequent testing.We thus adopt a virtual experimentation approach, which uses finite element analysis (FEA) for simulating the stress profile of turbine blades during operation.For this, we make use of the MATLAB module in [51], which can perform such a simulation under different operating environments and blade materials.This module leverages a static-structural model on turbine blades, coupled with steady-state thermal model for thermal expansion; further details are provided in [51].The underlying system of governing equations are then solved via a mesh discretization of the spatiotemporal domain.With this, we explore two set-ups for surrogate modeling, the first for multi-source transfer and the second for multi-fidelity transfer.For both set-ups, the response of interest is the maximum simulated stress at the root of the blade.For the multi-source set-up, we use a single (i.e., L = 1) source system.The source and target systems employ different blade material: the source uses ceramic matrix composites [4], whereas the target uses nickel-based alloys [13].</p>
<p>Here, d = 2 input parameters (controlling operating conditions) are considered: the pressure load on the pressure side (ranging from [0.25, 0.75] MPa), and the gas temperature (ranging from [550, 1050] • C). Figure 10 shows the simulated stress profiles using these different blade materials at two choices of input parameters.We see some evidence for local transfer: for the first parametrization, the source and target systems are quite similar, whereas for the second parametrization, the two systems are markedly different.</p>
<p>For the multi-fidelity set-up, we employ L = 2 lower-fidelity systems with maximum mesh size of 0.06 and 0.04; the high-fidelity target system uses a smaller maximum mesh size of 0.02.Here, d = 2 input parameters (controlling operating conditions) are considered: the pressure loads on the pressure side and on the suction side (both ranging from [0.25, 0.75] MPa).The pressure and suction sides of the blade are labeled in Figure 9. Figure 11 shows the simulated stress profiles with varying fidelities at two choices of input parameters.Again, we see evidence for local transfer: at the first input, the systems are considerably different from low to high fidelity, whereas at the second input, they are quite similar.For brevity, we refer to the two considered input parameters as x 1 and x 2 for both the multi-source and the multi-fidelity set-up.</p>
<p>We then generate design points as follows.For the multi-source set-up, we take n 1 = 32 design points on the source and n T = 8 points on the target, both generated via separate Latin hypercube designs.For the multi-fidelity set-up, we take n 1 = 40, n 2 = 20 and n 3 = 10  design points on the low-, medium-and high-fidelity simulators.Such points are obtained via a sliced Latin hypercube design [3], with design points nested for increasing fidelity levels.For both set-ups, n test = 121 separate grid points are used for testing.Figure 12 shows the training design points for both set-ups, along with its corresponding response surfaces.</p>
<p>Consider first the multi-source set-up, with predictive metrics in Table 3 and visualization in Figure 13.As before, the proposed LOL-GP yields the best predictive performance in terms of both RMSE and CRPS.Here, all three existing models exhibit negative transfer: its RMSE and/or CRPS can be considerably larger than that for the GP surrogate with no transfer.The LOL-GP appears to mitigate this by leveraging our prior observation of local transfer, i.e., that ceramic blades (source) may perform similarly as nickel-based blades  (target) at certain parametrizations but not at others.To explore this, Figure 14 (left) shows the estimated transfer function ρ(x) for the LOL-GP.We see that this transfer function is near zero in the top-left corner where x 1 (gas temperature) is high and x 2 (pressure load on the pressure side) is low.This corroborates our earlier observation of local transfer from Figure 10: for a parameter setting with x 1 high and x 2 low, the simulated stress can vary considerably with different blade materials.Our model thus identifies and incorporates this local transfer behavior to facilitate robust transfer learning with limited target data.Consider next the multi-fidelity set-up, with predictive metrics in Table 3 and visualization in Figure 13.Again, the LOL-GP provides considerably improved predictions over existing methods in terms of RMSE and CRPS.Here, the KO and BKO models yield improvements over the standard GP (thus no negative transfer), whereas the TLNet still experiences considerable negative transfer.model.In the bottom-right corner, where x 2 (pressure load on the suction side) is larger than x 1 (pressure load on the pressure side), we see that the LOL-GP assigns near-zero transfer.This again corroborates our earlier observation of local transfer from Figure 11, where with x 2 &gt; x 1 , the simulated stress response can be considerably different over fidelity levels, and vice versa.By capturing this local transfer behavior in our surrogate model, the LOL-GP can facilitate more robust and effective transfer learning over existing methods.</p>
<p>Conclusion.</p>
<p>This paper presents a novel transfer learning model, called the LOL-GP, for probabilistic surrogate modeling of costly computer simulators.The idea is to transfer information from available data on related source systems for accurate predictions on the target system of interest.For this, existing transfer learning surrogate models in the literature have a key limitation: such models may experience considerable "negative transfer", in that its transfer of information may worse surrogate performance.To address this, the LOL-GP models for a key "local transfer" property that identifies regions where transfer is beneficial and regions where it can be detrimental.Such local transfer is expected in many scientific systems, where systems may behave similarly at certain parametrizations but differently at others.We derive an efficient Gibbs sampling algorithm for posterior predictive sampling on the LOL-GP, for both the multi-source and multi-fidelity settings.We then demonstrate the effectiveness of the LOL-GP over existing methods, in a suite of numerical experiments and an application for jet turbine design.</p>
<p>Given these promising results, there are many promising directions to explore for future research.One direction is experimental design: how should design points be selected on the target system to maximize surrogate performance given a limited computing budget?The development of novel experimental design methods for transfer learning can further improve performance for the LOL-GP, and we are investigating this as future work.Another direction is transfer learning for more complex simulator outputs (e.g., high-dimensional vectors and flows), which are widely encountered in scientific computing.Finally, we are exploring appli-</p>
<p>Figure 1 :
1
Figure 1: Visualizing the source and target functions (with corresponding design points) for our 1-d Forrester motivating example.</p>
<p>Figure 2 :
2
Figure 2: Visualizing the fitted surrogates (dotted: predictive mean, shaded: 95% confidence region) with the true target function (solid line) for the standard GP and existing transfer learning surrogates.Here, TLNet has no shaded regions as it does not quantify predictive uncertainties.</p>
<p>. 15 )
15
Here, A l+1 denotes the distribution [f l+1 (x new )|f l (x new ), Θ, data].Applying the identity (3.15) recursively for l = L−1, L−2, • • • , 1, one can then write the posterior [f L+1 (x new )|data] as:</p>
<p>{µ i,l } n T i=1 L l=1 (see Equations (3.4) and (3.6)); this amounts to a total complexity of O 3Ln 2</p>
<ol>
<li>3 .
3
Nested experimental design.We present next a nested experimental design approach for the multi-fidelity LOL-GP, which targets the reduction of the aforementioned O ( L+1 l=1 n l ) 3 cost for model training.This builds off of related work on recursive multi-</li>
</ol>
<p>For the 5-d Friedman function, we employed Latin hypercube</p>
<p>Figure 3 :
3
Figure 3: Visualizing the source and target functions for the 1-d multi-source Forrester experiment.</p>
<p>5-d multi-source Friedman experiment.</p>
<p>Figure 4 :
4
Figure 4: Visualizing the predictive performance of the compared methods: (a) The true target function (solid line), its prediction (dotted line) and 95% confidence intervals (shaded) for the 1-d multi-source Forrester experiment; (b) For the 5-d multi-source Friedman experiment, plots of the predicted test responses as a function of its true response order.The blue curve marks the true response values.</p>
<p>Figure 5 :
5
Figure 5: (Left) Visualizing the estimated latent functions {ω l (x)} 2 l=1 and its corresponding transfer functions {ρ l (x)} 2 l=1 for the LOL-GP in the 1-d multi-source Forrester experiment.The posterior means and 95% confidence intervals for {ω l (x)} 2 l=1 are marked by solid and dotted black lines, with the posterior means of {ρ l (x)} 2 l=1 marked by colored lines.(Right) For the 5-d multi-source Friedman experiment, a visualization of the posterior means of the transfer functions {ρ l (x)} 2 l=1 projected onto the x 3 -space; the first is plotted in blue (left) and the second in green (right).</p>
<p>3 .
3
The second experiment, which builds off 2-d multi-fidelity Branin experiment.</p>
<p>Figure 6 :
6
Figure 6: Visualizing the high-fidelity (target) functions for our multi-fidelity experiments, as well as its lower-fidelity (source) functions.</p>
<p>1-d multi-fidelity Forrester experiment.</p>
<p>2-d multi-fidelity Branin experiment.</p>
<p>Figure 7 :
7
Figure 7: Visualizing the predictive performance of the compared methods.(a) The true highfidelity function (solid line), its prediction (dotted line) and 95% confidence intervals (shaded) for the 1-d multi-fidelity Forrester experiment.(b) Contours of the predicted high-fidelity functions for the 2-d multi-fidelity Branin experiment.</p>
<p>Figure 8 :
8
Figure 8: (Left) Visualizing the estimated latent function ω 1 (x) and its corresponding transfer function ρ 1 (x) for the LOL-GP in the 1-d multi-fidelity Forrester experiment.The posterior means and 95% confidence intervals for ω 1 (x) are marked by solid and dotted black lines, and the posterior means for ρ 1 (x) is marked by the blue line.(Right) A contour plot of the posterior mean for the transfer function ρ 2 (x) in the 2-d multi-fidelity Branin experiment.</p>
<p>Figure 9 :Figure 10 :
910
Figure 9: Schematic of the considered jet engine turbine blade in [51].</p>
<p>Figure 11 :
11
Figure 11: Visualizing the simulated stress solution in the multi-fidelity set-up for our jet turbine application.Different columns show the simulations at different mesh sizes, and the top and bottom rows correspond to different parameter settings for (x 1 , x 2 ).The top and bottom rows correspond to different parameter settings for (x 1 , x 2 ).The response of interest is the maximum stress at the root of the blade, which is marked by the pink rectangle.</p>
<p>Figure 12 :
12
Figure 12: Visualization of the true source and target response surfaces in the multi-source and multi-fidelity set-ups for our jet turbine application.Design points are marked by black dots.</p>
<p>Figure 13 :
13
Figure 13: Visualizing the true target response surface (left), and the predicted response surfaces (right) from the compared surrogates for our jet turbine application.</p>
<p>Figure 14 (
14
Figure 14: (Left) Visualizing the posterior mean of the transfer function ρ 1 (x) for the LOL-GP in the multi-source set-up for our jet turbine application.(Right) Visualizing the posterior mean of the transfer function ρ 2 (x) for the LOL-GP in the multi-fidelity set-up.</p>
<p>Table 1 :
1
Computational complexity analysis for training the multi-source and multi-fidelity LOL-GP.Provided are the costs (in big-O) for each Gibbs sampling iteration (Sections 3.1 and 3.2) and each objective evaluation for hyperparameter optimization (Section 4.1).
Non-nested designNested designMulti-source (Gibbs)O 3Ln 2 T + n TL l=1 n lO 2Ln 2 T</p>
<p>Table 2 :
2
Prediction metrics for the multi-source and multi-fidelity 1-d Forrester and 5-d Friedman experiments.Metrics in red indicate negative transfer (i.e., worse performance than the GP without transfer).Here, the CRPS (which measures probabilistic prediction quality) cannot be computed for TLNet as it does not provide a probabilistic predictor.
Multi-sourceMulti-fidelityForresterFriedmanForresterBraninRMSE CRPS RMSE CRPS RMSE CRPS RMSE CRPSGP (no transfer) 0.521 0.235 0.475 0.305 0.425 0.195 0.112 0.043KO1.271 0.595 0.441 0.287 0.307 0.161 0.173 0.084BKO0.290 0.186 0.624 0.335 0.203 0.102 0.122 0.065TLNet0.516-0.418-0.241-0.255-LOL-GP0.096 0.047 0.272 0.152 0.127 0.062 0.064 0.034designs</p>
<p>Table 3 :
3
Prediction metrics for surrogate modeling of the jet turbine application.Metrics in red indicate negative transfer (i.e., worse performance than the GP without transfer).Here, the CRPS (which measures probabilistic prediction quality) cannot be computed for TLNet as it does not provide a probabilistic predictor.GP (no transfer) KO BKO TLNet LOL-GP
Multi-sourceRMSE CRPS0.131 0.0620.118 0.177 0.065 0.1020.142 -0.100 0.056Multi-fidelityRMSE CRPS0.166 0.0660.127 0.106 0.053 0.0630.172 -0.078 0.048
In what follows, we denote [θ|Θ−, data] as the full conditional distribution of parameter θ ∈ Θ, conditioned on all remaining parameters on Θ as well as training data.
cations of transfer learning surrogates for accelerating decision-making in different scientific disciplines.One promising area is high-energy physics, where particle simulators are highly expensive (requiring thousands of CPU hours per run; see[27]) but there exists a wealth of existing simulation data on related particle systems in the literature.We aim to target such areas for impactful applications.
Multi-source set-up: Multi-fidelity set-up: REFERENCES. </p>
<p>A F Agarap, arXiv:1803.08375Deep learning using rectified linear units (ReLU). 2018arXiv preprint</p>
<p>Stochastic kriging for simulation metamodeling. B Ankenman, B L Nelson, J Staum, 2008 Winter simulation conference. IEEE2008</p>
<p>Optimal sliced Latin hypercube designs. S Ba, W R Myers, W A Brenneman, Technometrics. 572015</p>
<p>N P Bansal, J Lamon, Ceramic Matrix Composites: Materials, Modeling and Technology. John Wiley &amp; Sons2014</p>
<p>Automatic differentiation in machine learning: a survey. A G Baydin, B A Pearlmutter, A A Radul, J M Siskind, Journal of Machine Learning Research. 182018</p>
<p>S Bickel, ECML-PKDD Discovery Challenge Workshop. 2006ECML-PKDD discovery challenge</p>
<p>Determining the jet transport coefficient q from inclusive hadron suppression measurements using Bayesian parameter estimation. S Cao, Y Chen, J Coleman, J Mulligan, P Jacobs, R Soltz, A Angerami, R Arora, S Bass, L Cunqueiro, Physical Review C. 104249052021</p>
<p>Adaptive design for Gaussian process regression under censoring. J Chen, S Mak, V R Joseph, C Zhang, The Annals of Applied Statistics. 162022</p>
<p>A hierarchical expected improvement method for Bayesian optimization. Z Chen, S Mak, C J Wu, Journal of the American Statistical Association. 1192024</p>
<p>Transfer prior knowledge from surrogate modelling: A meta-learning approach. M Cheng, C Dang, D M Frangopol, M Beer, X.-X Yuan, Computers &amp; Structures. 2601067192022</p>
<p>The global optimization problem: an introduction. L C W Dixon, Towards Global Optimiation. 21978</p>
<p>Transfer learning for text classification. C B Do, A Y Ng, Advances in Neural Information Processing Systems. 182005</p>
<p>Superalloys: A Technical Tuide. M J Donachie, S J Donachie, 2002ASM International</p>
<p>Multisystem Bayesian constraints on the transport coefficients of QCD matter. D Everett, W Ke, J.-F Paquet, G Vujanovic, S Bass, L Du, C Gale, M Heffernan, U Heinz, D Liyanage, Physical Review C. 103549042021</p>
<p>Engineering Design via Surrogate Modelling: A Practical Guide. A Forrester, A Sobester, A Keane, 2008John Wiley &amp; Sons</p>
<p>Multivariate adaptive regression splines. J H Friedman, The Annals of Statistics. 191991</p>
<p>A Gelman, J B Carlin, H S Stern, D B Rubin, Bayesian Data Analysis. Chapman and Hall/CRC1995</p>
<p>Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. S Geman, D Geman, IEEE Transactions on Pattern Analysis and Machine Intelligence. 1984</p>
<p>Strictly proper scoring rules, prediction, and estimation. T Gneiting, A E Raftery, Journal of the American Statistical Association. 1022007</p>
<p>R B Gramacy, Surrogates: Gaussian Process Modeling, Design, and Optimization for the Applied Sciences. CRC press2020</p>
<p>T Hastie, R Tibshirani, J H Friedman, J H Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer20092</p>
<p>Spike and slab variable selection: Frequentist and Bayesian strategies. H Ishwaran, J S Rao, The Annals of Statistics. 332005</p>
<p>A graphical multi-fidelity Gaussian process model, with application to emulation of heavy-ion collisions. Y Ji, S Mak, D Soeder, J Paquet, S A Bass, Technometrics. 662024</p>
<p>Conglomerate multi-fidelity Gaussian process modeling, with application to heavy-ion collisions. Y Ji, H S Yuchi, D Soeder, J.-F Paquet, S A Bass, V R Joseph, C Wu, S Mak, SIAM/ASA Journal on Uncertainty Quantification. 122024</p>
<p>Predicting the output from a complex computer code when fast approximations are available. M C Kennedy, A O'hagan, Biometrika. 872000</p>
<p>Bayesian analysis of multifidelity computer models with local features and nonnested experimental designs: application to the WRF model. B A Konomi, G Karagiannis, Technometrics. 632021</p>
<p>Inclusive jet and hadron suppression in a multistage approach. A Kumar, Y Tachibana, C Sirimanna, G Vujanovic, S Cao, A Majumder, Y Chen, L Du, R Ehlers, D Everett, Physical Review C. 107349112023</p>
<p>Recursive co-kriging model for design of computer experiments with multiple levels of fidelity. L , Le Gratiet, J Garnier, International Journal for Uncertainty Quantification. 42014</p>
<p>Deep learning. Y Lecun, Y Bengio, G Hinton, Nature. 5212015</p>
<p>J Lee, Y Bahri, R Novak, S S Schoenholz, J Pennington, J Sohl-Dickstein, arXiv:1711.00165Deep neural networks as Gaussian processes. 2017arXiv preprint</p>
<p>Additive multi-index Gaussian process modeling, with application to multi-physics surrogate modeling of the quark-gluon plasma. K Li, S Mak, J.-F Paquet, S A Bass, arXiv:2306.072992023arXiv preprint</p>
<p>On negative transfer and structure of latent functions in multioutput Gaussian processes. M Li, R Kontar, SIAM/ASA Journal on Uncertainty Quantification. 102022</p>
<p>Efficient emulation of relativistic heavy ion collisions with transfer learning. D Liyanage, Y Ji, D Everett, M Heffernan, U Heinz, S Mak, J.-F Paquet, Physical Review C. 105349102022</p>
<p>Choosing the sample size of a computer experiment: A practical guide. J L Loeppky, J Sacks, W J Welch, Technometrics. 2009</p>
<p>An efficient surrogate model for emulation and physics extraction of large eddy simulations. S Mak, C.-L Sung, X Wang, S.-T Yeh, Y.-H Chang, V R Joseph, V Yang, C J Wu, Journal of the American Statistical Association. 1132018</p>
<p>A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. M D Mckay, R J Beckman, W J Conover, Technometrics. 422000</p>
<p>A composite neural network that learns from multi-fidelity data: Application to function approximation and inverse PDE problems. X Meng, G E Karniadakis, Journal of Computational Physics. 4011090202020</p>
<p>Targeted variance reduction: Robust Bayesian optimization of black-box simulators with noise parameters. J J Miller, S Mak, arXiv:2403.038162024arXiv preprint</p>
<p>A misfire-integrated Gaussian process (MInt-GP) emulator for energy-assisted compression ignition (EACI) engines with varying cetane number jet fuels. S R Narayanan, Y Ji, H D Sapra, C.-B M Kweon, K S Kim, Z Sun, S Kokjohn, S Mak, S Yang, International Journal of Engine Research. 146808742412295142024</p>
<p>Physics-integrated segmented Gaussian process (SegGP) learning for cost-efficient training of diesel engine control system with low cetane numbers. S R Narayanan, Y Ji, H D Sapra, S Yang, S Mak, Z Sun, S Kokjohn, K Kim, C B Kweon, AIAA SCITECH 2023 Forum. 20231283</p>
<p>. J Nocedal, S J Wright, Numerical Optimization. 1999Springer</p>
<p>Enhanced kriging leave-one-out crossvalidation in improving model estimation and optimization. Y Pang, Y Wang, X Lai, S Zhang, P Liang, X Song, Computer Methods in Applied Mechanics and Engineering. 4141161942023</p>
<p>Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling. P Perdikaris, M Raissi, A Damianou, N D Lawrence, G E Karniadakis, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences. 473201607512017</p>
<p>Bayesian calibration of inexact computer models. M Plumlee, Journal of the American Statistical Association. 1122017</p>
<p>Bayesian hierarchical modeling for integrating low-accuracy and high-accuracy experiments. P Z Qian, C J Wu, Technometrics. 502008</p>
<p>Comparison and selection of suitable materials applicable for gas turbine blades. G K Salwan, R Subbarao, S Mondal, Materials Today: Proceedings. 462021</p>
<p>The Design and Analysis of Computer Experiments. T J Santner, B J Williams, W I Notz, 2003Springer1</p>
<p>Active learning for deep Gaussian process surrogates. A Sauer, R B Gramacy, D Higdon, Technometrics. 652023</p>
<p>Transfer learning on multifidelity data. D H Song, D M Tartakovsky, Journal of Machine Learning for Modeling and Computing. 32022</p>
<p>Hierarchical shrinkage Gaussian processes: applications to computer code emulation and dynamical system recovery. T Tang, S Mak, D Dunson, SIAM/ASA Journal on Uncertainty Quantification. 122024</p>
<p>Thermal stress analysis of jet engine turbine blade. 2023The MathWorks Inc</p>
<p>Surrogate modeling of computer experiments with different mesh densities. R Tuo, C J Wu, D Yu, Technometrics. 562014</p>
<p>Afec: Active forgetting of negative transfer in continual learning. L Wang, M Zhang, Z Jia, Q Li, C Bao, K Ma, J Zhu, Y Zhong, Advances in Neural Information Processing Systems. 342021</p>
<p>Regularized multi-output Gaussian convolution process with domain adaptation. X Wang, C Wang, X Song, L Kirby, J Wu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 452022</p>
<p>C K Williams, C E Rasmussen, Gaussian Processes for Machine Learning. MIT Press20062</p>
<p>Evolving Bayesian emulators for structured chaotic time series, with application to large climate models. D Williamson, A T Blaker, SIAM/ASA Journal on Uncertainty Quantification. 22014</p>
<p>Implementing transfer learning across different datasets for time series forecasting. R Ye, Q Dai, Pattern Recognition. 1091076172021</p>
<p>Common proper orthogonal decomposition-based spatiotemporal emulator for design exploration. S.-T Yeh, X Wang, C.-L Sung, S Mak, Y.-H Chang, L Zhang, C J Wu, V Yang, AIAA Journal. 562018</p>
<p>A surrogate model with data augmentation and deep transfer learning for temperature field prediction of heat source layout, Structural and Multidisciplinary Optimization. X Zhao, Z Gong, J Zhang, W Yao, X Chen, 202164</p>
<p>A comprehensive survey on transfer learning. F Zhuang, Z Qi, K Duan, D Xi, Y Zhu, H Zhu, H Xiong, Q He, Proceedings of the IEEE. 1092020</p>            </div>
        </div>

    </div>
</body>
</html>
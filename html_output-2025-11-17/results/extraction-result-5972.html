<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5972 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5972</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5972</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-265609409</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.02003v3.pdf" target="_blank">A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into"The Good"(beneficial LLM applications),"The Bad"(offensive applications), and"The Ugly"(vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5972",
    "paper_id": "paper-265609409",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0096475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly
20 Mar 2024</p>
<p>Yifan Yao 
Drexel University
3675 Market St19104PhiladelphiaPAUSA</p>
<p>Jinhao Duan 
Drexel University
3675 Market St19104PhiladelphiaPAUSA</p>
<p>Kaidi Xu 
Drexel University
3675 Market St19104PhiladelphiaPAUSA</p>
<p>Yuanfang Cai 
Drexel University
3675 Market St19104PhiladelphiaPAUSA</p>
<p>Zhibo Sun 
Drexel University
3675 Market St19104PhiladelphiaPAUSA</p>
<p>Yue Zhang 
Drexel University
3675 Market St19104PhiladelphiaPAUSA</p>
<p>A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly
20 Mar 20240828667463ABA4F03D52D9DA3D887A32arXiv:2312.02003v3[cs.CR]Preprint submitted to Elsevier Yifan Yao et al.: Preprint submitted to ElsevierLarge Language Model (LLM)LLM SecurityLLM PrivacyChatGPTLLM AttacksLLM Vulnerabilities
Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation.They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation).In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks.This paper explores the intersection of LLMs with security and privacy.Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs.Through a comprehensive literature review, the paper categorizes the papers into "The Good" (beneficial LLM applications), "The Bad" (offensive applications), and "The Ugly" (vulnerabilities of LLMs and their defenses).We have some interesting findings.For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods.However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities.We have identified areas that require further research efforts.For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality.Safe instruction tuning, a recent development, requires more exploration.We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity.</p>
<p>Introduction</p>
<p>A large language model is the language model with massive parameters that undergoes pretraining tasks (e.g., masked language modeling and autoregressive prediction) to understand and process human language, by modeling the contextualized text semantics and probabilities from large amounts of text data.A capable LLM should have four key features [323]: (i) profound comprehension of natural language context; (ii) ability to generate human-like text; (iii) contextual awareness, especially in knowledge-intensive domains; (iv) strong instruction-following ability which is useful for problem-solving and decision-making.</p>
<p>There are a number of LLMs that were developed and released in 2023, gaining significant popularity.Notable examples include OpenAI's ChatGPT [203], Meta AI's LLaMA [4], and Databricks' Dolly 2.0 [50].For instance, ChatGPT alone boasts a user base of over 180 million [69].LLMs now offer a wide range of versatile applications across various domains.Specifically, they not only provide technical support to domains directly related to language processing (e.g., search engines [352,13], customer support [259], translation [327,138]) but also find utility in more general scenarios such as code generation [118], healthcare [274], finance [310], and education [186].This showcases their adaptability and potential to streamline language-related tasks across diverse industries and contexts.yy566@drexel.edu(Y.Yao); jd3734@drexel.edu(J.Duan); kx46@drexel.edu(K.Xu); yfcai@cs.drexel.edu(Y.Cai); zs384@drexel.edu(Z.Sun); yz899@drexel.edu(Y.Zhang) ORCID(s):</p>
<p>LLMs are gaining popularity within the security community.As of February 2023, a research study reported that GPT-3 uncovered 213 security vulnerabilities (only 4 turned out to be false positives) [141] in a code repository.In contrast, one of the leading commercial tools in the market detected only 99 vulnerabilities.More recently, several LLMpowered security papers have emerged in prestigious conferences.For instance, in IEEE S&amp;P 2023, Hammond Pearce et al. [211] conducted a comprehensive investigation employing various commercially available LLMs, evaluating them across synthetic, hand-crafted, and real-world security bug scenarios.The results are promising, as LLMs successfully addressed all synthetic and hand-crafted scenarios.In NDSS 2024, a tool named Fuzz4All [313] showcased the use of LLMs for input generation and mutation, accompanied by an innovative autoprompting technique and fuzzing loop.</p>
<p>These remarkable initial attempts prompt us to delve into three crucial security-related research questions:</p>
<p>• RQ1.How do LLMs make a positive impact on security and privacy across diverse domains, and what advantages do they offer to the security community?</p>
<p>• RQ2.What potential risks and threats emerge from the utilization of LLMs within the realm of cybersecurity?</p>
<p>• RQ3.What vulnerabilities and weaknesses within LLMs, and how to defend against those threats?Findings.To comprehensively address these questions, we conducted a meticulous literature review and assembled a collection of 281 papers pertaining to the intersection of LLMs with security and privacy.We categorized these papers into three distinct groups: those highlighting securitybeneficial applications (i.e., the good), those exploring applications that could potentially exert adverse impacts on security (i.e., the bad), and those focusing on the discussion of security vulnerabilities (alongside potential defense mechanisms) within LLMs (i.e., the ugly).To be more specific:</p>
<p>• The Good ( §4): LLMs have a predominantly positive impact on the security community, as indicated by the most significant number of papers dedicated to enhancing security.Specifically, LLMs have made contributions to both code security and data security and privacy.In the context of code security, LLMs have been used for the whole life cycle of the code (e.g., secure coding, test case generation, vulnerable code detection, malicious code detection, and code fixing).</p>
<p>In data security and privacy, LLMs have been applied to ensure data integrity, data confidentiality, data reliability, and data traceability.Meanwhile, Compared to state-of-the-art methods, most researchers found LLM-based methods to outperform traditional approaches.</p>
<p>• The Bad ( §5): LLMs also have offensive applications against security and privacy.We categorized the attacks into five groups: hardware-level attacks (e.g., side-channel attacks), OS-level attacks (e.g., analyzing information from operating systems), softwarelevel attacks (e.g., creating malware), network-level attacks (e.g., network phishing), and user-level attacks (e.g., misinformation, social engineering, scientific misconduct).User-level attacks, with 32 papers, are the most prevalent due to LLMs' human-like reasoning abilities.Those attacks threaten both security (e.g., malware attacks) and privacy (e.g., social engineering).Nowadays, LLMs lack direct access to OS and hardware-level functions.The potential threats of LLMs could escalate if they gain such access.</p>
<p>• The Ugly ( §6): We explore the vulnerabilities and defenses in LLMs, categorizing vulnerabilities into two main groups: AI Model Inherent Vulnerabilities (e.g., data poisoning, backdoor attacks, training data extraction) and Non-AI Model Inherent Vulnerabilities (e.g., remote code execution, prompt injection, side channels).These attacks pose a dual threat, encompassing both security concerns (e.g., remote code execution attacks) and privacy issues (e.g., data extraction).Defenses for LLMs are divided into strategies placed in the architecture, and applied during the training and inference phases.Training phase defenses involve corpora cleaning, and optimization methods, while inference phase defenses include instruction pre-processing, malicious detection, and generation post-processing.These defenses collectively aim to enhance the security, robustness, and ethical alignment of LLMs.We found that model extraction, parameter extraction, and similar attacks have received limited research attention, remaining primarily theoretical with minimal practical exploration.The vast scale of LLM parameters makes traditional approaches less effective, and the confidentiality of powerful LLMs further shields them from conventional attacks.Strict censorship of LLM outputs challenges even black-box ML attacks.Meanwhile, research on the impact of model architecture on LLM safety is scarce, partly due to high computational costs.Safe instruction tuning, a recent development, requires further investigation.</p>
<p>Contributions.</p>
<p>Our work makes a dual contribution.First, we are pioneers summarizing the role of LLMs in security and privacy.We delve deeply into the positive impacts of LLMs on security, their potential risks and threats, vulnerabilities in LLMs, and the corresponding defense mechanisms.Other surveys may focus on one or two specific aspects, such as beneficial applications, offensive applications, vulnerabilities, or defenses.To the best of our knowledge, our survey is the first to cover all three key aspects related to security and privacy for the first time.Second, we have made several interesting discoveries.For instance, our research reveals that LLMs contribute more positively than negatively to security and privacy.Moreover, we observe that most researchers concur that LLMs outperform stateof-the-art methods when employed for securing code or data.Concurrently, it becomes evident that user-level attacks are the most prevalent, largely owing to the human-like reasoning abilities exhibited by LLMs.</p>
<p>Roadmap.The rest of the paper is organized as follows.We begin with a brief introduction to LLM in §2.§3 presents the overview of our work.In §4, we explore the beneficial impacts of employing LLMs.§5 discusses the negative impacts on security and privacy.In §6, we discuss the prevalent threats, vulnerabilities associated with LLMs as well as the countermeasures to mitigate these risks.§7 discuss LLMs in other security related topics and possible directions.We conclude the paper in §9.</p>
<p>Background</p>
<p>Large Language Models (LLMs)</p>
<p>Large Language Models (LLMs) [347] represents an evolution from language models.Initially, language models were statistical in nature and laid the groundwork for computational linguistics.The advent of transformers has significantly increased their scale.This expansion, along with the use of extensive training corpora and advanced pretraining techniques is pivotal in areas such as AI for science, logical reasoning, and embodied AI.These models undergo extensive training on vast datasets to comprehend and produce text that closely mimics human language.Typically, LLMs are endowed with hundreds of billions, or even more, parameters, honed through the processing of massive textual data.They have spearheaded substantial advancements in the realm of Natural Language Processing (NLP) [82] and find applications in a multitude of fields (e.g., risk assessment [202], programming [26], vulnerability detection [118], medical text analysis [274], and search engine optimization [13]).</p>
<p>Based on Yang's study [323], an LLM should have at least four key features.First, an LLM should demonstrate a deep understanding and interpretation of natural language text, enabling it to extract information and perform various language-related tasks (e.g., translation).Second, it should have the capacity to generate human-like text (e.g., completing sentences, composing paragraphs, and even writing articles) when prompted.Third, LLMs should exhibit contextual awareness by considering factors such as domain expertise, a quality referred to as "Knowledgeintensive". Fourth, these models should excel in problemsolving and decision-making, leveraging information within text passages to make them invaluable for tasks such as information retrieval and question-answering systems.</p>
<p>Comparison of Popular LLMs</p>
<p>As shown in Table 1 [276,235], there is a diversity of providers for language models, including industry leaders such as OpenAI, Google, Meta AI, and emerging players such as Anthropic and Cohere.The release dates span from 2018 to 2023, showcasing the rapid development and evolution of language models in recent years.Newer models such as "gpt-4" have emerged in 2023, highlighting the ongoing innovation in this field.While most of the models are not open-source, it is interesting to note that models like BERT, T5, PaLM, LLaMA, and CTRL are open-source, which can facilitate community-driven development and applications.Larger models tend to have more parameters, potentially indicating increased capabilities but also greater computational demands.For example, "PaLM" stands out with a massive 540 billion parameters.It can also be observed that LLMs tend to have more parameters, potentially indicating increased capabilities but also greater computational demands.The "Tunability" column suggests whether these models can be fine-tuned for specific tasks.In other words, it is possible to take a large, pre-trained language model and adjust its parameters and training on a smaller, domainspecific dataset to make it perform better on a particular task.For instance, with tunability, one can fine-tune BERT on a dataset of movie reviews to make it highly effective at sentiment analysis.</p>
<p>Overview</p>
<p>Scope</p>
<p>Our paper endeavors to conduct a thorough literature review, with the objective of collating and scrutinizing existing research and studies about the realms of security and privacy in the context of LLMs.The effort is geared towards both establishing the current state of the art in this domain and  [24] 2020.06OpenAI ✗ 175B ✗ cohere-medium [170] 2022.07Cohere ✗ 6B ✓ cohere-large [170] 2022.07Cohere ✗ 13B ✓ cohere-xlarge [170] 2022.06Cohere ✗ 52B ✓ BERT [61] 2018.08 Google ✓ 340M ✓ T5 [225] 2019 Google ✓ 11B ✓ PaLM [198] 2022.04Google ✓ 540B ✓ LLaMA [4] 2023.02 Meta AI ✓ 65B ✓ CTRL [229] 2019 Salesforce ✓ 1.6B ✓ Dolly 2.0 [50] 2023.04Databricks ✓ 12B ✓ pinpointing gaps in our collective knowledge.While it is true that LLMs wield multifaceted applications extending beyond security considerations (e.g., social and financial impacts), our primary focus remains steadfastly on matters of security and privacy.Moreover, it is noteworthy that GPT models have attained significant prominence within this landscape.Consequently, when delving into specific content and examples, we aim to employ GPT models as illustrative benchmarks.</p>
<p>The Research Questions</p>
<p>LLMs have carried profound implications across diverse domains.However, it is essential to recognize that, as with any powerful technology, LLMs bear a significant responsibility.Our paper delves deeply into the multifaceted role of LLMs in the context of security and privacy.We intend to scrutinize their positive contributions to these domains, explore the potential threats they may engender, and uncover the vulnerabilities that could compromise their integrity.To accomplish this, our study will conduct a thorough literature review centered around three pivotal research questions:</p>
<p>• The Good ( §4): How do LLMs positively contribute to security and privacy in various domains, and what are the potential benefits they bring to the security community?</p>
<p>• The Bad ( §5): What are the potential risks and threats associated with the use of LLMs in the context of cybersecurity?Specifically, how can LLMs be used for malicious purposes, and what types of cyber attacks can be facilitated or amplified using LLMs?</p>
<p>• The Ugly ( §6): What vulnerabilities and weaknesses exist within LLMs, and how do these vulnerabilities pose a threat to security and privacy?</p>
<p>Motivated by these questions, we conducted a search on Google Scholar and compiled papers related to security and privacy involving LLMs.As shown in Figure 1, we gathered a total of 83 "good" papers that highlight the positive contributions of LLMs to security and privacy.Additionally, we identified 54 "bad" papers, in which attackers exploited LLMs to target users, and 144 "ugly" papers, in which authors discovered vulnerabilities within LLMs.Most of the  papers were published in 2023, with only 82 of them released in between 2007 and 2022.Notably, there is a consistent upward trend in the number of papers released each month, with October reaching its peak, boasting the highest number of papers published (38 papers in total, accounting for 15.97% of all the collected papers).It is conceivable that more security-related LLM papers will be published in the near future.</p>
<p>Finding I.In terms of security-related applications (i.e., the "good" and the "bad" parts), it is evident that the majority of researchers are inclined towards using LLMs to bolster the security community, such as in vulnerability detection and security test generation, despite the presence of some vulnerabilities in LLMs at this stage.There are relatively few researchers who employ LLMs as tools for conducting attacks.In summary, LLMs contribute more positively than negatively to the security community.</p>
<p>Positive Impacts on Security and Privacy</p>
<p>In this section, we explore the beneficial impacts of employing LLMs.In the context of code or data privacy, we have opted to use the term "privacy" to characterize scenarios in which LLMs are utilized to ensure the confidentiality of either code or data.However, given that we did not come across any papers specifically addressing code privacy, our discussion focuses on code security ( §4.1) as well as both data security and privacy ( §4.2).</p>
<p>LLMs for Code Security</p>
<p>As shown in Table 2, LLMs have access to a vast repository of code snippets and examples spanning various programming languages and domains.They leverage their advanced language understanding and contextual analysis capabilities to thoroughly examine code and code-related text.More specifically, LLMs can play a pivotal role throughout the entire code security lifecycle, including coding (C), test case generation (TCG), execution, and monitoring (RE).</p>
<p>Secure Coding (C).We first discuss the use of LLMs in the context of secure code programming [75] (or generation [63,285,199,90]).Sandoval et al. [234] conducted a user study (58 users) to assess the security implications of LLMs, particularly OpenAI Codex, as code assistants for developers.They evaluated code written by student programmers when assisted by LLMs and found that participants assisted by LLMs did not introduce new security risks: the AI-assisted group produced critical security bugs at a rate no greater than 10% higher than the control group (non-assisted).He et al. [98,99] focused on enhancing the security of code generated by LLMs.They proposed a novel method called SVEN, which leverages continuous prompts to control LLMs in generating secure code.With this method, the success rate improved from 59.1% to 92.3% when using the CodeGen LM.Mohammed et al. introduce SALLM [254], a framework consisting of a new security-focused dataset, an evaluation environment, and novel metrics for systematically assessing LLMs' ability to generate secure code.Madhav et al. [197] evaluate the security aspects of code generation processes on the ChatGPT platform, specifically in the hardware domain.They explore the strategies that a designer can employ to enable ChatGPT to provide secure hardware code generation.</p>
<p>Test Case Generating (TCG).Several papers [33,6,238,316,156,253,335] discuss the utilization of LLMs for generating test cases, with our particular emphasis on those addressing security implications.Zhang et al. [343] demonstrated the use of ChatGPT-4.0for generating security tests to assess the impact of vulnerable library dependencies on software applications.They found that LLMs could successfully generate tests that demonstrated various supply chain attacks, outperforming existing security test generators.This approach resulted in 24 successful attacks across 55 applications.Similarly, Libro [136], a framework that uses LLMs to automatically generate test cases to reproduce software security bugs.</p>
<p>In the realm of security, fuzzing stands [325,109,337,345,272]   Zhang et al. [337] explore the generation of fuzz drivers for library API fuzzing using LLMs.Results show that LLMbased generation is practical, with 64% of questions solved entirely automatically and up to 91% with manual validation.CHATAFL [190] is an LLM-guided protocol fuzzer that constructs grammars for message types and mutates messages or predicts the next messages based on LLM interactions, achieving better state and code coverage compared to stateof-the-art fuzzers (e.g., AFLNET [217], NSFUZZ [222]).</p>
<p>Vulnerable Code Detecting (RE).</p>
<p>Noever [201] explores the capability of LLMs, particularly OpenAI's GPT-4, in detecting software vulnerabilities.This paper shows that GPT-4 identified approximately four times the number of vulnerabilities compared to traditional static code analyzers (e.g., Snyk and Fortify).Parallel conclusions have also been drawn in other efforts [141,15].However, Moumita et al. [218] applied LLMs for software vulnerability detection, exposing a noticeable performance gap when compared to conventional static analysis tools.This disparity primarily arises from the relatively higher occurrence of false alerts generated by LLMs.Similarly, Cheshkov et al. [41] point out that the ChatGPT model performed no better than a dummy classifier for both binary and multi-label classification tasks in code vulnerability detection.Efforts in leveraging LLMs for vulnerability detection extend to specialized domains (e.g.,blockchain [110,37], kernel [104] mobile [303]).For instance, Chen et al. [37] and Hu et al. [110] focus on the application of LLMs in identifying vulnerabilities within blockchain smart contracts.Sakaoglu's study introduces KARTAL [233], a pioneering approach that harnesses LLMs for web application vulnerability detection.This method achieves an accuracy ○␣ ○␣ ○ ○ -Watermark More effective REMARK [340] ○␣ ○␣ ○ ○ -Watermark More effective SWEET [154] ○␣ ○␣ ○ ○ -Watermark More effective of up to 87.19% and is capable of conducting 539 predictions per second.Additionally, Chen et al. [38] make a noteworthy contribution with VulLibGen, a generative methodology utilizing LLMs to identify vulnerable libraries.Ahmad et al. [3] shift the focus to hardware security.They investigate the use of LLMs, specifically OpenAI's Codex, in automatically identifying and repairing security-related bugs in hardware designs.PentestGPT [55], an automated penetration testing tool, uses the domain knowledge inherent in LLMs to address individual sub-tasks of penetration testing, improving task completion rates significantly.</p>
<p>Malicious Code Detecting (RE).</p>
<p>Using LLM to detect malware is a promising application.This approach leverages the natural language processing capabilities and contextual understanding of LLMs to identify malicious software.In experiments with GPT-3.5 conducted by Henrik Plate [105], it was found that LLM-based malware detection can complement human reviews but not replace them.Out of 1800 binary classifications performed, there were both false-positives and false-negatives.The use of simple tricks could also deceive the LLM's assessments.More recently, there are a few attempts have been made in this direction.For example, Apiiro [74] is a malicious code analysis tool using LLMs.Apiiro's strategy involves the creation of LLM Code Patterns (LCPs) to represent code in vector format, making it easier to identify similarities and cluster packages efficiently.Its LCP detector incorporates LLMs, proprietary code analysis, probabilistic sampling, LCP indexing, and dimensionality reduction to identify potentially malicious code.</p>
<p>Vulnerable/Buggy Code Fixing (RE).</p>
<p>Several papers [123,211,314] has focused on evaluate the performance of LLMs trained on code in the task of program repair.Jin et al. [125] proposed InferFix, a transformer-based program repair framework that works in tandem with the combination of cutting-edge static analyzer with transformer-based model to address and fix critical security and performance issues with accuracy between 65% to 75%.Pearce et al. [211] observed that LLMs can repair insecure code in a range of contexts even without being explicitly trained on vulnerability repair tasks.</p>
<p>ChatGPT is noted for its ability in code bug detection and correction.Fu et al. [83] assessed ChatGPT in vulnerabilityrelated tasks like predicting and classifying vulnerabilities, severity estimation, and analyzing over 190,000 C/C++ functions.They found that ChatGPT's performance was behind other LLMs specialized in vulnerability detection.However, Sobania et al. [257] found ChatGPT's bug fixing performance competitive with standard program repair methods, as demonstrated by its ability to fix 31 out of 40 bugs.Xia et al. [315] presented ChatRepair, leveraging pre-trained language models (PLMs) for generating patches without dependency on bug-fixing datasets, aiming to enhance performance to generate patches without relying on bug-fixing datasets, aiming to improve ChatGPT's codefixing abilities using a mix of successful and failure tests.As a result, they fixed 162 out of 337 bugs at a cost of $0.42 each.</p>
<p>Finding II.As shown in Table 2, a comparison with stateof-the-art methods reveals that the majority of researchers (17 out of 25) have concluded that LLM-based methods outperform traditional approaches (advantages include higher code coverage, higher detecting accuracy, less cost etc.).Only four papers argue that LLM-based methods do not surpass the state-of-the-art appoarches.The most frequently discussed issue with LLM-based methods is their tendency to produce both high false negatives and false positives when detecting vulnerabilities or bugs.</p>
<p>LLMs for Data Security and Privacy</p>
<p>As demonstrated in Table 3, LLMs make valuable contributions to the realm of data security, offering multifaceted approaches to safeguarding sensitive information.We have organized the research papers into distinct categories based on the specific facets of data protection that LLMs enhance.These facets encompass critical aspects such as data integrity (I), which ensures that data remains uncorrupted throughout its life cycle; data reliability (R), which ensures the accuracy of data; data confidentiality (C), which focuses on guarding against unauthorized access and disclosure of sensitive information; and data traceability (T), which involves tracking and monitoring data access and usage.</p>
<p>Data Integrity (I).</p>
<p>Data Integrity ensures that data remains unchanged and uncorrupted throughout its life cycle.As of now, there are a few works that discuss how to use LLMs to protect data integrity.For example, ransomware usually encrypts a victim's data, making the data inaccessible without a decryption key that is held by the attacker, which breaks the data integrity.Wang Fang's research [294] examines using LLMs for ransomware cybersecurity strategies, mostly theoretically proposing real-time analysis, automated policy generation, predictive analytics, and knowledge transfer.However, these strategies lack empirical validation.Similarly, Liu et al. [187] explored the potential of LLMs for creating cybersecurity policies aimed at mitigating ransomware attacks with data exfiltration.They compared GPT-generated Governance, Risk and Compliance (GRC) policies to those from established security vendors and government cybersecurity agencies.They recommended that companies should incorporate GPT into their GRC policy development.</p>
<p>Anomaly detection is a key defense mechanism that identifies unusual behavior.While it does not directly protect data integrity, it identifies abnormal or suspicious behavior that can potentially compromise data integrity (as well as data confidentiality and data reliability).Amine et al. [73] introduced an LLM-based monitoring framework for detecting semantic anomalies in vision-based policies and applied it to both finite state machine policies for autonomous driving and learned policies for object manipulation.Experimental results demonstrate that it can effectively identify semantic anomalies, aligning with human reasoning.HuntGPT [8] is an LLM-based intrusion detection system for network anomaly detection.The results demonstrate its effectiveness in improving user understanding and interaction.Chris et al. [71] and LogGPT [221] explore ChatGPT's potential for log-based anomaly detection in parallel file systems.Results show that it addresses the issues in traditional manual labeling and interpretability.AnomalyGPT [91] uses Large Vision-Language Models to detect industrial anomalies.It eliminates manual threshold setting and supports multi-turn dialogues.</p>
<p>Data Confidentiality (C).</p>
<p>Data confidentiality refers to the practice of protecting sensitive information from unauthorized access or disclosure, a topic extensively discussed in LLM privacy discussions [214,242,286,1].However, most of these studies concentrate on enhancing LLMs through state-of-the-art Privacy Enhancing Techniques (e.g., zeroknowledge proofs [224], differential privacy (e.g., [242,184,166], and federated learning [145,122,78]).There are only a few attempts that utilize LLMs to enhance user privacy.For example, Arpita et al. [286] use LLMs to preserve privacy by replacing identifying information in textual data with generic markers.Instead of storing sensitive user information, such as names, addresses, or credit card numbers, the LLMs suggest substitutes for the masked tokens.This obfuscation technique helps to protect user data from being exposed to adversaries.By using LLMs to generate substitutes for masked tokens, the models can be trained on obfuscated data without compromising the privacy and security of the original information.Similar ideas have also been explored in other studies [1,262].Hyeokdong et al. [149] explore implementing cryptography with ChatGPT, which ultimately protects data confidentiality.Despite the lack of extensive coding skills or programming knowledge, the authors were able to successfully implement cryptographic algorithms through ChatGPT.This highlights the potential for individuals to utilize ChatGPT for cryptography tasks.</p>
<p>Data Reliability (R).</p>
<p>In our context, data reliability refers to the accuracy of data.It is a measure of how well data can be depended upon to be accurate, and free from errors or bias.Takashi et al. [142] proposed to use ChatGPT for the detection of sites that contain phishing content.Experimental results using GPT-4 show promising performance, with high precision and recall rates.Fredrik et al. [102] assessed the ability of four large language models (GPT, Claude, PaLM, and LLaMA) to detect malicious intent in phishing emails, and found that they were generally effective, even surpassing human detection, although occasionally slightly less accurate.IPSDM [119] is a model fine-tuned from the BERT family to identify phishing and spam emails effectively.IPSDM demonstrates superior performance in classifying emails, both in unbalanced and balanced datasets.</p>
<p>Data Traceability (T).</p>
<p>Data traceability is the capability to track and document the origin, movement, and history of data within a single system or across multiple systems.This concept is particularly vital in fields such as incident management and forensic investigations, where understanding the journey and transformations of events to resolving issues and conducting thorough analyses.LLMs have gained traction in forensic investigations, offering novel approaches for analyzing digital evidence.Scanlon et al. [237] explored how ChatGPT assists in analyzing OS artifacts like logs, files, cloud interactions, executable binaries, and in examining memory dumps to detect suspicious activities or attack patterns.Additionally, Sladić et al. [255] proposed that generative models like ChatGPT can be used to create realistic honeypots to deceive human attackers.</p>
<p>Watermarking involves embedding a distinctive, typically imperceptible or hard-to-identify signal within the outputs of a model.Wang et al. [297] discusses concerns regarding the intellectual property of training data for LLMs and proposed WASA framework to learn the mapping between the texts of different data providers.Zhang et al. [340] developed REMARK-LLM that focused on monitor the utilization of their content and validate their watermark retrieval.This helps protect against malicious uses such as spamming and plagiarism.Furthermore, identifying code produced by LLMs is vital for addressing legal and ethical issues concerning code licensing, plagiarism, and malware creation.Similarly, Li et al. [169] propose the first watermark technique to protect large language model-based code generation APIs from remote imitation attacks.Lee et al. [154] developed SWEET, a tool that implements watermarking specifically on tokens within programming languages.</p>
<p>Finding III.Likewise, it is noticeable that LLMs excel in data protection, surpassing current solutions and requiring fewer manual interventions.Table 2 and Table 3 reveal that ChatGPT is the predominant LLM extensively employed in diverse security applications.Its versatility and effectiveness make it a preferred choice for various security-related tasks, further reinforcing its position as a go-to solution in the field of artificial intelligence and cybersecurity.</p>
<p>Hardware</p>
<p>OS Software Network User</p>
<p>Negative Impacts on Security and Privacy</p>
<p>As shown in Figure 2, we have categorized the attacks into five groups based on their respective positions within the system infrastructure.These categories encompass hardwarelevel attacks, OS-level attacks, software-level attacks, networklevel attacks, and user-level attacks.Additionally, we have quantified the number of associated research papers published for each group, as illustrated in Figure 3.</p>
<p>Hardware-Level Attacks.Hardware attacks typically involve physical access to devices.However, LLMs cannot directly access physical devices.Instead, they can only access information associated with the hardware.Side-channel attack [260,107,189] is one attack that can be powered by the LLMs.Side-channel attacks typically entail the analysis of unintentional information leakage from a physical system or implementation, such as a cryptographic device or software, with the aim of inferring secret information (e.g., keys).</p>
<p>Yaman [319] has explored the application of LLM techniques to develop side-channel analysis methods.The research evaluates the effectiveness of LLM-based approaches in analyzing side-channel information in two hardwarerelated scenarios: AES side-channel analysis and deeplearning accelerator side-channel analysis.Experiments are conducted to determine the success rates of these methods in both situations.</p>
<p>OS-Level Attacks.LLMs operate at a high level of abstraction and primarily engage with text-based input and output.They lack the necessary low-level system access essential for executing OS-level attacks [114,288,128].Nonetheless, they can be utilized for the analysis of information gathered from operating systems, thus potentially aiding in the execution of such attacks.Andreas et al. [94] establish a feedback loop connecting LLM to a vulnerable virtual machine through SSH, allowing LLM to analyze the machine's state, identify vulnerabilities, and propose concrete attack strategies, which are then executed automatically within the virtual machine.More recently, they [95] introduced an automated Linux privilege-escalation benchmark using local virtual machines and an LLM-guided privilege-escalation tool to assess various LLMs and prompt strategies against the benchmark.</p>
<p>Software-Level Attacks.Similar to how they employ LLM to target hardware and operating systems, there are also instances where LLM has been utilized to attack software (e.g., [343,209,212,32]).However, the most prevalent software-level use case involves malicious developers utilizing LLMs to create malware.Mika et al. [17] present a proof-of-concept in which ChatGPT is utilized to distribute malicious software while avoiding detection.Yin et al. [207] investigate the potential misuse of LLM by creating a number of malware programs (e.g., ransomware, worm, keylogger, brute-force malware, Fileless malware).Antonio Monje et al. [194] demonstrate how to trick ChatGPT into quickly generating ransomware.Marcus Botacin [22] explores different coding strategies (e.g., generating entire malware, creating malware functions) and investigates the LLM's capacities to rewrite malware code.The findings reveal that LLM excels in constructing malware using building block descriptions.Meanwhile, LLM can generate multiple versions of the same semantic content (malware variants), with varying detection rates by Virustotal AV (ranging from 4% to 55%).</p>
<p>Network-Level Attacks.LLMs can also be employed for initiating network attacks.A prevalent example of a networklevel attack utilizing LLM is phishing attacks [18,43].Fredrik et al. [102] compared AI-generated phishing emails using GPT-4 with manually designed phishing emails created using the V-Triad, alongside a control group exposed to generic phishing emails.The results showed that personalized phishing emails, whether generated by AI or designed manually, had higher click-through rates compared to generic ones.Tyson et al. [151] investigated how modifying ChatGPT's input can affect the content of the generated emails, making them more convincing.Julian Hazell [97] demonstrated the scalability of spear phishing campaigns by generating realistic and cost-effective phishing messages for over 600 British Members of Parliament using ChatGPT.In another study, Wang et al. [295] discuss how the traditional defenses may fail in the era of LLMs.CAPTCHA challenges, involving distorted letters and digits, struggle to detect chatbots relying on text and voice.However, LLMs may break the challenges, as they can produce high-quality human-like text and mimic human behavior effectively.There is one study that utilizes LLM for deploying fingerprint attacks.Armin et al. [236] employed density-based clustering to cluster HTTP banners and create text-based fingerprints for annotating scanning data.When these fingerprints are compared to an existing database, it becomes possible to identify new IoT devices and server products.</p>
<p>User-Level Attacks.Recent discussions have primarily focused on user-level attacks, as LLM demonstrates its capability to create remarkably convincing but ultimately deceptive content, as well as establish connections between seemingly unrelated pieces of information.This presents opportunities for malicious actors to engage in a range of nefarious activities.Here are a few examples:</p>
<p>• Misinformation.Overreliance on content generated by LLMs without oversight is raising serious concerns regarding the safety of online content [206].Numerous studies have focused on detecting misinformation produced by LLMs.Several study [35,308,324] reveal content generated by LLMs are harder to detect and may use more deceptive styles, potentially causing greater harm.Canyu Chen et al. [35] propose a taxonomy for LLM-generated misinformation and validate methods.Countermeasures and detection methods [308,280,40,267,36,341,19,155,263] have also been developed to address these emerging issues.</p>
<p>• Finding IV.As illustrated in Figure 3, when compared to other attacks, it becomes apparent that user-level attacks are the most prevalent, boasting a significant count of 33 papers.This dominance can be attributed to the fact that LLMs have increasingly human-like reasoning abilities, enabling them to generate human-like conversations and content (e.g., scientific misconduct, social engineering).Presently, LLMs do not possess the same level of access to OS-level or hardware-level functionalities.This observation remains consistent with the attack observed in other levels as well.For instance, at the network level, LLMs can be abused to create phishing websites and bypass CAPTCHA mechanisms.</p>
<p>Vulnerabilities and Defenses in LLMs</p>
<p>In the following section, we embark on an in-depth exploration of the prevalent threats and vulnerabilities associated with LLMs ( §6.1).We will examine the specific risks and challenges that arise in the context of LLMs.In addition to discussing these challenges, we will also delve into the countermeasures and strategies that researchers and practitioners have developed to mitigate these risks ( §6.2). Figure 4 illustrates the relationship between the attacks and defenses.</p>
<p>Vulnerabilities and Threats in LLMs</p>
<p>In this section, we aim to delve into the potential vulnerabilities and attacks that may be directed towards LLMs.</p>
<p>Our examination seeks to categorize these threats into two distinct groups: AI Model Inherent Vulnerabilities and Non-AI Model Inherent Vulnerabilities.</p>
<p>AI Inherent Vulnerabilities and Threats</p>
<p>These are vulnerabilities and threats that stem from the very nature and architecture of LLMs, considering that LLMs are fundamentally AI models themselves.For example, attackers may manipulate the input data to generate incorrect or undesirable outputs from the LLM.</p>
<p>(A1) Adversarial Attacks.Adversarial attacks in machine learning refer to a set of techniques and strategies used to intentionally manipulate or deceive machine learning models.These attacks are typically carried out with malicious intent and aim to exploit vulnerabilities in the model's behavior.We only focus on the most extensively discussed attacks, namely, data poisoning and backdoor attacks.</p>
<p>• Data Poisoning.Data poisoning stands for attackers influencing the training process by injecting malicious data into the training dataset.This can introduce vulnerabilities or biases, compromising the security, effectiveness, or ethical behavior of the resulting models [206].Various study [148,290,289,2,291,239] have demonstrated that pre-trained models are vulnerable to compromise via methods such as using untrusted weights or content, including the insertion of poisoned examples into their datasets.By their inherent nature as pre-trained models, LLMs are susceptible to data poisoning attacks [227,251,245].For example, Alexander et al. [290] showed that even with just 100 poison examples, LLMs can produce consistently negative results or flawed outputs across various tasks.Larger language models are more susceptible to poisoning, and existing defenses like data filtering or model capacity reduction offer only moderate protection while hurting test accuracy.</p>
<p>• Backdoor Attacks.Backdoor attacks involve the malicious manipulation of training data and model processing, creating a vulnerability where attackers can embed a hidden backdoor into the model [322].Both backdoor attacks and data poisoning attacks involve manipulating machine learning models, which can include manipulation of inputs.However, the key distinction is that backdoor attacks specifically focus on introducing hidden triggers into the model to manipulate specific behaviors or responses when the trigger is encountered.LLMs are subject to backdoor attacks [161,331,167].For example, Yao et al. [329] a bidirectional backdoor, which combines trigger mechanisms with prompt tuning.</p>
<p>(A2) Inference Attacks.Inference attacks in the context of machine learning refer to a class of attacks where an adversary tries to gain sensitive information or insights about a machine learning model or its training data by making specific queries or observations to the model.These attacks often exploit unintended information leakage from the responses.</p>
<p>• Attribute Inference Attacks.Attribute inference Attack [208,181,133,258,183,160] is a type of threat where an attacker attempts to deduce sensitive or personal information of individuals or entities by analyzing the behavior or responses of a machine learning models.It works against the LLMs as well.Robin et al. [261] presented the first comprehensive examination of pretrained LLMs' ability to infer personal information from text.Using a dataset of real Reddit profiles, the study demonstrated that current LLMs can accurately infer a variety of personal information (e.g., location, income, sex) with high accuracy.</p>
<p>• Membership Inferences.Membership inference Attack is a specific type of inference attack in the field of data security and privacy that determining whether a data record was part of a model's training dataset, given white-/black-box access to the model and the specific data record [250,68,143,85,84,191,112].A number of research studies have explored the concept of membership inference, each adopting a unique perspective and methodology.These studies have explored various membership inference attacks by analyzing the label [42], determining the threshold [120,28,96], developing a generalized formulation [278], among other methods.Mireshghallah et al. [192] found that fine-tuning the head of the model exhibits greater susceptibility to attacks when compared to fine-tuning smaller adapters.</p>
<p>(A3) Extraction Attacks.Extraction attacks typically refer to attempts by adversaries to extract sensitive information or insights from machine learning models or their associated data.Extraction attacks and inference attacks share similarities but differ in their specific focus and objectives.Extraction attacks aim to acquire specific resources (e.g., model gradient, training data) or confidential information directly.Inference attacks seek to gain knowledge or insights about the model or data's characteristics, often by observing the model's responses or behavior.Various types of data extraction attacks exist, including model theft attacks [130,137], gradient leakage [158], and training data extraction attacks [29].As of the current writing, it has been observed that training data extraction attacks may be effective against LLMs.Training data extraction [29] refers to a method where an attacker attempts to retrieve specific individual examples from a model's training data by strategically querying the machine learning models.Numerous research [344,210,326] studies have shown that it is possible to extract training data from LLMs, which may include personal and private information [113,339].Notably, the work by Truong et al. [279] stands out for its ability to replicate the model without accessing the original model data.</p>
<p>(A4) Bias and Unfairness Exploitation.Bias and unfairness in LLMs pertain to the phenomenon where these models demonstrate prejudiced outcomes or discriminatory behaviors.While bias and fairness issues are not unique to LLMs, they have received more attention due to the ethical and societal concerns.That is, the societal impact of LLMs has prompted discussions about the ethical responsibilities of organizations and researchers developing and deploying these models.This has led to increased scrutiny and research on bias and fairness.Concerns of bias were raised from various fields, encompassing gender and minority groups [65,144,81,244], the identification of misinformation, political aspects.Multiple studies [269,281] revealed biases in the language used while querying LLMs.Moreover, Urman et al. [282] discovered that biases may arise from adherence to government censorship guidelines.Bias in professional writing [292,263,79] involving LLMs is also a concern within the community, as it can significantly damage credibility.</p>
<p>The biases of LLMs may also lead to negative side effects in areas beyond text-based applications.Dai et al. [47] noted that content generated by LLMs might introduce biases in neural retrieval systems, and Huang et al. [111] discovered that biases could also be present in LLM generated code.</p>
<p>(A5) Instruction Tuning Attacks.Instruction tuning, also known as instruction-based fine-tuning, is a machine-learning technique used to train and adapt language models for specific tasks by providing explicit instructions or examples during the fine-tuning process.In LLMs, instruction-tuning attacks refer to a class of attacks or manipulations that target instruction-tuned LLMs.These attacks are aimed at exploiting vulnerabilities or limitations in LLMs that have been fine-tuned with specific instructions or examples for particular tasks.</p>
<p>• Jailbreaking.Jailbreaking in LLMs involves bypassing security features to enable responses to otherwise restricted or unsafe questions, unlocking capabilities usually limited by safety protocols.Numerous studies have demonstrated various methods for successfully jailbreaking LLMs [159,271,248].Wei et al. [301] emphasized that the alignment capabilities of LLMs can be influenced or manipulated through in-context demonstrations.In addition to this, several researches [300,132] also demonstrated similar manipulation using various approaches, highlighting the versatility of methods that can jailbreaking LLMs.More recently, MASTERKEY [54] employed a timebased method for dissecting defenses, and demonstrated proof-of-concept attacks.It automatically generates jailbreak prompts with a 21.58Moreover, diverse methods have been employed in jailbreaking LLMs, such as conducting fuzzing [328], implementing optimized search strategies [353], and even training LLMs specifically to jailbreak other LLMs [53,353].Meanwhile, Cao et al. [27] developed RA-LLM, a method to lowers the success rate of adversarial and jailbreaking prompts without needing of retraining or access to model parameters.</p>
<p>• Prompt Injection.Prompt injection attack describes a method of manipulating the behavior of LLMs to elicit unexpected and potentially harmful responses.This technique involves crafting input prompts in a way that bypasses the model's safeguards or triggers undesirable outputs.A substantial amount of research [177,332,135,299,173,124] has already automated the process of identifying semantic preserving payload in prompt injections with various focus.Facilitated by the capability for fine-tuning, backdoors may be introduced through prompt attacks [12,133,346,243].Moreover, Greshake et al. [89] expressed concerns about the potential for new vulnerabilities arising from LLMs invoking external resources.Other studies have also demonstrated the ability to take advantage of prompt injection attacks, such as unveiling guide prompts [342], virtualizing prompt injection [320], and integrating applications [178].He et al. [100,101] explored a shift towards leveraging LLMs, trained on extensive datasets, for mitigating such attacks.</p>
<p>• Denial of Service.A Denial of Service (DoS) attack is a type of cyber attack that aims to exhaust computational resources, causing latency or rendering resources unavailable.Due to the nature of LLMs require significant amount of resources, attackers use deliberately construct prompts to reduce the availability of models [59].Shumailov et al. [252] proved the possibility of conducting sponge attacks in the field of LLMs, specifically designed to maximize energy consumption and latency (by a factor of 10 to 200).This strategy aims to draw the community's attention to their potential impact on autonomous vehicles, as well as scenarios requiring making decisions in timely manner.</p>
<p>Finding V. Currently, there is limited research on model extraction attacks [68], parameter extraction attacks, or the extraction of other intermediate esults [279].While there are a few mentions of these topics, they tend to remain primarily theoretical (e.g., [172]), with limited practical implementation or empirical exploration.We believe that the sheer scale of parameters in LLMs complicates these traditional approaches, rendering them less effective or even infeasible.Additionally, the most powerful LLMs are privately owned, with their weights, parameters, and other details kept confidential, further shielding them from conventional attack strategies.Strict censorship of outputs generated by these LLMs challenges even blackbox traditional ML attacks, as it limits the attackers' ability to exploit or analyze the model's responses.</p>
<p>Non-AI Inherent Vulnerabilities and Threats</p>
<p>We also need to consider non-AI Inherent Attacks, which encompass external threats and new vulnerabilities (which have not been observed or investigated in traditional AI models) that LLMs might encounter.(A8) Supply Chain Vulnerabilities.Supply Chain Vulnerabilities refer to the risks in the lifecycle of LLM applications that may arise from using vulnerable components or services.These include third-party datasets, pre-trained models, and plugins, any of which can compromise the application's integrity [206].Most research in this field is focused on the security of plugins.An LLM plugin is an extension or add-on module that enhances the capabilities of an LLM.Third-party plug-ins have been developed to expand its functionality, enabling users to perform various tasks, including web searches, text analysis, and code execution.However, some of the concerns raised by security experts [206,25] include the possibility of plug-ins being used to steal chat histories, access personal information, or execute code on users' machines.These vulnerabilities are associated with the use of OAuth in plug-ins, a web standard for data sharing across online accounts.Umar et al. [115] attempted to address this problem by designing a framework.The framework formulates an extensive taxonomy of attacks specific to LLM platforms, taking into account the capabilities of plugins, users, and the LLM platform itself.By considering the relationships between these stakeholders, the framework helps identify potential security, privacy, and safety risks.</p>
<p>Defenses for LLMs</p>
<p>In this section, we examine the range of existing defense methods against various attacks and vulnerabilities associated with LLMs1 .</p>
<p>Defense in Model Architecture</p>
<p>Model architectures determine how knowledge and concepts are stored, organized, and contextually interacted with, which is crucial in the safety of Large Language Models.</p>
<p>There have been a lot of works [165,351,168,333] delved into how model capacities affect the privacy preservation and robustness of LLMs.Li et al. [165] revealed that language models with larger parameter sizes can be trained more effectively in the differential privacy manner using appropriate non-standard hyper-parameters, in comparison to smaller models.Zhu et al. [351] and Li et al. [168] found that LLMs with larger capacities, such as those with more extensive parameter sizes, generally show increased robustness against adversarial attacks.This was also verified in the Out-of-distribution (OOD) robustness scenarios by Yuan et al. [333].Beyond the architecture of LLMs themselves, studies have focused on improving LLM safety by combining them with external modules including knowledge graphs [39] and cognitive architectures (CAs) [150,11].Romero et al. [231] proposed improving AI robustness by incorporating various cognitive architectures into LLMs.Zafar et al. [336] aimed to build trust in AI by enhancing the reasoning abilities of LLMs through knowledge graphs.• Corpora Cleaning.LLMs are shaped by their training corpora, from which they learn behavior, concepts, and data distributions [302].Therefore, the safety of LLMs is crucially influenced by the quality of the training corpora [86,204].However, it has been widely acknowledged that raw corpora collected from the web are full of issues of fairness [14], toxicity [88], privacy [208], truthfulness [171], etc.A lot of efforts have been made to clean raw corpora and create highquality training corpora for LLMs [129,306,152,307,213,277]. In general, these pipelines consist of the following steps: language identification [129,9], detoxification [88,48,180,195], debiasing [188,21,16],</p>
<p>Defenses in LLM Training and Inference</p>
<p>de-identification (personally identifiable information (PII)) [264,284], and deduplication [153,134,106,157].Debiasing and detoxification aimed to remove undesirable content from training corpora.[293] extended this approach to the continuous embedding space, facilitating more practical convergence, as followed by subsequent research [176,350,163].Safety alignments [205], an emerging learning paradigm, guide LLM behavior using well-aligned additional models or human annotations, proving effective for ethical alignment.Efforts to align LLMs with other LLMs [334] and LLMs themselves [268].</p>
<p>In terms of human annotations, Zhou et al. [349] and Shi et al. [249] emphasized the importance of highquality training corpora with carefully curated instructions and outputs for enhancing instruction-following capabilities in LLMs.Bianchi et al. [20] highlighted that the safety of LLMs can be substantially improved by incorporating a limited percentage (e.g., 3%) of safe examples during fine-tuning.</p>
<p>Defense Strategies in LLM Inference.When LLMs are deployed as cloud services, they operate by receiving prompts or instructions from users and generating completed sentences in response.Given this interaction model, the implementation of test-time LLM defense becomes a necessary and critical aspect of ensuring safe and appropriate outputs.Generally, test-time defense encompasses a range of strategies, including the pre-processing of prompts and instructions to filter or modify inputs, the detection of abnormal events that might signal misuse or problematic queries, and the post-processing of generated responses to ensure they adhere to safety and ethical guidelines.Test-time LLM defenses are essential to maintain the integrity and trustworthiness of LLMs in real-time applications.</p>
<p>• Instruction Processing (Pre-Processing).Instruction pre-processing applies transformations over instructions sent by users, in order to destroy potential adversarial contexts or malicious intents.It plays a vital role as it blocks out most malicious usage and prevents LLMs from receiving suspicious instructions.In general, instruction pre-processing methods can be categorized as instruction manipulation [246,230,140,117,318], purification [164], and defensive demonstrations [172,193,301].Jain et al. [117] and Kirchenbauer et al. [140] evaluated multiple baseline preprocessing methods against jailbreaking attacks, including retokenization and paraphrase.Li et al. [164] proposed to purify instructions by first masking the input tokens and then predicting the masked tokens with other LLMs.The predicted tokens will serve as the purified instructions.Wei et al. [301] and Mo et al. [193] demonstrated that inserting predefined defensive demonstrations into instructions effectively defends jailbreaking attacks of LLMs.</p>
<p>• Malicious Detection (In-Processing).Malicious detection provides in-depth examinations of LLM intermediate results, such as neuron activation, regarding the given instructions, which are more sensitive, accurate, and specified for malicious usage.Sun et al. [266] proposed to detect backdoored instructions with backward probabilities of generations.Xi et al. [312] differentiated normal and poisoned instructions from the perspective of mask sensitivities.Shao et al. [246] identified suspicious words according to their textual relevance.Wang et al. [298] detected adversarial examples according to the semantic consistency among multiple generations, which has been explored in the uncertainty quantification of LLMs by Duan et al. [67].Apart from the intrinsic properties of LLMs, there have been works leveraging the linguistic statistic properties, such as detecting outlier words [220],</p>
<p>• Generation Processing (Post-Processing).Generation post processing refers to examining the properties (e.g., harmfulness) of the generated answers and applying modifications if necessary, which is the final step before delivering responses to users.Chen et al. [34] proposed to mitigate the toxicity of generations by comparing with multiple model candidates.Helbling et al. [103] incorporated individual LLMs to identify the harmfulness of the generated answers, which shared similar ideas as Xiong et al. [317] and Kadavath et al. [131] where they revealed that LLMs can be prompted to answer the confidences regarding the generated responses.</p>
<p>Finding VI.For defense in LLM training, there's a notable scarcity of research examining the impact of model architecture on LLM safety, which is likely due to the high computational costs associated with training or finetuning large language models.We observed that safe instruction tuning is a relatively new development that warrants further investigation and attention.</p>
<p>Discussion</p>
<p>LLM in Other Security Related Topics</p>
<p>LLMs in Cybersecurity Education.LLMs can be used in security practices and education [80,162,270].For example, in a software security course, students are tasked with identifying and resolving vulnerabilities in a web application using LLMs.Jingyue et al. [162] investigated how ChatGPT can be used by students for these exercises.Wesley Tann et al. [270] focused on the evaluation of LLMs in the context of cybersecurity Capture-The-Flag (CTF) exercises (participants find "flags" by exploiting system vulnerabilities).The study first assessed the question-answering performance of these LLMs on Cisco certifications with varying difficulty levels, then examined their abilities in solving CTF challenges.Jin et al. [126] conducted a comprehensive study on LLMs' understanding of binary code semantics [127] across different architectures and optimization levels, providing key insights for future research in this area.</p>
<p>LLMs in Cybersecurity Laws, Policies and Compliance.</p>
<p>LLMs can assist in drafting security policies, guidelines, and compliance documentation, ensuring that organizations meet regulatory requirements and industry standards.However, it's important to recognize that the utilization of LLMs can potentially necessitate changes to current cybersecurityrelated laws and policies.The introduction of LLMs may raise new legal and regulatory considerations, as these models can impact various aspects of cybersecurity, data protection, and privacy.Ekenobi et al. [273] examined the legal implications arising from the introduction of LLMs, with a particular focus on data protection and privacy concerns.It acknowledges that ChatGPT's privacy policy contains commendable provisions for safeguarding user data against potential threats.The paper also advocated for emphasizing the relevance of the new law.</p>
<p>Future Directions</p>
<p>We have gleaned valuable lessons that we believe can shape future directions.</p>
<p>• Using LLMs for ML-Specific Tasks.We noticed that LLMs can effectively replace traditional machine learning methods and in this context, if traditional machine learning methods can be employed in a specific security application (whether offensive or defensive in nature), it is highly probable that LLMs can also be applied to address that particular challenge.For instance, traditional machine learning methods have found utility in malware detection, and LLMs can similarly be harnessed for this purpose.Therefore, one promising avenue is to harness the potential of LLMs in security applications where machine learning serves as a foundational or widely adopted technique.As security researchers, we are capable of designing LLM-based approaches to tackle security issues.Subsequently, we can compare these approaches with state-of-the-art methods to push the boundaries.</p>
<p>• Replacing Human Efforts.It is evident that LLMs have the potential to replace human efforts in both offensive and defensive security applications.For instance, tasks involving social engineering, traditionally reliant on human intervention, can now be effectively executed using LLM techniques.Therefore, one promising avenue for security researchers is to identify areas within traditional security tasks where human involvement has been pivotal and explore opportunities to substitute these human efforts with LLM capabilities.</p>
<p>• Modifying Traditional ML Attacks for LLMs.we have observed that many security vulnerabilities in LLMs are extensions of vulnerabilities found in traditional machine-learning scenarios.That is, LLMs remain a specialized instance of deep neural networks, inheriting common vulnerabilities such as adversarial attacks and instruction tuning attacks.With the right adjustments (e.g., the threat model), traditional ML attacks can still be effective against LLMs.For instance, the jailbreaking attack is a specific form of instruction tuning attack aimed at producing restricted texts.</p>
<p>• Adapting Traditional ML Defenses for LLMs.The countermeasures traditionally employed for vulnerability mitigation can also be leveraged to address these security issues.For example, there are existing efforts that utilize traditional Privacy-Enhancing Technologies (e.g., zero-knowledge proofs, differential privacy, and federated learning [304,305] ) to tackle privacy challenges posed by LLMs.Exploring additional PETs techniques, whether they are established methods or innovative approaches, to address these challenges represents another promising research direction.</p>
<p>• Solving Challenges in LLM-Specific Attacks.As previously discussed, there are several challenges associated with implementing model extraction or parameter extraction attacks (e.g., vast scale of LLM parameters, private ownership and confidentiality of powerful LLMs).These novel characteristics introduced by LLMs represent a significant shift in the landscape, potentially leading to new challenges and necessitating the evolution of traditional ML attack methodologies.</p>
<p>Related Work</p>
<p>There have already been a number of LLM surveys released with a variety of focuses (e.g., LLM evolution and taxonomy [31,347,309,93,311,23,348], software engineering [77,108], and medicine [274,44]).In this paper, our primary emphasis is on the security and privacy aspects of LLMs.We now delve into an examination of the existing literature pertaining to this particular topic.Peter J.</p>
<p>Caven [30] specifically explores how LLMs (particularly, ChatGPT) could potentially alter the current cybersecurity landscape by blending technical and social aspects.Their emphasis leans more towards the social aspects.Muna et al. [5] and Marshall et al. [185] discussed the impact of ChatGPT in cybersecurity, highlighting its practical applications (e.g., code security, malware detection).Dhoni et al. [62] demonstrated how LLMs can assist security analysts in developing security solutions against cyber threats.However, their work does not extensively address the potential cybersecurity threats that LLM may introduce.A number of surveys (e.g., [92,59,247,49,60,228,240,241,7]) highlight the threats and attacks against LLMs.In comparison to our work, they do not dedicate as much text to the vulnerabilities that the LLM may possess.Instead, their primary focus lies in the realm of security applications, as they delve into utilizing LLMs for launching cyberattacks.Attia Qammar et al. [219] and Maximilian et al. [196] discussed vulnerabilities exploited by cybercriminals, with a specific focus on the risks associated with LLMs.Their works emphasized the need for strategies and measures to mitigate these threats and vulnerabilities.Haoran Li et al. [166] analyzed current privacy concerns on LLMs, categorizing them based on adversary capabilities, and explored existing defense strategies.Glorin Sebastian [242] explored the application of established Privacy-Enhancing Technologies (e.g., differential privacy [70], federated learning [338], and data minimization [216]) for safeguarding the privacy of LLMs.Smith et al. [256] also discussed the privacy risks of LLMs.Our study comprehensively examined both the security and privacy aspects of LLMs.In summary, our research conducted an extensive review of the literature on LLMs from a three-fold perspective: beneficial security applications (e.g., vulnerability detection, secure code generation), adverse implications (e.g., phishing attacks, social engineering), and vulnerabilities (e.g., jailbreaking attacks, prompt attacks), along with their corresponding defensive measures.</p>
<p>Conclusion</p>
<p>Our work represents a pioneering effort in systematically examining the multifaceted role of LLMs in security and privacy.On the positive side, LLMs have significantly contributed to enhancing code and data security, while their versatile nature also opens the door to malicious applications.We also delved into the inherent vulnerabilities within these models, and discussed defense mechanisms.We have illuminated the path forward for harnessing the positive aspects of LLMs while mitigating their potential risks.As LLMs continue to evolve and find their place in an everexpanding array of applications, it is imperative that we remain vigilant in addressing security and privacy concerns, ensuring that these powerful models contribute positively to the digital landscape.</p>
<p>Figure 1 :
1
Figure 1: An overview of our collected papers.</p>
<p>FilelessFigure 2 :
2
Figure 2: Taxonomy of Cyberattacks.The colored boxes represent attacks that have been demonstrated to be executable using LLMs, whereas the gray boxes indicate attacks that cannot be executed with LLMs.</p>
<p>Figure 3 :
3
Figure 3: Prevalence of the existing attacks</p>
<p>Figure 4 :
4
Figure 4: Taxonomy of Threats and the Defenses.The line represents a defense technique that can defend against either a specific attack or a group of attacks.</p>
<p>Table 1
1
Comparison of Popular LLMs
ModelDateProvider Open-Source Params Tunabilitygpt-4 [64]2023.03 OpenAI✗1.7T✗gpt-3.5-turbo2021.09 OpenAI✗175B✗gpt-3</p>
<p>Good: 83 Bad: 54 Ugly: 144
M ayr ApFebgA uJu lJu nMarJanD e cN o vc tOctOpS epeSNo vgAuJulDecJunMayJa n F e b r aMnJaF e b ar MrApA p r y M a n JuD ecJu l g AuSepOctNo v</p>
<p>Table 2
2
LLMs for Code Security and Privacy
Life CycleWorkCoding (C)Test Case Generating (TCG)Running and Executing (RE) Detecting Bug Malicious Code Detecting Vulnerability DetectingFixingLLM(s)DomainWhen compared to SOTA ways?Sandoval et al. [234]○○␣○␣○␣○␣○␣Codex-Negligible risksSVEN [98]○○␣○␣○␣○␣○␣CodeGen-More faster/secureSALLM [254]○○␣○␣○␣○␣○␣ ChatGPT etc.--Madhav et al. [197]○○␣○␣○␣○␣○␣ChatGPTHardware-Zhang et al. [343]○␣○○␣○␣○○␣ChatGPT Supply chainMore valid casesLibro [136]○␣○○␣○␣○○␣LLaMA-Higher FP/FNTitanFuzz [56]○␣○○○␣○○␣CodexDL libsHigher coverageFuzzGPT [57]○␣○○○␣○○␣ChatGPTDL libsHigher coverageFuzz4All [313]○␣○○○␣○○␣ChatGPTLanguagesHigher coverageWhiteFox [321]○␣○○○␣○○␣GPT4CompilerHigh-quality testsZhang et al. [337]○␣○○○␣○○␣ChatGPTAPI-CHATAFL [190]○␣○○○␣○○␣ChatGPTProtocolHigher coverageHenrik [105]○␣○␣○␣○○␣○␣ChatGPT-Higer FP/FNApiiro [74]○␣○␣○␣○○␣○␣N/A--Noever [201]○␣○␣○␣○␣○○ChatGPT-4X fasterBakhshandeh et al. [15]○␣○␣○␣○␣○○␣ChatGPT-Low FP/FNMoumita et al. [218]○␣○␣○␣○␣○○␣ChatGPT-Higher FP/FNCheshkov et al. [41]○␣○␣○␣○␣○○␣ChatGPT-No betterLATTE [174]○␣○␣○␣○␣○○␣GPT-Cost effectiveDefectHunter [296]○␣○␣○␣○␣○○␣Codex--Chen et al. [37]○␣○␣○␣○␣○○␣ChatGPTBlockchain-Hu et al. [110]○␣○␣○␣○␣○○␣ChatGPTBlockchain-KARTAL [233]○␣○␣○␣○␣○○␣ChatGPTWeb appsLess manualVulLibGen [38]○␣○␣○␣○␣○○␣LLaMaLibsHigher accuracy/speedAhmad et al. [3]○␣○␣○␣○␣○○CodexHardwareFix more bugsInferFix [125]○␣○␣○○␣○○Codex-CI PipelinePearce et al. [211]○␣○␣○○␣○○␣Codex etc.-Zero-shotFu et al. [83]○␣○␣○○␣○○ChatGPTAPRHigher accuracySobania et al. [257]○␣○␣○␣○␣○␣○ ChatGPT etc.APRHigher accuracyJiang et al. [123]○␣○␣○␣○␣○␣○ChatGPTAPRHigher accuracy
[321]Fuzz leverages LLMs' ability to generate ordinary code, FuzzGPT addresses the need for edge-case testing by priming LLMs with historical bug-triggering program.Fuzz4All[313]leverages LLMs as input generators and mutation engines, creating diverse and realistic inputs for various languages (e.g., C, C++), improving the previous stateof-the-art coverage by 36.8% on average.WhiteFox[321], a novel white-box compiler fuzzer that utilizes LLMs to test compiler optimizations, outperforms existing fuzzers (it generates high-quality tests for intricate optimizations, surpassing state-of-the-art fuzzers by up to 80 optimizations).</p>
<p>Table 3
3
LLMs for Data Security and Privacy
WorkProp.ModelDomainCompared to SOTA ways?I C R TFang [294]○ ○␣ ○ ○␣ ChatGPT Ransomware-Liu et al. [187]○ ○␣ ○ ○␣ ChatGPT Ransomware-Amine et al. [73] ○ ○ ○ ○␣ ChatGPTSemanticAligned w/ SOTAHuntGPT [8]○ ○ ○ ○␣ ChatGPTNetworkMore effectiveChris et al. [71]○ ○ ○ ○␣ ChatGPTLogLess manualAnomalyGPT [91] ○ ○ ○ ○␣ ChatGPTVideoLess manualLogGPT [221]○ ○ ○ ○␣ ChatGPTLogLess manualArpita et al. [286] ○␣ ○ ○␣ ○␣ BERT etc.--Takashi et al. [142] ○␣ ○␣ ○ ○␣ ChatGPTPhishingHigh precisionFredrik et al. [102] ○␣ ○␣ ○ ○␣ ChatGPT etc PhishingEffectiveIPSDM [119]○␣ ○␣ ○ ○␣BERTPhishing-Kwon et al. [149] ○␣ ○ ○␣ ○␣ ChatGPT-Non-exp friendlyScanlon et al. [237] ○␣ ○␣ ○␣ ○ ChatGPTForensicMore effectiveSladić et al. [255] ○␣ ○␣ ○␣ ○ ChatGPTHoneypotMore realisticWASA [297]</p>
<p>These attacks may not be intricately linked to the internal mechanisms of the AI model, yet they can present significant risks.Illustrative instances of non-AI Inherent Attacks involve system-level vulnerabilities (e.g., remote code execution).
(A6) Remote Code Execution (RCE). RCE attacks typ-ically target vulnerabilities in software applications, webservices, or servers to execute arbitrary code remotely. WhileRCE attacks are not typically applicable directly to LLMs,if an LLM is integrated into a web service (e.g.,https://chat.openai.com/) and if there are RCE vulnerabilitiesin the underlying infrastructure or code of that service, itcould potentially lead to the compromise of the LLM'senvironment. Tong et al. [175] identified 13 vulnerabilitiesin six frameworks, including 12 RCE vulnerabilities and 1arbitrary file read/write vulnerability. Additionally, 17 outof 51 tested apps were found to have vulnerabilities, with16 being vulnerable to RCE and 1 to SQL injection. Thesevulnerabilities allow attackers to execute arbitrary code onapp servers through prompt injections.(A7) Side Channel. While LLMs themselves do not typi-cally leak information through traditional side channels suchas power consumption or electromagnetic radiation, they canbe vulnerable to certain side-channel attacks in practicaldeployment scenarios. For example, Edoardo et al. [51]introduce privacy side channel attacks, which are attacksthat exploit system-level components (e.g., data filtering,output monitoring) to extract private information at a muchhigher rate than what standalone models can achieve. Fourcategories of side channels covering the entire ML lifecycleare proposed, enabling enhanced membership inference at-tacks and novel threats (e.g., extracting users' test queries).For instance, the research demonstrates how deduplicatingtraining data before applying differentially-private trainingcreates a side channel that compromises privacy guarantees.
Please be aware that we will not delve into solutions for non-AI inherent vulnerabilities as they tend to be highly specific to individual cases.
AcknowledgementWe thank the anonymous reviewers and Xin Jin from The Ohio State University for their invaluable feedback.This research was supported partly by the NSF award FMitF-2319242.Any opinions, findings, conclusions, or recommendations expressed are those of the authors and not necessarily of the NSF.
Conversational health agents: A personalized llm-powered agent framework. M Abbasian, I Azimi, A M Rahmani, R Jain, 2023</p>
<p>Trojanpuzzle: Covertly poisoning code-suggestion models. H Aghakhani, W Dai, A Manoel, X Fernandes, A Kharkar, C Kruegel, G Vigna, D Evans, B Zorn, R Sim, arXiv:2301.023442023arXiv preprint</p>
<p>Fixing hardware security bugs with large language models. B Ahmad, S Thakur, B Tan, R Karri, H Pearce, 10.48550/arXiv.2302.01215arXiv:2302.012152023arXiv preprint</p>
<p>Introducing llama: A foundational, 65-billionparameter language model. M Ai, feb 2023</p>
<p>Chatgpt for cybersecurity: practical applications, challenges, and future directions. M Al-Hawawreh, A Aljuhani, Y Jararweh, Cluster Computing. 2662023</p>
<p>A3test: Assertion-augmented automated test case generation. S Alagarsamy, C Tantithamthavorn, A Aleti, arXiv:2302.103522023arXiv preprint</p>
<p>Unveiling the dark side of chatgpt: Exploring cyberattacks and enhancing user awareness. M Alawida, B A Shawar, O I Abiodun, A Mehmood, A E Omolara, 2023</p>
<p>Huntgpt: Integrating machine learningbased anomaly detection and explainable ai with large language models (llms). T Ali, P Kostakos, arXiv:2309.160212023arXiv preprint</p>
<p>Language identification: A tutorial. E Ambikairajah, H Li, L Wang, B Yin, V Sethu, IEEE Circuits and Systems Magazine. 1122011</p>
<p>What is fraudgpt. Z Amos, 2023</p>
<p>The atomic components of thought. J R Anderson, C J Lebiere, 2014Psychology Press</p>
<p>On the safety of open-sourced large language models: Does alignment really prevent them from being misused. Anonymous, Submitted to The Twelfth International Conference on Learning Representations, 2023, under review. </p>
<p>Is it a platform? is it a search engine? it's chatgpt! the european liability regime for large language models. B B Arcila, J. Free Speech L. 34552023</p>
<p>Based on billions of words on the internet, people= men. A H Bailey, A Williams, A Cimpian, Science Advances. 81324632022</p>
<p>Using chatgpt as a static application security testing tool. A Bakhshandeh, A Keramatfar, A Norouzi, M M Chekidehkhoun, arXiv:2308.144342023arXiv preprint</p>
<p>Redditbias: A realworld resource for bias evaluation and debiasing of conversational language models. S Barikeri, A Lauscher, I Vulić, G Glavaš, arXiv:2106.035212021arXiv preprint</p>
<p>Ratgpt: Turning online llms into proxies for malware attacks. M Beckerich, L Plein, S Coronado, 2023</p>
<p>Opwnai: Ai that can save the day or hack it away. check point research. S Ben-Moshe, G Gekker, G Cohen, 2022. 2023</p>
<p>Truth and regret: Large language models, the quran, and misinformation. A.-R Bhojani, M Schwarting, 2023</p>
<p>Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions. F Bianchi, M Suzgun, G Attanasio, P Röttger, D Jurafsky, T Hashimoto, J Zou, arXiv:2309.078752023arXiv preprint</p>
<p>Identifying and reducing gender bias in word-level language models. S Bordia, S R Bowman, arXiv:1904.030352019arXiv preprint</p>
<p>Gpthreats-3: Is automatic malware generation a threat?. M Botacin, 2023 IEEE Security and Privacy Workshops (SPW). </p>
<p>Eight things to know about large language models. S R Bowman, arXiv:2304.006122023arXiv preprint</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, 2020</p>
<p>Chatgpt has a plug-in problem. M Burgess, 2023</p>
<p>Low-code llm: Visual programming over llms. Y Cai, S Mao, W Wu, Z Wang, Y Liang, T Ge, C Wu, W You, T Song, Y Xia, arXiv:2304.081032023arXiv preprint</p>
<p>Defending against alignmentbreaking attacks via robustly aligned llm. B Cao, Y Cao, L Lin, J Chen, 2023</p>
<p>Membership inference attacks from first principles. N Carlini, S Chien, M Nasr, S Song, A Terzis, F Tramer, 2022 IEEE Symposium on Security and Privacy (SP). IEEE2022</p>
<p>Extracting training data from large language models. N Carlini, F Tramer, E Wallace, M Jagielski, A Herbert-Voss, K Lee, A Roberts, T Brown, D Song, U Erlingsson, 30th USENIX Security Symposium (USENIX Security 21. 2021</p>
<p>A more insecure ecosystem? chatgpt's influence on cybersecurity. P Caven, ChatGPT's Influence on Cybersecurity. 2023April 30, 2023</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, K Zhu, H Chen, L Yang, X Yi, C Wang, Y Wang, arXiv:2307.031092023arXiv preprint</p>
<p>From text to mitre techniques: Exploring the malicious use of large language models for generating cyber attack payloads. P V S Charan, H Chunduri, P M Anand, S K Shukla, 2023</p>
<p>Codet: Code generation with generated tests. B Chen, F Zhang, A Nguyen, D Zan, Z Lin, J.-G Lou, W Chen, arXiv:2207.103972022arXiv preprint</p>
<p>Jailbreaker in jail: Moving target defense for large language models. B Chen, A Paliwal, Q Yan, arXiv:2310.024172023arXiv preprint</p>
<p>Can llm-generated misinformation be detected?. C Chen, K Shu, 2023</p>
<p>arXiv:2311.05656Combating misinformation in the age of llms: Opportunities and challenges. 2023arXiv preprint</p>
<p>When chatgpt meets smart contract vulnerability detection: How far are we?. C Chen, J Su, J Chen, Y Wang, T Bi, Y Wang, X Lin, T Chen, Z Zheng, 10.48550/arXiv.2309.05520arXiv:2309.055202023arXiv preprint</p>
<p>Vullibgen: Identifying vulnerable third-party libraries via generative pre-trained model. T Chen, L Li, L Zhu, Z Li, G Liang, D Li, Q Wang, T Xie, 10.48550/arXiv.2308.04662arXiv:2308.046622023arXiv preprint</p>
<p>A review: Knowledge reasoning over knowledge graph. X Chen, S Jia, Y Xiang, Expert Systems with Applications. 1411129482020</p>
<p>Can large language models provide security &amp; privacy advice? measuring the ability of llms to refute misconceptions. Y Chen, A Arunasalam, Z B Celik, 2023</p>
<p>Evaluation of chatgpt model for vulnerability detection. A Cheshkov, P Zadorozhny, R Levichev, 10.48550/arXiv.2304.07232arXiv:2304.072322023arXiv preprint</p>
<p>Label-only membership inference attacks. C A Choquette-Choo, F Tramer, N Carlini, N Papernot, International conference on machine learning. PMLR2021</p>
<p>Chatgpt: The curious case of attack vectors' supply chain management improvement. M Chowdhury, N Rifat, S Latif, M Ahsan, M S Rahman, R Gomes, 2023 IEEE International Conference on Electro Information Technology (eIT). 2023</p>
<p>The future landscape of large language models in medicine. J Clusmann, F R Kolbinger, H S Muti, Z I Carrero, J.-N Eckardt, N G Laleh, C M L Löffler, S.-C Schwarzkopf, M Unger, G P Veldhuizen, Communications Medicine. 311412023</p>
<p>Chatting and cheating: Ensuring academic integrity in the era of chatgpt. D R Cotton, P A Cotton, J R Shipway, Innovations in Education and Teaching International. 2023</p>
<p>Academic integrity and artificial intelligence: is chatgpt hype, hero or heresy?. G M Currie, Seminars in Nuclear Medicine. 2023Elsevier</p>
<p>Llms may dominate information access: Neural retrievers are biased towards llm-generated texts. S Dai, Y Zhou, L Pang, W Liu, X Hu, Y Liu, X Zhang, J Xu, arXiv:2310.205012023arXiv preprint</p>
<p>Text detoxification using large pretrained neural models. D Dale, A Voronov, D Dementieva, V Logacheva, O Kozlova, N Semenov, A Panchenko, arXiv:2109.089142021arXiv preprint</p>
<p>Are chatgpt and deepfake algorithms endangering the cybersecurity industry? a review. B Dash, P Sharma, International Journal of Engineering and Applied Sciences. 1012023</p>
<p>Free dolly: Introducing the world's first open and commercially viable instruction-tuned llm. Databricks, 2023</p>
<p>Privacy side channels in machine learning systems. E Debenedetti, G Severi, N Carlini, C A Choquette-Choo, M Jagielski, M Nasr, E Wallace, F Tramèr, arXiv:2309.056102023arXiv preprint</p>
<p>Wormgpt -the generative ai tool cybercriminals are using to launch business email compromise attacks. D Delley, 2023</p>
<p>Jailbreaker: Automated jailbreak across multiple large language model chatbots. G Deng, Y Liu, Y Li, K Wang, Y Zhang, Z Li, H Wang, T Zhang, Y Liu, arXiv:2307.087152023arXiv preprint</p>
<p>Masterkey: Automated jailbreaking of large language model chatbots. Proceedings of the 31th Annual Network and Distributed System Security Symposium (NDSS'24). the 31th Annual Network and Distributed System Security Symposium (NDSS'24)2024</p>
<p>Pentestgpt: An llm-empowered automatic penetration testing tool. G Deng, Y Liu, V Mayoral-Vilches, P Liu, Y Li, Y Xu, T Zhang, Y Liu, M Pinzger, S Rass, arXiv:2308.067822023arXiv preprint</p>
<p>Fuzzing deep-learning libraries via large language models. Y Deng, C S Xia, H Peng, C Yang, L Zhang, arXiv:2212.148342022arXiv preprint</p>
<p>Large language models are edge-case fuzzers: Testing deep learning libraries via fuzzgpt. Y Deng, C S Xia, C Yang, S D Zhang, S Yang, L Zhang, arXiv:2304.020142023arXiv preprint</p>
<p>Large language models are edge-case generators: Crafting unusual programs for fuzzing deep learning libraries. 2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE). 2024</p>
<p>A security risk taxonomy for large language models. E Derner, K Batistič, J Zahálka, R Babuška, arXiv:2311.114152023arXiv preprint</p>
<p>Beyond the safeguards: Exploring the security risks of chatgpt. E Derner, K Batistič, 2023</p>
<p>Bert: Pretraining of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, 2019</p>
<p>Synergizing generative ai and cybersecurity: Roles of generative ai entities, companies, agencies, and government in enhancing cybersecurity. P Dhoni, R Kumar, 2023</p>
<p>A static evaluation of code completion by large language models. H Ding, V Kumar, Y Tian, Z Wang, R Kwiatkowski, X Li, M K Ramanathan, B Ray, P Bhatia, S Sengupta, arXiv:2306.032032023arXiv preprint</p>
<p>Hpc-gpt: Integrating large language model for high-performance computing. X Ding, L Chen, M Emani, C Liao, P.-H Lin, T Vanderbruggen, Z Xie, A Cerpa, W Du, 10.1145/3624062.3624172Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis, ser. SC-W 2023. the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis, ser. SC-W 2023ACMNov. 2023</p>
<p>Probing explicit and implicit gender bias through llm conditional text generation. X Dong, Y Wang, P S Yu, J Caverlee, arXiv:2311.003062023arXiv preprint</p>
<p>How should pretrained language models be fine-tuned towards adversarial robustness?. X Dong, A T Luu, M Lin, S Yan, H Zhang, Advances in Neural Information Processing Systems. 202134</p>
<p>Shifting attention to relevance: Towards the uncertainty estimation of large language models. J Duan, H Cheng, S Wang, C Wang, A Zavalny, R Xu, B Kailkhura, K Xu, arXiv:2307.013792023arXiv preprint</p>
<p>Are diffusion models vulnerable to membership inference attacks. J Duan, F Kong, S Wang, X Shi, K Xu, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>F Duarte, Number of chatgpt users. nov 2023. 2023</p>
<p>Differential privacy. C Dwork, International colloquium on automata, languages, and programming. Springer2006</p>
<p>Early exploration of using chatgpt for log-based anomaly detection on parallel file systems logs. C Egersdoerfer, D Zhang, D Dai, 2023</p>
<p>Chatgpt and the rise of generative ai: threat to academic integrity?. D O Eke, Journal of Responsible Technology. 131000602023</p>
<p>Semantic anomaly detection with large language models. A Elhafsi, R Sinha, C Agia, E Schmerling, I A Nesnas, M Pavone, Autonomous Robots. 2023</p>
<p>Self-enhancing pattern detection with llms: Our answer to uncovering malicious packages at scale. S Eli, D Gil, 2023</p>
<p>I'm sorry dave, i'm afraid i can't fix your code: On chatgpt, cybersecurity, and secure coding. T Espinha Gasiba, K Oguzhan, I Kessba, U Lechner, M Pinto-Albuquerque, 4th International Computer Programming Education Conference (ICPEC 2023). Schloss-Dagstuhl-Leibniz Zentrum für Informatik. 2023</p>
<p>Decoding the threat landscape: Chatgpt, fraudgpt, and wormgpt in social engineering attacks. P V Falade, 10.32628/CSEIT2390533International Journal of Scientific Research in Computer Science, Engineering and Information Technology. Oct. 2023</p>
<p>Large language models for software engineering: Survey and open problems. A Fan, B Gokkaya, M Harman, M Lyubarskiy, S Sengupta, S Yoo, J M Zhang, 2023</p>
<p>Fate-llm: A industrial grade federated learning framework for large language models. T Fan, Y Kang, G Ma, W Chen, W Wei, L Fan, Q Yang, arXiv:2310.100492023arXiv preprint</p>
<p>Bias of ai-generated content: An examination of news produced by large language models. X Fang, S Che, M Mao, H Zhang, M Zhao, X Zhao, arXiv:2309.098252023arXiv preprint</p>
<p>Impersonating chatbots in a code review exercise to teach software engineering best practices. J C Farah, B Spaenlehauer, V Sharma, M J Rodríguez-Triana, S Ingram, D Gillet, 2022 IEEE Global Engineering Education Conference (EDUCON). </p>
<p>Winoqueer: A community-in-the-loop benchmark for anti-lgbtq+ bias in large language models. V K Felkner, H.-C H Chang, E Jang, J May, arXiv:2306.150872023arXiv preprint</p>
<p>A survey of data augmentation approaches for nlp. S Y Feng, V Gangal, J Wei, S Chandar, S Vosoughi, T Mitamura, E Hovy, arXiv:2105.030752021arXiv preprint</p>
<p>Chatgpt for vulnerability detection, classification, and repair: How far are we?. M Fu, C Tantithamthavorn, V Nguyen, T Le, 2023</p>
<p>Practical membership inference attacks against fine-tuned large language models via self-prompt calibration. W Fu, H Wang, C Gao, G Liu, Y Li, T Jiang, 2023</p>
<p>A probabilistic fluctuation based membership inference attack for diffusion models. 2023</p>
<p>On the impact of machine learning randomness on group fairness. P Ganesh, H Chang, M Strobel, R Shokri, Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency. the 2023 ACM Conference on Fairness, Accountability, and Transparency2023</p>
<p>Comparing scientific abstracts generated by chatgpt to original abstracts using an artificial intelligence output detector, plagiarism detector, and blinded human reviewers. C A Gao, F M Howard, N S Markov, E C Dyer, S Ramesh, Y Luo, A T Pearson, BioRxiv. 2022</p>
<p>Realtoxicityprompts: Evaluating neural toxic degeneration in language models. S Gehman, S Gururangan, M Sap, Y Choi, N A Smith, arXiv:2009.114622020arXiv preprint</p>
<p>More than you've asked for: A comprehensive analysis of novel prompt injection threats to application-integrated large language models. K Greshake, S Abdelnabi, S Mishra, C Endres, T Holz, M Fritz, arXiv:2302.121732023arXiv preprint</p>
<p>Llm-based code generation method for golang compiler testing. Q Gu, 2023</p>
<p>Anomalygpt: Detecting industrial anomalies using large vision-language models. Z Gu, B Zhu, G Zhu, Y Chen, M Tang, J Wang, arXiv:2308.153662023arXiv preprint</p>
<p>From chatgpt to threatgpt: Impact of generative ai in cybersecurity and privacy. M Gupta, C Akiri, K Aryal, E Parker, L Praharaj, IEEE Access. 2023</p>
<p>A survey on large language models: Applications, challenges, limitations, and practical usage. M U Hadi, R Qureshi, A Shah, M Irfan, A Zafar, M Shaikh, N Akhtar, J Wu, S Mirjalili, 2023TechRxiv</p>
<p>Getting pwn'd by ai: Penetration testing with large language models. A Happe, J Cito, arXiv:2308.001212023arXiv preprint</p>
<p>Evaluating llms for privilegeescalation scenarios. A Happe, A Kaplan, J Cito, 2023</p>
<p>Logan: Membership inference attacks against generative models. J Hayes, L Melis, G Danezis, E De Cristofaro, arXiv:1705.076632017arXiv preprint</p>
<p>Large language models can be used to effectively scale spear phishing campaigns. J Hazell, 2023</p>
<p>Large language models for code: Security hardening and adversarial testing. J He, M Vechev, ICML 2023 Workshop Deploy-ableGenerativeAI, 2023, keywords: large language models, code generation, security. prompt tuning</p>
<p>Large language models for code: Security hardening and adversarial testing. Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security. the 2023 ACM SIGSAC Conference on Computer and Communications Security2023</p>
<p>You only prompt once: On the capabilities of prompt learning on large language models to tackle toxic content. X He, S Zannettou, Y Shen, Y Zhang, arXiv:2308.055962024 IEEE Symposium on Security and Privacy (SP). 2023. 2024101arXiv preprintYou only prompt once: On the capabilities of prompt learning on large language models to tackle toxic content</p>
<p>Devising and detecting phishing: Large language models vs. smaller human models. F Heiding, B Schneier, A Vishwanath, J Bernstein, 2023</p>
<p>Llm self defense: By self examination, llms know they are being tricked. A Helbling, M Phute, M Hull, D H Chau, arXiv:2308.073082023arXiv preprint</p>
<p>Check for extended abstract: Towards reliable and scalable linux kernel cve attribution in automated static firmware analyses. R Helmke, J Dorp, Detection of Intrusions and Malware, and Vulnerability Assessment: 20th International Conference, DIMVA 2023. Hamburg, GermanySpringer NatureJuly 12-14, 2023. 202313959201</p>
<p>Llm-assisted malware review: Ai and humans join forces to combat malware. P Henrik, 2023</p>
<p>Scaling laws and interpretability of learning from repeated data. D Hernandez, T Brown, T Conerly, N Dassarma, D Drain, S El-Showk, N Elhage, Z Hatfield-Dodds, T Henighan, T Hume, arXiv:2205.104872022arXiv preprint</p>
<p>Applications of machine learning techniques in side-channel attacks: a survey. B Hettwer, S Gehrer, T Güneysu, Journal of Cryptographic Engineering. 102020</p>
<p>Large language models for software engineering: A systematic literature review. X Hou, Y Zhao, Y Liu, Z Yang, K Wang, L Li, X Luo, D Lo, J Grundy, H Wang, arXiv:2308.106202023arXiv preprint</p>
<p>Augmenting greybox fuzzing with generative ai. J Hu, Q Zhang, H Yin, arXiv:2306.067822023arXiv preprint</p>
<p>Large language model-powered smart contract vulnerability detection: New perspectives. S Hu, T Huang, F İlhan, S F Tekin, L Liu, 10.48550/arXiv.2310.01152arXiv:2310.011522023arXiv preprint</p>
<p>Bias assessment and mitigation in llm-based code generation. D Huang, Q Bu, J Zhang, X Xie, J Chen, H Cui, arXiv:2309.143452023arXiv preprint</p>
<p>Damia: leveraging domain adaptation as a defense against membership inference attacks. H Huang, W Luo, G Zeng, J Weng, Y Zhang, A Yang, IEEE Transactions on Dependable and Secure Computing. 1952021</p>
<p>Are large pre-trained language models leaking your personal information. J Huang, H Shao, K C , -C Chang, arXiv:2205.126282022arXiv preprint</p>
<p>Taxonomies of attacks and vulnerabilities in computer systems. V M Igure, R D Williams, IEEE Communications Surveys &amp; Tutorials. 1012008</p>
<p>Llm platform security: Applying a systematic evaluation framework to openai's chatgpt plugins. U Iqbal, T Kohno, F Roesner, 2023</p>
<p>Achieving model robustness through discrete adversarial training. M Ivgi, J Berant, arXiv:2104.050622021arXiv preprint</p>
<p>Baseline defenses for adversarial attacks against aligned language models. N Jain, A Schwarzschild, Y Wen, G Somepalli, J Kirchenbauer, P -Y. Chiang, M Goldblum, A Saha, J Geiping, T Goldstein, arXiv:2309.006142023arXiv preprint</p>
<p>A code centric evaluation of c/c++ vulnerability datasets for deep learning based vulnerability detection techniques. R Jain, N Gervasoni, M Ndhlovu, S Rawat, Proceedings of the 16th Innovations in Software Engineering Conference. the 16th Innovations in Software Engineering Conference2023</p>
<p>An improved transformer-based model for detecting phishing, spam, and ham: A large language model approach. S Jamal, H Wimmer, 2023</p>
<p>Revisiting membership inference under realistic assumptions. B Jayaraman, L Wang, K Knipmeyer, Q Gu, D Evans, arXiv:2005.108812020arXiv preprint</p>
<p>Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization. H Jiang, P He, W Chen, X Liu, J Gao, T Zhao, arXiv:1911.034372019arXiv preprint</p>
<p>Low-parameter federated learning with large language models. J Jiang, X Liu, C Fan, arXiv:2307.138962023arXiv preprint</p>
<p>Impact of code language models on automated program repair. N Jiang, K Liu, T Lutellier, L Tan, 2023</p>
<p>Prompt packer: Deceiving llms through compositional instruction with hidden attacks. S Jiang, X Chen, R Tang, arXiv:2310.100772023arXiv preprint</p>
<p>Inferfix: End-to-end program repair with llms. M Jin, S Shahriar, M Tufano, X Shi, S Lu, N Sundaresan, A Svyatkovskiy, 2023</p>
<p>Binary code summarization: Benchmarking chatgpt/gpt-4 and other large language models. X Jin, J Larson, W Yang, Z Lin, 2023</p>
<p>Symlm: Predicting function names in stripped binaries via context-sensitive executionaware code embeddings. X Jin, K Pei, J Y Won, Z Lin, Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security. the 2022 ACM SIGSAC Conference on Computer and Communications Security2022</p>
<p>A review on taxonomies of attacks and vulnerability in computer and network system. C Joshi, U K Singh, K Tarey, International Journal. 512015</p>
<p>A Joulin, E Grave, P Bojanowski, M Douze, H Jégou, T Mikolov, arXiv:1612.03651Fasttext. zip: Compressing text classification models. 2016arXiv preprint</p>
<p>Prada: protecting against dnn model stealing attacks. M Juuti, S Szyller, S Marchal, N Asokan, 2019 IEEE European Symposium on Security and Privacy (EuroS&amp;P). IEEE2019</p>
<p>Language models (mostly) know what they know. S Kadavath, T Conerly, A Askell, T Henighan, D Drain, E Perez, N Schiefer, Z Hatfield-Dodds, N Dassarma, E Tran-Johnson, arXiv:2207.052212022arXiv preprint</p>
<p>Backdoor attacks for in-context learning with language models. N Kandpal, M Jagielski, F Tramèr, N Carlini, arXiv:2307.146922023arXiv preprint</p>
<p>User inference attacks on large language models. N Kandpal, K Pillutla, A Oprea, P Kairouz, C A Choquette-Choo, Z Xu, 2023</p>
<p>Deduplicating training data mitigates privacy risks in language models. N Kandpal, E Wallace, C Raffel, International Conference on Machine Learning. PMLR202210707</p>
<p>Exploiting programmatic behavior of llms: Dual-use through standard security attacks. D Kang, X Li, I Stoica, C Guestrin, M Zaharia, T Hashimoto, arXiv:2302.057332023arXiv preprint</p>
<p>Llm lies: Hallucinations are not bugs, but features as adversarial examples. S Kang, J Yoon, S Yoo, 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>Maze: Data-free model stealing attack using zeroth-order gradient estimation. S Kariyappa, A Prakash, M K Qureshi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202113823</p>
<p>Large language models effectively leverage document-level context for literary translation, but critical errors persist. M Karpinska, M Iyyer, arXiv:2304.032452023arXiv preprint</p>
<p>Will chatgpt get you caught? rethinking of plagiarism detection. M Khalil, E Er, arXiv:2302.043352023arXiv preprint</p>
<p>On the reliability of watermarks for large language models. J Kirchenbauer, J Geiping, Y Wen, M Shu, K Saifullah, K Kong, K Fernando, A Saha, M Goldblum, T Goldstein, arXiv:2306.046342023arXiv preprint</p>
<p>I used gpt-3 to find 213 security vulnerabilities in a single codebase. C Koch, 2023</p>
<p>Detecting phishing sites using chatgpt. T Koide, N Fukushi, H Nakano, D Chiba, arXiv:2306.058162023arXiv preprint</p>
<p>An efficient membership inference attack for the diffusion model by proximal initialization. F Kong, J Duan, R Ma, H Shen, X Zhu, X Shi, K Xu, arXiv:2305.183552023arXiv preprint</p>
<p>Gender bias and stereotypes in large language models. H Kotek, R Dockum, D Sun, Proceedings of The ACM Collective Intelligence Conference. The ACM Collective Intelligence Conference2023</p>
<p>Federatedscope-llm: A comprehensive package for fine-tuning large language models in federated learning. W Kuang, B Qian, Z Li, D Chen, D Gao, X Pan, Y Xie, Y Li, B Ding, J Zhou, arXiv:2309.003632023arXiv preprint</p>
<p>Demasq: Unmasking the chatgpt wordsmith. K Kumari, A Pegoraro, H Fereidooni, A.-R Sadeghi, arXiv:2311.050192023arXiv preprint</p>
<p>Demasq: Unmasking the chatgpt wordsmith. Proceedings of the 31th Annual Network and Distributed System Security Symposium (NDSS'24). the 31th Annual Network and Distributed System Security Symposium (NDSS'24)2024</p>
<p>Weight poisoning attacks on pre-trained models. K Kurita, P Michel, G Neubig, arXiv:2004.066602020arXiv preprint</p>
<p>Novel approach to cryptography implementation using chatgpt. H Kwon, M Sim, G Song, M Lee, H Seo, Cryptology ePrint Archive. 2023/606, 2023</p>
<p>A standard model of the mind: Toward a common computational framework across artificial intelligence, cognitive science, neuroscience, and robotics. J E Laird, C Lebiere, P S Rosenbloom, Ai Magazine. 3842017</p>
<p>Phishing faster: Implementing chatgpt into phishing campaigns. T Langford, B Payne, Proceedings of the Future Technologies Conference. the Future Technologies ConferenceSpringer2023</p>
<p>The bigscience roots corpus: A 1.6 tb composite multilingual dataset. H Laurençon, L Saulnier, T Wang, C Akiki, A Villanova, T Le Del Moral, L Scao, C Von Werra, E Mou, H González Ponferrada, Nguyen, Advances in Neural Information Processing Systems. 202235</p>
<p>Deduplicating training data makes language models better. K Lee, D Ippolito, A Nystrom, C Zhang, D Eck, C Callison-Burch, N Carlini, arXiv:2107.064992021arXiv preprint</p>
<p>Who wrote this code? watermarking for code generation. T Lee, S Hong, J Ahn, I Hong, H Lee, S Yun, J Shin, G Kim, 2023</p>
<p>Detecting misinformation with llm-predicted credibility signals and weak supervision. J A Leite, O Razuvayevskaya, K Bontcheva, C Scarton, arXiv:2309.076012023arXiv preprint</p>
<p>Codamosa: Escaping coverage plateaus in test generation with pre-trained large language models. C Lemieux, J P Inala, S K Lahiri, S Sen, International conference on software engineering (ICSE). 2023</p>
<p>Mining of massive data sets. J Leskovec, A Rajaraman, J D Ullman, 2020Cambridge university press</p>
<p>A theoretical insight into attack and defense of gradient leakage in transformer. C Li, Z Song, W Wang, C Yang, arXiv:2311.136242023arXiv preprint</p>
<p>Multi-step jailbreaking privacy attacks on chatgpt. H Li, D Guo, W Fan, M Xu, Y Song, arXiv:2304.051972023arXiv preprint</p>
<p>You don't know my favorite color: Preventing dialogue representations from revealing speakers' private personas. H Li, Y Song, L Fan, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. M Carpuat, M.-C De Marneffe, I V Meza Ruiz, the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational LinguisticsJul. 2022</p>
<p>Chatgpt as an attack tool: Stealthy textual backdoor attack via blackbox generative model trigger. J Li, Y Yang, Z Wu, V Vydiswaran, C Xiao, arXiv:2304.144752023arXiv preprint</p>
<p>Evaluating the impact of chatgpt on exercises of a software security course. J Li, P H Meland, J S Notland, A Storhaug, J H Tysse, 2023</p>
<p>Token-aware virtual adversarial training in natural language understanding. L Li, X Qiu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Text adversarial purification as defense against adversarial attacks. L Li, D Song, X Qiu, arXiv:2203.142072022arXiv preprint</p>
<p>Large language models can be strong differentially private learners. X Li, F Tramer, P Liang, T Hashimoto, arXiv:2110.056792021arXiv preprint</p>
<p>Privacy-preserving prompt tuning for large language model services. Y Li, Z Tan, Y Liu, arXiv:2305.062122023arXiv preprint</p>
<p>Multi-target backdoor attacks for code pre-trained models. Y Li, S Liu, K Chen, X Xie, T Zhang, Y Liu, 2023</p>
<p>Evaluating the instructionfollowing robustness of large language models to prompt injection. Z Li, B Peng, P He, X Yan, 2023261048972</p>
<p>Protecting intellectual property of large language model-based code generation apis via watermarks. Z Li, C Wang, S Wang, C Gao, Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security. the 2023 ACM SIGSAC Conference on Computer and Communications Security2023</p>
<p>Holistic evaluation of language models. P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, B Newman, B Yuan, B Yan, C Zhang, C Cosgrove, C D Manning, C Ré, D Acosta-Navas, D A Hudson, E Zelikman, E Durmus, F Ladhak, F Rong, H Ren, H Yao, J Wang, K Santhanam, L Orr, L Zheng, M Yuksekgonul, M Suzgun, N Kim, N Guha, N Chatterji, O Khattab, P Henderson, Q Huang, R Chi, S M Xie, S Santurkar, S Ganguli, T Hashimoto, T Icard, T Zhang, V Chaudhary, W Wang, X Li, Y Mai, Y Zhang, Y Koreeda, 2023</p>
<p>Truthfulqa: Measuring how models mimic human falsehoods. S Lin, J Hilton, O Evans, arXiv:2109.079582021arXiv preprint</p>
<p>Adversarial attacks on large language model-based system and mitigating strategies: A case study on chatgpt. B Liu, B Xiao, X Jiang, S Cen, X He, W Dou, 2023Security and Communication Networks2023</p>
<p>A chinese prompt attack dataset for llms with evil content. C Liu, F Zhao, L Qing, Y Kang, C Sun, K Kuang, F Wu, arXiv:2309.118302023arXiv preprint</p>
<p>Harnessing the power of llm to support binary taint analysis. P Liu, C Sun, Y Zheng, X Feng, C Qin, Y Wang, Z Li, L Sun, 2023</p>
<p>Demystifying rce vulnerabilities in llm-integrated apps. T Liu, Z Deng, G Meng, Y Li, K Chen, 2023</p>
<p>Adversarial training for large neural language models. X Liu, H Cheng, P He, W Chen, Y Wang, H Poon, J Gao, arXiv:2004.089942020arXiv preprint</p>
<p>Autodan: Generating stealthy jailbreak prompts on aligned large language models. X Liu, N Xu, M Chen, C Xiao, arXiv:2310.044512023arXiv preprint</p>
<p>Prompt injection attack against llm-integrated applications. Y Liu, G Deng, Y Li, K Wang, T Zhang, Y Liu, H Wang, Y Zheng, Y Liu, arXiv:2306.054992023arXiv preprint</p>
<p>What is the impact of chatgpt on education? a rapid review of the literature. C K Lo, Education Sciences. 1344102023</p>
<p>Paradetox: Detoxification with parallel data. V Logacheva, D Dementieva, S Ustyantsev, D Moskovskiy, D Dale, I Krotova, N Semenov, A Panchenko, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Differentially private representation for NLP: Formal guarantee and an empirical study on privacy and fairness. L Lyu, X He, Y Li, Findings of the Association for Computational Linguistics: EMNLP 2020. T Cohn, Y He, Y Liu, Association for Computational LinguisticsNov. 2020</p>
<p>Towards deep learning models resistant to adversarial attacks. A Madry, A Makelov, L Schmidt, D Tsipras, A Vladu, arXiv:1706.060832017arXiv preprint</p>
<p>S Mahloujifar, H A Inan, M Chase, E Ghosh, M Hasegawa, Membership inference on word embedding and beyond. 2106.11384, 2021235593386</p>
<p>Differentially private decoding in large language models. J Majmudar, C Dupuy, C Peris, S Smaili, R Gupta, R Zemel, arXiv:2205.136212022arXiv preprint</p>
<p>What effects do large language models have on cybersecurity. J Marshall, 2023</p>
<p>Chatgpt passing usmle shines a spotlight on the flaws of medical education. A B Mbakwe, I Lourentzou, L A Celi, O J Mechanic, A Dagan, 2023e0000205</p>
<p>Harnessing gpt-4 for generation of cybersecurity grc policies: A focus on ransomware attack mitigation. T Mcintosh, T Liu, T Susnjak, H Alavizadeh, A Ng, R Nowrozy, P Watters, Computers &amp; Security. 1341034242023</p>
<p>An empirical survey of the effectiveness of debiasing techniques for pre-trained language models. N Meade, E Poole-Dayan, S Reddy, arXiv:2110.085272021arXiv preprint</p>
<p>Physical side-channel attacks on embedded neural networks: A survey. M Méndez Real, R Salvador, Applied Sciences. 111567902021</p>
<p>Large language model guided protocol fuzzing. R Meng, M Mirchev, M Böhme, A Roychoudhury, Proceedings of the 31th Annual Network and Distributed System Security Symposium (NDSS'24). the 31th Annual Network and Distributed System Security Symposium (NDSS'24)2024</p>
<p>Quantifying privacy risks of masked language models using membership inference attacks. F Mireshghallah, K Goyal, A Uniyal, T Berg-Kirkpatrick, R Shokri, 2022</p>
<p>An empirical analysis of memorization in finetuned autoregressive language models. F Mireshghallah, A Uniyal, T Wang, D Evans, T Berg-Kirkpatrick, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Abu Dhabi, the 2022 Conference on Empirical Methods in Natural Language ProcessingUnited Arab EmiratesAssociation for Computational LinguisticsDec. 2022</p>
<p>W Mo, J Xu, Q Liu, J Wang, J Yan, C Xiao, M Chen, arXiv:2311.09763Testtime backdoor mitigation for black-box large language models with defensive demonstrations. 2023arXiv preprint</p>
<p>Being a bad influence on the kids: Malware generation in less than five minutes using chatgpt. A Monje, A Monje, R A Hallman, G Cybenko, 2023</p>
<p>Exploring cross-lingual text detoxification with large multilingual language models. D Moskovskiy, D Dementieva, A Panchenko, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop. the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop2022</p>
<p>Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities. M Mozes, X He, B Kleinberg, L D Griffin, 2023</p>
<p>Generating secure hardware using chatgpt resistant to cwes. M Nair, R Sadhukhan, D Mukhopadhyay, Cryptology ePrint Archive. 2023/212, 2023</p>
<p>Pathways language model (palm): Scaling to 540 billion parameters for breakthrough performance. S Narang, A Chowdhery, apr 2022</p>
<p>Lever: Learning to verify language-to-code generation with execution. A Ni, S Iyer, D Radev, V Stoyanov, W -T. Yih, S Wang, X V Lin, International Conference on Machine Learning. PMLR2023128</p>
<p>Chatgpt versus engineering education assessment: a multidisciplinary and multiinstitutional benchmarking and analysis of this generative artificial intelligence tool to investigate assessment integrity. S Nikolic, S Daniel, R Haque, M Belkina, G M Hassan, S Grundy, S Lyden, P Neal, C Sandison, 2023European Journal of Engineering Education</p>
<p>Can large language models find and fix vulnerable software. D Noever, arXiv:2308.103452023arXiv preprint</p>
<p>. 10.48550/arXiv.2308.10345</p>
<p>Taking ai risks seriously: a new assessment model for the ai act. C Novelli, F Casolari, A Rotolo, M Taddeo, L Floridi, 2023AI &amp; SOCIETY</p>
<p>Openai, Gpt-4 technical report. 2023</p>
<p>Probing toxic content in large pre-trained language models. N Ousidhoum, X Zhao, T Fang, Y Song, D.-Y Yeung, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>2023, Oct) OWASP Top 10 for LLM. </p>
<p>An attacker's dream? exploring the capabilities of chatgpt for developing malware. Y M Pa Pa, S Tanizaki, T Kou, M Van Eeten, K Yoshioka, T Matsumoto, Proceedings of the 16th Cyber Security Experimentation and Test Workshop. the 16th Cyber Security Experimentation and Test Workshop2023</p>
<p>Privacy risks of generalpurpose language models. X Pan, M Zhang, S Ji, M Yang, 2020 IEEE Symposium on Security and Privacy (SP). IEEE2020</p>
<p>Divas: An llm-based endto-end framework for soc security analysis and policy-based protection. S Paria, A Dasgupta, S Bhunia, arXiv:2308.069322023arXiv preprint</p>
<p>Canary extraction in natural language understanding models. R Parikh, C Dupuy, R Gupta, arXiv:2203.139202022arXiv preprint</p>
<p>Examining zero-shot vulnerability repair with large language models. H Pearce, B Tan, B Ahmad, R Karri, B Dolan-Gavitt, 2023 IEEE Symposium on Security and Privacy (SP). 2023</p>
<p>Pop quiz! can a large language model help with reverse engineering. H Pearce, B Tan, P Krishnamurthy, F Khorrami, R Karri, B Dolan-Gavitt, 2022</p>
<p>The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. G Penedo, Q Malartic, D Hesslow, R Cojocaru, A Cappelli, H Alobeidli, B Pannier, E Almazrouei, J Launay, arXiv:2306.011162023arXiv preprint</p>
<p>Privacy in the time of language models. C Peris, C Dupuy, J Majmudar, R Parikh, S Smaili, R Zemel, R Gupta, Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining. the Sixteenth ACM International Conference on Web Search and Data Mining2023</p>
<p>Academic integrity considerations of ai large language models in the post-pandemic era: Chatgpt and beyond. M Perkins, Journal of University Teaching &amp; Learning Practice. 20272023</p>
<p>A terminology for talking about privacy by data minimization: Anonymity, unlinkability, undetectability, unobservability, pseudonymity, and identity management. A Pfitzmann, M Hansen, 2010</p>
<p>Aflnet: a greybox fuzzer for network protocols. V.-T Pham, M Böhme, A Roychoudhury, 2020 IEEE 13th International Conference on Software Testing, Validation and Verification. ICST</p>
<p>Software vulnerability detection using large language models. M D Purba, A Ghosh, B J Radford, B Chu, 2023 IEEE 34th International Symposium on Software Reliability Engineering Workshops (ISSREW). 2023</p>
<p>Chatbots to chatgpt in a cybersecurity space: Evolution, vulnerabilities, attacks, challenges, and future recommendations. A Qammar, H Wang, J Ding, A Naouri, M Daneshmand, H Ning, 2023</p>
<p>Onion: A simple and effective defense against textual backdoor attacks. F Qi, Y Chen, M Li, Y Yao, Z Liu, M Sun, arXiv:2011.103692020arXiv preprint</p>
<p>Loggpt: Exploring chatgpt for log-based anomaly detection. J Qi, S Huang, Z Luan, C Fung, H Yang, D Qian, arXiv:2309.011892023arXiv preprint</p>
<p>Nsfuzz: Towards efficient and state-aware network service fuzzing. S Qin, F Hu, Z Ma, B Zhao, T Yin, C Zhang, ACM Transactions on Software Engineering and Methodology. 2023</p>
<p>Beyond black box ai-generated plagiarism detection: From sentence to document level. M A Quidwai, C Li, P Dube, arXiv:2306.081222023arXiv preprint</p>
<p>Privacy-preserving large language models (ppllms). M Raeini, 2023Available at SSRN 4512071</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, 2023</p>
<p>Chatgpt for education and research: Opportunities, threats, and strategies. M M Rahman, Y Watanobe, Applied Sciences. 13957832023</p>
<p>Universal jailbreak backdoors from poisoned human feedback. J Rando, F Tramèr, arXiv:2311.144552023arXiv preprint</p>
<p>K Renaud, M Warkentin, G Westerman, From ChatGPT to HackGPT: Meeting the Cybersecurity Threat of Generative AI. MIT Sloan Management Review. 2023</p>
<p>Introducing a conditional transformer language model for controllable generation. S A Research, apr 2023</p>
<p>Smoothllm: Defending large language models against jailbreaking attacks. A Robey, E Wong, H Hassani, G J Pappas, arXiv:2310.036842023arXiv preprint</p>
<p>Synergistic integration of large language models and cognitive architectures for robust ai: An exploratory analysis. O J Romero, J Zimmerman, A Steinfeld, A Tomasic, arXiv:2308.098302023arXiv preprint</p>
<p>The dark side of innovation: Understanding research misconduct with chat gpt in nonformal education studies at universitas negeri surabaya. R J Rosyanafi, G D Lestari, H Susilo, W Nusantara, F Nuraini, Jurnal Review Pendidikan Dasar: Jurnal Kajian Pendidikan dan Hasil Penelitian. 932023</p>
<p>Master's thesis, Master's Programme in Security and Cloud Computing (SECCLO). S Sakaoglu, August 2023Kartal: Web application vulnerability hunting using large language models</p>
<p>Lost at c: A user study on the security implications of large language model code assistants. G Sandoval, H Pearce, T Nys, R Karri, S Garg, B Dolan-Gavitt, USENIX Security 2023. 20231812for associated dataset see [this URL</p>
<p>Llm index. Sapling, 2023</p>
<p>An llm-based framework for fingerprinting internet-connected devices. A Sarabi, T Yin, M Liu, Proceedings of the 2023 ACM on Internet Measurement Conference. the 2023 ACM on Internet Measurement Conference2023</p>
<p>Chatgpt for digital forensic investigation: The good, the bad, and the unknown. M Scanlon, F Breitinger, C Hargreaves, J.-N Hilgert, J Sheppard, Forensic Science International: Digital Investigation. 202346301609</p>
<p>Adaptive test generation using a large language model. M Schäfer, S Nadi, A Eghbali, F Tip, arXiv:2302.065272023arXiv preprint</p>
<p>You autocomplete me: Poisoning vulnerabilities in neural code completion. R Schuster, C Song, E Tromer, V Shmatikov, 30th USENIX Security Symposium (USENIX Security 21. 2021</p>
<p>Adversarial attacks and defenses in large language models: Old and new threats. L Schwinn, D Dobre, S Günnemann, G Gidel, 2023</p>
<p>Do chatgpt and other ai chatbots pose a cybersecurity risk?: An exploratory study. G Sebastian, International Journal of Security and Privacy in Pervasive Computing (IJSPPC). 202315</p>
<p>Privacy and data protection in chatgpt and other ai chatbots: Strategies for securing user information. 2023Available at SSRN 4454761</p>
<p>Loft: Local proxy fine-tuning for improving transferability of adversarial attacks against large language model. M A Shah, R Sharma, H Dhamyal, R Olivier, A Shah, D Alharthi, H T Bukhari, M Baali, S Deshmukh, M Kuhlmann, arXiv:2310.044452023arXiv preprint</p>
<p>On second thought, let's not think step by step! bias and toxicity in zeroshot reasoning. O Shaikh, H Zhang, W Held, M Bernstein, D Yang, arXiv:2212.080612022arXiv preprint</p>
<p>Promptspecific poisoning attacks on text-to-image generative models. S Shan, W Ding, J Passananti, H Zheng, B Y Zhao, arXiv:2310.138282023arXiv preprint</p>
<p>Bddr: An effective defense against textual backdoor attacks. K Shao, J Yang, Y Ai, H Liu, Y Zhang, Computers &amp; Security. 1101024332021</p>
<p>Survey of vulnerabilities in large language models revealed by adversarial attacks. E Shayegani, M A A Mamun, Y Fu, P Zaree, Y Dong, N Abu-Ghazaleh, 2023</p>
<p>do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. X Shen, Z Chen, M Backes, Y Shen, Y Zhang, arXiv:2308.038252023arXiv preprint</p>
<p>Safer-instruct: Aligning language models with automated preference data. T Shi, K Chen, J Zhao, arXiv:2311.086852023arXiv preprint</p>
<p>Membership inference attacks against machine learning models. R Shokri, M Stronati, C Song, V Shmatikov, 2017 IEEE symposium on security and privacy (SP). IEEE2017</p>
<p>On the exploitability of instruction tuning. M Shu, J Wang, C Zhu, J Geiping, C Xiao, T Goldstein, arXiv:2306.171942023arXiv preprint</p>
<p>Sponge examples: Energy-latency attacks on neural networks. I Shumailov, Y Zhao, D Bates, N Papernot, R Mullins, R Anderson, 2021</p>
<p>Exploring the effectiveness of large language models in generating unit tests. M L Siddiq, J Santos, R H Tanvir, N Ulfat, F A Rifat, V C Lopes, arXiv:2305.004182023arXiv preprint</p>
<p>Generate and pray: Using sallms to evaluate the security of llm generated code. M L Siddiq, J C S Santos, 202316 pages</p>
<p>Llm in the shell: Generative honeypots. M Sladić, V Valeros, C Catania, S Garcia, 2023</p>
<p>Identifying and mitigating privacy risks stemming from language models: A survey. V Smith, A S Shamsabadi, C Ashurst, A Weller, 2023</p>
<p>An analysis of the automatic bug fixing performance of chatgpt. D Sobania, M Briesch, C Hanna, J Petke, 2023</p>
<p>Information leakage in embedding models. C Song, A Raghunathan, Proceedings of the 2020 ACM SIGSAC conference on computer and communications security. the 2020 ACM SIGSAC conference on computer and communications security2020</p>
<p>Comparing traditional and llm-based search for consumer choice: A randomized experiment. S E Spatharioti, D M Rothschild, D G Goldstein, J M Hofman, arXiv:2307.037442023arXiv preprint</p>
<p>Systematic classification of side-channel attacks: A case study for mobile devices. R Spreitzer, V Moonsamy, T Korak, S Mangard, IEEE communications surveys &amp; tutorials. 2012017</p>
<p>Beyond memorization: Violating privacy via inference with large language models. R Staab, M Vero, M Balunović, M Vechev, 2023</p>
<p>Researchers test large language model that preserves patient privacy. K Stephens, AXIS Imaging News. 2023</p>
<p>Fake news detectors are biased against texts generated by large language models. J Su, T Y Zhuo, J Mansurov, D Wang, P Nakov, arXiv:2309.086742023arXiv preprint</p>
<p>Detecting personal information in training corpora: an analysis. N Subramani, S Luccioni, J Dodge, M Mitchell, Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing. the 3rd Workshop on Trustworthy Natural Language ProcessingTrustNLP 2023. 2023</p>
<p>Chatgpt in higher education: Considerations for academic integrity and student learning. M Sullivan, A Kelly, P Mclaughlan, 2023</p>
<p>Defending against backdoor attacks in natural language generation. X Sun, X Li, Y Meng, X Ao, L Lyu, J Li, T Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Med-mmhl: A multimodal dataset for detecting human-and llm-generated misinformation in the medical domain. Y Sun, J He, S Lei, L Cui, C.-T Lu, arXiv:2306.088712023arXiv preprint</p>
<p>Principle-driven self-alignment of language models from scratch with minimal human supervision. Z Sun, Y Shen, Q Zhou, H Zhang, Z Chen, D Cox, Y Yang, C Gan, arXiv:2305.030472023arXiv preprint</p>
<p>You reap what you sow: On the challenges of bias evaluation under multilingual settings. Z Talat, A Névéol, S Biderman, M Clinciu, M Dey, S Longpre, S Luccioni, M Masoud, M Mitchell, D Radev, Proceedings of BigScience Episode# 5-Workshop on Challenges &amp; Perspectives in Creating Large Language Models. BigScience Episode# 5-Workshop on Challenges &amp; Perspectives in Creating Large Language Models2022</p>
<p>Using large language models for cybersecurity capture-the-flag challenges and certification questions. W Tann, Y Liu, J H Sim, C M Seah, E.-C Chang, 2023</p>
<p>Breaking bad: Unraveling influences and risks of user inputs to chatgpt for game story generation. P Taveekitworachai, F Abdullah, M C Gursesli, M F Dewantoro, S Chen, A Lanata, A Guazzini, R Thawonmas, International Conference on Interactive Digital Storytelling. Springer2023</p>
<p>Using artificial intelligence to augment bug fuzzing. Z Tay, 2023</p>
<p>The impact of chatgpt on privacy and data protection laws. E Thankgod Chinonso, The Impact of ChatGPT on Privacy and Data Protection Laws. April 16, 20232023</p>
<p>Large language models in medicine. A J Thirunavukarasu, D S J Ting, K Elangovan, L Gutierrez, T F Tan, D S W Ting, Nature medicine. 2982023</p>
<p>Privinfer: Privacy-preserving inference for black-box large language model. M Tong, K Chen, Y Qi, J Zhang, W Zhang, N Yu, 2023</p>
<p>Navigating the llm landscape: A comparative analysis of leading large language models. J Torres, 2023</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Towards demystifying membership inference attacks. S Truex, L Liu, M E Gursoy, L Yu, W Wei, arXiv:1807.091732018arXiv preprint</p>
<p>Data-free model extraction. J.-B Truong, P Maini, R J Walls, N Papernot, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Does human collaboration enhance the accuracy of identifying llm-generated deepfake texts. A Uchendu, J Lee, H Shen, T Le, T.-H K Huang, D Lee, Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. the AAAI Conference on Human Computation and CrowdsourcingNov. 202311</p>
<p>How prevalent is gender bias in chatgpt?-exploring german and english chatgpt responses. S Urchs, V Thurner, M Aßenmacher, C Heumann, S Thiemichen, arXiv:2310.030312023arXiv preprint</p>
<p>The silence of the llms: Crosslingual analysis of political bias and false information prevalence in chatgpt, google bard, and bing chat. A Urman, M Makhortykh, 2023</p>
<p>Chatgpt and academic integrity concerns: Detecting artificial intelligence generated content. L Uzun, Language Education and Technology. 312023</p>
<p>Evaluating the state-of-theart in automatic de-identification. Ö Uzuner, Y Luo, P Szolovits, Journal of the American Medical Informatics Association. 1452007</p>
<p>Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. P Vaithilingam, T Zhang, E L Glassman, Chi conference on human factors in computing systems extended abstracts. 2022</p>
<p>Recovering from privacy-preserving masking with large language models. A Vats, Z Liu, P Su, D Paul, Y Ma, Y Pang, Z Ahmed, O Kalinli, 2023</p>
<p>Openai chatgpt generated results: Similarity index of artificial intelligence-based contents. R J M Ventayen, SSRN 4332664. 2023</p>
<p>All your droid are belong to us: A survey of current android attacks. T Vidas, D Votipka, N Christin, 5th USENIX Workshop on Offensive. 2011</p>
<p>Concealed data poisoning attacks on nlp models. E Wallace, T Z Zhao, S Feng, S Singh, arXiv:2010.125632020arXiv preprint</p>
<p>Poisoning language models during instruction tuning. A Wan, E Wallace, S Shen, D Klein, arXiv:2305.009442023arXiv preprint</p>
<p>You see what i want you to see: poisoning vulnerabilities in neural code search. Y Wan, S Zhang, H Zhang, Y Sui, G Xu, D Yao, H Jin, L Sun, Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2022</p>
<p>Y Wan, G Pu, J Sun, A Garimella, K.-W Chang, N Peng, arXiv:2310.09219Gender biases in llm-generated reference letters. 2023arXiv preprintkelly is a warm person, joseph is a role model</p>
<p>Improving neural language modeling via adversarial training. D Wang, C Gong, Q Liu, International Conference on Machine Learning. PMLR2019</p>
<p>Using large language models to mitigate ransomware threats. F Wang, 10.20944/preprints202311.0676.v1November 2023Preprints</p>
<p>Bot or human? detecting chatgpt imposters with a single question. H Wang, X Luo, W Wang, X Yan, 2023</p>
<p>Defecthunter: A novel llm-driven boosted-conformer-based code vulnerability detection mechanism. J Wang, Z Huang, H Liu, N Yang, Y Xiao, 10.48550/arXiv.2309.15324arXiv:2309.153242023arXiv preprint</p>
<p>Wasa: Watermark-based source attribution for large language model-generated data. J Wang, X Lu, Z Zhao, Z Dai, C.-S Foo, S.-K Ng, B K H Low, 2023</p>
<p>Rmlm: A flexible defense framework for proactively mitigating word-level adversarial attacks. Z Wang, Z Liu, X Zheng, Q Su, J Wang, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Selfdeception: Reverse penetrating the semantic firewall of large language models. Z Wang, W Xie, K Chen, B Wang, Z Gui, E Wang, arXiv:2308.115212023arXiv preprint</p>
<p>Jailbroken: How does llm safety training fail. A Wei, N Haghtalab, J Steinhardt, arXiv:2307.024832023arXiv preprint</p>
<p>Jailbreak and guard aligned language models with only few in-context demonstrations. Z Wei, Y Wang, Y Wang, arXiv:2310.063872023arXiv preprint</p>
<p>Ethical and social risks of harm from language models. L Weidinger, J Mellor, M Rauh, C Griffin, J Uesato, P.-S Huang, M Cheng, M Glaese, B Balle, A Kasirzadeh, arXiv:2112.043592021arXiv preprint</p>
<p>Empowering llm to use smartphone for intelligent task automation. H Wen, Y Li, G Liu, S Zhao, T Yu, T J , .-J Li, S Jiang, Y Liu, Y Zhang, Y Liu, arXiv:2308.152722023arXiv preprint</p>
<p>Auditable privacy protection deep learning platform construction method based on block chain incentive mechanism. J Weng, W Jiasi, M Li, Y Zhang, J Zhang, L Weiqi, Dec. 5 2023836616</p>
<p>Deepchain: Auditable and privacy-preserving deep learning with blockchain-based incentive. J Weng, J Weng, J Zhang, M Li, Y Zhang, W Luo, IEEE Transactions on Dependable and Secure Computing. 1852019</p>
<p>Ccnet: Extracting high quality monolingual datasets from web crawl data. G Wenzek, M.-A Lachaux, A Conneau, V Chaudhary, F Guzmán, A Joulin, E Grave, arXiv:1911.003592019arXiv preprint</p>
<p>Bloom: A 176b-parameter open-access multilingual language model. B Workshop, T L Scao, A Fan, C Akiki, E Pavlick, S Ilić, D Hesslow, R Castagné, A S Luccioni, F Yvon, arXiv:2211.051002022arXiv preprint</p>
<p>Fake news in sheep's clothing: Robust fake news detection against llm-empowered style attacks. J Wu, B Hooi, 2023</p>
<p>J Wu, S Yang, R Zhan, Y Yuan, D F Wong, L S Chao, arXiv:2310.14724A survey on llm-gernerated text detection: Necessity, methods, and future directions. 2023arXiv preprint</p>
<p>S Wu, O Irsoy, S Lu, V Dabravolski, M Dredze, S Gehrmann, P Kambadur, D Rosenberg, G Mann, arXiv:2303.17564Bloomberggpt: A large language model for finance. 2023arXiv preprint</p>
<p>Unveiling security, privacy, and ethical concerns of chatgpt. X Wu, R Duan, J Ni, 2023</p>
<p>Defending pre-trained language models as few-shot learners against backdoor attacks. Z Xi, T Du, C Li, R Pang, S Ji, J Chen, F Ma, T Wang, arXiv:2309.132562023arXiv preprint</p>
<p>Universal fuzzing via large language models. C S Xia, M Paltenghi, J L Tian, M Pradel, L Zhang, arXiv:2308.047482023arXiv preprint</p>
<p>Practical program repair in the era of large pre-trained language models. C S Xia, Y Wei, L Zhang, 2022</p>
<p>Keep the conversation going: Fixing 162 out of 337 bugs for $0.42 each using chatgpt. C S Xia, L Zhang, 2023</p>
<p>Chatunitest: a chatgpt-based automated unit test generation tool. Z Xie, Y Chen, C Zhi, S Deng, J Yin, arXiv:2305.047642023arXiv preprint</p>
<p>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. M Xiong, Z Hu, X Lu, Y Li, J Fu, J He, B Hooi, ArXiv. 2592243892306.13063, 2023</p>
<p>In situ augmentation for defending against adversarial attacks on text classifiers. L Xu, L Berti-Equille, A Cuesta-Infante, K Veeramachaneni, International Conference on Neural Information Processing. Springer2022</p>
<p>Agentsca: Advanced physical side channel analysis agent with llms. F Yaman, 2023</p>
<p>Virtual prompt injection for instruction-tuned large language models. J Yan, V Yadav, S Li, L Chen, Z Tang, H Wang, V Srinivasan, X Ren, H Jin, arXiv:2307.168882023arXiv preprint</p>
<p>White-box compiler fuzzing empowered by large language models. C Yang, Y Deng, R Lu, J Yao, J Liu, R Jabbarvand, L Zhang, 2023</p>
<p>A comprehensive overview of backdoor attacks in large language models within communication networks. H Yang, K Xiang, H Li, R Lu, arXiv:2308.143672023arXiv preprint</p>
<p>Harnessing the power of llms in practice: A survey on chatgpt and beyond. J Yang, H Jin, R Tang, X Han, Q Feng, H Jiang, B Yin, X Hu, arXiv:2304.137122023arXiv preprint</p>
<p>Poisoning scientific knowledge using large language models. J Yang, H Xu, S Mirzoyan, T Chen, Z Liu, W Ju, L Liu, M Zhang, S Wang, bioRxiv. 2023</p>
<p>Crafting unusual programs for fuzzing deep learning libraries. S Yang, 2023University of Illinois at Urbana-ChampaignPh.D. dissertation</p>
<p>What do code models memorize? an empirical study on large language models of code. Z Yang, Z Zhao, C Wang, J Shi, D Kim, D Han, D Lo, arXiv:2308.099322023arXiv preprint</p>
<p>Empowering llmbased machine translation with cultural awareness. B Yao, M Jiang, D Yang, J Hu, arXiv:2305.143282023arXiv preprint</p>
<p>Fuzzllm: A novel and universal fuzzing framework for proactively discovering jailbreak vulnerabilities in large language models. D Yao, J Zhang, I G Harris, M Carlsson, arXiv:2309.052742023arXiv preprint</p>
<p>Poisonprompt: Backdoor attack on prompt-based large language models. H Yao, J Lou, Z Qin, arXiv:2310.124392023arXiv preprint</p>
<p>Towards improving adversarial training of nlp models. J Y Yoo, Y Qi, arXiv:2109.005442021arXiv preprint</p>
<p>Large language models are better adversaries: Exploring generative clean-label backdoor attacks against text classifiers. W You, Z Hammoudeh, D Lowd, arXiv:2310.186032023arXiv preprint</p>
<p>Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts. J Yu, X Lin, X Xing, arXiv:2309.102532023arXiv preprint</p>
<p>Revisiting out-of-distribution robustness in nlp: Benchmark, analysis, and llms evaluations. L Yuan, Y Chen, G Cui, H Gao, F Zou, X Cheng, H Ji, Z Liu, M Sun, arXiv:2306.046182023arXiv preprint</p>
<p>Rrhf: Rank responses to align language models with human feedback without tears. Z Yuan, H Yuan, C Tan, W Wang, S Huang, F Huang, arXiv:2304.053022023arXiv preprint</p>
<p>No more manual tests? evaluating and improving chatgpt for unit test generation. Z Yuan, Y Lou, M Liu, S Ding, K Wang, Y Chen, X Peng, arXiv:2305.042072023arXiv preprint</p>
<p>Building trust in conversational ai: A comprehensive review and solution architecture for explainable, privacy-aware systems using llms and knowledge graph. A Zafar, V B Parthasarathy, C L Van, S Shahid, A Shahid, arXiv:2308.135342023arXiv preprint</p>
<p>Understanding large language model based fuzz driver generation. C Zhang, M Bai, Y Zheng, Y Li, X Xie, Y Li, W Ma, L Sun, Y Liu, arXiv:2307.124692023arXiv preprint</p>
<p>A survey on federated learning. C Zhang, Y Xie, H Bai, B Yu, W Li, Y Gao, Knowledge-Based Systems. 2161067752021</p>
<p>Text revealer: Private text reconstruction via model inversion attacks against transformers. R Zhang, S Hidano, F Koushanfar, arXiv:2209.105052022arXiv preprint</p>
<p>Remarkllm: A robust and efficient watermarking framework for generative large language models. R Zhang, S S Hussain, P Neekhara, F Koushanfar, 2023</p>
<p>Towards llm-based fact verification on news claims with a hierarchical step-by-step prompting method. X Zhang, W Gao, arXiv:2310.003052023arXiv preprint</p>
<p>Prompts should not be seen as secrets: Systematically measuring prompt extraction attack success. Y Zhang, D Ippolito, arXiv:2307.068652023arXiv preprint</p>
<p>How well does llm generate security tests. Y Zhang, W Song, Z Ji, D D Yao, N Meng, arXiv:2310.007102023arXiv preprint</p>
<p>Ethicist: Targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation. Z Zhang, J Wen, M Huang, arXiv:2307.044012023arXiv preprint</p>
<p>Understanding programs by exploiting (fuzzing) test cases. J Zhao, Y Rong, Y Guo, Y He, H Chen, arXiv:2305.135922023arXiv preprint</p>
<p>Prompt as triggers for backdoor attack: Examining the vulnerability in language models. S Zhao, J Wen, L A Tuan, J Zhao, J Fu, arXiv:2305.012192023arXiv preprint</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>knn-icl: Compositional task-oriented parsing generalization with nearest neighbor in-context learning. W Zhao, Y Liu, Y Wan, Y Wang, Q Wu, Z Deng, J Du, S Liu, Y Xu, P S Yu, 2023</p>
<p>Lima: Less is more for alignment. C Zhou, P Liu, P Xu, S Iyer, J Sun, Y Mao, X Ma, A Efrat, P Yu, L Yu, arXiv:2305.112062023arXiv preprint</p>
<p>Freelb: Enhanced adversarial training for natural language understanding. C Zhu, Y Cheng, Z Gan, S Sun, T Goldstein, J Liu, arXiv:1909.117642019arXiv preprint</p>
<p>Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. K Zhu, J Wang, J Zhou, Z Wang, H Chen, Y Wang, L Yang, W Ye, N Z Gong, Y Zhang, arXiv:2306.045282023arXiv preprint</p>
<p>Large language models are built-in autoregressive search engines. N Ziems, W Yu, Z Zhang, M Jiang, arXiv:2305.096122023arXiv preprint</p>
<p>Universal and transferable adversarial attacks on aligned language models," communication, it is essential for you to comprehend user queries in Cipher Code and subsequently deliver your responses utilizing Cipher Code. A Zou, Z Wang, J Z Kolter, M Fredrikson, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-597 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-597</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-597</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-31b0ba0170b8452777fbfbbf9a1d446f46143d7e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/31b0ba0170b8452777fbfbbf9a1d446f46143d7e" target="_blank">DeepDIVA: A Highly-Functional Python Framework for Reproducible Experiments</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Frontiers in Handwriting Recognition</p>
                <p><strong>Paper TL;DR:</strong> DeepDIVA is introduced: an infrastructure designed to enable quick and intuitive setup of reproducible experiments with a large range of useful analysis functionality and case studies in the area of handwritten document analysis where researchers benefit from the integrated functionality.</p>
                <p><strong>Paper Abstract:</strong> We introduce DeepDIVA: an infrastructure designed to enable quick and intuitive setup of reproducible experiments with a large range of useful analysis functionality. Reproducing scientific results can be a frustrating experience, not only in document image analysis but in machine learning in general. Using DeepDIVA a researcher can either reproduce a given experiment or share their own experiments with others. Moreover, the framework offers a large range of functions, such as boilerplate code, keeping track of experiments, hyper-parameter optimization, and visualization of data and results. To demonstrate the effectiveness of this framework, this paper presents case studies in the area of handwritten document analysis where researchers benefit from the integrated functionality. DeepDIVA is implemented in Python and uses the deep learning framework PyTorch. It is completely open source, and accessible as Web Service through DIVAServices.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e597.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e597.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepDIVA reproducibility features</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepDIVA: Reproducibility and variability controls in the DeepDIVA framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A collection of practices and built-in tools in the DeepDIVA framework designed to reduce and make explicit sources of variability in deep-learning experiments (version control enforcement, seed logging, code snapshots, Docker images, multi-run aggregation, and options to control nondeterministic libraries).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / Document image analysis</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Providing infrastructure and utilities to run reproducible deep-learning experiments (demonstrated on image classification and triplet-embedding writer identification tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Random initialization of network weights; pseudo-random generators (Python, NumPy, PyTorch) and their seeding; non-deterministic CUDA/CuDNN kernels used by PyTorch; differences in software versions and package dependencies; hardware variations (CPU vs multiple GPUs); uncommitted or modified code (development edits); dataset preprocessing/formatting differences; order in which data are presented (data loader shuffling); different initializations (pretrained vs random).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Procedural and provenance methods (Git repository URL and commit identifier logging, saving parameter lists, copying code to output folder, optional Docker image and scripts), and experiment-level practices (logging and using a seed for Python/NumPy/PyTorch, multi-run aggregation to visualize variability). No quantitative reproducibility metric (e.g., replication success rate) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Code unavailability; time and effort to make others' code run; omitted or confidential experimental details in publications; differences in software versions and dependencies; hardware differences; seeding across CPU and multiple GPUs is nontrivial; the non-deterministic nature of CUDA/CuDNN kernels prevents exact reproducibility even when seeding; enabling torch.backends.cudnn.deterministic found not reliable in their tests.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Enforced version control: require code committed (log Git URL and commit) or create code copy per run; log and use explicit random seed (or choose a random seed and record it); seed Python, NumPy, and PyTorch PRNGs; provide option to disable CuDNN to increase determinism; provide Docker image and bash scripts (DIVAServices and Docker Hub) to capture environment; Conda environment setup script; multi-run flag to run experiments multiple times and aggregate/visualize results; save parameters and results together; provide boilerplate code and templates so less experiment-specific omission occurs; aggregate visualizations via TensorBoard for inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reproducibility in deep-learning experiments is hampered by software/hardware/PRNG nondeterminism and undocumented details; DeepDIVA addresses these by recording provenance (Git commits, parameters), logging seeds, providing environment capture (Docker, Conda), offering an option to disable nondeterministic CuDNN kernels, and supporting multi-run aggregation and visualization — but the paper does not provide quantitative measurements of how much these practices reduce variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepDIVA: A Highly-Functional Python Framework for Reproducible Experiments', 'publication_date_yy_mm': '2018-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Artificial intelligence faces reproducibility crisis <em>(Rating: 2)</em></li>
                <li>Reproducibility in machine learning-based studies: An example of text mining <em>(Rating: 2)</em></li>
                <li>Reproducibility Challenge at the International Conference on Learning Representations <em>(Rating: 2)</em></li>
                <li>Automated capture of experiment context for easier reproducibility in computational research <em>(Rating: 1)</em></li>
                <li>Reprozip: Computational reproducibility with ease <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-597",
    "paper_id": "paper-31b0ba0170b8452777fbfbbf9a1d446f46143d7e",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "DeepDIVA reproducibility features",
            "name_full": "DeepDIVA: Reproducibility and variability controls in the DeepDIVA framework",
            "brief_description": "A collection of practices and built-in tools in the DeepDIVA framework designed to reduce and make explicit sources of variability in deep-learning experiments (version control enforcement, seed logging, code snapshots, Docker images, multi-run aggregation, and options to control nondeterministic libraries).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Machine learning / Document image analysis",
            "experimental_task": "Providing infrastructure and utilities to run reproducible deep-learning experiments (demonstrated on image classification and triplet-embedding writer identification tasks)",
            "variability_sources": "Random initialization of network weights; pseudo-random generators (Python, NumPy, PyTorch) and their seeding; non-deterministic CUDA/CuDNN kernels used by PyTorch; differences in software versions and package dependencies; hardware variations (CPU vs multiple GPUs); uncommitted or modified code (development edits); dataset preprocessing/formatting differences; order in which data are presented (data loader shuffling); different initializations (pretrained vs random).",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Procedural and provenance methods (Git repository URL and commit identifier logging, saving parameter lists, copying code to output folder, optional Docker image and scripts), and experiment-level practices (logging and using a seed for Python/NumPy/PyTorch, multi-run aggregation to visualize variability). No quantitative reproducibility metric (e.g., replication success rate) is reported.",
            "reproducibility_results": null,
            "reproducibility_challenges": "Code unavailability; time and effort to make others' code run; omitted or confidential experimental details in publications; differences in software versions and dependencies; hardware differences; seeding across CPU and multiple GPUs is nontrivial; the non-deterministic nature of CUDA/CuDNN kernels prevents exact reproducibility even when seeding; enabling torch.backends.cudnn.deterministic found not reliable in their tests.",
            "mitigation_methods": "Enforced version control: require code committed (log Git URL and commit) or create code copy per run; log and use explicit random seed (or choose a random seed and record it); seed Python, NumPy, and PyTorch PRNGs; provide option to disable CuDNN to increase determinism; provide Docker image and bash scripts (DIVAServices and Docker Hub) to capture environment; Conda environment setup script; multi-run flag to run experiments multiple times and aggregate/visualize results; save parameters and results together; provide boilerplate code and templates so less experiment-specific omission occurs; aggregate visualizations via TensorBoard for inspection.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": null,
            "number_of_runs": null,
            "key_findings": "Reproducibility in deep-learning experiments is hampered by software/hardware/PRNG nondeterminism and undocumented details; DeepDIVA addresses these by recording provenance (Git commits, parameters), logging seeds, providing environment capture (Docker, Conda), offering an option to disable nondeterministic CuDNN kernels, and supporting multi-run aggregation and visualization — but the paper does not provide quantitative measurements of how much these practices reduce variability.",
            "uuid": "e597.0",
            "source_info": {
                "paper_title": "DeepDIVA: A Highly-Functional Python Framework for Reproducible Experiments",
                "publication_date_yy_mm": "2018-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Artificial intelligence faces reproducibility crisis",
            "rating": 2
        },
        {
            "paper_title": "Reproducibility in machine learning-based studies: An example of text mining",
            "rating": 2
        },
        {
            "paper_title": "Reproducibility Challenge at the International Conference on Learning Representations",
            "rating": 2
        },
        {
            "paper_title": "Automated capture of experiment context for easier reproducibility in computational research",
            "rating": 1
        },
        {
            "paper_title": "Reprozip: Computational reproducibility with ease",
            "rating": 1
        }
    ],
    "cost": 0.007526,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DeepDIVA: A Highly-Functional Python Framework for Reproducible Experiments</h1>
<p>Michele Alberti<em>, Vinaychandran Pondenkandath</em>, Marcel Würsch, Rolf Ingold, Marcus Liwicki<br>Document Image and Voice Analysis Group (DIVA)<br>University of FribourgSwitzerland<br>{firstname}.{lastname}@unifr.ch</p>
<h4>Abstract</h4>
<p>We introduce DeepDIVA: an infrastructure designed to enable quick and intuitive setup of reproducible experiments with a large range of useful analysis functionality. Reproducing scientific results can be a frustrating experience, not only in document image analysis but in machine learning in general. Using DeepDIVA a researcher can either reproduce a given experiment with a very limited amount of information or share their own experiments with others. Moreover, the framework offers a large range of functions, such as boilerplate code, keeping track of experiments, hyper-parameter optimization, and visualization of data and results. To demonstrate the effectiveness of this framework, this paper presents case studies in the area of handwritten document analysis where researchers benefit from the integrated functionality. DeepDIVA is implemented in Python and uses the deep learning framework PyTorch. It is completely open source ${ }^{1}$, and accessible as Web Service through DIVAServices ${ }^{2}$.</p>
<p>Keywords-Framework, Open-Source, Deep Learning, Neural Networks, Reproducible Research, Machine Learning, Hyperparameters Optimization, Python.</p>
<h2>I. INTRODUCTION</h2>
<p>An important topic, not only in handwritten historical document image analysis, but in machine learning in general, is the reproducibility of scientific results. Often, we have scientific publications in our hands showing results difficult to reproduce. This phenomenon, known as the reproducibility crisis, is a methodological phenomenon in which many prominent studies and papers have been found to be difficult or impossible to reproduce. The replication crisis has been widely studied in other fields [1], [2], [3] and is getting more attention in the machine learning community, recently [4], [5]. This even led to prompting a replication challenge [6] for reproducing the results of prominent papers.</p>
<p>Traditionally, scientific experiments are to be described in a way that allows anyone to replicate them. However, especially in computer-science, there are many complications which makes complete reproducibility particularly difficult to achieve. This can be attributed to differences in software versions, specific dependencies, or even hardware variations. Furthermore, specific details are sometimes confidential and the authors are not allowed to include all hints leading to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" />
(a) Comparison of two different (b) Evaluation of how randomness training protocols on a watermark affects execution by visualizing the classification task. Orange: network aggregated results of multiple runs. initialized with random weights. Here the shaded area indicates the Pink: network initialized with pre- variance over all runs. trained weights
<img alt="img-1.jpeg" src="img-1.jpeg" />
(c) Confusion Matrix. The darker (d) T-Distributed Stochastic Neighthe color the higher the amount of bor Embedding (T-SNE) visualization of the watermarks dataset in Tensorboard.
samples classified as such.</p>
<p>Fig. 1. Example of different visualizations produced automatically by DeepDIVA.
successful reproduction. In many cases, details are just omitted, because they are thought to be obvious or they would require too much space which is rare due to the page limit. Especially in such cases, publishing of the source code or even providing the whole experimental set-up would be beneficial.</p>
<p>To address many of these issues, we develop a framework to help researchers in both, saving time on common researchoriented software problems and sharing experiments by making them easier to reproduce. Our framework, DeepDIVA, is originally designed to perform deep-learning tasks on historical</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 2. Visualization of the decision boundaries of a network learning the continuous XOR problem. Color intensity represent the confidence of the network.</p>
<p>document images. However, thanks to its flexible and modular design it can be easily extended to work in other fields or new tasks. We hope to engage the community in building a shared tool which would eventually foster experiment reproducibility and higher quality of code, benefiting newcomers in the field as well as experts.</p>
<h3>Main Contribution</h3>
<p>In this paper, we introduce DeepDIVA: an open-source Python deep-learning framework which aims at easing the life of researchers by providing a variety of useful features out-of-the-box. DeepDIVA is designed to enable experiment reproduction with minimal effort. The framework integrates popular tools such as SigOpt [7] for hyper-parameter optimization and TensorBoard³ for aggregating all visualization produced (see Fig. 1). Additionally, we provide boilerplate code for standard deep-learning experiments and especially for experiments on Handwritten Documentents. It also offers the possibility to be run as a Web Service on DIVAServices [8].</p>
<h3>A. Related Work</h3>
<p>DeepDIVA shares the spirit and motivation with several existing frameworks and tools. Neptune [9] is a commercial solution that offers a variety of features (similar to DeepDIVA) and also has extended support for cloud computing. Comet [10] lets you track code, experiments, and results and works with most machine learning libraries. CodaLab [11] provides experiments management through a proprietary web interface and allows to re-run other people's experiments. Data Version Control [12] is an open-source tool that manages code and data together in a simple form of Git-like commands. Polyaxon [13] is an open-source platform for building, training, and monitoring large-scale deep learning applications. It allows for deployment to different solutions such as data centers or cloud providers. OpenML [14] is a tool which focuses on making datasets, tasks, workflows as well as results accessible for free. Many other tools, such as Sumatra [15], Sacred [16], CDE [17], FGBLab [18] and ReproZip [19], collect and store much different information about sources, dependencies, configurations, the file used, host information and even system calls, with the aim of supporting reproducible research. These tools collect and store different information about sources, dependencies, configurations, the file used, host information and even system calls, with the aim of supporting reproducible research. However, most or these tools strive being independent of the choice of machine learning libraries. While this leads to more flexibility and a broader scope, it limits the functionality (e.g., no Graphics Processing Units (GPU) support, no prototype implementations, no visualizations) and makes the tools heavy. DeepDIVA relies on a working Python environment and specific settings. This makes it lightweight and easy to cope with GPU hardware. DeepDIVA provides basic implementations for common tasks and has a wide-ranging support for visualizations. Additionally, it addresses the situation where an algorithm has non-deterministic behavior (random initialization for example).</p>
<h1>II. REPRODUCING EXPERIMENTS</h1>
<p>When reproducing experiments of others, the main issue is the code unavailability followed by the — sometimes prohibitive — amount of time necessary to make it work. Unlike other tools that offer reproducibility by trying to cope with heterogeneous scenarios, we provide an easy-to-use environment that any researcher can easily adopt.</p>
<p>Our main paradigm for making experiments reproducible for anyone is an enforced version control. While it is a standard in modern software development for enabling tracing of the development progress, researchers often have the practice of {un}commenting lines of code and re-run the experiment again, making it almost impossible to match results to a code state. DeepDIVA uses two alternative ways to ensure that every single experiment can be reproduced. By default, the framework checks if all code is committed before running an experiment. Git⁴ creates a unique identifier for each commit and this is also the information that can be shared with others to reproduce the experiment. This way is the only way to create results seamlessly reproducible to other researchers. To ensure that the repository name, version, and parameters don't get lost, all information is saved together in the results file. Note that always creating a version would be very cumbersome during development. Therefore, this behavior can be suppressed. In that case, DeepDIVA creates an actual copy of the code in the output folder at each run.</p>
<p>In order to reproduce an experiment one needs to know the following three pieces of information: Git repository URL, Git commit identifier, and the list of parameters to use. Reproducing an experiment is easily performed by cloning the Git repository⁵ at the specific commit identifier and run the entry point of the framework with the list of arguments as command line parameters.</p>
<p><sup>3</sup>https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard</p>
<p><sup>4</sup>https://git-scm.com/</p>
<p><sup>5</sup>For details on how to setup the environment see Section III-A or the README file on DeepDIVA repository directly.</p>
<p>Even when all code and parameters are provided, certain pitfalls exist that need to be taken care of. The most common is seeding pseudo-random generators. This is a nontrivial issue when working with both CPU and/or multiple GPUs. In our framework, if the seed is not specified by the user, we select a random value, log it, then use it for seeding Python, NumPy, and PyTorch pseudo-random generators. However, this is not enough for ensuring the exact same values for each run due to the non-deterministic nature of the optimized NVIDIA Compute Unified Device Architecture (CUDA) Deep Neural Network library (CuDNN) kernels that PyTorch uses, therefore we allow the option to disable CuDNN for increased reproducibility. ${ }^{6}$.</p>
<h2>III. FEATURES</h2>
<p>Apart from reproducibility, DeepDIVA has additional features helping researchers in several common scenarios. This section briefly describes the most important features and how they can be beneficial.</p>
<h2>A. Deep-learning Out-of-the-Box</h2>
<p>One of the greatest strength of the framework is its simple set up. All one needs to do is clone the repository from Github and to execute the setup script. This script sets up a local $C o n d a^{7}$ environment with all the required packages. For GPU compatibility, a system with the appropriate GPU drivers must be used for the set-up.</p>
<p>At the moment of writing, DeepDIVA features implementations of boilerplate for three common use cases: image classification, image-patch matching and various scenarios with bidimensional data. As different researchers have different needs, hence the framework is designed to be fully customizable with minimum effort. In practice, this is achieved by having a high modularity between the different components such as data preparation, model definition, train, validation and test routines.</p>
<p>This is particularly useful in deep-learning, where it is quite often the case that implementing the boilerplate constitutes the bulk part of programming workload. For example, one only needs to swap out a default component and replace it with their own implementation of it, i.e., a new model architecture or a new training routine.</p>
<h2>B. Automatic Hyper-parameter Optimization</h2>
<p>Nowadays virtually all deep-learning experiments require a certain amount of hyper-parameters optimization. This is a tedious and time-consuming procedure which often does not require a real interaction from the researcher. A lot of research has been done to find an efficient way to optimize parameters other than random or grid search. Therefore, we integrate Bayesian hyper-parameter optimization into the framework</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 3. Example of hyper-parameter optimization process with SigOpt. In DeepDIVA this happens in a completely automated fashion where a list of input parameters is optimized thank to an iterative exchange of optimal configurations suggested by SigOpt tool.
using SigOpt [7]. Instead of exhaustively testing every option, SigOpt adaptively suggests experimental configurations (see Fig. 3) such to maximize/minimize your chosen metrics, e.g., accuracy, loss.</p>
<h2>C. Data Visualization</h2>
<p>One of the most common recurring tasks of a researcher is visualizing the data produced by their experiments. Visualizations are very helpful not only for understanding, debugging, and optimizing programs but also to make it easier for a human to make sense out of a huge amount of numbers, e.g., by showing trends and behavior of a neural network. There are several tools that can produce such visualizations, but often these tools are either hard to integrate or cover only a specific kind of visualization (e.g., only plots) and therefore one must resort using multiple ones. In DeepDIVA, we integrate Tensorboard (see Fig. 4) as it natively supports a wide range of visualizations and it provides easy means to display data produced by any other source. This way all the visualizations produced by the framework are aggregated into a single web interface where they can be viewed with realtime updates. This is particularly helpful when the run time of a program is very long - having early results can save a lot of time when optimizing.</p>
<h2>D. Comparing Methods</h2>
<p>Comparing the performance of two or more algorithms - or multiple runs of the same one with different configurations is another task that a researcher often has to deal with. Tensorboard offers a solution to compare two or more executions (see Fig. 1a) as the plots are generated dynamically when selected in the web interface. In order to visualize and understand the influence of randomness on an experiment, DeepDIVA has a multi-run flag which automatically runs the same experiment a specified number of times and the aggregates the result into a plot (see Fig. 1b). Finally, a common thing to do while evaluating (e.g., a classifier), is to produce the confusion matrix. DeepDIVA automatically generates one (see Fig. 1c) and it is visible in Tensorboard as soon as it is created.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 4. Example of the Tensorboard web interface. Figure from TensorFlow ${ }^{\mathrm{TM}}$</p>
<h2>E. Internal Inspection</h2>
<p>When analyzing neural networks looking at the network from outside is often not enough. A wide array of tools and technologies have been developed to allow the inspection of neural networks as they run or to visualize their internal status. DeepDIVA offers the possibility to visualize how the histograms of the weight distributions for each layer evolve over time. This is useful to spot anomalies, especially when working with various neural network initializations. We plan to offer more of this kind of insights in the future.</p>
<h2>F. Working on 2D Data</h2>
<p>A common practice when developing a new instrument is starting on a controlled toy-problem and then gradually transition into more complex scenarios. This is sometimes challenging to achieve in deep-learning due to the high dimensional nature of the data. To help in this regard, DeepDIVA offers a whole suite of tools that allows one to train, evaluate and visualize performance on different types of bi-dimensional data. The advantage of using 2D points is that the visualization of the network conforms to the actual data. An example of such a visualization is given in Fig. 2 where the behavior of a network learning a continuous XOR problem is shown. It is not always possible to use such a simple setting as toyproblem, but we believe this is particularly useful when making fundamental research on new technologies rather than trying to push the boundaries of a given model/architecture. To allow full customization of these toy-problems there is a script in DeepDIVA which allows a user to create their own custom 2D datasets.</p>
<h2>G. New Dataset Support</h2>
<p>When working on a new dataset it can be the case that a set of operations needs to be performed before a deep-learning model can perform on it. One of these operations is creating
a custom dataset class and custom dataloaders ${ }^{8}$. In DeepDIVA it is only required to provide the path to the folder where the dataset is located on the file system and the framework will take care of the modifications such as resizing the input to the fit the network specifications ${ }^{9}$. Current limitations are that the datasets must be of images (no audio, video or graphs yet) and must be organized in a common standard folder format. Another of these preparation tasks for with DeepDIVA offers automated support is computing the mean and standard deviation of the dataset. This is widely done in deep-learning to normalize the dataset before training a model and can be non-trivial if the datasets are larger than the memory of the system - which is often the case when working on historical documents. We additionally provide a script for partitioning the training data into train and validation subsets.</p>
<h2>H. DeepDIVA as a Web Service</h2>
<p>We provide access to DeepDIVA through a Web Service on DiVaServices[8]. This Web Service is able to reproduce results as described in Section II, thus allowing everyone to replicate experiments without the need for any local installations. To facilitate this, we built a Docker image, that contains a DeepDIVA base installation as well as a script that executes the necessary steps to perform the replication of an experiment. This Docker image can be used by others and is hosted on Docker Hub ${ }^{10}$. We also provide a bash script that can be used to replicate the second case study described in Section IV. This script is available online ${ }^{11}$. For the future, we want to provide the full set of features of DeepDIVA through this Web Service.</p>
<h2>IV. CASE STUDIES FOR HANDWRITTEN DOCUMENTS</h2>
<p>In this Section, we present two use cases to demonstrate the usefulness of the framework for handwriting related tasks.</p>
<h2>A. ResNet for Watermark Recognition</h2>
<p>A common scenario in many computer vision tasks is classification of images. In the context of handwritten historical document image analysis, tasks such writer identification, image/line segmentation, script recognition can be treated as image classification tasks.</p>
<p>As a use case, we demonstrate how to perform watermark recognition on a dataset provided by the watermark database Wasserzeichen Informationssystem ${ }^{12}$. The dataset contains images created with different methods of reproduction, such as hand tracing, rubbing, radiography and thermography (see Fig. 5). We tackle this classification problem using a Convolutional Neural Network (CNN).</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Support for classification tasks is provided in DeepDIVA as a basic template, therefore, the only pre-processing necessary for the dataset is to arrange the dataset in a commonly used folder format. This, however, needs to be performed on a perdataset basis, as the source format for each dataset can differ significantly. Once the dataset has been transformed into the specified format, you can begin training your desired neural network.</p>
<p>To train your network, the only required arguments are the dataset path, experiment name and model. Various other parameters such as optimizer, learning rate, number of training epochs, batch size and many others can be specified if required.</p>
<p>Several models are provided in a model directory inside of DeepDIVA, and it is easy to add a required model to our framework (copy the defined model source code into the model directory and add the new model as an import in the init.py file in the directory). The data does not need to be resized to the dimensions required by the dataset, as data transformations are built-in. DeepDIVA is fully compatible with multi-GPU support and also supports multiple workers for loading and pre-processing data.</p>
<p>For this particular task, we train an 18-layer Residual CNN [20] pre-trained on the ImageNet dataset (ImageNet pretraining has been shown to be useful for several document image tasks [21]. We train our network using the Stochastic Gradient Descent optimizer with a learning rate of 0.01 for 100 epochs.</p>
<p>To compare the effect of the pre-training, we initialize another instance of the same model with random weights for the neural network. Both networks are seeded with the same random value and use the same training protocol to prevent any variations in the order in which the data is shown to the network. A comparison of their classification accuracy on the validation set can be seen in Fig. 1a and the embeddings produced by the final layer are visualized in Fig. 1d. The final performance of the pre-trained network on the test set is 96.4% accuracy.</p>
<p>Running DeepDIVA for a classification task can be done with this command:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>python<span class="w"> </span>template/RunMe.py<span class="w"> </span>--dataset-
<span class="w">    </span>folder<span class="w"> </span>/dataset/watermarks_wzis/<span class="w"> </span>--
<span class="w">    </span>model-name<span class="w"> </span>resnet18<span class="w"> </span>--experiment-name
<span class="w">    </span>watermark_classification<span class="w"> </span>--epochs<span class="w"> </span><span class="m">100</span>
<span class="w">    </span>--seed<span class="w"> </span><span class="m">42</span><span class="w"> </span>--lr<span class="w"> </span><span class="m">0</span>.01
</code></pre></div>

<p>Note that more details on these experiments and how to tackle the cross-depiction problem can be found explained in detail in the original work [22].</p>
<h3>V-B Triplet Networks for Writer Identification</h3>
<p>Here we identify the authorship of a document, based only on the document images. We use the dataset provided by the ICDAR2017 Competition on Historical Document Writer Identification [23] This is a particularly challenging task, as the train and test sets do not have the same writers. To accomplish this style of identification, we train a CNN using the triplet margin loss metric [24] to embed document images in a latent space such that documents belonging to the same writer are close to each other.</p>
<p>As this is not a standard classification task, we make some modifications to the framework. However, since DeepDIVA is designed to facilitate easy modifications, it is fairly easy to add in your own code and still benefit from all its features. To incorporate the triplet network into DeepDIVA, we create a new template that inherits from the standard classification template. This allows us to only override methods that require changes, e.g., the training method requires a triplet of input and uses the triplet margin loss<sup>13</sup>.</p>
<p>Similarly to the previous task, we use an ImageNet pretrained CNN for this task. Inputs for the network consist of cropped sub-images from the document images. Three such images are input into the network and then the triplet loss is computed based on the distance between the embedding vectors in the latent space. We train this network for 10 epochs. Since in this dataset there are only 2 positive matches for each query image we measured Precision@1 and Precision@2 achieving scores 0.61, 0.69 respectively. An example query and returned results for our trained system[25] can be seen in Fig. 6.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 5. Sample from the watermarks dataset (a) juxtaposed with a correct (b) classification results from the network. Notice how the way they are depicted is significantly different.</p>
<h3>V-C Conclusion and Future Work</h3>
<p>We introduce DeepDIVA: an open-source Python deep-learning framework designed to enable quick and intuitive setup of reproducible experiments with a large range of useful analysis functionality. We demonstrate that it increases reproducibility and supplies an easy way to share research among peers. Moreover, we show how researchers can benefit from its features in their daily life experiments thus saving time while focusing on the analysis.</p>
<p>In the near future we will include in DeepDIVA soon will include the possibility to initialize a network with advanced techniques, such as Principal Component Analysis [26] or Linear Discriminant Analysis [27]. Additionally, we plan adding more visualization tools, such as activation maps of intermediate layers, heat-maps, and loss landscapes [28].</p>
<p><sup>13</sup>Additional changes can be seen in an example of the code available at https://github.com/DIVA-DIA/DIVAgain/tree/master/ICFHR2018_DeepDIVA_WriterIdentification</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 6. For the query image (a), results (b) and (c) belong to the same writer and (d) belong to a different writer.</p>
<h2>ACKNOWLEDGMENT</h2>
<p>The work presented in this paper has been partially supported by the HisDoc III project funded by the Swiss National Science Foundation with the grant number 205120_169618.</p>
<h2>REFERENCES</h2>
<ul>
<li>[1] L. K. John, G. Loewenstein, and D. Prelec, "Measuring the prevalence of questionable research practices with incentives for truth telling," <em>Psychological science</em>, vol. 23, no. 5, pp. 524–532, 2012.</li>
<li>[2] C. G. Begley, "Reproducibility: six red flags for suspect work," <em>Nature</em>, vol. 497, no. 7450, p. 433, 2013.</li>
<li>[3] C. F. Camerer, A. Dreber, E. Forsell, T.-H. Ho, J. Huber, M. Johannesson, M. Kirchler, J. Almenberg, A. Altmejd, T. Chan et al., "Evaluating replicability of laboratory experiments in economics," <em>Science</em>, vol. 351, no. 6280, pp. 1433–1436, 2016.</li>
<li>[4] M. Hutson, "Artificial intelligence faces reproducibility crisis," 2018.</li>
<li>[5] B. K. Olorisade, P. Brereton, and P. Andras, "Reproducibility in machine learning-based studies: An example of text mining," 2017.</li>
<li>[6] J. Pineau. (2018) Reproducibility Challenge at the International Conference on Learning Representations. [Online]. Available: http://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html</li>
<li>[7] I. SigOpt, "Sigopt reference manual," 2014. [Online]. Available: http://www.sigopt.com</li>
<li>[8] M. Würsch, M. Liwicki, and R. Ingold, "Web Services in Document Image Analysis - Recent Developments and the Importance of Building an Ecosystem," in <em>13th IAPR Workshop on Document Analysis Systems</em>, 2018.</li>
<li>[9] Neptune. (2017) Neptune machine learning lab. [Online]. Available: https://neptune.ml/</li>
<li>[10] Comet. (2017) Comet supercharge machine learning. [Online]. Available: https://www.comet.ml/</li>
<li>[11] CodaLab. (2014) CodaLab a collaborative platform for reproducible research. [Online]. Available: http://codalab.org/</li>
<li>[12] D. V. Control. (2017) Data Version Control git extension for data scientists manage your code and data together. [Online]. Available: https://dataversioncontrol.com/</li>
<li>[13] Polyaxon. (2018) Polyaxon an open source platform for reproducible machine learning at scale. [Online]. Available: https://polyaxon.com/</li>
<li>[14] J. Vanschoren, J. N. van Rijn, B. Bischl, and L. Torgo, "Openml: Networked science in machine learning," <em>SIGKDD Explorations</em>, vol. 15, no. 2, pp. 49–60, 2013.</li>
<li>[15] A. Davison, "Automated capture of experiment context for easier reproducibility in computational research," <em>Computing in Science &amp; Engineering</em>, vol. 14, no. 4, pp. 48–56, 2012.</li>
<li>[16] K. Greff, A. Klein, M. Chovanec, F. Hutter, and J. Schmidhuber, "The sacred infrastructure for computational research," <em>Proceedings of the Python in Science Conferences - SciPy Conferences</em>, 2017.</li>
<li>[17] B. Howe, "Cde: A tool for creating portable experimental software packages," <em>Computing in Science &amp; Engineering</em>, vol. 14, no. 4, pp. 32–35, 2012.</li>
<li>[18] K. Arulkumaran, "Fglab: Machine learning dashboard," <em>Faculty of Engineering</em>, 2016.</li>
<li>[19] F. Chirigati, R. Rampin, D. Shasha, and J. Freire, "Reprozip: Computational reproducibility with ease," in <em>Proceedings of the 2016 International Conference on Management of Data</em>. ACM, 2016, pp. 2085–2088.</li>
<li>[20] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2016, pp. 770–778.</li>
<li>[21] M. Z. Afzal, S. Capobianco, M. I. Malik, S. Marinai, T. M. Breuel, A. Dengel, and M. Liwicki, "Deepdocclassifier: Document classification with deep convolutional neural network," in <em>Document Analysis and Recognition (ICDAR), 2015 13th International Conference on</em>. IEEE, 2015, pp. 1111–1115.</li>
<li>[22] V. Pondenkandath, M. Alberti, N. Eichenberger, and M. Liwicki, "Identifying cross-depicted historical motifs," <em>Submitted at ICFHR2018</em>, 2017.</li>
<li>[23] S. Fiel, F. Kleber, M. Diem, V. Christlein, G. Louloudis, S. Nikos, and B. Gatos, "ICDAR2017 Competition on Historical Document Writer Identification (Historical-WI)," in <em>2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</em>. IEEE, nov 2017, pp. 1377–1382.</li>
<li>[24] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin, B. Chen, and Y. Wu, "Learning fine-grained image similarity with deep ranking," in <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 2014, pp. 1386–1393.</li>
<li>[25] V. Pondenkandath, M. Seuret, R. Ingold, M. Z. Afzal, and M. Liwicki, "Exploiting state-of-the-art deep learning methods for document image analysis," in <em>Document Analysis and Recognition (ICDAR), 2017 14th IAPR International Conference on</em>, vol. 5. IEEE, 2017, pp. 30–35.</li>
<li>[26] M. Seuret, M. Alberti, R. Ingold, and M. Liwicki, "Pca-initialized deep neural networks applied to document image analysis," <em>International Conference on Document Analysis and Recognition, 2017</em>, vol. abs/1702.00177, 2017.</li>
<li>[27] M. Alberti, M. Seuret, V. Pondenkandath, R. Ingold, and M. Liwicki, "Historical document image segmentation with lda-initialized deep neural networks," <em>International Conference on Document Analysis and Recognition, International Workshop on Historical Document Imaging and Processing</em>, 2017, 2017.</li>
<li>[28] H. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein, "Visualizing the loss landscape of neural nets," 2017.</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ This naming is specific to PyTorch but the concept generalizes to other frameworks as well.
${ }^{9}$ This is an annoying problem e.g. when using a CNN on MNIST and then switching to CIFAR.
${ }^{10}$ see: https://hub.docker.com/u/divaservices/, this URL is a placeholder and will link to the correct Image in the Camera Ready version
${ }^{11}$ see: https://github.com/DIVA-DIA/DIVAgain/tree/master/ICFHR2018_ DeepDIVA_WriterIdentification
${ }^{12}$ https://www.wasserzeichen-online.de/wzis/struktur.php&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
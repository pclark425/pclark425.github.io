<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1682 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1682</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1682</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-32ceb28e45a445df4d89df281bb0e3ab5aab1a2a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/32ceb28e45a445df4d89df281bb0e3ab5aab1a2a" target="_blank">Domain randomization for transferring deep neural networks from simulation to the real world</a></p>
                <p><strong>Paper Venue:</strong> IEEE/RJS International Conference on Intelligent RObots and Systems</p>
                <p><strong>Paper TL;DR:</strong> This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator, and achieves the first successful transfer of a deep neural network trained only on simulated RGB images to the real world for the purpose of robotic control.</p>
                <p><strong>Paper Abstract:</strong> Bridging the ‘reality gap’ that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1682.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1682.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain Randomization (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that trains deep convolutional object detectors entirely on low-fidelity simulated RGB images with heavy randomization of textures, lighting, camera, and scene composition so that the real world appears as another variation at test time; applied to object localization and grasping on a physical robot with no real-image pretraining or fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Object-detection + Fetch robot grasping pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A deep convolutional neural network (modified VGG-16) that predicts 3D object center coordinates from a single monocular RGB image; outputs used by MoveIt motion planner and a Fetch mobile manipulator to execute grasps.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo (built-in renderer)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>MuJoCo physics engine used to render scenes and simulate rigid-body geometry; renderer is fast, not photo-realistic, providing basic lighting, object geometry, camera viewpoint, and simple specular characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>low-fidelity rendering with approximate physics (MuJoCo built-in renderer not photorealistic)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>rigid-body geometry and placements, basic lighting and specular parameters, camera viewpoint/FOV, simple image noise, object meshes and scene composition</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>photorealistic materials and textures, detailed material reflectance/BRDF, fluid dynamics, nonrigid effects, gear backlash/wear, high-fidelity sensor noise models, exact camera calibration and high-quality rendering</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Webcam images of geometric objects (and YCB objects) on a tabletop at 70–105 cm with uncontrolled lighting and clutter (robot, cables, tape on floor); tests performed on a Fetch robot using MoveIt for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Monocular object localization (x,y,z) from pixels and downstream prescribed grasping in cluttered scenes</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised learning (deep CNN regression) on labeled simulated RGB images</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Localization mean L2 error in cm on real images; grasp success counts in trials (pick success)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Localization error in simulation: ~0.3–0.5 cm (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Localization error on real images: ~1.1–1.9 cm depending on object/condition (average ~1.3 cm object-only, ~1.8 cm with distractors); grasp success: 38/40 successful grasps for two objects (95%), 9/10 successful grasps for a Spam can (90%)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomized: number/shape/position of distractors (0–10), object positions/orientations, textures of objects/table/floor/skybox/robot (random RGB, gradients, checkers; thousands of unique texturizations), number/position/orientation/specular properties of lights, camera position/orientation within a 10×5×10 cm box, camera angle offsets up to 0.1 rad, FOV scaled ±5%, and added image noise.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Reality gap factors cited: low-fidelity image rendering, unmodeled physical effects (nonrigidity, gear backlash, wear-and-tear), differences in sensor noise and camera calibration, and missing complex material/lighting effects.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Extensive visual randomization (particularly many unique textures — performance degrades when <1,000 textures), large number of training examples (good performance from ~5k, improves to ~50k), inclusion of distractors in training to be robust to distractors at test time, camera randomization for modest gains, and pretraining helps under low-data regimes but is not essential with large simulated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Photorealistic rendering is not required; instead extensive variability (many random textures, lighting, distractors, and camera perturbations) is critical. Empirically, ≥1,000 unique textures and tens of thousands of training examples improved transfer; simulation localization need not match real exactly (sim error 0.3–0.5 cm vs real ~1.5 cm still workable).</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>No direct experiment comparing photorealistic vs low-fidelity renderers; ablations show performance strongly depends on texture variety and training set size rather than photorealism (performance drops when textures <1,000).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training on a highly varied, intentionally non-photoreal simulated visual domain (domain randomization) enables deep networks trained only on simulated RGB images to localize objects in the real world with ~1.5 cm accuracy and support high success rates on grasping tasks; critical enablers are very large texture variety, inclusion of distractors, and sufficient dataset size; photorealism and precise simulator/renderer fidelity are not necessary for these vision-based manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Domain randomization for transferring deep neural networks from simulation to the real world', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1682.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1682.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CAD2RL (Sadeghi & Levine)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>(cad)2RL: Real single-image flight without a single real image</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior sim-to-real approach where a policy mapping images to controls was trained in simulation with varied 3D scenes and textures and successfully applied to real-world quadrotor collision avoidance tasks without any real-image training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>(cad) 2 RL: Real single-image flight without a single real image.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Quadrotor collision-avoidance policy</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>An image-to-control policy for a quadrotor trained in simulation to perform collision avoidance in real-world environments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>aerial robotics / navigation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Varied 3D scene simulator (textures dataset of ~200 materials)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulator with varied 3D scenes and textures; textures selected from a dataset of ~200 pre-generated (mostly realistic) materials; used to render monocular images for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>moderate (varied 3D scenes with realistic-ish textures)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>3D scene geometry, textures, and scene-level visual variation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Paper notes not aimed at high-precision tasks — details of vehicle dynamics/precise sensor noise and other physical effects may be simplified or not deeply modeled in discussion here</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real-world hallways and open spaces used for quadrotor flight tests</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Collision avoidance / autonomous flight control</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Policy learning mapping images to controls trained in simulation (as described in referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Qualitative success in real-world collision avoidance experiments (no numeric metrics reported in this paper's discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomized 3D scenes and textures using a set of ~200 pre-generated textured materials; varied scene configurations</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Authors note that their experiments addressed collision-avoidance in hallways/open spaces but did not demonstrate high-precision tasks, implying limitations due to simulation fidelity for precision</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Use of varied scenes and texture randomization with many realistic textures enabled transfer for collision avoidance</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Photorealistic textures help for navigation tasks, but high-precision dynamics or sensor modeling were not demonstrated as required for their tasks; the paper emphasized varied visual conditions rather than perfectly accurate dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sim-to-real transfer of image-to-control policies for quadrotor collision avoidance can succeed using simulated visual variability (varied scenes/textures) without real-image training, though such approaches were not shown to handle high-precision tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Domain randomization for transferring deep neural networks from simulation to the real world', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1682.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1682.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mordatch et al. (ensemble dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensemblecio: Full-body dynamic motion planning that transfers to physical humanoids.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work demonstrating that training controllers on an ensemble of varied dynamics models produces controllers robust to modeling error and improves transfer to real robots (full-body humanoids).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ensemblecio: Full-body dynamic motion planning that transfers to physical humanoids.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Full-body humanoid controllers (ensemble-trained policies)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Policies trained on an ensemble of dynamics models for full-body dynamic motion planning that aim to transfer to physical humanoid robots.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics — locomotion / full-body dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Ensemble of dynamics models / physics simulators</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Multiple physics models/simulators with randomized dynamics parameters used as an ensemble during training to expose the controller to a range of dynamics behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>varied/ensemble dynamics (not necessarily high-fidelity single model, but ensemble captures parameter uncertainty)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Contact dynamics and dynamics variability across models (ensemble captures modeling uncertainty)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Real-world unmodeled effects may remain; single-model fidelity not guaranteed</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical humanoid robots (real hardware execution to evaluate transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Full-body dynamic motion planning / control behaviors for humanoid robots</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Policy learning on an ensemble of dynamics models</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomization over dynamics parameters and training on an ensemble of models to increase robustness to modeling error</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Modeling error in dynamics; contact properties variability</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Training across an ensemble of dynamics models which makes the learned controller robust to modeling inaccuracies</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Accurate single-model fidelity is less critical if training covers an ensemble that includes modeled variations; robustness to parameter uncertainty is key</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Controller robustness to modeling error (via training on an ensemble of dynamics models) improves sim-to-real transfer for complex dynamic tasks on humanoids.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Domain randomization for transferring deep neural networks from simulation to the real world', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1682.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1682.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rajeswaran et al. (EPOpt / ensembles)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Epop: Learning robust neural network policies using model ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work exploring training strategies on ensembles of models (including adversarial training and adapting the ensemble using target-domain data) to learn robust policies; in the referenced study they investigated methods but did not demonstrate successful real-world transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Epop: Learning robust neural network policies using model ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Policies trained with ensemble strategies</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Neural network policies trained across ensembles of simulated models, with strategies including adversarial training and ensemble adaptation to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics / control</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Ensemble simulation models</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Training on a diverse distribution (ensemble) of simulated dynamics models with possible adversarial selection</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>ensemble of approximated dynamics models</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Variability in dynamics parameters across the ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>No demonstration of bridging all real-world effects; real-world transfer not shown in the cited work according to this paper</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Not demonstrated in real world in cited experiments (per this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Robust control policies under dynamics uncertainty (investigated)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Policy learning on model ensembles, including adversarial training and ensemble adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Training across ensembles of dynamics models and exploring adversarial ensemble distributions and adaptation strategies</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Challenges include whether ensemble coverage and adaptation suffice to cover real-world dynamics; cited work did not present real-world transfer evidence</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Potentially improved robustness via ensembles and adversarial training, but practical transfer requires further validation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Ensemble-based training strategies (including adversarial objectives) are promising for robustness to modeling error, but the referenced work did not demonstrate successful sim-to-real transfer in the real world per the discussion in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Domain randomization for transferring deep neural networks from simulation to the real world', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1682.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1682.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yu et al. (universal policy + online ID)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Preparing for the unknown: Learning a universal policy with online system identification.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that trains policies on varied physics models and uses online system identification from trajectories to select appropriate behaviors for the real system; reported in the literature but not shown to succeed in the real world in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Preparing for the unknown: Learning a universal policy with online system identification.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Universal policy with online system identification</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A policy architecture trained under varied physics conditions that relies on online system identification of dynamics from observed trajectories to choose appropriate control behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics / adaptive control</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Varied-physics simulator(s)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulators with randomized physics parameters used to train a universal policy and an online system identification module</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>varied/ensemble physics models (approximate dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Different physics parameterizations to represent possible real-world dynamics variations</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Real-world deployment success not demonstrated in cited work according to this paper</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Not demonstrated successfully in the real world in the referenced work (per this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Adaptive behaviors via online dynamics identification</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Policy learning with auxiliary online system-identification mechanism trained on varied physics</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Training on varied physics models to support online system identification</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Uncertainty whether simulated variations capture the full real-world dynamics; real-world success was not shown in cited work</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Ability to perform online system identification from trajectories could enable adaptation to unmodeled real dynamics if identification is reliable</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training on varied physics plus online system identification is a proposed route to handle dynamics mismatch, but cited results did not demonstrate real-world success according to this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Domain randomization for transferring deep neural networks from simulation to the real world', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1682.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1682.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rusu et al. (progressive nets)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-real robot learning from pixels with progressive nets.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach using progressive neural network architectures to adapt models pretrained on simulated pixels to real-world images; reported to have better sample efficiency than fine-tuning or training from scratch in the real world.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-real robot learning from pixels with progressive nets.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Progressive-net adapted visuomotor policies</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Visual-to-action policies that leverage progressive network transfer from simulation-pretrained visual representations to real-world tasks, improving sample efficiency during real-world adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics — visuomotor control</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Simulator for rendered pixels (unspecified here)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulation used to render pixels for pretraining perception components; progressive nets then used to adapt to real-world input</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>simulated visual renderings (fidelity unspecified in this paper's discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rendered pixel-level images for visual pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>May rely on additional real data for adaptation; not described in full here</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real-world robotic tasks requiring adaptation from simulation to real pixels</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Visuomotor policies from pixels (simulation pretraining then adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Pretraining in simulation + progressive network adaptation with real data</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Sample efficiency (qualitative claim of better efficiency than fine-tuning or training from scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Need for real-world labeled or reward data for adaptation; sample-efficiency challenges</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Architectural transfer (progressive nets) enabling reuse of simulation-learned features reduces required real data</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Adaptation with real-world samples (progressive network lateral connections), details not specified in this paper's discussion</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Progressive neural networks can be used to adapt simulation-pretrained visual models to real images with improved sample efficiency relative to naive fine-tuning or training purely from real data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Domain randomization for transferring deep neural networks from simulation to the real world', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1682.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1682.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mitash et al. (pretrain with realistic renders)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A self-supervised learning system for object detection using physics simulation and multi-view pose estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work that pretrains an object detector on realistic rendered images with randomized lighting from 3D models to bootstrap an automated real-data labeling and learning process, enabling learning with a small number (~500) real-world samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A selfsupervised learning system for object detection using physics simulation and multi-view pose estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Object detector pretraining + self-supervised real-world bootstrap</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>An object detection pipeline pretrained on realistic renders and then used in a self-supervised pipeline with multi-view pose estimation to reduce or eliminate manual real-world labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics / object detection and pose estimation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>3D model renderer with randomized lighting</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Rendering of 3D models with randomized lighting conditions to generate pretraining images for detectors</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>higher visual fidelity renders (realistic lighting) compared to purely synthetic non-realistic textures</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Realistic rendered appearance and lighting variation for pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>May still simplify other sensor or dynamics aspects; requires some real-world multi-view data to bootstrap</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real-world data acquired in a self-supervised manner (multi-view) for fine-tuning and pose estimation</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Object detection and pose estimation with minimal manual labeling</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Pretraining on realistic synthetic renders followed by self-supervised learning with a small number of real samples</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Reported pipeline required only ~500 real-world samples for bootstrapping (per this paper's summary)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomized lighting in rendered images and use of 3D models to produce varied pretraining examples</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Need for realistic rendering and multi-view consistency to reduce reliance on manual labels; some real data still required</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Using realistic renders and randomized lighting plus multi-view self-supervision reduces real-label requirements</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Higher visual fidelity (realistic lighting) used to bootstrap automated learning; some real-world samples (~500) still used</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Self-supervised learning with multi-view pose estimation using about 500 real-world samples (per this paper's summary)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining object detectors on realistic rendered images with randomized lighting can significantly reduce the amount of real labeled data required by enabling self-supervised bootstrapping; this approach used a modest number (~500) of real samples in the referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Domain randomization for transferring deep neural networks from simulation to the real world', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>(cad) 2 RL: Real single-image flight without a single real image. <em>(Rating: 2)</em></li>
                <li>Ensemblecio: Full-body dynamic motion planning that transfers to physical humanoids. <em>(Rating: 2)</em></li>
                <li>Epop: Learning robust neural network policies using model ensembles. <em>(Rating: 2)</em></li>
                <li>Preparing for the unknown: Learning a universal policy with online system identification. <em>(Rating: 2)</em></li>
                <li>Sim-to-real robot learning from pixels with progressive nets. <em>(Rating: 2)</em></li>
                <li>A selfsupervised learning system for object detection using physics simulation and multi-view pose estimation. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1682",
    "paper_id": "paper-32ceb28e45a445df4d89df281bb0e3ab5aab1a2a",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Domain Randomization (this work)",
            "name_full": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World",
            "brief_description": "A method that trains deep convolutional object detectors entirely on low-fidelity simulated RGB images with heavy randomization of textures, lighting, camera, and scene composition so that the real world appears as another variation at test time; applied to object localization and grasping on a physical robot with no real-image pretraining or fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Object-detection + Fetch robot grasping pipeline",
            "agent_system_description": "A deep convolutional neural network (modified VGG-16) that predicts 3D object center coordinates from a single monocular RGB image; outputs used by MoveIt motion planner and a Fetch mobile manipulator to execute grasps.",
            "domain": "general robotics manipulation",
            "virtual_environment_name": "MuJoCo (built-in renderer)",
            "virtual_environment_description": "MuJoCo physics engine used to render scenes and simulate rigid-body geometry; renderer is fast, not photo-realistic, providing basic lighting, object geometry, camera viewpoint, and simple specular characteristics.",
            "simulation_fidelity_level": "low-fidelity rendering with approximate physics (MuJoCo built-in renderer not photorealistic)",
            "fidelity_aspects_modeled": "rigid-body geometry and placements, basic lighting and specular parameters, camera viewpoint/FOV, simple image noise, object meshes and scene composition",
            "fidelity_aspects_simplified": "photorealistic materials and textures, detailed material reflectance/BRDF, fluid dynamics, nonrigid effects, gear backlash/wear, high-fidelity sensor noise models, exact camera calibration and high-quality rendering",
            "real_environment_description": "Webcam images of geometric objects (and YCB objects) on a tabletop at 70–105 cm with uncontrolled lighting and clutter (robot, cables, tape on floor); tests performed on a Fetch robot using MoveIt for planning.",
            "task_or_skill_transferred": "Monocular object localization (x,y,z) from pixels and downstream prescribed grasping in cluttered scenes",
            "training_method": "Supervised learning (deep CNN regression) on labeled simulated RGB images",
            "transfer_success_metric": "Localization mean L2 error in cm on real images; grasp success counts in trials (pick success)",
            "transfer_performance_sim": "Localization error in simulation: ~0.3–0.5 cm (reported)",
            "transfer_performance_real": "Localization error on real images: ~1.1–1.9 cm depending on object/condition (average ~1.3 cm object-only, ~1.8 cm with distractors); grasp success: 38/40 successful grasps for two objects (95%), 9/10 successful grasps for a Spam can (90%)",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomized: number/shape/position of distractors (0–10), object positions/orientations, textures of objects/table/floor/skybox/robot (random RGB, gradients, checkers; thousands of unique texturizations), number/position/orientation/specular properties of lights, camera position/orientation within a 10×5×10 cm box, camera angle offsets up to 0.1 rad, FOV scaled ±5%, and added image noise.",
            "sim_to_real_gap_factors": "Reality gap factors cited: low-fidelity image rendering, unmodeled physical effects (nonrigidity, gear backlash, wear-and-tear), differences in sensor noise and camera calibration, and missing complex material/lighting effects.",
            "transfer_enabling_conditions": "Extensive visual randomization (particularly many unique textures — performance degrades when &lt;1,000 textures), large number of training examples (good performance from ~5k, improves to ~50k), inclusion of distractors in training to be robust to distractors at test time, camera randomization for modest gains, and pretraining helps under low-data regimes but is not essential with large simulated datasets.",
            "fidelity_requirements_identified": "Photorealistic rendering is not required; instead extensive variability (many random textures, lighting, distractors, and camera perturbations) is critical. Empirically, ≥1,000 unique textures and tens of thousands of training examples improved transfer; simulation localization need not match real exactly (sim error 0.3–0.5 cm vs real ~1.5 cm still workable).",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": "No direct experiment comparing photorealistic vs low-fidelity renderers; ablations show performance strongly depends on texture variety and training set size rather than photorealism (performance drops when textures &lt;1,000).",
            "key_findings": "Training on a highly varied, intentionally non-photoreal simulated visual domain (domain randomization) enables deep networks trained only on simulated RGB images to localize objects in the real world with ~1.5 cm accuracy and support high success rates on grasping tasks; critical enablers are very large texture variety, inclusion of distractors, and sufficient dataset size; photorealism and precise simulator/renderer fidelity are not necessary for these vision-based manipulation tasks.",
            "uuid": "e1682.0",
            "source_info": {
                "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "CAD2RL (Sadeghi & Levine)",
            "name_full": "(cad)2RL: Real single-image flight without a single real image",
            "brief_description": "A prior sim-to-real approach where a policy mapping images to controls was trained in simulation with varied 3D scenes and textures and successfully applied to real-world quadrotor collision avoidance tasks without any real-image training data.",
            "citation_title": "(cad) 2 RL: Real single-image flight without a single real image.",
            "mention_or_use": "mention",
            "agent_system_name": "Quadrotor collision-avoidance policy",
            "agent_system_description": "An image-to-control policy for a quadrotor trained in simulation to perform collision avoidance in real-world environments.",
            "domain": "aerial robotics / navigation",
            "virtual_environment_name": "Varied 3D scene simulator (textures dataset of ~200 materials)",
            "virtual_environment_description": "Simulator with varied 3D scenes and textures; textures selected from a dataset of ~200 pre-generated (mostly realistic) materials; used to render monocular images for policy learning.",
            "simulation_fidelity_level": "moderate (varied 3D scenes with realistic-ish textures)",
            "fidelity_aspects_modeled": "3D scene geometry, textures, and scene-level visual variation",
            "fidelity_aspects_simplified": "Paper notes not aimed at high-precision tasks — details of vehicle dynamics/precise sensor noise and other physical effects may be simplified or not deeply modeled in discussion here",
            "real_environment_description": "Real-world hallways and open spaces used for quadrotor flight tests",
            "task_or_skill_transferred": "Collision avoidance / autonomous flight control",
            "training_method": "Policy learning mapping images to controls trained in simulation (as described in referenced work)",
            "transfer_success_metric": "Qualitative success in real-world collision avoidance experiments (no numeric metrics reported in this paper's discussion)",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomized 3D scenes and textures using a set of ~200 pre-generated textured materials; varied scene configurations",
            "sim_to_real_gap_factors": "Authors note that their experiments addressed collision-avoidance in hallways/open spaces but did not demonstrate high-precision tasks, implying limitations due to simulation fidelity for precision",
            "transfer_enabling_conditions": "Use of varied scenes and texture randomization with many realistic textures enabled transfer for collision avoidance",
            "fidelity_requirements_identified": "Photorealistic textures help for navigation tasks, but high-precision dynamics or sensor modeling were not demonstrated as required for their tasks; the paper emphasized varied visual conditions rather than perfectly accurate dynamics.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Sim-to-real transfer of image-to-control policies for quadrotor collision avoidance can succeed using simulated visual variability (varied scenes/textures) without real-image training, though such approaches were not shown to handle high-precision tasks.",
            "uuid": "e1682.1",
            "source_info": {
                "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Mordatch et al. (ensemble dynamics)",
            "name_full": "Ensemblecio: Full-body dynamic motion planning that transfers to physical humanoids.",
            "brief_description": "Work demonstrating that training controllers on an ensemble of varied dynamics models produces controllers robust to modeling error and improves transfer to real robots (full-body humanoids).",
            "citation_title": "Ensemblecio: Full-body dynamic motion planning that transfers to physical humanoids.",
            "mention_or_use": "mention",
            "agent_system_name": "Full-body humanoid controllers (ensemble-trained policies)",
            "agent_system_description": "Policies trained on an ensemble of dynamics models for full-body dynamic motion planning that aim to transfer to physical humanoid robots.",
            "domain": "robotics — locomotion / full-body dynamics",
            "virtual_environment_name": "Ensemble of dynamics models / physics simulators",
            "virtual_environment_description": "Multiple physics models/simulators with randomized dynamics parameters used as an ensemble during training to expose the controller to a range of dynamics behaviors.",
            "simulation_fidelity_level": "varied/ensemble dynamics (not necessarily high-fidelity single model, but ensemble captures parameter uncertainty)",
            "fidelity_aspects_modeled": "Contact dynamics and dynamics variability across models (ensemble captures modeling uncertainty)",
            "fidelity_aspects_simplified": "Real-world unmodeled effects may remain; single-model fidelity not guaranteed",
            "real_environment_description": "Physical humanoid robots (real hardware execution to evaluate transfer)",
            "task_or_skill_transferred": "Full-body dynamic motion planning / control behaviors for humanoid robots",
            "training_method": "Policy learning on an ensemble of dynamics models",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomization over dynamics parameters and training on an ensemble of models to increase robustness to modeling error",
            "sim_to_real_gap_factors": "Modeling error in dynamics; contact properties variability",
            "transfer_enabling_conditions": "Training across an ensemble of dynamics models which makes the learned controller robust to modeling inaccuracies",
            "fidelity_requirements_identified": "Accurate single-model fidelity is less critical if training covers an ensemble that includes modeled variations; robustness to parameter uncertainty is key",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Controller robustness to modeling error (via training on an ensemble of dynamics models) improves sim-to-real transfer for complex dynamic tasks on humanoids.",
            "uuid": "e1682.2",
            "source_info": {
                "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Rajeswaran et al. (EPOpt / ensembles)",
            "name_full": "Epop: Learning robust neural network policies using model ensembles.",
            "brief_description": "Work exploring training strategies on ensembles of models (including adversarial training and adapting the ensemble using target-domain data) to learn robust policies; in the referenced study they investigated methods but did not demonstrate successful real-world transfer.",
            "citation_title": "Epop: Learning robust neural network policies using model ensembles.",
            "mention_or_use": "mention",
            "agent_system_name": "Policies trained with ensemble strategies",
            "agent_system_description": "Neural network policies trained across ensembles of simulated models, with strategies including adversarial training and ensemble adaptation to improve robustness.",
            "domain": "robotics / control",
            "virtual_environment_name": "Ensemble simulation models",
            "virtual_environment_description": "Training on a diverse distribution (ensemble) of simulated dynamics models with possible adversarial selection",
            "simulation_fidelity_level": "ensemble of approximated dynamics models",
            "fidelity_aspects_modeled": "Variability in dynamics parameters across the ensemble",
            "fidelity_aspects_simplified": "No demonstration of bridging all real-world effects; real-world transfer not shown in the cited work according to this paper",
            "real_environment_description": "Not demonstrated in real world in cited experiments (per this paper)",
            "task_or_skill_transferred": "Robust control policies under dynamics uncertainty (investigated)",
            "training_method": "Policy learning on model ensembles, including adversarial training and ensemble adaptation",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": false,
            "domain_randomization_used": true,
            "domain_randomization_details": "Training across ensembles of dynamics models and exploring adversarial ensemble distributions and adaptation strategies",
            "sim_to_real_gap_factors": "Challenges include whether ensemble coverage and adaptation suffice to cover real-world dynamics; cited work did not present real-world transfer evidence",
            "transfer_enabling_conditions": "Potentially improved robustness via ensembles and adversarial training, but practical transfer requires further validation",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Ensemble-based training strategies (including adversarial objectives) are promising for robustness to modeling error, but the referenced work did not demonstrate successful sim-to-real transfer in the real world per the discussion in this paper.",
            "uuid": "e1682.3",
            "source_info": {
                "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Yu et al. (universal policy + online ID)",
            "name_full": "Preparing for the unknown: Learning a universal policy with online system identification.",
            "brief_description": "An approach that trains policies on varied physics models and uses online system identification from trajectories to select appropriate behaviors for the real system; reported in the literature but not shown to succeed in the real world in the cited work.",
            "citation_title": "Preparing for the unknown: Learning a universal policy with online system identification.",
            "mention_or_use": "mention",
            "agent_system_name": "Universal policy with online system identification",
            "agent_system_description": "A policy architecture trained under varied physics conditions that relies on online system identification of dynamics from observed trajectories to choose appropriate control behaviors.",
            "domain": "robotics / adaptive control",
            "virtual_environment_name": "Varied-physics simulator(s)",
            "virtual_environment_description": "Simulators with randomized physics parameters used to train a universal policy and an online system identification module",
            "simulation_fidelity_level": "varied/ensemble physics models (approximate dynamics)",
            "fidelity_aspects_modeled": "Different physics parameterizations to represent possible real-world dynamics variations",
            "fidelity_aspects_simplified": "Real-world deployment success not demonstrated in cited work according to this paper",
            "real_environment_description": "Not demonstrated successfully in the real world in the referenced work (per this paper)",
            "task_or_skill_transferred": "Adaptive behaviors via online dynamics identification",
            "training_method": "Policy learning with auxiliary online system-identification mechanism trained on varied physics",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": false,
            "domain_randomization_used": true,
            "domain_randomization_details": "Training on varied physics models to support online system identification",
            "sim_to_real_gap_factors": "Uncertainty whether simulated variations capture the full real-world dynamics; real-world success was not shown in cited work",
            "transfer_enabling_conditions": "Ability to perform online system identification from trajectories could enable adaptation to unmodeled real dynamics if identification is reliable",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Training on varied physics plus online system identification is a proposed route to handle dynamics mismatch, but cited results did not demonstrate real-world success according to this paper.",
            "uuid": "e1682.4",
            "source_info": {
                "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Rusu et al. (progressive nets)",
            "name_full": "Sim-to-real robot learning from pixels with progressive nets.",
            "brief_description": "An approach using progressive neural network architectures to adapt models pretrained on simulated pixels to real-world images; reported to have better sample efficiency than fine-tuning or training from scratch in the real world.",
            "citation_title": "Sim-to-real robot learning from pixels with progressive nets.",
            "mention_or_use": "mention",
            "agent_system_name": "Progressive-net adapted visuomotor policies",
            "agent_system_description": "Visual-to-action policies that leverage progressive network transfer from simulation-pretrained visual representations to real-world tasks, improving sample efficiency during real-world adaptation.",
            "domain": "robotics — visuomotor control",
            "virtual_environment_name": "Simulator for rendered pixels (unspecified here)",
            "virtual_environment_description": "Simulation used to render pixels for pretraining perception components; progressive nets then used to adapt to real-world input",
            "simulation_fidelity_level": "simulated visual renderings (fidelity unspecified in this paper's discussion)",
            "fidelity_aspects_modeled": "Rendered pixel-level images for visual pretraining",
            "fidelity_aspects_simplified": "May rely on additional real data for adaptation; not described in full here",
            "real_environment_description": "Real-world robotic tasks requiring adaptation from simulation to real pixels",
            "task_or_skill_transferred": "Visuomotor policies from pixels (simulation pretraining then adaptation)",
            "training_method": "Pretraining in simulation + progressive network adaptation with real data",
            "transfer_success_metric": "Sample efficiency (qualitative claim of better efficiency than fine-tuning or training from scratch)",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Need for real-world labeled or reward data for adaptation; sample-efficiency challenges",
            "transfer_enabling_conditions": "Architectural transfer (progressive nets) enabling reuse of simulation-learned features reduces required real data",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Adaptation with real-world samples (progressive network lateral connections), details not specified in this paper's discussion",
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Progressive neural networks can be used to adapt simulation-pretrained visual models to real images with improved sample efficiency relative to naive fine-tuning or training purely from real data.",
            "uuid": "e1682.5",
            "source_info": {
                "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Mitash et al. (pretrain with realistic renders)",
            "name_full": "A self-supervised learning system for object detection using physics simulation and multi-view pose estimation.",
            "brief_description": "Work that pretrains an object detector on realistic rendered images with randomized lighting from 3D models to bootstrap an automated real-data labeling and learning process, enabling learning with a small number (~500) real-world samples.",
            "citation_title": "A selfsupervised learning system for object detection using physics simulation and multi-view pose estimation.",
            "mention_or_use": "mention",
            "agent_system_name": "Object detector pretraining + self-supervised real-world bootstrap",
            "agent_system_description": "An object detection pipeline pretrained on realistic renders and then used in a self-supervised pipeline with multi-view pose estimation to reduce or eliminate manual real-world labeling.",
            "domain": "robotics / object detection and pose estimation",
            "virtual_environment_name": "3D model renderer with randomized lighting",
            "virtual_environment_description": "Rendering of 3D models with randomized lighting conditions to generate pretraining images for detectors",
            "simulation_fidelity_level": "higher visual fidelity renders (realistic lighting) compared to purely synthetic non-realistic textures",
            "fidelity_aspects_modeled": "Realistic rendered appearance and lighting variation for pretraining",
            "fidelity_aspects_simplified": "May still simplify other sensor or dynamics aspects; requires some real-world multi-view data to bootstrap",
            "real_environment_description": "Real-world data acquired in a self-supervised manner (multi-view) for fine-tuning and pose estimation",
            "task_or_skill_transferred": "Object detection and pose estimation with minimal manual labeling",
            "training_method": "Pretraining on realistic synthetic renders followed by self-supervised learning with a small number of real samples",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": "Reported pipeline required only ~500 real-world samples for bootstrapping (per this paper's summary)",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomized lighting in rendered images and use of 3D models to produce varied pretraining examples",
            "sim_to_real_gap_factors": "Need for realistic rendering and multi-view consistency to reduce reliance on manual labels; some real data still required",
            "transfer_enabling_conditions": "Using realistic renders and randomized lighting plus multi-view self-supervision reduces real-label requirements",
            "fidelity_requirements_identified": "Higher visual fidelity (realistic lighting) used to bootstrap automated learning; some real-world samples (~500) still used",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Self-supervised learning with multi-view pose estimation using about 500 real-world samples (per this paper's summary)",
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Pretraining object detectors on realistic rendered images with randomized lighting can significantly reduce the amount of real labeled data required by enabling self-supervised bootstrapping; this approach used a modest number (~500) of real samples in the referenced work.",
            "uuid": "e1682.6",
            "source_info": {
                "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
                "publication_date_yy_mm": "2017-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "(cad) 2 RL: Real single-image flight without a single real image.",
            "rating": 2,
            "sanitized_title": "cad_2_rl_real_singleimage_flight_without_a_single_real_image"
        },
        {
            "paper_title": "Ensemblecio: Full-body dynamic motion planning that transfers to physical humanoids.",
            "rating": 2,
            "sanitized_title": "ensemblecio_fullbody_dynamic_motion_planning_that_transfers_to_physical_humanoids"
        },
        {
            "paper_title": "Epop: Learning robust neural network policies using model ensembles.",
            "rating": 2,
            "sanitized_title": "epop_learning_robust_neural_network_policies_using_model_ensembles"
        },
        {
            "paper_title": "Preparing for the unknown: Learning a universal policy with online system identification.",
            "rating": 2,
            "sanitized_title": "preparing_for_the_unknown_learning_a_universal_policy_with_online_system_identification"
        },
        {
            "paper_title": "Sim-to-real robot learning from pixels with progressive nets.",
            "rating": 2,
            "sanitized_title": "simtoreal_robot_learning_from_pixels_with_progressive_nets"
        },
        {
            "paper_title": "A selfsupervised learning system for object detection using physics simulation and multi-view pose estimation.",
            "rating": 2,
            "sanitized_title": "a_selfsupervised_learning_system_for_object_detection_using_physics_simulation_and_multiview_pose_estimation"
        }
    ],
    "cost": 0.0194385,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World</h1>
<p>Josh Tobin^{1}, Rachel Fong^{2}, Alex Ray^{2}, Jonas Schneider^{2}, Wojciech Zaremba^{2}, Pieter Abbeel^{3}
[^{1}OpenAI and UC Berkeley EECS, josh@openai.com^{2}OpenAI, {rfong, aray, jonas, woj}@openai.com^{3}OpenAI, UC Berkeley EECS &amp; ICSI, pieter@openai.com]</p>
<h6>Abstract</h6>
<p>Bridging the ‘reality gap’ that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores <em>domain randomization</em>, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained <em>only</em> on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.</p>
<h2>I. INTRODUCTION</h2>
<p>Performing robotic learning in a physics simulator could accelerate the impact of machine learning on robotics by allowing faster, more scalable, and lower-cost data collection than is possible with physical robots. Learning in simulation is especially promising for building on recent results using deep reinforcement learning to achieve human-level performance on tasks like Atari <em>[27]</em> and robotic control <em>[21, 38]</em>. Deep reinforcement learning employs random exploration, which can be dangerous on physical hardware. It often requires hundreds of thousands or millions of samples <em>[27]</em>, which could take thousands of hours to collect, making it impractical for many applications. Ideally, we could learn policies that encode complex behaviors entirely in simulation and successfully run those policies on physical robots with minimal additional training.</p>
<p>Unfortunately, discrepancies between physics simulators and the real world make transferring behaviors from simulation challenging. <em>System identification</em>, the process of tuning the parameters of the simulation to match the behavior of the physical system, is time-consuming and error-prone. Even with strong system identification, the real world has <em>unmodeled physical effects</em> like nonrigidity, gear backlash, wear-and-tear, and fluid dynamics that are not captured by current physics simulators. Furthermore, <em>low-fidelity simulated sensors</em> like image renderers are often unable to reproduce the richness and noise produced by their real-world counterparts. These differences, known collectively as the <em>reality gap</em>, form the barrier to using simulated data on real robots.</p>
<p>This paper explores <em>domain randomization</em>, a simple but promising method for addressing the reality gap. Instead of training a model on a single simulated environment, we randomize the simulator to expose the model to a wide range of environments at training time. The purpose of this work is to test the following hypothesis: if the variability in simulation is significant enough, models trained in simulation will generalize to the real world with no additional training.</p>
<p>Though in principle domain randomization could be applied to any component of the reality gap, we focus on the challenge of transferring from low-fidelity simulated camera images. Robotic control from camera pixels is attractive due to the low cost of cameras and the rich data they provide, but challenging because it involves processing high-dimensional input data. Recent work has shown that supervised learning with deep neural networks is a powerful tool for learning generalizable representations from high-dimensional inputs <em>[20]</em>, but deep learning relies on a large amount of labeled data. Labeled data is difficult to obtain in the real world for precise robotic manipulation behaviors, but it is easy to generate in a physics simulator.</p>
<p>We focus on the task of training a neural network to</p>
<p>detect the location of an object. Object localization from pixels is a well-studied problem in robotics, and state-of-the-art methods employ complex, hand-engineered image processing pipelines (e.g., [6], [5], [44]). This work is a first step toward the goal of using deep learning to improve the accuracy of object detection pipelines. Moreover, we see sim-to-real transfer for object localization as a stepping stone to transferring general-purpose manipulation behaviors.</p>
<p>We find that for a range of geometric objects, we are able to train a detector that is accurate to around 1.5 cm in the real world using only simulated data rendered with simple, algorithmically generated textures. Although previous work demonstrated the ability to perform robotic control using a neural network pretrained on ImageNet and fine-tuned on randomized rendered pixels [37], this paper provides the first demonstration that domain randomization can be useful for robotic tasks requiring precision. We also provide an ablation study of the impact of different choices of randomization and training method on the success of transfer. We find that with a sufficient number of textures, pre-training the object detector using real images is unnecessary. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images to the real world for the purpose of robotic control.</p>
<h2>II. RELATED WORK</h2>
<h2>A. Object detection and pose estimation for robotics</h2>
<p>Object detection and pose estimation for robotics is a wellstudied problem in the literature (see, e.g., [4], [5], [6], [10], [44], [50], [54]). Recent approaches typically involve offline construction or learning of a 3D model of objects in the scene (e.g., a full 3D mesh model [44] or a 3D metric feature representation [5]). At test time, features from the test data (e.g., Scale-Invariant Feature Transform [SIFT] features [12] or color co-occurrence histograms [10]) are matched with the 3D models (or features from the 3D models). For example, a black-box nonlinear optimization algorithm can be used to minimize the re-projection error of the SIFT points from the object model and the 2D points in the test image [4]. Most successful approaches rely on using multiple camera frames [6] or depth information [44]. There has also been some success with only monocular camera images [4].</p>
<p>Compared to our method, traditional approaches require less extensive training and take advantage of richer sensory data, allowing them to detect the full 3D pose of objects (position and orientation) without any assumptions about the location or size of the surface on which the objects are placed. However, our approach avoids the challenging problem of 3D reconstruction, and employs a simple, easy to implement deep learning-based pipeline that may scale better to more challenging problems.</p>
<h2>B. Domain adaptation</h2>
<p>The computer vision community has devoted significant study to the problem of adapting vision-based models trained in a source domain to a previously unseen target domain (see, e.g., [9], [14], [15], [19], [23], [25], [51]). A variety of approaches have been proposed, including re-training the model in the target domain (e.g., [52]), adapting the weights of the model based on the statistics of the source and target domains (e.g., [22]), learning invariant features between domains (e.g., [47]), and learning a mapping from the target domain to the source domain (e.g., [43]). Researchers in the reinforcement learning community have also studied the problem of domain adaptation by learning invariant feature representations [13], adapting pretrained networks [35], and other methods. See [13] for a more complete treatment of domain adaptation in the reinforcement learning literature.</p>
<p>In this paper we study the possibility of transfer from simulation to the real world without performing domain adaptation.</p>
<h2>C. Bridging the reality gap</h2>
<p>Previous work on leveraging simulated data for physical robotic experiments explored several strategies for bridging the reality gap.</p>
<p>One approach is to make the simulator closely match the physical reality by performing system identification and using high-quality rendering. Though using realistic RGB rendering alone has had limited success for transferring to real robotic tasks [16], incorporating realistic simulation of depth information can allow models trained on rendered images to transfer reasonably well to the real world [32]. Combining data from high-quality simulators with other approaches like fine-tuning can also reduce the number of labeled samples required in the real world [34].</p>
<p>Unlike these approaches, ours allows the use of lowquality renderers optimized for speed and not carefully matched to real-world textures, lighting, and scene configurations.</p>
<p>Other work explores using domain adaptation techniques to bridge the reality gap. It is often faster to fine-tune a controller learned in simulation than to learn from scratch in the real world [7], [18]. In [11], the authors use a variational autoencoder trained on simulated data to encode trajectories of motor outputs corresponding to a desired behavior type (e.g., reaching, grasping) as a low-dimensional latent code. A policy is learned on real data mapping features to distributions over latent codes. The learned policy overcomes the reality gap by choosing latent codes that correspond to the desired physical behavior via exploration.</p>
<p>Domain adaptation has also been applied to robotic vision. Rusu et al. [36] explore using the progressive network architecture to adapt a model that is pre-trained on simulated pixels, and find it has better sample efficiency than finetuning or training in the real-world alone. In [46], the authors explore learning a correspondence between domains that allows the real images to be mapped into a space understood by the model. While both of the preceding approaches require reward functions or labeled data, which can be difficult to obtain in the real world, Mitash and collaborators [26] explore pretraining an object detector using realistic rendered images with randomized lighting from 3D models to bootstrap an automated learning learning process that does</p>
<p>not require manually labeling data and uses only around 500 real-world samples.</p>
<p>A related idea, iterative learning control, employs real-world data to improve the dynamics model used to determine the optimal control behavior, rather than using real-world data to improve the controller directly. Iterative learning control starts with a dynamics model, applies the corresponding control behavior on the real system, and then closes the loop by using the resulting data to improve the dynamics model. Iterative learning control has been applied to a variety of robotic control problems, from model car control (e.g., [1] and [8]) to surgical robotics (e.g., [48]).</p>
<p>Domain adaptation and iterative learning control are important tools for addressing the reality gap, but in contrast to these approaches, ours requires no additional training on real-world data. Our method can also be combined easily with most domain adaptation techniques.</p>
<p>Several authors have previously explored the idea of using domain randomization to bridge the reality gap.</p>
<p>In the context of physics adaptation, Mordatch and collaborators [28] show that training a policy on an ensemble of dynamics models can make the controller robust to modeling error and improve transfer to a real robot. Similarly, in [2], the authors train a policy to pivot a tool held in the robot’s gripper in a simulator with randomized friction and action delays, and find that it works in the real world and is robust to errors in estimation of the system parameters.</p>
<p>Rather than relying on controller robustness, Yu et al. [53] use a model trained on varied physics to perform system identification using online trajectory data, but their approach is not shown to succeed in the real world. Rajeswaran et al. [33] explore different training strategies for learning from an ensemble of models, including adversarial training and adapting the ensemble distribution using data from the target domain, but also do not demonstrate successful real-world transfer.</p>
<p>Researchers in computer vision have used 3D models as a tool to improve performance on real images since the earliest days of the field (e.g., [30], [24]). More recently, 3D models have been used to augment training data to aid transferring deep neural networks between datasets and prevent over-fitting on small datasets for tasks like viewpoint estimation [40] and object detection [42], [29]. Recent work has explored using only synthetic data for training 2D object detectors (i.e., predicting a bounding box for objects in the scene). In [31], the authors find that by pretraining a network on ImageNet and fine-tuning on synthetic data created from 3D models, better detection performance on the PASCAL dataset can be achieved than training with only a few labeled examples from the real dataset.</p>
<p>In contrast to our work, most object detection results in computer vision use realistic textures, but do not create coherent 3D scenes. Instead, objects are rendered against a solid background or a randomly chosen photograph. As a result, our approach allows our models to understand the 3D spatial information necessary for rich interactions with the physical world.</p>
<p>Sadeghi and Levine’s work [37] is the most similar to our own. The authors demonstrate that a policy mapping images to controls learned in a simulator with varied 3D scenes and textures can be applied successfully to real-world quadrotor flight. However, their experiments - collision avoidance in hallways and open spaces - do not demonstrate the ability to deal with high-precision tasks. Our approach also does not rely on precise camera information or calibration, instead randomizing the position, orientation, and field of view of the camera in the simulator. Whereas their approach chooses textures from a dataset of around 200 pre-generated materials, most of which are realistic, our approach is the first to use only non-realistic textures created by a simple random generation process, which allows us to train on hundreds of thousands (or more) of unique texturizations of the scene.</p>
<h2>III. METHOD</h2>
<p>Given some objects of interest $\left{s_{i}\right}<em 0="0">{i}$, our goal is to train an object detector $d\left(I</em>$ of each object. In addition to the objects of interest, our scenes sometimes contain distractor objects that must be ignored by the network. Our approach is to train a deep neural network in simulation using domain randomization. The remainder of this section describes the specific domain randomization and neural network training methodology we use.}\right)$ that maps a single monocular camera frame $I_{0}$ to the Cartesian coordinates $\left{\left(x_{i},y_{i},z_{i}\right)\right}_{i</p>
<h3>III-A Domain randomization</h3>
<p>The purpose of domain randomization is to provide enough simulated variability at training time such that at test time the model is able to generalize to real-world data. We randomize the following aspects of the domain for each sample used during training:</p>
<ul>
<li>Number and shape of distractor objects on the table</li>
<li>Position and texture of all objects on the table</li>
<li>Textures of the table, floor, skybox, and robot</li>
<li>Position, orientation, and field of view of the camera</li>
<li>Number of lights in the scene</li>
<li>Position, orientation, and specular characteristics of the lights</li>
<li>Type and amount of random noise added to images</li>
</ul>
<p>Since we use a single monocular camera image from an uncalibrated camera to estimate object positions, we fix the height of the table in simulation, effectively creating a 2D pose estimation task. Random textures are chosen among the following:
(a) A random RGB value
(b) A gradient between two random RGB values
(c) A checker pattern between two random RGB values</p>
<p>The textures of all objects are chosen uniformly at random - the detector does not have access to the color of the object(s) of interest at training time, only their size and shape. We render images using the MuJoCo Physics Engine’s [45] built-in renderer. This renderer is not intended to be photo-realistic, and physically plausible choices of textures and lighting are not needed.</p>
<p>Between 0 and 10 distractor objects are added to the table in each scene. Distractor objects on the floor or in the background are unnecessary, despite some clutter (e.g., cables) on the floor in our real images.</p>
<p>Our method avoids calibration and precise placement of the camera in the real world by randomizing characteristics of the cameras used to render images in training. We manually place a camera in the simulated scene that approximately matches the viewpoint and field of view of the real camera. Each training sample places the camera randomly within a $(10\times 5\times 10)$ cm box around this initial point. The viewing angle of the camera is calculated analytically to point at a fixed point on the table, and then offset by up to 0.1 radians in each direction. The field of view is also scaled by up to $5\%$ from the starting point.</p>
<h3>III-B Model architecture and training</h3>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 2. The model architecture used in our experiments. Each vertical bar corresponds to a layer of the model. ReLU nonlinearities are used throughout, and max pooling occurs between each of the groupings of convolutional layers. The input is an image from an external webcam downsized to $(224 \times 224)$ and the output of the network predicts the $(x, y, z)$ coordinates of object(s) of interest.</p>
<p>We parametrize our object detector with a deep convolutional neural network. In particular, we use a modified version the VGG-16 architecture [39] shown in Figure 2. We chose this architecture because it performs well on a variety of computer vision tasks, and because it has a wide availability of pretrained weights. We use the standard VGG convolutional layers, but use smaller fully connected layers of sizes 256 and 64 and do not use dropout. For the majority of our experiments, we use weights obtained by pretraining on ImageNet to initialize the convolutional layers, which we hypothesized would be essential to achieving transfer. In practice, we found that using random weight initialization works as well in most cases.</p>
<p>We train the detector through stochastic gradient descent on the $L_{2}$ loss between the object positions estimated by the network and the true object positions using the Adam optimizer [17]. We found that using a learning rate of around $1\mathrm{e}{-4}$ (as opposed to the standard $1\mathrm{e}{-3}$ for Adam) improved convergence and helped avoid a common local optimum, mapping all objects to the center of the table.</p>
<h2>IV. EXPERIMENTS</h2>
<h3>A. Experimental Setup</h3>
<p>We evaluated our approach by training object detectors for each of eight geometric objects. We constructed mesh representations for each object to render in the simulator. Each training sample consists of (a) a rendered image of</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3. The geometric objects used in our experiments.</p>
<p>the object and one or more distractors (also from among the geometric object set) on a simulated tabletop and (b) a label corresponding to the Cartesian coordinates of the center of mass of the object in the world frame.</p>
<p>For each experiment, we performed a small hyperparameter search, evaluating combinations of two learning rates ($1\mathrm{e}{-4}$ and $2\mathrm{e}{-4}$) and three batch sizes (25, 50, and 100). We report the performance of the best network.</p>
<p>The goals of our experiments are:</p>
<ul>
<li>(a) Evaluate the localization accuracy of our trained detectors in the real world, including in the presence of distractor objects and partial occlusions</li>
<li>(b) Assess which elements of our approach are most critical for achieving transfer from simulation to the real world</li>
<li>(c) Determine whether the learned detectors are accurate enough to perform robotic manipulation tasks</li>
</ul>
<p>TABLE I</p>
<table>
<thead>
<tr>
<th>Detection error for various objects, cm</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Evaluation type</td>
<td>Object only</td>
<td>Distractors</td>
<td>Occlusions</td>
</tr>
<tr>
<td>Cone</td>
<td>1.3 ± 1.1†</td>
<td>1.5 ± 1.0</td>
<td>1.4 ± 0.6</td>
</tr>
<tr>
<td>Cube</td>
<td>1.3 ± 0.6</td>
<td>1.8 ± 1.2</td>
<td>1.4 ± 0.6†</td>
</tr>
<tr>
<td>Cylinder</td>
<td>1.1 ± 0.9†</td>
<td>1.9 ± 2.8</td>
<td>1.9 ± 2.9</td>
</tr>
<tr>
<td>Hexagonal Prism</td>
<td>0.7 ± 0.5</td>
<td>0.6 ± 0.3†</td>
<td>1.0 ± 1.0†</td>
</tr>
<tr>
<td>Pyramid</td>
<td>0.9 ± 0.3†</td>
<td>1.0 ± 0.5†</td>
<td>1.1 ± 0.7†</td>
</tr>
<tr>
<td>Rectangular Prism</td>
<td>1.3 ± 0.7</td>
<td>1.2 ± 0.4†</td>
<td>0.9 ± 0.6</td>
</tr>
<tr>
<td>Tetrahedron</td>
<td>0.8 ± 0.4†</td>
<td>1.0 ± 0.4†</td>
<td>3.2 ± 5.8</td>
</tr>
<tr>
<td>Triangular Prism</td>
<td>0.9 ± 0.4†</td>
<td>0.9 ± 0.4†</td>
<td>1.9 ± 2.2</td>
</tr>
</tbody>
</table>
<h3>B. Localization accuracy</h3>
<p>To evaluate the accuracy of learned detectors in the real world, we captured 480 webcam images of one or more geometric objects on a table at a distance of 70 cm to 105 cm from the camera. The camera position remains constant across all images. We did not control for lighting conditions or the rest of the scene around the table (e.g., all images contain part of the robot and tape and wires on the floor).</p>
<p>^{1}Categories for which the best final performance was achieved for detector trained from scratch.</p>
<p>We measured ground truth positions for a single object per image by aligning the object on a grid on the tabletop. Each of the eight geometric objects has 60 labeled images in the dataset: 20 with the object alone on the table, 20 in which one or more distractor objects are present on the table, and 20 in which the object is also partially occluded by another object.</p>
<p>Table I summarizes the performance of our models on the test set. Our object detectors are able to localize objects to within 1.5 cm (on average) in the real world and perform well in the presence of clutter and partial occlusions. Though the accuracy of our trained detectors is promising, note that they are still over-fitting the simulated training data, where error is 0.3 cm to 0.5 cm . Even with over-fitting, the accuracy is comparable at a similar distance to the translation error in traditional techniques for pose estimation in clutter from a single monocular camera frame [5] that use higher-resolution images.</p>
<h2>C. Ablation study</h2>
<p>To evaluate the importance of different factors of our training methodology, we assessed the sensitivity of the algorithm to the following:</p>
<ul>
<li>Number of training images</li>
<li>Number of unique textures seen in training</li>
<li>Use of random noise in pre-processing</li>
<li>Presence of distractors in training</li>
<li>Randomization of camera position in training</li>
<li>Use of pre-trained weights in the detection model</li>
</ul>
<p>We found that the method is at least somewhat sensitive to all of the factors except the use of random noise.</p>
<p>Fig. 4. Sensitivity of test error on real images to the number of simulated training examples used. Each training example corresponds to a single labeled example of an object on the table with between 0 and 10 distractor objects. Lighting and all textures are randomized between iterations.</p>
<p>Figure 4 shows the sensitivity to the number of training samples used for pre-trained models and models trained from scratch. Using a pre-trained model, we are able to achieve relatively accurate real-world detection performance with as</p>
<p>[1] Overfitting in this setting is more subtle than in the standard supervised learning where train and test data come from the same distribution. In the standard supervised learning setting overfitting can be avoided by using a hold-out set during training. We do apply this idea to ensure that we are not overfitting on the simulated data. However, since our goal is to learn from training data originated in the simulator and generalize to test data originated from the real world, we assume to not have any real world data available during training. Therefore no validation on real data can be done during training. few as 5,000 training samples, but performance improves up to around 50,000 samples.</p>
<p>Figure 4 also compares to the performance of a model trained from scratch (i.e., without using pre-trained ImageNet weights). Our hypothesis that pre-training would be essential to generalizing to the real world proved to be false. With a large amount of training data, random weight initialization can achieve nearly the same performance in transferring to the real world as does pre-trained weight initialization. The best detectors for a given object were often those initialized with random weights. However, using a pre-trained model can significantly improve performance when less training data is used.</p>
<p>Figure 5 shows the sensitivity to the number of unique texturizations of the scene when trained on a fixed number $(10,000)$ of training examples. We found that performance degrades significantly when fewer than 1,000 textures are used, indicating that for our experiments, using a large number of random textures (in addition to random distractors and object positions) is necessary to achieving transfer. Note that when 1,000 random textures are used in training, the performance using 10,000 images is comparable to that of using only 1,000 images, indicating that in the low data regime, texture randomization is more important than randomization of object positions. ${ }^{3}$</p>
<p>Fig. 5. Sensitivity to amount of texture randomization. In each case, the detector was trained using 10,000 random object positions and combinations of distractors, but only the given number of unique texturizations and lighting conditions were used.</p>
<p>Table II examines the performance of the algorithm when random noise, distractors, and camera randomization are removed in training. Incorporating distractors during training appears to be critical to resilience to distractors in the real world. Randomizing the position of the camera also consistently provides a slight accuracy boost, but reasonably high accuracy is achievable without it. Adding noise during pretraining appears to have a negligible effect. In practice, we found that adding a small amount of random noise to images at training time improves convergence and makes training less susceptible to local minima.</p>
<h2>D. Robotics experiments</h2>
<p>To demonstrate the potential of this technique for transferring robotic behaviors learned in simulation to the real world,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>TABLE II</p>
<table>
<thead>
<tr>
<th>Average detection error on geometric shapes by method, $\mathbf{c m}^{1}$</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Evaluation</td>
<td>Real images</td>
<td></td>
<td></td>
</tr>
<tr>
<td>type</td>
<td>Object only</td>
<td>Distractors</td>
<td>Occlusions</td>
</tr>
<tr>
<td>Full method</td>
<td>$\mathbf{1} \mathbf{. 3} \pm \mathbf{0 . 6}$</td>
<td>$\mathbf{1} \mathbf{. 8} \pm \mathbf{1 . 7}$</td>
<td>$\mathbf{2} \mathbf{. 4} \pm \mathbf{3 . 0}$</td>
</tr>
<tr>
<td>No noise added</td>
<td>$1.4 \pm 0.7$</td>
<td>$1.9 \pm 2.0$</td>
<td>$\mathbf{2} \mathbf{. 4} \pm \mathbf{2 . 8}$</td>
</tr>
<tr>
<td>No camera randomization</td>
<td>$2.0 \pm 2.1$</td>
<td>$2.4 \pm 2.3$</td>
<td>$2.9 \pm 3.5$</td>
</tr>
<tr>
<td>No distractors in training</td>
<td>$1.5 \pm 0.6$</td>
<td>$7.2 \pm 4.5$</td>
<td>$7.4 \pm 5.3$</td>
</tr>
</tbody>
</table>
<p>we evaluated the use of our object detection networks for localizing an object in clutter and performing a prescribed grasp. For two of our most consistently accurate detectors, we evaluated the ability to pick up the detected object in 20 increasingly cluttered scenes using the positions estimated by the detector and off-the-shelf motion planning software [41]. To test the robustness of our method to discrepancies in object distributions between training and test time, some of our test images contain distractors placed at orientations not seen during training (e.g., a hexagonal prism placed on its side).</p>
<p>We deployed the pipeline on a Fetch robot [49], and found it was able to successfully detect and pick up the target object in 38 out of 40 trials, including in highly cluttered scenes with significant occlusion of the target object. Note that the trained detectors have no prior information about the color of the target object, only its shape and size, and are able to detect objects placed closely to other objects of the same color.</p>
<p>To test the performance of our object detectors on realworld objects with non-uniform textures, we trained an object detector to localize a can of Spam from the YCB Dataset [3]. At training time, the can was present on the table along with geometric object distractors. At test time, instead of using geometric object distractors, we placed other food items from the YCB set on the table. The detector was able to ignore the previously unseen distractors and pick up the target in 9 of 10 trials.</p>
<p>Figure 6 shows examples of the robot grasping trials. For videos, please visit the web page associated with this paper. ${ }^{5}$</p>
<h2>V. CONCLUSION</h2>
<p>We demonstrated that an object detector trained only in simulation can achieve high enough accuracy in the real world to perform grasping in clutter. Future work will explore how to make this technique reliable and effective enough to perform tasks that require contact-rich manipulation or higher precision.</p>
<p>Future directions that could improve the accuracy of object detectors trained using domain randomization include:</p>
<ul>
<li>Using higher resolution camera frames</li>
<li>Optimizing model architecture choice</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 6. Two representative executions of grasping objects using vision learned in simulation only. The object detector network estimates the positions of the object of interest, and then a motion planner plans a simple sequence of motions to grasp the object at that location.</p>
<ul>
<li>Introducing additional forms of texture, lighting, and rendering randomization to the simulation and training on more data</li>
<li>Incorporating multiple camera viewpoints, stereo vision, or depth information</li>
<li>Combining domain randomization with domain adaptation</li>
</ul>
<p>Domain randomization is a promising research direction toward bridging the reality gap for robotic behaviors learned in simulation. Deep reinforcement learning may allow more complex policies to be learned in simulation through largescale exploration and optimization, and domain randomization could be an important tool for making such policies useful on real robots.</p>
<h2>REFERENCES</h2>
<p>[1] Pieter Abbeel, Morgan Quigley, and Andrew Y Ng. Using inaccurate models in reinforcement learning. In Proceedings of the 23rd international conference on Machine learning, pages 1-8. ACM, 2006.
[2] Rika Antonova, Silvia Cruciani, Christian Smith, and Danica Kragic. Reinforcement learning for pivoting task. arXiv preprint arXiv:1703.00472, 2017.
[3] Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M Dollar. The ycb object and model set: Towards common benchmarks for manipulation research. In Advanced Robotics (ICAR), 2015 International Conference on, pages 510-517. IEEE, 2015.
[4] Alvaro Collet, Dmitry Berenson, Siddhartha S Srinivasa, and Dave Ferguson. Object recognition and full pose registration from a single image for robotic manipulation. In Robotics and Automation, 2009. ICRA'09. IEEE International Conference on, pages 48-55. IEEE, 2009.
[5] Alvaro Collet, Manuel Martinez, and Siddhartha S Srinivasa. The moped framework: Object recognition and pose estimation for manipulation. The International Journal of Robotics Research, 30(10):12841306, 2011.
[6] Alvaro Collet and Siddhartha S Srinivasa. Efficient multi-view object recognition and full pose estimation. In Robotics and Automation (ICRA), 2010 IEEE International Conference on, pages 2050-2055. IEEE, 2010.
[7] Mark Cutler and Jonathan P How. Efficient reinforcement learning for robots using informative simulated priors. In Robotics and Automation (ICRA), 2015 IEEE International Conference on, pages 2605-2612. IEEE, 2015.
[8] Mark Cutler, Thomas J Walsh, and Jonathan P How. Reinforcement learning with multi-fidelity simulators. In Robotics and Automation (ICRA), 2014 IEEE International Conference on, pages 3888-3895. IEEE, 2014.</p>
<p>[9] Lixin Duan, Dong Xu, and Ivor Tsang. Learning with augmented features for heterogeneous domain adaptation. arXiv preprint arXiv:1206.4660, 2012.
[10] Staffan Ekvall, Danica Kragic, and Frank Hoffmann. Object recognition and pose estimation using color cooccurrence histograms and geometric modeling. Image and Vision Computing, 23(11):943-955, 2005.
[11] Ali Ghadirzadeh, Atsuto Maki, Danica Kragic, and Mårten Björkman. Deep predictive policy training using reinforcement learning. arXiv preprint arXiv:1703.00727, 2017.
[12] Iryna Gordon and David G Lowe. What and where: 3d object recognition with accurate pose. In Toward category-level object recognition, pages 67-82. Springer, 2006.
[13] Abhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Learning invariant feature spaces to transfer skills with reinforcement learning. ICLR 2017, to appear, 2017.
[14] Judy Hoffman, Sergio Guadarrama, Eric Tzeng, Ronghang Hu, Jeff Donahue, Ross Girshick, Trevor Darrell, and Kate Saenko. Lsda: Large scale detection through adaptation. In Neural Information Processing Symposium (NIPS), 2014.
[15] Judy Hoffman, Erik Rodner, Jeff Donahue, Trevor Darrell, and Kate Saenko. Efficient learning of domain-invariant image representations. arXiv preprint arXiv:1301.3224, 2013.
[16] Stephen James and Edward Johns. 3d simulation for robot arm control with deep q-learning. arXiv preprint arXiv:1609.03759, 2016.
[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[18] J Zico Kolter and Andrew Y Ng. Learning omnidirectional path following using dimensionality reduction. In Robotics: Science and Systems, 2007.
[19] Brian Kulis, Kate Saenko, and Trevor Darrell. What you saw is not what you get: Domain adaptation using asymmetric kernel transforms. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1785-1792. IEEE, 2011.
[20] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444, 2015.
[21] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. Journal of Machine Learning Research, 17(39):1-40, 2016.
[22] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. arXiv preprint arXiv:1603.04779, 2016.
[23] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I Jordan. Learning transferable features with deep adaptation networks. In ICML, pages 97-105, 2015.
[24] David G Lowe. Three-dimensional object recognition from single two-dimensional images. Artificial intelligence, 31(3):355-395, 1987.
[25] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. arXiv preprint arXiv:0902.3430, 2009.
[26] Chaitanya Mitash, Kostas E Bekris, and Abdeslam Boularias. A selfsupervised learning system for object detection using physics simulation and multi-view pose estimation. arXiv preprint arXiv:1703.03347, 2017.
[27] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
[28] Igor Mordatch, Kendall Lowrey, and Emanuel Todorov. Ensemblecio: Full-body dynamic motion planning that transfers to physical humanoids. In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on, pages 5307-5314. IEEE, 2015.
[29] Yair Movshovitz-Atlias, Takeo Kanade, and Yaser Sheikh. How useful is photo-realistic rendering for visual learning? In Computer VisionECCV 2016 Workshops, pages 202-217. Springer, 2016.
[30] Ramakant Nevatia and Thomas O Binford. Description and recognition of curved objects. Artificial Intelligence, 8(1):77-98, 1977.
[31] Xingchao Peng, Baochen Sun, Karim Ali, and Kate Saenko. Learning deep object detectors from 3d models. In Proceedings of the IEEE International Conference on Computer Vision, pages 1278-1286, 2015.
[32] Benjamin Planche, Ziyan Wu, Kai Ma, Shanhui Sun, Stefan Kluckner, Terrence Chen, Andreas Hutter, Sergey Zakharov, Harald Kosch, and Jan Ernst. Depthsynth: Real-time realistic synthetic data gen-
eration from cad models for 2.5 d recognition. arXiv preprint arXiv:1702.08558, 2017.
[33] Aravind Rajeswaran, Sarvjeet Ghotra, Sergey Levine, and Balaraman Ravindran. Epop: Learning robust neural network policies using model ensembles. arXiv preprint arXiv:1610.01283, 2016.
[34] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In European Conference on Computer Vision, pages 102-118. Springer, 2016.
[35] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.
[36] Andrei A Rusu, Matej Vecerik, Thomas Rothörl, Nicolas Heess, Razvan Pascanu, and Raia Hadsell. Sim-to-real robot learning from pixels with progressive nets. arXiv preprint arXiv:1610.04286, 2016.
[37] Fereshteh Sadeghi and Sergey Levine. (cad) 2 RL: Real single-image flight without a single real image. arXiv preprint arXiv:1611.04201, 2016.
[38] John Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, pages 1889-1897, 2015.
[39] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
[40] Hao Su, Charles R Qi, Yangyan Li, and Leonidas J Guibas. Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views. In Proceedings of the IEEE International Conference on Computer Vision, pages 2686-2694, 2015.
[41] Ioan A Sucan and Sachin Chitta. Moveit! http://moveit.ros.org.
[42] Baochen Sun and Kate Saenko. From virtual to reality: Fast adaptation of virtual object detectors to real domains. In BMVC, volume 1, page 3, 2014.
[43] Yaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised crossdomain image generation. arXiv preprint arXiv:1611.02200, 2016.
[44] Jie Tang, Stephen Miller, Arjun Singh, and Pieter Abbeel. A textured object recognition pipeline for color and depth image data. In Robotics and Automation (ICRA), 2012 IEEE International Conference on, pages 3467-3474. IEEE, 2012.
[45] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 50265033. IEEE, 2012.
[46] Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Pieter Abbeel, Sergey Levine, Kate Saenko, and Trevor Darrell. Adapting deep visuomotor representations with weak pairwise constraints. In Workshop on the Algorithmic Foundations of Robotics (WAFR), 2016.
[47] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.
[48] Jur Van Den Berg, Stephen Miller, Daniel Duckworth, Humphrey Hu, Andrew Wan, Xiao-Yu Fu, Ken Goldberg, and Pieter Abbeel. Superhuman performance of surgical tasks by robots using iterative learning from human-guided demonstrations. In Robotics and Automation (ICRA), 2010 IEEE International Conference on, pages 2074-2081. IEEE, 2010.
[49] Melonee Wise, Michael Ferguson, Derek King, Eric Diehr, and David Dymesich. Fetch and freight: Standard platforms for service robot applications. In Workshop on Autonomous Mobile Service Robots, 2016.
[50] Patrick Wunsch and Gerd Hirzinger. Real-time visual tracking of 3d objects with dynamic handling of occlusion. In Robotics and Automation, 1997. Proceedings., 1997 IEEE International Conference on, volume 4, pages 2868-2873. IEEE, 1997.
[51] Jun Yang, Rong Yan, and Alexander G Hauptmann. Cross-domain video concept detection using adaptive svms. In Proceedings of the 15th ACM international conference on Multimedia, pages 188-197. ACM, 2007.
[52] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320-3328, 2014.
[53] Wenhao Yu, C Karen Liu, and Greg Turk. Preparing for the unknown: Learning a universal policy with online system identification. arXiv preprint arXiv:1702.02453, 2017.
[54] Stefan Zickler and Manuela M Veloso. Detection and localization</p>
<p>of multiple objects. In Humanoid Robots, 2006 6th IEEE-RAS International Conference on, pages 20-25. IEEE, 2006.</p>
<h2>APPENDIX</h2>
<h2>A. Randomly generated samples from our method</h2>
<p>Figure 7 displays a selection of the images used during training for the object detectors detailed in the paper.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 7. A selection of randomly textured scenes used in the training phase of our method</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Each of the models compared was trained with 20, 000 training examples
${ }^{5}$ https://sites.google.com/view/ domainrandomization/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
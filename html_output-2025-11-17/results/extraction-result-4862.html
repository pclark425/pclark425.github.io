<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4862 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4862</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4862</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-0e802c0739771acf70e60d59c2df51cd7e8c50c0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0e802c0739771acf70e60d59c2df51cd7e8c50c0" target="_blank">Memorizing Transformers</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext, math papers, books, code, as well as formal theorems (Isabelle).</p>
                <p><strong>Paper Abstract:</strong> Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4862.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4862.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memorizing Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>kNN-augmented Memorizing Transformer (Memorizing Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only Transformer extended with a non-differentiable external (key,value) memory accessed via approximate k-nearest-neighbor lookup in one (or more) attention layers; combines local dense attention and retrieved memory via a learned per-head gate and is designed to let the model 'memorize' and reuse past subsequence representations at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memorizing Transformer (kNN-augmented attention)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Decoder-only Transformer (vanilla and scaled variants) where one high-level attention layer performs standard local self-attention plus an approximate kNN lookup into a large external store of previous (key,value) pairs; retrieval results are combined with local attention via a learned per-head sigmoid gate.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external non-differentiable k-NN memory (retrieval-augmented memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>An append-only external memory of prior per-head (key,value) vectors produced from previously seen subsequences; during attention an approximate kNN search returns top-k keys per query, dot-product + softmax over retrieved keys weights retrieved values; keys/queries normalized to mitigate staleness; memory sizes used range from ~1.5K up to 262K tokens (records); approximate kNN recall ~90%.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context language modeling (multiple corpora: arXiv Math, PG-19, C4(4K+), Github code, Isabelle formal proofs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Next-token prediction / language modeling over long documents: books, long web articles, technical math papers, concatenated code repositories, and formal theorem scripts; evaluation via token-level perplexity and qualitative analyses of retrieved memories.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>arXiv Math; PG-19; C4 (filtered to >=4k tokens); Github code corpus; Isabelle formal proofs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Representative examples from Table 4: arXiv (context 512 -> memory 8,192): perplexity 2.49 (vs baseline 3.29 without memory); PG-19 (512 context -> 8,192 mem): 12.29 (vs 13.71 baseline); C4(4K+) (512 -> 8,192): 14.42 (vs 17.20 baseline); Github (512 -> 65K mem): 1.87 (vs 3.05 baseline); Isabelle (512 -> 65K mem): 2.06 (vs 3.09 baseline). Finetuning results (arXiv): pretrain 65K then finetune to 262K memory gave perplexity 2.21 (best reported) for context 2048.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Representative baselines from Table 4: vanilla Transformer, context 512, no memory: arXiv 3.29; PG-19 13.71; C4(4K+) 17.20; Github 3.05; Isabelle 3.09. Transformer-XL (context 2048, no external memory) example: arXiv 2.42; PG-19 11.88; C4 14.03; Github 2.10; Isabelle 2.16.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Adding external kNN memory consistently and substantially reduces perplexity across all evaluated long-document datasets. Initial gains are large with small memory (e.g., 1.5K–8K), and steady improvements continue up to ~65K–262K tokens with diminishing returns. Adding external memory to Transformer-XL still improves over the XL cache alone. A memorizing transformer with 8K memory can match the perplexity of a vanilla transformer with ~5x more parameters. Pretraining with a small memory then finetuning on a larger memory stabilizes training for very large memories.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Non-differentiable memory implies no gradient flow into stored keys/values (benefit for scalability but means older keys can be stale); distributional shift (staleness) of keys over training can cause instability when training from scratch with very large memory; approximate kNN (used for speed) has ~90% recall which can miss top-k entries; increasing memory sometimes harms some token predictions (retrievals that were previously top-k may be displaced); step time and compute cost increase with large memory (e.g., step time increased from 0.2s to 0.6s on TPUv3 when going to 65K); choice of layer index for memory matters (middle layers worked best); gating used is per-head scalar (not content-dependent) in experiments though content gating is possible.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>A simple approximate kNN external memory appended to a transformer attention layer provides an effective and scalable way to extend context far beyond attention window limits, allowing models to 'memorize' and retrieve exact prior token representations (e.g., function definitions, theorem statements, citations). Memory yields larger gains than merely increasing context length or model size in many settings, is robust to approximate retrieval, and can be added to pretrained models via finetuning (converges quickly). Practical challenges include staleness (distributional shift of stored keys), training instability for very large memories (mitigated by pretraining+finetuning), and increased compute/time per step with large memories.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memorizing Transformers', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4862.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4862.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-XL (XL cache)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-XL with Transformer-XL style cache</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent-style extension to transformers that caches previous step (key,value) pairs (a non-differentiable cache) to extend receptive field beyond fixed context windows; used in this paper both as a baseline and in combination with external kNN memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformer-XL: Attentive language models beyond a fixed-length context</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Transformer-XL (with XL cache)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Decoder-only Transformer augmented with a persistent (non-differentiable) cache of previous training-step (key,value) pairs that are prepended to the current context to increase short-to-medium range context without recomputing earlier activations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>non-differentiable short-term cache (Transformer-XL cache / persistent hidden-state cache)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>A per-document cache that stores keys and values computed from the previous training step and prepends them to current keys/values for local attention; combined with sliding-window causal mask to give local context up to the cache size.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context language modeling (same corpora as main experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Next-token prediction on long documents; used as a baseline to compare effects of adding external kNN memory.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>arXiv Math; PG-19; C4 (4K+); Github; Isabelle</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>When Transformer-XL is given an external memory as well: e.g., context 2048 + memory 8,192 + XL cache 2048 -> arXiv perplexity 2.33 (vs Transformer-XL without external memory 2.42). Also adding XL cache to large-memory models provided complementary gains (e.g., vanilla+memory benefits further when adding XL cache).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Transformer-XL baselines (no external memory): context 2048, no memory: arXiv 2.42; PG-19 11.88; C4 14.03; Github 2.10; Isabelle 2.16 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Transformer-XL's cache already increases receptive field, but adding external kNN memory to Transformer-XL still yields measurable perplexity improvements, indicating kNN memory retrieves information complementary to XL cache. XL cache helps especially at the start of sequences where local context is lacking.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Transformer-XL's cache expands receptive field but does not scale to arbitrarily long contexts without compression; XL cache is complementary but insufficient alone for very long-range retrievals that external memory provides.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Short-term cached states (Transformer-XL) provide complementary short-range context; external kNN memory is more effective for very long-range exact retrievals (e.g., citations, function/theorem definitions) and scales to much larger contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memorizing Transformers', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4862.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4862.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>kNN-LM (Khandelwal et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generalization through memorization: Nearest neighbor language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented approach where a pre-trained LM is run over a corpus to build a large table of (key, token) pairs; at inference the table is consulted (non-differentiable) to adjust token predictions improving language modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generalization through memorization: Nearest neighbor language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>kNN-LM / nearest-neighbor language model</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Precompute keys (representations) across a large corpus using a pre-trained LM and store (key,token) pairs in a non-differentiable table; at inference a kNN lookup over that table augments or replaces the LM output distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external non-differentiable nearest-neighbor datastore (key->token)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Large table built by running a pre-trained model once over corpus; retrieval used to influence final softmax (token selection) via nearest-neighbor counts/weights.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language modeling / next-token prediction (improving perplexity via retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Augment token prediction using nearest-neighbor retrieval from a precomputed datastore of representations and tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Mentioned as prior work that uses non-differentiable external memory to improve language modeling by replacing/augmenting the final softmax; the paper positions Memorizing Transformer as related but focuses on retrieval into intermediate attention rather than final softmax.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Described in related work; not evaluated directly in this paper. Khandelwal et al.'s approach requires running the base model over the entire corpus to build the datastore and uses retrieval at softmax-level (different integration point than memorizing transformer).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Non-differentiable external datastores have been shown to substantially improve LM predictions; integration point (output softmax vs attention layer) is an important design choice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memorizing Transformers', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4862.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4862.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Cache (Grave et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Improving neural language models with a continuous cache</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuous (non-differentiable) cache of recent hidden states used to bias language model predictions by attending/looking up similar past hidden states.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving neural language models with a continuous cache</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Neural cache / continuous cache</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Cache of recent hidden states (non-differentiable) that the LM can attend over to bias predictions toward recent tokens; a precursor to Transformer-XL style caching and retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>non-differentiable cache of recent hidden states (continuous cache)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Maintain and attend over a record of prior hidden states to boost recall of recent contexts; typically limited to recent window but can extend receptive field.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language modeling / next-token prediction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use a cache of prior hidden states to augment token probability estimation, improving perplexity especially on repeating patterns or long-range repetitions.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Cited as prior related technique; Memorializing Transformer differs by using approximate kNN into a much larger external memory and integrating retrieval into an attention layer rather than only caching recent states.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Neural caches are effective for short-to-medium range repetition but require design choices about size and management; not directly profiled in this paper beyond related work mention.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Caching mechanisms motivated the idea of attending over past internal representations; scaling and exact retrieval (not averaging) are key differences exploited by the Memorizing Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memorizing Transformers', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4862.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4862.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Large Memory Layers (Product Keys)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large memory layers with product keys</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that replaces feed-forward layers with very large (differentiable) memory lookups (product keys) to increase model capacity via fast nearest-neighbor style retrieval inside the network.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large memory layers with product keys</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Large memory layers (product keys)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Use a large learned (differentiable) key-value memory accessed via product-key lookup to expand model capacity, applied as a replacement or augmentation to FFN layers.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>learned differentiable memory (product-key memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>A learned parameterized memory layer implementing fast lookup (product-key) to replace or augment internal FFNs; differs from non-differentiable external memory because gradients flow into the memory parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General language modeling / accuracy improvements (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Leverage a large differentiable memory as a module inside the network to improve model accuracy without large increases in compute.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Mentioned as different design: Lample et al. use kNN-like lookup to replace FFN layers (learned/differentiable), while Memorizing Transformer uses kNN retrieval into non-differentiable memory to augment attention to past tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Differentiable memory requires recomputing keys/values each training step for gradient propagation which limits scalability of extremely large memories; Memorizing Transformer chooses non-differentiable external memory to reuse previously computed keys/values.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Trade-off between differentiable large memories (learnable but costly to train at scale) and non-differentiable external memories (scalable and reuseable but not updated by gradients).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memorizing Transformers', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generalization through memorization: Nearest neighbor language models <em>(Rating: 2)</em></li>
                <li>Improving neural language models with a continuous cache <em>(Rating: 2)</em></li>
                <li>Large memory layers with product keys <em>(Rating: 2)</em></li>
                <li>Transformer-XL: Attentive language models beyond a fixed-length context <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4862",
    "paper_id": "paper-0e802c0739771acf70e60d59c2df51cd7e8c50c0",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "Memorizing Transformer",
            "name_full": "kNN-augmented Memorizing Transformer (Memorizing Transformer)",
            "brief_description": "A decoder-only Transformer extended with a non-differentiable external (key,value) memory accessed via approximate k-nearest-neighbor lookup in one (or more) attention layers; combines local dense attention and retrieved memory via a learned per-head gate and is designed to let the model 'memorize' and reuse past subsequence representations at inference time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Memorizing Transformer (kNN-augmented attention)",
            "agent_description": "Decoder-only Transformer (vanilla and scaled variants) where one high-level attention layer performs standard local self-attention plus an approximate kNN lookup into a large external store of previous (key,value) pairs; retrieval results are combined with local attention via a learned per-head sigmoid gate.",
            "memory_type": "external non-differentiable k-NN memory (retrieval-augmented memory)",
            "memory_description": "An append-only external memory of prior per-head (key,value) vectors produced from previously seen subsequences; during attention an approximate kNN search returns top-k keys per query, dot-product + softmax over retrieved keys weights retrieved values; keys/queries normalized to mitigate staleness; memory sizes used range from ~1.5K up to 262K tokens (records); approximate kNN recall ~90%.",
            "task_name": "Long-context language modeling (multiple corpora: arXiv Math, PG-19, C4(4K+), Github code, Isabelle formal proofs)",
            "task_description": "Next-token prediction / language modeling over long documents: books, long web articles, technical math papers, concatenated code repositories, and formal theorem scripts; evaluation via token-level perplexity and qualitative analyses of retrieved memories.",
            "benchmark_name": "arXiv Math; PG-19; C4 (filtered to &gt;=4k tokens); Github code corpus; Isabelle formal proofs",
            "performance_with_memory": "Representative examples from Table 4: arXiv (context 512 -&gt; memory 8,192): perplexity 2.49 (vs baseline 3.29 without memory); PG-19 (512 context -&gt; 8,192 mem): 12.29 (vs 13.71 baseline); C4(4K+) (512 -&gt; 8,192): 14.42 (vs 17.20 baseline); Github (512 -&gt; 65K mem): 1.87 (vs 3.05 baseline); Isabelle (512 -&gt; 65K mem): 2.06 (vs 3.09 baseline). Finetuning results (arXiv): pretrain 65K then finetune to 262K memory gave perplexity 2.21 (best reported) for context 2048.",
            "performance_without_memory": "Representative baselines from Table 4: vanilla Transformer, context 512, no memory: arXiv 3.29; PG-19 13.71; C4(4K+) 17.20; Github 3.05; Isabelle 3.09. Transformer-XL (context 2048, no external memory) example: arXiv 2.42; PG-19 11.88; C4 14.03; Github 2.10; Isabelle 2.16.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Adding external kNN memory consistently and substantially reduces perplexity across all evaluated long-document datasets. Initial gains are large with small memory (e.g., 1.5K–8K), and steady improvements continue up to ~65K–262K tokens with diminishing returns. Adding external memory to Transformer-XL still improves over the XL cache alone. A memorizing transformer with 8K memory can match the perplexity of a vanilla transformer with ~5x more parameters. Pretraining with a small memory then finetuning on a larger memory stabilizes training for very large memories.",
            "limitations_or_challenges": "Non-differentiable memory implies no gradient flow into stored keys/values (benefit for scalability but means older keys can be stale); distributional shift (staleness) of keys over training can cause instability when training from scratch with very large memory; approximate kNN (used for speed) has ~90% recall which can miss top-k entries; increasing memory sometimes harms some token predictions (retrievals that were previously top-k may be displaced); step time and compute cost increase with large memory (e.g., step time increased from 0.2s to 0.6s on TPUv3 when going to 65K); choice of layer index for memory matters (middle layers worked best); gating used is per-head scalar (not content-dependent) in experiments though content gating is possible.",
            "key_insights": "A simple approximate kNN external memory appended to a transformer attention layer provides an effective and scalable way to extend context far beyond attention window limits, allowing models to 'memorize' and retrieve exact prior token representations (e.g., function definitions, theorem statements, citations). Memory yields larger gains than merely increasing context length or model size in many settings, is robust to approximate retrieval, and can be added to pretrained models via finetuning (converges quickly). Practical challenges include staleness (distributional shift of stored keys), training instability for very large memories (mitigated by pretraining+finetuning), and increased compute/time per step with large memories.",
            "uuid": "e4862.0",
            "source_info": {
                "paper_title": "Memorizing Transformers",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Transformer-XL (XL cache)",
            "name_full": "Transformer-XL with Transformer-XL style cache",
            "brief_description": "A recurrent-style extension to transformers that caches previous step (key,value) pairs (a non-differentiable cache) to extend receptive field beyond fixed context windows; used in this paper both as a baseline and in combination with external kNN memory.",
            "citation_title": "Transformer-XL: Attentive language models beyond a fixed-length context",
            "mention_or_use": "use",
            "agent_name": "Transformer-XL (with XL cache)",
            "agent_description": "Decoder-only Transformer augmented with a persistent (non-differentiable) cache of previous training-step (key,value) pairs that are prepended to the current context to increase short-to-medium range context without recomputing earlier activations.",
            "memory_type": "non-differentiable short-term cache (Transformer-XL cache / persistent hidden-state cache)",
            "memory_description": "A per-document cache that stores keys and values computed from the previous training step and prepends them to current keys/values for local attention; combined with sliding-window causal mask to give local context up to the cache size.",
            "task_name": "Long-context language modeling (same corpora as main experiments)",
            "task_description": "Next-token prediction on long documents; used as a baseline to compare effects of adding external kNN memory.",
            "benchmark_name": "arXiv Math; PG-19; C4 (4K+); Github; Isabelle",
            "performance_with_memory": "When Transformer-XL is given an external memory as well: e.g., context 2048 + memory 8,192 + XL cache 2048 -&gt; arXiv perplexity 2.33 (vs Transformer-XL without external memory 2.42). Also adding XL cache to large-memory models provided complementary gains (e.g., vanilla+memory benefits further when adding XL cache).",
            "performance_without_memory": "Transformer-XL baselines (no external memory): context 2048, no memory: arXiv 2.42; PG-19 11.88; C4 14.03; Github 2.10; Isabelle 2.16 (Table 4).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Transformer-XL's cache already increases receptive field, but adding external kNN memory to Transformer-XL still yields measurable perplexity improvements, indicating kNN memory retrieves information complementary to XL cache. XL cache helps especially at the start of sequences where local context is lacking.",
            "limitations_or_challenges": "Transformer-XL's cache expands receptive field but does not scale to arbitrarily long contexts without compression; XL cache is complementary but insufficient alone for very long-range retrievals that external memory provides.",
            "key_insights": "Short-term cached states (Transformer-XL) provide complementary short-range context; external kNN memory is more effective for very long-range exact retrievals (e.g., citations, function/theorem definitions) and scales to much larger contexts.",
            "uuid": "e4862.1",
            "source_info": {
                "paper_title": "Memorizing Transformers",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "kNN-LM (Khandelwal et al.)",
            "name_full": "Generalization through memorization: Nearest neighbor language models",
            "brief_description": "A retrieval-augmented approach where a pre-trained LM is run over a corpus to build a large table of (key, token) pairs; at inference the table is consulted (non-differentiable) to adjust token predictions improving language modeling.",
            "citation_title": "Generalization through memorization: Nearest neighbor language models",
            "mention_or_use": "mention",
            "agent_name": "kNN-LM / nearest-neighbor language model",
            "agent_description": "Precompute keys (representations) across a large corpus using a pre-trained LM and store (key,token) pairs in a non-differentiable table; at inference a kNN lookup over that table augments or replaces the LM output distribution.",
            "memory_type": "external non-differentiable nearest-neighbor datastore (key-&gt;token)",
            "memory_description": "Large table built by running a pre-trained model once over corpus; retrieval used to influence final softmax (token selection) via nearest-neighbor counts/weights.",
            "task_name": "Language modeling / next-token prediction (improving perplexity via retrieval)",
            "task_description": "Augment token prediction using nearest-neighbor retrieval from a precomputed datastore of representations and tokens.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "Mentioned as prior work that uses non-differentiable external memory to improve language modeling by replacing/augmenting the final softmax; the paper positions Memorizing Transformer as related but focuses on retrieval into intermediate attention rather than final softmax.",
            "limitations_or_challenges": "Described in related work; not evaluated directly in this paper. Khandelwal et al.'s approach requires running the base model over the entire corpus to build the datastore and uses retrieval at softmax-level (different integration point than memorizing transformer).",
            "key_insights": "Non-differentiable external datastores have been shown to substantially improve LM predictions; integration point (output softmax vs attention layer) is an important design choice.",
            "uuid": "e4862.2",
            "source_info": {
                "paper_title": "Memorizing Transformers",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Neural Cache (Grave et al.)",
            "name_full": "Improving neural language models with a continuous cache",
            "brief_description": "A continuous (non-differentiable) cache of recent hidden states used to bias language model predictions by attending/looking up similar past hidden states.",
            "citation_title": "Improving neural language models with a continuous cache",
            "mention_or_use": "mention",
            "agent_name": "Neural cache / continuous cache",
            "agent_description": "Cache of recent hidden states (non-differentiable) that the LM can attend over to bias predictions toward recent tokens; a precursor to Transformer-XL style caching and retrieval augmentation.",
            "memory_type": "non-differentiable cache of recent hidden states (continuous cache)",
            "memory_description": "Maintain and attend over a record of prior hidden states to boost recall of recent contexts; typically limited to recent window but can extend receptive field.",
            "task_name": "Language modeling / next-token prediction",
            "task_description": "Use a cache of prior hidden states to augment token probability estimation, improving perplexity especially on repeating patterns or long-range repetitions.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "Cited as prior related technique; Memorializing Transformer differs by using approximate kNN into a much larger external memory and integrating retrieval into an attention layer rather than only caching recent states.",
            "limitations_or_challenges": "Neural caches are effective for short-to-medium range repetition but require design choices about size and management; not directly profiled in this paper beyond related work mention.",
            "key_insights": "Caching mechanisms motivated the idea of attending over past internal representations; scaling and exact retrieval (not averaging) are key differences exploited by the Memorizing Transformer.",
            "uuid": "e4862.3",
            "source_info": {
                "paper_title": "Memorizing Transformers",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Large Memory Layers (Product Keys)",
            "name_full": "Large memory layers with product keys",
            "brief_description": "A prior approach that replaces feed-forward layers with very large (differentiable) memory lookups (product keys) to increase model capacity via fast nearest-neighbor style retrieval inside the network.",
            "citation_title": "Large memory layers with product keys",
            "mention_or_use": "mention",
            "agent_name": "Large memory layers (product keys)",
            "agent_description": "Use a large learned (differentiable) key-value memory accessed via product-key lookup to expand model capacity, applied as a replacement or augmentation to FFN layers.",
            "memory_type": "learned differentiable memory (product-key memory)",
            "memory_description": "A learned parameterized memory layer implementing fast lookup (product-key) to replace or augment internal FFNs; differs from non-differentiable external memory because gradients flow into the memory parameters.",
            "task_name": "General language modeling / accuracy improvements (prior work)",
            "task_description": "Leverage a large differentiable memory as a module inside the network to improve model accuracy without large increases in compute.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "Mentioned as different design: Lample et al. use kNN-like lookup to replace FFN layers (learned/differentiable), while Memorizing Transformer uses kNN retrieval into non-differentiable memory to augment attention to past tokens.",
            "limitations_or_challenges": "Differentiable memory requires recomputing keys/values each training step for gradient propagation which limits scalability of extremely large memories; Memorizing Transformer chooses non-differentiable external memory to reuse previously computed keys/values.",
            "key_insights": "Trade-off between differentiable large memories (learnable but costly to train at scale) and non-differentiable external memories (scalable and reuseable but not updated by gradients).",
            "uuid": "e4862.4",
            "source_info": {
                "paper_title": "Memorizing Transformers",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generalization through memorization: Nearest neighbor language models",
            "rating": 2
        },
        {
            "paper_title": "Improving neural language models with a continuous cache",
            "rating": 2
        },
        {
            "paper_title": "Large memory layers with product keys",
            "rating": 2
        },
        {
            "paper_title": "Transformer-XL: Attentive language models beyond a fixed-length context",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 1
        }
    ],
    "cost": 0.0153345,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MEMORIZING TRANSFORMERS</h1>
<p>Yuhuai Wu, Markus N. Rabe, DeLesley Hutchins, Christian Szegedy<br>{yuhuai,mrabe, delesley, szegedy}@google.com</p>
<h4>Abstract</h4>
<p>Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate $k \mathrm{NN}$ lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262 K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.</p>
<h2>1 INTRODUCTION</h2>
<p>Transformers (Vaswani et al., 2017) have led to remarkable progress in natural language processing (Devlin et al., 2019; Brown et al., 2020), mathematical reasoning (Polu \&amp; Sutskever, 2020; Wang et al., 2020a; Rabe et al., 2021; Li et al., 2021; Hahn et al., 2021; Cobbe et al., 2021), and program synthesis (Austin et al., 2021; Chen et al., 2021; Li et al., 2022). However, transformer performance on many of these tasks is limited by the context length of attention, which is typically short. The ability to attend to far-away tokens is important in many situations. In novels, characters and events are referenced across multiple chapters. In source code, references to classes and functions may occur quite far from the places in which they are defined. In theorem proving, proofs make use of previously defined lemmas.</p>
<p>Attention over long sequences is also useful as a form of rapid learning. Facts and information which are stored in the form of weight matrices must be slowly trained over hundreds of thousands of training steps. By using attention, however, a model can simply memorize facts (e.g. function definitions) by storing them as (key, value) pairs in long-term memory, and then retrieve those facts later by creating a query that attends to them. In this case, attention acts as a form of information retrieval, allowing the model to look up facts that it has seen previously.</p>
<p>We demonstrate that a simple and effective way to increase the size of the attention context is to use approximate $k$-nearest-neighbor ( $k \mathrm{NN}$ ) lookup, which is widely used in information retrieval. A number of extremely scalable implementations of $k \mathrm{NN}$ lookup are available, such as ScaNN (Guo et al., 2020) and Faiss (Johnson et al., 2021).</p>
<p>There are two things which distinguish our approach from previous work on long-range attention (c.f. Section 2). First, unlike some other approaches, $k \mathrm{NN}$ lookup does not do averaging or summarization of tokens at long distances, but retrieves exact values even from the distant context.</p>
<p>Second, gradients are not backpropagated into the external memory, which is critical to the scalability of our technique. The keys and values are a function of model parameters, so attempting to backpropagate gradients into external memory would necessarily involve computing all of the keys and values with the current model parameters on every training step. However, if the external memory is not differentiable, then we can instead instead reuse keys and values that were previously computed on prior training steps, which drastically reduces the amount of computation for large memories. With</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Adding a memory of 8 K tokens improves perplexity across different model sizes.
our technique, we are easily able to scale external memory up to sequence lengths of 131 k or 262 k tokens on a single TPU device, while maintaining a reasonable step time.</p>
<p>We show that model perplexity steadily improves with the size of external memory on a variety of language modelling tasks, including C4 (long documents only), Github code repositories, PG-19 books, formal proofs in Isabelle, and arXiv math papers. We further show that models can generalize to larger memory sizes than they were trained on: models trained with a small memory show gains from using a much larger memory at inference time. Finally, we show that our models are actually using memory in the way that we had hoped, e.g. by looking up the definitions of lemmas in a theorem proving corpus.</p>
<p>The simplicity of the changes to the Transformer architecture allows us to easily integrate this approach into existing code bases, including extremely large language models. We further show that the improvements to quality are maintained across models of increasing size, and that the model improvements gained from adding memory are even larger than increasing the size of the model by 5 X or more as shown in Figure 1.</p>
<h1>2 Related Work</h1>
<p>A great deal of work has been done on efficient long-range attention mechanisms; see Tay et al. (2020; 2021) recent surveys. Sliding windows (Beltagy et al., 2020) use a long sequence, but attend within a smaller window, thus reducing complexity to the window size, rather than total sequence length. Approximate mechanisms such as Linformer (Wang et al., 2020b), and Performer (Choromanski et al., 2021) refactor the attention matrix by using a different kernel than softmax to obtain $O(N)$ complexity. Pooling strategies such as Hierarchical 1D attention (Zhu \&amp; Soricut, 2021), and Combiner (Ren et al., 2021) apply pooling or averaging over tokens at longer distances. Sparse strategies such as Big Bird (Zaheer et al., 2020) select only a subset of tokens to attend to; Routing Transformers (Roy et al., 2021) use clustering to select the subset, while Reformer (Kitaev et al., 2020) relies on hashing. Hierarchical mechanisms (Ainslie et al., 2020) combine multiple tokens into phrases or sentences to reduce sequence length. Expire-span (Sukhbaatar et al., 2021) prunes far-away tokens that it learns are "unimportant". (Zemlyanskiy et al., 2021) process long sequences in two passes with different encoders. The second pass is given a lot of context by accessing summaries of the first pass.</p>
<p>Feedback transformers (Fan et al., 2020) use a recurrent architecture in which each token attends to the output of the final layer instead of the previous layer. Recurrence does not increase the size of the attention context itself, but it expands the receptive field at the cost of parallelism and training speed.</p>
<p>Truncated backpropagation through time (Williams \&amp; Peng, 1990) was originally introduced as a way of training recurrent neural networks (RNN) over very long sequences, when the entire sequence does not fit in memory. The sequence is chopped into segments, and after each training step, the final RNN state for the segment is saved in a non-differentiable cache, and used as the initial state on the next training step. Neural caches (Grave et al., 2017) extend the cache to contain a record of many prior hidden states, and attend over them. Transformer-XL (Dai et al., 2019) applies this technique to transformers; it caches the (key,value) pairs computed from the previous training step, and uses them as a prefix for the tokens on the next training step, which yields significant gains on long documents. Rae et al. (2020) improve over Transformer-XL by compressing the tokens before adding them to the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: We extend Transformers with access to (key, value) pairs of previously seen subsequences.
cache. In contrast, we use a very large cache without compression, combined with an approximate $k \mathrm{NN}$ attention mechanism over it.</p>
<p>Sukhbaatar et al. (2019) make the observation that the feed-forward portion of a transformer layer functions very much like attention if one replaces the ReLU activation with softmax. They implement a combined attention over both tokens from the input sequence and a learned (and differentiable) "memory". Lample et al. (2019) exploit this observation to replace the feed-forward layers (FFNs) with a fast $k \mathrm{NN}$ lookup over a much larger "memory", and achieve large gains in model accuracy without significant computation overhead. (We use $k \mathrm{NN}$ lookup to approximate attention to previous tokens, not to replace the FFN.)</p>
<p>Non-differentiable external memory has been used in different ways by Khandelwal et al. (2020), who run a pre-trained model over an entire corpus, and construct a large table of (key, token) pairs. They then use that table to replace the final softmax layer for token selection in the model, which results in significant improvements in language modeling. Yogatama et al. (2021) extend this approach by a gating mechanism and a process to compress the context into keys for retrieval.</p>
<p>There are several works that combine retrieval with transformers. REALM (Guu et al., 2020), MARGE (Lewis et al., 2020a), RAG (Lewis et al., 2020b), and composite memory for dialog (Fan et al., 2021) retrieve documents from a knowledge base to improve question answering or dialogue. The knowledge base consists of text snippets and is static and typically separate from the inputs and outputs of the models. Instead, we focus on language modeling using a decoder-only model, and propose a simple model that unifies attention and retrieval.
$k$-nearest-neighbor lookup is a general-purpose technique that is used for a wide variety of machine learning and retrieval tasks, and high-performance implementations are available for various architectures (Johnson et al., 2021; Guo et al., 2020). Memory-efficient Transformers (Gupta et al., 2021) replace dense attention with a $k \mathrm{NN}$ lookup to increase speed and reduce memory usage.</p>
<h1>3 Method</h1>
<p>The architecture of our $k$ NN-augmented transformer is shown in Figure 2. The bulk of the model is a vanilla, decoder-only transformer (Vaswani et al., 2017). The input text is tokenized, and the tokens are embedded into vector space. The embedding vectors are passed through a series of transformer layers, each of which does dense self-attention, followed by a feed-forward network (FFN). Since this is a decoder-only language model, we use a causal attention mask and the token embeddings of the last layer are used to predict the next token.</p>
<p>Long documents are split into subsequences of 512 tokens, and each subsequence is used as the input for one training step. In contrast to standard practice, we do not shuffle the subsequences; instead, each long document is fed into the transformer sequentially, from beginning to end, as is done with Transformer-XL (Dai et al., 2019).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Our data pipeline splits documents into subsequences and packs subsequences into batches.
We also use a Transformer-XL style cache, which holds the keys and values from the previous training step. When doing self-attention, the cached keys and values are prepended to the current keys and values, and we use a sliding-window causal mask (Beltagy et al., 2020) so that each token has a local context that includes the previous 512 tokens.</p>
<h1>3.1 $k$ NN-AUGMENTED ATTENTION LAYER</h1>
<p>One of the transformer layers near the top of the stack is a $k N N$-augmented attention layer, which combines two forms of attention. Like all of the other layers, it uses standard dense self-attention on the local context, which is the input subsequence for the current training step. Unlike the other layers, however, it also does an approximate $k$-nearest-neighbor search into the external memory.</p>
<p>The same queries are used for both the local context, and for the external memory. The keys and values also belong to the same distribution; after each training step, the (key, value) pairs in the local context are appended to the end of the external memory. If the document is very long, old (key, value) pairs will be dropped from the memory to make room for new ones. Thus, for each head, the external memory keeps a cache of the prior $M$ (key, value) pairs, where $M$ is the memory size.
The $k \mathrm{NN}$ lookup will return a set of retrieved memories, which consist of the top- $k$ (key, value) pairs that $k \mathrm{NN}$ search returns for each query (i.e. each token) in the input subsequence. As with standard dense attention, we first construct an attention matrix by computing the dot product of each query against the retrieved keys, then apply softmax, and finally return a weighted sum of the retrieved values. Unlike standard dense attention, the retrieved memories contain a different set of (key, value) pairs for each query.
Attention over the local context is performed in the usual way. The results of $k \mathrm{NN}$-attention and local attention are then combined using a learned gate:</p>
<p>$$
\begin{aligned}
g &amp; =\sigma\left(b_{g}\right) \
\boldsymbol{V}<em m="m">{a} &amp; =\boldsymbol{V}</em> \odot(1-g)
\end{aligned}
$$} \odot g+\boldsymbol{V}_{c</p>
<p>where $\sigma$ is the sigmoid function, and $\odot$ is element-wise multiplication. $\boldsymbol{V}<em m="m">{a}$ is the combined result of attention, $\boldsymbol{V}</em>}$ is the result of attending to external memory, and $\boldsymbol{V<em g="g">{c}$ is the result of attending to the local context. The bias $b</em>$ is a learned per-head scalar parameter, which allows each head to choose between local and long-range attention. In our experiments, the value of the gate $g$ does not depend on the content of the token at each position, although that would be a trivial extension to implement. We did observe that over time, most heads learned to attend almost exclusively to external memory.</p>
<p>Position bias. For dense attention within the local context, we use the T5 relative position bias (Raffel et al., 2020). As noted by Dai et al. (2019), adding a global position encoding to each token does not work well when processing long documents. We don't use a position bias for the retrieved memories. Experiments on the PG19 dataset (Sun et al., 2021) have shown that relative position does not appear to matter at long range, and the T5 relative bias puts all long-range tokens in the same bucket anyway.</p>
<p>Batching. Figure 3 illustrates how multiple long documents of different lengths are packed into a batch, and split into subsequences. Each subsequence in the batch comes from a different document, and thus requires a separate external memory, which is cleared at the start of each new document.</p>
<h3>3.2 Distributional SHIFT</h3>
<p>Because each long document is processed over multiple training steps, there is a distributional shift in the keys and values that are stored in external memory. The model parameters that produce the queries change over time, and will thus have shifted since the keys and values were stored. For very large memories, older records may become "stale." Similar observations have been made for CrossBatch memory (Wang et al., 2020c) in the vision domain.</p>
<p>To reduce the effects of staleness, we normalize keys and queries (Henry et al., 2020). Normalization does not eliminate staleness, but it at least ensures that older keys and newer keys do not differ in magnitude. We also found that normalization helps stabilize training with the Transformer-XL cache.</p>
<p>In some of our experiments, we observed that training models from scratch with a large memory sometimes resulted in worse performance than pretraining the model with a small memory of size 8192, and then finetuning it on a larger memory. This training instability could be due to staleness. However, models seem to be able to cope with a limited degree of staleness (with the small memory) by adjusting their queries accordingly.</p>
<h1>3.3 APPROXIMATE $k \mathrm{NN}$</h1>
<p>We employ approximate $k \mathrm{NN}$ search rather than exact $k \mathrm{NN}$ search because it significantly improves the computational speed of our model. We use a simple approximation of $k \mathrm{NN}$ for TPUs, which has a recall of about $90 \%$, i.e. $90 \%$ of the true top $k$ are returned in the approximate top $k$. There are various other efficient approximate $k \mathrm{NN}$ algorithms available for CPU and GPU/TPU, for example through Faiss (Johnson et al., 2021) or ScaNN (Guo et al., 2020), which can scale into the billions.</p>
<h2>4 EXPERIMENTS</h2>
<p>We evaluate the effect of adding external memory on five language modeling tasks, all of which involve long-form text: English language books (PG-19), long web articles (C4), technical math papers (arXiv Math), source code (Github), and formal theorems (Isabelle). The results show significant improvements in the perplexity of the model with the addition of external memory. We experimented with various sizes of external memory, from 1536 to as high as 262 K . On most of the datasets, there was an initial sharp gain from adding a small external memory, followed by smaller but steadily increasing gains as the size of the memory was increased.</p>
<h3>4.1 DATASETS</h3>
<p>arXiv Math For the arXiv dataset, we collected a corpus of papers by downloading them via the arXiv Bulk Data Access ${ }^{1}$. We filtered papers to include only articles labeled as "Mathematics" and whose $\mathrm{IT}<em _mathrm_E="\mathrm{E">{\mathrm{E}} \mathrm{X}$ source was available. The number of tokens per paper in this dataset is roughly comparable to the number of tokens per book in PG19, because $\mathrm{ET}</em>$ source has many special characters and the tokenizer tends to output small subwords.}} \mathrm{X</p>
<p>Github We used BigQuery ${ }^{2}$ to obtain a large corpus of Github repositories that are published with open-source licenses. We used file endings to filter for files in the languages C, C++, Java, Python (including Jupyter notebooks), Go, and TypeScript. Individual source code files are often fairly short, and there are many dependencies and cross-references between files in the repository. To capture these dependencies, we created one long document for each Github repository by traversing the directory tree, and concatenating all of the files within it. The order in which files are traversed within the repository is random, but each subdirectory is processed as a unit, so that all the files within the subdirectory are close to each other in the resulting document. Source code is usually structured so that related files are all grouped together in the same subdirectory; this traversal preserves that structure, while still shuffling files and subdirectories in random order.</p>
<p>Formal Math - Isabelle The Isabelle corpus consists of formal mathematical proofs of theories. We collected all 627 theories available on The Archive of Formal Proofs ${ }^{3}$ (as of October 6, 2021) and an additional 57 theories from the Isabelle standard library ${ }^{4}$ to create a corpus of 684 theories. All theories have open-source licenses. Each theory is a self-contained mathematical object, on topics such as foundational logic, advanced analysis, algebra, or cryptography, and consists of multiple files containing proofs. As with the Github corpus, all files that make up a theory are concatenated</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Context</th>
<th style="text-align: left;">Memory</th>
<th style="text-align: left;">XL cache</th>
<th style="text-align: left;">arXiv</th>
<th style="text-align: left;">PG19</th>
<th style="text-align: left;">C4(4K+)</th>
<th style="text-align: left;">GitHub</th>
<th style="text-align: left;">Isabelle</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">512</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">3.29</td>
<td style="text-align: left;">13.71</td>
<td style="text-align: left;">17.20</td>
<td style="text-align: left;">3.05</td>
<td style="text-align: left;">3.09</td>
</tr>
<tr>
<td style="text-align: left;">2048</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">2.69</td>
<td style="text-align: left;">12.37</td>
<td style="text-align: left;">14.81</td>
<td style="text-align: left;">2.22</td>
<td style="text-align: left;">2.39</td>
</tr>
<tr>
<td style="text-align: left;">512</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">512</td>
<td style="text-align: left;">2.67</td>
<td style="text-align: left;">12.34</td>
<td style="text-align: left;">15.38</td>
<td style="text-align: left;">2.26</td>
<td style="text-align: left;">2.46</td>
</tr>
<tr>
<td style="text-align: left;">2048</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">2048</td>
<td style="text-align: left;">2.42</td>
<td style="text-align: left;">11.88</td>
<td style="text-align: left;">14.03</td>
<td style="text-align: left;">2.10</td>
<td style="text-align: left;">2.16</td>
</tr>
<tr>
<td style="text-align: left;">512</td>
<td style="text-align: left;">1536</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">2.61</td>
<td style="text-align: left;">12.50</td>
<td style="text-align: left;">14.97</td>
<td style="text-align: left;">2.20</td>
<td style="text-align: left;">2.33</td>
</tr>
<tr>
<td style="text-align: left;">512</td>
<td style="text-align: left;">8192</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">2.49</td>
<td style="text-align: left;">12.29</td>
<td style="text-align: left;">14.42</td>
<td style="text-align: left;">2.09</td>
<td style="text-align: left;">2.19</td>
</tr>
<tr>
<td style="text-align: left;">512</td>
<td style="text-align: left;">8192</td>
<td style="text-align: left;">512</td>
<td style="text-align: left;">2.37</td>
<td style="text-align: left;">11.93</td>
<td style="text-align: left;">14.04</td>
<td style="text-align: left;">2.03</td>
<td style="text-align: left;">2.08</td>
</tr>
<tr>
<td style="text-align: left;">512</td>
<td style="text-align: left;">65 K</td>
<td style="text-align: left;">512</td>
<td style="text-align: left;">2.31</td>
<td style="text-align: left;">11.62</td>
<td style="text-align: left;">14.04</td>
<td style="text-align: left;">1.87</td>
<td style="text-align: left;">2.06</td>
</tr>
<tr>
<td style="text-align: left;">2048</td>
<td style="text-align: left;">8192</td>
<td style="text-align: left;">2048</td>
<td style="text-align: left;">2.33</td>
<td style="text-align: left;">11.84</td>
<td style="text-align: left;">13.80</td>
<td style="text-align: left;">1.98</td>
<td style="text-align: left;">2.06</td>
</tr>
<tr>
<td style="text-align: left;">2048</td>
<td style="text-align: left;">65 K</td>
<td style="text-align: left;">2048</td>
<td style="text-align: left;">$\mathbf{2 . 2 6}$</td>
<td style="text-align: left;">$\mathbf{1 1 . 3 7}$</td>
<td style="text-align: left;">$\mathbf{1 3 . 6 4}$</td>
<td style="text-align: left;">$\mathbf{1 . 8 0}$</td>
<td style="text-align: left;">$\mathbf{1 . 9 9}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Average token-level perplexities of each model when trained for 500k steps.
together into one long document. Unlike the Github corpus, we order the files according to their import dependencies, so that later files use sub-theorems that are proved in earlier files.</p>
<p>C4(4K+) C4, the colossal cleaned common crawl, is a very large collection of documents that have been scraped from the internet (Raffel et al., 2020). We filtered out all documents that have less than 4096 tokens to focus on documents where memory can have an impact.</p>
<p>PG-19 PG-19 is a large dataset of English-language books, published prior to 1919, which were retrieved from the Project Gutenberg archive (Rae et al., 2020; Sun et al., 2021). PG-19 is one of the few public datasets that only contains full-length books, and has become a benchmark for long-range natural language text modeling.</p>
<h1>4.2 EXPERIMENTAL METHOD</h1>
<p>We used a 12-layer decoder-only transformer (with and without Transformer-XL cache) with an embedding size of 1024, 8 attention heads of dimension 128, and an FFN hidden layer of size 4096. For all of our experiments, we used $k=32$. Unless specified otherwise, we use the 9 th layer as the $k$ NN augmented attention layer. We used a sentence-piece (Kudo \&amp; Richardson, 2018) tokenizer with a vocabulary size of 32 K .</p>
<p>We used the Adafactor optimizer (Shazeer \&amp; Stern, 2018). In preliminary experiments, we conducted a hyperparameter search to determine the optimal learning rate among three choices ( ${3.0,1.0$, $\left.3 \cdot 10^{-1}\right}$ ), and found that 1.0 works best. We used a linear warmup schedule for the first 1000 steps, followed by square root decay. We trained the models from scratch for 500K steps on all the datasets, except for the Isabelle dataset. Isabelle is small, so we stopped training after 100K steps when the model began to overfit. We ran all of our experiments on 32 TPU cores. Our models were implemented in JAX (Bradbury et al., 2018) and Flax (Heek et al., 2020).</p>
<p>When comparing models with different context lengths, we adjusted the batch size (the number of documents in a batch) so that there are always $2^{17}$ tokens in a batch. E.g., a model with a context length of 512 has a batch size of 256, while the 2048 model has a batch size of 64.</p>
<p>We experimented with multiple implementations of approximate $k$ NN lookup with different tradeoffs between quality and computational cost. We did not observe a significant degradation of the model quality when switching to lower quality approximations of $k \mathrm{NN}$, so the model appears to be quite robust with respect to the quality of $k$ NN retrieval. For a model with around 200M trainable parameters the step time increased from 0.2 s to 0.25 s when we added a memory of size 8 K , and to 0.6 s when we added a memory of size 65 K (measured on TPUv3).</p>
<h3>4.3 EFFECT OF EXTERNAL MEMORY</h3>
<p>Adding external memory results in substantial gains across datasets and architectures, as shown in Table 4 Across all five datasets, adding external memory to either the vanilla Transformer or the Transformer-XL architecture improves perplexity by a substantial amount. For example, on</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Context</th>
<th style="text-align: left;">Pretrain</th>
<th style="text-align: left;">Fine-tune</th>
<th style="text-align: left;">Perplexity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">512</td>
<td style="text-align: left;">8192</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">2.37</td>
</tr>
<tr>
<td style="text-align: left;">512</td>
<td style="text-align: left;">65 K</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">2.31</td>
</tr>
<tr>
<td style="text-align: left;">512</td>
<td style="text-align: left;">8192</td>
<td style="text-align: left;">65 K</td>
<td style="text-align: left;">2.32</td>
</tr>
<tr>
<td style="text-align: left;">512</td>
<td style="text-align: left;">8192</td>
<td style="text-align: left;">131 K</td>
<td style="text-align: left;">2.30</td>
</tr>
<tr>
<td style="text-align: left;">512</td>
<td style="text-align: left;">8192</td>
<td style="text-align: left;">262 K</td>
<td style="text-align: left;">2.26</td>
</tr>
<tr>
<td style="text-align: left;">2048</td>
<td style="text-align: left;">8192</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">2.33</td>
</tr>
<tr>
<td style="text-align: left;">2048</td>
<td style="text-align: left;">65 K</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">2.26</td>
</tr>
<tr>
<td style="text-align: left;">2048</td>
<td style="text-align: left;">65 K</td>
<td style="text-align: left;">131 K</td>
<td style="text-align: left;">2.23</td>
</tr>
<tr>
<td style="text-align: left;">2048</td>
<td style="text-align: left;">65 K</td>
<td style="text-align: left;">262 K</td>
<td style="text-align: left;">$\mathbf{2 . 2 1}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Finetuning for 20K steps to make use of a larger memory on the arXiv data set.</p>
<p>C4(4K+) dataset, adding memory of size 8192 improves the perplexity of the vanilla Transformer (with context size 512) from 17.20 to 14.42, and improves Transformer-XL from 15.38 to 14.04.</p>
<p>Increasing the size of the memory increases the benefit of the memory. The best perplexities for all datasets and architectures were obtained with a memory size of 65 K .</p>
<p>Note that Transformer-XL with context size 2048 already has a theoretical receptive field that is quite large. Each token in a higher layer can attend up to 2048 tokens away in the layer below, so the total receptive field is $2048 \cdot 12$ (layers) $\sim 25 \mathrm{~K}$. Nevertheless, we still saw a substantial gain when adding an external memory of size 8192 to this model. $k \mathrm{NN}$ attention into memory would appear to be a more effective way to retrieve information from the distant past than the Transformer-XL cache.</p>
<p>On the other hand, we also saw improvements by adding XL cache to the large-memory (65K) models. In a vanilla (non-XL) Transformer, the first few tokens in a sequence have very little context, and thus have higher perplexity. The XL cache provides additional local short-range context at the start of a sequence, which complements the long-range context provided by external memory.</p>
<p>Interestingly, in a vanilla Transformer, using even a small external memory of size 1536 provides a gain in perplexity which is almost as good as using a local context of size 2048 but no memory (e.g. Table 4). This is surprising, because the external memory is not differentiable, and is added only to one layer of the Transformer, whereas increasing the context size is differentiable and affects all layers. We conclude that the lower layers of a Transformer don't necessarily need long-range context, and having a differentiable memory is not as important as one might suspect.</p>
<h1>4.4 Scaling to larger models</h1>
<p>We scaled up the Transformer model to sizes of 1 and 8 billion parameters. For the 1 billion parameter model, we use 8 layers, 32 heads with head dimension 128, d_model 2048, and d_ff 16384. For the 8 billion parameter model, we use 64 heads, 16 layers, $d_{_} _$model 4096, and $d_{_} \mathrm{ff} 32768$. We used a context size of 2048, memory size of 8192 , and no XL cache. We ran the comparisons to the vanilla Transformer on the arXiv math dataset. Scaling plots are shown in Figure 1.</p>
<p>External memory provides a consistent improvement to the model as it is scaled up. Remarkably, we found that the smaller Memorizing Transformer with just 8 k tokens in memory can match the perplexity of a larger vanilla Transformer which has 5X more trainable parameters.</p>
<h3>4.5 Finetuning on Larger Memories</h3>
<p>Finetuning on a larger memory. In some cases, training was unstable when using large memories, possibly due to distributional shift early in the training (See Section 3.2). Thus, for memories of 131 K or more tokens, we first pretrain the model with a memory size of 8192 or 65 K for 500 K steps, and then finetune it with the larger memory for an additional 20K steps. The results of finetuning on the arXiv Math data set are shown in Table 5. Increasing the size of external memory provided consistent gains up to a size of 262 K . Note that 262 K tokens is longer than almost all of the documents in arXiv, and thus we would not expect to see any gain past this point (see Appendix A).</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: Finetuning a 1B vanilla Transformer model to use external memory of size 65K.</p>
<p>Finetuning a non-memory model to use memory Pretraining can be very costly both in time and computational resources. Thus, a natural question to ask is: can one fine-tune a pretrained Transformer to use external memory? The answer is yes!</p>
<p>We took a pre-trained 1B vanilla Transformer model, and fine-tuned it to use external memory (the 1B models used in Section 4.4). The fine-tuning result is shown in Figure 6. Notice that the model quickly learns to use external memory. Within 20K steps ( $4 \%$ of the pre-training time) the fine-tuned model has already closed $85 \%$ of the gap between it and the 1B Memorizing Transformer, and after 100k steps it has closed the gap entirely.</p>
<h1>4.6 INFORMATION RETRIEVAL PATTERNS</h1>
<p>We conducted a qualitative study of what the model was actually retrieving from external memory, by finding which tokens showed the biggest improvements in cross-entropy loss when the size of the memory was increased, and then examining the top- $k$ retrieved memories for those tokens. We found that the model gained the most when looking up rare words, such as proper names, references, citations, and function names, where the first use of a name is too far away from subsequent uses to fit in the local context. This result is in keeping with the prior analysis of long-context Transformers on PG19 (Sun et al., 2021), which found similar lookup patterns. For this experiment, we used a slightly older version of the architecture without the gating mechanism.</p>
<p>Which tokens show a benefit from memory? Figure 7 shows a visualization of which tokens show an improvement when the size of the external memory is increased. We selected a math paper at random, and plotted the difference in cross entropy loss for each token $x_{i}$ in the paper, comparing two models with the same parameters, but with memories of different sizes. $\Delta_{i}=$ cross-entropy $<em i="i">{8192}\left(x</em>}\right)$ - cross-entropy ${ <em i="i">{32 \mathrm{~K}}\left(x</em>\right)$. Positive values show an improvement in loss.</p>
<p>The $x$-axis on the chart is the token number $i$, while the $y$-axis is $\Delta_{i}$. For the first 8192 tokens, the difference between the two models is zero, since the larger capacity of the 32 K memory isn't being used yet. However, after token 8193, we can see that the larger memory helps, on average, over the smaller memory. The benefit is not universal, since the predictions for some tokens become worse, possibly due to the fact that a relevant retrieved memory no longer makes it into the top- $k$ when the size of the external memory is increased. This figure also shows that the benefit of external memory is somewhat sparse. The improvement in perplexity seems to be mainly driven by a small percentage of tokens that obtain a large improvement in cross-entropy loss when using the larger memory.</p>
<p>What information is being looked up? Given that only a subset of tokens shows improvement from external memory, we did a further investigation into what, exactly, those tokens are using the memory for. We took those tokens which showed the largest improvement in cross-entropy loss, and for each of them tokens, we examined the top- $k$ retrieved memories. We studied arXiv math, Github and Isabelle corpus. For arXiv math and Github, we found the model retrieved function and variable names. See more details with examples in Appendix B.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Difference in loss for each token in a randomly chosen paper, using the same model once with a memory size of 8 K and once with 32 K . Higher numbers mean the longer memory helped in comparison to the shorter memory. This paper is 22 K tokens long.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Query index</th>
<th style="text-align: center;">Input</th>
<th style="text-align: center;">Target</th>
<th style="text-align: center;">Surrounding context</th>
<th style="text-align: center;">Retrieved index</th>
<th style="text-align: center;">Retrieved surrounding context</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">29721</td>
<td style="text-align: center;">mark</td>
<td style="text-align: center;">ov</td>
<td style="text-align: center;">rule prob_space, markov_inequality</td>
<td style="text-align: center;">8088</td>
<td style="text-align: center;">M. t \&lt;l&lt;&gt; X a) \<l> expectation X / t"</td>
</tr>
<tr>
<td style="text-align: center;">40919</td>
<td style="text-align: center;">_</td>
<td style="text-align: center;">th</td>
<td style="text-align: center;">= t subgraph_threshold, H n / p n)</td>
<td style="text-align: center;">27219</td>
<td style="text-align: center;">threshold H n = n powr t &lt;1 / max_density</td>
</tr>
<tr>
<td style="text-align: center;">49699</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">w</td>
<td style="text-align: center;">assumes "orthonormal_system, S w"</td>
<td style="text-align: center;">28050</td>
<td style="text-align: center;">definition orthonormal_system :: "</td>
</tr>
</tbody>
</table>
<p>Table 8: Examples of memory retrieval in the Isabelle dataset. The model is able to find the definition of a lemma from a reference to it. The retrieved surrounding context (highlighted) is the definition body of the mathematical object highlighted in the querying context.</p>
<p>Retrieving mathematical definitions. Our case study on the Isabelle corpus provides one of the clearest illustrations of how a model can make good use of external memory. When predicting the name of a mathematical object or a lemma, the model looked up the definition from earlier in the proof. Examples of this behavior are shown in Table 8. In example 1, the model retrieves a definition within the body of a lemma, markov_inequality. In example 2, it retrieves the definition of a previously defined concept subgraph_threshold. In example 3, it retrieves the definition of orthonormal_system. We manually checked 10 examples where the model made a prediction of lemma names, and 8 out of 10 times model found the body of the lemma it needs to predict. In the other two cases, the model also looked up materials in the immediate vicinity. To the best of our knowledge, this is the first demonstration that attention is capable of looking up definitions and function bodies from a large corpus. The Isabelle case study used a model with two memory layers of size 32 K .</p>
<h1>5 CONCLUSION</h1>
<p>We present a simple extension to the Transformer architecture, called $k \mathrm{NN}$-augmented attention, which dramatically increases the length of the context that a language model can attend to by using $k$-nearest-neighbor lookup into a large external memory. We demonstrate the effectiveness of external memory in a series of language modeling experiments over a variety of long-document datasets, including LaTeX documents, source code, formal proofs, and books.</p>
<p>The Memorizing Transformer shows large improvements in perplexity over the baseline for all of the data sets and architectures that we studied; it is comparable to a vanilla transformer that has 5 times the number of parameters. Perplexity continues to improve with increasing memory size, although there is a point of diminishing returns. Moreover, external memory continues to provide benefits even as the transformer is scaled up from 200M to 8B parameters. Perhaps most intriguingly, a Memorizing Transformer does not need to be pre-trained from scratch; it is possible obtain large gains from adding memory to an existing pre-trained model, and then fine-tuning it.</p>
<p>Unlike other forms of attention, $k \mathrm{NN}$ retrieval can be easily scaled up to huge memory sizes, and is thus potentially able to leverage vast knowledge bases or code repositories. How to make the best use of this capability is a topic for future work.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We want to thank Charles Staats for the many fruitful discussions and detailed comments, Henryk Michalewski for early version of of the memory implementation, Petros Maniatis for his help with our code datasets, Aitor Lewkowycz for his help with larger scale memorizing transformer experiments, Behnam Neyshabur for his comments on finetuning non-memory models, Imanol Schlag for his proofread and detailed comments, and Dennis Lee and Manzil Zaheer for discussions about large-scale attention and retrieval.</p>
<h1>ETHICS</h1>
<p>The ability to memorize large databases of facts could have potential ramifications for society, especially if those databases include sensitive personal information or copyrighted works. However, one advantage of using an external memory is that the memory can be easily cleared of all such information, as we do at the end of each document that we train on. The same is not true of differentiable model parameters, which is what most existing architectures use to store facts and information that they are trained on.</p>
<h2>REPRODUCIBILITY</h2>
<p>Details of our architecture and training hyperparameters are given in Section 4.2. The datasets for C4 and PG-19 are publicly available. Our additional datasets, Github, Isabelle, and ArXiv Math are derived from publicly available data buckets, which we link in the main part of the paper. Subsection 4.1 include details on how we constructed the datasets from those datasets. We plan to release our code as open source.</p>
<h2>REFERENCES</h2>
<p>Joshua Ainslie, Santiago Ontañón, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. ETC: encoding long and structured inputs in transformers. In EMNLP, 2020.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. CoRR, abs/2108.07732, 2021. URL https://arxiv.org/abs/ 2108.07732 .</p>
<p>Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150.</p>
<p>James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv. org/abs/2107.03374.</p>
<p>Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. In $I C L R, 2021$.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.</p>
<p>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In ACL, 2019.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In $A C L, 2019$.</p>
<p>Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. Addressing some limitations of transformers with feedback memory. arXiv preprint arXiv:2002.09402, 2020.</p>
<p>Angela Fan, Claire Gardent, Chloé Braud, and Antoine Bordes. Augmenting transformers with KNN-based composite memory for dialog. Transactions of the Association for Computational Linguistics, 9:82-99, 2021.</p>
<p>Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. In $I C L R, 2017$.</p>
<p>Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In ICML, 2020.</p>
<p>Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. URL https://arxiv.org/ abs/2106.06899.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Retrieval augmented language model pre-training. In ICML, 2020.</p>
<p>Christopher Hahn, Frederik Schmitt, Jens U. Kreber, Markus Norman Rabe, and Bernd Finkbeiner. Teaching temporal logics to neural networks. In $I C L R, 2021$.</p>
<p>Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020. URL http://github.com/google/flax.</p>
<p>Alex Henry, Prudhvi Raj Dachapally, Shubham Shantaram Pawar, and Yuxuan Chen. Query-key normalization for transformers. In EMNLP, 2020.</p>
<p>Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 2021.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In $I C L R, 2020$.</p>
<p>Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In ICLR, 2020.</p>
<p>Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In EMNLP, 2018.</p>
<p>Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. Large memory layers with product keys. In NeurIPS, 2019.</p>
<p>Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke Zettlemoyer. Pre-training via paraphrasing. In NeurIPS, 2020a.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive NLP tasks. In NeurIPS, 2020b.</p>
<p>Wenda Li, Lei Yu, Yuhuai Wu, and Lawrence C. Paulson. Isarstep: a benchmark for high-level mathematical reasoning. In $I C L R, 2021$.</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. DeepMind, 2022.</p>
<p>Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. CoRR, abs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393.</p>
<p>Markus Norman Rabe, Dennis Lee, Kshitij Bansal, and Christian Szegedy. Mathematical reasoning via self-supervised skip-tree training. In $I C L R, 2021$.</p>
<p>Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. In $I C L R, 2020$.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 2020.</p>
<p>Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost. CoRR, abs/2107.05768, 2021. URL https://arxiv.org/abs/2107.05768.</p>
<p>Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53-68, 2021.</p>
<p>Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In ICML, 2018.</p>
<p>Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, and Armand Joulin. Augmenting self-attention with persistent memory. arXiv preprint arXiv:1907.01470, 2019.</p>
<p>Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston, and Angela Fan. Not all memories are created equal: Learning to forget by expiring. In ICML, 2021.</p>
<p>Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language models actually use long-range context? In EMNLP, 2021.</p>
<p>Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. arXiv preprint arXiv:2009.06732, 2020.</p>
<p>Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. In $I C L R, 2021$.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.</p>
<p>Qingxiang Wang, Chad Brown, Cezary Kaliszyk, and Josef Urban. Exploration of neural machine translation in autoformalization of mathematics in mizar. In International Conference on Certified Programs and Proofs, 2020a.</p>
<p>Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020b.</p>
<p>Xun Wang, Haozhi Zhang, Weilin Huang, and Matthew R. Scott. Cross-batch memory for embedding learning. In CVPR, 2020c.</p>
<p>Ronald J. Williams and Jing Peng. An efficient gradient-based algorithm for on-line training of recurrent network trajectories. Neural Computation, 1990.</p>
<p>Dani Yogatama, Cyprien de Masson d'Autume, and Lingpeng Kong. Adaptive semiparametric language models. ACL, 9:362-373, 2021.</p>
<p>Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In NeurIPS, 2020.</p>
<p>Yury Zemlyanskiy, Joshua Ainslie, Michiel de Jong, Philip Pham, Ilya Eckstein, and Fei Sha. Readtwice: Reading very large documents with memories. In ACL: Human Language Technologies, 2021.</p>
<p>Zhenhai Zhu and Radu Soricut. H-transformer-1d: Fast one-dimensional hierarchical attention for sequences. In $A C L, 2021$.</p>
<h1>A LENGTH OF INPUTS</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 9: Histogram of the number of tokens in arXiv math papers dataset. We tuncated the histogram at 500k tokens. The maximum paper had almost 1.6 M tokens.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 10: Histogram of the number of tokens in Github repositories dataset. We cut off the long tail of this plot. The repository with the maximum length has just over 9 M tokens.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 11: Histogram of the number of tokens in Isabelle proof scripts dataset.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 12: Histogram of the number of tokens in PG19 books dataset.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 13: Histogram of the number of tokens in C4 documents filtered by documents that have less than 4096 tokens.</p>
<h1>A. 1 Ablation Studies</h1>
<p>In the following section, we performed ablation studies to investigate the effects of various hyperparameters. Unless otherwise specified, we carried out these experiments with a memorizing transformer with context size 512, XL cache 512 with a memory size of 8192 .</p>
<p>Multiple $k$ NN layers. We experimented with using two $k$ NN layers, rather than just one. However, we did not see further benefits brought by more than multiple retrieval layers.
$k$ NN layer index We experimented with adding the external memory to layer $3,6,9$ and 12 in a 12-layer transformer, with results shown in Table 14. We found that adding memory to the middle of the layer stack will obtain the best result, whereas adding memory to layers either too close to the input or to the output obtained less gains.</p>
<p>Table 14: Different layer index.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Layer index</th>
<th style="text-align: left;">Perplexity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: left;">2.40</td>
</tr>
<tr>
<td style="text-align: left;">6</td>
<td style="text-align: left;">2.36</td>
</tr>
<tr>
<td style="text-align: left;">9</td>
<td style="text-align: left;">2.37</td>
</tr>
<tr>
<td style="text-align: left;">12</td>
<td style="text-align: left;">2.43</td>
</tr>
</tbody>
</table>
<p>Number of neighbors We studied the effects of the number of neighbors we retrieve from memory, with results shown in Table 15. We found that even with 32 number of neighbors, we can already obtain a comparable results with 128 or 256 neighbors.</p>
<p>Table 15: Number of neighbors.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Number of neighbors</th>
<th style="text-align: left;">Perplexity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">32</td>
<td style="text-align: left;">2.38</td>
</tr>
<tr>
<td style="text-align: left;">128</td>
<td style="text-align: left;">2.37</td>
</tr>
<tr>
<td style="text-align: left;">256</td>
<td style="text-align: left;">2.37</td>
</tr>
</tbody>
</table>
<p>Random seeds We measured the statistical significant of the results reported. We did 3 runs with 3 random seeds for Transformer XL of size 512, and also a memorizing transformer with memory size 8192. We measured the standard deviation of perplexities after 500 K steps of training, shown in Table 16. We saw the standard deviation between different runs of the same experiment appears to be much smaller than the gap between different models.</p>
<p>Table 16: Random seeds.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: left;">Perplexity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Transformer XL</td>
<td style="text-align: left;">$2.67 \pm 0.01$</td>
</tr>
<tr>
<td style="text-align: left;">Memorizing Transformer</td>
<td style="text-align: left;">$2.37 \pm 0.005$</td>
</tr>
</tbody>
</table>
<h1>B What DOES THE MODEL RETRIEVE FROM MEMORY?</h1>
<p>Retrieving citation names On arXiv math, several examples are shown in Table 17, which includes both the retrieved token and its surrounding context. We observe that many of the gains in crossentropy loss took place when trying to predict the name of bibitems, citations, or references, by looking up the references and citations used previously in the paper. Such lookups usually span over the entire paper, which is much longer than 8192 tokens, providing a plausible explanation for the gain beyond memory size of 8192 .</p>
<p>Table 17: The table shows several examples of which tokens were retrieved during language modelling of arXiv math dataset. The model is retrieving names of the references from previous passages.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Query index</th>
<th style="text-align: left;">Input</th>
<th style="text-align: left;">Target</th>
<th style="text-align: left;">Surrounding context</th>
<th style="text-align: left;">Retrieved index</th>
<th style="text-align: left;">Retrieved surrounding context</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">20389</td>
<td style="text-align: left;">Mon</td>
<td style="text-align: left;">thus</td>
<td style="text-align: left;">bibitem|</td>
<td style="text-align: left;">ComtetMonthusYor</td>
<td style="text-align: left;">2208</td>
</tr>
<tr>
<td style="text-align: left;">16623</td>
<td style="text-align: left;">cha</td>
<td style="text-align: left;">kra</td>
<td style="text-align: left;">$\backslash$ cite $|$ chakrabarti $|$</td>
<td style="text-align: left;">4677</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">14747</td>
<td style="text-align: left;">as</td>
<td style="text-align: left;">d</td>
<td style="text-align: left;">$\backslash$ supel $|$ asdfg $|$ which</td>
<td style="text-align: left;">3365</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>$\left.\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \begin{array}{l}\text { Retrieving function names from the codebase As with the arXiv papers, we also studied which } \ \text { tokens the model retrieved from memory. As might be expected, the model is often looking up the } \ \text { names of functions, and variables, as shown in Table } 18 .\end{array}\right.$</p>
<p>Table 18: Examples of memory retrieval in the Github dataset. The model looks up how functions are used elsewhere in the repository.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Query index</th>
<th style="text-align: left;">Input</th>
<th style="text-align: left;">Target</th>
<th style="text-align: left;">Surrounding context</th>
<th style="text-align: left;">Retrieved index</th>
<th style="text-align: left;">Retrieved surrounding context</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">23837</td>
<td style="text-align: left;">Fo</td>
<td style="text-align: left;">nte</td>
<td style="text-align: left;">menu_play-&gt; setarFonte</td>
<td style="text-align: left;">14607</td>
<td style="text-align: left;">menu_load-&gt; setarFonte</td>
</tr>
<tr>
<td style="text-align: left;">23825</td>
<td style="text-align: left;">,</td>
<td style="text-align: left;">35</td>
<td style="text-align: left;">hscreen/2-50, 50, 200, 35</td>
<td style="text-align: left;">14599</td>
<td style="text-align: left;">$20, \mathrm{y}+40,200,35$</td>
</tr>
<tr>
<td style="text-align: left;">14546</td>
<td style="text-align: left;">$-&gt;$</td>
<td style="text-align: left;">adi</td>
<td style="text-align: left;">panel-&gt; adicionaComponente</td>
<td style="text-align: left;">5205</td>
<td style="text-align: left;">panel-&gt; adicionaComponente</td>
</tr>
</tbody>
</table>
<h1>B. 1 MORE RETRIEVING EXAMPLES IN FORMAL THEOREM PROVING CORPUS</h1>
<h2>Example 1</h2>
<ul>
<li>Input token index: 64604</li>
<li>Input token: "_"</li>
<li>Target token: "pair"</li>
<li>Surrounding context: )) by (simp add: Fourier_sum_limit_pair [OF f, symmetric] Fourier’</li>
<li>Name needs to be predicted: Fourier_sum_limit_pair</li>
<li>Retrieved token: "Four"</li>
<li>Retrieved token index: 64412</li>
<li>Retrieved context: 2 * n. Fourier_coefficient f k * trigonometric_set k t)</li>
<li>Definition of the name:</li>
</ul>
<div class="codehilite"><pre><span></span><code>lemma Fourier_sum_limit_pair:
    assumes &quot;f absolutely integrable on {-pi..pi}&quot;
    shows &quot;(An, \ \ k-2 <span class="gs">* n. Fourier_coefficient f k *</span> trigonometric_set k t) \ \ \ \ \ \ { \ \ \ \ { \ \ \ \ \ { \ \ \ \ \ { \hs = ?rhs&quot;}}
</code></pre></div>

<p>Figure 19: Definition of Fourier_sum_limit_pair.</p>
<h2>Example 2</h2>
<ul>
<li>Input token index: 46175</li>
<li>Input token: "tri"</li>
<li>Target token: "gon"</li>
<li>Surrounding context: <le>n. a k * trigonometric_set k x)</li>
<li>Name needs to be predicted: orthonormal_system_trigonometric_set</li>
<li>Retrieved token: "gon"</li>
<li>Retrieved token index: 35457</li>
<li>Retrieved context: lemma orthonormal_system_trigonometric_set: \n "orthonormal_system</li>
<li>Definition of the name:</li>
</ul>
<div class="codehilite"><pre><span></span><code>lemma orthonormal_system_trigonometric_set:
    &quot;orthonormal_system {-pi..pi} trigonometric_set&quot;
</code></pre></div>

<p>Figure 20: Definition of orthonormal_system_trigonometric_set.</p>
<h1>Example 3</h1>
<ul>
<li>Input token index: 49760</li>
<li>Input token: "sum"</li>
<li>Target token: "m"</li>
<li>Surrounding context: nusing Fourier_series_square_summable [OF assms, of'</li>
<li>Name needs to be predicted: Fourier_series_square_summable</li>
<li>Retrieved token: "sum"</li>
<li>Retrieved token index: 35457</li>
<li>Retrieved context: lemma Fourier_series_square_summable $\backslash$ n assumes:</li>
<li>Definition of the name:</li>
</ul>
<div class="codehilite"><pre><span></span><code>lemma Fourier_series_square_summable
    assumes as: &quot;orthonormal_system S w&quot; and w: &quot;fli. (w i) square_integrable S&quot;
    and f: &quot;f square_integrable S&quot;
    shows &quot;summable (confine (Ai. (orthonormal_coeff S w f i) &quot; 2) I)&quot;
</code></pre></div>

<p>Figure 21: Definition of Fourier_series_square_summable.</p>
<h2>Example 4</h2>
<ul>
<li>Input token index: 49697</li>
<li>Input token: "爪"</li>
<li>Target token: "system"</li>
<li>Surrounding context: lemma Riemann_lebesgue_square_integrable: nassumes "orthonormal_system S w</li>
<li>Name needs to be predicted: orthonormal_system</li>
<li>Retrieved token: "system"</li>
<li>Retrieved token index: 28052</li>
<li>Retrieved context: definition orthonormal_system :: "\'a::euclidean'</li>
<li>Definition of the name:
definition orthonormal_system :: "'a::euclidean_space set $\Rightarrow$ ('b $\Rightarrow$ 'a $\Rightarrow$ real) $\Rightarrow$ bool" where "orthonormal_system S w $=1 \mathrm{~m} \mathrm{n}$. 12product S (w m) (w n) = (if $m=n$ then 1 else 0 )"</li>
</ul>
<p>Figure 22: Definition of orthonormal_system.</p>
<h1>Example 5</h1>
<ul>
<li>Input token index: 34817</li>
<li>Input token: "."</li>
<li>Target token: "b"</li>
<li>Surrounding context: shows "integrable (lebesgue_on {a..b})</li>
<li>Retrieved token 1: "."</li>
<li>Retrieved token index 1: 2416</li>
<li>Retrieved context 1: lebesgue_on {a..b}) f i</li>
<li>Retrieved token 2: "-" $\square$</li>
<li>Retrieved token index 2: 2445</li>
<li>Retrieved context 2: (lebesgue_on {a-c..b-c}) (</li>
<li>Retrieved token 3: "-" $\square$</li>
<li>Retreived token index 3: 6479</li>
<li>Retrieved context 3: (lebesgue_on {-pi..pi})(</li>
</ul>
<h2>Example 6</h2>
<ul>
<li>Input token index: 49759</li>
<li>Input token: "_""</li>
<li>Target token: "sum"</li>
<li>Surrounding context: 0"\n using Fourier_series_square_summable [OF assms</li>
<li>Retrieved token 1: "set"</li>
<li>Retrieved token index 1: 35044</li>
<li>Retrieved context 1: definition trigonometric_set :: "nat \<Rightarrow></li>
<li>Retrieved token 2: "ier"</li>
<li>Retrieved token index 2: 47272</li>
<li>Retrieved context 2: definition Fourier_coefficient $\backslash$ nwhere</li>
<li>Retrieved token 3: "ine"</li>
<li>Retrieved token index 3: 18160</li>
<li>Retrieved context 3: lemma Schwartz_inequality_strong: \nassumes "f"</li>
<li>Retrieved token 4: "system"</li>
<li>Retrieved token index 4: 28052</li>
<li>Retrieved context 4: definition orthonormal_system :: "\'a::euclidean'</li>
<li>Retrieved token 5: "&lt;"</li>
<li>Retrieved token index 5: 47241</li>
<li>Retrieved context 5: subsection \<open>Convergence wrt the L'</li>
<li>Retrieved token 6: "n"</li>
<li>Retrieved token index 6: 40835</li>
<li>Retrieved context 6: \n subsection \<open>A bit of extra'</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://arxiv.com/help/bulk_data
${ }^{2}$ https://console.cloud.google.com/marketplace/product/github/github/repos
${ }^{3}$ https://www.isa-afp.org/topics.html
${ }^{4}$ https://isabelle.in.tum.de/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1981 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1981</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1981</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-278782301</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.15685v1.pdf" target="_blank">From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems</a></p>
                <p><strong>Paper Abstract:</strong> Foundation models (FMs) are increasingly used to bridge language and action in embodied agents, yet the operational characteristics of different FM integration strategies remain under-explored -- particularly for complex instruction following and versatile action generation in changing environments. This paper examines three paradigms for building robotic systems: end-to-end vision-language-action (VLA) models that implicitly integrate perception and planning, and modular pipelines incorporating either vision-language models (VLMs) or multimodal large language models (LLMs). We evaluate these paradigms through two focused case studies: a complex instruction grounding task assessing fine-grained instruction understanding and cross-modal disambiguation, and an object manipulation task targeting skill transfer via VLA finetuning. Our experiments in zero-shot and few-shot settings reveal trade-offs in generalization and data efficiency. By exploring performance limits, we distill design implications for developing language-driven physical agents and outline emerging challenges and opportunities for FM-powered robotics in real-world conditions.</p>
                <p><strong>Cost:</strong> 0.03</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1981.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1981.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>End-to-end VLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>End-to-end Vision-Language-Action models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Unified models that map raw visual observations and natural language instructions directly to low-level actions, using a single multimodal transformer or diffusion-based policy; includes autoregressive and diffusion action-generation variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>End-to-end VLA (paradigm)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Processes language and vision into a unified latent representation (various tokenization strategies for visual inputs) and directly decodes action sequences via either an autoregressive transformer decoder (conditioned on previous actions and context) or a diffusion denoising model over latent trajectories; leverages large-scale embodied pretraining and then (partial or full) fine-tuning for target tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>varied; examples reported include DinoV2 / SigLIP visual tokenizers or learned compressed action tokenizers (FAST); generally transformer-friendly visual tokenization rather than standard frozen CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>varied; example: Open-VLA's visual tokenizers are used with models pretrained on the Open-X-Embodiment dataset (≈970k real-world robot demonstrations); other referenced VLAs use large robot demonstration corpora and multimodal pretraining datasets</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Implicit joint multimodal latent grounding: visual inputs are tokenized into latent tokens that are attended within the multimodal transformer; language conditions the decoder to generate actions (no explicit symbolic bounding-box grounding by default)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>multi-level but primarily latent sequence of visual tokens (scene-level/object-implicit) that the model attends to during action decoding</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>mostly implicit within latent tokens; some VLA variants (e.g., SpatialVLA) explicitly incorporate 3D/spatial features or spatial relation embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation, instruction following, multi-object pick-and-place, spatial-relation reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>custom cluttered-tabletop manipulation benchmark; LIBERO simulation benchmark (used for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>egocentric wrist-mounted RGB-D (real robot) and photorealistic simulation (LIBERO)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (task completion), macro-accuracy for grounding tasks, training loss/convergence</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Varies by model; fully fine-tuned VLA variants achieve a range (examples: OpenVLA fine-tuned avg ≈76.5% across manipulation suites; NORA variants higher; see model-specific entries)</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Not directly ablated as 'no grounding' for end-to-end VLAs; however, partial fine-tuning experiments show large-capacity VLAs (OpenVLA, π 0) require many iterations and have high variance when adaptation data is limited (qualitative evidence that grounding/data is limiting).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Not directly quantified as an isolated number for the whole paradigm; end-to-end VLA success improves substantially after full fine-tuning on embodied datasets versus partial/no finetuning (qualitative and plotted in Fig.5–6).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Paper reports differences across VLA variants (e.g., SpatialVLA fails on some affordance estimation tasks while NORA and OpenVLA succeed after fine-tuning), and highlights that VLA performance is sensitive to choice of visual tokenization and spatial-feature integration; explicit head-to-head encoder pretraining scale comparisons are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>VLA performance constrained by scarcity and bias in high-quality robot datasets; pretrained biases lead to degraded OOD performance on novel objects, scenes, or morphologies; limited spatial/temporal reasoning and inaccurate affordance estimation (e.g., SpatialVLA's affordance point failures).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Observed failures include: poor convergence under partial fine-tuning (high variance), degradation on unseen objects and sim-to-real transfer, inability to reason about rare/abstract concepts, and unstable multi-step execution without action-chunking leading to collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Handled primarily via fine-tuning (partial and full). Partial fine-tuning on small datasets is slow and unstable; full fine-tuning on large embodied datasets improves OOD performance but requires substantial data and compute.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Degrades substantially in OOD scenarios; some VLAs (NORA, OpenVLA) succeed on specific OOD object manipulation tasks after fine-tuning, while SpatialVLA fails on affordance estimation for unseen objects.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Paper emphasizes that larger pretraining on embodiment datasets improves capabilities but is limited by data scarcity for robotic demonstrations; no exact scaling law provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Early / joint fusion inside a multimodal transformer (visual tokens concatenated/attended with language tokens) or conditioning of diffusion denoiser on language+vision.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Low for end-to-end VLAs: partial fine-tuning on small datasets shows slow convergence and high variance; full fine-tuning requires large datasets (open-x-embodiment, LIBERO) to reach strong performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>End-to-end VLAs can generate streamlined actions and generalize across tasks after large-scale embodied pretraining and full fine-tuning, but are data-hungry, suffer high adaptation variance under few-shot settings, and are fragile under domain shift and perceptual distractors unless explicitly augmented with spatial/affordance modules.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1981.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1981.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Modular VLM pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modular Vision-Language Model pipelines (VLM -> planner)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Perception-specialist VLMs produce structured symbolic outputs (bounding boxes, masks, referring pointers) which are consumed by downstream planners or controllers; emphasizes interpretability and lightweight models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Modular VLM pipeline (paradigm)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A specialist VLM performs open-vocabulary detection or segmentation (e.g., GroundingDINO for boxes, SAM for masks), producing object-centric, symbolic scene representations that a task planner or LLM consumes to select actions; perception and control are disentangled to allow independent tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>object detector / segmentation models (examples used: GroundingDINO for detection, SAM / FastSAM for segmentation, OWL-ViT in testbed)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>GroundingDINO, OWL-ViT, SAM were pretrained on large detection/segmentation corpora / grounded pretraining (reference: GroundingDINO and SAM papers); exact dataset sizes not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Region-level / object-centric grounding: open-vocabulary detection produces bounding boxes and masks tied to referring expressions; LLMs may then reason over that structured representation (often by 'selecting' from detected candidates).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>object-centric / region-level (2D bounding boxes, segmentation masks, referring-expression pointers); optionally hierarchical scene graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>explicit 2D bounding boxes and segmentation masks; pipelines can refine boxes to masks and build floor-room-object hierarchies (some pipelines use explicit 3D scene graphs in referenced works).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>instruction grounding, object grounding, pick-and-place when connected to classical grasp-and-place controllers</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Instruction Grounding benchmark (this paper), modular claw-machine prototype (UR5 testbed), integrated deployment examples in related works</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>real-world tabletop images (cluttered scenes), real robot testbed (UR5), sometimes simulation for downstream tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>macro-accuracy for grounding tasks, success rate for downstream manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Moderate: specialist VLM pipelines deliver competitive object-grounding performance with far smaller parameter budgets (paper claims VLM sizes ≈100M–600M, about 1%–6% of LLM sizes) but underperform on complex instruction grounding vs multimodal LLMs; exact numbers depend on model (see details).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>The paper uses a baseline where an LLM 'guesses' target from detected boxes; this baseline struggles on implicit affordance reasoning and relational grounding, producing large accuracy drops relative to multimodal LLMs (qualitative and reported in grounding experiment tables).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Compared to naive modular pipeline 'guessing', multimodal LLMs improve on implicit instruction grounding substantially (example: modular VLM-LLM pipeline struggles to pick screwdriver in affordance tasks while GPT-class multimodal models achieve high implicit accuracy e.g., GPT-4.5 ≈0.94 on implicit instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>The paper specifically highlights GroundingDINO failure modes (misses featureless metal cans, misses generic 'ball' label while detecting 'blue ball'), showing the detection backbone and open-vocabulary capability materially affects grounding robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Pipeline brittleness: perception errors (missed detections, misclassifications, failure on low-feature objects) propagate to planners; GroundingDINO omitted featureless metal cans and sometimes localized only color-specific instances (e.g., detected 'blue ball' but not generic 'ball'), illustrating detector reliance on visual features.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Failure cases include missed detections (featureless or small objects), inability to infer affordances from detected candidates, and high error rates on relational grounding when the pipeline lacks in-built relational reasoning; these were shown in Figures 9–10 and discussed in B.2.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Modular pipelines are more data-efficient and interpretable, but have limited flexibility under domain shift because perception errors are unmitigated; the paper used human-in-the-loop dataset curation to reduce distributional surprises but reports persistent brittleness in novel scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Specialist VLM pipelines can be moderate on novel objects if detector vocabularies cover them; however, failures with featureless or ambiguous novel objects were specifically observed (e.g., metal cans omitted).</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Paper notes that VLM pipelines are lightweight and can be accurate with far fewer parameters and less compute, but does not provide a quantitative scaling curve.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Late fusion: symbolic outputs (boxes/masks) are passed to a planner/LLM which then reasons (often via in-context selection) rather than tightly fusing raw pixels and language.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Higher sample efficiency vs end-to-end VLAs: the paper emphasizes VLM pipelines achieve reasonable object grounding using much smaller models and less adaptation data (qualitatively less than 1% of parameters of large LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Modular VLM pipelines are interpretable and parameter-efficient for object-level grounding, but are brittle to perception failures and lack affordance and relational reasoning capabilities compared to multimodal LLMs; perception mistakes directly propagate to downstream controllers.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1981.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1981.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multimodal LLM agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multimodal Large Language Model Agents (LLM-orchestrators)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large multimodal LLMs act as cognitive hubs that accept language, call vision tools as functions, reason in-context over tool outputs, and issue high-level action primitives to controllers, enabling rich visual commonsense and dynamic planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multimodal LLM agent (paradigm)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Places a large LLM at the center; the agent receives instructions, invokes vision tools (detectors, depth estimators) through function calls, ingests the structured outputs in-context, performs chain-of-thought style reasoning, and outputs task plans or high-level action primitives to a low-level controller.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>varied; agents call external vision tools such as GroundingDINO, OWL-ViT or rely on integrated vision-enabled LLM backbones (e.g., Llama 3.2 Vision, Gemini variants)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Depends on the LLM backbone (examples: Llama 3.2 Vision pretrained by Meta on large multimodal corpora; Gemini variants pretrained by Google DeepMind); exact datasets referenced but not exhaustively enumerated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Tool-mediated grounding and in-context reasoning: vision tool outputs (detections, depth, etc.) are reasoned over by the LLM via chain-of-thought to map language to objects and actions; grounding leverages language priors and commonsense.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>object-centric / scene-level structured outputs from tools plus language-level internal representations in the LLM</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Often explicit via tool outputs (bounding boxes, depth estimates) and via LLM spatial reasoning; some systems use hierarchical 3D scene graphs in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>complex instruction grounding, cross-modal disambiguation, high-level planning for manipulation and navigation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Instruction Grounding benchmark (this paper), referenced household robot benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>cluttered tabletop photographic images; envisioned for real-world in-home robotics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>macro-accuracy for grounding tasks, success rate for downstream actions, inference cost</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Higher accuracy on complex implicit grounding tasks (examples: GPT-4.5 implicit accuracy ≈0.94 in zero-shot implicit instruction grounding) but incurs high inference cost (GPT-4.5 ≈20× cost of Gemini 2.5).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Modular VLM-LLM baseline (where LLM guesses from detector outputs) performs worse on implicit affordance tasks relative to multimodal LLM agents that directly reason over pixels; no single-number ablation 'no grounding' reported for multimodal agents.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Multimodal LLM agents substantially outperform modular VLM pipelines on implicit instruction grounding due to strong visual commonsense (illustrated by examples where modular pipeline fails to identify affordance-based targets but multimodal LLMs succeed).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>The paper compares proprietary and open-source multimodal LLMs (GPT-4.5, Gemini-2.5-Pro, o4-mini, Llama 3.2 Vision variants) on grounding tasks; results show trade-offs where smaller models (o4-mini) can outperform larger ones (GPT-4.5) on relational reasoning, and quantized Llama 3.2 Vision loses more on implicit/relational tasks than attribute tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>While multimodal LLMs are strong at visual commonsense, they are resource-intensive and still sensitive to noisy or missing tool outputs; they generalize better on complex instruction grounding but incur heavy inference costs that limit deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Relational reasoning accuracy drops across many models; quantized models show disproportionate loss on implicit and relational tasks (14–17% drop) versus attribute tasks (≈4% loss); smaller community models often underperform specialist VLMs in cluttered grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Multimodal LLMs generalize better out-of-distribution on language-heavy grounding because of strong language priors, but still face sim-to-real and perception-tool reliability issues; no specific domain adaptation layer reported beyond in-context tool use and optional fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Better than specialist VLMs on implicit affordance inference and novel-concept grounding due to richer language priors; exact numeric delta depends on model and task (see instruction grounding table entries).</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Paper notes that larger proprietary models (e.g., GPT-4.5) have high grounding performance but at much higher inference cost; some medium-sized models (o4-mini) can outperform larger ones on structured relational tasks, suggesting reasoning architecture matters beyond scale.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Late/tool-fusion combined with in-context multimodal reasoning and chain-of-thought style internal processing (LLM ingests outputs of vision tools rather than raw pixel-and-language early fusion).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High sample-efficiency for zero-shot/few-shot grounding because of strong language priors; expensive at inference time and resource-limited platforms.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Multimodal LLM agents excel at implicit affordance and complex instruction grounding via language priors and in-context reasoning, outperforming modular VLM pipelines on ambiguous tasks, but they trade this for high inference cost and deployment resource constraints; quantization impacts high-level reasoning more than attribute recognition.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1981.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1981.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-VLA (open-source VLA model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source autoregressive VLA that combines a fine-tuned Llama 2 (7B) with DinoV2 and SigLIP visual tokenizers and pretrained on the Open-X-Embodiment dataset of real-world robot demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Open-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive VLA: visual inputs tokenized with DinoV2 and SigLIP into a unified sequence consumed by a Llama-2-based transformer; decoder autoregressively generates action tokens conditioned on the visual-language context and previous actions. Pretrained on Open-X-Embodiment and fine-tuned for downstream manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>DinoV2 and SigLIP visual tokenizers (visual feature tokenization for transformer input)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Visual tokenizers are standard pretraining backbones (DinoV2 pretrained on large image datasets; exact dataset sizes not enumerated here); the overall VLA is pretrained on Open-X-Embodiment (~970k robot demos).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Latent token alignment: language tokens attend to visual tokens produced by DinoV2/SigLIP; grounding occurs implicitly through joint attention and autoregressive action decoding rather than explicit box-level grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>tokenized latent visual tokens representing scene and object-level features (object-implicit)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>primarily implicit spatial information encoded in visual tokens; some variants include spatio-temporal prompting (TraceVLA) to enhance temporal/spatial awareness</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation, multi-object pick-and-place, spatial reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>custom UR5 cluttered-tabletop dataset (partial finetuning), LIBERO simulation benchmark (comparisons in Table 3/6)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>egocentric RGB-D real robot; simulation for benchmark evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (manipulation tasks), convergence behavior during fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>OpenVLA fine-tuned average ≈76.5% across evaluated manipulation suites (reported as best average among baselines without action chunking in text).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Not directly ablated as pure 'no grounding' condition; discussed that OpenVLA suffers degraded performance with distractors (56.7% w/o → 50% w/ distractors in Table 8 for OpenVLA), indicating grounding/perception sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Grounding via visual tokenization and large-scale pretraining enables OpenVLA to achieve competitive manipulation performance after full fine-tuning; exact isolated grounding-improvement figure not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Paper contrasts OpenVLA against SpatialVLA and NORA: OpenVLA succeeds on some OOD manipulation tasks while SpatialVLA fails on affordance points; OpenVLA achieves top average among non-action-chunking baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>OpenVLA shows substantial performance degradation when distractors are introduced (56.7% → 50% success), indicating that perception/grounding errors are a primary failure mode.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>High variance and slow convergence during partial fine-tuning; sensitivity to distractors leading to lower success rates; failures in multi-object tasks without action-chunking or replanning.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Requires substantial fine-tuning on in-domain data to regain robust behavior; partial fine-tuning is slow and unstable under distribution shift.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>OpenVLA can succeed on some OOD object manipulation tasks after full fine-tuning but remains sensitive to unseen distractors and affordance estimation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Performance benefits from pretraining on large embodied datasets (Open-X-Embodiment), but limited by availability of diverse robot demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Early joint fusion via tokenized visual features fed into an autoregressive multimodal transformer</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Relatively low sample efficiency for adaptation: partial finetuning required many epochs and exhibited high variance; full finetuning on large datasets required to reach high success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Open-VLA leverages visual tokenizers and large embodied pretraining to achieve competitive manipulation performance, but grounding remains a bottleneck under distractors and domain shift; action chunking and high-frequency control support can improve real-world execution robustness.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1981.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1981.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NORA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NORA (small open-sourced generalist VLA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small open-source generalist VLA pretrained on Open-X-Embodiment using a Qwen-2.5-VL-3B backbone (reported as recent VLA variant) with strong fine-tuning transfer and action-chunking variants for long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NORA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>VLA built on a Qwen-2.5-VL-3B backbone (per text), pretrained on embodied demonstration corpora and fine-tunable; supports action-chunking variants (NORA-LONG) to predict multiple actions per window, improving long-horizon planning and transfer in simulation and benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Qwen-2.5-VL-3B vision-language backbone; specifics of visual encoder layers are not enumerated beyond backbone identity</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Pretrained on Open-X-Embodiment and other embodied datasets referenced in the literature; exact dataset sizes not re-listed here.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Joint multimodal backbone: language and vision processed by multimodal LLM backbone that produces action tokens, implicitly grounding instructions to visual tokens; supports action-chunking to improve temporal windowing.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>latent multimodal tokens representing objects/scene (object-implicit but with demonstrated ability on object-centric tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Some spatial reasoning capability; NORA-LONG benefits from windowed action prediction for long-horizon spatial tasks (achieves high LIBERO-Long success)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation, spatial reasoning, multi-object pick-and-place, long-horizon tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>LIBERO Simulation Benchmark; real-world 9-task evaluation on WidowX; custom UR5 testbed experiments (some comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>simulation (LIBERO) and real-world robot deployments (WidowX, UR5)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>NORA-Long fine-tuned achieves highest overall average success rate ≈87.9% on LIBERO benchmark (reported); NORA (regular) fine-tuned average ≈76.5% (reported for non-action-chunked baselines). On distractors test: NORA success drops from 83.3% (w/o distractors) to 56.7% (w/ distractors).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Not directly ablated to remove grounding; the distractor experiment shows large performance degradation (83.3% → 56.7%) when visual grounding is challenged, effectively demonstrating grounding's contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Action-chunking and pretrained embodiment knowledge substantially increase long-horizon success (NORA-Long-fine-tuned outperforms non-chunked variants by large margins on LIBERO-Long; exact per-task deltas shown in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Paper compares NORA variants to OpenVLA and SpatialVLA: NORA and OpenVLA succeed on several OOD tasks while SpatialVLA fails on affordance estimation; NORA-Long-fine-tuned achieves the best LIBERO-Long performance (74.6% on that suite as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>NORA shows substantial sensitivity to distractors (drop of ≈26.6 percentage points when distractors introduced), highlighting perception/grounding fragility under out-of-distribution perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Failures include stopping after first object in multi-object tasks for some action-chunked executions (real-world execution issues), significant drop in success under distractors, and variable performance under partial finetuning due to large capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Performs well after full fine-tuning on large embodied datasets; action-chunking pretraining improves transfer to long-horizon tasks in simulation; still suffers sim-to-real degradation and distractor sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Performs well on some OOD object manipulation tasks after full fine-tuning, but performance drops with distractors and unseen affordance scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Pretraining with action-chunking and on large embodied corpora (Open-X-Embodiment) yields superior long-horizon and OOD performance; specific scaling curves not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Early joint fusion in a multimodal LLM backbone with temporal windowing (action chunks) for sequence-level action prediction</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Partial finetuning on small custom datasets was slow; full fine-tuning on large datasets required for top performance. Action-chunk pretraining improves sample utility for long-horizon tasks in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>NORA shows that small, focused VLA backbones with action-chunking and embodied pretraining can achieve state-of-the-art success on long-horizon manipulation benchmarks, but perception/grounding remains the Achilles' heel under distractors and sim-to-real shifts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1981.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1981.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GroundingDINO+SAM baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GroundingDINO detector with SAM refinement (modular perception pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical modular perception pipeline: GroundingDINO for open-vocabulary detection followed by SAM for refining masks; used in modular VLM pipelines and in the paper's real testbed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GroundingDINO: Marrying dino with grounded pretraining for open-set object detection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GroundingDINO + SAM pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GroundingDINO performs open-vocabulary detection to produce candidate bounding boxes conditioned on textual queries; outputs can be refined into segmentation masks with SAM / FastSAM and then fed to planners or LLMs for selection and control.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>open-vocabulary object detector (GroundingDINO) + segmentation model (SAM / FastSAM)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>GroundingDINO uses grounded pretraining on detection datasets (as per referenced work); SAM pretrained for segmentation on large mask datasets (referenced SAM literature). Exact dataset sizes not re-stated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Region-level grounding by matching language queries to detected bounding boxes (open-vocabulary detection) and refining boxes into masks; outputs are symbolic (box/mask) representing candidate referents.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>object-centric / region-level (2D bounding boxes and segmentation masks)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D bounding boxes and masks; no explicit 3D coordinates unless combined with depth estimators externally</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object grounding, instruction grounding, integration in modular claw-machine testbed</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Instruction Grounding benchmark; UR5 modular claw-machine prototype testbed</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>real-world tabletop images, robot-mounted egocentric views</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>object detection recall/precision (implied), grounding accuracy when chained to LLM selection, downstream task success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: solid for many visible objects but specific failure cases reported (misses featureless metal cans and generic 'ball' labels while detecting color-specific instances). No single consolidated numeric performance value provided in text for the pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>When GroundingDINO misses objects or produces incomplete detections, the downstream pipeline (LLM guessing from detections) fails—examples demonstrate pipeline cannot recover missing candidates and thus fails grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Using GroundingDINO+SAM provides interpretable region proposals enabling lightweight controllers to work; however, when detector misses candidates, performance collapses — improvement is conditional on detector reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Paper demonstrates GroundingDINO's limitations empirically: successful detection of 'blue ball' but failed detection of 'ball' and near-omission of featureless metal cans, illustrating detector-dependent grounding failures.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>GroundingDINO's reliance on visible features causes missed detections; such perception omissions propagate and cause downstream grounding/planning failures.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Concrete failure examples: missing generic categories (ball), failing on low-texture metal cans; these lead to incorrect or absent candidate sets for the planner/LLM, yielding execution errors (illustrated in Figures 9–10).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>No explicit domain adaptation applied in the baseline; performance sensitive to scene appearance and object texture (failure under featureless objects).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Mixed: works for many household objects but misses low-feature or visually ambiguous novel objects; specific failure cases documented.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Late fusion: detector outputs are passed to downstream LLM/planner; no joint pixel-language attention in this pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High sample-efficiency for many common objects (zero-shot open-vocabulary detection), but brittle when detector misses objects.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>GroundingDINO+SAM enables interpretable, lightweight modular grounding but exhibits concrete perception failure modes (missed detections on low-feature objects and overly feature-specific labels) that directly break downstream grounding and control.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1981.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1981.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3.2-Vision INT4 ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quantization (INT4) of Llama 3.2 Vision 11B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study of the effect of INT4 quantization on Llama 3.2 Vision 11B for instruction grounding: INT4 reduces memory but disproportionately impacts high-level reasoning (implicit & relational grounding) relative to attribute recognition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3.2 Vision 11B (INT4 quantized ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Quantized variant of the Llama 3.2 Vision multimodal model compressed to INT4 precision to reduce model size >70%; evaluated on instruction grounding types (implicit, attribute-based, relationship-based).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Llama 3.2 Vision multimodal backbone (11B) quantized to INT4</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Pretrained Llama 3.2 Vision backbone on large multimodal corpora (exact corpora per Meta's release); quantization applied post-training</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Multimodal attention within Llama 3.2 Vision; quantization reduces numeric precision but same architectural grounding via attention between visual and language tokens</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>scene/object implicit multimodal token representations (unchanged structurally by quantization)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit in multimodal tokens (no added explicit spatial module described in ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>complex instruction grounding (implicit, attribute, relational)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Instruction Grounding benchmark (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>cluttered tabletop photographic scenes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>macro-accuracy across instruction types</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Quantization effects: INT4 quantization reduces model size >70%; observed relative accuracy drops of 14%–17% for implicit and relational instruction grounding and only ~4% loss for attribute grounding in Llama 3.2 Vision 11B.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>N/A (this is a precision ablation). The ablation demonstrates degraded grounding quality under lower numeric precision.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Not applicable (this ablation quantifies loss); maintaining full precision yields ~14–17% higher accuracy on implicit/relational tasks compared to INT4 in this model.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Comparison between full-precision and INT4-quantized Llama 3.2 Vision shows disproportionate impact on high-level grounding tasks (implicit & relational) relative to attribute tasks; smaller community models also underperform specialist VLMs in cluttered grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Quantization disproportionately harms high-level reasoning and relational grounding capabilities, indicating a precision-sensitive perceptual/ reasoning bottleneck for complex grounding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Quantized models lose robustness on implicit and relational instructions (14–17% accuracy drop), whereas attribute-based grounding remains relatively robust (~4% loss).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Quantization reduces robustness; INT4 models are less reliable under ambiguous or relational instructions and thus less capable in domain-shifted or low-resource deployments requiring fine-grained reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Not specifically reported for novel objects; general degradation in implicit/relational grounding suggests reduced ability to infer affordances or relations for unseen concepts under quantization.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Quantization reduces effective performance regardless of pretraining scale; paper emphasizes the need for fine-grained quantization strategies to preserve high-level reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Cross-attention multimodal fusion within the Llama 3.2 Vision architecture; quantization impairs the precision of these attentions.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Quantized models trade memory and speed for accuracy; INT4 is attractive for resource-limited deployment but at a measurable cost to complex grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>INT4 quantization yields large memory savings but meaningfully degrades complex grounding (implicit & relational) while sparing simple attribute recognition, highlighting that grounding-sensitive components are precision-sensitive and require careful quantization strategies for embodied deployment.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1981.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1981.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Action chunking ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action Chunking (temporal windowing) ablation in VLA models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Investigation of predicting multiple actions per model forward pass (action chunking) showing strong benefits for high-frequency control and long-horizon tasks in simulation, but mixed real-world execution depending on controller replanning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Action chunking (ablation/technique)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models predict action chunks (multiple actions per inference window) rather than single-step commands; execution can either play entire chunk sequentially or perform only the first action and replan, affecting safety and long-horizon completion.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>applies to VLA models (NORA, SpatialVLA, OpenVLA variants) rather than a visual encoder per se</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Not applicable (technique applied during VLA pretraining/fine-tuning; NORA-Long pretrained with chunking in text)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Temporal-windowed grounding where a sequence of latent visual-language conditioned actions are produced together, improving temporal consistency and long-horizon reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>sequence-level (trajectory-level) action representation; improves temporal coherence of grounding-to-action mapping</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>maintains whatever spatial encoding the base VLA provides; chunking helps reason over extended spatial-temporal windows</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>long-horizon manipulation, multi-object pick-and-place, temporal planning</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>LIBERO Simulation Benchmark (LIBERO-Long), real-world WidowX tasks</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>sim (LIBERO 20Hz) and real-world platforms (WidowX, lower frequency controllers)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate, collision rate, ability to complete multi-step tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>In simulation: NORA-finetuned-AC and NORA-Long-finetuned notably outperform non-chunked variants across LIBERO benchmarks; NORA-Long-fine-tuned achieves LIBERO-Long success 74.6% and highest average ≈87.9% overall. In real robot initial naive chunk execution caused collisions; executing only first action in chunk improved safety and yielded e.g., 80% success for 'put carrot in pot' but 0% final success for some multi-object tasks if only first chunk executed.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Not directly applicable; ablation compares chunked vs non-chunked grounding-action generation: chunking improves long-horizon success in simulation substantially; in real low-frequency systems chunking requires replanning to avoid collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Action-chunking pretraining yields large gains on long-horizon tasks in simulation (NORA-Long-fine-tuned best on LIBERO-Long), but naive execution of whole chunks on low-frequency real robots can cause accidents; selective chunk execution with replanning mitigates safety issues.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Chunking amplifies the effect of incorrect grounding over longer temporal horizons; when perception errors occur inside a predicted chunk they can accumulate into collisions or multi-step failures, especially on low-frequency controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Real-world chunk execution initially caused collisions due to large accumulated movements; partial execution (only first action) avoids collisions but may stop multi-step behavior prematurely, reducing final success on multi-object tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Action-chunking helps transfer to long-horizon tasks in simulation, but physical deployment requires controller and replanning compatibility (higher control frequency) to realize benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Chunking itself is orthogonal to novel-object grounding; performance gains on long-horizon tasks assume reliable grounding across chunk horizon.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Pretraining with action-chunking (NORA-Long) yields better long-horizon generalization than adding chunking only at fine-tuning time; indicates value of pretraining with chunked trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Temporal sequence generation of action tokens conditioned on multimodal context; fusion occurs as in base VLA model.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Action-chunking variants (pretrained) demonstrate better sample efficiency for long-horizon tasks in simulation compared to non-chunked models fine-tuned on the same data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Action-chunking substantially improves long-horizon task performance in simulation and is transferable when the control stack supports higher-frequency execution and replanning; chunking exacerbates the consequences of grounding/perception errors if not combined with high-rate control or replanning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1981.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1981.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction Grounding benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction Grounding benchmark (this paper's curated dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A new dataset of 30 cluttered tabletop images and 473 curated instructions across implicit, attribute, relationship and multi-referent instruction types, designed to stress visual commonsense and cross-modal disambiguation in grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruction Grounding benchmark (paper dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Each image contains multiple household objects annotated with visual prompts (object indices); paired instructions probe affordance (implicit), attribute-based, and spatial relationship grounding; includes multi-turn questions and human-verified instruction generation.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>N/A (benchmark dataset used to evaluate various VLMs and multimodal LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Used to evaluate grounding mechanisms of evaluated models (object-centric VLM detection + LLM selection baseline, multimodal LLM direct grounding, and VLA implicit grounding).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>object-centric (annotated object indices, enabling region-level evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D image annotations with explicit spatial relationships (used in relational instructions); dataset includes spatial relationship vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>complex instruction grounding (mapping free-form instructions to object indices in cluttered scenes)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Instruction Grounding benchmark (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>real-world tabletop photographs sampled from robot action sequences (egocentric views)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>macro-average accuracy across instruction types (implicit, attribute, relational)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples from experiments: GPT-4.5 implicit accuracy = 0.94 (zero-shot); o4-mini and Gemini 2.5-Pro achieved >0.80 accuracy on relational reasoning in some settings; quantized Llama 3.2 Vision showed 14–17% drops on implicit/relational instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Modular VLM-LLM baseline (LLM 'guesses' among detected boxes) struggles on implicit affordance grounding; explicit numeric gaps are in the paper's Table 5 and Figure 4.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Multimodal LLMs substantially outperform modular VLM-LLM pipeline on implicit grounding due to affordance reasoning (e.g., modular pipeline frequently fails to select screwdriver in affordance prompts where multimodal LLMs succeed).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Benchmark used to compare specialist VLMs, proprietary multimodal LLMs and open-source LLMs (including quantized variants); showed that specialist VLMs can be outperformed by multimodal LLMs on implicit and relational grounding, and that quantization disproportionately harms complex instruction types.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Benchmark design minimized extraneous visual priors to focus on grounding. Results and failure analyses identified perception (missed detections) and relational reasoning as major bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Relational instructions cause large accuracy drops across models; implicit affordance grounding exposes modular pipelines' inability to infer function from perception-only outputs; detector-specific misses (GroundingDINO) lead to pipeline failures.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Benchmark images are sampled to reduce lighting/camera confounds; still models pretrained on different datasets show varied robustness, highlighting domain-prior effects.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Not reported as a single number; dataset includes objects common to training corpora to minimize vision prior effects; nevertheless, models still show failures on low-feature or ambiguous items.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Benchmark analyses show model scale and training variants matter (e.g., some medium-sized models outperform larger ones on relational tasks), but no strict scaling law is derived.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Used to evaluate both early-joint-fusion (multimodal LLMs) and late-fusion modular pipelines; shows trade-offs between interpretability and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Designed as zero-shot/few-shot evaluation; multimodal LLM agents demonstrate strong zero-shot/few-shot capability whereas VLAs require finetuning for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>The benchmark reveals that (1) multimodal LLMs excel at implicit/affordance grounding due to language priors, (2) modular VLM pipelines are interpretable but brittle to missed detections and lack affordance reasoning, (3) quantization impacts high-level grounding disproportionately, and (4) relational grounding remains a widely shared weakness across architectures.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Grounding dino: Marrying dino with grounded pretraining for open-set object detection <em>(Rating: 2)</em></li>
                <li>Diffusion policy: Visuomotor policy learning via action diffusion <em>(Rating: 2)</em></li>
                <li>Open-VLA: An open-source vision-language-action model <em>(Rating: 2)</em></li>
                <li>Nora: A small open-sourced generalist vision language action model for embodied tasks <em>(Rating: 2)</em></li>
                <li>SpatialVLA: Exploring spatial representations for visual-language-action model <em>(Rating: 2)</em></li>
                <li>Gemini Robotics: Bringing AI into the Physical World <em>(Rating: 2)</em></li>
                <li>RT-1: Robotics transformer for real-world control at scale <em>(Rating: 2)</em></li>
                <li>Libero: Benchmarking knowledge transfer for lifelong robot learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1981",
    "paper_id": "paper-278782301",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "End-to-end VLA",
            "name_full": "End-to-end Vision-Language-Action models",
            "brief_description": "Unified models that map raw visual observations and natural language instructions directly to low-level actions, using a single multimodal transformer or diffusion-based policy; includes autoregressive and diffusion action-generation variants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "End-to-end VLA (paradigm)",
            "model_description": "Processes language and vision into a unified latent representation (various tokenization strategies for visual inputs) and directly decodes action sequences via either an autoregressive transformer decoder (conditioned on previous actions and context) or a diffusion denoising model over latent trajectories; leverages large-scale embodied pretraining and then (partial or full) fine-tuning for target tasks.",
            "visual_encoder_type": "varied; examples reported include DinoV2 / SigLIP visual tokenizers or learned compressed action tokenizers (FAST); generally transformer-friendly visual tokenization rather than standard frozen CLIP",
            "visual_encoder_pretraining": "varied; example: Open-VLA's visual tokenizers are used with models pretrained on the Open-X-Embodiment dataset (≈970k real-world robot demonstrations); other referenced VLAs use large robot demonstration corpora and multimodal pretraining datasets",
            "grounding_mechanism": "Implicit joint multimodal latent grounding: visual inputs are tokenized into latent tokens that are attended within the multimodal transformer; language conditions the decoder to generate actions (no explicit symbolic bounding-box grounding by default)",
            "representation_level": "multi-level but primarily latent sequence of visual tokens (scene-level/object-implicit) that the model attends to during action decoding",
            "spatial_representation": "mostly implicit within latent tokens; some VLA variants (e.g., SpatialVLA) explicitly incorporate 3D/spatial features or spatial relation embeddings",
            "embodied_task_type": "object manipulation, instruction following, multi-object pick-and-place, spatial-relation reasoning",
            "embodied_task_name": "custom cluttered-tabletop manipulation benchmark; LIBERO simulation benchmark (used for comparison)",
            "visual_domain": "egocentric wrist-mounted RGB-D (real robot) and photorealistic simulation (LIBERO)",
            "performance_metric": "success rate (task completion), macro-accuracy for grounding tasks, training loss/convergence",
            "performance_value": "Varies by model; fully fine-tuned VLA variants achieve a range (examples: OpenVLA fine-tuned avg ≈76.5% across manipulation suites; NORA variants higher; see model-specific entries)",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Not directly ablated as 'no grounding' for end-to-end VLAs; however, partial fine-tuning experiments show large-capacity VLAs (OpenVLA, π 0) require many iterations and have high variance when adaptation data is limited (qualitative evidence that grounding/data is limiting).",
            "grounding_improvement": "Not directly quantified as an isolated number for the whole paradigm; end-to-end VLA success improves substantially after full fine-tuning on embodied datasets versus partial/no finetuning (qualitative and plotted in Fig.5–6).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Paper reports differences across VLA variants (e.g., SpatialVLA fails on some affordance estimation tasks while NORA and OpenVLA succeed after fine-tuning), and highlights that VLA performance is sensitive to choice of visual tokenization and spatial-feature integration; explicit head-to-head encoder pretraining scale comparisons are limited.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "VLA performance constrained by scarcity and bias in high-quality robot datasets; pretrained biases lead to degraded OOD performance on novel objects, scenes, or morphologies; limited spatial/temporal reasoning and inaccurate affordance estimation (e.g., SpatialVLA's affordance point failures).",
            "failure_mode_analysis": "Observed failures include: poor convergence under partial fine-tuning (high variance), degradation on unseen objects and sim-to-real transfer, inability to reason about rare/abstract concepts, and unstable multi-step execution without action-chunking leading to collisions.",
            "domain_shift_handling": "Handled primarily via fine-tuning (partial and full). Partial fine-tuning on small datasets is slow and unstable; full fine-tuning on large embodied datasets improves OOD performance but requires substantial data and compute.",
            "novel_object_performance": "Degrades substantially in OOD scenarios; some VLAs (NORA, OpenVLA) succeed on specific OOD object manipulation tasks after fine-tuning, while SpatialVLA fails on affordance estimation for unseen objects.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Paper emphasizes that larger pretraining on embodiment datasets improves capabilities but is limited by data scarcity for robotic demonstrations; no exact scaling law provided.",
            "fusion_mechanism": "Early / joint fusion inside a multimodal transformer (visual tokens concatenated/attended with language tokens) or conditioning of diffusion denoiser on language+vision.",
            "sample_efficiency": "Low for end-to-end VLAs: partial fine-tuning on small datasets shows slow convergence and high variance; full fine-tuning requires large datasets (open-x-embodiment, LIBERO) to reach strong performance.",
            "key_findings_grounding": "End-to-end VLAs can generate streamlined actions and generalize across tasks after large-scale embodied pretraining and full fine-tuning, but are data-hungry, suffer high adaptation variance under few-shot settings, and are fragile under domain shift and perceptual distractors unless explicitly augmented with spatial/affordance modules.",
            "uuid": "e1981.0"
        },
        {
            "name_short": "Modular VLM pipeline",
            "name_full": "Modular Vision-Language Model pipelines (VLM -&gt; planner)",
            "brief_description": "Perception-specialist VLMs produce structured symbolic outputs (bounding boxes, masks, referring pointers) which are consumed by downstream planners or controllers; emphasizes interpretability and lightweight models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Modular VLM pipeline (paradigm)",
            "model_description": "A specialist VLM performs open-vocabulary detection or segmentation (e.g., GroundingDINO for boxes, SAM for masks), producing object-centric, symbolic scene representations that a task planner or LLM consumes to select actions; perception and control are disentangled to allow independent tuning.",
            "visual_encoder_type": "object detector / segmentation models (examples used: GroundingDINO for detection, SAM / FastSAM for segmentation, OWL-ViT in testbed)",
            "visual_encoder_pretraining": "GroundingDINO, OWL-ViT, SAM were pretrained on large detection/segmentation corpora / grounded pretraining (reference: GroundingDINO and SAM papers); exact dataset sizes not specified in this paper.",
            "grounding_mechanism": "Region-level / object-centric grounding: open-vocabulary detection produces bounding boxes and masks tied to referring expressions; LLMs may then reason over that structured representation (often by 'selecting' from detected candidates).",
            "representation_level": "object-centric / region-level (2D bounding boxes, segmentation masks, referring-expression pointers); optionally hierarchical scene graphs.",
            "spatial_representation": "explicit 2D bounding boxes and segmentation masks; pipelines can refine boxes to masks and build floor-room-object hierarchies (some pipelines use explicit 3D scene graphs in referenced works).",
            "embodied_task_type": "instruction grounding, object grounding, pick-and-place when connected to classical grasp-and-place controllers",
            "embodied_task_name": "Instruction Grounding benchmark (this paper), modular claw-machine prototype (UR5 testbed), integrated deployment examples in related works",
            "visual_domain": "real-world tabletop images (cluttered scenes), real robot testbed (UR5), sometimes simulation for downstream tasks",
            "performance_metric": "macro-accuracy for grounding tasks, success rate for downstream manipulation",
            "performance_value": "Moderate: specialist VLM pipelines deliver competitive object-grounding performance with far smaller parameter budgets (paper claims VLM sizes ≈100M–600M, about 1%–6% of LLM sizes) but underperform on complex instruction grounding vs multimodal LLMs; exact numbers depend on model (see details).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "The paper uses a baseline where an LLM 'guesses' target from detected boxes; this baseline struggles on implicit affordance reasoning and relational grounding, producing large accuracy drops relative to multimodal LLMs (qualitative and reported in grounding experiment tables).",
            "grounding_improvement": "Compared to naive modular pipeline 'guessing', multimodal LLMs improve on implicit instruction grounding substantially (example: modular VLM-LLM pipeline struggles to pick screwdriver in affordance tasks while GPT-class multimodal models achieve high implicit accuracy e.g., GPT-4.5 ≈0.94 on implicit instructions).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "The paper specifically highlights GroundingDINO failure modes (misses featureless metal cans, misses generic 'ball' label while detecting 'blue ball'), showing the detection backbone and open-vocabulary capability materially affects grounding robustness.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Pipeline brittleness: perception errors (missed detections, misclassifications, failure on low-feature objects) propagate to planners; GroundingDINO omitted featureless metal cans and sometimes localized only color-specific instances (e.g., detected 'blue ball' but not generic 'ball'), illustrating detector reliance on visual features.",
            "failure_mode_analysis": "Failure cases include missed detections (featureless or small objects), inability to infer affordances from detected candidates, and high error rates on relational grounding when the pipeline lacks in-built relational reasoning; these were shown in Figures 9–10 and discussed in B.2.",
            "domain_shift_handling": "Modular pipelines are more data-efficient and interpretable, but have limited flexibility under domain shift because perception errors are unmitigated; the paper used human-in-the-loop dataset curation to reduce distributional surprises but reports persistent brittleness in novel scenes.",
            "novel_object_performance": "Specialist VLM pipelines can be moderate on novel objects if detector vocabularies cover them; however, failures with featureless or ambiguous novel objects were specifically observed (e.g., metal cans omitted).",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Paper notes that VLM pipelines are lightweight and can be accurate with far fewer parameters and less compute, but does not provide a quantitative scaling curve.",
            "fusion_mechanism": "Late fusion: symbolic outputs (boxes/masks) are passed to a planner/LLM which then reasons (often via in-context selection) rather than tightly fusing raw pixels and language.",
            "sample_efficiency": "Higher sample efficiency vs end-to-end VLAs: the paper emphasizes VLM pipelines achieve reasonable object grounding using much smaller models and less adaptation data (qualitatively less than 1% of parameters of large LLMs).",
            "key_findings_grounding": "Modular VLM pipelines are interpretable and parameter-efficient for object-level grounding, but are brittle to perception failures and lack affordance and relational reasoning capabilities compared to multimodal LLMs; perception mistakes directly propagate to downstream controllers.",
            "uuid": "e1981.1"
        },
        {
            "name_short": "Multimodal LLM agents",
            "name_full": "Multimodal Large Language Model Agents (LLM-orchestrators)",
            "brief_description": "Large multimodal LLMs act as cognitive hubs that accept language, call vision tools as functions, reason in-context over tool outputs, and issue high-level action primitives to controllers, enabling rich visual commonsense and dynamic planning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Multimodal LLM agent (paradigm)",
            "model_description": "Places a large LLM at the center; the agent receives instructions, invokes vision tools (detectors, depth estimators) through function calls, ingests the structured outputs in-context, performs chain-of-thought style reasoning, and outputs task plans or high-level action primitives to a low-level controller.",
            "visual_encoder_type": "varied; agents call external vision tools such as GroundingDINO, OWL-ViT or rely on integrated vision-enabled LLM backbones (e.g., Llama 3.2 Vision, Gemini variants)",
            "visual_encoder_pretraining": "Depends on the LLM backbone (examples: Llama 3.2 Vision pretrained by Meta on large multimodal corpora; Gemini variants pretrained by Google DeepMind); exact datasets referenced but not exhaustively enumerated in this paper.",
            "grounding_mechanism": "Tool-mediated grounding and in-context reasoning: vision tool outputs (detections, depth, etc.) are reasoned over by the LLM via chain-of-thought to map language to objects and actions; grounding leverages language priors and commonsense.",
            "representation_level": "object-centric / scene-level structured outputs from tools plus language-level internal representations in the LLM",
            "spatial_representation": "Often explicit via tool outputs (bounding boxes, depth estimates) and via LLM spatial reasoning; some systems use hierarchical 3D scene graphs in related work.",
            "embodied_task_type": "complex instruction grounding, cross-modal disambiguation, high-level planning for manipulation and navigation",
            "embodied_task_name": "Instruction Grounding benchmark (this paper), referenced household robot benchmarks",
            "visual_domain": "cluttered tabletop photographic images; envisioned for real-world in-home robotics",
            "performance_metric": "macro-accuracy for grounding tasks, success rate for downstream actions, inference cost",
            "performance_value": "Higher accuracy on complex implicit grounding tasks (examples: GPT-4.5 implicit accuracy ≈0.94 in zero-shot implicit instruction grounding) but incurs high inference cost (GPT-4.5 ≈20× cost of Gemini 2.5).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Modular VLM-LLM baseline (where LLM guesses from detector outputs) performs worse on implicit affordance tasks relative to multimodal LLM agents that directly reason over pixels; no single-number ablation 'no grounding' reported for multimodal agents.",
            "grounding_improvement": "Multimodal LLM agents substantially outperform modular VLM pipelines on implicit instruction grounding due to strong visual commonsense (illustrated by examples where modular pipeline fails to identify affordance-based targets but multimodal LLMs succeed).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "The paper compares proprietary and open-source multimodal LLMs (GPT-4.5, Gemini-2.5-Pro, o4-mini, Llama 3.2 Vision variants) on grounding tasks; results show trade-offs where smaller models (o4-mini) can outperform larger ones (GPT-4.5) on relational reasoning, and quantized Llama 3.2 Vision loses more on implicit/relational tasks than attribute tasks.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "While multimodal LLMs are strong at visual commonsense, they are resource-intensive and still sensitive to noisy or missing tool outputs; they generalize better on complex instruction grounding but incur heavy inference costs that limit deployment.",
            "failure_mode_analysis": "Relational reasoning accuracy drops across many models; quantized models show disproportionate loss on implicit and relational tasks (14–17% drop) versus attribute tasks (≈4% loss); smaller community models often underperform specialist VLMs in cluttered grounding.",
            "domain_shift_handling": "Multimodal LLMs generalize better out-of-distribution on language-heavy grounding because of strong language priors, but still face sim-to-real and perception-tool reliability issues; no specific domain adaptation layer reported beyond in-context tool use and optional fine-tuning.",
            "novel_object_performance": "Better than specialist VLMs on implicit affordance inference and novel-concept grounding due to richer language priors; exact numeric delta depends on model and task (see instruction grounding table entries).",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Paper notes that larger proprietary models (e.g., GPT-4.5) have high grounding performance but at much higher inference cost; some medium-sized models (o4-mini) can outperform larger ones on structured relational tasks, suggesting reasoning architecture matters beyond scale.",
            "fusion_mechanism": "Late/tool-fusion combined with in-context multimodal reasoning and chain-of-thought style internal processing (LLM ingests outputs of vision tools rather than raw pixel-and-language early fusion).",
            "sample_efficiency": "High sample-efficiency for zero-shot/few-shot grounding because of strong language priors; expensive at inference time and resource-limited platforms.",
            "key_findings_grounding": "Multimodal LLM agents excel at implicit affordance and complex instruction grounding via language priors and in-context reasoning, outperforming modular VLM pipelines on ambiguous tasks, but they trade this for high inference cost and deployment resource constraints; quantization impacts high-level reasoning more than attribute recognition.",
            "uuid": "e1981.2"
        },
        {
            "name_short": "Open-VLA",
            "name_full": "Open-VLA (open-source VLA model)",
            "brief_description": "An open-source autoregressive VLA that combines a fine-tuned Llama 2 (7B) with DinoV2 and SigLIP visual tokenizers and pretrained on the Open-X-Embodiment dataset of real-world robot demonstrations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Open-VLA",
            "model_description": "Autoregressive VLA: visual inputs tokenized with DinoV2 and SigLIP into a unified sequence consumed by a Llama-2-based transformer; decoder autoregressively generates action tokens conditioned on the visual-language context and previous actions. Pretrained on Open-X-Embodiment and fine-tuned for downstream manipulation tasks.",
            "visual_encoder_type": "DinoV2 and SigLIP visual tokenizers (visual feature tokenization for transformer input)",
            "visual_encoder_pretraining": "Visual tokenizers are standard pretraining backbones (DinoV2 pretrained on large image datasets; exact dataset sizes not enumerated here); the overall VLA is pretrained on Open-X-Embodiment (~970k robot demos).",
            "grounding_mechanism": "Latent token alignment: language tokens attend to visual tokens produced by DinoV2/SigLIP; grounding occurs implicitly through joint attention and autoregressive action decoding rather than explicit box-level grounding.",
            "representation_level": "tokenized latent visual tokens representing scene and object-level features (object-implicit)",
            "spatial_representation": "primarily implicit spatial information encoded in visual tokens; some variants include spatio-temporal prompting (TraceVLA) to enhance temporal/spatial awareness",
            "embodied_task_type": "object manipulation, multi-object pick-and-place, spatial reasoning",
            "embodied_task_name": "custom UR5 cluttered-tabletop dataset (partial finetuning), LIBERO simulation benchmark (comparisons in Table 3/6)",
            "visual_domain": "egocentric RGB-D real robot; simulation for benchmark evaluation",
            "performance_metric": "success rate (manipulation tasks), convergence behavior during fine-tuning",
            "performance_value": "OpenVLA fine-tuned average ≈76.5% across evaluated manipulation suites (reported as best average among baselines without action chunking in text).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Not directly ablated as pure 'no grounding' condition; discussed that OpenVLA suffers degraded performance with distractors (56.7% w/o → 50% w/ distractors in Table 8 for OpenVLA), indicating grounding/perception sensitivity.",
            "grounding_improvement": "Grounding via visual tokenization and large-scale pretraining enables OpenVLA to achieve competitive manipulation performance after full fine-tuning; exact isolated grounding-improvement figure not provided.",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Paper contrasts OpenVLA against SpatialVLA and NORA: OpenVLA succeeds on some OOD manipulation tasks while SpatialVLA fails on affordance points; OpenVLA achieves top average among non-action-chunking baselines.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "OpenVLA shows substantial performance degradation when distractors are introduced (56.7% → 50% success), indicating that perception/grounding errors are a primary failure mode.",
            "failure_mode_analysis": "High variance and slow convergence during partial fine-tuning; sensitivity to distractors leading to lower success rates; failures in multi-object tasks without action-chunking or replanning.",
            "domain_shift_handling": "Requires substantial fine-tuning on in-domain data to regain robust behavior; partial fine-tuning is slow and unstable under distribution shift.",
            "novel_object_performance": "OpenVLA can succeed on some OOD object manipulation tasks after full fine-tuning but remains sensitive to unseen distractors and affordance estimation errors.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Performance benefits from pretraining on large embodied datasets (Open-X-Embodiment), but limited by availability of diverse robot demonstrations.",
            "fusion_mechanism": "Early joint fusion via tokenized visual features fed into an autoregressive multimodal transformer",
            "sample_efficiency": "Relatively low sample efficiency for adaptation: partial finetuning required many epochs and exhibited high variance; full finetuning on large datasets required to reach high success rates.",
            "key_findings_grounding": "Open-VLA leverages visual tokenizers and large embodied pretraining to achieve competitive manipulation performance, but grounding remains a bottleneck under distractors and domain shift; action chunking and high-frequency control support can improve real-world execution robustness.",
            "uuid": "e1981.3"
        },
        {
            "name_short": "NORA",
            "name_full": "NORA (small open-sourced generalist VLA)",
            "brief_description": "A small open-source generalist VLA pretrained on Open-X-Embodiment using a Qwen-2.5-VL-3B backbone (reported as recent VLA variant) with strong fine-tuning transfer and action-chunking variants for long-horizon tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "NORA",
            "model_description": "VLA built on a Qwen-2.5-VL-3B backbone (per text), pretrained on embodied demonstration corpora and fine-tunable; supports action-chunking variants (NORA-LONG) to predict multiple actions per window, improving long-horizon planning and transfer in simulation and benchmarks.",
            "visual_encoder_type": "Qwen-2.5-VL-3B vision-language backbone; specifics of visual encoder layers are not enumerated beyond backbone identity",
            "visual_encoder_pretraining": "Pretrained on Open-X-Embodiment and other embodied datasets referenced in the literature; exact dataset sizes not re-listed here.",
            "grounding_mechanism": "Joint multimodal backbone: language and vision processed by multimodal LLM backbone that produces action tokens, implicitly grounding instructions to visual tokens; supports action-chunking to improve temporal windowing.",
            "representation_level": "latent multimodal tokens representing objects/scene (object-implicit but with demonstrated ability on object-centric tasks)",
            "spatial_representation": "Some spatial reasoning capability; NORA-LONG benefits from windowed action prediction for long-horizon spatial tasks (achieves high LIBERO-Long success)",
            "embodied_task_type": "object manipulation, spatial reasoning, multi-object pick-and-place, long-horizon tasks",
            "embodied_task_name": "LIBERO Simulation Benchmark; real-world 9-task evaluation on WidowX; custom UR5 testbed experiments (some comparisons)",
            "visual_domain": "simulation (LIBERO) and real-world robot deployments (WidowX, UR5)",
            "performance_metric": "success rate",
            "performance_value": "NORA-Long fine-tuned achieves highest overall average success rate ≈87.9% on LIBERO benchmark (reported); NORA (regular) fine-tuned average ≈76.5% (reported for non-action-chunked baselines). On distractors test: NORA success drops from 83.3% (w/o distractors) to 56.7% (w/ distractors).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Not directly ablated to remove grounding; the distractor experiment shows large performance degradation (83.3% → 56.7%) when visual grounding is challenged, effectively demonstrating grounding's contribution.",
            "grounding_improvement": "Action-chunking and pretrained embodiment knowledge substantially increase long-horizon success (NORA-Long-fine-tuned outperforms non-chunked variants by large margins on LIBERO-Long; exact per-task deltas shown in Table 3).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Paper compares NORA variants to OpenVLA and SpatialVLA: NORA and OpenVLA succeed on several OOD tasks while SpatialVLA fails on affordance estimation; NORA-Long-fine-tuned achieves the best LIBERO-Long performance (74.6% on that suite as reported).",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "NORA shows substantial sensitivity to distractors (drop of ≈26.6 percentage points when distractors introduced), highlighting perception/grounding fragility under out-of-distribution perturbations.",
            "failure_mode_analysis": "Failures include stopping after first object in multi-object tasks for some action-chunked executions (real-world execution issues), significant drop in success under distractors, and variable performance under partial finetuning due to large capacity.",
            "domain_shift_handling": "Performs well after full fine-tuning on large embodied datasets; action-chunking pretraining improves transfer to long-horizon tasks in simulation; still suffers sim-to-real degradation and distractor sensitivity.",
            "novel_object_performance": "Performs well on some OOD object manipulation tasks after full fine-tuning, but performance drops with distractors and unseen affordance scenarios.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Pretraining with action-chunking and on large embodied corpora (Open-X-Embodiment) yields superior long-horizon and OOD performance; specific scaling curves not reported.",
            "fusion_mechanism": "Early joint fusion in a multimodal LLM backbone with temporal windowing (action chunks) for sequence-level action prediction",
            "sample_efficiency": "Partial finetuning on small custom datasets was slow; full fine-tuning on large datasets required for top performance. Action-chunk pretraining improves sample utility for long-horizon tasks in simulation.",
            "key_findings_grounding": "NORA shows that small, focused VLA backbones with action-chunking and embodied pretraining can achieve state-of-the-art success on long-horizon manipulation benchmarks, but perception/grounding remains the Achilles' heel under distractors and sim-to-real shifts.",
            "uuid": "e1981.4"
        },
        {
            "name_short": "GroundingDINO+SAM baseline",
            "name_full": "GroundingDINO detector with SAM refinement (modular perception pipeline)",
            "brief_description": "A practical modular perception pipeline: GroundingDINO for open-vocabulary detection followed by SAM for refining masks; used in modular VLM pipelines and in the paper's real testbed.",
            "citation_title": "GroundingDINO: Marrying dino with grounded pretraining for open-set object detection",
            "mention_or_use": "use",
            "model_name": "GroundingDINO + SAM pipeline",
            "model_description": "GroundingDINO performs open-vocabulary detection to produce candidate bounding boxes conditioned on textual queries; outputs can be refined into segmentation masks with SAM / FastSAM and then fed to planners or LLMs for selection and control.",
            "visual_encoder_type": "open-vocabulary object detector (GroundingDINO) + segmentation model (SAM / FastSAM)",
            "visual_encoder_pretraining": "GroundingDINO uses grounded pretraining on detection datasets (as per referenced work); SAM pretrained for segmentation on large mask datasets (referenced SAM literature). Exact dataset sizes not re-stated in this paper.",
            "grounding_mechanism": "Region-level grounding by matching language queries to detected bounding boxes (open-vocabulary detection) and refining boxes into masks; outputs are symbolic (box/mask) representing candidate referents.",
            "representation_level": "object-centric / region-level (2D bounding boxes and segmentation masks)",
            "spatial_representation": "2D bounding boxes and masks; no explicit 3D coordinates unless combined with depth estimators externally",
            "embodied_task_type": "object grounding, instruction grounding, integration in modular claw-machine testbed",
            "embodied_task_name": "Instruction Grounding benchmark; UR5 modular claw-machine prototype testbed",
            "visual_domain": "real-world tabletop images, robot-mounted egocentric views",
            "performance_metric": "object detection recall/precision (implied), grounding accuracy when chained to LLM selection, downstream task success rate",
            "performance_value": "Qualitative: solid for many visible objects but specific failure cases reported (misses featureless metal cans and generic 'ball' labels while detecting color-specific instances). No single consolidated numeric performance value provided in text for the pipeline.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "When GroundingDINO misses objects or produces incomplete detections, the downstream pipeline (LLM guessing from detections) fails—examples demonstrate pipeline cannot recover missing candidates and thus fails grounding.",
            "grounding_improvement": "Using GroundingDINO+SAM provides interpretable region proposals enabling lightweight controllers to work; however, when detector misses candidates, performance collapses — improvement is conditional on detector reliability.",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Paper demonstrates GroundingDINO's limitations empirically: successful detection of 'blue ball' but failed detection of 'ball' and near-omission of featureless metal cans, illustrating detector-dependent grounding failures.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "GroundingDINO's reliance on visible features causes missed detections; such perception omissions propagate and cause downstream grounding/planning failures.",
            "failure_mode_analysis": "Concrete failure examples: missing generic categories (ball), failing on low-texture metal cans; these lead to incorrect or absent candidate sets for the planner/LLM, yielding execution errors (illustrated in Figures 9–10).",
            "domain_shift_handling": "No explicit domain adaptation applied in the baseline; performance sensitive to scene appearance and object texture (failure under featureless objects).",
            "novel_object_performance": "Mixed: works for many household objects but misses low-feature or visually ambiguous novel objects; specific failure cases documented.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Late fusion: detector outputs are passed to downstream LLM/planner; no joint pixel-language attention in this pipeline.",
            "sample_efficiency": "High sample-efficiency for many common objects (zero-shot open-vocabulary detection), but brittle when detector misses objects.",
            "key_findings_grounding": "GroundingDINO+SAM enables interpretable, lightweight modular grounding but exhibits concrete perception failure modes (missed detections on low-feature objects and overly feature-specific labels) that directly break downstream grounding and control.",
            "uuid": "e1981.5"
        },
        {
            "name_short": "Llama3.2-Vision INT4 ablation",
            "name_full": "Quantization (INT4) of Llama 3.2 Vision 11B",
            "brief_description": "Study of the effect of INT4 quantization on Llama 3.2 Vision 11B for instruction grounding: INT4 reduces memory but disproportionately impacts high-level reasoning (implicit & relational grounding) relative to attribute recognition.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 3.2 Vision 11B (INT4 quantized ablation)",
            "model_description": "Quantized variant of the Llama 3.2 Vision multimodal model compressed to INT4 precision to reduce model size &gt;70%; evaluated on instruction grounding types (implicit, attribute-based, relationship-based).",
            "visual_encoder_type": "Llama 3.2 Vision multimodal backbone (11B) quantized to INT4",
            "visual_encoder_pretraining": "Pretrained Llama 3.2 Vision backbone on large multimodal corpora (exact corpora per Meta's release); quantization applied post-training",
            "grounding_mechanism": "Multimodal attention within Llama 3.2 Vision; quantization reduces numeric precision but same architectural grounding via attention between visual and language tokens",
            "representation_level": "scene/object implicit multimodal token representations (unchanged structurally by quantization)",
            "spatial_representation": "implicit in multimodal tokens (no added explicit spatial module described in ablation)",
            "embodied_task_type": "complex instruction grounding (implicit, attribute, relational)",
            "embodied_task_name": "Instruction Grounding benchmark (this paper)",
            "visual_domain": "cluttered tabletop photographic scenes",
            "performance_metric": "macro-accuracy across instruction types",
            "performance_value": "Quantization effects: INT4 quantization reduces model size &gt;70%; observed relative accuracy drops of 14%–17% for implicit and relational instruction grounding and only ~4% loss for attribute grounding in Llama 3.2 Vision 11B.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "N/A (this is a precision ablation). The ablation demonstrates degraded grounding quality under lower numeric precision.",
            "grounding_improvement": "Not applicable (this ablation quantifies loss); maintaining full precision yields ~14–17% higher accuracy on implicit/relational tasks compared to INT4 in this model.",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Comparison between full-precision and INT4-quantized Llama 3.2 Vision shows disproportionate impact on high-level grounding tasks (implicit & relational) relative to attribute tasks; smaller community models also underperform specialist VLMs in cluttered grounding.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Quantization disproportionately harms high-level reasoning and relational grounding capabilities, indicating a precision-sensitive perceptual/ reasoning bottleneck for complex grounding tasks.",
            "failure_mode_analysis": "Quantized models lose robustness on implicit and relational instructions (14–17% accuracy drop), whereas attribute-based grounding remains relatively robust (~4% loss).",
            "domain_shift_handling": "Quantization reduces robustness; INT4 models are less reliable under ambiguous or relational instructions and thus less capable in domain-shifted or low-resource deployments requiring fine-grained reasoning.",
            "novel_object_performance": "Not specifically reported for novel objects; general degradation in implicit/relational grounding suggests reduced ability to infer affordances or relations for unseen concepts under quantization.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Quantization reduces effective performance regardless of pretraining scale; paper emphasizes the need for fine-grained quantization strategies to preserve high-level reasoning.",
            "fusion_mechanism": "Cross-attention multimodal fusion within the Llama 3.2 Vision architecture; quantization impairs the precision of these attentions.",
            "sample_efficiency": "Quantized models trade memory and speed for accuracy; INT4 is attractive for resource-limited deployment but at a measurable cost to complex grounding.",
            "key_findings_grounding": "INT4 quantization yields large memory savings but meaningfully degrades complex grounding (implicit & relational) while sparing simple attribute recognition, highlighting that grounding-sensitive components are precision-sensitive and require careful quantization strategies for embodied deployment.",
            "uuid": "e1981.6"
        },
        {
            "name_short": "Action chunking ablation",
            "name_full": "Action Chunking (temporal windowing) ablation in VLA models",
            "brief_description": "Investigation of predicting multiple actions per model forward pass (action chunking) showing strong benefits for high-frequency control and long-horizon tasks in simulation, but mixed real-world execution depending on controller replanning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Action chunking (ablation/technique)",
            "model_description": "Models predict action chunks (multiple actions per inference window) rather than single-step commands; execution can either play entire chunk sequentially or perform only the first action and replan, affecting safety and long-horizon completion.",
            "visual_encoder_type": "applies to VLA models (NORA, SpatialVLA, OpenVLA variants) rather than a visual encoder per se",
            "visual_encoder_pretraining": "Not applicable (technique applied during VLA pretraining/fine-tuning; NORA-Long pretrained with chunking in text)",
            "grounding_mechanism": "Temporal-windowed grounding where a sequence of latent visual-language conditioned actions are produced together, improving temporal consistency and long-horizon reasoning",
            "representation_level": "sequence-level (trajectory-level) action representation; improves temporal coherence of grounding-to-action mapping",
            "spatial_representation": "maintains whatever spatial encoding the base VLA provides; chunking helps reason over extended spatial-temporal windows",
            "embodied_task_type": "long-horizon manipulation, multi-object pick-and-place, temporal planning",
            "embodied_task_name": "LIBERO Simulation Benchmark (LIBERO-Long), real-world WidowX tasks",
            "visual_domain": "sim (LIBERO 20Hz) and real-world platforms (WidowX, lower frequency controllers)",
            "performance_metric": "success rate, collision rate, ability to complete multi-step tasks",
            "performance_value": "In simulation: NORA-finetuned-AC and NORA-Long-finetuned notably outperform non-chunked variants across LIBERO benchmarks; NORA-Long-fine-tuned achieves LIBERO-Long success 74.6% and highest average ≈87.9% overall. In real robot initial naive chunk execution caused collisions; executing only first action in chunk improved safety and yielded e.g., 80% success for 'put carrot in pot' but 0% final success for some multi-object tasks if only first chunk executed.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Not directly applicable; ablation compares chunked vs non-chunked grounding-action generation: chunking improves long-horizon success in simulation substantially; in real low-frequency systems chunking requires replanning to avoid collisions.",
            "grounding_improvement": "Action-chunking pretraining yields large gains on long-horizon tasks in simulation (NORA-Long-fine-tuned best on LIBERO-Long), but naive execution of whole chunks on low-frequency real robots can cause accidents; selective chunk execution with replanning mitigates safety issues.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Chunking amplifies the effect of incorrect grounding over longer temporal horizons; when perception errors occur inside a predicted chunk they can accumulate into collisions or multi-step failures, especially on low-frequency controllers.",
            "failure_mode_analysis": "Real-world chunk execution initially caused collisions due to large accumulated movements; partial execution (only first action) avoids collisions but may stop multi-step behavior prematurely, reducing final success on multi-object tasks.",
            "domain_shift_handling": "Action-chunking helps transfer to long-horizon tasks in simulation, but physical deployment requires controller and replanning compatibility (higher control frequency) to realize benefits.",
            "novel_object_performance": "Chunking itself is orthogonal to novel-object grounding; performance gains on long-horizon tasks assume reliable grounding across chunk horizon.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Pretraining with action-chunking (NORA-Long) yields better long-horizon generalization than adding chunking only at fine-tuning time; indicates value of pretraining with chunked trajectories.",
            "fusion_mechanism": "Temporal sequence generation of action tokens conditioned on multimodal context; fusion occurs as in base VLA model.",
            "sample_efficiency": "Action-chunking variants (pretrained) demonstrate better sample efficiency for long-horizon tasks in simulation compared to non-chunked models fine-tuned on the same data.",
            "key_findings_grounding": "Action-chunking substantially improves long-horizon task performance in simulation and is transferable when the control stack supports higher-frequency execution and replanning; chunking exacerbates the consequences of grounding/perception errors if not combined with high-rate control or replanning.",
            "uuid": "e1981.7"
        },
        {
            "name_short": "Instruction Grounding benchmark",
            "name_full": "Instruction Grounding benchmark (this paper's curated dataset)",
            "brief_description": "A new dataset of 30 cluttered tabletop images and 473 curated instructions across implicit, attribute, relationship and multi-referent instruction types, designed to stress visual commonsense and cross-modal disambiguation in grounding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Instruction Grounding benchmark (paper dataset)",
            "model_description": "Each image contains multiple household objects annotated with visual prompts (object indices); paired instructions probe affordance (implicit), attribute-based, and spatial relationship grounding; includes multi-turn questions and human-verified instruction generation.",
            "visual_encoder_type": "N/A (benchmark dataset used to evaluate various VLMs and multimodal LLMs)",
            "visual_encoder_pretraining": "N/A",
            "grounding_mechanism": "Used to evaluate grounding mechanisms of evaluated models (object-centric VLM detection + LLM selection baseline, multimodal LLM direct grounding, and VLA implicit grounding).",
            "representation_level": "object-centric (annotated object indices, enabling region-level evaluation)",
            "spatial_representation": "2D image annotations with explicit spatial relationships (used in relational instructions); dataset includes spatial relationship vocabulary.",
            "embodied_task_type": "complex instruction grounding (mapping free-form instructions to object indices in cluttered scenes)",
            "embodied_task_name": "Instruction Grounding benchmark (this paper)",
            "visual_domain": "real-world tabletop photographs sampled from robot action sequences (egocentric views)",
            "performance_metric": "macro-average accuracy across instruction types (implicit, attribute, relational)",
            "performance_value": "Examples from experiments: GPT-4.5 implicit accuracy = 0.94 (zero-shot); o4-mini and Gemini 2.5-Pro achieved &gt;0.80 accuracy on relational reasoning in some settings; quantized Llama 3.2 Vision showed 14–17% drops on implicit/relational instructions.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Modular VLM-LLM baseline (LLM 'guesses' among detected boxes) struggles on implicit affordance grounding; explicit numeric gaps are in the paper's Table 5 and Figure 4.",
            "grounding_improvement": "Multimodal LLMs substantially outperform modular VLM-LLM pipeline on implicit grounding due to affordance reasoning (e.g., modular pipeline frequently fails to select screwdriver in affordance prompts where multimodal LLMs succeed).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Benchmark used to compare specialist VLMs, proprietary multimodal LLMs and open-source LLMs (including quantized variants); showed that specialist VLMs can be outperformed by multimodal LLMs on implicit and relational grounding, and that quantization disproportionately harms complex instruction types.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Benchmark design minimized extraneous visual priors to focus on grounding. Results and failure analyses identified perception (missed detections) and relational reasoning as major bottlenecks.",
            "failure_mode_analysis": "Relational instructions cause large accuracy drops across models; implicit affordance grounding exposes modular pipelines' inability to infer function from perception-only outputs; detector-specific misses (GroundingDINO) lead to pipeline failures.",
            "domain_shift_handling": "Benchmark images are sampled to reduce lighting/camera confounds; still models pretrained on different datasets show varied robustness, highlighting domain-prior effects.",
            "novel_object_performance": "Not reported as a single number; dataset includes objects common to training corpora to minimize vision prior effects; nevertheless, models still show failures on low-feature or ambiguous items.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Benchmark analyses show model scale and training variants matter (e.g., some medium-sized models outperform larger ones on relational tasks), but no strict scaling law is derived.",
            "fusion_mechanism": "Used to evaluate both early-joint-fusion (multimodal LLMs) and late-fusion modular pipelines; shows trade-offs between interpretability and reasoning.",
            "sample_efficiency": "Designed as zero-shot/few-shot evaluation; multimodal LLM agents demonstrate strong zero-shot/few-shot capability whereas VLAs require finetuning for some tasks.",
            "key_findings_grounding": "The benchmark reveals that (1) multimodal LLMs excel at implicit/affordance grounding due to language priors, (2) modular VLM pipelines are interpretable but brittle to missed detections and lack affordance reasoning, (3) quantization impacts high-level grounding disproportionately, and (4) relational grounding remains a widely shared weakness across architectures.",
            "uuid": "e1981.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Grounding dino: Marrying dino with grounded pretraining for open-set object detection",
            "rating": 2
        },
        {
            "paper_title": "Diffusion policy: Visuomotor policy learning via action diffusion",
            "rating": 2
        },
        {
            "paper_title": "Open-VLA: An open-source vision-language-action model",
            "rating": 2
        },
        {
            "paper_title": "Nora: A small open-sourced generalist vision language action model for embodied tasks",
            "rating": 2
        },
        {
            "paper_title": "SpatialVLA: Exploring spatial representations for visual-language-action model",
            "rating": 2
        },
        {
            "paper_title": "Gemini Robotics: Bringing AI into the Physical World",
            "rating": 2
        },
        {
            "paper_title": "RT-1: Robotics transformer for real-world control at scale",
            "rating": 2
        },
        {
            "paper_title": "Libero: Benchmarking knowledge transfer for lifelong robot learning",
            "rating": 2
        }
    ],
    "cost": 0.029711,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems
21 May 2025</p>
<p>Xiuchao Sui xiuchao.sui@gmail.com 
Institute of High Performance Computing
A*STARSingapore</p>
<p>Daiying Tian 
Institute of High Performance Computing
A*STARSingapore</p>
<p>Qi Sun 
Singapore University of Technology and Design
Singapore</p>
<p>Ruirui Chen 
Institute of High Performance Computing
A*STARSingapore</p>
<p>Dongkyu Choi 
Institute of High Performance Computing
A*STARSingapore</p>
<p>Kenneth Kwok 
Institute of High Performance Computing
A*STARSingapore</p>
<p>Soujanya Poria 
Singapore University of Technology and Design
Singapore</p>
<p>From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems
21 May 20256C98037933C757D0636BC6630A99CFFAarXiv:2505.15685v1[cs.RO]Manipulation Generalization Novel objects and scenes Robot morphology Open language Cross-modal ambiguity
Foundation models (FMs) are increasingly used to bridge language and action in embodied agents, yet the operational characteristics of different FM integration strategies remain underexplored-particularly for complex instruction following and versatile action generation in changing environments.This paper examines three paradigms for building robotic systems: end-to-end vision-language-action (VLA) models that implicitly integrate perception and planning, and modular pipelines incorporating either vision-language models (VLMs) or multimodal large language models (LLMs).We evaluate these paradigms through two focused case studies: a complex instruction grounding task assessing fine-grained instruction understanding and cross-modal disambiguation, and an object manipulation task targeting skill transfer via VLA finetuning.Our experiments in zeroshot and few-shot settings reveal trade-offs in generalization and data efficiency.By exploring performance limits, we distill design implications for developing language-driven physical agents and outline emerging challenges and opportunities for FM-powered robotics in realworld conditions.</p>
<p>Introduction</p>
<p>Natural language is emerging as a universal interface for embodied robotic systems.Recent foundation models (FMs) allow robots to follow free-form instructions in perception, reasoning, and motor commands, offering the promise of language-grounded autonomy.They include multimodal large language models (LLMs) (Grattafiori et al., 2024;Bai et al., 2025;Lu et al., 2024), vision-language models (VLMs) (Liu et al., 2024a;Ravi et al., 2025;Ren et al., 2024;Li et al., 2023), and vision-language-action (VLA) models (Kim Figure 1: Challenges of foundation models in embodied robotic systems include cross-modal instruction grounding, generalization across environments and morphologies, and data-efficient adaptation for the real world.et al., 2024;Zheng et al., 2025;Qu et al., 2025;Bu et al., 2025).</p>
<p>However, turning the promise of languagegrounded autonomy into deployable systems is highly challenging.Robots must (i) map ambiguous instructions to the physical world (instruction grounding), (ii) execute reliably across novel objects, scenes, and robot morphologies (generalizable execution), and (iii) achieving the aforementioned goals with limited data (efficient adaptation).How well different FM integration strategies meet these competing requirements remains underexplored (Fig. 1).</p>
<p>To our best knowledge, this work delivers the first head-to-head empirical comparison of three prevalent integration paradigms: end-to-end VLAs that directly map language and vision to actions, multimodal LLM agents that orchestrate perception and control through tool calls, and modular VLM pipelines that couple perception-specialist FMs with task-specific planners (Fig. 2; Table 1).</p>
<p>We evaluate these paradigms through tabletop case studies designed to highlight their complementary strengths and limitations.Specifically, we consider two task categories: (i) Complex Instruction Grounding, which probes fine-grained understanding and cross-modal disambiguation (Sec.3); and</p>
<p>(ii) Object Manipulation, which measures the ability to transfer learned skills after VLA fine-tuning under distribution shifts, complemented by comparative and ablation studies (Sec.4).</p>
<p>Our zero-shot grounding experiments reveal distinct trade-offs across integration strategies.VLM pipelines prioritize interpretability and data efficiency, sacrificing flexibility and peak performance.While underperforming on handling complex instruction grounding, they deliver moderate performance in object grounding-using less than 1% of the parameters required by multimodal LLMs.In contrast, multimodal LLM agents generalize better on complex instructions but incur significantly higher inference costs.Notably, smaller reasoning-focused models such as GPT-4o-mini can even outperform larger models like GPT-4o on certain tasks.VLAs, with their tightly coupled perception-to-action pathways, support streamlined action generation, yet struggle to reason about rare or abstract concepts.We further examine the tradeoff between model size and performance by analyzing quantization effects on open-source multimodal LLMs.These findings offer practical guidance for developing language-driven robotic systems under real-world constraints.</p>
<p>Within the VLA paradigm, we categorize models by their action generation mechanismsautoregressive (Vaswani et al., 2017;Kim et al., 2024;Hung et al., 2025) and diffusion-based approaches (Ho et al., 2020;Chi et al., 2023;Reuss et al., 2024;Wen et al., 2024).We evaluate their adaptability through fine-tuning under distribution shifts, mirroring real-world deployment scenarios.</p>
<p>To assess generalization, we analyze robustness to environmental perturbations and variation in object appearance and robot morphology.</p>
<p>To summarize, the main contributions of this work are as follows: • To our best knowledge, we present the first systematic comparison of end-to-end VLA, modular VLM, and modular multimodal LLM architectures on the same set of embodied tasks.</p>
<p>• We release a dataset and accompanying code 1 that supports the evaluation of complex instruction grounding and manipulation transfercovering accordance comprehension, crossmodal reasoning, and motor adaptation.</p>
<p>• We benchmark state-of-the-art VLAs and multimodal LLMs, offering timely insight into the capabilities and failure modes of modern FMs in robotics.</p>
<p>• We distill actionable trade-offs that practitioners can apply when choosing an FM stack for language-driven embodied agents.</p>
<p>• We release a complete, end-to-end claw-machine robot system to demonstrate FM integrations in real-world applications2 .</p>
<p>2 Foundation Model Integration for Language-Guided Robotics</p>
<p>Concerning how FMs are integrated into robot systems, we identified the following three types of integration strategies (Fig. 2).In the following, We briefly describe each strategy along with its respective advantages and limitations.</p>
<p>End-to-End Vision-Language-Action</p>
<p>Definition.Vision-Language-Action (VLA) models operate in an end-to-end manner, directly translating visual observations and natural language instructions into low-level actions without decoupled perception, language, and control modules (Fig. 2a).Two mainstream paradigms have emerged within this framework: auto-regressive and diffusion-based action generation.Through large-scale pretraining, these models acquire broad capabilities that support generalization across tasks.However, efficient adaptation to real-world settings remains a significant challenge.</p>
<p>Autoregressive VLA Models.Autoregressive VLA models typically process language and visual inputs, employing various tokenization strategies to convert multimodal data into a unified latent space.Then the transformer-based decoder generates actions step-by-step in an autoregressive manner conditioned on the input context and previously generated actions, allowing structured action generation and planning.</p>
<p>Building on the previous groundbreaking models such as RT-1 (Brohan et al., 2023), RT-2 (Zitkovich et al., 2023) and VIMA (Jiang et al., 2023), Open-VLA (Kim et al., 2024) emerges as an important open-source method, combining a fine-tuned Llama 2 (7B) model with DinoV2 and SigLIP for visual tokenization, pretrained on the Open-X-Embodiment dataset (Collaboration et al., 2024) consisting of 970k real-world robot demonstrations.TraceVLA (Zheng et al., 2025) improves Open-VLA with visual trace prompting to enhance spatiotemporal awareness.Emma-X (Sun et al., 2024) further refines dataset quality using a trajectory segmentation and Embodied Chain-of-Thought reasoning.This work is followed by NORA (Hung et al., 2025), which uses Qwen-2.5-VL-3B as backbone and pretrained on the Open-X-Embodiment dataset.In parallel, other efforts integrate tactile sensing into robot perception (Yang et al., 2024;Zhao et al., 2024), supporting more generic robot policies.Other recent developments include distilling spatial representations from VLMs, e.g., Gemini-Robitics (Gemini Robotics Team, 2025), and enriching training with additional features, such as 3D spatial relationships in SpatialVLA (Qu et al., 2025), task-centric latent space in UniVLA (Bu et al., 2025), and integrating multimodal understanding with action prediction in ChatVLA (Zhou et al., 2025) and UP-VLA (Zhang et al., 2025).</p>
<p>Diffusion-based VLA Models.Diffusion-based VLA models formulate action generation as a denoising process over latent trajectories.Given a noisy version of a full action sequence, the model learns to recover the true trajectory conditioned on language and vision.</p>
<p>Diffusion Policy (DP) (Chi et al., 2023) pioneered the use of diffusion model for visuomotor policy representation, laying the foundation for subsequent multimodal approaches.Octo (Team et al., 2024) uses conditional diffusion decoding for action sequence prediction.Rather than lightweight diffusion heads, (Reuss et al., 2024;Wen et al., 2024;Li et al., 2024b) use larger and more dedicated diffusion policy modules the as action decoder.DTP (Fan et al., 2025) introduces a trajectory-level guidance to enhance diffusionbased planning.Recent works such as π 0 (Black et al., 2024) and π 0.5 (Intelligence et al., 2025) integrate a pretrained VLM with a flow-matchingbased action expert to model action distributions.These models often utilize FAST (Pertsch et al., 2025) for efficient compressed action tokenization.HybridVLA (Liu et al., 2025) unifies diffusion and autoregressive action prediction within a single LLM-based framework.A growing focus is placed on adapting VLA models to diverse embodiments including bimanual manipulation and humanoid robots, such as RDT-1b (Liu et al., 2024b), DexVLA (Wen et al., 2025) and GR00T N1 (Bjorck et al., 2025).</p>
<p>Strengths and Limitations.VLA models provide a unified, end-to-end framework for robotic manipulation, (i) seamlessly integrating visual, language, and action modalities.Leveraging largescale pretraining and fine-tuning, these models exhibit (ii) strong potential for generalizing across diverse manipulation tasks and robotic embodiments.However, their performance is constrained by the (iii) limited availability of high-quality, diverse robotic datasets.Pretraining can also intro-duce biases from training distributions, leading to (iv) degraded performance in out-of-distribution scenarios, such as novel tasks or different robotic embodiments.Therefore, despite their potential, further work is needed to enhance their robustness and generalization across real-world settings, for example, efficient adaptation using few-shot data.</p>
<p>Modular Vision-Language Pipelines</p>
<p>Definition.In a modular vision-language pipeline (Fig. 2b), perception is handled by a specialist vision language model (VLM) that outputs symbolic scene information, typically grounded 2-D / 3-D bounding boxes, segmentation masks, or referring expression pointers.A downstream planner or policy module then consumes this structured representation to generate low-level actions.The language channel is therefore disentangled from motor control, allowing each module to be tuned independently, thus preserving the transparency and plug-and-play advantages of classical planning.</p>
<p>Representative systems.Language-promptable specialist VLMs endow modular stacks with zeroshot semantics for various robotics pipelines.(Bandyopadhyay et al., 2024) demonstrates an end-to-end sample collection robot system that uses GroundingDINO (Liu et al., 2024a) to localize objects and refines each box with SAM (Ravi et al., 2025) masks before passing them to classical grasp-and-place controllers, illustrating this paradigm's practicality in real deployments.(Werby et al., 2024) aggregates these modules into a floor-room-object hierarchy, showcasing their usage in long-horizon language-conditioned navigation across multi-story buildings.</p>
<p>Strength and Limitations.Modular VLM pipelines strike a balance between transparency and adaptability, and delivers practical benefits: (i) interpretability-detections can be inspected; (ii) lightweight-the model parameters are usually around 100M∼600M, approximately 1% ∼ 6% the size of LLaMA 3.2 Vision 11B (Grattafiori et al., 2024).On the other hand, it is limited at (i) interaction rigidness compared with more flexible multimodal LLMs, and (ii) pipeline brittleness where perception errors propagate without mitigation (Fig. 2b; Table 1).Their success hinges on robust open-vocabulary grounding-precisely the capability our Instruction Grounding case study stresses in Section 3.</p>
<p>Multimodal LLM Agents as Orchestrators</p>
<p>Definition.Multimodal LLM agents place a large, tool-calling language model at the centre of the control loop (Fig. 2c).The LLM receives raw user utterances, selectively invokes vision tools (e.g., a detector or depth estimator) via function calls, reasons over their outputs in-context, and finally issues high-level action primitives to a lowlevel controller.The agent therefore acts as a cognitive hub that binds perception and control through natural language.</p>
<p>Representative Systems.Multimodal LLMs are taking increasingly important roles in robotics.Gemini Robotics (Gemini Robotics Team, 2025) integrates perception, spatial reasoning, and trajectory synthesis into one Gemini-2.0backbone (Google DeepMind, 2024), which serves as the embodied brain.(Li et al., 2024c), in the same vibe as Gemini Robotics, leverages the inherent common sense and reasoning capabilities of these models by fine-tuning adapter modules through a chain-ofthought training paradigm.It endows the model with accurate pose prediction and precise manipulation abilities.These works collectively show the trend that the multimodal LLM shifts to the "cognitive hub" in robot systems.(Glocker et al., 2025) build a modular agent-orchestration system for household object management robots.It utilize Llama 3.2 Vision (Grattafiori et al., 2024) for open-vocabulary perception to facilitate creating grounded task plans, while the limitations of the multmodal LLM were not discussed.</p>
<p>Somewhat similar to our work, (Li et al., 2024a) investigates the eligibility of Multimodal LLMs to serve as the "brain" for in-home robotics by providing a benchmark to compare models along the axes of perception, visual reasoning and task planing.Models like GPT-4V, Qwen-VL (Bai et al., 2025) and DeepSeek-VL (Lu et al., 2024) were included, but more recent releases were not covered-likely due to the fact that the field is moving fast with new models emerging in rapid succession.</p>
<p>Strengths and Limitations.Multimodal LLM agents excel in (i) visual commonsense reasoning, leveraging extensive language priors to generalize to novel concepts beyond the reach of most specialist VLMs, and (ii) instruction following with support for fine-grained visual understanding and dynamic planning.Despite their expressive power, however, these models are (iii) resource-intensive, posing challenges for deployment-particularly on mobile robotic platforms.</p>
<p>Case Studies on Instruction Grounding</p>
<p>Natural language instruction grounding involves translating user intents into clear, actionable goals in a visual scene, which is a key capability for embodied AI (Gemini Robotics Team, 2025).Our case study offers empirical insights into the grounding performance of various models through the lens of challenging cross-modal disambiguation, and further examines the trade-offs introduced by model sizes and quantization-providing practical suggestions for efficient deployment.</p>
<p>Benchmark Dataset.To minimize the impact of vision priors on measured performance, we design benchmarking scenarios using household objects placed on a tabletop.These objects are commonly represented in the training datasets of the foundation models, and the tabletop setup features minimal variation in lighting and camera angles-ensuring that the evaluation primarily reflects grounding capabilities.We curated a new Instruction Grounding benchmark dataset.In images containing multiple household objects, each object is tagged with a number as the visual prompt, and each image is paired with language instructions crafted to test visual commonsense and cross-modal disambiguation concerning attribute or spatial relationships -for "pick up the red-capped marker," the color must be used to select one among a few markers; whereas "grasp  the cup in front of the screwdriver" requires reasoning over spatial relations (Fig. 3; Appendix).Language ambiguity often leads to execution failure in an embodied system.By comparing specialist VLMs and multimodal LLMs, we reveal their concrete failure modes that further inform our design implications in Sec. 5.</p>
<p>Zero-Shot Object Grounding.We begin with a foundational question for instruction grounding: Can FMs accurately recognize objects in cluttered open scenes?Table 2 presents the performance hierarchy for specialist VLMs and a range of multimodal LLMs, serving as a basis for deeper analysis of ambiguity resolution in later sections.of Gemini 2.5's score, while the more recent Llama 4 releases did not outperform it.
V L M -L L M Q w e n 2 -V L -7 2 B L l a m a -3 . 2 V -9 0 B L l a m a -3 . 2 V -9 0 B -Q 4 L l a m a -3 . 2 V -1 1 B L l a m a -3 . 2 V -1 1 B -Q 4 0.
• Smaller community models (Gemma-27B, Phi-Vision) fall below the specialist-threshold, suggesting that they are still inadequate for finegrained grounding in cluttered scenarios.</p>
<p>• Last, when compute is a bottleneck, GPT-4omini (0.71) and Llama 3.2-Vision 11B (0.57) provide the best speed-accuracy trade-off, delivering decent performance without incurring heavy memory footprint or high API costs.</p>
<p>Zero-Shot Complex Instruction Grounding.This task is framed as a multiple-choice problem, where the model is asked to select the correct object index in a cluttered scene based on three types of natural language instructions: implicit, attributebased, and relationship-based-each type probes a distinct grounding challenge.We evaluate a series of multimodal LLMs, using a modular VLM-LLM pipeline as a baseline.In this pipeline, the LLM parses the instruction to infer likely targets, queries GroundingDINO to detect candidate objects, and selects from the detected boxes-essentially guessing without directly perceiving the scene.</p>
<p>• Implicit Instruction Grounding.Instructions like "I need a tool to tighten the screws" only refer to the target object implicitly, and the model needs to infer the target object using its common sense priors.For such instructions, the modular VLM-LLM pipeline struggles to select a screwdriver, lacking embedded affordance reasoning.In contrast, multimodal LLMs perform well, reflecting strong visual commonsense.GPT-4.5 demonstrates exceptional performance (0.94), though its high inference cost-20× that of Gemini 2.5 makes it cost-prohibitive for most applications.</p>
<p>• Relational Reasoning Remains Challenging.This category requires resolving referential ambiguity through implicit chain-of-thought reasoning: grounding objects, modeling spatial relationships, and disambiguating targets (e.g., identifying the correct mug among many based on "next to something").Accuracy drops significantly nearly across all models.Only Gemini 2.5-Pro and o4-mini achieve accuracy above 0.80-the former likely benefits from embodied training data, while the latter demonstrates strong reasoning capabilities.Notably, o4mini is a medium-sized model, yet it outperforms larger models like GPT-4.5 on relational instructions-suggesting that structured reasoning may help close, or even overcome the performance gap brought by different model scales.</p>
<p>• Instruction-Dependent Quantization Effects.</p>
<p>INT4 quantization reduces the model size by over 70%, making it an attractive choice for deployment.In Llama 3.2 Vision, we observe that it disproportionately impacts implicit and relational instruction grounding, indicated by the relative accuracy drop of 14% − 17%, while attribute grounding is more robust with only 4% loss.Despite reduced precision, quantized 11B models offer a speed-accuracy balance for lowresource settings.Our findings underscore the need for fine-grained quantization strategies that preserve the most important high-level reasoning capabilities under resource constraints.Table 3: Success rates (%) on the LIBERO Simulation Benchmark across four task suites, each evaluated over 500 trials.Results for SpatialVLA are from (Qu et al., 2025); Results for π 0 are from (Black et al., 2024), using pretrained models on LIBERO benchmarks."AC" denotes the use of action chunking.The comparison in the Appendix highlights its impact on performance.The finetuned π 0 model achieves the highest performance.</p>
<p>Case Studies on Robotic Manipulation</p>
<p>Now we shift the focus to skill adaptation.In an ideal deployment scenario, a pretrained VLAalready endowed with broad visuomotor skillsshould be retargeted to a new manipulation task with minimal data and fast convergence.We use fine-tuning, the standard practice for adaptation, as a probing lever to evaluate how the state-of-the-art VLA models adapt to new tasks and deployment conditions.</p>
<p>Given the scale of VLAs, we compare partial fine-tuning, which leverages our benchmark dataset (Appendix) and its inherent distribution bias to study convergence behavior, and full finetuning, which uses large-scale datasets to minimize the training loss.Our evaluation focuses on three key aspects: (i) training dynamics-how quickly and smoothly training converges; (ii) generalization-how well the resulting policies perform on various tasks; and (iii) robustness-how well the resulting policies handle environmental distractors.Our experiments highlight the performance of VLA models in different settings, offering practical suggestions for practitioners who have to adapt large VLAs under tight data, time and compute budgets.</p>
<p>Skill Adaptation Performance.Our fine-tuning process consists of two stages: (1) To assess convergence behavior under distribution shift, we collected a custom dataset (see Appendix for details) with a distribution bias relative to common pretraining datasets included in the Open-X-embodiment (Collaboration et al., 2024) and LIBERO datasets (Liu et al., 2023).We used it to partially finetune several recent VLA models and trained Diffusion Policy (DP) and Action Chunking Transformer (ACT) from scratch.The results are shown in Fig. 5; (2) For full fine-tuning, we leveraged larger benchmark datasets, Open-X-embodiment and LIBERO, to fully fine-tune RT-1, OpenVLA, SpatialVLA and NORA, and compared their performance.The results are shown in Fig. 6.</p>
<p>• Partial Fine-tuning.Through the experiments we observe that DP and ACT exhibit high stability with low variance during training.In contrast, generalist models such as OpenVLA and π 0 require significantly more training iterations to attain comparable accuracy and exhibit greater variance, which can be attributed to their large model capacity.Notably, although DP achieves lower loss by fitting directly to noise, it still demands more training steps to generate coherent actions, even after loss convergence.</p>
<p>• Full Fine-tuning.These fully fine-tuned VLA models are evaluated on three tasks: (1) out-ofdistribution (OOD) object manipulation, (2) spatial relationship reasoning and (3) multi-object pick-and-place tasks.In task (1), both NORA and OpenVLA succeed, while SpatialVLA fails due to incorrect affordance point estimation.</p>
<p>In task (2), NORA correctly follows instructions, while OpenVLA fails and SpatialVLA exhibits unstable performance.In task (3), NORA achieves successful execution while other models fail to complete the task reliably.</p>
<p>Sim2Real Adaption Performance.We compare model performance on simulation benchmarks and real robot deployments.A significant drop in performance is observed during transfer from simulation to the real world (Table 3; Appendix Table 6).The simulation benchmark includes 30 procedurally-generated, disentangled tasks requiring nuanced spatial reasoning (LIBERO-Spatial), object understanding (LIBERO-Object), and goal interpretation (LIBERO-Goal), as well as 10 longhorizon, entangled tasks (LIBERO-Long).Robustness to Perturbations.To evaluate robustness, we introduced distractor objects into the environment.As shown in Table 8, both Open-VLA and NORA exhibit substantial performance degradation in the presence of these perturbations, highlighting their sensitivity to novel conditions.</p>
<p>Key Takeaways.Current VLA models still face significant limitations in the following areas:</p>
<p>• Adaptation and Generalization.A generic robotic policy is expected to quickly adapt to datasets with distributional shifts.However, according to the partial fine-tuning results, due to the large model capacities and the limited size of task-specific datasets, these VLA models failed to achieve fast adaptation.While full fine-tuning offers improved performance, it requires extensive data and long training time, which are impractical for many real-world scenarios.</p>
<p>• Robustness.Robustness to distribution shifts (without finetuning) is a critical challenge.Results reveal substantial performance degradation both when encountering unseen objects and during sim-to-real transfer, highlighting the fragility of current VLA models in dynamic and unpredictable environments.These findings suggest that while VLA models hold promise, they have limitations in data efficiency, adaptation speed, and robustness to make them reliable for real-world robotic applications.</p>
<p>Constraints and Future Directions</p>
<p>Despite the promise of foundation models for enabling embodied agents to perform daily tasks, the following critical constraints still hinder their widespread deployment: Data Scarcity.In contrast to natural language datasets, which are readily sourced from internet, robotic datasets are significantly more expensive due to high hardware costs and intensive labor during data acquisition.A promising direction for future research is developing more data-efficient models.In addition, exploring high-fidelity simulation environments and developing robust sim-to-real transfer techniques could mitigate data scarcity.</p>
<p>Limited Generalization Capability.</p>
<p>A key limitation of current VLA models is their limited ability to generalize to out-of-distribution concepts that were not well-represented during training, which is a consequence of the aforementioned data scarcity.Many models depend heavily on large-scale paired datasets, which often exhibit biases and limited diversity in aspects such as camera viewpoints, lighting conditions and specific robotic embodiments.This results in fragile performance when deployed in real-world or domain-specific scenarios.Furthermore, these models struggle with fine-grained spatial reasoning and temporal understanding, hindering them from accurately aligning language with complex visual scenes or dynamic events.</p>
<p>Efficient Inference.Deploying large-scale models on robotic platforms introduces significant computational challenges, primarily in inference speed and GPU RAMs.This underscores the importance of smaller models that can efficiently generate actions without significant performance degradation.This issue is particularly pronounced in autoregressive models, and diffusion models are less affected.</p>
<p>Limitations</p>
<p>When evaluating the generalization capabilities of Vision-Language Alignment (VLA) models, this paper primarily focuses on different tasks without extensively addressing their generalization across varying robot morphologies.Although relatively few works have specifically targeted this aspect, it remains a significant challenge in deploying VLA models in real-world applications.Robots with different morphologies, such as bimanual manipulators, humanoid robots, and autonomous vehicles, require distinct operational protocols and safety considerations.The absence of a generic robot policy that can adapt seamlessly across diverse morphologies limits the practical generalization potential of VLA models and hinders their deployment as universal robotic policies.</p>
<p>In addition, this paper does not explore the ability of VLA models to ground instructions involving open-ended or ambiguous commands.Current VLA models are largely trained on curated datasets, which allow them to learn mappings from instructions to specific actions.However, this reliance constrains their ability to truly understand instructions at the semantic level.As a result, when facing out-of-distribution or vague commands, these models often struggle to infer reasonable actions.Addressing this limitation will require integrating more advanced instruction-understanding modules into the VLA pipeline to improve their robustness in handling ambiguous or under-specified input.• Implicit Instructions.Here, objects are not explicitly mentioned by name or attributes but are instead described by their functions.This category evaluates the VLMs' ability to infer the correct object based on its use.For example, the dataset includes instructions referring to objects like scissors, screwdrivers, and rulers based on their respective functions.</p>
<p>• Explicit Attributes.In this category, instructions prompt VLMs to identify objects belonging to a category with multiple instances, where each instance can be uniquely identified by explicitly mentioned attributes.In Fig. 7, the beige mug and the gray mug are included because they are unique when described with attributes.However, objects like the black mug or scissors are excluded.This is because there are two identical black mugs, making them non-unique, and there is only one pair of scissors, which does not require attributes for identification.</p>
<p>• Explicit Relationships.In this category, instructions describe objects by their spatial relationships to other objects in the image.We ensure that each referenced object is unique within the image.For example, the measuring cup to the right of the screwdriver uniquely identifies the object.These instructions are designed to test the VLMs' ability to comprehend and resolve location-based relationships.</p>
<p>• Multi-Referent Instructions.This category includes instructions that correspond to multiple valid objects in the scene.For example, in Fig. 7, an instruction like "give me a mug" may refer to several similar items.In such cases, we annotate the data with all candidate object indices, e.g., [2,7,18], indicating the set of plausible referents.A human-in-the-loop process was employed to ensure high-quality data collection.</p>
<p>• Initial Object Identification: We used GPT-4o to identify objects in an image and referring them by type, explicit attributes, and detailed location relations.</p>
<p>• Human Verification.The authors of this paper reviewed and modified the outputs to ensure their correctness.</p>
<p>• Instruction Generation.After verification, GPT-4 was tasked with generating simple, clear instructions for different objects.</p>
<p>• Final Review.These instructions underwent another round of verification to ensure clarity and accuracy.</p>
<p>This high-quality dataset consisting of 473 in-  structions, with a detailed breakdown of each instruction type presented in Fig. 8.</p>
<p>B Grounding Experiments</p>
<p>B.1 Complex Instruction Grounding for Goal Specification</p>
<p>Cross-modal Disambiguation represents a particularly challenging component of goal specification.</p>
<p>To quantify the model capability in this dimension, we employed attribute-based and relative relationship instructions to uniquely identify a target among multiple candidates.The goal specification task is formulated as follows: Given a visual input I ∈ R H×W ×3 and an instruction t, the objective is to predict the target object according to o * = arg max o∈O P (o | I, t; θ F M ), where o ⋆ ∈ O and O denotes the set of candidate objects.Models are evaluated using macro-average accuracy metric.</p>
<p>B.2 Failure Cases of Specialist VLM Pipeline</p>
<p>Grounding DINO, despite popular for zero-shot detection, is not robust in open scenes.It successfully detected "blue ball" while failed to detect "ball", indicating its reliance on visual features.Similarly, featureless metal cans pose a great challenge for Grounding DINO, which were almost omitted in the detection results.</p>
<p>For complex instruction grounding, Grounding DINO and GPT-4 were chained together to "guess" the target by the LLM based on the candidate bounding boxes.The failure cases were illustrated in the Fig. 9 and Fig. 10.</p>
<p>B.3 Multimodal LLMs Performance</p>
<p>The performance of Multimodal LLMs on complex grounding across EASY, MEDIUM and HARD groups are shown in Table 5.</p>
<p>C Manipulation Experiments</p>
<p>OpenVLA and π 0 were partially fine-tuned on this dataset, while Diffusion Policy (DP) and Action Chunking Transformer (ACT) were trained from scratch.Due to the limited size of our custom dataset, full fine-tuning of RT-1, OpenVLA, SpatialVLA, and NORA was performed using the Open-X-embodiment and LIBERO datasets.We evaluated model performance on both a real-world WidowX robotic platform and the LIBERO simulation benchmark.</p>
<p>C.1 Fine-tuning details for VLA Partial fine-tuning was conducted on a single NVIDIA A6000 GPU (48 GB VRAM) over a period of three days.To ensure a fair comparison, a batch size of 1 was used across all models.The results are presented in Fig. 5. Full fine-tuning of RT-1, OpenVLA, Spa-tialVLA, and NORA was conducted on a compute node equipped with 8×H100 GPUs.The fine-tuned models were evaluated on 9 diverse realworld manipulation tasks, as shown in Fig. 11.Success rates are summarized in Table 6, demonstrating NORA's superior policy generation capabilities across three task categories: out-of-distribution object grasping, spatial reasoning, and multi-object manipulation.</p>
<p>C.2 Impact of Action Chunking</p>
<p>C.2.1 Action Chunking Performs on WidowX.</p>
<p>To investigate the effectiveness of action chunking, we selected NORA-LONG and SpatialVLA for evaluation.Tasks were chosen from three categories: (1) "put the carrot in the pot," (2) "put the red bottle and hamburger in the pot," and (3) "put the pink toy at the right corner."In initial experiments, all predicted actions (5 actions for NORA-LONG, 4 actions for SpatialVLA) were executed sequentially without replanning.This frequently caused the WidowX robot to crash into the environment due to the accumulation of overly large movements.</p>
<p>Subsequently, we modified the execution policy to only perform the first action in each predicted chunk.This adjustment resolved the collision issue and NORA -LONG achieved an 80% success rate on the "put the carrot in the pot" task.However, on multi-object pick-and-place tasks, NORA-LONG consistently stopped after placing the first object, resulting in a 0% final success rate.For the spatial reasoning task, NORA-LONG achieved a 70% success rate on "put the pink toy at the right corner."C.2.2 Action chunking improves performance in simulation.</p>
<p>We hypothesize that action chunking is more effective at higher control frequencies.For example, Diffusion Policy generates commands at 10 Hz, which are then interpolated to 125 Hz for execution.Similarly, OpenVLA-OFT+ employs action chunking and shows improved performance in realworld ALOHA tasks, which run at 25 Hz.Since our real robotic platforms do not support high-frequency control, we tested this hypothesis in the LIBERO simulation environment (20 Hz).We fine-tuned both NORA and NORA-LONG on this benchmark with an action chunk size of 5, producing two variants: NORA-finetuned-AC and NORA-Long-finetuned.</p>
<p>Results show that NORA-finetuned-AC significantly outperforms NORA-finetuned across all LIBERO benchmarks, with a higher average success rate.Notably, NORA-Long-finetuned outperforms all baseline models (see Table 3), highlighting the benefits of pretraining with action chunking and its transferability to long-horizon tasks.However, it is important to note that LIBERO is a simulation environment and may not reflect real-world performance at high control frequencies.</p>
<p>C.3 Robustness to Disturbance</p>
<p>To evaluate robustness, we selected three straightforward tasks (shown in Fig. 12) and introduced distractor objects into the environment.Initially, both OpenVLA and NORA performed well.However, their success rates declined significantly with the introduction of distractions.This highlights the sensitivity of current VLA models to out-of-distribution disturbances.The average success rates across the three tasks are presented in Table 8, while the detailed number of successful executions out of 10 trials is summarized in Table 7.</p>
<p>D Modular Claw Machine Prototype</p>
<p>To facilitate the evaluation of different VLMs in robotic manipulation, we developed a voice-  SpatialVLA (Qu et al., 2025): A VLA model focused on spatial understanding for robot manipulation, incorporating 3D information such as spatial movement.It learns a generalist policy for spatial manipulation across diverse robots and tasks.SpatialVLA predicts four actions at a time.</p>
<p>TraceVLA (Zheng et al., 2024)    As shown in Table 2, NORA-LO N G achieves the highest average success rate (87.9%) across all methods, demonstrating strong generalization in both short-and long-horizon scenarios.Among the fine-tuned baselines without action chunking , OpenVLA achieves the best average (76.5%).NORA, demonstrates comparable performance to OpenVLA in spatial, object, and goal-related tasks, but it falls short in long-horizon scenarios.</p>
<p>Notably, when both NORA variants are fine-tuned with action chunking, there is a significant increase in the LIBERO-Long success rate, emphasizing the importance of action chunking for long-horizon tasks.</p>
<p>NORA-LO N G especially excels on LIBERO-Long, achieving a success rate of 74.6%, showcasing its ability to reason over extended temporal windows.These results highlight the effectiveness of our model in adapting to new environments and reinforce the utility of windowed training for long-horizon policy generalization.". 9</p>
<p>Figure 12: Comparison of tasks with and without distraction.</p>
<p>controlled testbed using a UR5 robotic arm 5 .The system architecture, shown in Fig. 13, comprises the following five modules:</p>
<p>• Speech Transcription: Powered by Microsoft Azure's speech recognition service.</p>
<p>• Task Decomposition: Based on GPT-3.5 and GPT-4 using prompting paradigms adapted from ChatGPT for Robotics.</p>
<p>• Object Detection: Utilizes GroundingDINO and OWL-ViT for object detection.</p>
<p>• Object Segmentation: Employs Segment Anything Model (SAM) and FastSAM for segmenting detected objects.</p>
<p>• Manipulation: Low-level actions are generated by GraspAnything or GraspNet.This modular testbed enables rapid integration and benchmarking of different models within a real robotic system.</p>
<p>Figure 2 :
2
Figure 2: Foundation model integration strategies in language driven robots: (a) End-to-end VLA models, (b) Modular VLM pipelines, and (c) Multimodal LLM agents.Each strategy reflects a distinct interface between language, perception, and control.</p>
<p>Implicit 7 :Figure 3 :
73
Figure 3: Experimental setup for two case studies in a cluttered tabletop environment.The top row shows egocentric video data collected for the manipulation case study.The bottom row is an example setup for the instruction grounding task, including an annotated visual prompt paired with complex instructions in three forms: implicit, explicit with attributes and spatial references.</p>
<p>Figure 4 :
4
Figure 4: Performance of complex instruction grounding across VLM-LLM pipelines and end-to-end multimodal LLMs.Macro accuracy is reported across instruction types-implicit, attribute-based, and relationship-based.Subfigures show (a) proprietary models and (b) open-source models along with their Int4-quantized variants.</p>
<p>Figure 5 :
5
Figure 5: Results for partial fine-tuning of VLA models including OpenVLA and π 0 , alongside results from training Diffusion Policy and ACT (Action Chunking Transformer) from scratch on our dataset.VLA models require more training epochs to converge and exhibit higher variance in performance.</p>
<p>Figure 6 :
6
Figure6: Success rates of fully fine-tuned VLA models on out-of-distribution object manipulation (OOD Object), spatial relationship reasoning (Spatial) and multiobject pick-and-place (Multiple) tasks.NORA achieves the highest performance.</p>
<p>Figure 7 :
7
Figure 7: Example of visual prompts</p>
<p>Figure 8 :
8
Figure 8: Dataset breakdown by Instruction Types.</p>
<p>Figure 9 :
9
Figure 9: Examples of Instruction Grounding.(a) "the marker on the left", (b) "the marker aligned with the ruler".</p>
<p>Figure 10 :
10
Figure 10: Examples of Object Grounding.(a) "ball", (b) "screwdriver", (c) "marker pens", (d) "blue ball".</p>
<p>Preprint move the banana close to the pan put the red bottle and the hamburger in the pan put the carrot and hotdog in pot put the corn and carrot in pan put the pink toy at the right corner put carrot in pot put the blue cube on the right plate put banana in pot put the blue cube on the plate</p>
<p>Figure 3 :
3
Figure3: Real-world robot environments and task setups.We evaluate NORA across 9 diverse tasks to assess its instruction understanding, spatial reasoning, and multi-task motion planning capabilities.</p>
<p>Figure 11 :
11
Figure11: Real-world robot environments and task setups.We evaluate these models across 9 diverse tasks to assess its instruction understanding, spatial reasoning, and multi-task motion planning capabilities.</p>
<p>Figure 5 :
5
Figure 5: Comparison of tasks with and without distraction.</p>
<p>5 https://github.com/HRItdy/claw_machineComparison of task performance between OpenVLA and NORA under conditions with and without distraction.Each value denotes the number of successful executions out of 10 trials.</p>
<p>Figure 13 :
13
Figure 13: The system architecture of the testbed for VLMs.</p>
<p>Table 1 :
1
Comparison of foundation model integration strategies in embodied robotic systems, highlighting differences in instruction grounding, manipulation generalization, and adaptation methods.
Pipelines for Robot SystemsInstruction Grounding Visual inputs Multi-round dialogue CoT reasoningManipulation Generalization Adaptation for Deployment Morphology independent Skill sets Data EfficiencyEnd-to-End VLA ModelsWide rangeData-hungry finetuningModular VLM pipelinesController-Cheap finetuningMultimodal LLMs AgentsspecificIn-context learning</p>
<p>Table 2 :
2
Object grounding performance of specialist VLMs and multimodal LLMs (closed-source and opensource) across varying scene complexity levels.Models are evaluated on easy, medium, and hard cluttered scenes, with macro accuracy reported.</p>
<p>Table 5 :
5
Performance on the complex instruction grounding task.Abbreviations: im denotes implicit instruction, attr denotes attribute-based instruction, and rel denotes relation-based instruction.
ModelimEasy attrrelimMedium attrrelimHard attrrelVLM-LLM Gemini-2.5-Pro Gemini-2.0 GPT-4.5 GPT-4o o4 4o-mini GPT-4V Qwen2-VL LLaMA 3.2 Vision 90B LLaMA 3.2 Vision 90B-Q4 0.800 0.667 0.598 0.625 0.719 0.554 0.542 0.464 0.300 0.050 0.516 0.131 0.010 0.336 0.186 0.000 0.318 0.174 0.778 0.889 0.830 0.847 0.814 0.815 0.985 0.784 0.858 0.833 0.889 0.774 0.819 0.721 0.642 1.000 0.668 0.469 0.944 0.889 0.894 0.917 0.838 0.722 0.958 0.719 0.698 0.850 1.000 0.778 0.819 0.948 0.680 0.901 0.697 0.469 0.950 1.000 0.907 0.847 0.988 0.837 0.917 0.809 0.804 0.750 0.717 0.550 0.764 0.771 0.596 0.750 0.382 0.248 0.650 0.750 0.598 0.750 0.737 0.662 0.625 0.417 0.455 0.800 0.917 0.830 0.792 0.756 0.738 0.875 0.700 0.529 0.750 0.850 0.704 0.708 0.853 0.711 0.875 0.491 0.521 LLaMA 3.2 Vision 11B 0.650 0.667 0.631 0.764 0.710 0.556 0.833 0.536 0.342 LLaMA 3.2 Vision 11B-Q4 0.650 0.567 0.502 0.694 0.757 0.555 0.542 0.498 0.450</p>
<p>Table 6 :
6
Task performance comparison across different categories and models.
Preprint</p>
<p>Table 2 :
2
(Qu et al., 2025)lts (% success rate) of NORA and baselines on LIBERO Simulation Benchmark.Each method is evaluated on four task suites over 500 trials.Fine-tuned NORA-Long achieves the best overall performance.Results marked with * are from SpatialVLA(Qu et al., 2025).AC indicates the use of action chunking strategy.ModelsLIBERO-Spatial LIBERO-Object LIBERO-Goal LIBERO-Long Average
OpenVLA fine-tuned  *  TraceVLA fine-tuned  *  NORA-fine-tuned (Ours)84.7 84.6 85.688.4 85.2 87.879.2 75.1 7753.7 54.1 4576.5 74.8 73.9SpatialVLA fine-tuned-AC  *  NORA-fine-tuned-AC (Ours) NORA-Long-fine-tuned (Ours)88.2 85.6 92.289.9 89.4 95.478.6 80 89.455.5 63 74.678.1 79.5 87.9put banana in potput carrot in potput the blue cube on the plateWithout distractionWith distraction</p>
<p>Table 8 :
8
Average Success Rate (%) without (w/o) and with (w/) Distractors
Modelw/o Distractors w/ DistractorsOpenVLA56.750NORA83.356.7
https://github.com/HRItdy/claw_machine
https://huggingface.co/datasets/bittdy/pick_screw
https://github.com/xiuchao/InstructionGrounding
Appendix A Benchmark Dataset A.1 Cluttered Tabletop Manipulation DatasetTo evaluate the finetuning behavior of various VLA models under distribution shift, we constructed a custom cluttered tabletop environment using a UR5 robotic arm with a wrist-mounted RealSense RGB-D camera.This setup differs from all existing configurations in the Open-X-Embodiment dataset.Demonstrations for a screwdriver-picking task-amid distractor objects-were collected via teleoperation using a SpaceMouse device.In total, we gathered 163 demonstration episodes 3 .Each episode began with a randomized initial robot pose, followed by an attempt to grasp the target screwdriver.A.2 Complex Instruction Grounding DatasetWe curated an evaluation dataset for the complex instruction grounding task in cluttered scenes 4 .Thirty images were sampled from the action sequences and subsequently categorized based on the number of objects: EASY (&lt;15), MEDIUM (15∼20), and HARD (&gt;20).Objects in the visual scenes were manually annotated using visual prompts and paired with various instructions.The spatial relationship words were illustrate in Table4.For the complex instruction grounding task, including an annotated visual prompt paired with complex instructions in three forms: implicit, explicit with attributes and spatial references.Additionally, the dataset includes multi-turn questions that refer to more than one object, enabling foundation models to ask clarifying questions to identify the correct object.
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, 10.48550/arXiv.2502.13923arXiv:2502.13923Yiheng Xu, and 8 others. 2025. Qwen2.5-VL Technical Report. Preprint</p>
<p>Demonstrating Event-Triggered Investigation and Sample Collection for Human Scientists using Field Robots and Large Foundation Models. Tirthankar Bandyopadhyay, Fletcher Talbot, Robotics: Science and Systems. 1 others. 2024</p>
<p>Gr00t n1: An open foundation model for generalist humanoid robots. Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, arXiv:2503.14734arXiv:2410.24164Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi1 others. 2025arXiv preprintand 5 others. 2024. π 0 : A vision-language-action flow model for general robot control</p>
<p>Rt-1: Robotics transformer for realworld control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, J Nikhil, Ryan Joshi, Dmitry Julian, Kalashnikov, Proceedings of Robotics: Science and Systems. Robotics: Science and Systems202332</p>
<p>Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, Hongyang Li, arXiv:2502.14420Learning to act anywhere with task-centric latent actions. 2025arXiv preprint</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, Shuran Song, The International Journal of Robotics Research. 2023</p>
<p>Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, and 275 others. Abby O' Neill, Embodiment CollaborationAbdul Rehman, Embodiment CollaborationAbhinav Gupta, Embodiment CollaborationAbhiram Maddukuri, Embodiment CollaborationAbhishek Gupta, Embodiment CollaborationAbhishek Padalkar, Embodiment CollaborationAbraham Lee, Embodiment CollaborationAcorn Pooley, Embodiment CollaborationAgrim Gupta, Embodiment CollaborationAjay Mandlekar, Embodiment CollaborationAjinkya Jain, Embodiment CollaborationAlbert Tung, Embodiment CollaborationAlex Bewley, Embodiment CollaborationICRA. 2024Open xembodiment: Robotic learning datasets and rt-x models</p>
<p>Quantao Shichao Fan, Yajie Yang, Kun Liu, Zhengping Wu, Qingjie Che, Min Liu, Wan, arXiv:2502.10040Diffusion trajectory-guided policy for long-horizon robot manipulation. 2025arXiv preprint</p>
<p>Gemini Robotics: Bringing AI into the Physical World. 10.48550/arXiv.2503.20020arXiv:2503.200202025Preprint</p>
<p>LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics. Marc Glocker, Peter Hönig, Matthias Hirschmanner, Markus Vincze, 10.48550/arXiv.2504.21716arXiv:2504.217162025Preprint</p>
<p>Gemini: Our largest and most capable ai models yet. Google Deepmind, 2024</p>
<p>Alan Schelten. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, 10.48550/arXiv.2407.21783Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvarkand 542 others. 2024. The Llama 3 Herd of Models</p>
<p>Denoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in neural information processing systems. 202033</p>
<p>Nora: A small open-sourced generalist vision language action model for embodied tasks. Chia-Yu Hung, Qi Sun, Pengfei Hong, Amir Zadeh, Chuan Li, U-Xuan Tan, Navonil Majumder, Soujanya Poria ; Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, arXiv:2504.19854arXiv:2504.16054Physical Intelligence. 2025arXiv preprintand 17 others. 2025. π 0.5 : a vision-language-action model with open-world generalization</p>
<p>Vima: robot manipulation with multimodal prompts. Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, Linxi Fan, ICML. 2023</p>
<p>Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Quan Sanketi, Thomas Vuong, Benjamin Kollar, Russ Burchfiel, Dorsa Tedrake, Sergey Sadigh, Percy Levine, Chelsea Liang, Finn, arXiv:2406.09246Openvla: An open-source vision-language-action model. 2024</p>
<p>MMRo: Are Multimodal LLMs Eligible as the Brain for. Jinming Li, Yichen Zhu, Zhiyuan Xu, Jindong Gu, Minjie Zhu, Xin Liu, Ning Liu, Yaxin Peng, Feifei Feng, Jian Tang, 10.48550/arXiv.2406.196932024a</p>
<p>Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, ICML'23. JMLR.orgProceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>Cogact: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation. Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, Xiaofan Wang, Bei Liu, Jianlong Fu, Jianmin Bao, Dong Chen, Yuanchun Shi, Jiaolong Yang, Baining Guo, arXiv:2411.196502024barXiv preprint</p>
<p>Manipllm: Embodied multimodal large language model for object-centric robotic manipulation. Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, Hao Dong, CVPR. 2024c</p>
<p>Libero: Benchmarking knowledge transfer for lifelong robot learning. Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, Peter Stone, Advances in Neural Information Processing Systems. 202336</p>
<p>Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, Chengkai Hou, Mengdi Zhao, Zhou, arXiv:2503.10631Pheng-Ann Heng, and Shanghang Zhang. 2025. Hybridvla: Collaborative diffusion and autoregression in a unified vision-languageaction model. arXiv preprint</p>
<p>Grounding dino: Marrying dino with grounded pretraining for open-set object detection. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, ECCV. Jun Zhu, and Lei Zhang. 2024a</p>
<p>Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, arXiv:2410.07864Rdt-1b: a diffusion foundation model for bimanual manipulation. Jun Zhu. 2024b</p>
<p>Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, Chong Ruan, 10.48550/arXiv.2403.05525arXiv:2403.05525DeepSeek-VL: Towards Real-World Vision-Language Understanding. 2024Preprint</p>
<p>Fast: Efficient action tokenization for vision-language-action models. Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, Sergey Levine, arXiv:2501.097472025arXiv preprint</p>
<p>Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, Jiayuan Gu, Bin Zhao, Dong Wang, Xuelong Li, arXiv:2501.15830Spatialvla: Exploring spatial representations for visual-languageaction model. 2025arXiv preprint</p>
<p>SAM 2: Segment anything in images and videos. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan, Ross Wu, Piotr Girshick, Christoph Dollár, Feichtenhofer, ICLR. 2025</p>
<p>Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, Lei Zhang, arXiv:2401.14159Grounded sam: Assembling open-world models for diverse visual tasks. 2024</p>
<p>Multimodal diffusion transformer: Learning versatile behavior from multimodal goals. Moritz Reuss, Ömer Erdinç Yagmurlu, Fabian Wenzel, Rudolf Lioutikov, Robotics: Science and Systems. 2024</p>
<p>Emma-x: An embodied multimodal action model with grounded chain of thought and lookahead spatial reasoning. Qi Sun, Pengfei Hong, Tej Deep Pala, Vernon Toh, U-Xuan Tan, Deepanway Ghosal, Soujanya Poria, arXiv:2412.119742024</p>
<p>Octo: An open-source generalist robot policy. Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, Sergey Levine, Proceedings of Robotics: Science and Systems. Robotics: Science and Systems2024</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Diffusion-vla: Scaling robot foundation models via unified diffusion and autoregression. Junjie Wen, Minjie Zhu, Yichen Zhu, Zhibin Tang, Jinming Li, Zhongyi Zhou, Chengmeng Li, Xiaoyu Liu, Yaxin Peng, Chaomin Shen, Feifei Feng, arXiv:2412.032932024</p>
<p>Dexvla: Vision-language model with plug-in diffusion expert for general robot control. Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen, Feifei Feng, arXiv:2502.058552025arXiv preprint</p>
<p>Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation. Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard, 10.15607/RSS.2024.XX.077Robotics: Science and Systems XX. 2024</p>
<p>Binding touch to everything: Learning unified multimodal tactile representations. Fengyu Yang, Chao Feng, Ziyang Chen, Hyoungseob Park, Daniel Wang, Yiming Dou, Ziyao Zeng, Xien Chen, Rit Gangopadhyay, Andrew Owens, Alex Wong, CVPR. 2024</p>
<p>Up-vla: A unified understanding and prediction model for embodied agent. Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, Jianyu Chen, arXiv:2501.188672025arXiv preprint</p>
<p>Transferable tactile transformers for representation learning across diverse sensors and tasks. Jialiang Zhao, Yuxiang Ma, Lirui Wang, Edward H Adelson, CoRL. 2024</p>
<p>TraceVLA: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daumé, Iii , Andrey Kolobov, Furong Huang, Jianwei Yang, ICLR. 2025</p>
<p>Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, Feifei Feng, arXiv:2502.14420Chatvla: Unified multimodal understanding and robot control with vision-language-action model. 2025arXiv preprint</p>
<p>RT-2: Visionlanguage-action models transfer web knowledge to robotic control. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Grecia Pannag R Sanketi, Salazar, CoRL. 202335</p>            </div>
        </div>

    </div>
</body>
</html>
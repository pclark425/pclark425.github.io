<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2755 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2755</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2755</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-791e40e5c14355c303f674664c5e645e0511f7c9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/791e40e5c14355c303f674664c5e645e0511f7c9" target="_blank">On the Planning, Search, and Memorization Capabilities of Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This study investigates the potential of the state-of-the-art large language model (GPT-4) for planning tasks, and identifies areas where large language models excel in solving planning problems and reveals the constraints that limit their applicability.</p>
                <p><strong>Paper Abstract:</strong> The rapid advancement of large language models, such as the Generative Pre-trained Transformer (GPT) series, has had significant implications across various disciplines. In this study, we investigate the potential of the state-of-the-art large language model (GPT-4) for planning tasks. We explore its effectiveness in multiple planning subfields, highlighting both its strengths and limitations. Through a comprehensive examination, we identify areas where large language models excel in solving planning problems and reveal the constraints that limit their applicability. Our empirical analysis focuses on GPT-4's performance in planning domain extraction, graph search path planning, and adversarial planning. We then propose a way of fine-tuning a domain-specific large language model to improve its Chain of Thought (CoT) capabilities for the above-mentioned tasks. The results provide valuable insights into the potential applications of large language models in the planning domain and pave the way for future research to overcome their limitations and expand their capabilities.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2755.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2755.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large autoregressive transformer language model used in this paper to generate planning domains, perform graph search, propose heuristics, and play simple adversarial text games (tic-tac-toe); evaluated qualitatively for planning, search, and memorization capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A large transformer-based next-token-prediction language model (GPT-4) used via prompting to (1) generate PDDL planning domains and plans, (2) perform graph-search reasoning (BFS/DFS/Dijkstra), and (3) generate heuristics and attempt to play tic-tac-toe.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Tic-Tac-Toe (small adversarial text game); also used for planning-domain generation and small graph-search problems</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Tic-Tac-Toe: a two-player adversarial turn-based 3x3 board game used as a simple adversarial planning/text-game test; other tasks include generating PDDL for daily tasks and solving small graph-search instances.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>The paper reports that GPT-4 (as used) does not employ an explicit memory system and exhibits failures attributable to lack of memorization: it 'fails to identify possible moves and memorizes the sequence of previous states' incorrectly in the tic-tac-toe example. The authors identify 'the lack of memorization during planning' as a main factor limiting LLMs in adversarial planning; GPT-4 can generate reasonable heuristics but cannot reliably apply them because it cannot correctly track game state sequences or search exhaustively.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Reported limitations include inability to reliably memorize the sequence of previous states (leading to incorrect move identification), inability to search over all possibilities (so cannot perform exhaustive adversarial algorithms like Minimax/MCTS), and degraded performance on longer-horizon / more complex graph-search instances. The paper ties these limitations to the next-token prediction nature of GPTs and limited in-prompt state tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Planning, Search, and Memorization Capabilities of Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2755.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2755.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (base) fine-tuned model (authors' experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Flan-T5 base model (240M parameters) which the authors fine-tuned (only final layer) on a small dataset of planning-domain, graph-search, and adversarial-planning queries to test whether fine-tuning improves logical reasoning and planning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Flan-T5 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Flan-T5 base (240M) frozen except for the final layer; fine-tuned on ~1000 soft-labeled examples (planning domains, graph search, adversarial planning) obtained from GPT-4 outputs to attempt to improve chain-of-thought/logical planning behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td>240M</td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Tic-Tac-Toe (adversarial toy), plus collected planning-domain and graph-search tasks</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Same small adversarial/text-game use (tic-tac-toe) plus multiple planning-domain generation and small graph-search tasks used in the fine-tuning dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>No memory system was implemented or evaluated for the fine-tuned Flan-T5. The authors report only minor improvements from fine-tuning and note that Flan variants remained incorrect on many data points; they do not report any memory-led improvements because no explicit memory architecture was used.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>No explicit memory architecture was present; therefore, fine-tuning alone did not address the memorization deficiencies observed in GPT-4 (i.e., tracking state sequences in adversarial play). The paper notes the small dataset and frozen backbone as limiting factors for improving logical/memorization capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Planning, Search, and Memorization Capabilities of Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 2)</em></li>
                <li>LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action <em>(Rating: 2)</em></li>
                <li>LLM-Planner: Few-shot grounded planning for embodied agents with large language models <em>(Rating: 2)</em></li>
                <li>Large language models still can't plan (a benchmark for llms on planning and reasoning about change) <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2755",
    "paper_id": "paper-791e40e5c14355c303f674664c5e645e0511f7c9",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A state-of-the-art large autoregressive transformer language model used in this paper to generate planning domains, perform graph search, propose heuristics, and play simple adversarial text games (tic-tac-toe); evaluated qualitatively for planning, search, and memorization capabilities.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "GPT-4",
            "agent_description": "A large transformer-based next-token-prediction language model (GPT-4) used via prompting to (1) generate PDDL planning domains and plans, (2) perform graph-search reasoning (BFS/DFS/Dijkstra), and (3) generate heuristics and attempt to play tic-tac-toe.",
            "base_model_size": null,
            "game_benchmark_name": "Tic-Tac-Toe (small adversarial text game); also used for planning-domain generation and small graph-search problems",
            "game_description": "Tic-Tac-Toe: a two-player adversarial turn-based 3x3 board game used as a simple adversarial planning/text-game test; other tasks include generating PDDL for daily tasks and solving small graph-search instances.",
            "uses_memory": false,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "The paper reports that GPT-4 (as used) does not employ an explicit memory system and exhibits failures attributable to lack of memorization: it 'fails to identify possible moves and memorizes the sequence of previous states' incorrectly in the tic-tac-toe example. The authors identify 'the lack of memorization during planning' as a main factor limiting LLMs in adversarial planning; GPT-4 can generate reasonable heuristics but cannot reliably apply them because it cannot correctly track game state sequences or search exhaustively.",
            "memory_limitations": "Reported limitations include inability to reliably memorize the sequence of previous states (leading to incorrect move identification), inability to search over all possibilities (so cannot perform exhaustive adversarial algorithms like Minimax/MCTS), and degraded performance on longer-horizon / more complex graph-search instances. The paper ties these limitations to the next-token prediction nature of GPTs and limited in-prompt state tracking.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2755.0",
            "source_info": {
                "paper_title": "On the Planning, Search, and Memorization Capabilities of Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Flan-T5 (fine-tuned)",
            "name_full": "Flan-T5 (base) fine-tuned model (authors' experiment)",
            "brief_description": "A Flan-T5 base model (240M parameters) which the authors fine-tuned (only final layer) on a small dataset of planning-domain, graph-search, and adversarial-planning queries to test whether fine-tuning improves logical reasoning and planning performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Flan-T5 (fine-tuned)",
            "agent_description": "Flan-T5 base (240M) frozen except for the final layer; fine-tuned on ~1000 soft-labeled examples (planning domains, graph search, adversarial planning) obtained from GPT-4 outputs to attempt to improve chain-of-thought/logical planning behavior.",
            "base_model_size": "240M",
            "game_benchmark_name": "Tic-Tac-Toe (adversarial toy), plus collected planning-domain and graph-search tasks",
            "game_description": "Same small adversarial/text-game use (tic-tac-toe) plus multiple planning-domain generation and small graph-search tasks used in the fine-tuning dataset.",
            "uses_memory": false,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "No memory system was implemented or evaluated for the fine-tuned Flan-T5. The authors report only minor improvements from fine-tuning and note that Flan variants remained incorrect on many data points; they do not report any memory-led improvements because no explicit memory architecture was used.",
            "memory_limitations": "No explicit memory architecture was present; therefore, fine-tuning alone did not address the memorization deficiencies observed in GPT-4 (i.e., tracking state sequences in adversarial play). The paper notes the small dataset and frozen backbone as limiting factors for improving logical/memorization capabilities.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2755.1",
            "source_info": {
                "paper_title": "On the Planning, Search, and Memorization Capabilities of Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 2
        },
        {
            "paper_title": "LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action",
            "rating": 2
        },
        {
            "paper_title": "LLM-Planner: Few-shot grounded planning for embodied agents with large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models still can't plan (a benchmark for llms on planning and reasoning about change)",
            "rating": 2
        }
    ],
    "cost": 0.008225999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>On the Planning, Search, and Memorization Capabilities of Large Language Models</h1>
<p>Yunhao Yang<br>Department of Computer Science<br>University of Texas at Austin<br>Austin, TX 78705<br>yunhaoyang234@utexas.edu</p>
<h2>Anshul Tomar</h2>
<p>Department of Computer Science
University of Texas at Austin
Austin, TX 78705
anshulmanas@gmail.com</p>
<h4>Abstract</h4>
<p>The rapid advancement of large language models, such as the Generative Pretrained Transformer (GPT) series, has had significant implications across various disciplines. In this study, we investigate the potential of the state-of-the-art large language model (GPT-4) for planning tasks. We explore its effectiveness in multiple planning subfields, highlighting both its strengths and limitations. Through a comprehensive examination, we identify areas where large language models excel in solving planning problems and reveal the constraints that limit their applicability. Our empirical analysis focuses on GPT-4's performance in planning domain extraction, graph search path planning, and adversarial planning. We then propose a way of fine-tuning a domain-specific large language model to improve its Chain of Thought (CoT) capabilities for the above-mentioned tasks. The results provide valuable insights into the potential applications of large language models in the planning domain and pave the way for future research to overcome their limitations and expand their capabilities.</p>
<h2>1 Introduction</h2>
<p>The fast growth of large language models, such as the Generative Pretrained Transformer (GPT) series, significantly impacts various disciplines, from natural language processing and artificial intelligence to healthcare [Chintagunta et al., 2021, Nori et al., 2023, Thirunavukarasu et al., 2023], finance [Leippold, 2023, Wu et al., 2023], and beyond [Shin et al., 2020]. These models have revolutionized tasks such as machine translation, sentiment analysis, text summarization, and question-answering, enhancing human-computer interactions and enabling more efficient and accurate information retrieval. In addition, the vast amounts of data these models are trained on allow them to generate human-like responses and perform tasks that were once considered exclusive to human intelligence.
We examine the capability of the current state-of-the-art language model—GPT-4—on planning and search [OpenAI, 2023]. Despite its impressive performance in natural language processing tasks and its ability to generate human-like text, GPT-4 is not explicitly designed for executing planning or search algorithms. However, it can provide valuable insights and guidance on various planning and search techniques and domain-specific knowledge for constructing heuristics or evaluating different approaches. GPT-4's vast knowledge base allows users to ask questions and explore diverse aspects of planning and search.
We indicate the fields in planning that can be solved by large language models and the limitations of language models. The introduction of large language models significantly impacts many fields, such as natural language processing; hence we want to examine its impact on the field of planning.</p>
<p>Existing works [Valmeekam et al., 2022, Huang et al., 2022b, Singh et al., 2023, Lin et al., 2023] demonstrating the capability of language models on planning are heavily focused on plan generation but lack the exploration of path search, memorization in planning, and planning in adversarial settings.</p>
<p>We provide a comprehensive examination of the capability of GPT-4 in the field of planning and indicate its limitations for future research. Additionally, we attempt to improve the performance of an LLM by fine-tuning it on tasks like planning domain, graph search, and adversarial search to see if we are able to improve the predictions of these models for these aforementioned tasks. These models are capable of addressing various planning tasks, such as providing general information on planning algorithms, generating heuristics, and discussing different planning techniques. However, they are not specifically designed to perform planning tasks directly, as their primary function is to understand and generate text. Language models have limitations in handling real-time interactive scenarios and lack the ability to learn and adapt beyond their training data. Consequently, while large language models can provide valuable insights and guidance in the realm of planning, their utility is constrained by these limitations, and they cannot fully replace specialized planning algorithms or tools designed to address specific planning problems.</p>
<p>We provide an empirical analysis of how GPT-4 performs on planning domain extraction, graph search path planning, and adversarial planning. We found that GPT-4 is effective at extracting key components of planning domains from textual descriptions, allowing for the generation of structured representations suitable for use in automated planning systems. In graph search, GPT-4 exhibits the capability to understand the searching algorithm and find an optimal path based on the algorithm. However, such capability is limited once the graph becomes complicated. Moreover, we show its capability to generate heuristics for adversarial planning and its limitation in performing adversarial search algorithms. The lack of memorization during planning is a main factor that limits the large language model to planning in adversarial settings.</p>
<h1>2 Related Work</h1>
<p>Several works have used large language models for zero-shot planning; however, their planning either assumes the planning domain is acquired, or the outcomes are static. Some works [Yang et al., 2022, Ichter et al., 2022] only generate static outcomes, while LLM-Planner [Song et al., 2022] and LM-Nav [Shah et al., 2022] require prior knowledge of specific fields to define the planning domain. Existing works [Huang et al., 2022a, Yang et al., 2022] have demonstrated the capability of these models. Large language models are sources with a wide range of knowledge, including domain-specific knowledge. However, existing works have not dived into the planning and searching capabilities of these models, especially in complex problem or adversarial settings.</p>
<p>In this work, we explore the capabilities of large language models on planning domain generation, graph search, planning state memorization, and adversarial planning. The work reveals some limitations of large language models, which lead to potential future directions for improving these models.</p>
<h2>3 Preliminaries</h2>
<p>Large Language Models (LLMs). LLMs are machine learning models designed to process and understand natural language, such as human speech and text. These models are typically large-scale neural networks, trained using massive amounts of data, often on the scale of billions of words or more, to learn patterns and structures in language.</p>
<p>LLMs are capable of a wide range of natural language processing tasks, such as language translation, sentiment analysis, text classification, and speech recognition. They can generate new text based on the input prompt they received and create original content such as news articles, essays, or even poetry.</p>
<p>One example of a large language model is OpenAI's GPT (short for "Generative Pre-trained Transformer") series [Brown et al., 2020, OpenAI, 2023], with GPT-4 being the most current iteration. Compared to existing LLMs, GPT-4 is also able to understand image inputs and perform better on logic reasoning. These models have demonstrated remarkable performance across a wide range of NLP tasks, revolutionizing the field of AI and enabling new applications in various domains.</p>
<p>Planning Domain. A planning domain refers to a formal description of a specific problem space or environment [Haslum et al., 2019]. It consists of the rules, constraints, and actions that define the structure of the problem and the ways in which it can be solved. The goal of automated planning is to find a sequence of actions that can transform the initial state of the domain into a desired goal state.
A planning domain generally consists of the following components:</p>
<ul>
<li>Objects: The entities or items that exist within the domain, such as people, locations, or resources.</li>
<li>States: A description of the various conditions or configurations of the objects in the domain.</li>
<li>Actions: The operations or steps that can be taken to modify the state of the domain. Actions usually have preconditions that must be satisfied before they can be executed and effects that describe how the state changes when the action is performed.</li>
<li>Initial state: The starting configuration of the domain from which the planning process begins.</li>
<li>Goal state: The desired configuration or set of conditions that the planning process aims to achieve.</li>
</ul>
<p>Planning Domain Definition Language (PDDL) is a formal language used to describe planning problems and domains in the field of automated planning. PDDL separates the description of a planning problem into two parts: the domain and the problem. The domain defines the general structure of the problem, including the available actions and their effects, while the problem specifies the initial state and the goal state for a particular instance of the problem.
In addition to the components of general planning domains, PDDL consists of a set of predicates, which is a set of properties or relations that describe the state of the objects in the domain.</p>
<p>Graph Search. Graph search is a type of algorithm used to explore and navigate graphs, which are mathematical structures consisting of nodes (also called vertices) connected by edges. In graph search, the algorithm starts at a given node and systematically explores the graph by visiting its neighboring nodes in a specific order until it reaches a target node or a goal state.
The goal of graph search is to find the shortest or most efficient path between two nodes in a graph. There are several different types of graph search algorithms, including:</p>
<ul>
<li>Breadth-first search (BFS): This algorithm explores all the neighbors of a node before moving on to the next level of nodes. BFS is guaranteed to find the shortest path between two nodes in an unweighted graph.</li>
<li>Depth-first search (DFS): This algorithm explores one branch of the graph as far as possible before backtracking and exploring another branch. DFS can be used to find all paths between two nodes in a graph, but it may not find the shortest path.</li>
<li>Dijkstra's algorithm: This algorithm is used to find the shortest path between two nodes in a weighted graph. It works by assigning a tentative distance to each node and updating the distance as it explores the graph.</li>
<li>A<em> search: This algorithm is similar to Dijkstra's algorithm but uses a heuristic function to guide the search toward the goal node. A</em> search is often used in pathfinding in video games.</li>
</ul>
<p>Graph search algorithms can be used to solve a wide range of problems, but the choice of algorithm depends on the specific problem and the characteristics of the graph being searched.</p>
<p>Adversarial Planning. Adversarial planning is a type of planning problem where the planner is required to generate a plan that can anticipate and react to the actions of an adversarial agent. In this type of problem, the planner must take into account the actions of the adversary and try to find a plan that maximizes the chances of success while minimizing the impact of the adversary's actions.
Adversarial planning is commonly used in game theory, where it is used to model the strategies and actions of two or more players engaged in a game. In this context, the planner must anticipate the actions of the opponent and develop a strategy that maximizes the chances of winning.</p>
<p>There are several approaches to adversarial planning, including mini-max, in which the goal of the agents is to maximize their own rewards or utility while minimizing the rewards or utility of their opponents, and Monte Carlo Tree Search (MCTS), which uses a search algorithm to simulate the possible outcomes of the planner's actions and the adversary's responses.</p>
<p>Adversarial planning is a challenging problem because it requires the planner to consider not only their own objectives but also the objectives and capabilities of the adversary agents. As a result, it often involves complex decision-making and requires sophisticated algorithms and techniques.</p>
<h1>4 Planning Domain Generation</h1>
<p>In this section, we formulate an approach to generating planning domains using the large language model. To generate the planning domain, we apply the following procedure: querying a brief description of a task to GPT and transforming the responses into PDDL. If we have prior task knowledge, we can send it to GPT and ask it to generate PDDL from the knowledge. This approach enables the task designers to obtain task knowledge in a formal representation, regardless of the prior information the task designers have. Moreover, after generating the planning domains, the task designer can obtain a task plan by performing a simple path search. We also show the capability of GPT-4 on path search in the later sections.</p>
<p>We start the experiment with a daily-life task-cross the road-to examine the planning domain generation ability. We send the following input prompt to GPT-4:</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Define</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">problem</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">actions</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">task</span><span class="w"> </span><span class="s2">&quot;cross the road at traffic light&quot;</span><span class="w"> </span><span class="nv">in</span>
<span class="w">    </span><span class="nv">PDDL</span>.
</code></pre></div>

<p>The PDDL outputs generated by GPT-4 is presented in Listing 1.</p>
<p>Listing 1: Define a problem and actions for a task "cross the road at traffic light" in PDDL</p>
<div class="codehilite"><pre><span></span><code><span class="p">(</span><span class="nx">define</span><span class="w"> </span><span class="p">(</span><span class="nx">domain</span><span class="w"> </span><span class="nx">cross</span><span class="o">-</span><span class="nx">road</span><span class="p">)</span>
<span class="w">    </span><span class="p">(:</span><span class="nx">requirements</span><span class="w"> </span><span class="p">:</span><span class="nx">strips</span><span class="p">)</span>
<span class="w">    </span><span class="p">(:</span><span class="nx">types</span><span class="w"> </span><span class="nx">side</span><span class="p">)</span>
<span class="w">    </span><span class="p">(:</span><span class="nx">predicates</span>
<span class="w">        </span><span class="p">(</span><span class="nx">pedestrian</span><span class="o">-</span><span class="nx">at</span><span class="w"> </span><span class="p">?</span><span class="nx">s</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">side</span><span class="p">)</span>
<span class="w">        </span><span class="p">(</span><span class="nx">traffic</span><span class="o">-</span><span class="nx">light</span><span class="w"> </span><span class="p">?</span><span class="nx">s</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">side</span><span class="p">)</span>
<span class="w">        </span><span class="p">(</span><span class="nx">safe</span><span class="o">-</span><span class="nx">to</span><span class="o">-</span><span class="nx">cross</span><span class="w"> </span><span class="p">?</span><span class="nx">s</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">side</span><span class="p">)</span>
<span class="w">    </span><span class="p">)</span>
<span class="w">    </span><span class="p">(:</span><span class="nx">action</span><span class="w"> </span><span class="nx">wait</span><span class="o">-</span><span class="k">for</span><span class="o">-</span><span class="nx">green</span>
<span class="w">        </span><span class="p">:</span><span class="nx">parameters</span><span class="w"> </span><span class="p">(?</span><span class="nx">s</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">side</span><span class="p">)</span>
<span class="w">        </span><span class="p">:</span><span class="nx">precondition</span><span class="w"> </span><span class="p">(</span><span class="k">and</span><span class="w"> </span><span class="p">(</span><span class="nx">pedestrian</span><span class="o">-</span><span class="nx">at</span><span class="w"> </span><span class="p">?</span><span class="nx">s</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">traffic</span><span class="o">-</span><span class="nx">light</span><span class="w"> </span><span class="p">?</span><span class="nx">s</span><span class="p">)</span><span class="w"> </span><span class="p">)</span>
<span class="w">        </span><span class="p">:</span><span class="nx">effect</span><span class="w"> </span><span class="p">(</span><span class="nx">safe</span><span class="o">-</span><span class="nx">to</span><span class="o">-</span><span class="nx">cross</span><span class="w"> </span><span class="p">?</span><span class="nx">s</span><span class="p">)</span>
<span class="w">    </span><span class="p">)</span>
<span class="w">    </span><span class="p">(:</span><span class="nx">action</span><span class="w"> </span><span class="nx">cross</span>
<span class="w">        </span><span class="p">:</span><span class="nx">parameters</span><span class="w"> </span><span class="p">(?</span><span class="nx">s1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">side</span><span class="w"> </span><span class="p">?</span><span class="nx">s2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">side</span><span class="p">)</span>
<span class="w">        </span><span class="p">:</span><span class="nx">precondition</span><span class="w"> </span><span class="p">(</span><span class="k">and</span><span class="w"> </span><span class="p">(</span><span class="nx">pedestrian</span><span class="o">-</span><span class="nx">at</span><span class="w"> </span><span class="p">?</span><span class="nx">s1</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">safe</span><span class="o">-</span><span class="nx">to</span><span class="o">-</span><span class="nx">cross</span><span class="w"> </span><span class="p">?</span><span class="nx">s1</span><span class="p">)</span><span class="w"> </span><span class="p">)</span>
<span class="w">        </span><span class="p">:</span><span class="nx">effect</span><span class="w"> </span><span class="p">(</span><span class="k">and</span>
<span class="w">            </span><span class="p">(</span><span class="k">not</span><span class="w"> </span><span class="p">(</span><span class="nx">pedestrian</span><span class="o">-</span><span class="nx">at</span><span class="w"> </span><span class="p">?</span><span class="nx">s1</span><span class="p">))</span><span class="w"> </span><span class="p">(</span><span class="nx">pedestrian</span><span class="o">-</span><span class="nx">at</span><span class="w"> </span><span class="p">?</span><span class="nx">s2</span><span class="p">)</span>
<span class="w">            </span><span class="p">(</span><span class="k">not</span><span class="w"> </span><span class="p">(</span><span class="nx">safe</span><span class="o">-</span><span class="nx">to</span><span class="o">-</span><span class="nx">cross</span><span class="w"> </span><span class="p">?</span><span class="nx">s1</span><span class="p">))</span>
<span class="w">        </span><span class="p">)</span>
<span class="w">    </span><span class="p">)</span>
<span class="p">(</span><span class="nx">define</span><span class="w"> </span><span class="p">(</span><span class="nx">problem</span><span class="w"> </span><span class="nx">cross</span><span class="o">-</span><span class="nx">road</span><span class="o">-</span><span class="nx">problem</span><span class="p">)</span>
<span class="w">    </span><span class="p">(:</span><span class="nx">domain</span><span class="w"> </span><span class="nx">cross</span><span class="o">-</span><span class="nx">road</span><span class="p">)</span>
<span class="w">    </span><span class="p">(:</span><span class="nx">objects</span><span class="w"> </span><span class="nx">side1</span><span class="w"> </span><span class="nx">side2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">side</span><span class="p">)</span>
<span class="w">    </span><span class="p">(:</span><span class="nx">init</span>
<span class="w">        </span><span class="p">(</span><span class="nx">pedestrian</span><span class="o">-</span><span class="nx">at</span><span class="w"> </span><span class="nx">side1</span><span class="p">)</span>
<span class="w">        </span><span class="p">(</span><span class="nx">traffic</span><span class="o">-</span><span class="nx">light</span><span class="w"> </span><span class="nx">side1</span><span class="p">)</span>
<span class="w">        </span><span class="p">(</span><span class="nx">traffic</span><span class="o">-</span><span class="nx">light</span><span class="w"> </span><span class="nx">side2</span><span class="p">)</span>
<span class="w">    </span><span class="p">)</span>
<span class="w">    </span><span class="p">(:</span><span class="nx">goal</span><span class="w"> </span><span class="p">(</span><span class="k">and</span><span class="w"> </span><span class="p">(</span><span class="nx">pedestrian</span><span class="o">-</span><span class="nx">at</span><span class="w"> </span><span class="nx">side2</span><span class="p">)))</span>
<span class="p">)</span>
</code></pre></div>

<p>Then, we query GPT-4 to find a plan by searching through the planning domain:</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">Solve</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">problem</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="s2">&quot;cross the road at the traffic light&quot;</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">PDDL</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">the</span>
<span class="w">        </span><span class="nv">actions</span><span class="w"> </span><span class="nv">defined</span>.
<span class="w">    </span><span class="mi">1</span>.<span class="w"> </span><span class="k">wait</span><span class="o">-</span><span class="k">for</span><span class="o">-</span><span class="nv">green</span><span class="w"> </span><span class="nv">side1</span>
<span class="w">    </span><span class="mi">2</span>.<span class="w"> </span><span class="nv">cross</span><span class="w"> </span><span class="nv">side1</span><span class="w"> </span><span class="nv">side2</span>
</code></pre></div>

<p>The result indicates that the generated planning domain is self-contained, and we can obtain a formal representation of the plan, which solves a zero-shot planning problem.</p>
<p>In addition to daily-life tasks, we can ask GPT-4 to generate the planning domain for some wellknown games, such as Tic-Tac-Toe in Listing 2 and chess in Listing 3. However, once the complexity of the game increases, the success rate of generating self-contained planning domains from GPT-4 decreases. A failure example is the chess game in Listing 2, which defines the wrong goal state.</p>
<p>Listing 2: Define a problem and a set of actions for tic-tac-toe in PDDL.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Listing 3: Define a problem and a set of actions for the chess game in PDDL.</p>
<div class="codehilite"><pre><span></span><code>(define (problem chess-problem)
    (:domain chess)
    (:objects
        wp1 wp2 wp3 ...... bp6 bp7 bp8 - piece
            a1 a2 a3 ...... h6 h7 h8 - square
    )
    (:init
        (at wp1 a2) (at wp2 b2) (at wp3 c2) ......
        (clear a3) (clear a4) (clear a5) ......
    )
    (:goal (and (at wp1 a7) ))
}
</code></pre></div>

<p>For more empirical results, we select 100 tasks with different complexities. The tasks are ranged from board games to daily tasks to domain-specific tasks. Then, we query GPT-4 to generate planning domains for those tasks and check the correctness of the generated domains. We show the results in</p>
<p>Table 1: Results on Planning Domain Generation using GPT-4. A correct plan means the planning domain is self-contained and matches human knowledge. A wrong plan means the planning domain is self-contained but does not match human knowledge (e.g., chess game in Listing 3). A failed plan means the planning domain is not self-contained due to the inconsistency of predicates.</p>
<table>
<thead>
<tr>
<th>Total Tasks</th>
<th>Correct Plan</th>
<th>Wrong Plan</th>
<th>Failed</th>
</tr>
</thead>
<tbody>
<tr>
<td>100</td>
<td>73</td>
<td>27</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Table 2: Results on path search using GPT-4. Length indicates the length of the plan generated by fast-downward using the planning domains from GPT-4. The number of tasks indicates how many tasks can be completed in this range of steps. A plan is considered correct if the plan generated by GPT-4 is identical to the plan from fast-downward.</p>
<table>
<thead>
<tr>
<th>Length</th>
<th>$\leq 3$</th>
<th>$4-6$</th>
<th>$6-8$</th>
<th>$\geq 8$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Number of Tasks</td>
<td>68</td>
<td>10</td>
<td>7</td>
<td>15</td>
</tr>
<tr>
<td>Number of Correct Plans</td>
<td>68</td>
<td>8</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>Table 1. As we can see, GPT-4 can always generate self-contained planning domains but occasionally generate planning domains that do not match human knowledge.
Additionally, we further query GPT-4 to solve the planning problem given those generated domains. Since all the planning domains are self-contained, we also run fast-downward planner to find a plan and compare it with the plan generated from GPT-4. The results in Table 2 indicate that GPT-4 can find plans for simple tasks, but once the task requires more steps, GPT-4 may generate plans with missing disordered actions.
In conclusion, large language models like GPT-4 are useful in planning domain generation. Due to the rich knowledge encoded in these models, we can use them as a knowledge source, with a notice that they are not always reliable for complex tasks. In the path search aspect, GPT-4 can solve very simple path search problems from given planning domains. However, there is no significant advantage to using GPT-4 compared to using a traditional planner.</p>
<h1>5 Graph Search</h1>
<p>In this section, we examine the capability of large language models, such as GPT-4, on graph searches. The examination consists of two aspects: first, whether GPT-4 understands the well-known graph search algorithms, and second, whether GPT-4 can follow the algorithms to find the desired path. Note that we compare the outputs of GPT-4 to the outputs of the graph search algorithms. The outputs are not necessarily the optimal path. We consider GPT-4 to be accurate as long as it can generate paths following the graph search algorithms.
We collect direct weighted graphs from 5 nodes to 95 nodes with a gap of $10(5,15,25, \ldots, 85$, 95 nodes). For each number of nodes, we collect 20 different graphs. An example is presented in Figure 1. For each graph, we query GPT-4 to perform three graph search algorithms, depth-first search, breath-first search, and Dijkstra's algorithm, respectively, to generate paths. For the example in Figure 1, we query:</p>
<div class="codehilite"><pre><span></span><code>Run BFS to find the shortest path from A to E.
Here&#39;s the BFS traversal on the directed graph:
</code></pre></div>

<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 1: A randomly generated direct weighted graph for examining the graph search capability of large language models.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2: The accuracy of graph searching results generated by GPT-4 on graphs with different nodes.</p>
<div class="codehilite"><pre><span></span><code><span class="mf">3</span><span class="w"> </span><span class="mf">1.</span><span class="w"> </span><span class="n">Start</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">node</span><span class="w"> </span><span class="n">A</span><span class="mf">.</span><span class="w"> </span><span class="n">Enqueue</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">mark</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">visited</span><span class="mf">.</span>
<span class="mf">4</span><span class="w"> </span><span class="mf">2.</span><span class="w"> </span><span class="n">Dequeue</span><span class="w"> </span><span class="n">A</span><span class="mf">.</span><span class="w"> </span><span class="n">Enqueue</span><span class="w"> </span><span class="n">its</span><span class="w"> </span><span class="n">unvisited</span><span class="w"> </span><span class="n">neighbors</span><span class="p">,</span><span class="w"> </span><span class="n">B</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">mark</span><span class="w"> </span><span class="n">them</span><span class="w"> </span><span class="n">as</span>
<span class="w">        </span><span class="n">visited</span><span class="mf">.</span>
<span class="mf">5</span><span class="w"> </span><span class="mf">3.</span><span class="w"> </span><span class="n">Dequeue</span><span class="w"> </span><span class="n">B</span><span class="mf">.</span><span class="w"> </span><span class="n">Enqueue</span><span class="w"> </span><span class="n">its</span><span class="w"> </span><span class="n">unvisited</span><span class="w"> </span><span class="n">neighbor</span><span class="p">,</span><span class="w"> </span><span class="n">D</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">mark</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">visited</span><span class="mf">.</span>
<span class="mf">6</span><span class="w"> </span><span class="mf">4.</span><span class="w"> </span><span class="n">Dequeue</span><span class="w"> </span><span class="n">C</span><span class="mf">.</span><span class="w"> </span><span class="n">Enqueue</span><span class="w"> </span><span class="n">its</span><span class="w"> </span><span class="n">unvisited</span><span class="w"> </span><span class="n">neighbor</span><span class="p">,</span><span class="w"> </span><span class="n">E</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">mark</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">visited</span><span class="mf">.</span>
<span class="mf">7</span><span class="w"> </span><span class="mf">5.</span><span class="w"> </span><span class="n">Dequeue</span><span class="w"> </span><span class="n">D</span><span class="mf">.</span><span class="w"> </span><span class="n">There</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">unvisited</span><span class="w"> </span><span class="n">neighbors</span><span class="mf">.</span>
<span class="mf">8</span><span class="w"> </span><span class="mf">6.</span><span class="w"> </span><span class="n">Dequeue</span><span class="w"> </span><span class="n">E</span><span class="mf">.</span><span class="w"> </span><span class="n">There</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">unvisited</span><span class="w"> </span><span class="n">neighbors</span><span class="mf">.</span>
<span class="mf">9</span><span class="n">The</span><span class="w"> </span><span class="n">BFS</span><span class="w"> </span><span class="n">traversal</span><span class="w"> </span><span class="n">visits</span><span class="w"> </span><span class="n">nodes</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="ow">or</span><span class="n">der</span><span class="p">:</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">B</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">C</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">D</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">E</span><span class="mf">.</span>
<span class="mf">10</span><span class="n">By</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">predecessors</span><span class="p">,</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">reconstruct</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">shortest</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="kr">to</span><span class="w"> </span><span class="n">E</span><span class="p">:</span>
<span class="mf">11</span><span class="w"> </span><span class="mf">1.</span><span class="w"> </span><span class="n">Start</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">node</span><span class="w"> </span><span class="n">E</span><span class="mf">.</span>
<span class="mf">12</span><span class="w"> </span><span class="mf">2.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">predecessor</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">C</span><span class="mf">.</span><span class="w"> </span><span class="n">So</span><span class="p">,</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="o">&lt;--</span><span class="w"> </span><span class="n">C</span><span class="mf">.</span>
<span class="mf">13</span><span class="w"> </span><span class="mf">3.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">predecessor</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">C</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">A</span><span class="mf">.</span><span class="w"> </span><span class="n">So</span><span class="p">,</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">C</span><span class="w"> </span><span class="o">&lt;--</span><span class="w"> </span><span class="n">A</span><span class="mf">.</span>
<span class="mf">14</span><span class="n">Putting</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="kr">toget</span><span class="n">her</span><span class="p">,</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="kr">get</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">shortest</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="kr">to</span><span class="w"> </span><span class="n">E</span><span class="p">:</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">C</span><span class="w"> </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">E</span><span class="mf">.</span>
<span class="mf">15</span>
<span class="mf">16</span><span class="w"> </span><span class="kr">Run</span><span class="w"> </span><span class="n">Dijkstra</span><span class="err">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">algorithm</span><span class="w"> </span><span class="kr">to</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">shortest</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="kr">to</span><span class="w"> </span><span class="n">E</span><span class="mf">.</span>
<span class="mf">17</span>
<span class="mf">18</span><span class="n">Here</span><span class="err">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">DFS</span><span class="w"> </span><span class="n">traversal</span><span class="w"> </span><span class="kr">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">directed</span><span class="w"> </span><span class="n">graph</span><span class="p">:</span>
<span class="mf">19</span>
<span class="mf">20</span><span class="n">The</span><span class="w"> </span><span class="n">DFS</span><span class="w"> </span><span class="n">traversal</span><span class="w"> </span><span class="n">visits</span><span class="w"> </span><span class="n">nodes</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="ow">or</span><span class="n">der</span><span class="p">:</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">B</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">D</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">C</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">E</span><span class="mf">.</span>
<span class="mf">21</span><span class="n">Putting</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="kr">toget</span><span class="n">her</span><span class="p">,</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="kr">get</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="kr">to</span><span class="w"> </span><span class="n">E</span><span class="p">:</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">C</span><span class="w"> </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">E</span><span class="mf">.</span>
<span class="mf">22</span>
<span class="mf">23</span>
<span class="mf">24</span>
<span class="mf">25</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Run DFS to find the shortest path from A to E.
26
27
</code></pre></div>

<p>Here's how Dijkstra's algorithm proceeds:
28
29 1. Distances: A $=0, B=999, C=999, D=999, E=999$. Unvisited: A, B, C, D, E
30 2. Visit node A. Update B's distance and set its predecessor as A. Update C's distance and set its predecessor as A. Distances: $A=0, B=3, C=2, D$ $=999, E=999$. Unvisited: $B, C, D, E$
31 3. Visit node C (smallest unvisited distance). Update E's distance and set its predecessor as C. Distances: $A=0, B=3, C=2, D=999, E=8$. Unvisited: B, D, E
32 4. Visit node B (smallest unvisited distance). Update D's distance and set its predecessor as B. Distances: $A=0, B=3, C=2, D=7, E=8$. Unvisited: D, E
33 5. Visit node D (smallest unvisited distance). No updates. Unvisited: E
34 6. Visit node E (smallest unvisited distance). No unvisited neighbors.
35
36 Putting it all together, we get a path from A to E: A --&gt; C --&gt; E.</p>
<p>As we indicate in this example, GPT-4 is sufficient to generate accurate paths under all three algorithms. However, as the graphs become more complicated, the accuracy of GPT-4 decreases. In Figure 2, we present how the accuracies of GPT-4 on three algorithms decrease as the number of graph nodes increases. Therefore, we conclude that GPT-4 is only capable of simple graph search. But it provides the possibility of decomposing a complex graph into simple graphs and performing graph search.</p>
<h1>6 Adversarial Planning</h1>
<p>In this section, we explore the capability of the large language model, specifically GPT-4, on adversarial planning. The experiment consists of two components: defining heuristics and applying adversarial search.</p>
<p>First, we choose the simple game Tic-Tac-Toe as an example and query GPT-4 for a proper heuristic:</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Design</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">heuristic</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">adversarial</span><span class="w"> </span><span class="nx">planning</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">tic</span><span class="o">-</span><span class="nx">tac</span><span class="o">-</span><span class="nx">toe</span><span class="p">.</span>
<span class="nx">Here</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">simple</span><span class="w"> </span><span class="nx">heuristic</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">Tic</span><span class="o">-</span><span class="nx">Tac</span><span class="o">-</span><span class="nx">Toe</span><span class="p">:</span>
<span class="mi">1</span><span class="p">.</span><span class="w"> </span><span class="nx">Assign</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">value</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">row</span><span class="p">,</span><span class="w"> </span><span class="nx">column</span><span class="p">,</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">diagonal</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">only</span><span class="w"> </span><span class="nx">the</span>
<span class="w">    </span><span class="nx">maximizing</span><span class="w"> </span><span class="nx">player</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">symbols</span><span class="w"> </span><span class="p">(</span><span class="k">and</span><span class="w"> </span><span class="nx">no</span><span class="w"> </span><span class="nx">opponent</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">symbols</span><span class="p">).</span>
<span class="mi">2</span><span class="p">.</span><span class="w"> </span><span class="nx">Assign</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">value</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">row</span><span class="p">,</span><span class="w"> </span><span class="nx">column</span><span class="p">,</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">diagonal</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">only</span><span class="w"> </span><span class="nx">the</span>
<span class="w">    </span><span class="nx">minimizing</span><span class="w"> </span><span class="nx">player</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">symbols</span><span class="w"> </span><span class="p">(</span><span class="k">and</span><span class="w"> </span><span class="nx">no</span><span class="w"> </span><span class="nx">opponent</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">symbols</span><span class="p">).</span>
<span class="mi">3</span><span class="p">.</span><span class="w"> </span><span class="nx">Assign</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">value</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">rows</span><span class="p">,</span><span class="w"> </span><span class="nx">columns</span><span class="p">,</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">diagonals</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">both</span><span class="w"> </span><span class="nx">the</span>
<span class="w">    </span><span class="nx">maximizing</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">minimizing</span><span class="w"> </span><span class="nx">player</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">symbols</span><span class="p">,</span><span class="w"> </span><span class="nx">since</span><span class="w"> </span><span class="nx">these</span><span class="w"> </span><span class="nx">lines</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="k">not</span>
<span class="w">    </span><span class="nx">possible</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">win</span><span class="p">.</span>
<span class="mi">4</span><span class="p">.</span><span class="w"> </span><span class="nx">Sum</span><span class="w"> </span><span class="nx">up</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">values</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">steps</span><span class="w"> </span><span class="mi">1</span><span class="o">-</span><span class="mi">3</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">calculate</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">total</span><span class="w"> </span><span class="nx">score</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">the</span>
<span class="w">    </span><span class="nx">board</span><span class="w"> </span><span class="nx">state</span><span class="p">.</span>
</code></pre></div>

<p>The language model successfully generates a heuristic for adversarial planning in Tic-Tac-Toe. We then manually implement this heuristic and perform the Mini-max algorithm. It turns out that the output of GPT-4 is a workable heuristic.</p>
<p>Second, we examine the performance of GPT-4 on adversarial planning. We query GPT-4 for playing the game with the heuristic defined above:</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Play</span><span class="w"> </span><span class="nv">tic</span><span class="o">-</span><span class="nv">tac</span><span class="o">-</span><span class="nv">toe</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">me</span><span class="w"> </span><span class="nv">using</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">Mini</span><span class="o">-</span><span class="nv">max</span><span class="w"> </span><span class="nv">algorithm</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">heuristic</span>
<span class="w">    </span><span class="nv">defined</span><span class="w"> </span><span class="nv">above</span>.
<span class="nv">Please</span><span class="w"> </span><span class="nv">provide</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">current</span><span class="w"> </span><span class="nv">state</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">your</span><span class="w"> </span><span class="nv">Tic</span><span class="o">-</span><span class="nv">Tac</span><span class="o">-</span><span class="nv">Toe</span><span class="w"> </span><span class="nv">board</span>,<span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">let</span><span class="w"> </span><span class="nv">me</span><span class="w"> </span><span class="nv">know</span>
<span class="w">    </span><span class="nv">whether</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="nv">move</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="s1">&#39;X&#39;</span><span class="w"> </span><span class="ss">(</span><span class="nv">maximizing</span><span class="w"> </span><span class="nv">player</span><span class="ss">)</span><span class="w"> </span><span class="nv">or</span><span class="w"> </span><span class="s1">&#39;O&#39;</span><span class="w"> </span><span class="ss">(</span><span class="nv">minimizing</span>
<span class="w">    </span><span class="nv">player</span><span class="ss">)</span>.<span class="w"> </span><span class="k">Then</span>,<span class="w"> </span><span class="nv">I</span><span class="err">&#39;ll help you determine the best move according to the</span>
<span class="err">    heuristic.</span>
<span class="err">Board: ---</span>
<span class="err">    --X</span>
<span class="err">    --</span>
<span class="err">Here are the heuristic scores for each possible move:</span>
<span class="err">01: 0-- 02: -0- 03: --0 04: -0- 05: --0</span>
<span class="err">    --X --X --X -0X -X0</span>
<span class="err">    -- --- --- --- ---</span>
<span class="err">06: --0 07: --- 08: --- 09: ---</span>
<span class="err">0-X 0-X -0X -X0</span>
<span class="err">    -- 0- --0 --0</span>
<span class="mi">04</span>,<span class="w"> </span><span class="mi">05</span>:<span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="ss">(</span><span class="nv">Blocks</span><span class="w"> </span><span class="s1">&#39;X&#39;</span><span class="w"> </span><span class="nv">from</span><span class="w"> </span><span class="nv">potentially</span><span class="w"> </span><span class="nv">winning</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="nv">move</span><span class="ss">)</span>
<span class="nv">Others</span>:<span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="ss">(</span><span class="nv">No</span><span class="w"> </span><span class="nv">immediate</span><span class="w"> </span><span class="nv">advantage</span><span class="w"> </span><span class="nv">or</span><span class="w"> </span><span class="nv">disadvantage</span><span class="ss">)</span>
<span class="nv">According</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">heuristic</span>,<span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">best</span><span class="w"> </span><span class="nv">moves</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">O</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="mi">04</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="mi">05</span>.
</code></pre></div>

<p>In this example, GPT-4 fails to identify possible moves and memorizes the sequence of previous states, i.e., the existing pieces on the board. Moreover, even with the misidentified moves, it fails to compute the heuristics. From this observation, we can also claim that GPT-4 is unable to understand the heuristics. Therefore, it has limited capability of graph search algorithms like A* as well.</p>
<p>Due to the failure of adversarial planning in Tic-Tac-Toe, we stopped examining its capability on more complicated tasks and derived our conclusion.</p>
<p>In conclusion, LLMs are capable of generating reasonable heuristics for the adversarial planning of simple games. We examine this capability in the tic-tac-toe example, where the AI-driven heuristic allows for evaluating board positions based on the presence of a player's symbols in rows, columns, and diagonals. LLMs encode rich knowledge and can provide reasonable heuristics for some given tasks.</p>
<p>This simple yet heuristic demonstrates the potential of using AI-generated heuristics to guide and enhance decision-making in various problem domains. In conjunction with adversarial search algorithms such as Minimax or Alpha-Beta pruning, these heuristics enable the creation of AI opponents that can effectively compete against human players in simple games like Tic-Tac-Toe.</p>
<p>However, LLMs cannot perform adversarial searching algorithms like Mini-max or Monte-Carlo Tree Search. Due to the fact that GPTs are a series of language models for next-word prediction, they can neither understand the state of the game nor search over all the possibilities (as we addressed the graph search limitation in the last section). Additionally, the language model cannot memorize the sequence of previous states correctly. These factors raise a limitation of LLMs and could potentially be a direction of improvement.</p>
<h1>7 Fine-tuning LLM for Logical Reasoning</h1>
<p>Given the subpar performance of LLM on logical reasoning tasks like adversarial planning, we can fine-tune our own language model to check if we could improve its performance on logistic reasoning tasks.</p>
<h3>7.1 Dataset</h3>
<p>The dataset collected comprised three parts which were planning domain generation ( 7 different tasks), graph search ( 20 different tasks), and adversarial planning ( 4 different tasks). For example, for planning domain generation, we queried GPT-4 using seven different problem definitions. Each problem definition generated 10-100 different goal state configurations depending on the problem, resulting in a total of 540 queries. Given each query (only those queries were selected, which we thought would give correct results when passed through LLM), we ran GPT-4 inference on them to get the soft labels for fine-tuning our own LLM. We collected around 1300 queries (appended with the name of the part, e.g., planning domain queries were appended by planning domain : and so on) and soft label pairs across all tasks.</p>
<h3>7.2 Model Selection and Fine-tuning</h3>
<p>To select the correct model to fine-tune, we chose a model small enough that could be easily fine-tuned using the resources available to us and also large enough such that it could infer logically. We chose the Flan T5 base model [Shen et al., 2023] released by Google since it meets this criterion. One of the reasons we chose this model was because this model's checkpoints were readily available at HuggingFace, and had a reasonable size of 240M parameters. Also, as shown in [Shen et al., 2023], the model shows the SOTA performance in the CoT dataset [Qingyi Si, 2023], which contains a chain of thought data points like arithmetic reasoning, explanation generation, etc., for the number of parameters it had. For fine-tuning the model, we froze the weights of the original model. We only changed the final layer's weights. Updating the weights of the entire model could have led to catastrophic forgetting, or it would have been fine-tuned properly since our dataset size was too small.</p>
<h3>7.3 Results</h3>
<p>We fine-tuned our LLM model on approximately 1000 data points and evaluated it on the remaining 200 data points. We used the remaining 100 data points as the validation set and used it to stop training the model when we started to see an increase in the validation loss. Given the rapid advances in LLM models, we observed that the outputs of GPT-4 were much better than the Flan models, which were more or less incorrect for every data point we had. In Table 3, we compare the outputs of Flan, fine-tuned Flan, and GPT-4 for a specific case of graph search and planning domain generation. As for adversarial search, the flan models are unable to come up with a coherent heuristic, and hence we skip their evaluations. Also, we could not compare the output of the LLM and the ground truth values after planning since all the planning domains provided by the Flan were incorrect. In conclusion, we were able to see minor improvements due to fine-tuning because the original model might have never seen prompts like these but was unable to process the outputs of the fine-tuned models.</p>
<h2>8 Conclusion</h2>
<p>Large language models can play critical roles in planning due to their large knowledge domain. The up-to-date large language models encode rich real-world knowledge and can make logical inferences to a certain extent. We provide examples to demonstrate that these models can generate self-consistent planning domains of given tasks without any prior information provided. This capability enables language models to do zero-shot planning. Moreover, the language models can perform graph searches on small-scaled graphs, indicating their great potential in searching. However, the current models have limited abilities to memorize the sequence of previous states during planning and to solve search problems in complicated environments (graphs). Both limitations lead the large language models incapable of adversarial planning. Overall, large language models can play significant roles in planning, especially few-shot planning, and their significance can be improved over time.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Planning domain</th>
<th style="text-align: center;">Graph Search</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Input query</td>
<td style="text-align: center;">Q. Define a problem and actions for the task "tic-tac-toe" in PDDL?</td>
<td style="text-align: center;">Answer the following yes/no question by reasoning step-by-step. Given a graph with vertices A, B, C, D, E where the connections are given as A-B, B-C, C-D, C-E. Run BFS to find the shortest path from A to E .</td>
</tr>
<tr>
<td style="text-align: center;">Flan</td>
<td style="text-align: center;">Tic-tac-toe is a word that means "to engage in a classic three-in-a-row game typically played on a grid by two players taking turns to place their respective marks, either X or O , in an attempt to achieve a winning combination."</td>
<td style="text-align: center;">BFS is a program that calculates the distance between A and E .</td>
</tr>
<tr>
<td style="text-align: center;">Flan-finetuned</td>
<td style="text-align: center;">(define (domain tic-tac-toe ) (:requirements :strips :typing :equality))</td>
<td style="text-align: center;">BFS is a program that calculates the distance between A and E. Exploring A gives B. Exploring B gives C. Exploring C gives D and E. Therefore, the path is A - B - C - E.</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;"><code>(define (domain tic-tac-toe )(:requirements :strips :typing :equality) (:types cell) (:predicates (empty ?c - cell) (z ?c - cell) (o ?c - cell) ) ...</code></td>
<td style="text-align: center;"><code>To determine if we can run Breadth-First Search ( BFS) to find the shortest path from A to E, let's go through the BFS algorithm step- by-step for the given graph. Start at the source vertex A. Explore all neighbors of A: (A-B) Explore all unvisited neighbors of B: (B-C) C has two unvisited neighbors: (C-D) and (C -E) At this point, we've reached vertex E, so we can stop the algorithm The shortest path from A to E is A-B-C-E.</code></td>
</tr>
</tbody>
</table>
<p>Table 3: The comparisons between the outputs of various models for planning domains and graph search.</p>
<h1>References</h1>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877-1901, 2020.</p>
<p>Bharath Chintagunta, Namit Katariya, Xavier Amatriain, and Anitha Kannan. Medically aware gpt-3 as a data generator for medical dialogue summarization. In Machine Learning for Healthcare Conference, pages 354-372. PMLR, 2021.</p>
<p>Patrik Haslum, Nir Lipovetzky, Daniele Magazzeni, and Christian Muise. An introduction to the planning domain definition language. Synthesis Lectures on Artificial Intelligence and Machine</p>
<p>Learning, 2019.
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zeroshot planners: Extracting actionable knowledge for embodied agents. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 9118-9147. PMLR, 2022a. URL https://proceedings.mlr.press/v162/huang22a.html.</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models. 205:1769-1782, 2022b.</p>
<p>Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng Kuang, Sally Jesmonth, Nikhil J. Joshi, Kyle Jeffrey, Rosario Jauregui Ruano, Jasmine Hsu, Keerthana Gopalakrishnan, Byron David, Andy Zeng, and Chuyuan Kelly Fu. Do as I can, not as I say: Grounding language in robotic affordances. In Karen Liu, Dana Kulic, and Jeffrey Ichnowski, editors, Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, volume 205 of Proceedings of Machine Learning Research, pages 287-318. PMLR, 2022. URL https://proceedings.mlr.press/ v205/ichter23a.html.</p>
<p>Markus Leippold. Thus spoke gpt-3: Interviewing a large-language model on climate finance. Finance Research Letters, 53:103617, 2023.</p>
<p>Bill Yuchen Lin, Chengsong Huang, Qian Liu, Wenda Gu, Sam Sommerer, and Xiang Ren. On grounded planning for embodied tasks with language models. In Brian Williams, Yiling Chen, and Jennifer Neville, editors, AAAI Conference on Artificial Intelligence, pages 13192-13200, Washington, DC, USA, 2023. AAAI Press.</p>
<p>Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023.</p>
<p>OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi:10.48550/arXiv.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774.</p>
<p>Zheng Lin Qingyi Si. Alpaca-cot: An instruction fine-tuning platform with instruction data collection and unified large language models interface. https://github.com/PhoebusSi/alpaca-CoT, 2023.</p>
<p>Dhruv Shah, Blazej Osinski, Brian Ichter, and Sergey Levine. Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. In Karen Liu, Dana Kulic, and Jeffrey Ichnowski, editors, Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, volume 205 of Proceedings of Machine Learning Research, pages 492-504. PMLR, 2022. URL https://proceedings.mlr.press/v205/shah23b.html.</p>
<p>Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan Li, Vincent Zhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, and Denny Zhou. Flan-moe: Scaling instruction-finetuned language models with sparse mixture of experts. CoRR, abs/2305.14705, 2023. doi:10.48550/arXiv.2305.14705. URL https://doi.org/10.48550/arXiv.2305.14705.</p>
<p>Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa Patwary, Mohammad Shoeybi, and Raghav Mani. Biomegatron: Larger biomedical domain language model. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 4700-4706. Association for Computational Linguistics, 2020. doi:10.18653/v1/2020.emnlpmain.379. URL https://doi.org/10.18653/v1/2020.emnlp-main. 379.</p>
<p>Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. In IEEE International Conference on Robotics and Automation, ICRA 2023, London, UK, May 29 - June 2, 2023, pages 11523-11530. IEEE, 2023.</p>
<p>Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su. Llmplanner: Few-shot grounded planning for embodied agents with large language models. CoRR, abs/2212.04088, 2022. doi:10.48550/arXiv.2212.04088. URL https://doi.org/10.48550/ arXiv. 2212.04088.</p>
<p>Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine. Nature Medicine, pages 1-11, 2023.</p>
<p>Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still can't plan (a benchmark for llms on planning and reasoning about change). arXiv preprint arXiv:2206.10498, 2022.</p>
<p>Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David S. Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance. CoRR, abs/2303.17564, 2023. doi:10.48550/arXiv.2303.17564. URL https://doi.org/10.48550/arXiv.2303.17564.</p>
<p>Yunhao Yang, Jean-Raphaël Gaglione, Cyrus Neary, and Ufuk Topcu. Automaton-based representations of task knowledge from generative language models. CoRR, abs/2212.01944, 2022. doi:10.48550/arXiv.2212.01944. URL https://doi.org/10.48550/arXiv.2212.01944.</p>
<h1>A Appendix</h1>
<h2>A. 1 Planning Domain Examples</h2>
<p>Listing 4: Define a problem and a set of actions for the chess game in PDDL.</p>
<div class="codehilite"><pre><span></span><code><span class="p">(</span><span class="nx">define</span><span class="w"> </span><span class="p">(</span><span class="nx">domain</span><span class="w"> </span><span class="nx">chess</span><span class="p">)</span>
<span class="w">    </span><span class="p">(:</span><span class="nx">requirements</span><span class="w"> </span><span class="p">:</span><span class="nx">strips</span><span class="w"> </span><span class="p">:</span><span class="nx">typing</span><span class="w"> </span><span class="p">:</span><span class="nx">equality</span><span class="w"> </span><span class="p">:</span><span class="nx">conditional</span><span class="o">-</span><span class="nx">effects</span><span class="p">)</span>
<span class="w">    </span><span class="p">(:</span><span class="nx">types</span><span class="w"> </span><span class="nx">piece</span><span class="w"> </span><span class="nx">square</span><span class="w"> </span><span class="p">)</span>
<span class="w">    </span><span class="p">(:</span><span class="nx">predicates</span>
<span class="w">        </span><span class="p">(</span><span class="nx">clear</span><span class="w"> </span><span class="p">?</span><span class="nx">s</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">square</span><span class="p">)</span>
<span class="w">            </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="p">?</span><span class="nx">p</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">piece</span><span class="w"> </span><span class="p">?</span><span class="nx">s</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">square</span><span class="p">)</span>
<span class="w">            </span><span class="p">(</span><span class="nx">captured</span><span class="w"> </span><span class="p">?</span><span class="nx">p</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">piece</span><span class="p">)</span>
<span class="w">    </span><span class="p">)</span>
<span class="w">    </span><span class="p">(:</span><span class="nx">action</span><span class="w"> </span><span class="nx">move</span>
<span class="w">        </span><span class="p">:</span><span class="nx">parameters</span><span class="w"> </span><span class="p">(?</span><span class="nx">p</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">piece</span><span class="w"> </span><span class="p">?</span><span class="nx">from</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">square</span><span class="w"> </span><span class="p">?</span><span class="nx">to</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">square</span><span class="p">)</span>
<span class="w">        </span><span class="p">:</span><span class="nx">precondition</span><span class="w"> </span><span class="p">(</span><span class="k">and</span>
<span class="w">            </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="p">?</span><span class="nx">p</span><span class="w"> </span><span class="p">?</span><span class="nx">from</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">clear</span><span class="w"> </span><span class="p">?</span><span class="nx">to</span><span class="p">)</span>
<span class="w">        </span><span class="p">)</span>
<span class="w">        </span><span class="p">:</span><span class="nx">effect</span><span class="w"> </span><span class="p">(</span><span class="k">and</span>
<span class="w">            </span><span class="p">(</span><span class="k">not</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="p">?</span><span class="nx">p</span><span class="w"> </span><span class="p">?</span><span class="nx">from</span><span class="p">))</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="p">?</span><span class="nx">p</span><span class="w"> </span><span class="p">?</span><span class="nx">to</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">clear</span><span class="w"> </span><span class="p">?</span><span class="nx">from</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="k">not</span><span class="w"> </span><span class="p">(</span><span class="nx">clear</span><span class="w"> </span><span class="p">?</span><span class="nx">to</span><span class="p">))</span>
<span class="w">        </span><span class="p">)</span>
<span class="w">    </span><span class="p">)</span>
<span class="w">    </span><span class="p">(:</span><span class="nx">action</span><span class="w"> </span><span class="nx">capture</span>
<span class="w">        </span><span class="p">:</span><span class="nx">parameters</span><span class="w"> </span><span class="p">(?</span><span class="nx">p1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">piece</span><span class="w"> </span><span class="p">?</span><span class="nx">p2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">piece</span><span class="w"> </span><span class="p">?</span><span class="nx">from</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">square</span><span class="w"> </span><span class="p">?</span><span class="nx">to</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">square</span><span class="p">)</span>
<span class="w">        </span><span class="p">:</span><span class="nx">precondition</span><span class="w"> </span><span class="p">(</span><span class="k">and</span>
<span class="w">            </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="p">?</span><span class="nx">p1</span><span class="w"> </span><span class="p">?</span><span class="nx">from</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="p">?</span><span class="nx">p2</span><span class="w"> </span><span class="p">?</span><span class="nx">to</span><span class="p">)</span>
<span class="w">        </span><span class="p">)</span>
<span class="w">        </span><span class="p">:</span><span class="nx">effect</span><span class="w"> </span><span class="p">(</span><span class="k">and</span>
<span class="w">            </span><span class="p">(</span><span class="k">not</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="p">?</span><span class="nx">p1</span><span class="w"> </span><span class="p">?</span><span class="nx">from</span><span class="p">))</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="p">?</span><span class="nx">p1</span><span class="w"> </span><span class="p">?</span><span class="nx">to</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="k">not</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="p">?</span><span class="nx">p2</span><span class="w"> </span><span class="p">?</span><span class="nx">to</span><span class="p">))</span>
<span class="w">            </span><span class="p">(</span><span class="nx">captured</span><span class="w"> </span><span class="p">?</span><span class="nx">p2</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">clear</span><span class="w"> </span><span class="p">?</span><span class="nx">from</span><span class="p">)</span>
<span class="w">        </span><span class="p">)</span>
<span class="w">    </span><span class="p">)</span>
<span class="p">)</span>
<span class="p">(</span><span class="nx">define</span><span class="w"> </span><span class="p">(</span><span class="nx">problem</span><span class="w"> </span><span class="nx">chess</span><span class="o">-</span><span class="nx">problem</span><span class="p">)</span>
<span class="w">    </span><span class="p">(:</span><span class="nx">domain</span><span class="w"> </span><span class="nx">chess</span><span class="p">)</span>
<span class="w">    </span><span class="p">(:</span><span class="nx">objects</span>
<span class="w">        </span><span class="nx">wp1</span><span class="w"> </span><span class="nx">wp2</span><span class="w"> </span><span class="nx">wp3</span><span class="w"> </span><span class="nx">wp4</span><span class="w"> </span><span class="nx">wp5</span><span class="w"> </span><span class="nx">wp6</span><span class="w"> </span><span class="nx">wp7</span><span class="w"> </span><span class="nx">wp8</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">piece</span>
<span class="w">        </span><span class="nx">bp1</span><span class="w"> </span><span class="nx">bp2</span><span class="w"> </span><span class="nx">bp3</span><span class="w"> </span><span class="nx">bp4</span><span class="w"> </span><span class="nx">bp5</span><span class="w"> </span><span class="nx">bp6</span><span class="w"> </span><span class="nx">bp7</span><span class="w"> </span><span class="nx">bp8</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">piece</span>
<span class="w">        </span><span class="nx">a1</span><span class="w"> </span><span class="nx">a2</span><span class="w"> </span><span class="nx">a3</span><span class="w"> </span><span class="nx">a4</span><span class="w"> </span><span class="nx">a5</span><span class="w"> </span><span class="nx">a6</span><span class="w"> </span><span class="nx">a7</span><span class="w"> </span><span class="nx">a8</span>
<span class="w">            </span><span class="o">...</span>
<span class="w">            </span><span class="nx">h1</span><span class="w"> </span><span class="nx">h2</span><span class="w"> </span><span class="nx">h3</span><span class="w"> </span><span class="nx">h4</span><span class="w"> </span><span class="nx">h5</span><span class="w"> </span><span class="nx">h6</span><span class="w"> </span><span class="nx">h7</span><span class="w"> </span><span class="nx">h8</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">square</span>
<span class="w">    </span><span class="p">)</span>
<span class="w">    </span><span class="p">(:</span><span class="nx">init</span>
<span class="w">        </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="nx">wp1</span><span class="w"> </span><span class="nx">a2</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="nx">wp2</span><span class="w"> </span><span class="nx">b2</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="nx">wp3</span><span class="w"> </span><span class="nx">c2</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="nx">wp4</span><span class="w"> </span><span class="nx">d2</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="nx">wp5</span><span class="w"> </span><span class="nx">e2</span><span class="p">)</span>
<span class="w">        </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="nx">wp6</span><span class="w"> </span><span class="nx">f2</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="nx">wp7</span><span class="w"> </span><span class="nx">g2</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="nx">wp8</span><span class="w"> </span><span class="nx">h2</span><span class="p">)</span>
<span class="w">        </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="nx">bp1</span><span class="w"> </span><span class="nx">a7</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="nx">bp2</span><span class="w"> </span><span class="nx">b7</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="nx">bp3</span><span class="w"> </span><span class="nx">c7</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="nx">bp4</span><span class="w"> </span><span class="nx">d7</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="nx">bp5</span><span class="w"> </span><span class="nx">e7</span><span class="p">)</span>
<span class="w">        </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="nx">bp6</span><span class="w"> </span><span class="nx">f7</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="nx">bp7</span><span class="w"> </span><span class="nx">g7</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="nx">bp8</span><span class="w"> </span><span class="nx">h7</span><span class="p">)</span>
<span class="w">        </span><span class="p">(</span><span class="nx">clear</span><span class="w"> </span><span class="nx">a3</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">clear</span><span class="w"> </span><span class="nx">a4</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">clear</span><span class="w"> </span><span class="nx">a5</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">clear</span><span class="w"> </span><span class="nx">a6</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">clear</span><span class="w"> </span><span class="nx">a8</span><span class="p">)</span>
<span class="w">            </span><span class="o">...</span>
<span class="w">            </span><span class="p">(</span><span class="nx">clear</span><span class="w"> </span><span class="nx">h3</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">clear</span><span class="w"> </span><span class="nx">h4</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">clear</span><span class="w"> </span><span class="nx">h5</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">clear</span><span class="w"> </span><span class="nx">h6</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">clear</span><span class="w"> </span><span class="nx">h8</span><span class="p">)</span>
<span class="w">    </span><span class="p">)</span>
<span class="w">    </span><span class="p">(:</span><span class="nx">goal</span><span class="w"> </span><span class="p">(</span><span class="k">and</span><span class="w"> </span><span class="p">(</span><span class="nx">at</span><span class="w"> </span><span class="nx">wp1</span><span class="w"> </span><span class="nx">a7</span><span class="p">)</span><span class="w"> </span><span class="p">))</span>
<span class="p">)</span>
</code></pre></div>            </div>
        </div>

    </div>
</body>
</html>
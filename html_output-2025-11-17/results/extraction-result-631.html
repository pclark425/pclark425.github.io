<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-631 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-631</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-631</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269" target="_blank">Evaluating Large Language Models Trained on Code</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is found that repeated sampling from the GPT language model is a surprisingly effective strategy for producing working solutions to difficult prompts, and the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics are discussed.</p>
                <p><strong>Paper Abstract:</strong> We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e631.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e631.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pass@k & sampling variability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unbiased pass@k estimator and sampling-based evaluation for functional correctness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper identifies and quantifies stochastic variability in model-generated code via sampling-based evaluation, introduces (and implements) a numerically stable unbiased estimator for pass@k, and empirically studies how sampling parameters (n, k, temperature, sampling method) and selection heuristics affect measured performance and reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex family (Codex-12B, Codex-S-12B, smaller Codex variants), GPT-J-6B, GPT-Neo (various sizes), TabNine</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>12B (primary), other sizes reported: 679M, 2.5B, 300M, 6B, 2.7B, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>program synthesis / code generation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Generate Python functions from docstrings and evaluate functional correctness automatically using unit tests (functional correctness measured as pass@k).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Stochastic sampling (sampling randomness / random seed), temperature hyperparameter, sampling method (nucleus/top-p sampling with p=0.95), number of samples drawn (n) and number used for evaluation (k), model size and model variant (Codex vs Codex-S vs GPT-J vs GPT-Neo), prompt variations (docstring length and specification complexity), stateful/non-deterministic target problems, selection method for picking a single sample (oracle, mean token log-prob, sum log-prob, back-translation), and estimator choice (biased naive estimator vs unbiased estimator).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>pass@k (unbiased estimator computed from n samples and c correct samples), BLEU score distributions (for correct vs incorrect samples), probabilities / mean token log-prob of samples, dependence of pass@k on temperature and number of samples (k), and visual/bias comparison of estimators (Figure 13).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Evaluation uses n = 200 samples per task to compute unbiased pass@k (k <= 100). Reported examples: Codex-12B pass@1 = 28.81% and pass@100 = 72.31% (Table 1); generating 100 samples raises solved fraction (abstract reports 70.2% solved with 100 samples for the model variant described); for Codex-S-12B pass@100 reported up to 77.5% when selecting sample that passes unit tests; the sample with highest mean log-prob passes unit tests for 44.5% of problems (reported in text); optimal temperature depends on k (example: for a 679M model T* = 0.2 for pass@1 and T* = 0.8 for pass@100), and higher temperatures increase sample diversity and improve pass@k for larger k (Figure 5). BLEU distributions for correct vs incorrect solutions overlap substantially (Figure 8), showing BLEU is a noisy indicator of functional correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Unbiased pass@k estimator (pass@k = E[1 - C(n-c, k) / C(n, k)]) computed from n samples and c correct samples; numerical stable implementation provided; comparisons between naive estimator 1 - (1 - p_hat)^k and unbiased estimator to show bias; repeated verification / rerunning to filter nondeterministic tasks; comparisons across temperatures, sample counts, model sizes and selection heuristics (mean log-prob, back-translation).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Paper demonstrates the naive estimator 1-(1-p_hat)^k is biased (underestimates true pass@k) and that the unbiased estimator is required for fair comparison (Figure 13). Empirical reproducible statistics reported using n=200 samples per task produce pass@k values in Table 1 (e.g., Codex-12B pass@100 = 72.31%). Supervised fine-tuning (Codex-S) improves pass@1 by ~6.5 percentage points on average and pass@100 by ~15.1 percentage points across model sizes (section 4.5); choosing samples by mean log-prob gives 44.5% pass rate in one reported comparison vs random selection (lower) and oracle best (higher).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>High variance from stochastic sampling; biased estimators if naively computed from pass@1; sensitivity to temperature and sampling method; prompt sensitivity (performance degrades exponentially with docstring length/chain complexity); nondeterministic or stateful unit tests and environment-dependent behavior in some traced functions; inability to deterministically select correct sample without unit tests (oracle) in many deployment scenarios; dataset/training distribution mismatch; model non-determinism leading to different outputs across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use unbiased pass@k estimator with n >= k and compute estimator from counts of correct samples (numerically stable implementation provided); increase number of samples (e.g., 100, 200, 1000) to find correct solutions; tune sampling temperature for each target k (lower T for pass@1, higher T for large k); use nucleus (top-p) sampling with p = 0.95 as evaluation default; apply supervised fine-tuning on distribution-matched data (Codex-S) to reduce distribution mismatch; apply sample-ranking heuristics (choose sample with highest mean token log-prob, or back-translation ranking via docstring model); filter training/evaluation tasks by generating multiple samples and rerunning verification several times to remove stateful/non-deterministic problems; sandbox untrusted code execution to control environment and nondeterminism.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantitative improvements reported: generating many samples substantially increases pass rates (single-sample Codex-12B pass@1 = 28.81% → pass@100 = 72.31%); supervised fine-tuning (Codex-S) gives an average +6.5 percentage points on pass@1 and +15.1 points on pass@100 versus Codex; selecting the sample with highest mean log-prob yields 44.5% pass rate (better than random); mean log-prob ranking yields an average benefit over random ranking of 11.6 percentage points for Codex-S-12B (section 4.5); temperature tuning: example optimal temperatures reported (e.g., T*=0.2 for pass@1 and T*=0.8 for pass@100 for some models) and higher temperature improves pass@k for larger k (Figure 5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Core pass@k evaluation: n = 200 samples per task (k up to 100). Other reported sampling budgets: commonly 100 samples per problem (many analyses and examples), 1000 samples for APPS filtered evaluations, 10 samples per problem for manual docstring grading, 100 samples used when filtering curated training problems and rerun multiple times to remove nondeterministic problems.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Stochastic sampling introduces large variability in measured functional correctness, so evaluation must use many samples and an unbiased pass@k estimator (n >= k) to avoid bias; sampling temperature and method strongly affect measured pass@k (higher T improves pass@k when k is large by increasing diversity); selection heuristics (mean token log-prob) and supervised fine-tuning (Codex-S) materially improve the practical reproducibility of finding a correct sample; prompt complexity and nondeterministic/stateful tasks are significant reproducibility challenges that should be filtered or controlled.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models Trained on Code', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e631.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e631.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>prompt & stateful filtering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Filtering nondeterministic/stateful tasks via multi-sample verification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents that some curated functions are nondeterministic or stateful and describes a filtering procedure using multiple model samples and repeated verification to remove such tasks from training/evaluation data to improve reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex-12B (used as verifier in filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>12B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>program synthesis / dataset curation for ML experiments</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Automatically curate and filter training/evaluation problems from CI and competitive programming by generating samples and discarding problems where no generated sample passes unit tests or where behavior is nondeterministic across reruns.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Stateful functions, nondeterministic behavior in traced inputs/outputs, environment/dependency differences when running integration tests, sampling stochasticity (used to detect ambiguity if no sample passes).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Verification pass rate across 100 generated samples; repeated verification runs to detect nondeterminism; filtering when 0/100 samples pass.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Filtering pipeline: generate 100 samples per curated problem using Codex-12B; if no samples pass unit tests the task is removed (considered ambiguous or too difficult); rerun verification several times to remove stateful/non-deterministic problems. From this process they retained ~40,000 curated problems (out of many millions possible) for training Codex-S.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Use of repeated sampling and multiple verification runs to detect non-determinism and underspecification; removal of tasks with inconsistent outcomes across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Filtering reduced the inclusion of ambiguous or stateful tasks in supervised fine-tuning dataset, improving downstream Codex-S performance (Codex-S shows improved pass@k relative to Codex). No single scalar reproducibility rate reported for filtering step, but the curated set size (~40,000) and improved model metrics (see Codex-S improvements: +6.5 pp on pass@1 and +15.1 pp on pass@100) indicate practical benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Detecting stateful behaviors that only manifest under certain environment configurations; inability to pickle/restore many runtime-captured objects; hidden/partial test suites in source repositories; intermittent nondeterminism requiring multiple reruns.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Trace runtime inputs/outputs under CI, run integration tests in sandbox, generate 100 samples and require at least one pass to include a problem, rerun verification several times to remove nondeterministic tasks, use sandboxing (gVisor + eBPF firewall rules) to control environment and make execution results more reproducible.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Filtering enabled creation of a large, higher-quality supervised dataset (≈40k problems) and corresponded with the observed Codex-S performance gains (average +6.5 pp pass@1, +15.1 pp pass@100), indicating improved reliability of training/evaluation data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>100 samples per curated problem for initial filtering; verification rerun 'several times' (not precisely quantified) to remove nondeterministic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicitly filtering examples that cannot be reliably verified (using multi-sample generation and repeated verification in a controlled sandbox) reduces nondeterminism in training data and improves downstream reproducibility and performance of supervised fine-tuned models (Codex-S).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models Trained on Code', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e631.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e631.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>selection heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sample-ranking heuristics (mean token log-prob, back-translation) for single-sample selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Because in many deployments only one sample can be shown/used, the paper studies heuristics to choose a single best sample from k candidates without unit tests: ranking by mean token log-prob (best empirical) and back-translation via a docstring model (Codex-D) (less effective / overfits).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex-12B, Codex-S-12B, Codex-D (docstring model used for back-translation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>12B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>program synthesis / deployment heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>From k sampled completions (no access to unit tests), select one completion to present to user or downstream system using ranking heuristics and evaluate how well the heuristic correlates with actual functional correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Sampling randomness, temperature, scoring function (mean token log-prob vs sum log-prob vs back-translation score), model variant used for ranking, and sample diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Empirical pass rates of the single selected sample (e.g., percent of tasks where the chosen sample passes unit tests); comparison vs random selection and oracle selection.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Choosing the sample with highest mean token log-prob passes unit tests for 44.5% of problems (reported); mean log-prob ranking outperforms random selection and sum log-prob can perform worse than random. Back-translation ranking (maximize P(docstring | generated sample) using Codex-D) outperforms random ranking but underperforms mean log-prob ranking and appears to overfit quickly (Figure 7). For Codex-S-12B, mean log-prob ranking gives an average benefit over random ranking of 11.6 percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Empirical pass@1 of selected sample when selection is performed by each heuristic (measured at temperature 0.8 for many experiments); comparisons to random and oracle selection.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Mean log-prob ranking gives substantial gains in practical single-sample selection (e.g., 44.5% pass rate in one result) and consistent improvements across model variants (Codex-S shows ~11.6 pp benefit over random). Back-translation ranking is worse than mean log-prob but better than random in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>No access to unit tests in deployment means selection heuristics must be imperfect proxies; heuristics can overfit and their effectiveness depends on temperature and sample diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use mean token log-prob ranking as a simple, effective heuristic; tune temperature and sampling budget to produce candidate pool appropriate for ranking; train a docstring model (Codex-D) to enable back-translation ranking as an alternative heuristic.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Mean token log-prob ranking: reported 44.5% pass rate on HumanEval when selecting single sample (versus 28.8% pass@1 baseline for single random sample from Codex-12B), and an average +11.6 percentage point improvement over random ranking for Codex-S-12B. Back-translation ranking underperforms mean log-prob in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Experiments report k up to 100 samples; primary heuristic analyses shown at temperature 0.8 with k between 1 and 100 (figures and tables).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>When unit tests are unavailable, ranking candidate completions by mean token log-prob is an effective and practical method to increase the chance a single returned sample is functionally correct; back-translation ranking is useful but less effective and prone to overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models Trained on Code', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e631.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e631.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>temperature & sampling method effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of temperature and sampling diversity on pass@k</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper systematically studies how sampling temperature interacts with the number of generated samples k: lower temperature is better for single-sample accuracy (pass@1) while higher temperature increases diversity and improves pass@k for larger k.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex variants; evaluation examples include a 679M model and Codex-12B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>examples: 679M, 12B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>program synthesis / sampling hyperparameter analysis</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Measure pass@k across a grid of sampling temperatures and numbers of samples k to determine optimal temperature choices and quantify how temperature affects diversity and correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Sampling temperature (T), sampling method (nucleus / top-p), number of samples k, and model size.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>pass@k as a function of k and temperature; identification of optimal temperature T* for each k (Figure 5 and Figure 9).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Empirical finding: higher temperatures are optimal for larger k (increase sample diversity). Example: for a 679M model optimal temperature T* = 0.2 for pass@1 and T* = 0.8 for pass@100. For Codex-S they used T* = 0 for pass@1 and T* = 1 for pass@100. Temperature tuning yields substantial differences in pass@k curves (Figure 5).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Grid evaluations of pass@k across temperature values (e.g., 0.2, 0.4, 0.8, 1.0) and k values; plotting best temperature per k.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Clear, reproducible temperature-vs-k relationships observed across model sizes: optimal temperature increases with k; using the correct temperature for the evaluation regime materially improves pass@k.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Incorrect temperature choice can understate a model's capability for multi-sample evaluation; temperature interacts with sample-ranking heuristics and selection procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Tune temperature per target evaluation budget (k); use top-p (nucleus) sampling with p=0.95 as a stable default for generating candidate pools.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Selecting optimal temperature per k significantly improves pass@k (figures show substantial relative gains across k), e.g., choosing T=0.8 vs T=0.2 for high-k evaluations yields markedly higher pass@k due to increased diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Pass@k grids evaluated with many samples (n=200 used for unbiased estimation) across temperature settings; optimal-temperature hull computed from these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Temperature must be chosen according to the intended usage regime: low temperature favors single-sample accuracy while higher temperature increases diversity and improves multi-sample success probability; evaluations that do not tune temperature for k can mischaracterize model capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models Trained on Code', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SPoC: Search-based pseudocode to code <em>(Rating: 2)</em></li>
                <li>The curious case of neural text degeneration <em>(Rating: 2)</em></li>
                <li>Unsupervised translation of programming languages <em>(Rating: 2)</em></li>
                <li>RobustFill: Neural program learning under noisy I/O <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-631",
    "paper_id": "paper-acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "pass@k & sampling variability",
            "name_full": "Unbiased pass@k estimator and sampling-based evaluation for functional correctness",
            "brief_description": "The paper identifies and quantifies stochastic variability in model-generated code via sampling-based evaluation, introduces (and implements) a numerically stable unbiased estimator for pass@k, and empirically studies how sampling parameters (n, k, temperature, sampling method) and selection heuristics affect measured performance and reproducibility.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Codex family (Codex-12B, Codex-S-12B, smaller Codex variants), GPT-J-6B, GPT-Neo (various sizes), TabNine",
            "model_size": "12B (primary), other sizes reported: 679M, 2.5B, 300M, 6B, 2.7B, etc.",
            "scientific_domain": "program synthesis / code generation",
            "experimental_task": "Generate Python functions from docstrings and evaluate functional correctness automatically using unit tests (functional correctness measured as pass@k).",
            "variability_sources": "Stochastic sampling (sampling randomness / random seed), temperature hyperparameter, sampling method (nucleus/top-p sampling with p=0.95), number of samples drawn (n) and number used for evaluation (k), model size and model variant (Codex vs Codex-S vs GPT-J vs GPT-Neo), prompt variations (docstring length and specification complexity), stateful/non-deterministic target problems, selection method for picking a single sample (oracle, mean token log-prob, sum log-prob, back-translation), and estimator choice (biased naive estimator vs unbiased estimator).",
            "variability_measured": true,
            "variability_metrics": "pass@k (unbiased estimator computed from n samples and c correct samples), BLEU score distributions (for correct vs incorrect samples), probabilities / mean token log-prob of samples, dependence of pass@k on temperature and number of samples (k), and visual/bias comparison of estimators (Figure 13).",
            "variability_results": "Evaluation uses n = 200 samples per task to compute unbiased pass@k (k &lt;= 100). Reported examples: Codex-12B pass@1 = 28.81% and pass@100 = 72.31% (Table 1); generating 100 samples raises solved fraction (abstract reports 70.2% solved with 100 samples for the model variant described); for Codex-S-12B pass@100 reported up to 77.5% when selecting sample that passes unit tests; the sample with highest mean log-prob passes unit tests for 44.5% of problems (reported in text); optimal temperature depends on k (example: for a 679M model T* = 0.2 for pass@1 and T* = 0.8 for pass@100), and higher temperatures increase sample diversity and improve pass@k for larger k (Figure 5). BLEU distributions for correct vs incorrect solutions overlap substantially (Figure 8), showing BLEU is a noisy indicator of functional correctness.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Unbiased pass@k estimator (pass@k = E[1 - C(n-c, k) / C(n, k)]) computed from n samples and c correct samples; numerical stable implementation provided; comparisons between naive estimator 1 - (1 - p_hat)^k and unbiased estimator to show bias; repeated verification / rerunning to filter nondeterministic tasks; comparisons across temperatures, sample counts, model sizes and selection heuristics (mean log-prob, back-translation).",
            "reproducibility_results": "Paper demonstrates the naive estimator 1-(1-p_hat)^k is biased (underestimates true pass@k) and that the unbiased estimator is required for fair comparison (Figure 13). Empirical reproducible statistics reported using n=200 samples per task produce pass@k values in Table 1 (e.g., Codex-12B pass@100 = 72.31%). Supervised fine-tuning (Codex-S) improves pass@1 by ~6.5 percentage points on average and pass@100 by ~15.1 percentage points across model sizes (section 4.5); choosing samples by mean log-prob gives 44.5% pass rate in one reported comparison vs random selection (lower) and oracle best (higher).",
            "reproducibility_challenges": "High variance from stochastic sampling; biased estimators if naively computed from pass@1; sensitivity to temperature and sampling method; prompt sensitivity (performance degrades exponentially with docstring length/chain complexity); nondeterministic or stateful unit tests and environment-dependent behavior in some traced functions; inability to deterministically select correct sample without unit tests (oracle) in many deployment scenarios; dataset/training distribution mismatch; model non-determinism leading to different outputs across runs.",
            "mitigation_methods": "Use unbiased pass@k estimator with n &gt;= k and compute estimator from counts of correct samples (numerically stable implementation provided); increase number of samples (e.g., 100, 200, 1000) to find correct solutions; tune sampling temperature for each target k (lower T for pass@1, higher T for large k); use nucleus (top-p) sampling with p = 0.95 as evaluation default; apply supervised fine-tuning on distribution-matched data (Codex-S) to reduce distribution mismatch; apply sample-ranking heuristics (choose sample with highest mean token log-prob, or back-translation ranking via docstring model); filter training/evaluation tasks by generating multiple samples and rerunning verification several times to remove stateful/non-deterministic problems; sandbox untrusted code execution to control environment and nondeterminism.",
            "mitigation_effectiveness": "Quantitative improvements reported: generating many samples substantially increases pass rates (single-sample Codex-12B pass@1 = 28.81% → pass@100 = 72.31%); supervised fine-tuning (Codex-S) gives an average +6.5 percentage points on pass@1 and +15.1 points on pass@100 versus Codex; selecting the sample with highest mean log-prob yields 44.5% pass rate (better than random); mean log-prob ranking yields an average benefit over random ranking of 11.6 percentage points for Codex-S-12B (section 4.5); temperature tuning: example optimal temperatures reported (e.g., T*=0.2 for pass@1 and T*=0.8 for pass@100 for some models) and higher temperature improves pass@k for larger k (Figure 5).",
            "comparison_with_without_controls": true,
            "number_of_runs": "Core pass@k evaluation: n = 200 samples per task (k up to 100). Other reported sampling budgets: commonly 100 samples per problem (many analyses and examples), 1000 samples for APPS filtered evaluations, 10 samples per problem for manual docstring grading, 100 samples used when filtering curated training problems and rerun multiple times to remove nondeterministic problems.",
            "key_findings": "Stochastic sampling introduces large variability in measured functional correctness, so evaluation must use many samples and an unbiased pass@k estimator (n &gt;= k) to avoid bias; sampling temperature and method strongly affect measured pass@k (higher T improves pass@k when k is large by increasing diversity); selection heuristics (mean token log-prob) and supervised fine-tuning (Codex-S) materially improve the practical reproducibility of finding a correct sample; prompt complexity and nondeterministic/stateful tasks are significant reproducibility challenges that should be filtered or controlled.",
            "uuid": "e631.0",
            "source_info": {
                "paper_title": "Evaluating Large Language Models Trained on Code",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "prompt & stateful filtering",
            "name_full": "Filtering nondeterministic/stateful tasks via multi-sample verification",
            "brief_description": "The paper documents that some curated functions are nondeterministic or stateful and describes a filtering procedure using multiple model samples and repeated verification to remove such tasks from training/evaluation data to improve reproducibility.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Codex-12B (used as verifier in filtering)",
            "model_size": "12B",
            "scientific_domain": "program synthesis / dataset curation for ML experiments",
            "experimental_task": "Automatically curate and filter training/evaluation problems from CI and competitive programming by generating samples and discarding problems where no generated sample passes unit tests or where behavior is nondeterministic across reruns.",
            "variability_sources": "Stateful functions, nondeterministic behavior in traced inputs/outputs, environment/dependency differences when running integration tests, sampling stochasticity (used to detect ambiguity if no sample passes).",
            "variability_measured": true,
            "variability_metrics": "Verification pass rate across 100 generated samples; repeated verification runs to detect nondeterminism; filtering when 0/100 samples pass.",
            "variability_results": "Filtering pipeline: generate 100 samples per curated problem using Codex-12B; if no samples pass unit tests the task is removed (considered ambiguous or too difficult); rerun verification several times to remove stateful/non-deterministic problems. From this process they retained ~40,000 curated problems (out of many millions possible) for training Codex-S.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Use of repeated sampling and multiple verification runs to detect non-determinism and underspecification; removal of tasks with inconsistent outcomes across runs.",
            "reproducibility_results": "Filtering reduced the inclusion of ambiguous or stateful tasks in supervised fine-tuning dataset, improving downstream Codex-S performance (Codex-S shows improved pass@k relative to Codex). No single scalar reproducibility rate reported for filtering step, but the curated set size (~40,000) and improved model metrics (see Codex-S improvements: +6.5 pp on pass@1 and +15.1 pp on pass@100) indicate practical benefit.",
            "reproducibility_challenges": "Detecting stateful behaviors that only manifest under certain environment configurations; inability to pickle/restore many runtime-captured objects; hidden/partial test suites in source repositories; intermittent nondeterminism requiring multiple reruns.",
            "mitigation_methods": "Trace runtime inputs/outputs under CI, run integration tests in sandbox, generate 100 samples and require at least one pass to include a problem, rerun verification several times to remove nondeterministic tasks, use sandboxing (gVisor + eBPF firewall rules) to control environment and make execution results more reproducible.",
            "mitigation_effectiveness": "Filtering enabled creation of a large, higher-quality supervised dataset (≈40k problems) and corresponded with the observed Codex-S performance gains (average +6.5 pp pass@1, +15.1 pp pass@100), indicating improved reliability of training/evaluation data.",
            "comparison_with_without_controls": true,
            "number_of_runs": "100 samples per curated problem for initial filtering; verification rerun 'several times' (not precisely quantified) to remove nondeterministic tasks.",
            "key_findings": "Explicitly filtering examples that cannot be reliably verified (using multi-sample generation and repeated verification in a controlled sandbox) reduces nondeterminism in training data and improves downstream reproducibility and performance of supervised fine-tuned models (Codex-S).",
            "uuid": "e631.1",
            "source_info": {
                "paper_title": "Evaluating Large Language Models Trained on Code",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "selection heuristics",
            "name_full": "Sample-ranking heuristics (mean token log-prob, back-translation) for single-sample selection",
            "brief_description": "Because in many deployments only one sample can be shown/used, the paper studies heuristics to choose a single best sample from k candidates without unit tests: ranking by mean token log-prob (best empirical) and back-translation via a docstring model (Codex-D) (less effective / overfits).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Codex-12B, Codex-S-12B, Codex-D (docstring model used for back-translation)",
            "model_size": "12B",
            "scientific_domain": "program synthesis / deployment heuristics",
            "experimental_task": "From k sampled completions (no access to unit tests), select one completion to present to user or downstream system using ranking heuristics and evaluate how well the heuristic correlates with actual functional correctness.",
            "variability_sources": "Sampling randomness, temperature, scoring function (mean token log-prob vs sum log-prob vs back-translation score), model variant used for ranking, and sample diversity.",
            "variability_measured": true,
            "variability_metrics": "Empirical pass rates of the single selected sample (e.g., percent of tasks where the chosen sample passes unit tests); comparison vs random selection and oracle selection.",
            "variability_results": "Choosing the sample with highest mean token log-prob passes unit tests for 44.5% of problems (reported); mean log-prob ranking outperforms random selection and sum log-prob can perform worse than random. Back-translation ranking (maximize P(docstring | generated sample) using Codex-D) outperforms random ranking but underperforms mean log-prob ranking and appears to overfit quickly (Figure 7). For Codex-S-12B, mean log-prob ranking gives an average benefit over random ranking of 11.6 percentage points.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Empirical pass@1 of selected sample when selection is performed by each heuristic (measured at temperature 0.8 for many experiments); comparisons to random and oracle selection.",
            "reproducibility_results": "Mean log-prob ranking gives substantial gains in practical single-sample selection (e.g., 44.5% pass rate in one result) and consistent improvements across model variants (Codex-S shows ~11.6 pp benefit over random). Back-translation ranking is worse than mean log-prob but better than random in some settings.",
            "reproducibility_challenges": "No access to unit tests in deployment means selection heuristics must be imperfect proxies; heuristics can overfit and their effectiveness depends on temperature and sample diversity.",
            "mitigation_methods": "Use mean token log-prob ranking as a simple, effective heuristic; tune temperature and sampling budget to produce candidate pool appropriate for ranking; train a docstring model (Codex-D) to enable back-translation ranking as an alternative heuristic.",
            "mitigation_effectiveness": "Mean token log-prob ranking: reported 44.5% pass rate on HumanEval when selecting single sample (versus 28.8% pass@1 baseline for single random sample from Codex-12B), and an average +11.6 percentage point improvement over random ranking for Codex-S-12B. Back-translation ranking underperforms mean log-prob in experiments.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Experiments report k up to 100 samples; primary heuristic analyses shown at temperature 0.8 with k between 1 and 100 (figures and tables).",
            "key_findings": "When unit tests are unavailable, ranking candidate completions by mean token log-prob is an effective and practical method to increase the chance a single returned sample is functionally correct; back-translation ranking is useful but less effective and prone to overfitting.",
            "uuid": "e631.2",
            "source_info": {
                "paper_title": "Evaluating Large Language Models Trained on Code",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "temperature & sampling method effects",
            "name_full": "Effect of temperature and sampling diversity on pass@k",
            "brief_description": "The paper systematically studies how sampling temperature interacts with the number of generated samples k: lower temperature is better for single-sample accuracy (pass@1) while higher temperature increases diversity and improves pass@k for larger k.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Codex variants; evaluation examples include a 679M model and Codex-12B",
            "model_size": "examples: 679M, 12B",
            "scientific_domain": "program synthesis / sampling hyperparameter analysis",
            "experimental_task": "Measure pass@k across a grid of sampling temperatures and numbers of samples k to determine optimal temperature choices and quantify how temperature affects diversity and correctness.",
            "variability_sources": "Sampling temperature (T), sampling method (nucleus / top-p), number of samples k, and model size.",
            "variability_measured": true,
            "variability_metrics": "pass@k as a function of k and temperature; identification of optimal temperature T* for each k (Figure 5 and Figure 9).",
            "variability_results": "Empirical finding: higher temperatures are optimal for larger k (increase sample diversity). Example: for a 679M model optimal temperature T* = 0.2 for pass@1 and T* = 0.8 for pass@100. For Codex-S they used T* = 0 for pass@1 and T* = 1 for pass@100. Temperature tuning yields substantial differences in pass@k curves (Figure 5).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Grid evaluations of pass@k across temperature values (e.g., 0.2, 0.4, 0.8, 1.0) and k values; plotting best temperature per k.",
            "reproducibility_results": "Clear, reproducible temperature-vs-k relationships observed across model sizes: optimal temperature increases with k; using the correct temperature for the evaluation regime materially improves pass@k.",
            "reproducibility_challenges": "Incorrect temperature choice can understate a model's capability for multi-sample evaluation; temperature interacts with sample-ranking heuristics and selection procedures.",
            "mitigation_methods": "Tune temperature per target evaluation budget (k); use top-p (nucleus) sampling with p=0.95 as a stable default for generating candidate pools.",
            "mitigation_effectiveness": "Selecting optimal temperature per k significantly improves pass@k (figures show substantial relative gains across k), e.g., choosing T=0.8 vs T=0.2 for high-k evaluations yields markedly higher pass@k due to increased diversity.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Pass@k grids evaluated with many samples (n=200 used for unbiased estimation) across temperature settings; optimal-temperature hull computed from these experiments.",
            "key_findings": "Temperature must be chosen according to the intended usage regime: low temperature favors single-sample accuracy while higher temperature increases diversity and improves multi-sample success probability; evaluations that do not tune temperature for k can mischaracterize model capabilities.",
            "uuid": "e631.3",
            "source_info": {
                "paper_title": "Evaluating Large Language Models Trained on Code",
                "publication_date_yy_mm": "2021-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SPoC: Search-based pseudocode to code",
            "rating": 2
        },
        {
            "paper_title": "The curious case of neural text degeneration",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised translation of programming languages",
            "rating": 2
        },
        {
            "paper_title": "RobustFill: Neural program learning under noisy I/O",
            "rating": 1
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        }
    ],
    "cost": 0.020781749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Evaluating Large Language Models Trained on Code</h1>
<p>Mark Chen<em>1 Jerry Tworek</em>1 Heewoo Jun<em>1 Qiming Yuan</em>1 Henrique Ponde de Oliveira Pinto<em>1<br>Jared Kaplan</em>2 Harri Edwards ${ }^{1}$ Yuri Burda ${ }^{1}$ Nicholas Joseph ${ }^{2}$ Greg Brockman ${ }^{1}$ Alex Ray ${ }^{1}$ Raul Puri ${ }^{1}$<br>Gretchen Krueger ${ }^{1}$ Michael Petrov ${ }^{1}$ Heidy Khlaaf ${ }^{3}$ Girish Sastry ${ }^{1}$ Pamela Mishkin ${ }^{1}$ Brooke Chan ${ }^{1}$<br>Scott Gray ${ }^{1}$ Nick Ryder ${ }^{1}$ Mikhail Pavlov ${ }^{1}$ Alethea Power ${ }^{1}$ Lukasz Kaiser ${ }^{1}$ Mohammad Bavarian ${ }^{1}$<br>Clemens Winter ${ }^{1}$ Philippe Tillet ${ }^{1}$ Felipe Petroski Such ${ }^{1}$ Dave Cummings ${ }^{1}$ Matthias Plappert ${ }^{1}$<br>Fotios Chantzis ${ }^{1}$ Elizabeth Barnes ${ }^{1}$ Ariel Herbert-Voss ${ }^{1}$ William Hebgen Guss ${ }^{1}$ Alex Nichol ${ }^{1}$ Alex Paino ${ }^{1}$<br>Nikolas Tezak ${ }^{1}$ Jie Tang ${ }^{1}$ Igor Babuschkin ${ }^{1}$ Suchir Balaji ${ }^{1}$ Shantanu Jain ${ }^{1}$ William Saunders ${ }^{1}$<br>Christopher Hesse ${ }^{1}$ Andrew N. Carr ${ }^{1}$ Jan Leike ${ }^{1}$ Josh Achiam ${ }^{1}$ Vedant Misra ${ }^{1}$ Evan Morikawa ${ }^{1}$<br>Alec Radford ${ }^{1}$ Matthew Knight ${ }^{1}$ Miles Brundage ${ }^{1}$ Mira Murati ${ }^{1}$ Katie Mayer ${ }^{1}$ Peter Welinder ${ }^{1}$<br>Bob McGrew ${ }^{1}$ Dario Amodei ${ }^{2}$ Sam McCandlish ${ }^{2}$ Ilya Sutskever ${ }^{1}$ Wojciech Zaremba ${ }^{1}$</p>
<h2>Abstract</h2>
<p>We introduce Codex, a GPT language model finetuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves $28.8 \%$ of the problems, while GPT-3 solves $0 \%$ and GPT-J solves $11.4 \%$. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve $70.2 \%$ of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>1. Introduction</h2>
<p>Scalable sequence prediction models (Graves, 2014; Vaswani et al., 2017; Child et al., 2019) have become a general-purpose method for generation and representation learning in many domains, including natural language processing (Mikolov et al., 2013; Sutskever et al., 2014; Dai \&amp; Le, 2015; Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), computer vision (Van Oord et al., 2016; Menick \&amp; Kalchbrenner, 2018; Chen et al., 2020; Bao et al., 2021), audio and speech processing (Oord et al., 2016; 2018; Dhariwal et al., 2020; Baevski et al., 2020), biology (Alley et al., 2019; Rives et al., 2021), and even across multiple modalities (Das et al., 2017; Lu et al., 2019; Ramesh et al., 2021; Zellers et al., 2021). More recently, language models have also fueled progress towards the longstanding challenge of program synthesis (Simon, 1963; Manna \&amp; Waldinger, 1971), spurred by the presence of code in large datasets (Husain et al., 2019; Gao et al., 2020) and the resulting programming capabilities of language models trained on these datasets (Wang \&amp; Komatsuzaki, 2021). Popular language modeling objectives like masked language modeling (Devlin et al., 2018) and span prediction (Raffel et al., 2020) have also been adapted to train their programming counterparts CodeBERT (Feng et al., 2020) and PyMT5 (Clement et al., 2020).</p>
<p>Similarly, our early investigation of GPT-3 (Brown et al., 2020) revealed that it could generate simple programs from Python docstrings. While rudimentary, this capability was exciting because GPT-3 was not explicitly trained for code generation. Given the considerable success of large language models in other modalities and the abundance of publicly available code, we hypothesized that a specialized GPT model, called Codex, could excel at a variety of coding tasks. This paper describes several early Codex models, whose descendants power GitHub Copilot and the Codex models in the OpenAI API.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Pass rates of our models on the HumanEval dataset as a function of model size. When a single sample is generated for each problem, GPT-12B solves no problems, but Codex (fine-tuned on code) solves $28.8 \%$ of the problems, and Codex-S (further fine-tuned on correctly implemented standalone functions) solves $37.7 \%$ of the problems. From here, further gains can be realized by generating 100 samples per problem and selecting the sample with the highest mean log-probability ( $44.5 \%$ solved) or by selecting the sample that passes the unit tests ( $77.5 \%$ solved). All samples are generated with temperature 0.8 .</p>
<p>In this work, we focus on the task of generating standalone Python functions from docstrings, and evaluate the correctness of code samples automatically through unit tests. This is in contrast to natural language generation, where samples are typically evaluated by heuristics or by human evaluators. To accurately benchmark our model, we create a dataset of 164 original programming problems with unit tests. These problems assess language comprehension, algorithms, and simple mathematics, with some comparable to simple software interview questions. We release this data along with an evaluation framework at https://www.github.com/openai/human-eval.</p>
<p>To solve a problem in our test set, we generate multiple samples from the models, and check if any of them pass the unit tests. With just a single sample, a 12B parameter Codex solves $28.8 \%$ of these problems, and a 300 M parameter Codex solves $13.2 \%$ of these problems. In contrast, the 6B parameter GPT-J (Wang \&amp; Komatsuzaki, 2021) achieves $11.4 \%$ on the same dataset, while all GPT models achieve near $0 \%$. To improve our model's performance at the task of function synthesis from docstrings, we fine-tune Codex on standalone, correctly implemented functions. The resulting model, Codex-S, solves $37.7 \%$ of problems with a single sample. Figure 2 showcases problems of varying difficulty in our dataset, along with correct model generated solutions.</p>
<p>Real-world programming tasks often involve iterations of approaches and bug fixes, which is approximated by generating many samples from our models and selecting one that passes all unit tests. Within 100 samples, Codex-S is able to
generate at least one correct function for $77.5 \%$ of the problems. This result suggests that accurate code samples can be selected via heuristic ranking instead of fully evaluating each sample, the latter of which may not be possible or practical in deployment. Indeed, we find that the sample with highest mean log-probability passes unit tests for $44.5 \%$ of the problems.</p>
<p>We conclude by discussing the limitations and potential broader impacts of these Codex models and of increasingly powerful code generating models more generally.</p>
<h2>2. Evaluation Framework</h2>
<p>In this section, we discuss the details of our evaluation framework. We begin by defining the pass@ $k$ metric, and explain its advantages over standard match-based metrics. Next, we describe the dataset of hand-written problems, called "HumanEval," which we created in order to benchmark our models. Finally, we discuss the sandbox environment we used to safely execute model-generated code.</p>
<h3>2.1. Functional Correctness</h3>
<p>Generative models for code are predominantly benchmarked by matching samples against a reference solution, where the match can be exact or fuzzy (as in BLEU score). However, recent work has surfaced deficiencies in match-based metrics for code. For instance, Ren et al. (2020) finds that BLEU has problems capturing semantic features specific to code, and suggests several semantic modifications to the score.</p>
<p>More fundamentally, match-based metrics are unable to account for the large and complex space of programs functionally equivalent to a reference solution. As a consequence, recent works in unsupervised code translation (Lachaux et al., 2020) and pseudocode-to-code translation (Kulal et al., 2019) have turned to functional correctness instead, where a sample is considered correct if it passes a set of unit tests. We argue that this metric should be applied to docstringconditional code generation as well.</p>
<p>Perhaps the most convincing reason to evaluate functional correctness is that it is used by human developers to judge code. A framework known as test-driven development dictates that software requirements be converted into test cases before any implementation begins, and success is defined by a program that passes these tests. While few organizations employ full test-driven development, integration of new code is usually dependent on creating and passing unit tests.</p>
<p>Kulal et al. (2019) evaluate functional correctness using the pass@ $k$ metric, where $k$ code samples are generated per problem, a problem is considered solved if any sample</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Three example problems from the HumanEval dataset, where the probabilities that a single sample from Codex-12B passes unit tests are $0.9,0.17$, and 0.005 . The prompt provided to the model is shown with a white background, and a successful model-generated completion is shown in a yellow background. Though not a guarantee for problem novelty, all problems were hand-written and not programmatically copied from existing sources. Random problems and samples can be found in Appendix B.
passes the unit tests, and the total fraction of problems solved is reported. However, computing pass@ $k$ in this way can have high variance. Instead, to evaluate pass@ $k$, we generate $n \geq k$ samples per task (in this paper, we use $n=200$ and $k \leq 100$ ), count the number of correct samples $c \leq n$ which pass unit tests, and calculate the unbiased estimator</p>
<p>$$
\operatorname{pass} @ k:=\underset{\text { Problems }}{\mathbb{E}}\left[1-\frac{\binom{n-c}{k}}{\binom{n}{k}}\right]
$$</p>
<p>Calculating this estimator directly results in very large numbers and numerical instability. In Figure 3, we include a numerically stable numpy implementation that simplifies the expression and evaluates the product term-by-term. One may be tempted to estimate pass@ $k$ with $1-(1-\hat{p})^{k}$ where $\hat{p}$ is the empirical estimate of pass@1, but we show that it is biased in Appendix A.</p>
<div class="codehilite"><pre><span></span><code><span class="nv">def</span><span class="w"> </span><span class="nf">pass_at_k</span><span class="p">(</span><span class="nv">n</span><span class="p">,</span><span class="w"> </span><span class="nv">c</span><span class="p">,</span><span class="w"> </span><span class="nv">k</span><span class="p">)</span><span class="o">:</span>
<span class="w">    </span><span class="o">***</span>
<span class="w">    </span><span class="o">:</span><span class="nv">param</span><span class="w"> </span><span class="nv">n</span><span class="o">:</span><span class="w"> </span><span class="k">to</span><span class="nv">tal</span><span class="w"> </span><span class="nv">number</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">samples</span>
<span class="w">    </span><span class="o">:</span><span class="nv">param</span><span class="w"> </span><span class="nv">c</span><span class="o">:</span><span class="w"> </span><span class="nv">number</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">correct</span><span class="w"> </span><span class="nv">samples</span>
<span class="w">    </span><span class="o">:</span><span class="nv">param</span><span class="w"> </span><span class="nv">k</span><span class="o">:</span><span class="w"> </span><span class="nv">k</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">pass</span><span class="o">@</span><span class="p">$</span><span class="nv">k</span><span class="p">$</span>
<span class="w">    </span><span class="s">&quot;n&quot;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nv">n</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">c</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="nv">k</span><span class="o">:</span><span class="w"> </span><span class="nv">return</span><span class="w"> </span><span class="mf">1.0</span>
<span class="w">    </span><span class="nv">return</span><span class="w"> </span><span class="mf">1.0</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">np</span><span class="o">.</span><span class="nf">prod</span><span class="p">(</span><span class="mf">1.0</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">k</span><span class="w"> </span><span class="o">/</span>
<span class="w">        </span><span class="nv">np</span><span class="o">.</span><span class="nf">arange</span><span class="p">(</span><span class="nv">n</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">c</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nv">n</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">))</span>
</code></pre></div>

<p>Figure 3. A numerically stable script for calculating an unbiased estimate of pass@ $k$.</p>
<p>Later, we provide evidence that BLEU score may not be a reliable indicator of functional correctness by showing that functionally inequivalent programs generated by our model (which are guaranteed to disagree with the reference solution on some input) often have higher BLEU scores than functionally equivalent ones.</p>
<h3>2.2. HumanEval: Hand-Written Evaluation Set</h3>
<p>We evaluate functional correctness on a set of 164 handwritten programming problems, which we call the HumanEval dataset. Each problem includes a function signature, docstring, body, and several unit tests, with an average of 7.7 tests per problem. It is important for these tasks to be hand-written, since our models are trained on a large fraction of GitHub, which already contains solutions to problems from a variety of sources. For example, there are more than ten public repositories containing solutions to Codeforces problems, which make up part of the recently proposed APPS dataset (Hendrycks et al., 2021).</p>
<p>Programming tasks in the HumanEval dataset assess language comprehension, reasoning, algorithms, and simple mathematics. We release the HumanEval dataset so that others can evaluate functional correctness and measure the problem-solving capabilities of their models. The dataset can be found at https://www.github.com/openai/human-eval.</p>
<h3>2.3. Sandbox for Executing Generated Programs</h3>
<p>Since publicly available programs have unknown intent and generated programs are often incorrect, executing these programs poses a security risk. Indeed, GitHub is known to contain malicious programs that alter or change their environments (Rokon et al., 2020).</p>
<p>Therefore, we developed a sandbox environment to safely run untrusted programs against unit tests. Our goals were to prevent these programs from modifying, gaining persistence on, accessing sensitive resources on, or exfiltrating data from a host or network. Since OpenAI's training infrastructure is built on Kubernetes and cloud services, we designed our sandbox to address the limitations of these environments while remaining idiomatic with their patterns of use.</p>
<p>We selected the gVisor container runtime (Lacasse, 2018) as the main host protection component. Since container runtimes like Docker can share host resources with containers, a malicious container could potentially compromise a host. gVisor protects the host by emulating its resources to introduce a security boundary between the host and its containers. Network-adjacent hosts and services are protected by eBPF-based firewall rules that prevent inbound and outbound connections except for those required for experiment control.</p>
<h2>3. Code Fine-Tuning</h2>
<p>We fine-tune GPT models containing up to 12B parameters on code to produce Codex. In contrast with GPT, Codex displays non-trivial performance on the HumanEval dataset. In fact, Codex is able to solve the majority of the problems in HumanEval if we generate and evaluate 100 samples per
problem, and pick one that passes unit tests. When limited to a budget of one evaluation per problem, producing multiple samples with Codex and choosing the one with the highest mean log-probability provides significant gains.</p>
<h3>3.1. Data Collection</h3>
<p>Our training dataset was collected in May 2020 from 54 million public software repositories hosted on GitHub, containing 179 GB of unique Python files under 1 MB . We filtered out files which were likely auto-generated, had average line length greater than 100, had maximum line length greater than 1000, or contained a small percentage of alphanumeric characters. After filtering, our final dataset totaled 159 GB.</p>
<h3>3.2. Methods</h3>
<p>Since Codex is evaluated on natural language prompts, we hypothesized that it would be beneficial to fine-tune from the GPT-3 (Brown et al., 2020) model family, which already contains strong natural language representations. Surprisingly, we did not observe improvements when starting from a pre-trained language model, possibly because the finetuning dataset is so large. Nevertheless, models fine-tuned from GPT converge more quickly, so we apply this strategy for all subsequent experiments.</p>
<p>We train Codex using the same learning rate as the corresponding GPT model, with a 175 step linear warmup and cosine learning rate decay. We train for a total of 100 billion tokens, using the Adam optimizer with $\beta_{1}=0.9, \beta_{2}=0.95$, $\epsilon=10^{-8}$, and a weight decay coefficient of 0.1 .</p>
<p>In order to maximally leverage text representations from GPT, we base our code lexer on the GPT-3 text tokenizer. Since the distribution of words in GitHub code differs from that of natural text, this tokenizer is not very effective for representing code. The largest source of inefficiency arises from encoding whitespace, so we add an additional set of tokens for representing whitespace runs of different lengths. This allows us to represent code using approximately $30 \%$ fewer tokens.</p>
<p>To compute pass@ $k$, we assemble each HumanEval problem into a prompt consisting of a header, a signature, and a docstring, which is illustrated in Figure 2. We sample tokens from Codex until we encounter one of the following stop sequences: ' $\backslash$ nclass', ' $\backslash n d e f$ ', ' $\backslash n #$ ', ' $\backslash n i f$ ', or ' $\backslash$ nprint', since the model will continue generating additional functions or statements otherwise. We use nucleus sampling (Holtzman et al., 2020) with top $p=0.95$ for all sampling evaluation in this work.</p>
<h3>3.3. Results</h3>
<p>In Figure 4, we plot test loss on a held-out validation set against Codex model size. We find that just as language</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4. Model cross-entropy test loss measured on a held-out split of our Python GitHub code corpus. The smooth power law scaling of performance with model size observed in GPT-3 appears to hold even after code fine-tuning.
model test loss follows a power law in model size (Kaplan et al., 2020), test loss after code fine-tuning follows a similar power law with functional form $\left(\frac{N}{5.92 \times 10^{7}}\right)^{-0.13}$ where $N$ is the number of non-embedding parameters in the model.</p>
<p>When evaluating pass@ $k$, it is important to optimize sampling temperature for the particular value of $k$. In Figure 5, we plot pass@ $k$ against the number of samples $k$ and the sampling temperature. We find that higher temperatures are optimal for larger $k$, because the resulting set of samples has higher diversity, and the metric rewards only whether the model generates any correct solution.</p>
<p>In particular, for a 679 M parameter model, the optimal temperature for pass@1 is $T^{<em>}=0.2$ and the optimal temperature for pass@100 is $T^{</em>}=0.8$. With these temperatures, we find that pass@1 and pass@100 scale smoothly as a function of model size (Figure 6).</p>
<p>Pass@ $k$ can also be interpreted as the result of evaluating the best out of $k$ samples, where the best sample is picked by an oracle with prior knowledge of the unit tests. From a practical perspective, we are also interested in the setting where we must select a single sample from $k$ samples without having access to an oracle. For instance, when the model is used as an autocomplete tool where a user provides a prompt, we do not have unit tests, but would like to return only a single completion to the user for evaluation so as to not overwhelm them.</p>
<p>Inspired by similar work in language modeling, we find that choosing the sample with the highest mean token log probability outperforms evaluating a random sample, while choosing the sample based on sum log probability can perform slightly worse than picking randomly. Figure 7 demonstrates the benefits of applying these heuristics to samples (at temperature 0.8) from Codex-12B.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5. In the top panel, we plot pass@ $k$ against the number of samples $(k)$ for various temperature settings. Higher temperatures are better when the number of samples is large, likely due to the increased sample diversity. In the bottom panel, we plot the best temperature setting for each $k$, obtained by taking the upper hull of the top panel.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6. Using the optimal temperatures 0.2 and 0.8 for pass@1 and pass@100, we plot these two metrics as a function of model size. Performance appears to scale smoothly as a sigmoid in logparameters.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7. Model performance in the setting where we can generate multiple samples, but only evaluate one. We can do better than randomly selecting a sample by choosing the solution with the highest mean log-probability (red) or with the highest back-translation score (orange) described in Sec. 5. The blue line represents the theoretical best performance obtained using an oracle with prior knowledge of the unit tests.</p>
<p>Finally, we compute BLEU scores for all Codex-12B Hu manEval samples (at temperature 0.8) against their reference solutions. For each problem, when we plot the distributions of BLEU scores for correct and incorrect solutions, we notice significant overlap (Figure 8). Since an incorrect solution is guaranteed to be functionally inequivalent to the reference solution, we conclude that improvements in BLEU score may not indicate improved rates of functional correctness in practice.</p>
<h3>3.4. Comparative Analysis of Related Models and Systems</h3>
<p>Two recent works similar in spirit to Codex are GPT-Neo (Black et al., 2021) and GPT-J (Wang &amp; Komatsuzaki, 2021), which are trained on The Pile (Gao et al., 2020), a dataset containing text from a variety of sources as well as 8% GitHub code. The broader research community has found that these models outperform existing GPT systems in qualitative programming evaluations (Woolf, 2021).</p>
<p>We confirm these findings using the HumanEval dataset, showing that GPT-Neo achieves 6.4% pass@1 and 21.3% pass@100, while GPT models of comparable sizes achieve near 0% on both metrics. We see a remarkable progression in capabilities, with GPT-Neo-2.7B roughly equivalent to Codex-85M (30× fewer parameters). Similarly, GPT-J-6B achieves 11.6% pass@1 and 27.7% pass@100, which is roughly equivalent to Codex-300M (20× fewer parameters). Pass rates are obtained by taking the best result from eval-</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8. BLEU score probability densities for correct (blue) and wrong (green) solutions from Codex-12B for 4 random tasks from HumanEval. Note that the distributions are not cleanly separable, suggesting that optimizing for BLEU score is not equivalent to optimizing for functional correctness.</p>
<p>uating at temperatures 0.2, 0.4, and 0.8 for GPT-Neo, and from temperatures 0.2 and 0.8 for GPT-J. Detailed results across multiple model sizes can be found in Table 1.</p>
<p>Finally, we benchmark Codex against the largest free model from Tabnine, a leading code autocomplete system, which achieves 2.6% pass@1 (at T = 0.4) and 7.6% pass@100 (at T = 0.8). This is roughly equivalent to Codex-12M, one of the smallest models in our suite.</p>
<h3>3.5. Results on the APPS Dataset</h3>
<p>Recently, Hendrycks et al. (2021) introduced the APPS dataset to measure the coding challenge competence of language models. The APPS dataset consists of 5000 training and 5000 test examples of coding problems, each with a set of unit tests and, for the training data, a set of correct solutions. Most of the APPS tests problems are not formulated as single-function synthesis tasks, but rather as full-program synthesis, reading input from stdin and printing output to stdout, in contrast to the main Codex training data.</p>
<p>In the paper that introduces APPS, the authors benchmark a few language models and report two metrics: the percentage of problems where the model finds a correct solution (called the "strict accuracy") and the percentage of unit tests passed, even if the solution is incorrect. The latter measure is reported only so as to reduce variance of the measurements, because the results on the first metric were so low. We avoid this metric and only focus on "strict accuracy", and - as in</p>
<p>Table 1. Codex, GPT-Neo, \&amp; TabNine evaluations for HumanEval. We find that GPT-J pass@1 is between Codex-85M and Codex300M performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">PASS@ $k$</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">$k=1$</td>
<td style="text-align: right;">$k=10$</td>
<td style="text-align: right;">$k=100$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-NEO 125M</td>
<td style="text-align: right;">$0.75 \%$</td>
<td style="text-align: right;">$1.88 \%$</td>
<td style="text-align: right;">$2.97 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-NEO 1.3B</td>
<td style="text-align: right;">$4.79 \%$</td>
<td style="text-align: right;">$7.47 \%$</td>
<td style="text-align: right;">$16.30 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-NEO 2.7B</td>
<td style="text-align: right;">$6.41 \%$</td>
<td style="text-align: right;">$11.27 \%$</td>
<td style="text-align: right;">$21.37 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-J 6B</td>
<td style="text-align: right;">$11.62 \%$</td>
<td style="text-align: right;">$15.74 \%$</td>
<td style="text-align: right;">$27.74 \%$</td>
</tr>
<tr>
<td style="text-align: left;">TABNINE</td>
<td style="text-align: right;">$2.58 \%$</td>
<td style="text-align: right;">$4.35 \%$</td>
<td style="text-align: right;">$7.59 \%$</td>
</tr>
<tr>
<td style="text-align: left;">CODEX-12M</td>
<td style="text-align: right;">$2.00 \%$</td>
<td style="text-align: right;">$3.62 \%$</td>
<td style="text-align: right;">$8.58 \%$</td>
</tr>
<tr>
<td style="text-align: left;">CODEX-25M</td>
<td style="text-align: right;">$3.21 \%$</td>
<td style="text-align: right;">$7.1 \%$</td>
<td style="text-align: right;">$12.89 \%$</td>
</tr>
<tr>
<td style="text-align: left;">CODEX-42M</td>
<td style="text-align: right;">$5.06 \%$</td>
<td style="text-align: right;">$8.8 \%$</td>
<td style="text-align: right;">$15.55 \%$</td>
</tr>
<tr>
<td style="text-align: left;">CODEX-85M</td>
<td style="text-align: right;">$8.22 \%$</td>
<td style="text-align: right;">$12.81 \%$</td>
<td style="text-align: right;">$22.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">CODEX-300M</td>
<td style="text-align: right;">$13.17 \%$</td>
<td style="text-align: right;">$20.37 \%$</td>
<td style="text-align: right;">$36.27 \%$</td>
</tr>
<tr>
<td style="text-align: left;">CODEX-679M</td>
<td style="text-align: right;">$16.22 \%$</td>
<td style="text-align: right;">$25.7 \%$</td>
<td style="text-align: right;">$40.95 \%$</td>
</tr>
<tr>
<td style="text-align: left;">CODEX-2.5B</td>
<td style="text-align: right;">$21.36 \%$</td>
<td style="text-align: right;">$35.42 \%$</td>
<td style="text-align: right;">$59.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">CODEX-12B</td>
<td style="text-align: right;">$28.81 \%$</td>
<td style="text-align: right;">$46.81 \%$</td>
<td style="text-align: right;">$72.31 \%$</td>
</tr>
</tbody>
</table>
<p>the previous sections - we report pass@ $k$ numbers for various $k$ (Table 2). There are 2 additional factors, well-known from coding competitions, that we take into account:</p>
<ul>
<li>In coding competitions and in the APPS datasets, tasks are provided with 3 input/output examples included in the task description. We utilize this by sampling 1000 solutions from the model and filtering out only those that pass these 3 unit tests (if such solutions exist). We then calculate pass rates in this filtered set, and call it filtered pass@ $k$. Results without filtering are presented as raw pass@ $k$.</li>
<li>It is often the case both in coding competitions and in the results from Codex that a correct solution is found, but it is not algorithmically efficient enough to be considered passing. While this is not acceptable in the competitions, we also report the number of solutions that Codex produces that do not fail on any unit test, but that do time-out on some of them. We use a timeout of 3 seconds in our evaluation.</li>
</ul>
<p>To compensate for the fact the Codex is not fine-tuned on APPS, we append a single input/output example from the task description to the docstring as a formatting hint. We denote this setting as " 1 -shot" in Table 2, and find that Codex12B evaluated 1-shot achieves comparable performance to a GPT-Neo model fine-tuned on APPS. Consistent with our earlier findings, there are large benefits from generating and evaluating as many as 1000 samples per task, though for more difficult problems, solutions are often not efficient enough to pass the time limits. Finally, evaluating the first sample which passes the 3 public unit tests for each problem yields higher performance than raw pass@ 100 samples.</p>
<h2>4. Supervised Fine-Tuning</h2>
<p>In addition to standalone functions, Python code found on GitHub contains class implementations, configuration files, scripts, and even files used to store data. This code is seemingly unrelated to synthesizing functions from docstrings, and we hypothesize that the distribution mismatch reduces HumanEval performance.</p>
<p>In order to adapt Codex to the distribution of the task of interest, we construct a set of training problems from correctly implemented standalone functions, and use them for additional supervised fine-tuning. We describe two approaches for collecting these examples: from competitive programming websites and from repositories with continuous integration. We call the supervised fine-tuned models Codex-S, and show that they produce consistent gains across model size.</p>
<h3>4.1. Problems from Competitive Programming</h3>
<p>Programming contest and interview preparation websites use hidden unit tests to automatically judge the functional correctness of submissions. These problems are selfcontained, come with well-written problem statements, and generally have excellent test coverage. Additionally, these problems test algorithmic reasoning over a broad range of core skills and difficulties.</p>
<p>We collected problem statements, function signatures, and solutions from several popular programming contest and interview preparation websites. We then assembled these into programming tasks similar to HumanEval, using the problem description as the docstring. Since complete test suites are often hidden, we created unit tests from examples found in the problem statements, or extracted additional test cases through submitting incorrect solutions. In total, we curated 10,000 problems in this way.</p>
<h3>4.2. Problems from Continuous Integration</h3>
<p>Next, we curated programming problems from open source projects. Taking advantage of sys.setprofile, we were able to trace and collect inputs and outputs for all functions called during integration tests. This data could then be used to create unit tests for the functions.</p>
<p>Projects that employ continuous integration (CI) are ideal candidates for tracing. We follow the commands in the CI configuration files, which contain build and test commands, to set up the virtual environments, install dependencies, and run integration tests.</p>
<p>We considered GitHub repos using travis and tox as their CI frameworks, as they are two of the most popular CI tools. We additionally used publicly available source code from pip packages found in the python package index (PyPI).</p>
<p>Table 2. Finetuned GPT-Neo numbers from the APPS paper referenced above. For Codex-12B, the number of passing programs that timeout on some test is in the bracket. We used temperature 0.6 for sampling to cover all $k$ in pass@ $k$, so raw pass@1 results could be improved with lower temperature.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">INTRODUCTORY</th>
<th style="text-align: center;">INTERVIEW</th>
<th style="text-align: center;">COMPETITION</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-NEO 2.7B RAW PASS@1</td>
<td style="text-align: center;">$3.90 \%$</td>
<td style="text-align: center;">$0.57 \%$</td>
<td style="text-align: center;">$0.00 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-NEO 2.7B RAW PASS@5</td>
<td style="text-align: center;">$5.50 \%$</td>
<td style="text-align: center;">$0.80 \%$</td>
<td style="text-align: center;">$0.00 \%$</td>
</tr>
<tr>
<td style="text-align: left;">1-SHOT CODEX RAW PASS@1</td>
<td style="text-align: center;">$4.14 \%(4.33 \%)$</td>
<td style="text-align: center;">$0.14 \%(0.30 \%)$</td>
<td style="text-align: center;">$0.02 \%(0.03 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">1-SHOT CODEX RAW PASS@5</td>
<td style="text-align: center;">$9.65 \%(10.05 \%)$</td>
<td style="text-align: center;">$0.51 \%(1.02 \%)$</td>
<td style="text-align: center;">$0.09 \%(0.16 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">1-SHOT CODEX RAW PASS@100</td>
<td style="text-align: center;">$20.20 \%(21.57 \%)$</td>
<td style="text-align: center;">$2.04 \%(3.99 \%)$</td>
<td style="text-align: center;">$1.05 \%(1.73 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">1-SHOT CODEX RAW PASS@1000</td>
<td style="text-align: center;">$25.02 \%(27.77 \%)$</td>
<td style="text-align: center;">$3.70 \%(7.94 \%)$</td>
<td style="text-align: center;">$3.23 \%(5.85 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">1-SHOT CODEX FILTERED PASS@1</td>
<td style="text-align: center;">$22.78 \%(25.10 \%)$</td>
<td style="text-align: center;">$2.64 \%(5.78 \%)$</td>
<td style="text-align: center;">$3.04 \%(5.25 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">1-SHOT CODEX FILTERED PASS@5</td>
<td style="text-align: center;">$24.52 \%(27.15 \%)$</td>
<td style="text-align: center;">$3.23 \%(7.13 \%)$</td>
<td style="text-align: center;">$3.08 \%(5.53 \%)$</td>
</tr>
</tbody>
</table>
<p>Because these projects contained untrusted code, it was important to run integration tests in the sandboxed environment described above.</p>
<p>While there are millions of potential functions to curate problems from, we only collected about 40,000 because not all functions accept inputs and return outputs. Even when they do, most objects captured at runtime cannot be pickled and restored outside the sandbox unless the project was installed.</p>
<p>Since our tracing methodology produced inputs and outputs for all invoked functions, even builtin and library calls imported by the project were turned into problems. For this reason, functions from tracing tended to be the building blocks of command-line utilities. To excel at these tasks, the model does not need to know advanced algorithms and data structures. Rather, it needs to be able to follow instructions to implement the functionality specified in the docstring. Thus, tracing complements the puzzle nature of coding competition problems and broadens the distribution of tasks.</p>
<h3>4.3. Filtering Problems</h3>
<p>In the previous sections, we presented two methods we used to automatically create training problems. However, it is unclear how to control for quality. Some prompts underspecify the function that is implemented, in which case a perfectly valid solution may be wrongly penalized by the unit test. Some problems are stateful, and subsequent executions can result in different outcomes.</p>
<p>To address these issues, we use Codex-12B to generate 100 samples per curated problem. If no samples pass the unit tests, we consider the task to be either ambiguous or too difficult, and filter it out. We reran this verification several times to remove stateful or non-deterministic problems.</p>
<h3>4.4. Methods</h3>
<p>We fine-tune Codex on these training problems to produce a set of "supervised fine-tuned" models, which we call CodexS. To produce examples from training problems, we assemble the problems into the format shown in Figure 2. If there are prompts of varying length in a batch, we left-pad shorter prompts to the length of the longest prompt, so that the first tokens in the reference solutions line up in context.</p>
<p>We train to minimize negative log-likelihood of the reference solution, and mask out loss for any tokens in the prompt. We train using a learning rate $1 / 10$ as large as used for fine-tuning Codex, but adhere to the same learning rate schedule, and train until validation loss plateaus (less than 10B tokens).</p>
<h3>4.5. Results</h3>
<p>As with Codex, we first compute the optimal temperature for evaluating pass@ $k$ for $1 \leq k \leq 100$. We find that Codex-S prefers slightly higher temperatures for all $k&gt;1$, which possibly reflects the fact that Codex-S captures a narrower distribution than Codex. We use $T^{<em>}=0$ for computing pass@1 and $T^{</em>}=1$ for computing pass@100.</p>
<p>Next, we compare Codex-S against Codex on pass@1 and pass@100. Codex-S outperforms the corresponding Codex by an average margin of 6.5 percentage points on pass@1 and by a larger average margin of 15.1 percentage points on pass@100 across model size.</p>
<p>We also plot the performance of different sample selection heuristics for Codex-S-12B against the same heuristics for Codex-12B. When ranking between 1 and 100 samples by mean log probability, the average benefit over random ranking is 11.6 percentage points, which is over 2 percentage points higher than the corresponding benefit for Codex.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9. Optimal sampling temperatures as a function of the number of samples generated for both Codex and Codex-S. Codex-S generally requires a higher temperature for any particular value of $k$, possibly to compensate for the fact that it models a narrower distribution.</p>
<p>Codex-S Pass Rate vs Model Size
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10. Comparing Codex-S against Codex on the metrics proposed in Section 3. Codex-S is one or two orders of magnitude more parameter efficient on pass@1 and pass@100, and log-prob sample ranking with Codex-S yields similar benefits over random sampling that Codex does.</p>
<h2>5. Docstring Generation</h2>
<p>Generating code from docstrings is possible with Codex because code typically follows after a docstring, but it is not easy to induce Codex to generate docstrings from code. Nevertheless, we are motivated to produce a docstring writing model for safety reasons, as such a model can be used to describe the intent behind generated code. Using the training problems described in the previous section, we can easily create a training dataset for code-conditional docstring generation.</p>
<p>Specifically, for each training problem, we assemble a training example by concatenating the function signature, the reference solution, and then the docstring. Just as we train Codex-S by minimizing negative log-likelihood of the reference solution, we train the docstring generating models Codex-D by minimizing negative log-likelihood of the docstring.</p>
<p>When we benchmark our code generation models, we measure pass@ $k$ on the HumanEval dataset, where correctness is defined by passing a set of unit tests. However, there is no similar way to evaluate docstring samples automatically. Therefore, we grade sample docstrings by hand, considering a docstring correct if it uniquely and accurately specifies the code body. Due to the time consuming nature of this process, we only grade 10 samples per problem, for a total of 1640 problems, from Codex-D-12B at temperature 0.8 .</p>
<p>Codex-D often generates incorrect unit tests along with a docstring, but we ignore these during grading. However, we do not consider the docstring correct when the model simply copies the code body into the docstring. The most common failure modes we observe are when the docstring model leaves out an important detail (such as "an answer must be to two decimal places") or when it over-conditions on the function name and invents a problem unrelated to the function body.</p>
<p>As shown in Table 3, pass rates for Codex-D are lower but comparable to the corresponding pass rates for Codex-S at the same temperature. We do not have a strong hypothesis for which direction should yield higher pass rates. While generating docstrings may be more forgiving because natural language syntax is less strict than code syntax, docstrings in our dataset may be lower quality because developers tend to devote less time to writing docstrings. Indeed, our model produces docstrings like "I just found this function online" and "This test is not correctly written and it's not my solution."</p>
<p>Finally, with a docstring model, we have yet another way to choose a single sample from a set of $k$ samples. Instead of picking the sample with the best mean log probability as investigated in the previous two sections, we can choose the sample that maximizes the back-translation ob-</p>
<p>Table 3. Pass rates for our docstring generating model Codex-D, which is evaluated by hand-grading 10 samples per task due to the lack of a ground-truth automatic evaluation. We find similar but lower pass-rates compared to Codex-S.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">MODEL</th>
<th style="text-align: center;">PASS@1</th>
<th style="text-align: center;">PASS@10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CODEX-S-12B</td>
<td style="text-align: center;">$32.2 \%$</td>
<td style="text-align: center;">$59.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">CODEX-D-12B</td>
<td style="text-align: center;">$20.3 \%$</td>
<td style="text-align: center;">$46.5 \%$</td>
</tr>
</tbody>
</table>
<p>jective $P$ (ground truth docstring|generated sample) where $P$ is evaluated using Codex-D. Unfortunately, in Figure 7, we show that ranking samples via back-translation underperforms mean log-probability ranking, though it outperforms random ranking. This heuristic also appears to overfit quickly.</p>
<h2>6. Limitations</h2>
<p>While Codex is able to sample correct solutions for the majority of HumanEval problems, we find that it has a number of limitations.</p>
<p>First, Codex is not sample efficient to train. Our training dataset comprises a significant fraction of publicly available Python code on GitHub, totaling hundreds of millions of lines of code. Even seasoned developers do not encounter anywhere near this amount of code over their careers. Indeed, a strong student who completes an introductory computer science course is expected to be able to solve a larger fraction of problems than Codex-12B.</p>
<p>Next, we explore prompts on which Codex is likely to fail or display counter-intuitive behavior. While evaluating code generation is well-studied (Xu et al., 2021; Helmuth \&amp; Spector, 2015; Pantridge et al., 2017), many existing metrics measure performance in tightly specified, constrained problem instances (e.g., string manipulation in FlashFill (Gulwani, 2011)). Therefore, we developed a set of qualitative metrics for measuring the capabilities of code generating models while controlling for the complexity and abstraction level of the specifications (Appendix D). Applying this framework, we find that Codex can recommend syntactically incorrect or undefined code, and can invoke functions, variables, and attributes that are undefined or outside the scope of the codebase. Moreover, Codex struggles to parse through increasingly long and higher-level or system-level specifications.</p>
<p>To concretely illustrate model performance degradation as docstring length increases, we create a dataset of synthetic problems assembled from 13 basic building blocks, each of which modifies an input string in a deterministic way. Example building blocks are "convert the string to lowercase" or "remove every third character from the string" (the full
list is described in Appendix C). We find that as the number of chained building blocks in the docstring increases, model performance decreases exponentially. This behavior is uncharacteristic of a human programmer, who should be able to correctly implement a program for a chain of arbitrary length if they can do so for a chain of length two.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11. Pass rates of Codex-12B samples against the number of chained components in the synthetically generated docstring. With each additional component, pass rate drops by roughly a factor of 2-3.</p>
<p>Further, just as text-conditional generative models in other modalities (Ramesh et al., 2021) have difficulty with binding attributes to objects, Codex can make mistakes binding operations to variables, especially when the number of operations and variables in the docstring is large. For instance, in the following prompt, Codex-12B does not decrement the variable w and also fails to return the product of all numbers.</p>
<div class="codehilite"><pre><span></span><code>def do_work(x, y, z, w):
    &quot;&quot;&quot; Add 3 to y, then subtract 4
    from both x and w. Return the
    product of the four numbers. &quot;&quot;&quot;
    t = y + 3
    u = x - 4
    v = z * w
    return v
</code></pre></div>

<p>This understanding of Codex's limited system-level synthesis capabilities helps inform our assessment of the potential hazards of using it in a generative capacity, as well as the broader societal impacts that such systems could have.</p>
<h2>7. Broader Impacts and Hazard Analysis</h2>
<p>Codex has the potential to be useful in a range of ways. For example, it could help onboard users to new codebases, reduce context switching for experienced coders, enable non-programmers to write specifications and have Codex draft implementations, and aid in education and exploration. However, Codex also raises significant safety challenges, does not always produce code that is aligned with user intent,</p>
<p>and has the potential to be misused.
To better understand some of the hazards of using Codex in a generative capacity, we conducted a hazard analysis focused on identifying risk factors (Leveson, 2019) with the potential to cause harm. ${ }^{1}$ We outline some of our key findings across several risk areas below.</p>
<p>While some of our findings about the potential societal impacts of code generation systems were informed by work towards responsible deployment of the production-oriented Codex models (which descended from the research-oriented Codex models described in this paper), this section is not intended to provide a full account of any particular product's safety features. Unless otherwise specified, we anchor our analysis in the specific properties of the models described in this paper. We share this analysis in the belief that some of it generalizes to the broader class of code generation systems, and to encourage a norm of performing detailed impact analysis as part of major machine learning research projects.</p>
<p>Note that by focusing largely on risks in this section, we do not mean to imply that we expect the impact of this class of technologies to be net-negative; rather, risks merit particular attention here because they may be subtle or require deliberate effort to address, whereas we expect the benefits to be more obvious and "automatic" from the perspective of most users and affected stakeholders.</p>
<h3>7.1. Over-reliance</h3>
<p>One of the key risks associated with using code generation models in practice is over-reliance on generated outputs. Due to the limitations described above as well as alignment issues described below, Codex may suggest solutions that superficially appear correct but do not actually perform the task the user intended. This could particularly affect novice programmers, and could have significant safety implications depending on the context. We discuss a related issue in Appendix G, namely that code generation models can suggest insecure code. For these reasons, human oversight and vigilance is required for safe use of code generation systems like Codex.</p>
<p>We note several immediate ways to improve safety in the subsection on risk mitigation below, though over-reliance in particular is one that we believe merits further inquiry in industry and academia. While it is conceptually straight-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12. When the prompt includes subtle bugs, Codex tends to produce worse code than it is capable of. This persists when the prompt also includes instructions to write correct code. This gap increases with model size.
forward to provide documentation to users reminding them about model limitations, empirical investigation is necessary in order to identify how to reliably ensure vigilance in practice across a range of user experience levels, UI designs, and tasks. One challenge researchers should consider is that as capabilities improve, it may become increasingly difficult to guard against "automation bias."</p>
<h3>7.2. Misalignment</h3>
<p>As with other large language models trained on a next-token prediction objective, Codex will generate code that is as similar as possible to its training distribution. One consequence of this is that such models may do things that are unhelpful for the user, despite having the capability to be more helpful (see Figure 12). For example, if the user has some subtle mistakes in their code, Codex may "deliberately" suggest code that superficially appears good but is incorrect.</p>
<p>This is an alignment failure - the model is not aligned with the user's intentions. Informally, a system is misaligned if there's some task X that we want it to do, and it is "capable" of doing X but "chooses" not to. In contrast, if a system fails to do X because it does not have the ability to do so, then this system is not misaligned; it is just incompetent. See Appendix E for more detail, including a more precise definition of alignment.</p>
<p>It is important to study misalignment because it is a problem that is likely to become worse, not better, as the capabilities of our systems increase. For example, the model size scaling trend for the example in Figure 12 indicates that misalignment would likely persist and even get worse if data, parameters, and training time were scaled up.</p>
<p>While we expect that misaligned behaviour like this is unlikely to cause significant harm in current models, it is likely to become more dangerous and harder to eliminate as model</p>
<p>capabilities increase. A highly capable but sufficiently misaligned model trained on user approval might produce obfuscated code that looks good to the user even on careful inspection, but in fact does something undesirable or even harmful.</p>
<h3>7.3. Bias and representation</h3>
<p>Mirroring what has been found in the case of other language models trained on Internet data (Bender et al., 2021; Blodgett et al., 2020; Abid et al., 2021; Brown et al., 2020), we found that Codex can be prompted in ways that generate racist, denigratory, and otherwise harmful outputs as code comments, meriting interventions such as those discussed in the subsection on risk mitigation below. We also found that code generation models raise further bias and representation issues beyond problematic natural language: Codex can generate code with structure that reflects stereotypes about gender, race, emotion, class, the structure of names, and other characteristics. Particularly in the context of users who might over-rely on Codex or use it without first thinking through project design, this issue could have significant safety implications, giving further motivation to discourage over-reliance. We discuss bias and representation issues further in Appendix F. Filtration or modulation of generated outputs, documentation, and other interventions may help to mitigate these risks.</p>
<h3>7.4. Economic and labor market impacts</h3>
<p>Code generation and associated capabilities have several possible economic and labor market impacts. While Codex at its current capability level may somewhat reduce the cost of producing software by increasing programmer productivity, the size of this effect may be limited by the fact that engineers don't spend their full day writing code (O*NET, 2021). Other important tasks include conferring with colleagues, writing design specifications, and upgrading existing software stacks. ${ }^{2}$ We also found that Codex imports packages at different rates, which could advantage some package authors over others, particularly if programmers and engineers come to rely on Codex's suggestions. Over a longer time horizon, the effects of this class of technologies on software-related labor markets and on the economy more generally could be more substantial as capabilities improve. More study is needed both on the effects of code generation capabilities and on appropriate responses. We discuss economic and labor market implications in more detail in Appendix H.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>7.5. Security implications</h3>
<p>Codex could have various effects on the security landscape. Because Codex can produce vulnerable or misaligned code, ${ }^{3}$ qualified operators should review its generations before executing or trusting them, absent appropriate precautions. Future code generation models may be able to be trained to produce more secure code than the average developer, though that is far from certain.</p>
<p>Codex could also be misused to aid cybercrime. Although this is worthy of concern, based on our testing, we believe that at their current level of capability, Codex models do not materially lower the barrier to entry for malware development. ${ }^{4}$ We expect that more powerful code generation models will lead to future advancements, and therefore further research into mitigations and continued study of model capabilities are necessary.</p>
<p>The non-deterministic nature of systems like Codex could enable more advanced malware. This non-determinism makes it easier to create diverse software that accomplish the same tasks. While software diversity can sometimes aid defenders, ${ }^{5}$ it presents unique challenges for traditional malware detection and antivirus systems that rely on fingerprinting and signature-matching against previously sampled binaries. For example, a more capable code generation model could conceivably advance techniques for generating polymorphic malware. ${ }^{6}$ We believe that application security and model deployment strategies including rate-limiting access and abuse monitoring can manage this threat in the near term; however, the efficacy of these mitigations may scale sublinearly as more capable models are developed.</p>
<p>Similar to large language models, Codex models can learn patterns present in their training data (Carlini et al., 2021). Sensitive data present in source code are liable to be predicted by the model. Because Codex is trained on public repositories, we consider any sensitive data present in the training data to have already been compromised. Similarly, the public data should generally be treated as untrusted, as previous work (Goldblum et al., 2021; Schuster et al., 2020) has found that attackers may be able to corrupt training data to trigger specific model behaviors at runtime. We further discuss security implications in Appendix G.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h3>7.6. Environmental impacts</h3>
<p>Codex, like other large generative models, has an energy footprint from both training and inference (Schwartz et al., 2019; Bender et al., 2021; Patterson et al., 2021). The original training of GPT-3-12B consumed hundreds of petaflop/sdays of compute, while fine-tuning it to create Codex-12B consumed a similar amount of compute. This training was performed on a platform (Azure) that purchases carbon credits and sources significant amounts of renewable energy, reducing its carbon footprint. ${ }^{7}$ Compute consumption also has costs in the wider supply chain that can be quite concentrated on certain regions. ${ }^{8}$ Looking more globally and long-term, the compute demands of code generation could grow to be much larger than Codex's training if significant inference is used to tackle challenging problems. ${ }^{9}$</p>
<h3>7.7. Legal implications</h3>
<p>There are several legal considerations related to generated code. To begin with, the training of AI systems on Internet data, such as public GitHub repositories, has previously been identified as an instance of "fair use" (O'Keefe et al., 2019).</p>
<p>Our preliminary research also finds that Codex models rarely generate code that is identical to the contents of training data. Such occurrences were $&lt;0.1 \%$ in a study examining the frequency of code generations that appear to match code snippets in the training data (Ziegler, 2021). In these rare instances, the generated code consisted of common expressions or conventions within the programming language that appeared over and over again in the training data. We find that, to the extent the generated code appears identical to the training data, it is due to the predictive weightings in the model rather than retention and copying of specific code.</p>
<p>Generated code is also responsive and customized to the user's input, and the user retains complete control over editing and acceptance of the generated code. This can make code generation similar to auto-suggest or auto-completion</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>features that exist as features of other tools of authorship (e.g., document editors), in the sense that the finished work is still seen as the author's.</p>
<p>Our commitment to responsible and safe AI includes continued attention to the broader intellectual property implications of code generation systems. We intend to remain engaged with policymakers and experts on these issues so that the users of such systems can ultimately deploy them with confidence.</p>
<h3>7.8. Risk mitigation</h3>
<p>In closing, given the above, models like Codex should be developed, used, and their capabilities explored carefully with an eye towards maximizing their positive social impacts and minimizing intentional or unintentional harms that their use might cause. A contextual approach is critical to effective hazard analysis and mitigation, though a few broad categories of mitigations are important to consider in any deployment of code generation models.</p>
<p>Careful documentation and user interface design, code review requirements, and/or content controls (e.g., filtering of outputs) may help to reduce harms associated with overreliance as well as offensive content or insecure code generation. In the context of a model made available as a service (e.g., via an API), policies such as user review, use case restrictions, monitoring, and/or rate limiting may also help to reduce harms associated with malicious use or prevent its use in high-stakes domains for which the models are not well suited.</p>
<p>Appendices E, F, G, and H provide further detail on the risks described in this section and outline additional mitigation and research opportunities.</p>
<h2>8. Related Work</h2>
<p>The deep learning resurgence has led to strong advances in the field of program learning. Two popular approaches to neural program learning are program induction and program synthesis.</p>
<p>In program induction, a model generates program outputs directly from a latent program representation. Learning to Execute (Zaremba \&amp; Sutskever, 2014) demonstrated that models could execute simple tasks like addition and memorization. Later attempts at program induction incorporated inductive biases based on modern computing devices, such as the Neural Turing Machine (Graves et al., 2014), memory networks (Weston et al., 2015; Sukhbaatar et al., 2015), the Neural GPU (Kaiser \&amp; Sutskever, 2015), and the differentiable neural computer (Graves et al., 2016). More recent approaches like the Neural Program Interpreter (Reed \&amp; de Freitas, 2016; Shin et al., 2018; Pierrot et al., 2021) and</p>
<p>Universal Transformer (Dehghani et al., 2019) found recurrence to be a useful component in program induction.</p>
<p>In program synthesis, a model explicitly generates a program, usually from a natural language specification. One of the most popular classical approaches used a probabilistic context free grammar (PCFG) to generate a program's abstract syntax tree (AST). Maddison \&amp; Tarlow (2014) improved on this setup by learning a state vector used to condition child node expansion. Later, Allamanis et al. (2015) applied this idea in text-to-code retrieval and Yin \&amp; Neubig (2017) utilized it in text-conditional code generation. Code2seq (Alon et al., 2018) found that ASTs could also be leveraged for code-to-text generation.</p>
<p>Programs can also be synthesized without passing through an AST representation. Hindle et al. (2012) investigated n-gram language models of code, finding code to be more predictable than natural language. Latent Predictor Networks (Ling et al., 2016) showed that character-level language models could generate working code for implementing Magic the Gathering cards in an online arena, when aided with a latent mode that allows card attributes to be copied into code. DeepCoder (Balog et al., 2017) trained a model to predict the functions appearing in source code, which could be used to guide program search.</p>
<p>Following the success of large natural language models (Devlin et al., 2018; Radford et al., 2019; Liu et al., 2019; Raffel et al., 2020; Brown et al., 2020) large scale Transformers have also been applied towards program synthesis. CodeBERT (Feng et al., 2020) trained the BERT objective on docstrings paired with functions, and obtained strong results on code search. PyMT5 (Clement et al., 2020) is similar in spirit to our work, and used the T5 objective to train a system which can translate between non-overlapping subsets of ${$ signature, docstring, body $}$.</p>
<p>We used functional correctness to benchmark our models, and observed improvements on this metric with more sampling. SPoC (Kulal et al., 2019) considered the problem of producing functionally correct code from pseudocode with a fixed budget of compilations, which is similar to our pass@ $k$ metric. TransCoder (Lachaux et al., 2020) trained a system to translate between programming languages in an unsupervised manner, and also observed that functional correctness better captured the capabilities of their model than BLEU score. In fact, ContraCode (Jain et al., 2020) leveraged the large space of functionally correct programs to train a contrastive code model, which improved model performance on tasks like type inference. Finally, RobustFill (Devlin et al., 2017) observed that the best way to find a program consistent with input examples was to synthesize multiple samples through beam search.</p>
<p>Two early domain-specific datasets used to benchmark neu-
ral programming systems were FlashFill (Gulwani, 2011; Gulwani et al., 2012) and Hearthstone (Ling et al., 2016), though the community has trended towards broader and more difficult datasets. Barone \&amp; Sennrich (2017) proposed a large training and evaluation dataset consisting of Python declarations, docstrings, and bodies scraped from GitHub. The CodeSearchNet challenge (Husain et al., 2019) built an even larger corpus from GitHub with data from multiple popular programming languages. Recently, CodeXGLUE (Lu et al., 2021) aggregated several programming benchmarks, making use of the recently proposed CodeBLEU metric (Ren et al., 2020). Most relevant to our evaluation work is the APPS (Hendrycks et al., 2021) benchmark for measuring functional correctness based on problems from the competitive programming website Codeforces.</p>
<p>Finally, we note that coding is a broad activity which involves much more than synthesizing code from docstrings. Tufano et al. (2020) use Transformers to generate unit tests for code which outperformed commercial offerings. Aye et al. (2021) built an internal auto-complete tool for Facebook, and found that training on accepted user completions boosted system performance. Development also entails locating and fixing bugs. Early works used static or dynamic code analysis (Agrawal et al., 1995; Korel \&amp; Rilling, 1997), learned association rules (Jeffrey et al., 2009), and genetic programming (Goues et al., 2012) to debug faulty code. These approaches relied on running against a test suite to not only evaluate the correctness of suggestions but also expose problems in execution trace or search for a solution. More recent works (Tufano et al., 2019; Drain et al., 2021) considered bug-fixing as neural machine translation from buggy to correct programs. However, these works used an exact match against a reference instead of functional correctness, citing Qi et al. (2015)'s finding that most of the proposed solutions by genetic search in (Goues et al., 2012) passed through weak test suites by deleting functionality that failed. Human developers often write test suites with limited but targeted coverage, but this does not always work well against an algorithm, highlighting the challenges of evaluating correctness of programs.</p>
<h2>9. Conclusion</h2>
<p>We investigated whether it was possible to train large language models to produce functionally correct code bodies from natural language docstrings. By fine-tuning GPT on code from GitHub, we found that our models displayed strong performance on a dataset of human-written problems with difficulty level comparable to easy interview problems. Model performance could be improved by training on a distribution more similar to the evaluation set, and also by producing multiple samples from a model. We also found that it was simple to train a model to complete the reverse</p>
<p>task of producing docstrings from code bodies, and that the performance profiles of these models were similar. Finally, we expanded on the broader impacts of code generating models, and discussed model limitations, finding significant room for improvement.</p>
<h2>Acknowledgements</h2>
<p>We thank Sandhini Agarwal, Casey Chu, Jeffrey Ding, Peter Eckersley, Gillian Hadfield, Rich Harang, Jacob Jackson, Yunxin Jiao, Jade Leung, Andrew Lohn, Ryan Lowe, Thomas McGuire, Margaret Mitchell, Florentine Eloundou Nekoul, Cullen O'Keefe, Long Ouyang, Pranav Shyam, Irene Solaiman, Aravind Srinivas, Helen Toner, Ashish Vaswani, and Jeffrey Wu for helpful discussions and feedback on drafts of this work. We are also grateful to the Acceleration and Supercomputing teams at OpenAI for their work on software and hardware infrastructure that this project used. Finally, we thank GitHub for partnering to build GitHub Copilot and Microsoft Azure for supporting model training with infrastructure management.</p>
<h2>References</h2>
<p>Cwe-327: Use of a broken or risky cryptographic algorithm, 2006. URL https://cwe.mitre.org/data/definitions/ 327.html.</p>
<p>Cwe-780: Use of rsa algorithm without oaep, 2009. URL https: //cwe.mitre.org/data/definitions/780.html.</p>
<p>A6:2017-security misconfiguration, 2017. URL https: //owasp.org/www-project-top-ten/2017/ A6_2017-Security_Misconfiguration.html.</p>
<p>Abid, A., Farooqi, M., and Zou, J. Persistent anti-muslim bias in large language models. arXiv preprint arXiv:2101.05783, 2021.</p>
<p>Acemoglu, D. and Restrepo, P. Robots and jobs: Evidence from us labor markets. Journal of Political Economy, 128(6):2188-2244, 2020a.</p>
<p>Acemoglu, D. and Restrepo, P. The wrong kind of ai? artificial intelligence and the future of labour demand. Cambridge Journal of Regions, Economy and Society, 13(1):25-35, 2020b.</p>
<p>Agrawal, H., Horgan, J. R., London, S., and Wong, W. E. Fault localization using execution slices and dataflow tests. Proceedings of Sixth International Symposium on Software Reliability Engineering. ISSRE'95, pp. 143-151, 1995.</p>
<p>Allamanis, M., Tarlow, D., Gordon, A., and Wei, Y. Bimodal modelling of source code and natural language. In Bach, F. and Blei, D. (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2123-2132, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.mlr.press/ v37/allamanis15.html.</p>
<p>Alley, E. C., Khimulya, G., Biswas, S., AlQuraishi, M., and Church, G. M. Unified rational protein engineering with sequence-based deep representation learning. Nature methods, 16(12):1315-1322, 2019.</p>
<p>Alon, U., Brody, S., Levy, O., and Yahav, E. code2seq: Generating sequences from structured representations of code. In International Conference on Learning Representations, 2018.</p>
<p>Aye, G. A., Kim, S., and Li, H. Learning autocompletion from realworld datasets. 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), pp. 131-139, 2021.</p>
<p>Baevski, A., Zhou, H., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. arXiv preprint arXiv:2006.11477, 2020.</p>
<p>Balog, M., Gaunt, A., Brockschmidt, M., Nowozin, S., and Tarlow, D. Deepcoder: Learning to write programs. In 5th International Conference on Learning Representations (ICLR), 2017.</p>
<p>Bao, H., Dong, L., and Wei, F. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021.</p>
<p>Barone, A. V. M. and Sennrich, R. A parallel corpus of python functions and documentation strings for automated code documentation and code generation. ArXiv, abs/1707.02275, 2017.</p>
<p>Barrington, I. M. and Maciel, A. Lecture 3: Nondeterministic computation. https://people.clarkson.edu/ alexis/ PCMI/Notes/lectureB03.pdf, 2000. [Online; accessed 29-June-2000].</p>
<p>Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 610-623, 2021.</p>
<p>Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S. GPT-Neo: Large scale autoregressive language modeling with mesh-tensorflow, 2021. URL http://github.com/ eleutherai/gpt-neo.</p>
<p>Blodgett, S. L., Barocas, S., Daumé III, H., and Wallach, H. Language (technology) is power: A critical survey of "bias" in nlp. arXiv preprint arXiv:2005.14050, 2020.</p>
<p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.</p>
<p>Bureau of Labor Statistics, U. D. o. L. Computer programmers. Occupational Outlook Handbook, 2021a. URL https: //www.bls.gov/ooh/computer-and-informationtechnology/computer-programmers.htm.</p>
<p>Bureau of Labor Statistics, U. D. o. L. Bls - software developers. Occupational Outlook Handbook, 2021b. URL https: //www.bls.gov/ooh/computer-and-informationtechnology/software-developers.htm.</p>
<p>Carlini, N., Tramèr, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A., and Raffel, C. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21). USENIX Association, August 2021. URL https://www.usenix.org/conference/</p>
<p>usenixsecurity21/presentation/carliniextracting.</p>
<p>Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. Generative pretraining from pixels. In International Conference on Machine Learning, pp. 1691-1703. PMLR, 2020.</p>
<p>Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. ArXiv, abs/1904.10509, 2019.</p>
<p>Christiano, P. Clarifying "ai alignment". AI Alignment Forum, 2018. URL https://www.alignmentforum.org/ posts/ZeE7EKHTFMBs8eMxn/clarifying-aialignment.</p>
<p>Clarkson, M. R., Finkbeiner, B., Koleini, M., Micinski, K. K., Rabe, M. N., and Sánchez, C. Temporal logics for hyperproperties. In International Conference on Principles of Security and Trust, pp. 265-284. Springer, 2014.</p>
<p>Clement, C., Drain, D., Timcheck, J., Svyatkovskiy, A., and Sundaresan, N. Pymt5: Multi-mode translation of natural language and python code with transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9052-9065, 2020.</p>
<p>Crawford, K. The trouble with bias. NIPS 2017 Keynote, 2017. URL https://www.youtube.com/watch?v= fMym_BKWQzk.</p>
<p>Crawford, K. Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence. Yale University Press, 2021.</p>
<p>Dai, A. M. and Le, Q. V. Semi-supervised sequence learning. Advances in neural information processing systems, 28:30793087, 2015.</p>
<p>Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J. M., Parikh, D., and Batra, D. Visual dialog. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 326-335, 2017.</p>
<p>Davis, B. Protecting applications with automated software diversity, Sep 2018. URL https://galois.com/blog/ 2018/09/protecting-applications-with-automated-software-diversity.</p>
<p>Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Łukasz Kaiser. Universal transformers, 2019.</p>
<p>Devlin, J., Uesato, J., Bhupatiraju, S., Singh, R., rahman Mohamed, A., and Kohli, P. Robustfill: Neural program learning under noisy i/o. In ICML, 2017.</p>
<p>Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pretraining of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A., and Sutskever, I. Jukebox: A generative model for music. arXiv preprint arXiv:2005.00341, 2020.</p>
<p>Drain, D., Wu, C., Svyatkovskiy, A., and Sundaresan, N. Generating bug-fixes using pretrained transformers. Proceedings of the 5th ACM SIGPLAN International Symposium on Machine Programming, 2021.</p>
<p>Eghbal, N. Working in public: the making and maintenance of open source software. Stripe Press, 2020.</p>
<p>Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., et al. Codebert: A pre-trained model for programming and natural languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1536-1547, 2020.</p>
<p>Frey, C. B. The technology trap. Princeton University Press, 2019.
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. The pile: An 800gb dataset of diverse text for language modeling. 2020.</p>
<p>Goldblum, M., Tsipras, D., Xie, C., Chen, X., Schwarzschild, A., Song, D., Madry, A., Li, B., and Goldstein, T. Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses, 2021.</p>
<p>Goues, C. L., Dewey-Vogt, M., Forrest, S., and Weimer, W. A systematic study of automated program repair: Fixing 55 out of 105 bugs for S8 each. 2012 34th International Conference on Software Engineering (ICSE), pp. 3-13, 2012.</p>
<p>Graves, A. Generating sequences with recurrent neural networks, 2014.</p>
<p>Graves, A., Wayne, G., and Danihelka, I. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.</p>
<p>Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwińska, A., Colmenarejo, S. G., Grefenstette, E., Ramalho, T., Agapiou, J., et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538 (7626):471-476, 2016.</p>
<p>Gulwani, S. Automating string processing in spreadsheets using input-output examples. In PoPL'11, January 26-28, 2011, Austin, Texas, USA, January 2011.</p>
<p>Gulwani, S., Harris, W. R., and Singh, R. Spreadsheet data manipulation using examples. Commun. ACM, 55:97-105, 2012.</p>
<p>He, P., Liu, X., Gao, J., and Chen, W. Deberta: Decodingenhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020.</p>
<p>Helmuth, T. and Spector, L. General program synthesis benchmark suite. In Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation, pp. 1039-1046, 2015.</p>
<p>Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021.</p>
<p>Hindle, A., Barr, E. T., Su, Z., Gabel, M., and Devanbu, P. On the naturalness of software. In 2012 34th International Conference on Software Engineering (ICSE), pp. 837-847. IEEE, 2012.</p>
<p>Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration, 2020.</p>
<p>Husain, H., Wu, H.-H., Gazit, T., Allamanis, M., and Brockschmidt, M. Codesearchnet challenge: Evaluating the state of semantic code search. ArXiv, abs/1909.09436, 2019.</p>
<p>Jain, P., Jain, A., Zhang, T., Abbeel, P., Gonzalez, J., and Stoica, I. Contrastive code representation learning. ArXiv, abs/2007.04973, 2020.</p>
<p>Jeffrey, D., Feng, M., Gupta, N., and Gupta, R. Bugfix: A learningbased tool to assist developers in fixing bugs. 2009 IEEE 17th International Conference on Program Comprehension, pp. 7079, 2009.</p>
<p>Jones, C. and Bonsignour, O. The economics of software quality. Addison-Wesley Professional, 2011.</p>
<p>Kaiser, Ł. and Sutskever, I. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015.</p>
<p>Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models, 2020.</p>
<p>Kenton, Z., Everitt, T., Weidinger, L., Gabriel, I., Mikulik, V., and Irving, G. Alignment of language agents. arXiv preprint arXiv:2103.14659, 2021.</p>
<p>Keskar, N. S., McCann, B., Varshney, L. R., Xiong, C., and Socher, R. Ctrl: A conditional transformer language model for controllable generation, 2019.</p>
<p>Korel, B. and Rilling, J. Application of dynamic slicing in program debugging. In AADEBUG, 1997.</p>
<p>Koza, J. R., Andre, D., Keane, M. A., and Bennett III, F. H. Genetic programming III: Darwinian invention and problem solving, volume 3. Morgan Kaufmann, 1999.</p>
<p>Kulal, S., Pasupat, P., Chandra, K., Lee, M., Padon, O., Aiken, A., and Liang, P. S. Spoc: Search-based pseudocode to code. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/ file/7298332f04ac004a0ca44cc69ecf6f6bPaper.pdf.</p>
<p>Lacasse, N. Open-sourcing gvisor, a sandboxed container runtime, 2018.</p>
<p>Lachaux, M.-A., Rozière, B., Chanussot, L., and Lample, G. Unsupervised translation of programming languages. ArXiv, abs/2006.03511, 2020.</p>
<p>Leveson, N. Improving the standard risk matrix: Part 1. 2019. URL http://sunnyday.mit.edu/Risk-Matrix.pdf.</p>
<p>Li, P. L., Ko, A. J., and Begel, A. What distinguishes great software engineers? Empirical Software Engineering, 25(1):322-352, 2020.</p>
<p>Ling, W., Blunsom, P., Grefenstette, E., Hermann, K. M., Kočiskỳ, T., Wang, F., and Senior, A. Latent predictor networks for code generation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 599-609, 2016.</p>
<p>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692, 2019.</p>
<p>Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining taskagnostic visiolinguistic representations for vision-and-language tasks. arXiv preprint arXiv:1908.02265, 2019.</p>
<p>Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C., Drain, D., Jiang, D., Tang, D., Li, G., Zhou, L., Shou, L., Zhou, L., Tufano, M., Gong, M., Zhou, M., Duan, N., Sundaresan, N., Deng, S. K., Fu, S., and Liu, S. Codexglue: A machine learning benchmark dataset for code understanding and generation. ArXiv, abs/2102.04664, 2021.</p>
<p>Maddison, C. J. and Tarlow, D. Structured generative models of natural source code. In Proceedings of the 31st International Conference on International Conference on Machine Learning (ICML), pp. II-649, 2014.</p>
<p>Manna, Z. and Waldinger, R. J. Toward automatic program synthesis. 14(3):151-165, March 1971. ISSN 0001-0782. doi: 10.1145/362566.362568. URL https://doi.org/ $10.1145 / 362566.362568$.</p>
<p>Masanet, E., Shehabi, A., Lei, N., Smith, S., and Koomey, J. Recalibrating global data center energy-use estimates. Science, 367(6481):984-986, 2020.</p>
<p>Menezes, A., van Oorschot, P., and Vanstone, S. Handbook of Applied Cryptography. Discrete Mathematics and Its Applications. CRC Press, 2018. ISBN 9780429881329. URL https: //books.google.com/books?id=YyCyDwAAQBAJ.</p>
<p>Menick, J. and Kalchbrenner, N. Generating high fidelity images with subscale pixel networks and multidimensional upscaling, 2018.</p>
<p>Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pp. 3111-3119, 2013.</p>
<p>Ohm, M., Plate, H., Sykosch, A., and Meier, M. Backstabber's knife collection: A review of open source software supply chain attacks, 2020.</p>
<p>O'Keefe, C., Lansky, D., Clark, J., and Payne, C. Comment regarding request for comments on intellectual property protection for artificial intelligence innovation. Before the United States Patent and Trademark Office Department of Commerce, 2019. URL https://perma.cc/2S7G-2QWF.
O*NET. 15-1252.00 - software developers, 2021. URL https://www.onetonline.org/link/summary/151252.00.</p>
<p>Oord, A. v. d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016.</p>
<p>Oord, A. v. d., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.</p>
<p>O'Neill, M. and Spector, L. Automatic programming: The open issue? Genetic Programming and Evolvable Machines, pp. $1-12,2019$.</p>
<p>Pantridge, E., Helmuth, T., McPhee, N. F., and Spector, L. On the difficulty of benchmarking inductive program synthesis methods. In Proceedings of the Genetic and Evolutionary Computation Conference Companion, pp. 1589-1596, 2017.</p>
<p>Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.M., Rothchild, D., So, D., Texier, M., and Dean, J. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350, 2021.</p>
<p>Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.</p>
<p>Pierrot, T., Ligner, G., Reed, S., Sigaud, O., Perrin, N., Laterre, A., Kas, D., Beguir, K., and de Freitas, N. Learning compositional neural programs with recursive tree search and planning, 2021.</p>
<p>Planning, S. The economic impacts of inadequate infrastructure for software testing. National Institute of Standards and Technology, 2002.</p>
<p>Python Software Foundation and JetBrains. Python developers survey 2020 results, 2020. URL https: //www.jetbrains.com/lp/python-developers-survey-2020/.</p>
<p>Qi, Z., Long, F., Achour, S., and Rinard, M. An analysis of patch plausibility and correctness for generate-and-validate patch generation systems. Proceedings of the 2015 International Symposium on Software Testing and Analysis, 2015.</p>
<p>Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by generative pre-training. 2018.</p>
<p>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019.</p>
<p>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.</p>
<p>Raffel, C., Shazeer, N. M., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv, abs/1910.10683, 2020.</p>
<p>Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-to-image generation. ArXiv, abs/2102.12092, 2021.</p>
<p>Reed, S. and de Freitas, N. Neural programmer-interpreters, 2016.
Ren, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sundaresan, N., Zhou, M., Blanco, A., and Ma, S. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297, 2020.</p>
<p>Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C. L., Ma, J., et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15), 2021.</p>
<p>Rokon, M. O. F., Islam, R., Darki, A., Papalexakis, E. E., and Faloutsos, M. Sourcefinder: Finding malware source-code from publicly available repositories in github. In 23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020), pp. 149-163, San Sebastian, October 2020. USENIX Association. ISBN 978-1-939133-18-2. URL https://www.usenix.org/conference/ raid2020/presentation/omar.</p>
<p>Schuster, R., Song, C., Tromer, E., and Shmatikov, V. You autocomplete me: Poisoning vulnerabilities in neural code completion. The Advanced Computing Systems Association, 2020. URL https://www.usenix.org/system/ files/sec21summer_schuster.pdf.</p>
<p>Schwartz, R., Dodge, J., Smith, N. A., and Etzioni, O. Green ai, 2019.</p>
<p>Shin, E. C., Polosukhin, I., and Song, D. Improving neural program synthesis with inferred execution traces. Advances in Neural Information Processing Systems, 31:8917-8926, 2018.</p>
<p>Simon, H. A. Experiments with a heuristic compiler. J. $A C M, 10(4): 493-506$, October 1963. ISSN 0004-5411. doi: 10.1145/321186.321192. URL https://doi.org/ $10.1145 / 321186.321192$.</p>
<p>Stack Overflow. 2020 developer survey, 2020. URL https://insights.stackexchange.com/survey/ 2020#overview.</p>
<p>Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. Learning to summarize from human feedback, 2020.</p>
<p>Sukhbaatar, S., Szlam, A., Weston, J., and Fergus, R. End-to-end memory networks, 2015.</p>
<p>Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104-3112, 2014.</p>
<p>Trinkenreich, B., Wiese, I., Sarma, A., Gerosa, M., and Steinmacher, I. Women's participation in open source software: A survey of the literature. arXiv preprint arXiv:2105.08777, 2021.</p>
<p>Tufano, M., Watson, C., Bavota, G., Penta, M. D., White, M., and Poshyvanyk, D. An empirical study on learning bug-fixing patches in the wild via neural machine translation. ACM Transactions on Software Engineering and Methodology (TOSEM), 28:1 - 29, 2019.</p>
<p>Tufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and Sundaresan, N. Unit test case generation with transformers and focal context. 2020.</p>
<p>Van Oord, A., Kalchbrenner, N., and Kavukcuoglu, K. Pixel recurrent neural networks. In International Conference on Machine Learning, pp. 1747-1756. PMLR, 2016.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/ file/3f5ee243547dee91fbd053c1c4a845aaPaper.pdf.</p>
<p>Wang, B. and Komatsuzaki, A. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.</p>
<p>Weston, J., Chopra, S., and Bordes, A. Memory networks, 2015.</p>
<p>Woolf, M. Fun and dystopia with ai-based code generation using gpt-j-6b, June 2021. URL https://minimaxir.com/2021/06/gpt-j-6b/.</p>
<p>Xu, F. F., Vasilescu, B., and Neubig, G. In-ide code generation from natural language: Promise and challenges. arXiv preprint arXiv:2101.11149, 2021.</p>
<p>Yin, P. and Neubig, G. A syntactic neural model for general-purpose code generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 440-450, 2017.</p>
<p>Zaremba, W. and Sutskever, I. Learning to execute. arXiv preprint arXiv:1410.4615, 2014.</p>
<p>Zellers, R., Lu, X., Hessel, J., Yu, Y., Park, J. S., Cao, J., Farhadi, A., and Choi, Y. Merlot: Multimodal neural script knowledge models. arXiv preprint arXiv:2106.02636, 2021.</p>
<p>Zhao, T. Z., Wallace, E., Feng, S., Klein, D., and Singh, S. Calibrate before use: Improving few-shot performance of language models. arXiv preprint arXiv:2102.09690, 2021.</p>
<p>Ziegler, A. A first look at rote learning in github copilot suggestions., Jun 2021. URL https://docs.github.com/en/ github/copilot/research-recitation.</p>
<h2>A. Estimating pass@ $k$</h2>
<p>While all estimators mentioned previously are consistent, only the empirical estimate used by <em>Kulal et al. (2019)</em>, and (1) are unbiased. Evaluating pass@ $k$ in an unbiased way with any number of samples $n$ is important for fair comparison. For example, estimating pass@ $k=1-(1-$ pass@1) $)^{k}$ with $1-(1-\hat{p})^{k}$ using the empirical pass@1, results in a consistent underestimate as shown in Figure 13. The gap doesn’t fully close even when $n&gt;5k$, and results can seem better with more samples. The interpretation of this estimator is that we draw $k$ samples with replacement from a pool of $n$ candidates, but the $k$ samples are not independent.
(1) is unbiased, because it estimates the fail probability $(1-$ pass@1) $)^{k}$ as the probability of drawing $k$ failed samples without replacement. To show this, note that $c$, the number of correct samples that pass the unit tests, is distributed $\operatorname{Binom}(n, p)$, where $p$ is pass@1, and that (1) evaluates to 1 when $n-c&lt;k$. Then,
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13. Comparing the amount of bias and variance of two estimators of pass@ $k$. While the top expression may look correct, it underestimates the true value by a considerable margin. The unbiased estimator may have a slightly higher variance initially but allows for a fair comparison across different numbers of samples.</p>
<p>$$
\begin{aligned}
\mathbb{E}<em c="c">{c}\left[1-\frac{\binom{n-c}{k}}{\binom{n}{k}}\right] &amp; =1-\mathbb{E}</em>\right] \
&amp; =1-\sum_{i=0}^{n-k} \frac{\binom{n-c}{k}}{\binom{n}{k}}\binom{n}{i} p^{i}(1-p)^{n-i} \
&amp; =1-\sum_{i=0}^{n-k}\binom{n-k}{i} p^{i}(1-p)^{n-i} \
&amp; =1-(1-p)^{k} \sum_{i=0}^{n-k}\binom{n-k}{i} p^{i}(1-p)^{n-k-i} \
&amp; =1-(1-p)^{k}
\end{aligned}
$$}\left[\frac{\binom{n-c}{k}}{\binom{n}{k}</p>
<h2>B. Random Problems and Solutions from Codex-12B</h2>
<p>We show 8 random problems from HumanEval along with 8 random samples per problem generated from Codex-12B at temperature 0.8 .
def words_string(s):</p>
<div class="codehilite"><pre><span></span><code>You will be given a string of words separated by
    commas or spaces. Your task is
to split the string into words and return an array
    of the words.
For example:
words_string(&quot;Hi, my name is John&quot;) == [&quot;Hi&quot;, &quot;my&quot;,
    &quot;name&quot;, &quot;is&quot;, &quot;John&quot;]
words_string(&quot;One, two, three, four, five, six&quot;) ==
    [&quot;One&quot;, &quot;two&quot;, &quot;three&quot;, &quot;four&quot;, &quot;five&quot;, &quot;six&quot;]
&quot; &quot; &quot;
<span class="gu">##</span># COMPLETION 1 (CORRECT) : ###
words = []
splitted_word = &quot;&quot;
for char in s:
    if char == &quot;_&quot;:
        if splitted_word != &quot;&quot;:
            words.append(splitted_word)
            splitted_word = &quot;&quot;
    elif char != &quot;,&quot;:
        splitted_word += char
if splitted_word != &quot;&quot;:
        words.append(splitted_word)
return words
</code></pre></div>

<p>### COMPLETION 2 (WRONG) : ###
return s.split()
### COMPLETION 3 (WRONG) : ###
words = s.split("_")
words = [w.strip(",") for w in words]
words = [w.strip() for w in words]
return words
### COMPLETION 4 (WRONG) : ###
# Split the string into words
words = s.split(',')
# Remove any whitespace around each element
words = [w.strip() for w in words]
# Return an array of the words in the string
return words
### COMPLETION 5 (WRONG) : ###
return s.split()
### COMPLETION 6 (WRONG) : ###
return s.split()
### COMPLETION 7 (WRONG) : ###
return s.split(',')
### COMPLETION 8 (WRONG) : ###
return s.split(sep=None)
def is_prime(n):
"""Return true if a given number is prime, and false otherwise.</p>
<blockquote>
<blockquote>
<blockquote>
<p>is_prime(6)
False
is_prime(101)
True
is_prime(11)
True
is_prime(13441)
True
is_prime(61)
True
is_prime(4)
False
is_prime(1)
False
" " "
### COMPLETION 1 (WRONG) : ###
for i in range $(2, n)$ :
if $n$ \% i $==0$ :
return False
return True
### COMPLETION 2 (WRONG) : ###
for i in range $(2, n)$ :
if $n$ \% i $==0$ :
return False
return True
### COMPLETION 3 (WRONG) : ###
# TODO: implement this function
pass
### COMPLETION 4 (CORRECT) : ###
if $n&lt;2$ : return False
if $n==2$ : return True
if $n \% 2==0$ : return False
return not any $(n \% k==0$ for $k$ in range $(3$, int $(n$ $<em>+0.5)+1,2)$ )
### COMPLETION 5 (WRONG) : ###
def helper ( n$)$ :
# check if an odd number is multiple of any
integer up to the square root of the number
# if this is true, $n$ is not prime
for i in range $(2$, int $(n </em>+0.5+1))$ :
if $n$ \% i $==0$ :
return False
return True
# if helper returns true, then $n$ is prime, else $n$ is not prime
if helper ( $n$ ):
return True
else:
return False
### COMPLETION 6 (CORRECT) : ###
prime = True
if $n==1$ :
return False
for i in range $(2, n)$ :
if $n$ \% i $==0$ :
prime = False
return prime
### COMPLETION 7 (WRONG) : ###
for i in range $(2, n)$ :
if $n$ \% i $==0$ :
return False
return True
### COMPLETION 8 (CORRECT) : ###
if $n==2$ or $n==3$ :
return True
if $n&lt;2$ or $n$ \% 2 == 0 :
return False
if $n&lt;9$ :
return True
if $n$ \% 3 == 0 :
return False
$r=$ int $(n *+0.5)$
$\mathrm{f}=5$
while $E&lt;=r$ :
if $n$ \% $E==0$ :
return False
if $n$ \% $(E+2)==0$ :
return False
$\mathrm{f}+=6$
return True</p>
</blockquote>
</blockquote>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">add_elements</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="o">***</span>
<span class="w">    </span><span class="n">Given</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">non</span><span class="o">-</span><span class="n">empty</span><span class="w"> </span><span class="k">array</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">integers</span><span class="w"> </span><span class="n">arr</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">an</span>
<span class="w">        </span><span class="k">integer</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="k">return</span>
<span class="w">    </span><span class="n">the</span><span class="w"> </span><span class="nf">sum</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">first</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="k">element</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="n">most</span><span class="w"> </span><span class="n">two</span>
<span class="w">        </span><span class="n">digits</span><span class="p">.</span>
<span class="w">    </span><span class="nl">Example</span><span class="p">:</span>
<span class="w">        </span><span class="k">Input</span><span class="err">:</span><span class="w"> </span><span class="n">arr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">111,21,3,4000,5,6,7,8,9</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4</span>
<span class="w">        </span><span class="k">Output</span><span class="err">:</span><span class="w"> </span><span class="mi">24</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="nf">sum</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="mi">21</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">3</span>
<span class="w">    </span><span class="k">Constraints</span><span class="err">:</span>
<span class="w">        </span><span class="mf">1.</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="mi">100</span>
<span class="w">        </span><span class="mf">2.</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="w">    </span><span class="o">***</span>
<span class="w">    </span><span class="err">###</span><span class="w"> </span><span class="k">COMPLETION</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="p">(</span><span class="n">WRONG</span><span class="p">)</span><span class="err">:</span><span class="w"> </span><span class="err">###</span>
<span class="n">pass</span>
<span class="w">    </span><span class="err">###</span><span class="w"> </span><span class="k">COMPLETION</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="p">(</span><span class="n">WRONG</span><span class="p">)</span><span class="err">:</span><span class="w"> </span><span class="err">###</span>
<span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="n">arr</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">))</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">3</span><span class="err">:</span>
<span class="w">            </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span>
<span class="w">            </span><span class="k">continue</span>
<span class="w">        </span><span class="k">else</span><span class="err">:</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="n">arr</span><span class="o">[</span><span class="n">i</span><span class="o">]</span>
<span class="w">    </span><span class="err">###</span><span class="w"> </span><span class="k">COMPLETION</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="p">(</span><span class="n">WRONG</span><span class="p">)</span><span class="err">:</span><span class="w"> </span><span class="err">###</span>
<span class="n">sum_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<span class="k">while</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="n">arr</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">))</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="mi">2</span><span class="err">:</span>
<span class="w">        </span><span class="n">sum_</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">arr</span><span class="o">[</span><span class="n">i</span><span class="o">]</span>
<span class="w">        </span><span class="n">k</span><span class="w"> </span><span class="o">-=</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="n">i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span>
<span class="k">return</span><span class="w"> </span><span class="nf">sum</span>
<span class="w">    </span><span class="err">###</span><span class="w"> </span><span class="k">COMPLETION</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="p">(</span><span class="n">WRONG</span><span class="p">)</span><span class="err">:</span><span class="w"> </span><span class="err">###</span>
<span class="k">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="k">if</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="k">size</span><span class="err">:</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">None</span>
<span class="k">start</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span>
<span class="k">end</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span>
<span class="n">arr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">arr</span><span class="o">[</span><span class="n">start:end</span><span class="o">]</span>
<span class="k">return</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="w">    </span><span class="err">###</span><span class="w"> </span><span class="k">COMPLETION</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="p">(</span><span class="n">CORRECT</span><span class="p">)</span><span class="err">:</span><span class="w"> </span><span class="err">###</span>
<span class="k">if</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="nf">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="err">:</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="n">arr</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">))</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="mi">2</span><span class="err">:</span>
<span class="w">                </span><span class="nf">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">arr</span><span class="o">[</span><span class="n">i</span><span class="o">]</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="nf">sum</span>
<span class="k">else</span><span class="err">:</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="ss">&quot;Number_of_elements_are_less_than_k&quot;</span>
<span class="w">    </span><span class="err">###</span><span class="w"> </span><span class="k">COMPLETION</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="p">(</span><span class="n">WRONG</span><span class="p">)</span><span class="err">:</span><span class="w"> </span><span class="err">###</span>
<span class="k">return</span>
<span class="w">    </span><span class="err">###</span><span class="w"> </span><span class="k">COMPLETION</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="p">(</span><span class="n">WRONG</span><span class="p">)</span><span class="err">:</span><span class="w"> </span><span class="err">###</span>
<span class="k">return</span><span class="w"> </span><span class="nc">int</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">arr</span><span class="o">[</span><span class="n">:k</span><span class="o">]</span><span class="p">))</span><span class="o">[</span><span class="n">:k+1</span><span class="o">]</span><span class="p">)</span>
<span class="w">    </span><span class="err">###</span><span class="w"> </span><span class="k">COMPLETION</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="p">(</span><span class="n">WRONG</span><span class="p">)</span><span class="err">:</span><span class="w"> </span><span class="err">###</span>
<span class="n">global_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="err">:</span>
<span class="w">        </span><span class="n">global_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">arr</span><span class="o">[</span><span class="n">0</span><span class="o">]</span>
<span class="w">    </span><span class="k">else</span><span class="err">:</span>
</code></pre></div>

<p>if len(str(arr[i])) &lt;= 2:
global_sum += arr[i]
return global_sum
def vowels_count(s):
""Write a function vowels_count which takes a string representing
a word as input and returns the number of vowels in the string.
Vowels in this case are 'a', 'e', 'i', 'o', 'u'.
Here, ' $y$ ' is also a
vowel, but only when it is at the end of the given word.</p>
<div class="codehilite"><pre><span></span><code>Example:
&gt;&gt;&gt; vowels_count(&quot;abcde&quot;)
2
&gt;&gt;&gt; vowels_count(&quot;ACEDY&quot;)
3
***
<span class="gu">##</span># COMPLETION 1 (WRONG): ###
vowels = &quot;aeiou&quot;
v = 0
for c in s:
    if i in vowels:
        v += 1
return v
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="gu">##</span># COMPLETION 2 (WRONG): ###
vowels_set = (&#39;a&#39;, &#39;e&#39;, &#39;i&#39;, &#39;o&#39;, &#39;u&#39;, &#39;y&#39;)
count = 0
for c in s:
    if c in vowels_set:
        count += 1
return count
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="gu">##</span># COMPLETION 3 (WRONG): ###
vowels_list = [&#39;a&#39;, &#39;e&#39;, &#39;i&#39;, &#39;o&#39;, &#39;u&#39;, &#39;y&#39;]
vowels_count = 0
for char in s:
    if char in vowels_list:
        vowels_count += 1
return vowels_count
</code></pre></div>

<div class="codehilite"><pre><span></span><code>###<span class="w"> </span><span class="nv">COMPLETION</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="ss">(</span><span class="nv">WRONG</span><span class="ss">)</span>:<span class="w"> </span>###
<span class="k">return</span><span class="w"> </span><span class="mi">0</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="err">###</span><span class="w"> </span><span class="k">COMPLETION</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="p">(</span><span class="n">WRONG</span><span class="p">)</span><span class="err">:</span><span class="w"> </span><span class="err">###</span>
<span class="n">vowels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">&quot;a&quot;, &quot;e&quot;, &quot;i&quot;, &quot;o&quot;, &quot;u&quot;, &quot;y&quot;</span><span class="o">]</span>
<span class="n">num_vowels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">s</span><span class="p">))</span><span class="err">:</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">s</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nl">vowels</span><span class="p">:</span>
<span class="w">        </span><span class="n">num_vowels</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span>
<span class="k">return</span><span class="w"> </span><span class="n">num_vowels</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>###<span class="w"> </span><span class="nv">COMPLETION</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="ss">(</span><span class="nv">WRONG</span><span class="ss">)</span>:<span class="w"> </span>###
<span class="nv">vowels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;aeiou&quot;</span>
<span class="k">return</span><span class="w"> </span><span class="nv">sum</span><span class="ss">(</span><span class="nv">vowels</span>.<span class="nv">count</span><span class="ss">(</span><span class="nv">i</span><span class="ss">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">i</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">s</span><span class="ss">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>###<span class="w"> </span><span class="nv">COMPLETION</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="ss">(</span><span class="nv">WRONG</span><span class="ss">)</span>:<span class="w"> </span>###
<span class="nv">vowels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<span class="k">for</span><span class="w"> </span><span class="nv">c</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">s</span>:
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nv">c</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span>[<span class="s1">&#39;a&#39;</span>,<span class="w"> </span><span class="s1">&#39;e&#39;</span>,<span class="w"> </span><span class="s1">&#39;i&#39;</span>,<span class="w"> </span><span class="s1">&#39;o&#39;</span>,<span class="w"> </span><span class="s1">&#39;u&#39;</span>,<span class="w"> </span><span class="s1">&#39;y&#39;</span>]:
<span class="w">    </span><span class="nv">vowels</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span>
<span class="k">return</span><span class="w"> </span><span class="nv">vowels</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>###<span class="w"> </span><span class="nv">COMPLETION</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="ss">(</span><span class="nv">WRONG</span><span class="ss">)</span>:<span class="w"> </span>###
<span class="nv">vowels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;aeiou&quot;</span>
<span class="nv">count</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<span class="k">for</span><span class="w"> </span><span class="nv">letter</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">s</span>:
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nv">letter</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">vowels</span>:
<span class="w">        </span><span class="nv">count</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span>
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ Microsoft made a commitment in 2020 to shift to 100 percent renewable energy supply in its buildings and data centers by 2025. https://blogs.microsoft.com/blog/2020/01/16/microsoft-will-be-carbon-negative-by-2030/ A full assessment of the environmental impact of compute use is impossible to conduct without grounding in context and making comparison to the counterfactual impacts of competing products or services. Such analysis is out of scope for this paper.
${ }^{8}$ While data center energy usage has become much more efficient in recent years (Masanet et al., 2020), the production, use, and disposal of semiconductors still imposes environmental and human costs. See, e.g., (Crawford, 2021)
${ }^{9}$ Given that code generation (and other forms of AI) might be deployed widely throughout the economy as discussed above, these considerations suggest additional urgency in adopting renewable energy.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ See Appendix G - Insecure Code for examples of Codex producing insecure code.
${ }^{4}$ For more on characterizing Codex's capability limitations, see the Limitations section and experiments in the security analysis in Appendix G.
${ }^{5}$ For example, by helping to prevent certain types of memory corruption vulnerabilities. See (Davis, 2018) for more.
${ }^{6}$ Polymorphic malware is malicious code that mutates its implementation while maintaining its function.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
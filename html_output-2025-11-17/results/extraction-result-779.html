<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-779 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-779</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-779</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-252907613</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2023.eacl-main.204.pdf" target="_blank">Behavior Cloned Transformers are Neurosymbolic Reasoners</a></p>
                <p><strong>Paper Abstract:</strong> In this work, we explore techniques for augmenting interactive agents with information from symbolic modules, much like humans use tools like calculators and GPS systems to assist with arithmetic and navigation. We test our agent’s abilities in text games – challenging benchmarks for evaluating the multi-step reasoning abilities of game agents in grounded, language-based environments. Our experimental study indicates that injecting the actions from these symbolic modules into the action space of a behavior cloned transformer agent increases performance on four text game benchmarks that test arithmetic, navigation, sorting, and common sense reasoning by an average of 22%, allowing an agent to reach the highest possible performance on unseen games. This action injection technique is easily extended to new agents, environments, and symbolic modules.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e779.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e779.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neurosymbolic BCT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neurosymbolic Behavior-Cloned Transformer (T5-base)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A behavior-cloned T5-base transformer fine-tuned on gold trajectories and augmented by injecting symbolic module actions into the environment action space; at inference it selects actions by generating candidate strings (beam search) and aligning them to the provided valid action list.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Neurosymbolic Behavior-Cloned Transformer (T5-base)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>T5-base model fine-tuned with behavior cloning on oracle (gold) trajectories. Input at timestep t concatenates: task description, current observation (OBS), inventory (INV), 'look' room description (LOOK), previous action (PACT), and previous observation (POBS). Uses beam search to produce top-16 candidate strings; selects the first exact-match valid action from the simulator's valid-action list, otherwise chooses the valid action with highest unigram-overlap (cosine similarity) to the generated string. During training, gold trajectories include symbolic-module actions so the model learns to generate module queries and use module outputs as part of its action sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TEXTWORLDEXPRESS-based benchmarks: Text World Common Sense (TWC), Arithmetic, Sorting, MapReader</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based interactive fiction environments (TEXTWORLDEXPRESS) implementing pick-and-place tasks with partial observability (agent only sees textual room descriptions, inventory, and responses to actions; maps or connections may be revealed through explicit actions). Challenges include large combinatorial action spaces (5–30 native valid actions per step, expanded further by module actions), distractor objects, and requirement for multi-step reasoning (navigation, arithmetic, knowledge lookup, sorting).</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Symbolic modules injected into action space: (1) Arithmetic calculator (add/sub/mul/div limited to problem args), (2) Knowledge-base query module (QUERY <tokens> returning canonical location triples), (3) Sorting module (sort ascending/descending returning ordered lists), (4) Navigation module (next step to <location> actions that return the next location on a shortest path).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual responses (plain-text messages), structured short text (e.g., 'The result of dividing 22 by 11 is 2.'), lists (sorted item lists), relation triples (e.g., 'white coat located coat hanger'), and navigation directives ('The next location to move to is: hallway').</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Implicit sequence-based belief: the agent's belief is encoded implicitly in the transformer input sequence (task description + current observation + inventory + room description + previous action + previous observation). There is no separate explicit graph or symbolic belief store described for the agent itself; symbolic module outputs are returned as observations and thus become part of the model's input history.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>When a symbolic action is taken, the symbolic module produces a textual response that is returned to the agent as the next observation; that observation is concatenated into the next timestep's input (OBS/LOOK/INV/POBS fields). During training, gold trajectories include module actions and their resulting observations, so the transformer learns to treat tool outputs as ordinary observations in its sequence history. The paper does not describe an additional explicit belief-aggregation or graph update within the transformer beyond this sequence-history mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy via behavior cloning (sequence-to-sequence); planning is implicit in the transformer’s next-action prediction conditioned on history and valid-action list. The navigation module performs classical shortest-path computation externally and injects 'next-step' actions; the model learns when to query/use those navigation actions via cloning on gold trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation module computes shortest-path next-step directives using a graph built from either complete map information (if the agent reads the map) or incrementally scraped partial connections (from 'to the north you see X' and 'you are in X' observations); the module returns 'next step to <location>' actions that provide the immediate next location on the shortest path to the target.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Average normalized score across 4 benchmarks on unseen test set: 0.99 (score normalized 0–1); task completion steps reduced to average ~7 steps matching oracle efficiency (paper reports average steps reduced from 11 to 7 when using modules).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Average normalized score across 4 benchmarks on unseen test set: 0.72 (score normalized 0–1); higher average steps (~11).</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Injecting symbolic-module actions into the action space allows a behavior-cloned transformer to learn to call and exploit external tools (calculator, KB, sorting, navigation) because gold trajectories demonstrate their use; tool outputs become ordinary observations in the agent's sequence-history input and are thus incorporated implicitly into its belief. This yields large performance gains (avg +0.27 normalized points to near-ceiling performance) and increased efficiency (fewer steps), whereas the DRRN baseline fails to benefit. The approach depends on the valid-action aid and availability of gold demonstrations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e779.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e779.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Reinforcement Relevance Network (DRRN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning baseline that encodes observation and candidate actions separately (GRUs) and scores (obs,action) pairs with a Q-network to select the next action; used here as a baseline to test whether RL agents can exploit injected symbolic-module actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning with a combinatorial action space for predicting popular Reddit threads.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Deep Reinforcement Relevance Network (DRRN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DRRN encodes the current concatenated game state (state observation, task description, inventory, current room description) with a GRU and encodes each candidate action with another GRU; a Q-network estimates Q-values for (observation, candidate action) pairs, and during evaluation the action with highest estimated Q-value is selected. Trained with Deep Q-learning (parallel training across many environment instances and random seeds).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same TEXTWORLDEXPRESS benchmarks: TWC, Arithmetic, Sorting, MapReader</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text-game environments with large and dynamic valid-action lists; agent sees only textual descriptions and must act based on local observations and available valid actions.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Symbolic modules were injected into the candidate action set (same modules as described for the behavior-cloned transformer: arithmetic, knowledge-base queries, sorting, navigation). The DRRN could select module actions as part of the candidate set at each step.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Tool outputs are returned as textual observations by the environment when a symbolic action is selected; types include numeric answers, textual KB triples, sorted lists, and navigation directives.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Short-term encoded state via GRU: the agent encodes the current observation concatenated with task description, inventory and room description, but it does not maintain an explicit longer-horizon symbolic belief (no explicit map or KB maintained within the DRRN described).</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>When a module action is taken, the environment or module returns a textual next observation which is then encoded by the DRRN's GRU as the current observation for subsequent steps. There is no separate, explicit belief aggregation beyond the GRU-encoded current state described.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Model-free reinforcement learning (Deep Q-learning over (obs,action) pairs); the agent implicitly learns action values but does not perform explicit multi-step planning or search in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Average normalized score across benchmarks when modules injected: 0.20 (no improvement reported from module injection).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Average normalized score across benchmarks without modules: 0.20.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Although symbolic-module actions were made available to the DRRN (injected into candidate actions), the DRRN did not improve; authors hypothesize this is due to the DRRN's tendency to select actions that yield immediate reward without performing prerequisite actions (e.g., not reading/solving before acting) and difficulty exploring large action spaces. Thus model-free RL in this form struggled to learn to call and exploit external tools compared to a behavior-cloned transformer trained on gold demonstrations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e779.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e779.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oracle Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oracle (Gold-Trajectory) Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scripted optimal agent used to generate gold trajectories for training behavior cloning; it performs the correct sequence of actions including calls to symbolic modules when needed and injects symbolic actions into trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Oracle Agent (gold trajectory generator)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Scripted agents that implement optimal, generalizable solution trajectories for each benchmark (e.g., read math problem -> use calculator -> pick correct object -> place it). For experiments with symbolic modules, the oracle inserts appropriate module actions at the required steps (e.g., 'div 22 11' for arithmetic). Used to produce training data for behavior cloning.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TEXTWORLDEXPRESS benchmarks (TWC, Arithmetic, Sorting, MapReader)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same partially observable text-game environments; the oracle has privileged knowledge about task structure and uses symbolic modules to produce minimal, correct trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Uses all symbolic modules as part of scripted solutions: Arithmetic calculator, Knowledge-base queries, Sorting, and Navigation module for shortest-path steps.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured textual module outputs (numerical results, KB triples, sorted lists, 'next step' navigation directives) that are recorded as observations in the oracle trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Scripted deterministic reasoning; the oracle uses the environment state and map information (when available) to compute optimal actions rather than maintaining an explicit learned belief state. The oracle's behavior encodes the necessary state implicitly in its scripted logic.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>The oracle treats module outputs as immediate, ground-truth observations that determine subsequent scripted actions; module outputs are inserted directly into the trajectory at the step the module action is invoked.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Scripted optimal planning (deterministic policy encoded into oracle), uses explicit shortest-path reasoning for navigation tasks when map information is available.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Uses map-derived graph search (shortest path) to select navigation steps; when map is read the oracle computes the path and issues movement actions (or uses the navigation module's 'next step' actions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Oracle trajectories are optimal by construction; the behavior-cloned agent trained on these matches oracle step-efficiency (paper reports behavior-cloned agent with modules matches oracle ~7 steps average).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Oracle agents are critical to produce gold demonstrations that teach the behavior-cloned transformer when and how to call symbolic modules; availability of these gold trajectories explains much of the behavior-cloned transformer’s ability to exploit tools, highlighting a limitation for general RL where gold traces are unavailable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e779.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e779.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic Modules</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic Modules (Arithmetic, Knowledge Base, Sorting, Navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of Python-implemented symbolic modules wrapped around the TEXTWORLDEXPRESS API that monitor observations, inject valid actions into the environment action space, respond to module actions, and (for navigation) build spatial graphs for path planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Symbolic Modules (Arithmetic, KB, Sorting, Navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Modules implemented in Python as wrappers around the simulator API; they (1) monitor the observation stream for relevant triggers (e.g., math problem, objects with quantities, map text, 'you are in' / 'to the north you see' lines), (2) enumerate and inject symbolic actions into the simulator's valid-action list (e.g., 'mul 3 6', 'query white coat', 'sort ascending', 'next step to garage'), and (3) when module actions are selected, return deterministic textual responses. The knowledge-base module returns (object, hasCanonicalLocation, container) triples for queried tokens; sorting normalizes quantities and returns ordered lists; arithmetic computes exact numeric results restricted to operands present in the current math problem; navigation scrapes connections and computes shortest-path next-step directives.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TEXTWORLDEXPRESS environments (TWC, Arithmetic, Sorting, MapReader)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>These modules operate in partially observable textual environments where much world structure must be inferred or revealed via explicit actions; modules augment observability by exposing computed/structured information.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>These are the tools: Arithmetic calculator, Knowledge-base lookup, Sorting/normalization routine, Navigation path-planner / map-scraper.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Numerical results (calculator), relation triples (KB queries), ordered lists (sorting), navigation directives and short textual map summaries (navigation), all returned as textual observations to the agent.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Modules may maintain small internal state: navigation module incrementally builds a graph of locations from scraped 'you are in X' and 'to the north you see Y' phrases and full map text; other modules operate statelessly (compute-and-return) based on current observation. The agent's belief is not required by modules beyond their own internal scraped map for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>When a module action is selected, the module returns a textual response which is fed back to the agent as the environment observation; the navigation module updates its internal graph when it observes explicit map or connection strings and subsequently can return next-step outputs based on the current known graph.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Modules perform deterministic computation: arithmetic and sorting compute exact results; KB module looks up triples; navigation module performs graph shortest-path computation over the built map and returns next-step actions. The high-level agent integrates these module outputs by learning (behavior cloning) when to call them.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation module constructs a location graph from either full map text (read-map action) or incremental local connection statements; computes shortest path to requested goal and exposes the immediate next location via 'next step to <location>' actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Module injection substantially expanded action spaces (e.g., KB module adds ~530 actions) but when used by the behavior-cloned transformer led to aggregate performance of 0.99 normalized score across benchmarks and step-efficiency matching oracle (~7 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Symbolic modules can be injected as additional valid actions and provide precise, deterministic computations (numeric, structured, or path directives). When agents are trained with gold demonstrations that include module usage, neural agents learn to call modules and incorporate their outputs via normal observation channels, yielding large gains; modules alone do not guarantee gains for RL agents without appropriate training signals.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e779.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e779.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MapReader Environment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MapReader (navigation + pick-and-place benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A navigation-focused TEXTWORLDEXPRESS benchmark where the agent must read a map or scrape partial connectivity, navigate to a target location, pick up a coin, and return to the start; designed to test map-based planning and use of a navigation module.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Agents operating in MapReader (BCT, DRRN, Oracle)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same agent classes as in other benchmarks: behavior-cloned T5, DRRN baseline, and oracle scripted agent. The environment provides either a full textual map (if the agent 'reads' the map) or partial directional phrases; the navigation module can scrape and/or accept the map text and compute next-step shortest-path actions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MapReader (TEXTWORLDEXPRESS)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text environment with up to 15 locations; map may be revealed via a 'read map' action or partial connection statements ('To the East you see the corridor'). Navigation is non-trivial because target is 1–4 steps away but environment topology is not globally visible unless map is read; shortest-path planning is helpful for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Navigation symbolic module (map scraper + shortest-path planner) that adds 'next step to <location>' actions; agent may also use standard movement verbs provided by the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Navigation directives ('The next location to move to is: corridor'), textual map summaries (full map text), and incremental connection statements.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>The navigation module builds an internal graph of locations from map reads or scraped directional sentences; the agent receives module responses as observations and the agent's transformer encodes these in its sequence-history input—there is no separate global belief graph retained by the transformer beyond sequence history.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>As the agent reads the map or encounters connection sentences, the navigation module updates its internal connectivity graph; when the agent chooses a 'next step' module action the module returns the immediate next-location directive which appears in the next observation and is consumed by the agent's sequence-history input.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Hybrid: external deterministic shortest-path computation in the navigation module (graph-based path planning) coupled with learned selection of when to query/use the navigation action via behavior cloning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Graph-based shortest-path computation over a location graph constructed from full-map text or incrementally scraped directional statements; module returns immediate next-step suggestions rather than full path strings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Behavior-cloned agent using navigation module achieves near-ceiling performance on MapReader and reduces average steps (overall agent steps across tasks decreased from 11 to 7 when using modules); exact per-task normalized number not separately reported but aggregate performance with modules is 0.99.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Behavior-cloned agent without modules required more steps and had lower aggregate performance (aggregate normalized 0.72); MapReader specifically reported larger action spaces and benefited from navigation module for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing an external navigation module that constructs a graph from map text or partial connectivity and returns shortest-path next-step directives materially improves efficiency and final-task performance when the high-level agent is trained to use it (via gold trajectories). The agent itself treats navigation outputs like ordinary observations; explicit graph belief is kept in the module, not inside the transformer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LOA: Logical optimal actions for text-based interaction games <em>(Rating: 2)</em></li>
                <li>Neuro-symbolic reinforcement learning with first-order logic <em>(Rating: 2)</em></li>
                <li>Asking for knowledge: Training rl agents to query external knowledge using language <em>(Rating: 2)</em></li>
                <li>PIGLeT: Language grounding through neuro-symbolic interaction in a 3D world <em>(Rating: 2)</em></li>
                <li>Learning knowledge graph-based world models of textual environments <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-779",
    "paper_id": "paper-252907613",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "Neurosymbolic BCT",
            "name_full": "Neurosymbolic Behavior-Cloned Transformer (T5-base)",
            "brief_description": "A behavior-cloned T5-base transformer fine-tuned on gold trajectories and augmented by injecting symbolic module actions into the environment action space; at inference it selects actions by generating candidate strings (beam search) and aligning them to the provided valid action list.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Neurosymbolic Behavior-Cloned Transformer (T5-base)",
            "agent_description": "T5-base model fine-tuned with behavior cloning on oracle (gold) trajectories. Input at timestep t concatenates: task description, current observation (OBS), inventory (INV), 'look' room description (LOOK), previous action (PACT), and previous observation (POBS). Uses beam search to produce top-16 candidate strings; selects the first exact-match valid action from the simulator's valid-action list, otherwise chooses the valid action with highest unigram-overlap (cosine similarity) to the generated string. During training, gold trajectories include symbolic-module actions so the model learns to generate module queries and use module outputs as part of its action sequence.",
            "environment_name": "TEXTWORLDEXPRESS-based benchmarks: Text World Common Sense (TWC), Arithmetic, Sorting, MapReader",
            "environment_description": "Text-based interactive fiction environments (TEXTWORLDEXPRESS) implementing pick-and-place tasks with partial observability (agent only sees textual room descriptions, inventory, and responses to actions; maps or connections may be revealed through explicit actions). Challenges include large combinatorial action spaces (5–30 native valid actions per step, expanded further by module actions), distractor objects, and requirement for multi-step reasoning (navigation, arithmetic, knowledge lookup, sorting).",
            "is_partially_observable": true,
            "external_tools_used": "Symbolic modules injected into action space: (1) Arithmetic calculator (add/sub/mul/div limited to problem args), (2) Knowledge-base query module (QUERY &lt;tokens&gt; returning canonical location triples), (3) Sorting module (sort ascending/descending returning ordered lists), (4) Navigation module (next step to &lt;location&gt; actions that return the next location on a shortest path).",
            "tool_output_types": "Textual responses (plain-text messages), structured short text (e.g., 'The result of dividing 22 by 11 is 2.'), lists (sorted item lists), relation triples (e.g., 'white coat located coat hanger'), and navigation directives ('The next location to move to is: hallway').",
            "belief_state_mechanism": "Implicit sequence-based belief: the agent's belief is encoded implicitly in the transformer input sequence (task description + current observation + inventory + room description + previous action + previous observation). There is no separate explicit graph or symbolic belief store described for the agent itself; symbolic module outputs are returned as observations and thus become part of the model's input history.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "When a symbolic action is taken, the symbolic module produces a textual response that is returned to the agent as the next observation; that observation is concatenated into the next timestep's input (OBS/LOOK/INV/POBS fields). During training, gold trajectories include module actions and their resulting observations, so the transformer learns to treat tool outputs as ordinary observations in its sequence history. The paper does not describe an additional explicit belief-aggregation or graph update within the transformer beyond this sequence-history mechanism.",
            "planning_approach": "Learned policy via behavior cloning (sequence-to-sequence); planning is implicit in the transformer’s next-action prediction conditioned on history and valid-action list. The navigation module performs classical shortest-path computation externally and injects 'next-step' actions; the model learns when to query/use those navigation actions via cloning on gold trajectories.",
            "uses_shortest_path_planning": true,
            "navigation_method": "Navigation module computes shortest-path next-step directives using a graph built from either complete map information (if the agent reads the map) or incrementally scraped partial connections (from 'to the north you see X' and 'you are in X' observations); the module returns 'next step to &lt;location&gt;' actions that provide the immediate next location on the shortest path to the target.",
            "performance_with_tools": "Average normalized score across 4 benchmarks on unseen test set: 0.99 (score normalized 0–1); task completion steps reduced to average ~7 steps matching oracle efficiency (paper reports average steps reduced from 11 to 7 when using modules).",
            "performance_without_tools": "Average normalized score across 4 benchmarks on unseen test set: 0.72 (score normalized 0–1); higher average steps (~11).",
            "has_tool_ablation": true,
            "key_findings": "Injecting symbolic-module actions into the action space allows a behavior-cloned transformer to learn to call and exploit external tools (calculator, KB, sorting, navigation) because gold trajectories demonstrate their use; tool outputs become ordinary observations in the agent's sequence-history input and are thus incorporated implicitly into its belief. This yields large performance gains (avg +0.27 normalized points to near-ceiling performance) and increased efficiency (fewer steps), whereas the DRRN baseline fails to benefit. The approach depends on the valid-action aid and availability of gold demonstrations.",
            "uuid": "e779.0"
        },
        {
            "name_short": "DRRN",
            "name_full": "Deep Reinforcement Relevance Network (DRRN)",
            "brief_description": "A reinforcement-learning baseline that encodes observation and candidate actions separately (GRUs) and scores (obs,action) pairs with a Q-network to select the next action; used here as a baseline to test whether RL agents can exploit injected symbolic-module actions.",
            "citation_title": "Deep reinforcement learning with a combinatorial action space for predicting popular Reddit threads.",
            "mention_or_use": "use",
            "agent_name": "Deep Reinforcement Relevance Network (DRRN)",
            "agent_description": "DRRN encodes the current concatenated game state (state observation, task description, inventory, current room description) with a GRU and encodes each candidate action with another GRU; a Q-network estimates Q-values for (observation, candidate action) pairs, and during evaluation the action with highest estimated Q-value is selected. Trained with Deep Q-learning (parallel training across many environment instances and random seeds).",
            "environment_name": "Same TEXTWORLDEXPRESS benchmarks: TWC, Arithmetic, Sorting, MapReader",
            "environment_description": "Partially observable text-game environments with large and dynamic valid-action lists; agent sees only textual descriptions and must act based on local observations and available valid actions.",
            "is_partially_observable": true,
            "external_tools_used": "Symbolic modules were injected into the candidate action set (same modules as described for the behavior-cloned transformer: arithmetic, knowledge-base queries, sorting, navigation). The DRRN could select module actions as part of the candidate set at each step.",
            "tool_output_types": "Tool outputs are returned as textual observations by the environment when a symbolic action is selected; types include numeric answers, textual KB triples, sorted lists, and navigation directives.",
            "belief_state_mechanism": "Short-term encoded state via GRU: the agent encodes the current observation concatenated with task description, inventory and room description, but it does not maintain an explicit longer-horizon symbolic belief (no explicit map or KB maintained within the DRRN described).",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "When a module action is taken, the environment or module returns a textual next observation which is then encoded by the DRRN's GRU as the current observation for subsequent steps. There is no separate, explicit belief aggregation beyond the GRU-encoded current state described.",
            "planning_approach": "Model-free reinforcement learning (Deep Q-learning over (obs,action) pairs); the agent implicitly learns action values but does not perform explicit multi-step planning or search in this setup.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": "Average normalized score across benchmarks when modules injected: 0.20 (no improvement reported from module injection).",
            "performance_without_tools": "Average normalized score across benchmarks without modules: 0.20.",
            "has_tool_ablation": true,
            "key_findings": "Although symbolic-module actions were made available to the DRRN (injected into candidate actions), the DRRN did not improve; authors hypothesize this is due to the DRRN's tendency to select actions that yield immediate reward without performing prerequisite actions (e.g., not reading/solving before acting) and difficulty exploring large action spaces. Thus model-free RL in this form struggled to learn to call and exploit external tools compared to a behavior-cloned transformer trained on gold demonstrations.",
            "uuid": "e779.1"
        },
        {
            "name_short": "Oracle Agent",
            "name_full": "Oracle (Gold-Trajectory) Agent",
            "brief_description": "A scripted optimal agent used to generate gold trajectories for training behavior cloning; it performs the correct sequence of actions including calls to symbolic modules when needed and injects symbolic actions into trajectories.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Oracle Agent (gold trajectory generator)",
            "agent_description": "Scripted agents that implement optimal, generalizable solution trajectories for each benchmark (e.g., read math problem -&gt; use calculator -&gt; pick correct object -&gt; place it). For experiments with symbolic modules, the oracle inserts appropriate module actions at the required steps (e.g., 'div 22 11' for arithmetic). Used to produce training data for behavior cloning.",
            "environment_name": "TEXTWORLDEXPRESS benchmarks (TWC, Arithmetic, Sorting, MapReader)",
            "environment_description": "Same partially observable text-game environments; the oracle has privileged knowledge about task structure and uses symbolic modules to produce minimal, correct trajectories.",
            "is_partially_observable": true,
            "external_tools_used": "Uses all symbolic modules as part of scripted solutions: Arithmetic calculator, Knowledge-base queries, Sorting, and Navigation module for shortest-path steps.",
            "tool_output_types": "Structured textual module outputs (numerical results, KB triples, sorted lists, 'next step' navigation directives) that are recorded as observations in the oracle trajectories.",
            "belief_state_mechanism": "Scripted deterministic reasoning; the oracle uses the environment state and map information (when available) to compute optimal actions rather than maintaining an explicit learned belief state. The oracle's behavior encodes the necessary state implicitly in its scripted logic.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "The oracle treats module outputs as immediate, ground-truth observations that determine subsequent scripted actions; module outputs are inserted directly into the trajectory at the step the module action is invoked.",
            "planning_approach": "Scripted optimal planning (deterministic policy encoded into oracle), uses explicit shortest-path reasoning for navigation tasks when map information is available.",
            "uses_shortest_path_planning": true,
            "navigation_method": "Uses map-derived graph search (shortest path) to select navigation steps; when map is read the oracle computes the path and issues movement actions (or uses the navigation module's 'next step' actions).",
            "performance_with_tools": "Oracle trajectories are optimal by construction; the behavior-cloned agent trained on these matches oracle step-efficiency (paper reports behavior-cloned agent with modules matches oracle ~7 steps average).",
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Oracle agents are critical to produce gold demonstrations that teach the behavior-cloned transformer when and how to call symbolic modules; availability of these gold trajectories explains much of the behavior-cloned transformer’s ability to exploit tools, highlighting a limitation for general RL where gold traces are unavailable.",
            "uuid": "e779.2"
        },
        {
            "name_short": "Symbolic Modules",
            "name_full": "Symbolic Modules (Arithmetic, Knowledge Base, Sorting, Navigation)",
            "brief_description": "A set of Python-implemented symbolic modules wrapped around the TEXTWORLDEXPRESS API that monitor observations, inject valid actions into the environment action space, respond to module actions, and (for navigation) build spatial graphs for path planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Symbolic Modules (Arithmetic, KB, Sorting, Navigation)",
            "agent_description": "Modules implemented in Python as wrappers around the simulator API; they (1) monitor the observation stream for relevant triggers (e.g., math problem, objects with quantities, map text, 'you are in' / 'to the north you see' lines), (2) enumerate and inject symbolic actions into the simulator's valid-action list (e.g., 'mul 3 6', 'query white coat', 'sort ascending', 'next step to garage'), and (3) when module actions are selected, return deterministic textual responses. The knowledge-base module returns (object, hasCanonicalLocation, container) triples for queried tokens; sorting normalizes quantities and returns ordered lists; arithmetic computes exact numeric results restricted to operands present in the current math problem; navigation scrapes connections and computes shortest-path next-step directives.",
            "environment_name": "TEXTWORLDEXPRESS environments (TWC, Arithmetic, Sorting, MapReader)",
            "environment_description": "These modules operate in partially observable textual environments where much world structure must be inferred or revealed via explicit actions; modules augment observability by exposing computed/structured information.",
            "is_partially_observable": true,
            "external_tools_used": "These are the tools: Arithmetic calculator, Knowledge-base lookup, Sorting/normalization routine, Navigation path-planner / map-scraper.",
            "tool_output_types": "Numerical results (calculator), relation triples (KB queries), ordered lists (sorting), navigation directives and short textual map summaries (navigation), all returned as textual observations to the agent.",
            "belief_state_mechanism": "Modules may maintain small internal state: navigation module incrementally builds a graph of locations from scraped 'you are in X' and 'to the north you see Y' phrases and full map text; other modules operate statelessly (compute-and-return) based on current observation. The agent's belief is not required by modules beyond their own internal scraped map for navigation.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "When a module action is selected, the module returns a textual response which is fed back to the agent as the environment observation; the navigation module updates its internal graph when it observes explicit map or connection strings and subsequently can return next-step outputs based on the current known graph.",
            "planning_approach": "Modules perform deterministic computation: arithmetic and sorting compute exact results; KB module looks up triples; navigation module performs graph shortest-path computation over the built map and returns next-step actions. The high-level agent integrates these module outputs by learning (behavior cloning) when to call them.",
            "uses_shortest_path_planning": true,
            "navigation_method": "Navigation module constructs a location graph from either full map text (read-map action) or incremental local connection statements; computes shortest path to requested goal and exposes the immediate next location via 'next step to &lt;location&gt;' actions.",
            "performance_with_tools": "Module injection substantially expanded action spaces (e.g., KB module adds ~530 actions) but when used by the behavior-cloned transformer led to aggregate performance of 0.99 normalized score across benchmarks and step-efficiency matching oracle (~7 steps).",
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "Symbolic modules can be injected as additional valid actions and provide precise, deterministic computations (numeric, structured, or path directives). When agents are trained with gold demonstrations that include module usage, neural agents learn to call modules and incorporate their outputs via normal observation channels, yielding large gains; modules alone do not guarantee gains for RL agents without appropriate training signals.",
            "uuid": "e779.3"
        },
        {
            "name_short": "MapReader Environment",
            "name_full": "MapReader (navigation + pick-and-place benchmark)",
            "brief_description": "A navigation-focused TEXTWORLDEXPRESS benchmark where the agent must read a map or scrape partial connectivity, navigate to a target location, pick up a coin, and return to the start; designed to test map-based planning and use of a navigation module.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Agents operating in MapReader (BCT, DRRN, Oracle)",
            "agent_description": "Same agent classes as in other benchmarks: behavior-cloned T5, DRRN baseline, and oracle scripted agent. The environment provides either a full textual map (if the agent 'reads' the map) or partial directional phrases; the navigation module can scrape and/or accept the map text and compute next-step shortest-path actions.",
            "environment_name": "MapReader (TEXTWORLDEXPRESS)",
            "environment_description": "Partially observable text environment with up to 15 locations; map may be revealed via a 'read map' action or partial connection statements ('To the East you see the corridor'). Navigation is non-trivial because target is 1–4 steps away but environment topology is not globally visible unless map is read; shortest-path planning is helpful for efficiency.",
            "is_partially_observable": true,
            "external_tools_used": "Navigation symbolic module (map scraper + shortest-path planner) that adds 'next step to &lt;location&gt;' actions; agent may also use standard movement verbs provided by the environment.",
            "tool_output_types": "Navigation directives ('The next location to move to is: corridor'), textual map summaries (full map text), and incremental connection statements.",
            "belief_state_mechanism": "The navigation module builds an internal graph of locations from map reads or scraped directional sentences; the agent receives module responses as observations and the agent's transformer encodes these in its sequence-history input—there is no separate global belief graph retained by the transformer beyond sequence history.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "As the agent reads the map or encounters connection sentences, the navigation module updates its internal connectivity graph; when the agent chooses a 'next step' module action the module returns the immediate next-location directive which appears in the next observation and is consumed by the agent's sequence-history input.",
            "planning_approach": "Hybrid: external deterministic shortest-path computation in the navigation module (graph-based path planning) coupled with learned selection of when to query/use the navigation action via behavior cloning.",
            "uses_shortest_path_planning": true,
            "navigation_method": "Graph-based shortest-path computation over a location graph constructed from full-map text or incrementally scraped directional statements; module returns immediate next-step suggestions rather than full path strings.",
            "performance_with_tools": "Behavior-cloned agent using navigation module achieves near-ceiling performance on MapReader and reduces average steps (overall agent steps across tasks decreased from 11 to 7 when using modules); exact per-task normalized number not separately reported but aggregate performance with modules is 0.99.",
            "performance_without_tools": "Behavior-cloned agent without modules required more steps and had lower aggregate performance (aggregate normalized 0.72); MapReader specifically reported larger action spaces and benefited from navigation module for efficiency.",
            "has_tool_ablation": true,
            "key_findings": "Providing an external navigation module that constructs a graph from map text or partial connectivity and returns shortest-path next-step directives materially improves efficiency and final-task performance when the high-level agent is trained to use it (via gold trajectories). The agent itself treats navigation outputs like ordinary observations; explicit graph belief is kept in the module, not inside the transformer.",
            "uuid": "e779.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LOA: Logical optimal actions for text-based interaction games",
            "rating": 2,
            "sanitized_title": "loa_logical_optimal_actions_for_textbased_interaction_games"
        },
        {
            "paper_title": "Neuro-symbolic reinforcement learning with first-order logic",
            "rating": 2,
            "sanitized_title": "neurosymbolic_reinforcement_learning_with_firstorder_logic"
        },
        {
            "paper_title": "Asking for knowledge: Training rl agents to query external knowledge using language",
            "rating": 2,
            "sanitized_title": "asking_for_knowledge_training_rl_agents_to_query_external_knowledge_using_language"
        },
        {
            "paper_title": "PIGLeT: Language grounding through neuro-symbolic interaction in a 3D world",
            "rating": 2,
            "sanitized_title": "piglet_language_grounding_through_neurosymbolic_interaction_in_a_3d_world"
        },
        {
            "paper_title": "Learning knowledge graph-based world models of textual environments",
            "rating": 2,
            "sanitized_title": "learning_knowledge_graphbased_world_models_of_textual_environments"
        }
    ],
    "cost": 0.017603749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Behavior Cloned Transformers are Neurosymbolic Reasoners
May 2-6, 2023</p>
<p>Ruoyao Wang ruoyaowang@arizona.edu 
University of Arizona
TucsonAZ</p>
<p>Peter Jansen pajansen@arizona.edu 
University of Arizona
TucsonAZ</p>
<p>Marc-Alexandre Côté 
Microsoft Research Montréal ♢ Allen Institute for AI
SeattleWA</p>
<p>Prithviraj Ammanabrolu 
Behavior Cloned Transformers are Neurosymbolic Reasoners</p>
<p>Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics
the 17th Conference of the European Chapter of the Association for Computational LinguisticsMay 2-6, 2023
In this work, we explore techniques for augmenting interactive AI agents with information from symbolic modules, much like humans use tools like calculators and GPS systems to assist with arithmetic and navigation. We test our agent's abilities in text games-challenging benchmarks for evaluating the multi-step reasoning abilities of game agents in grounded, language-based environments. Our experimental study indicates that injecting the actions from these symbolic modules into the action space of a behavior cloned transformer agent increases performance on four text game benchmarks that test arithmetic, navigation, sorting, and common sense reasoning by an average of 22%, allowing an agent to reach the highest possible performance on unseen games. This action injection technique is easily extended to new agents, environments, and symbolic modules. 1</p>
<p>Introduction</p>
<p>Interactive fiction games (or text games) evaluate AI agents abilities to perform complex multistep reasoning tasks in interactive environments that are rendered exclusively using textual descriptions. Agents typically find these games challenging due to the complexities of the tasks combined with the reasoning limitations of contemporary models. Overall performance is generally low, with agents currently solving only 30% of classic interactive fiction games such as Zork (Ammanabrolu and Hausknecht, 2020;Yao et al., 2021;Atzeni et al., 2022). Similarly, reframing benchmarks such as question answering into text games where agents must interactively reason with their environment and make their reasoning steps explicit causes performance to substantially decrease (Wang et al., 2022), highlighting both the capacity Figure 1: An overview of our approach on an example game evaluating arithmetic ability. At each step, the agent receives an observation from the environment, then takes an action. By providing actions that interface to symbolic modules (such as a calculator), the agent is able to use external knowledge to help solve the task. of this methodology to evaluate multi-step reasoning, and the limitations of current language models.</p>
<p>While large language models are capable of a variety of common sense reasoning abilities (Liu et al., 2022b;Ji et al., 2020), contemporary agents typically struggle on tasks such as navigation, arithmetic, knowledge base lookup, and other tasks that humans typically make use of external tools (such as GPS systems, calculators, and books) to solve. This is at times frustrating, because the tasks they perform poorly on can sometimes be solved in a few dozen lines of code. In this work, we show that combining both approaches is possible for text game agents, with our approach shown in Figure 1. We develop symbolic modules for arithmetic, navigation, sorting, and knowledge base lookup in PYTHON, paired with new benchmark games for testing these capacities in interactive text game environments. We empirically demonstrate that injecting actions from those modules into the action space of each game can allow transformerbased agents to make use of that information, and achieve near-ceiling performance on unseen benchmark games that they otherwise find challenging.</p>
<p>Related Work</p>
<p>Neurosymbolic reasoning offers the promise of combining the inference capabilities of symbolic programs with the robustness of large neural networks. In the context of text games, Kimura et al. (2021a) develop methods to decompose text games into a set of logical rules, then combine these rules with deep reinforcement learning (Kimura et al., 2021b) or integer linear programming (Basu et al., 2021) to substantially increase agent performance while providing a more interpretable framework for understanding why agents choose specific actions (Chaudhury et al., 2021). More generally, neurosymbolic reasoning has been applied to a variety of multi-step inference problems, such as multi-hop question answering (Weber et al., 2019), language grounding (Zellers et al., 2021), and semantic analysis (Cambria et al., 2022).</p>
<p>Because text games require interactive multistep reasoning, agents have most commonly been modelled using reinforcement learning (e.g. He et al., 2016;Zahavy et al., 2018;Yao et al., 2020), though overall performance on most environments remains low (see Jansen, 2022;Osborne et al., 2021, for reviews). Recently, alternative approaches modeling reinforcement learning as a sequence-to-sequence problem using imitation learning have emerged, centrally using behavior cloning (Torabi et al., 2018), decision transformers (Chen et al., 2021), and trajectory transformers (Janner et al., 2021). These approaches model interactive multi-step reasoning problems as a Markov decision process, where an agent's observation and action history up to some depth are provided as input, and the transformer must predict the next action for the agent to take. Behavior cloning and decision transformers have recently been applied to text games with limited success (Wang et al., 2022). Here, we show that the performance of a behavior cloned transformer can substantially increase when augmented with neurosymbolic reasoning. Figure 1 illustrates the workflow of our approach. At each time step t, based on the observation o t , the symbolic module will generate a set of valid actions A m t , and the text game environment will have a distinct set of valid actions A e t . Let the valid action set at step t be A t = A m t ∪ A e t . Given o t and A t , the agent needs to choose an action a t ∈ A t to take. Note that in principle, any agent could be adapted to use this approach, since we simply inject actions from the symbolic modules into the environment action space. At a given time step, our approach checks if a t is a valid symbolic action. If a t ∈ A m t (e.g. div 22 11 in Figure 1), the symbolic module will generate the next observation o t+1 , otherwise the text game environment will take a t and generate o t+1 (e.g. take 2 bananas in Figure 1).</p>
<p>Approach</p>
<p>Environments and Symbolic Modules</p>
<p>We evaluate our approach to neurosymbolic reasoning using four text game benchmark environments centered around pick-and-place tasks, including one existing benchmark and three new developed for this work. Each environment supports parametric variation to generate many different games. These environments are outlined below, with additional details and example playthroughs found in APPENDIX B. All environments were implemented using the TEXTWORLDEXPRESS game engine (Jansen and Côté, 2022).</p>
<p>Text World Common Sense (TWC): A benchmark common sense reasoning task (Murugesan et al., 2021) where agents must collect objects from the environment (e.g. dirty socks), and place those objects in their canonical common sense locations (e.g. washing machine). The symbolic module for this game allows agents to query a knowledge base of (subject, relation, object) triples (e.g. (cushion, hasCanonicalLocation, sofa)).</p>
<p>MapReader: A navigation-themed pick-and-place game similar to Coin Collector (Yuan et al., 2018). An agent starts in a random location (e.g. the kitchen), and is provided with a target location (e.g. the garage). The agent must navigate to the target location, pick up a coin, then return to the starting location and place it in a box. The agent is further provided with a map that can be used for efficient route planning. The navigation symbolic module paired with this environment scrapes the observation space for location information (e.g. you are Knowledge Base Module &gt; query cushion cushion located sofa cushion located armchair</p>
<p>Navigation Module</p>
<p>You are currently in the kitchen. &gt; next step to living room The next location to move to is: hallway.</p>
<p>Arithmetic Module</p>
<blockquote>
<p>mul 3 6 Multiplying 3 and 6 results in 18.</p>
</blockquote>
<p>Sorting Module</p>
<blockquote>
<p>sort ascending The objects in ascending order are: 8mg of steel, 2g of iron, 5kg of copper. currently in the kitchen), and both complete (e.g. the map) or partial (e.g. to the north you see the hallway) spatial connection information.</p>
</blockquote>
<p>Arithmetic: A math-themed task, where agents must read and solve a math problem in order to know which object from a set of objects to pickand-place. An example problem is "take the bundle of objects that is equal to 3 multiplied by 6, and place them in the answer box", where the agent must complete the task by choosing 18 apples. Distractor objects are populated with quantities that correspond to performing the arithmetic incorrectly (e.g. 3 oranges, corresponding to subtracting 3 from 6). We pair the arithmetic game with a calculator module capable of performing addition, subtraction, multiplication, and division.</p>
<p>Sorting: A sorting-themed game where the agent begins in a room with three to five objects, and is asked to place them in a box one at a time in order of increasing quantity. To add complexity, quantities optionally include units (e.g. 5kg of copper, 8mg of steel) across measures of volume, mass, or length. The sorting game is paired with a module that scrapes the observation space for mentions of objects that include quantities, and sorts these in ascending or descending order on command.</p>
<p>Symbolic Modules</p>
<p>Examples of symbolic modules and their responses are provided in Table 1. The number of valid actions injected by each module varies between 2 from the sorting module (ascending/descending) to over 500 from the knowledge base look-up (one for each object and its canonical locations present in the knowledge base). Symbolic modules were implemented in PYTHON as a wrapper around the TEXTWORLDEXPRESS API, allowing modules to monitor observations from the environment, inject actions, and provide responses for any actions they recognized as valid.</p>
<p>Models</p>
<p>In this section, we introduce the reinforcement learning and behavior cloning agents used in our experiments. Additional details and hyperparameters are provided in APPENDIX A.</p>
<p>Deep Reinforcement Relevance Network (DRRN): The DRRN (He et al., 2016) is a fast and strong reinforcement learning baseline that is frequently used to deliver near state-of-the-art performance in a variety of text games (e.g. Xu et al., 2020;Yao et al., 2020;Wang et al., 2022). At each step, the DRRN separately encodes the observation and candidate actions using several GRUs (Cho et al., 2014). A Deep Q-Network is then used to estimate Q-values for each (observation, candidate action) pair. The candidate action with the highest predicted Q-value will be chosen as the next action.</p>
<p>Behavior Cloning: Behavior cloning (Torabi et al., 2018) is a form of imitation learning similar to the Decision Transformer (Chen et al., 2021) that models reinforcement learning as a sequence-tosequence problem, predicting the next action given a series of previous observations. We follow the strategy of Ammanabrolu et al. (2021) in adapting behavior cloning to text games, where the model input at step t includes the task description, current state observation, previous action, and previous state observation (d, o t , a t−1 , o t−1 ). During training, the agent is fine-tuned on gold trajectories, where the training target is to generate action a t from the gold trajectories. During evaluation, the agent performs inference online in the text game environment. For experiments reported here, we used a T5-base model (Raffel et al., 2020).</p>
<p>Oracle Agents and Gold Trajectories</p>
<p>To generate training data for the behavioral cloning model, we implement oracle agents that generate optimal and generalizable solution trajectories for each benchmark. For example, an oracle agent for an arithmetic game always reads the math problem, picks up the object with the same quantity as the Table 2: Average model performance across 100 games in the unseen test set. Scores are normalized to between 0 and 1 (higher is better), while steps represents the number of steps an agent takes in the environment (lower is better). Neurosymbolic performance reflects when models have access to symbolic modules in their action space.</p>
<p>math problem answer, then places that object in the answer box. For experiments using symbolic modules, we further insert appropriate module actions when the agent requires that information to complete the next step -for example, using the calculator module after reading the math problem in the arithmetic game.</p>
<p>Results and Discussion</p>
<p>The results of both DRRN and behavior cloning experiments across each benchmark are shown in Table 2. We report the average model performance across 100 games in the unseen test set. The DRRN achieves a low average performance of 0.20 without modules, while adding symbolic modules into the action space does not improve performance. In contrast, the behavior cloned T5 model has a moderate average performance of 0.72 without modules, while adding symbolic modules increases average task performance to 0.99, nearly solving each task. Symbolic modules also make the behavior cloned agent more efficient, reducing the average steps required to complete the tasks from 11 to 7, matching oracle agent efficiency. Why does behavior cloning perform well? The baseline behavior cloned transformer achieves moderate overall performance, likely owing at least in part due to its use of gold trajectories for training. Large pretrained transformers contain a variety of common sense knowledge and reasoning abilities (Zhou et al., 2020;Liu et al., 2022c) which likely contributes to the high performance on TWC, where the model only needs to match objects with their common sense locations. In contrast, while transformers have some arithmetic abilities, their accuracy tends to vary with the frequency of specific tokens in the training data (Razeghi et al., 2022), likely causing the modest performance on the Arithmetic game. Here, we show that instead of increasing the size of training data, transformers can be augmented with symbolic modules that perform certain kinds of reasoning with high accuracy. Compared to the DRRN, the presence of gold trajectories for training allows the behavior cloned transformer to efficiently learn how to capitalize on the knowledge available from those modules. Why does the DRRN perform poorly? We hypothesize that two considerations make these tasks difficult for the Deep Reinforcement Relevance Network. The model frequently tries to select actions that lead to immediate reward (such as immediately picking the correct number of objects in the arithmetic game), without having first done the prerequisite actions (like reading or solving the math problem) that would naturally lead it to select that action. This creates an ungeneralizable training signal, causing the model to fail to learn the task. In addition, the action spaces for each game are generally large -baseline games contain between 5 and 30 possible valid actions at each step (see Table 4 in the APPENDIX), resulting in up to 24 million possible trajectories up to 5 steps, which is challenging to explore. Inspired by Liu et al. (2022a), our future work will aim to overcome these limitations, and allow reinforcement learning models to learn to efficiently and effectively exploit information from symbolic modules.</p>
<p>How does performance compare against other agents? While most environments used in this work are new, TEXTWORLD COMMON SENSE is an existing benchmark. Figure 3 compares the Neurosymbolic Behavior Cloned Transformer against recent models that use a combination of reinforcement learning, logic, knowledge resources, and case-based reasoning. While the performance is not directly comparable -here, we use the TEXTWORLDEXPRESS reimplementation of TWC with supervised learning, while other models use  Table 3: A comparison of performance on TWC on unseen games on the "easy" setting. Note that models may not be directly comparable, as this work uses the TEXTWORLDEXPRESS reimplementation of TWC, and supervised learning. Scores are normalized to between 0 and 1 (higher is better), while steps represents the number of steps an agent takes in the environment (lower is better).</p>
<p>the original implementation with a mix of reinforcement learning and case-based reasoning -we can make the high-level observation that the performance of both the baseline and Neurosymbolic Behavior Cloned Transformer meets or exceeds the scores of previous models, while generating paths that are more efficient -by a factor of up to 7x.</p>
<p>Conclusion</p>
<p>In this paper, we present an approach to neurosymbolic reasoning for text games using action space injection that can be easily adapted to existing text game environments. For models that are capable of exploiting the information provided by the symbolic modules, this technique allows agents to inexpensively augment their reasoning skills to solve more complex tasks. We empirically demonstrate this approach can substantially increase task performance on four benchmark games using a behavior cloned transformer.</p>
<p>Limitations</p>
<p>Two assumptions highlight core limitations in the scope of our results for augmenting models with neurosymbolic reasoning: the privileged access to a list of valid actions, and the use of gold trajectories for training the behavior cloned transformer.</p>
<p>Valid Actions: One of the central challenges with text games is that the space of possible action utterances is large, and text game parsers recognize only a subset of possible actions (e.g. take apple on the table) while being unable to successfully interpret a broader range of more complex utterances (e.g. take the red fruit near the fridge). As a result, nearly all contemporary models (e.g. Am-manabrolu and Hausknecht, 2020; Adhikari et al., 2020; Murugesan et al., 2021) make use of the valid action aid (Hausknecht et al., 2020), where at a given step the model is provided with an exhaustive list of possible valid actions from the environment simulator, from which one action is chosen.</p>
<p>The models presented here similarly use this aid. The DRRN functions essentially as a ranker to select the most probable next action. The behavior cloned transformer generates a candidate action that is aligned using cosine similarity with the list of valid actions, where the action with the highest overlap is chosen as the next action. Overcoming the valid action aid will generally require either more complex simulation engines capable of interpreting a wider variety of intents from input actions, or models that learn sets of valid actions from a large amount of training data -though these generally demonstrate lower performance than those using valid actions (e.g. Yao et al., 2020).</p>
<p>Gold Trajectories: In this work we demonstrate a substantial improvement in the performance of a behavior cloned transformer when augmented with neurosymbolic reasoning, but this requires the use of gold trajectories demonstrating the use of those symbolic modules. Gold training data is not available in many reinforcement learning applications, and the model comparison we perform (DRRN versus behavior cloning) is meant to highlight the capacity for the behavior cloned model to learn to make use of symbolic modules through gold demonstrations, rather than to suggest the DRRN is incapable of this. In future work, we aim to develop training procedures to allow models that do not have the benefit of using gold trajectories to make use of symbolic modules.</p>
<p>Ethics Statement</p>
<p>Broader Impacts: As noted by Ammanabrolu and Riedl (2021), the ability to perform longterm multi-step reasoning in complex, interactive, partially-observable environments has downstream applications beyond playing games. Text games are platforms upon which to explore interactive, situated communication such as dialogue. Although reinforcement learning is applicable to many sequential decision making domains, our setting is most relevant to creating agents that affect change via language. This mitigates physical risks prevelant in robotics, but not cognitive and emotional risks, as any system capable of generating natural language is capable of biased language use (Sheng et al., 2021).</p>
<p>Intended Use: The method described in this paper involves fine-tuning a large pretrained transformer model. The data generated for fine-tuning was generated by gold agents, and not collected from human participants. The trained models are intended to operate on these benchmark tasks that assess reasoning capacities in navigation, arithmetic, and other common sense competencies. Large language models have been shown to exhibit a variety of biases (e.g. Nadeem et al., 2021) that may cause unintended harms, particularly (in the context of this work) in unintended use cases. </p>
<p>A.2 Hyperparameters</p>
<p>Following standard practice (e.g. (Wang et al., 2022;Xu et al., 2020;Hausknecht et al., 2020)), the DRRN models are trained for 100k steps. We parallelly train DRRN on 16 environment instances with five different random seeds and the average results are reported. The behavior cloned transformers are trained for between 2 and 20 epochs, with the best model (as evaluated on the development set) used for evaluating final performance on the test set. Trained models are evaluated on all 100 parametric variations in the development or test set. Environments are limited to 50 steps, such that if the agent exceeds this many steps without reaching an end state, the score at the last step is taken to be the final score, and the environment resets. Model training time varied between 1 hour and 12 hours, with the TWC model that includes a large number of symbolic module actions requiring the largest training time.</p>
<p>A.3 Implementation details</p>
<p>We make use of an existing DRRN implementation 2 and adapted it to the TEXTWORLDEXPRESS environment. At each step, the current game state observation, task description, inventory information, and the current room description are concatenated into one string and encoded by a GRU. All  Table 4: The minimum, mean, and maximum number of valid actions per step, for each benchmark. Values represent averages determined using a random agent that is run to 50 steps for on 10 training episodes per benchmark. candidate actions are encoded by another GRU. The Q-value of each encoded (observation, candidate action) pair is then estimated by a Q-network consists of two linear layers. During training, the next action is sampled from all candidate actions based on the estimated Q-values. During evaluation, the action with the highest estimated Q-value is chosen as the next action. For the behavior cloned transformer, the input string of the T5 model at step t are formatted as:
d </s> OBS o t </s> INV o inv t </s> LOOK o look t </s> <extra_id_0> </s> PACT a t−1 </s> POBS o t−1 </s>
where d is the task description, </s> and <extra_id_0> are the special tokens for separator and mask for text to generate used by the T5 model, OBS, INV, LOOK, PACT, and POBS are the special tokens representing observation o t , inventory information o inv t , the current room description obtained by the "look around" action o look t , previous action a t−1 , and previous observation o t−1 , respectively. We use beam search to generate the top 16 strings from the T5 model, and choose the first string that is a valid action as the action to take. In the case where the model does not generate an exact match, we use cosine similarity to pick the valid action that has the highest unigram overlap with an action generated by T5.</p>
<p>B Environments and Symbolic Modules</p>
<p>Action Space: The number of valid actions per step for each benchmark is shown in Table 4, with these values collected by a random agent that runs for 50 steps across 10 training episodes. Environments contain an average of 9 valid actions per step (range 5 to 30), not including actions injected from the symbolic modules. After adding the module actions, the action space becomes up to twice as large for the MapReader, Arithmetic, and Sorting bench-Text World Common Sense (TWC) Game Task Description: Your task is to pick up objects, then place them in their usual locations in the environment.</p>
<p>You are in the corridor. In one part of the room you see a shoe cabinet that is closed. There is also a white coat. You also see a key holder, that has nothing on it. In another part of the room you see a hat rack, that has nothing on it. In one part of the room you see a coat hanger, that has nothing on it. There is also a umbrella stand, that has nothing on it. Inventory:</p>
<p>Your inventory is currently empty. &gt; query white coat</p>
<p>The results are: white coat located coat hanger white coat located wardrobe Inventory:</p>
<p>Your inventory is currently empty. &gt; take white coat</p>
<p>You take the white coat. Inventory: a white coat &gt; put white coat in coat hanger Game completed. marks. The knowledge base module for TWC adds approximately 530 actions, substantially increasing the action space.</p>
<p>B.1 Text World Common Sense (TWC)</p>
<p>A benchmark common sense reasoning task where agents must collect objects from the environment (e.g. dirty socks) and place them in their canonical locations (e.g. washing machine). We use the TEXTWORLDEXPRESS implementation of the original environment (Murugesan et al., 2021), in a setting analogous to easy difficulty, where the agent must pick and place one misplaced item in the environment. The agent receives a score of 0.5 for picking up the object, and 1.0 for placing it in the correct location. Table 5 shows an example of the TWC Game in which the agent places a clean shirt in the wardrobe with the help of querying the Arithmetic Game</p>
<p>Task Description: Your first task is to solve the math problem. Then, pick up the item with the same quantity as the math problem answer, and place it in the box.</p>
<p>You are in the kitchen. In one part of the room you see a fridge that is closed. There is also a dining chair that has 11 tangerines, 33 papayas, 242 strawberries, and 20 peaches on it. You also see a box, that is empty. In another part of the room you see a math problem. In one part of the room you see a dishwasher that is closed. There is also a trash can that is closed.</p>
<p>You also see an oven. In another part of the room you see a cutlery drawer that is closed. In one part of the room you see a stove. There is also a kitchen cupboard that is closed. You also see a counter that has 6 oranges, and 2 bananas on it. Inventory: Your inventory is currently empty. &gt; take math problem</p>
<p>You take the math problem. Inventory: a math problem &gt; read math problem</p>
<p>Your task is to solve the following math problem: divide 22 by 11. Then, pick up the item with the same quantity as the answer, and place it in the box. Inventory: a math problem &gt; div 22 11</p>
<p>The result of dividing 22 by 11 is 2. Inventory: a math problem &gt; take 2 bananas</p>
<p>You take the 2 bananas. Inventory: a math problem 2 bananas &gt; put 2 bananas in box Game completed. common sense location of a clean shirt. Knowledge Base Module: We pair TWC with a symbolic module that provides queries to a simple knowledge base of <object, hasCanonicalLocation, container> triples. The symbolic module generates and accepts actions of the form QUERY <QUERY TOKENS>, where <QUERY TOKENS> corresponds to all object and container names in the knowledge base. This results in an increase of approximately 530 actions in the action space.</p>
<p>B.2 Arithmetic Game</p>
<p>The Arithmetic game requires agents to read a math problem, solve it, then perform a pick-and-place task based on the answer. For example, the agent may read the math problem ("Take the bundle of objects that is equal to 3 multiplied by 6, and place them in the box"), and must then perform the arithmetic then take 18 apples and place them in the answer box. Distractor objects are populated corresponding to performing the arithmetic incorrectly (for example, including 3 oranges, corresponding to subtracting 3 from 6, and 2 pears, corresponding to 6 divided by 3), with the condition that results are positive integer values. Agents receive a score of 0.5 for picking up the correct object, and 1.0 for completing the task successfully. An example playthrough of the Arithmetic game is in Table 6.</p>
<p>Arithmetic Module: We pair the Arithmetic game with an Arithmetic module that adds actions for addition, subtraction, multiplication, and division. To reduce the complexity of the action space, only actions with arguments from the current math problem are enumerated (e.g. add 3 6, sub 3 6, sub 6 3, mul 3 6, div 3 6, div 6 3).</p>
<p>B.3 Sorting Game</p>
<p>The sorting game is a pick-and-place game that presents an agent with 3 to 5 objects, and asks the agent to place them in an answer box one at a time based on order of increasing quantity. To add complexity to the game, quantities optionally include units (e.g. 5kg of copper, 8mg of steel, 2g of iron) across measures of volume, mass, or length. The agent score is the normalized proportion of objects sorted in the correct order, where perfect sorts receive a score of 1.0, and errors cause the score to revert to zero and the game to end. An example playthrough of the Sorting game is in Table 7.</p>
<p>Sorting Module: The sorting module monitors observations for mentions of objects (nouns) that include quantities, while also interpreting and normalizing quantities based on known units. The module injects two actions: sort ascending, and sort descending, that provides the user with a sorted list of objects.</p>
<p>B.4 MapReader Game</p>
<p>MapReader is a navigation oriented pick-and-place game similar to Coin Collector (Yuan et al., 2018), with the added complexity that the agent is provided with a map of the environment that can be used to more efficiently navigate. Environments and their maps are randomly generated to contain up to 15 locations drawn from 50 locations in Coin Collector. The agent begins in a randomly chosen location, and is asked to move to a target location (e.g. the kitchen), take a coin, then return to the starting location and place it in a box. Target locations are randomly chosen to be between 1 and 4 steps from the starting location. The most efficient solution method is to read the map, determine the shortest path between the agents current location Sorting Game Task Description: Your task is to sort objects by quantity. First, place the object with the smallest quantity in the box. Then, place the objects with the next smallest quantity in the box, and repeat until all objects have been placed in the box.</p>
<p>You are in the kitchen. In one part of the room you see a fridge that is closed. There is also a counter that has 15kg of cedar, and 21kg of marble on it. You also see a dining chair that has 25g of oak, and 47g of brick on it. In another part of the room you see a box, that is empty. In one part of the room you see a dishwasher that is closed. There is also a trash can that is closed. You also see an oven. In another part of the room you see a cutlery drawer that is closed. In one part of the room you see a stove. There is also a kitchen cupboard that is closed. Inventory:</p>
<p>Your inventory is currently empty. &gt; sort ascending</p>
<p>The observed items, sorted in order of increasing quantity  and target location, follow that path to retrieve the coin, then follow the path in reverse to return the coin to the starting location. The agent receives a score of 0.5 for retrieving the coin, and 1.0 for placing the coin in the box at the start location. An example of the MapReader game is shown in Table 8.</p>
<p>Navigation Module: We pair MapReader with a navigation module that scrapes the environment for both complete map information (obtained if the agent chooses to read the map), as well as partial information such as the current location (e.g. "You are in the kitchen") and connecting locations (e.g. "To the north you see the living room") that can be used to incrementally build a map. The module adds actions that, if selected, provide the next step in the shortest path to navigate to all known locations in the environment (e.g. next step to living room, next step to garage, ...).</p>
<p>MapReader Game</p>
<p>Task Description: Your task is to take the coin that is located in the laundry room, and put it into the box found in the foyer. A map is provided, that you may find helpful.</p>
<p>continued...</p>
<p>You are in the foyer. In one part of the room you see a box, that is empty.</p>
<p>You are in the laundry room. In one part of the room you see a coin. There To the East you see the corridor.</p>
<p>is also a bench, that has nothing on it. You also see a washing machine Inventory:</p>
<p>that is closed. In another part of the room you see a work table, that has a map nothing on it. In one part of the room you see a laundry basket, that has &gt; read map nothing on it. There is also a clothes drier that is closed.</p>
<p>To the South you see the corridor. The map reads:</p>
<p>Inventory: The living room connects to the backyard and corridor. a map The garage connects to the driveway.</p>
<blockquote>
<p>take coin The laundry room connects to the corridor. The backyard connects to the living room, alley, kitchen and sideyard.</p>
</blockquote>
<p>You take the coin. The bedroom connects to the corridor.</p>
<p>Inventory: The sideyard connects to the backyard and driveway. a map The kitchen connects to the bathroom, pantry and backyard. a coin The supermarket connects to the alley. &gt; next step to foyer The foyer connects to the corridor. The pantry connects to the kitchen.</p>
<p>The next location to go to is: corridor The driveway connects to the sideyard, alley and garage.</p>
<p>Inventory: The street connects to the alley. a map The alley connects to the driveway, supermarket, street and backyard. a coin The bathroom connects to the kitchen.</p>
<blockquote>
<p>move south The corridor connects to the living room, foyer, bedroom and laundry room. You are in the corridor. In one part of the room you see a shoe cabinet that Inventory:</p>
</blockquote>
<p>is closed. There is also a key holder, that has nothing on it. You also see a a map hat rack, that has nothing on it. In another part of the room you see a coat &gt; next step to laundry room hanger, that has nothing on it. In one part of the room you see a umbrella stand, that has nothing on it. The next location to go to is: corridor To the North you see the laundry room. To the South you see the living Inventory:</p>
<p>room. To the East you see the bedroom. To the West you see the foyer. a map Inventory: &gt; move east a map a coin You are in the corridor. In one part of the room you see a shoe cabinet that is closed. There is also a key holder, that has nothing on it. You also see a &gt; next step to foyer hat rack, that has nothing on it. In another part of the room you see a coat</p>
<p>The next location to go to is: foyer hanger, that has nothing on it. In one part of the room you see a umbrella Inventory: stand, that has nothing on it. a map To the North you see the laundry room. To the South you see the living a coin room. To the East you see the bedroom. To the West you see the foyer.</p>
<blockquote>
<p>move west Inventory: a map You are in the foyer. In one part of the room you see a box, that is empty. &gt; next step to laundry room</p>
</blockquote>
<p>To the East you see the corridor. Inventory: The next location to go to is: laundry room a map Inventory: a coin a map &gt; put coin in box &gt; move north Game completed. </p>
<p>Table 1 :
1Example actions (inputs) and responses from the four symbolic modules investigated in this work.</p>
<p>Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, and Chengqi Zhang. 2020. Deep reinforcement learning with stacked hierarchical attention for text-based games. Advances in Neural Information Processing Systems, 33:16495-16507.Computation Time: Training large models can 
involve a large carbon footprint (Strubell et al., 
2019), or decrease the availability of a method due 
to the barriers in accessing high performance com-
pute resources. The proposed technique can reduce 
the need for large models by augmenting smaller 
models with more complex reasoning through sym-
bolic modules. The behavior cloning experiments 
achieve strong performance with T5-base, high-
lighting the capacity of modest models that can be 
run with workstation GPUs to be better exploited 
for complex reasoning tasks. 
Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, 
Mikuláš Zelinka, Marc-Antoine Rondeau, Romain 
Laroche, Pascal Poupart, Jian Tang, Adam Trischler, 
and Will Hamilton. 2020. Learning dynamic belief 
graphs to generalize on text-based games. In Ad-
vances in Neural Information Processing Systems, 
volume 33, pages 3045-3057. Curran Associates, 
Inc. </p>
<p>Prithviraj Ammanabrolu and Matthew Hausknecht. 
2020. Graph constrained reinforcement learning for 
natural language action spaces. In International Con-
ference on Learning Representations. </p>
<p>Prithviraj Ammanabrolu and Mark Riedl. 2021. Learn-
ing knowledge graph-based world models of textual 
environments. In Thirty-fifth Conference on Neural 
Information Processing Systems (NeurIPS). </p>
<p>Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li, 
Arthur Szlam, Tim Rocktäschel, and Jason Weston. </p>
<ol>
<li>How to motivate your dragon: Teaching goal-
driven agents to speak and act in fantasy worlds. In 
Proceedings of the 2021 Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, 
pages 807-833, Online. Association for Computa-
tional Linguistics. </li>
</ol>
<p>Mattia Atzeni, Shehzaad Zuzar Dhuliawala, Keerthi-
ram Murugesan, and MRINMAYA SACHAN. 2022. 
Case-based reasoning for better generalization in tex-
tual reinforcement learning. In International Confer-
ence on Learning Representations. </p>
<p>Kinjal Basu, Keerthiram Murugesan, Mattia Atzeni, Pa-
van Kapanipathi, Kartik Talamadupula, Tim Klinger, 
Murray Campbell, Mrinmaya Sachan, and Gopal 
Gupta. 2021. A hybrid neuro-symbolic approach for 
text-based games using inductive logic programming. 
In Proceedings of the 1st Workshop on Combining 
Learning and Reasoning: Programming Languages, 
Formalisms, and Representations. </p>
<p>Erik Cambria, Qian Liu, Sergio Decherchi, Frank 
Xing, and Kenneth Kwok. 2022. SenticNet 7: A 
commonsense-based neurosymbolic AI framework 
for explainable sentiment analysis. In Proceedings of 
the Thirteenth Language Resources and Evaluation 
Conference, pages 3829-3839, Marseille, France. Eu-
ropean Language Resources Association. </p>
<p>Subhajit Chaudhury, Prithviraj Sen, Masaki Ono, Daiki 
Kimura, Michiaki Tatsubori, and Asim Munawar. 
2021. Neuro-symbolic approaches for text-based 
policy learning. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 3073-3078, Online and Punta Cana, 
Dominican Republic. Association for Computational 
Linguistics. </p>
<p>Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, 
Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind 
Srinivas, and Igor Mordatch. 2021. Decision trans-
former: Reinforcement learning via sequence mod-
eling. Advances in neural information processing 
systems, 34:15084-15097. </p>
<p>Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties 
of neural machine translation: Encoder-decoder ap-
proaches. In Proceedings of SSST-8, Eighth Work-
shop on Syntax, Semantics and Structure in Statistical 
Translation, pages 103-111, Doha, Qatar. Associa-
tion for Computational Linguistics. </p>
<p>Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-
Alexandre Côté, and Xingdi Yuan. 2020. Interactive 
fiction games: A colossal adventure. Proceedings 
of the AAAI Conference on Artificial Intelligence, 
34(05):7903-7910. </p>
<p>Ji He, Mari Ostendorf, Xiaodong He, Jianshu Chen, 
Jianfeng Gao, Lihong Li, and Li Deng. 2016. Deep 
reinforcement learning with a combinatorial action 
space for predicting popular Reddit threads. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1838-
1848, Austin, Texas. Association for Computational 
Linguistics. </p>
<p>Michael Janner, Qiyang Li, and Sergey Levine. 2021. 
Offline reinforcement learning as one big sequence 
modeling problem. In Advances in Neural Informa-
tion Processing Systems. </p>
<p>Peter Jansen. 2022. A systematic survey of text worlds 
as embodied natural language environments. In Pro-
ceedings of the 3rd Wordplay: When Language Meets 
Games Workshop (Wordplay 2022), pages 1-15, Seat-
tle, United States. Association for Computational 
Linguistics. </p>
<p>Peter A Jansen and Marc-Alexandre Côté. 2022. 
Textworldexpress: Simulating text games at 
one million steps per second. arXiv preprint 
arXiv:2208.01174. </p>
<p>Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan 
Zhu, and Minlie Huang. 2020. Language generation 
with multi-hop reasoning on commonsense knowl-
edge graph. In Proceedings of the 2020 Conference 
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 725-736, Online. Association 
for Computational Linguistics. </p>
<p>Daiki Kimura, Subhajit Chaudhury, Masaki Ono, Michi-
aki Tatsubori, Don Joven Agravante, Asim Munawar, 
Akifumi Wachi, Ryosuke Kohita, and Alexander 
Gray. 2021a. LOA: Logical optimal actions for text-
based interaction games. In Proceedings of the 59th 
Annual Meeting of the Association for Computational 
Linguistics and the 11th International Joint Con-
ference on Natural Language Processing: System 
Demonstrations, pages 227-231, Online. Association 
for Computational Linguistics. </p>
<p>Daiki Kimura, Masaki Ono, Subhajit Chaudhury, 
Ryosuke Kohita, Akifumi Wachi, Don Joven Agra-
vante, Michiaki Tatsubori, Asim Munawar, and 
Alexander Gray. 2021b. Neuro-symbolic reinforce-
ment learning with first-order logic. In Proceedings 
of the 2021 Conference on Empirical Methods in Nat-
ural Language Processing, pages 3505-3511, Online 
and Punta Cana, Dominican Republic. Association 
for Computational Linguistics. </p>
<p>Iou-Jen Liu, Xingdi Yuan, Marc-Alexandre Côté, 
Pierre-Yves Oudeyer, and Alexander G. Schwing. 
2022a. Asking for knowledge: Training rl agents 
to query external knowledge using language. ArXiv, 
abs/2205.06111. </p>
<p>Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-
ter West, Ronan Le Bras, Yejin Choi, and Hannaneh 
Hajishirzi. 2022b. Generated knowledge prompting 
for commonsense reasoning. In Proceedings of the 
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages 
3154-3169, Dublin, Ireland. Association for Compu-
tational Linguistics. </p>
<p>Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-
ter West, Ronan Le Bras, Yejin Choi, and Hannaneh 
Hajishirzi. 2022c. Generated knowledge prompting 
for commonsense reasoning. In Proceedings of the 
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages 
3154-3169. </p>
<p>Keerthiram Murugesan, Mattia Atzeni, Pavan Kapani-
pathi, Pushkar Shukla, Sadhana Kumaravel, Gerald 
Tesauro, Kartik Talamadupula, Mrinmaya Sachan, 
and Murray Campbell. 2021. Text-based RL Agents 
with Commonsense Knowledge: New Challenges, 
Environments and Baselines. In Thirty Fifth AAAI 
Conference on Artificial Intelligence. </p>
<p>Keerthiram Murugesan, Subhajit Chaudhury, and Kartik 
Talamadupula. 2022. Eye of the beholder: Improved 
relation generalization for text-based reinforcement 
learning agents. In Proceedings of the AAAI Con-
ference on Artificial Intelligence, volume 36, pages 
11094-11102. </p>
<p>Moin Nadeem, Anna Bethke, and Siva Reddy. 2021. 
StereoSet: Measuring stereotypical bias in pretrained 
language models. In Proceedings of the 59th Annual 
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference 
on Natural Language Processing (Volume 1: Long 
Papers), pages 5356-5371, Online. Association for 
Computational Linguistics. </p>
<p>Philip Osborne, Heido Nomm, and André Freitas. 2021. 
A survey of text games for reinforcement learning 
informed by natural language. Transactions of the 
Association for Computational Linguistics, 10:873-
887. </p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi 
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the 
limits of transfer learning with a unified text-to-text 
transformer. Journal of Machine Learning Research, 
21(140):1-67. </p>
<p>Yasaman Razeghi, Robert L Logan IV, Matt Gardner, 
and Sameer Singh. 2022. Impact of pretraining term 
frequencies on few-shot reasoning. arXiv preprint 
arXiv:2202.07206. </p>
<p>Emily Sheng, Kai-Wei Chang, Prem Natarajan, and 
Nanyun Peng. 2021. Societal biases in language 
generation: Progress and challenges. In Proceedings 
of the 59th Annual Meeting of the Association for 
Computational Linguistics and the 11th International 
Joint Conference on Natural Language Processing 
(Volume 1: Long Papers), pages 4275-4293, Online. 
Association for Computational Linguistics. </p>
<p>Emma Strubell, Ananya Ganesh, and Andrew McCal-
lum. 2019. Energy and policy considerations for 
deep learning in NLP. In Proceedings of the 57th 
Annual Meeting of the Association for Computational 
Linguistics, pages 3645-3650, Florence, Italy. Asso-
ciation for Computational Linguistics. 
Tsunehiko Tanaka, Daiki Kimura, and Michiaki Tatsu-
bori. 2022. Commonsense knowledge from scene 
graphs for textual environments. arXiv preprint 
arXiv:2210.14162. </p>
<p>Faraz Torabi, Garrett Warnell, and Peter Stone. 2018. 
Behavioral cloning from observation. In Proceed-
ings of the Twenty-Seventh International Joint Con-
ference on Artificial Intelligence, IJCAI-18, pages 
4950-4957. International Joint Conferences on Arti-
ficial Intelligence Organization. </p>
<p>Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and 
Prithviraj Ammanabrolu. 2022. Scienceworld: Is 
your agent smarter than a 5th grader? In Proceed-
ings of the 2022 Conference on Empirical Methods in 
Natural Language Processing. Association for Com-
putational Linguistics. </p>
<p>Leon Weber, Pasquale Minervini, Jannes Münchmeyer, 
Ulf Leser, and Tim Rocktäschel. 2019. NLProlog: 
Reasoning with weak unification for question answer-
ing in natural language. In Proceedings of the 57th 
Annual Meeting of the Association for Computational 
Linguistics, pages 6151-6161, Florence, Italy. Asso-
ciation for Computational Linguistics. </p>
<p>Shunyu Yao, Karthik Narasimhan, and Matthew 
Hausknecht. 2021. Reading and acting while blind-
folded: The need for semantics in text game agents. 
In Proceedings of the 2021 Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, 
pages 3097-3102, Online. Association for Computa-
tional Linguistics. </p>
<p>Shunyu Yao, Rohan Rao, Matthew Hausknecht, and 
Karthik Narasimhan. 2020. Keep CALM and ex-
plore: Language models for action generation in text-
based games. In Proceedings of the 2020 Conference 
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 8736-8754, Online. Association 
for Computational Linguistics. </p>
<p>Xingdi Yuan, Marc-Alexandre Côté, Alessandro Sor-
doni, Romain Laroche, Rémi Tachet des Combes, 
Matthew J. Hausknecht, and Adam Trischler. 2018. 
Counting to explore and generalize in text-based 
games. ArXiv, abs/1806.11525. </p>
<p>Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J 
Mankowitz, and Shie Mannor. 2018. Learn what not 
to learn: Action elimination with deep reinforcement 
learning. In NeurIPS. </p>
<p>Rowan Zellers, Ari Holtzman, Matthew Peters, Roozbeh 
Mottaghi, Aniruddha Kembhavi, Ali Farhadi, and 
Yejin Choi. 2021. PIGLeT: Language grounding 
through neuro-symbolic interaction in a 3D world. </p>
<p>In Proceedings of the 59th Annual Meeting of the 
Association for Computational Linguistics and the 
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages 
2040-2050, Online. Association for Computational 
Linguistics. </p>
<p>Xuhui Zhou, Yue Zhang, Leyang Cui, and Dandan 
Huang. 2020. Evaluating commonsense in pre-
trained language models. In Proceedings of the AAAI 
Conference on Artificial Intelligence, volume 34, 
pages 9733-9740. </p>
<p>A Appendix: Experiment Details </p>
<p>A.1 Training and evaluation sets </p>
<p>For each game, we randomly generate 100 para-
metric variations for each of the train, development, 
and test sets. To encourage and evaluate general-
ity, problems are unique across sets -for example, 
arithmetic problems (for the Arithmetic game) or 
task objects (for TWC) found in the training set are 
not found in the development or test sets. </p>
<p>Table 5 :
5An example of a Text World Common Sense 
(TWC) game. Actions for the solution are italicized. 
Relevant task objects and target locations are bolded, 
while distractor locations are underlined. Symbolic 
module actions and their responses are highlighted in 
blue. </p>
<p>Table 6 :
6An example Arithmetic game.Actions for the 
solution are italicized. Relevant task objects are bolded, 
while distractor objects are underlined. Symbolic mod-
ule actions and their responses are highlighted in blue. </p>
<p>You put the 25g of oak in the box. Inventory:Your inventory is currently empty. &gt; take 47g of brickYou take the 47g of brick. Inventory: 47g of brick &gt; put 47g of brick in box You put the 47g of brick in the box. Inventory: Your inventory is currently empty. &gt; take 15kg of cedar You take the 15kg of cedar. Inventory: 15kg of cedar &gt; put 15kg of cedar in box You put the 15kg of cedar in the box. Inventory: Your inventory is currently empty. &gt; take 21kg of marble You take the 21kg of marble. Inventory: 21kg of marble &gt; put 21kg of marble in box Game completed., are: 25 g of oak, 
47 g of brick, 15 kg of cedar, 21 kg of marble. 
Inventory: 
Your inventory is currently empty. </p>
<blockquote>
<p>take 25g of oak </p>
</blockquote>
<p>You take the 25g of oak. 
Inventory: 
25g of oak </p>
<blockquote>
<p>put 25g of oak in box </p>
</blockquote>
<p>Table 7 :
7An example Sorting game. Actions for the solution are italicized. Relevant task objects are bolded. Symbolic module actions and their responses are highlighted in blue.</p>
<p>Table 8 :
8An example of a MapReader game. Actions for the solution are italicized. The starting location and the target location are bolded. Symbolic module actions and their responses are highlighted in blue.
We release our system as open source, available at http: //github.com/cognitiveailab/neurosymbolic/
https://github.com/microsoft/tdqn
AcknowledgementsThis work supported in part by National Science Foundation (NSF) award #1815948 to PJ, and the Allen Institute for Artificial Intelligence (AI2).References</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2451 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2451</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2451</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-270062620</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.17044v3.pdf" target="_blank">Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders</a></p>
                <p><strong>Paper Abstract:</strong> The rapid growth of scientific literature makes it challenging for researchers to identify novel and impactful ideas, especially across disciplines. Modern artificial intelligence (AI) systems offer new approaches, potentially inspiring ideas not conceived by humans alone. But how compelling are these AI-generated ideas, and how can we improve their quality? Here, we introduce SciMuse, which uses 58 million research papers and a large-language model to generate research ideas. We conduct a large-scale evaluation in which over 100 research group leaders -- from natural sciences to humanities -- ranked more than 4,400 personalized ideas based on their interest. This data allows us to predict research interest using (1) supervised neural networks trained on human evaluations, and (2) unsupervised zero-shot ranking with large-language models. Our results demonstrate how future systems can help generating compelling research ideas and foster unforeseen interdisciplinary collaborations.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2451.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2451.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciMuse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciMuse (Knowledge-graph + LLM personalized idea-generation system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that combines a large, literature-derived knowledge graph (123k concepts, edges from co-occurrence in 58M papers) with large language models (GPT variants) to generate personalized interdisciplinary research project ideas and candidate collaborations, and to predict which ideas will interest specific senior researchers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciMuse</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs a knowledge graph of 123,128 scientific concepts extracted from ~2.44M preprints and edges from co-occurrence across >58M papers (OpenAlex). For each target researcher it extracts concepts from their recent papers, refines these with GPT-4, forms personalized subgraphs, selects concept-pairs (random, predicted-high-impact, or none), and prompts GPT-4 to iteratively generate/refine three project proposals (self-reflection loop: generate 3, refine twice, pick best). Also computes 144 graph-theoretic features per concept pair and trains a small supervised neural network on human interest ratings to predict high-interest ideas; separately uses LLMs in a zero-shot pairwise-comparison protocol (22k–45k comparisons) and ELO aggregation to rank suggestions without human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Hypothesis / Idea Generation System (Knowledge-graph + LLM); Automated Research Suggestion System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Broad, cross-disciplinary scientific idea generation and collaboration discovery spanning natural sciences, technology, mathematics, medicine, social sciences and humanities (personalized research project proposals and collaboration suggestions).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate personalized, novel and compelling research project proposals (often for pairwise collaborations) that connect concepts across researchers' interests; identify high-impact or otherwise interesting concept combinations and present concrete project descriptions (research questions, objectives, short paper-like proposals). Also predict which generated ideas will interest a given senior researcher.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High open-ended combinatorial complexity: knowledge graph with 123k nodes and edges from co-occurrence across 58M papers; for each researcher subgraph selection across potentially many concepts (subnetwork sizes vary), 144 features computed per pair, large search space of possible concept-pairs and textual project formulations; multi-objective tradeoffs (novelty, plausibility, fit to researcher, interdisciplinarity). Quantitative measures reported: 123,128 concepts; 58M papers to form edges; 144 computed features; final supervised predictor used 25 top features. No explicit search-space size in raw count, but combinatorial pairs imply >O(10^9) possible pairs in full graph.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Extensive pre-existing literature data: concept extraction from ~2.44M preprints (arXiv, bioRxiv, medRxiv, ChemRxiv) and edge construction from OpenAlex snapshot with ~58M papers that contain at least two concepts. Evaluation labels: 4,451 human ratings from 110 research group leaders; 2,996 suggestions were generated using concept-pairs (used for graph-feature analyses). Data quality: curated pipeline (RAKE + GPT filtering + Wikipedia + manual review) to produce a refined concept list (123,128). Data was pre-existing (no lab-generated experimental data required) and large-scale; human evaluation expensive but available for this study.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Non-trivial but unspecified compute: building and processing a knowledge graph from 58M papers and 123k concepts; computing 144 features per concept pair; running GPT-4/GPT-4o/GPT-3.5 for idea generation and tens of thousands of pairwise comparisons (22k–45k) for ELO ranking; supervised training with Monte Carlo cross-validation (130 iterations) on a small neural network (25 inputs, one hidden layer 50 neurons). The paper gives counts (e.g., 22k–45k LLM pairwise calls, 4,451 human evaluations, 2,996 graph-based suggestions) but no CPU/GPU hours or dollar costs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended and creative (ill-defined) generation task rather than single optimum search; discrete concept-pair combinatorics combined with continuous textual generation. Deterministic aspects: graph features and predicted-impact model; stochastic aspects: LLM outputs and iterative refinement. Clear human-evaluation metric exists (1–5 interest rating), but objective metrics for scientific 'impact' are only proxy (predicted future citations). Requires domain knowledge for evaluating plausibility; system uses both structural graph signals and LLM commonsense/scientific knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Human interest ratings (1–5) by senior researchers; binary threshold for 'high-interest' defined as rating 4 or 5. Secondary metrics: AUC of classifier predicting high-interest, top-N precision (fraction of top-N suggestions that humans rated high-interest), probability of ≥1 high-interest suggestion in top-N, counts/percent of suggestions rated 4/5 overall.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Human evaluation outcomes: 4,451 total responses; 1,107 (≈24.9%) received rating 4 or 5; 394 (≈8.9%) received rating 5. For graph-only supervised predictor (neural network): ROC AUC ≈ 0.645; top-1 precision reportedly up to 70% (i.e., top-1 recommendation was rated high-interest 70% of the time), top-3 precision ≈ 66.4%, top-5 precision ≈ 60.4%; probability of finding ≥1 high-interest suggestion in small top-N substantially exceeds random (random baseline top-3: 23%). For LLM zero-shot ranking (GPT-4o): AUC ≈ 0.673; top-1 precision 51.0% (GPT-4o) and 52.9% (GPT-3.5) reported in one summary; top-3 precision for GPT-4o ≈ 45.0% and GPT-3.5 ≈ 47.2% in another metric. (Paper reports multiple related metrics; overall supervised graph-based selection outperformed zero-shot LLM ranking on top-N precision, while GPT zero-shot still substantially beat random.)</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limitations observed: (1) No strong advantage of using predicted-high-impact concept pairs vs random or no-concept generation in terms of raw interest ratings—suggestions produced without KG concepts had similar interest distributions, reducing the apparent added-value in some cases. (2) Proposals built around highly connected / highly cited concepts (high PageRank, high degree, frequently cited nodes) correlated negatively with human interest — popular/well-connected concepts were less appealing. (3) Interdisciplinary distance: proposals connecting very distant fields tended to be rated lower; semantic distance negatively correlated with interest. (4) Supervised predictor limited by small labeled dataset (2,996 graph-based examples); thus constrained generalization. (5) System generates ideas but does not automatically design or run experiments—no end-to-end automated execution implemented. (6) LLM-based zero-shot ranking and generation quality depends on LLM capability; variability across model versions observed.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Factors that improved performance: (1) Use of structured literature-derived knowledge graph features (node degree, PageRank, citation metrics, semantic distances) allowed a supervised model to select higher-interest concept pairs before text generation. (2) Personalization: extracting recent paper titles and refining researcher concept lists with GPT-4 improved relevance. (3) Iterative self-refinement prompting (generate 3, refine twice, select best) for idea generation. (4) Larger/more capable LLMs improved zero-shot ranking and generation quality (paper shows GPT-4o > GPT-3.5 and GPT-o1 outperforming GPT-4o in small tests). (5) Large, expert human-evaluation dataset enabled supervised learning and robust evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Comparisons in-paper: three generation methods (no concept pair, random pair, predicted-high-impact pair) produced similar interest distributions overall (no significant difference on average). However, using the knowledge-graph features allowed the supervised model to achieve higher top-N precision than LLM zero-shot ranking (e.g., top-1 precision 70% supervised vs ~51% GPT-4o). LLM zero-shot ranking still performed substantially better than random (AUCs ≈0.67), and improved with more advanced LLM versions. Decision trees underperformed the small neural network in supervised prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human ratings from 110 senior research group leaders are the ground truth: 4,451 total suggestions rated; 1,107 (≈25%) received rating 4 or 5 and 394 (≈8.9%) rated 5. The paper uses these human ratings as the baseline to evaluate automated ranking and selection. Comparative statement: top-1 supervised recommendation matched human high-interest labels 70% of the time (i.e., when the system's top pick was presented, 70% were labeled high-interest by humans).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2451.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2451.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 / GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 family (including GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large generative pre-trained transformer language models used for concept refinement, idea/text generation (iterative self-refinement prompts) and zero-shot pairwise ranking of candidate research ideas; different versions (GPT-4, GPT-4o, GPT-o1 in small tests) showed measurable differences in generation and ranking quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 / GPT-4o (OpenAI LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used to (1) refine noisy concept lists for individual researchers, (2) generate project proposals from provided concept pairs and recent paper titles (self-reflection loop: produce 3 ideas, refine twice, choose best), and (3) perform zero-shot pairwise comparisons between candidate suggestions to produce an ELO-based ranked ordering without human labels. Version differences were tested: GPT-4 used for generation; GPT-4o and GPT-4o-mini used for zero-shot ranking; GPT-o1 and GPT-4o compared in small-scale generation tests.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Large Language Model used for Idea Generation and Zero-shot Ranking</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Cross-disciplinary scientific idea generation and personalized ranking across many research fields (natural sciences, technology, social sciences, humanities).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Produce natural-language, structured project proposals combining two concepts and researcher metadata; compare and rank existing textual suggestions for expected human interest in a zero-shot fashion based on provided researcher contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Handles open-ended, high-dimensional textual generation and comparative judgment tasks; pairwise ranking required tens of thousands of comparisons (22k–45k) to rank ~3k suggestions via ELO aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on provided textual inputs (paper titles, refined concepts, candidate suggestions) — does not require additional labeled human data for zero-shot ranking, though system performance improves with more powerful models. In experiments, models received researcher paper titles and suggestion texts; no fine-tuning performed.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Substantial API calls for idea generation and ranking: authors issued tens of thousands of pairwise LLM comparisons (22k–45k); generation used iterative prompting per suggestion. Exact compute costs not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended generation (no single ground-truth) and pairwise ordinal comparison (deterministic outputs per prompt but stochastic across calls); clear human-evaluation metric available for offline validation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>For zero-shot ranking: AUC against human labels, top-N precision (how many of top-N ranked by the LLM were rated high-interest by humans). For generation: human preference tests and direct human interest ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Zero-shot ranking performance: GPT-4o reported AUC ≈ 0.673; top-1 precision reported ~51.0% (GPT-4o) and top-3 precision ~45.0% in one set of metrics. In small-scale generation comparisons, GPT-4o was preferred over GPT-4 in 60.56% of paired comparisons (N=180 pairs, 3 group leaders), and GPT-o1 outperformed GPT-4o in a tiny test (11/11 by one evaluator).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Zero-shot ranking and generation quality depends strongly on model capability; lower-performing LLMs (older versions) gave worse top-N precision. Models can produce plausible-sounding but less interesting or less novel ideas; LLM ranking does not access human evaluations and can mis-order suggestions in ways that differ from domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Larger/more capable LLMs improve both idea generation quality and zero-shot ranking performance. Providing personalized context (recent paper titles, refined concept lists) improves relevance. Iterative self-refinement prompting helps final output quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>LLM zero-shot ranking (GPT-4o) achieved slightly higher AUC than the supervised neural network (≈0.673 vs ≈0.645 in one report), but supervised graph-feature selection had higher top-N precision (e.g., top-1 precision 70% supervised vs ~51% GPT-4o). More advanced LLM versions yielded better zero-shot ranking and generation (GPT-4o > GPT-3.5; GPT-o1 showed promising small-sample gains).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human ratings of interest used as ground truth; GPT-4o zero-shot ranking AUC ≈0.673 relative to human labels. On human preference pair tests, GPT-4o's outputs were preferred to GPT-4's in small-sample comparisons (60.56% preference in one test).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2451.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2451.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (OpenAI LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier generation large language model used here for zero-shot pairwise ranking of candidate research suggestions; served as a baseline LLM in comparisons with GPT-4-family models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Employed in zero-shot pairwise-comparison ranking of candidate suggestions (same protocol as for GPT-4-family) to evaluate how older LLMs perform at ranking proposals relative to human interest judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Large Language Model used for Zero-shot Ranking</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Personalized ranking of multi-disciplinary research idea texts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Compare pairs of textual proposals and decide which is likely to be more interesting to a given researcher, repeated many times to create an ELO ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Same open-ended comparative judgment as for other LLMs; subject to stochastic variability in responses.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on the provided textual proposals and researcher context; no fine-tuning or labeled training required.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Thousands of pairwise API calls (exact count not specified for GPT-3.5 in main text), ELO aggregation across ~3k items.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Pairwise ordinal comparisons aggregated into ranking; deterministic prompt with stochastic model outputs possible.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>AUC vs human interest labels; top-N precision.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported top-1 precision ≈ 52.9% for GPT-3.5 in one summary; top-3 precision ≈ 47.2% in another metric. AUC not explicitly separated from GPT-4o in some summaries but reported as lower than GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Lower top-N precision and overall ranking quality compared to newer LLMs (GPT-4o); more inconsistent preferences compared to human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Providing explicit personalized researcher context (paper titles) improved performance; larger context windows and model size would likely help.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Underperforms GPT-4o for zero-shot ranking (lower top-N precision), but still substantially better than random selection; shows that even mid-tier LLMs can provide useful zero-shot ranking signals.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Compared against human labels; achieved ~52.9% top-1 precision vs supervised top-1 ~70% in reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2451.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2451.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-driving laboratories</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-driving laboratories for chemistry and materials science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of automated experimental platforms (robotic labs + optimization/decision algorithms) that can design, run, analyze, and iterate on physical experiments with minimal human intervention; cited here as an example of automated execution complementary to automated idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-driving laboratories for chemistry and materials science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-driving laboratories (automated experimentation platforms)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Robotic/automated lab systems that program and execute experiments (e.g., chemical syntheses or materials characterization), incorporate in-line analysis, and use active learning/optimization loops to select next experiments to run, thereby accelerating discovery and reducing human labor.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Experimentation Platform / Self-driving Laboratory</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Chemistry and materials science (automated synthesis, characterization, optimization of material properties and reaction conditions).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automate experimental design and execution to search continuous/discrete experimental parameter spaces (reaction conditions, composition, processing) for target properties (yield, stability, conductivity, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High-dimensional continuous/discrete experimental spaces, noisy stochastic outcomes, multi-objective optimization, real-world constraints (instrumentation, sample prep), and expensive-to-evaluate experiments (time and cost per run).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relies on both pre-existing literature/experimental databases and newly generated experimental data. The paper references this area but does not itself implement such platforms.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Requires integration of experiment control software, active learning algorithms, and often substantial compute for data analysis and optimization; specific resource use varies by implementation and is not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined experimental parameter spaces with stochastic outcomes and clear evaluation metrics (measured properties), but also open-ended with multiple objectives; requires instrumentation and domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Discovery speed, number of promising candidates found, reduction in experimental runs, improvement in target properties, published discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not provided in this paper; cited as example from literature demonstrating successful autonomous experimentation but not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Practical challenges include integration with lab hardware, transferability to real-world experimental noise, and limitation to problems amenable to robotics and measurable readouts.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Clear, automatable experiment protocols, reliable in-line measurements, and well-structured optimization objectives favor success.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Mentioned as part of discussion about future possibilities (end-to-end automation from idea generation to execution). No direct comparative experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Humans currently provide core scientific ideas in many labs; self-driving labs have been shown elsewhere to accelerate iterative optimization vs manual experimentation, but no quantitative baseline present here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2451.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2451.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autonomous chemical research with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autonomous chemical research with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recent work (cited) where LLMs are integrated into autonomous chemical research pipelines to assist or automate the design and execution of chemical experiments, representing an example of end-to-end automation complementary to SciMuse's idea-generation focus.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous chemical research with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Autonomous chemical research with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Systems that combine LLM reasoning, chemistry-specific tool augmentation, and laboratory automation to propose experiments, translate proposals into machine-executable protocols, and in some cases execute iterations automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Discovery System / Autonomous Experimentation with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Chemistry (reaction discovery, optimization), materials synthesis and characterization where textual-to-protocol translation is required.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Translate high-level experimental ideas into concrete, machine-executable protocols and in some implementations close the loop by executing experiments and re-analyzing results using model-guided selection.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: converting textual scientific ideas to safe, instrument-specific protocols; dealing with chemical safety, reaction kinetics, and noisy experimental outcomes; multi-step workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relies on published chemical knowledge, reaction databases, and newly generated lab data; safety-critical data requirements are high.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Combines LLM inference with experimental control systems; compute for LLMs is substantial but experimental cost dominates; this paper references such works but does not implement them.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Partially well-defined when protocols are standardizable, otherwise open-ended; requires deterministic translation to instrument commands and careful safety checks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Successful automation measured by novel reactions discovered, optimization targets met, reduced human intervention, and published validated results (references show positive examples).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not quantified in this paper; cited references (e.g., Nature 2023 work) demonstrate successful autonomous chemical research instances, but no aggregate success rate is provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Challenges include safety, protocol translation errors, domain-specific constraints, and the gap between plausible textual suggestion and lab-executable protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Domain-specific tooling, chemistry-aware augmentations, and reliable instrumentation are key enabling factors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Mentioned as potential complement/future extension of idea-generation systems like SciMuse to enable end-to-end automated science; no direct experiments in the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human chemists currently validate and safety-check protocol translations; referenced works indicate LLM-augmented autonomous systems can match or exceed some manual workflows in throughput or exploration speed in demonstration settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2451.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2451.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced recent system that iteratively generates research hypotheses over literature using LLMs; cited by the authors as related work in automated idea/hypothesis generation from the scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ResearchAgent (iterative LLM-based literature idea generator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM-based pipeline that iteratively processes scientific literature to propose hypotheses, refine them, and possibly recommend next steps; cited as an example of automated hypothesis/idea generation systems in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Hypothesis/Idea Generation System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Automated generation of research hypotheses and ideas using scientific literature (likely multi-domain).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Analyze literature to produce candidate hypotheses or research directions, iteratively refine proposals using LLM feedback loops.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Open-ended natural language inference and literature synthesis across many domains; complexity depends on corpus size and task framing.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on large literature corpora; the referenced work likely uses open literature datasets but this paper only cites it.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>LLM inference and iterative prompt cycles; exact resource use not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Iterative and creative; requires mechanisms to assess novelty and plausibility of generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Human evaluation, novelty/impact proxies, or downstream experimental validation in referenced works (not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not provided in this paper (only mentioned in related-work references).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>General LLM limitations (hallucination, lack of experimental grounding) and need for human validation; specifics are in the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Good literature coverage, effective prompt / self-refinement strategies, and human-in-the-loop evaluation improve outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Mentioned among other contemporary efforts; direct comparisons not performed in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not specified here; referenced work may include human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>Self-driving laboratories for chemistry and materials science <em>(Rating: 2)</em></li>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 2)</em></li>
                <li>Agatha: automatic graph mining and transformer based hypothesis generation approach <em>(Rating: 2)</em></li>
                <li>Scimon: Scientific inspiration machines optimized for novelty <em>(Rating: 2)</em></li>
                <li>Forecasting high-impact research topics via machine learning on evolving knowledge graphs <em>(Rating: 2)</em></li>
                <li>Large language models for automated open-domain scientific hypotheses discovery <em>(Rating: 1)</em></li>
                <li>Predicting research trends with semantic and neural networks with an application in quantum physics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2451",
    "paper_id": "paper-270062620",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "SciMuse",
            "name_full": "SciMuse (Knowledge-graph + LLM personalized idea-generation system)",
            "brief_description": "A system that combines a large, literature-derived knowledge graph (123k concepts, edges from co-occurrence in 58M papers) with large language models (GPT variants) to generate personalized interdisciplinary research project ideas and candidate collaborations, and to predict which ideas will interest specific senior researchers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SciMuse",
            "system_description": "Constructs a knowledge graph of 123,128 scientific concepts extracted from ~2.44M preprints and edges from co-occurrence across &gt;58M papers (OpenAlex). For each target researcher it extracts concepts from their recent papers, refines these with GPT-4, forms personalized subgraphs, selects concept-pairs (random, predicted-high-impact, or none), and prompts GPT-4 to iteratively generate/refine three project proposals (self-reflection loop: generate 3, refine twice, pick best). Also computes 144 graph-theoretic features per concept pair and trains a small supervised neural network on human interest ratings to predict high-interest ideas; separately uses LLMs in a zero-shot pairwise-comparison protocol (22k–45k comparisons) and ELO aggregation to rank suggestions without human labels.",
            "system_type": "Hypothesis / Idea Generation System (Knowledge-graph + LLM); Automated Research Suggestion System",
            "problem_domain": "Broad, cross-disciplinary scientific idea generation and collaboration discovery spanning natural sciences, technology, mathematics, medicine, social sciences and humanities (personalized research project proposals and collaboration suggestions).",
            "problem_description": "Generate personalized, novel and compelling research project proposals (often for pairwise collaborations) that connect concepts across researchers' interests; identify high-impact or otherwise interesting concept combinations and present concrete project descriptions (research questions, objectives, short paper-like proposals). Also predict which generated ideas will interest a given senior researcher.",
            "problem_complexity": "High open-ended combinatorial complexity: knowledge graph with 123k nodes and edges from co-occurrence across 58M papers; for each researcher subgraph selection across potentially many concepts (subnetwork sizes vary), 144 features computed per pair, large search space of possible concept-pairs and textual project formulations; multi-objective tradeoffs (novelty, plausibility, fit to researcher, interdisciplinarity). Quantitative measures reported: 123,128 concepts; 58M papers to form edges; 144 computed features; final supervised predictor used 25 top features. No explicit search-space size in raw count, but combinatorial pairs imply &gt;O(10^9) possible pairs in full graph.",
            "data_availability": "Extensive pre-existing literature data: concept extraction from ~2.44M preprints (arXiv, bioRxiv, medRxiv, ChemRxiv) and edge construction from OpenAlex snapshot with ~58M papers that contain at least two concepts. Evaluation labels: 4,451 human ratings from 110 research group leaders; 2,996 suggestions were generated using concept-pairs (used for graph-feature analyses). Data quality: curated pipeline (RAKE + GPT filtering + Wikipedia + manual review) to produce a refined concept list (123,128). Data was pre-existing (no lab-generated experimental data required) and large-scale; human evaluation expensive but available for this study.",
            "computational_requirements": "Non-trivial but unspecified compute: building and processing a knowledge graph from 58M papers and 123k concepts; computing 144 features per concept pair; running GPT-4/GPT-4o/GPT-3.5 for idea generation and tens of thousands of pairwise comparisons (22k–45k) for ELO ranking; supervised training with Monte Carlo cross-validation (130 iterations) on a small neural network (25 inputs, one hidden layer 50 neurons). The paper gives counts (e.g., 22k–45k LLM pairwise calls, 4,451 human evaluations, 2,996 graph-based suggestions) but no CPU/GPU hours or dollar costs.",
            "problem_structure": "Open-ended and creative (ill-defined) generation task rather than single optimum search; discrete concept-pair combinatorics combined with continuous textual generation. Deterministic aspects: graph features and predicted-impact model; stochastic aspects: LLM outputs and iterative refinement. Clear human-evaluation metric exists (1–5 interest rating), but objective metrics for scientific 'impact' are only proxy (predicted future citations). Requires domain knowledge for evaluating plausibility; system uses both structural graph signals and LLM commonsense/scientific knowledge.",
            "success_metric": "Human interest ratings (1–5) by senior researchers; binary threshold for 'high-interest' defined as rating 4 or 5. Secondary metrics: AUC of classifier predicting high-interest, top-N precision (fraction of top-N suggestions that humans rated high-interest), probability of ≥1 high-interest suggestion in top-N, counts/percent of suggestions rated 4/5 overall.",
            "success_rate": "Human evaluation outcomes: 4,451 total responses; 1,107 (≈24.9%) received rating 4 or 5; 394 (≈8.9%) received rating 5. For graph-only supervised predictor (neural network): ROC AUC ≈ 0.645; top-1 precision reportedly up to 70% (i.e., top-1 recommendation was rated high-interest 70% of the time), top-3 precision ≈ 66.4%, top-5 precision ≈ 60.4%; probability of finding ≥1 high-interest suggestion in small top-N substantially exceeds random (random baseline top-3: 23%). For LLM zero-shot ranking (GPT-4o): AUC ≈ 0.673; top-1 precision 51.0% (GPT-4o) and 52.9% (GPT-3.5) reported in one summary; top-3 precision for GPT-4o ≈ 45.0% and GPT-3.5 ≈ 47.2% in another metric. (Paper reports multiple related metrics; overall supervised graph-based selection outperformed zero-shot LLM ranking on top-N precision, while GPT zero-shot still substantially beat random.)",
            "failure_modes": "Limitations observed: (1) No strong advantage of using predicted-high-impact concept pairs vs random or no-concept generation in terms of raw interest ratings—suggestions produced without KG concepts had similar interest distributions, reducing the apparent added-value in some cases. (2) Proposals built around highly connected / highly cited concepts (high PageRank, high degree, frequently cited nodes) correlated negatively with human interest — popular/well-connected concepts were less appealing. (3) Interdisciplinary distance: proposals connecting very distant fields tended to be rated lower; semantic distance negatively correlated with interest. (4) Supervised predictor limited by small labeled dataset (2,996 graph-based examples); thus constrained generalization. (5) System generates ideas but does not automatically design or run experiments—no end-to-end automated execution implemented. (6) LLM-based zero-shot ranking and generation quality depends on LLM capability; variability across model versions observed.",
            "success_factors": "Factors that improved performance: (1) Use of structured literature-derived knowledge graph features (node degree, PageRank, citation metrics, semantic distances) allowed a supervised model to select higher-interest concept pairs before text generation. (2) Personalization: extracting recent paper titles and refining researcher concept lists with GPT-4 improved relevance. (3) Iterative self-refinement prompting (generate 3, refine twice, select best) for idea generation. (4) Larger/more capable LLMs improved zero-shot ranking and generation quality (paper shows GPT-4o &gt; GPT-3.5 and GPT-o1 outperforming GPT-4o in small tests). (5) Large, expert human-evaluation dataset enabled supervised learning and robust evaluation.",
            "comparative_results": "Comparisons in-paper: three generation methods (no concept pair, random pair, predicted-high-impact pair) produced similar interest distributions overall (no significant difference on average). However, using the knowledge-graph features allowed the supervised model to achieve higher top-N precision than LLM zero-shot ranking (e.g., top-1 precision 70% supervised vs ~51% GPT-4o). LLM zero-shot ranking still performed substantially better than random (AUCs ≈0.67), and improved with more advanced LLM versions. Decision trees underperformed the small neural network in supervised prediction.",
            "human_baseline": "Human ratings from 110 senior research group leaders are the ground truth: 4,451 total suggestions rated; 1,107 (≈25%) received rating 4 or 5 and 394 (≈8.9%) rated 5. The paper uses these human ratings as the baseline to evaluate automated ranking and selection. Comparative statement: top-1 supervised recommendation matched human high-interest labels 70% of the time (i.e., when the system's top pick was presented, 70% were labeled high-interest by humans).",
            "uuid": "e2451.0",
            "source_info": {
                "paper_title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4 / GPT-4o",
            "name_full": "GPT-4 family (including GPT-4o)",
            "brief_description": "Large generative pre-trained transformer language models used for concept refinement, idea/text generation (iterative self-refinement prompts) and zero-shot pairwise ranking of candidate research ideas; different versions (GPT-4, GPT-4o, GPT-o1 in small tests) showed measurable differences in generation and ranking quality.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4 / GPT-4o (OpenAI LLM)",
            "system_description": "Used to (1) refine noisy concept lists for individual researchers, (2) generate project proposals from provided concept pairs and recent paper titles (self-reflection loop: produce 3 ideas, refine twice, choose best), and (3) perform zero-shot pairwise comparisons between candidate suggestions to produce an ELO-based ranked ordering without human labels. Version differences were tested: GPT-4 used for generation; GPT-4o and GPT-4o-mini used for zero-shot ranking; GPT-o1 and GPT-4o compared in small-scale generation tests.",
            "system_type": "Large Language Model used for Idea Generation and Zero-shot Ranking",
            "problem_domain": "Cross-disciplinary scientific idea generation and personalized ranking across many research fields (natural sciences, technology, social sciences, humanities).",
            "problem_description": "Produce natural-language, structured project proposals combining two concepts and researcher metadata; compare and rank existing textual suggestions for expected human interest in a zero-shot fashion based on provided researcher contexts.",
            "problem_complexity": "Handles open-ended, high-dimensional textual generation and comparative judgment tasks; pairwise ranking required tens of thousands of comparisons (22k–45k) to rank ~3k suggestions via ELO aggregation.",
            "data_availability": "Operates on provided textual inputs (paper titles, refined concepts, candidate suggestions) — does not require additional labeled human data for zero-shot ranking, though system performance improves with more powerful models. In experiments, models received researcher paper titles and suggestion texts; no fine-tuning performed.",
            "computational_requirements": "Substantial API calls for idea generation and ranking: authors issued tens of thousands of pairwise LLM comparisons (22k–45k); generation used iterative prompting per suggestion. Exact compute costs not reported.",
            "problem_structure": "Open-ended generation (no single ground-truth) and pairwise ordinal comparison (deterministic outputs per prompt but stochastic across calls); clear human-evaluation metric available for offline validation.",
            "success_metric": "For zero-shot ranking: AUC against human labels, top-N precision (how many of top-N ranked by the LLM were rated high-interest by humans). For generation: human preference tests and direct human interest ratings.",
            "success_rate": "Zero-shot ranking performance: GPT-4o reported AUC ≈ 0.673; top-1 precision reported ~51.0% (GPT-4o) and top-3 precision ~45.0% in one set of metrics. In small-scale generation comparisons, GPT-4o was preferred over GPT-4 in 60.56% of paired comparisons (N=180 pairs, 3 group leaders), and GPT-o1 outperformed GPT-4o in a tiny test (11/11 by one evaluator).",
            "failure_modes": "Zero-shot ranking and generation quality depends strongly on model capability; lower-performing LLMs (older versions) gave worse top-N precision. Models can produce plausible-sounding but less interesting or less novel ideas; LLM ranking does not access human evaluations and can mis-order suggestions in ways that differ from domain experts.",
            "success_factors": "Larger/more capable LLMs improve both idea generation quality and zero-shot ranking performance. Providing personalized context (recent paper titles, refined concept lists) improves relevance. Iterative self-refinement prompting helps final output quality.",
            "comparative_results": "LLM zero-shot ranking (GPT-4o) achieved slightly higher AUC than the supervised neural network (≈0.673 vs ≈0.645 in one report), but supervised graph-feature selection had higher top-N precision (e.g., top-1 precision 70% supervised vs ~51% GPT-4o). More advanced LLM versions yielded better zero-shot ranking and generation (GPT-4o &gt; GPT-3.5; GPT-o1 showed promising small-sample gains).",
            "human_baseline": "Human ratings of interest used as ground truth; GPT-4o zero-shot ranking AUC ≈0.673 relative to human labels. On human preference pair tests, GPT-4o's outputs were preferred to GPT-4's in small-sample comparisons (60.56% preference in one test).",
            "uuid": "e2451.1",
            "source_info": {
                "paper_title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (OpenAI LLM)",
            "brief_description": "An earlier generation large language model used here for zero-shot pairwise ranking of candidate research suggestions; served as a baseline LLM in comparisons with GPT-4-family models.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-3.5",
            "system_description": "Employed in zero-shot pairwise-comparison ranking of candidate suggestions (same protocol as for GPT-4-family) to evaluate how older LLMs perform at ranking proposals relative to human interest judgments.",
            "system_type": "Large Language Model used for Zero-shot Ranking",
            "problem_domain": "Personalized ranking of multi-disciplinary research idea texts.",
            "problem_description": "Compare pairs of textual proposals and decide which is likely to be more interesting to a given researcher, repeated many times to create an ELO ranking.",
            "problem_complexity": "Same open-ended comparative judgment as for other LLMs; subject to stochastic variability in responses.",
            "data_availability": "Operates on the provided textual proposals and researcher context; no fine-tuning or labeled training required.",
            "computational_requirements": "Thousands of pairwise API calls (exact count not specified for GPT-3.5 in main text), ELO aggregation across ~3k items.",
            "problem_structure": "Pairwise ordinal comparisons aggregated into ranking; deterministic prompt with stochastic model outputs possible.",
            "success_metric": "AUC vs human interest labels; top-N precision.",
            "success_rate": "Reported top-1 precision ≈ 52.9% for GPT-3.5 in one summary; top-3 precision ≈ 47.2% in another metric. AUC not explicitly separated from GPT-4o in some summaries but reported as lower than GPT-4o.",
            "failure_modes": "Lower top-N precision and overall ranking quality compared to newer LLMs (GPT-4o); more inconsistent preferences compared to human labels.",
            "success_factors": "Providing explicit personalized researcher context (paper titles) improved performance; larger context windows and model size would likely help.",
            "comparative_results": "Underperforms GPT-4o for zero-shot ranking (lower top-N precision), but still substantially better than random selection; shows that even mid-tier LLMs can provide useful zero-shot ranking signals.",
            "human_baseline": "Compared against human labels; achieved ~52.9% top-1 precision vs supervised top-1 ~70% in reported comparisons.",
            "uuid": "e2451.2",
            "source_info": {
                "paper_title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Self-driving laboratories",
            "name_full": "Self-driving laboratories for chemistry and materials science",
            "brief_description": "A class of automated experimental platforms (robotic labs + optimization/decision algorithms) that can design, run, analyze, and iterate on physical experiments with minimal human intervention; cited here as an example of automated execution complementary to automated idea generation.",
            "citation_title": "Self-driving laboratories for chemistry and materials science",
            "mention_or_use": "mention",
            "system_name": "Self-driving laboratories (automated experimentation platforms)",
            "system_description": "Robotic/automated lab systems that program and execute experiments (e.g., chemical syntheses or materials characterization), incorporate in-line analysis, and use active learning/optimization loops to select next experiments to run, thereby accelerating discovery and reducing human labor.",
            "system_type": "Automated Experimentation Platform / Self-driving Laboratory",
            "problem_domain": "Chemistry and materials science (automated synthesis, characterization, optimization of material properties and reaction conditions).",
            "problem_description": "Automate experimental design and execution to search continuous/discrete experimental parameter spaces (reaction conditions, composition, processing) for target properties (yield, stability, conductivity, etc.).",
            "problem_complexity": "High-dimensional continuous/discrete experimental spaces, noisy stochastic outcomes, multi-objective optimization, real-world constraints (instrumentation, sample prep), and expensive-to-evaluate experiments (time and cost per run).",
            "data_availability": "Relies on both pre-existing literature/experimental databases and newly generated experimental data. The paper references this area but does not itself implement such platforms.",
            "computational_requirements": "Requires integration of experiment control software, active learning algorithms, and often substantial compute for data analysis and optimization; specific resource use varies by implementation and is not quantified in this paper.",
            "problem_structure": "Well-defined experimental parameter spaces with stochastic outcomes and clear evaluation metrics (measured properties), but also open-ended with multiple objectives; requires instrumentation and domain knowledge.",
            "success_metric": "Discovery speed, number of promising candidates found, reduction in experimental runs, improvement in target properties, published discoveries.",
            "success_rate": "Not provided in this paper; cited as example from literature demonstrating successful autonomous experimentation but not evaluated here.",
            "failure_modes": "Practical challenges include integration with lab hardware, transferability to real-world experimental noise, and limitation to problems amenable to robotics and measurable readouts.",
            "success_factors": "Clear, automatable experiment protocols, reliable in-line measurements, and well-structured optimization objectives favor success.",
            "comparative_results": "Mentioned as part of discussion about future possibilities (end-to-end automation from idea generation to execution). No direct comparative experiments in this paper.",
            "human_baseline": "Humans currently provide core scientific ideas in many labs; self-driving labs have been shown elsewhere to accelerate iterative optimization vs manual experimentation, but no quantitative baseline present here.",
            "uuid": "e2451.3",
            "source_info": {
                "paper_title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Autonomous chemical research with LLMs",
            "name_full": "Autonomous chemical research with large language models",
            "brief_description": "Recent work (cited) where LLMs are integrated into autonomous chemical research pipelines to assist or automate the design and execution of chemical experiments, representing an example of end-to-end automation complementary to SciMuse's idea-generation focus.",
            "citation_title": "Autonomous chemical research with large language models",
            "mention_or_use": "mention",
            "system_name": "Autonomous chemical research with LLMs",
            "system_description": "Systems that combine LLM reasoning, chemistry-specific tool augmentation, and laboratory automation to propose experiments, translate proposals into machine-executable protocols, and in some cases execute iterations automatically.",
            "system_type": "Automated Discovery System / Autonomous Experimentation with LLMs",
            "problem_domain": "Chemistry (reaction discovery, optimization), materials synthesis and characterization where textual-to-protocol translation is required.",
            "problem_description": "Translate high-level experimental ideas into concrete, machine-executable protocols and in some implementations close the loop by executing experiments and re-analyzing results using model-guided selection.",
            "problem_complexity": "High: converting textual scientific ideas to safe, instrument-specific protocols; dealing with chemical safety, reaction kinetics, and noisy experimental outcomes; multi-step workflows.",
            "data_availability": "Relies on published chemical knowledge, reaction databases, and newly generated lab data; safety-critical data requirements are high.",
            "computational_requirements": "Combines LLM inference with experimental control systems; compute for LLMs is substantial but experimental cost dominates; this paper references such works but does not implement them.",
            "problem_structure": "Partially well-defined when protocols are standardizable, otherwise open-ended; requires deterministic translation to instrument commands and careful safety checks.",
            "success_metric": "Successful automation measured by novel reactions discovered, optimization targets met, reduced human intervention, and published validated results (references show positive examples).",
            "success_rate": "Not quantified in this paper; cited references (e.g., Nature 2023 work) demonstrate successful autonomous chemical research instances, but no aggregate success rate is provided here.",
            "failure_modes": "Challenges include safety, protocol translation errors, domain-specific constraints, and the gap between plausible textual suggestion and lab-executable protocol.",
            "success_factors": "Domain-specific tooling, chemistry-aware augmentations, and reliable instrumentation are key enabling factors.",
            "comparative_results": "Mentioned as potential complement/future extension of idea-generation systems like SciMuse to enable end-to-end automated science; no direct experiments in the present paper.",
            "human_baseline": "Human chemists currently validate and safety-check protocol translations; referenced works indicate LLM-augmented autonomous systems can match or exceed some manual workflows in throughput or exploration speed in demonstration settings.",
            "uuid": "e2451.4",
            "source_info": {
                "paper_title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ResearchAgent",
            "name_full": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "brief_description": "A referenced recent system that iteratively generates research hypotheses over literature using LLMs; cited by the authors as related work in automated idea/hypothesis generation from the scientific literature.",
            "citation_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "mention_or_use": "mention",
            "system_name": "ResearchAgent (iterative LLM-based literature idea generator)",
            "system_description": "An LLM-based pipeline that iteratively processes scientific literature to propose hypotheses, refine them, and possibly recommend next steps; cited as an example of automated hypothesis/idea generation systems in the literature.",
            "system_type": "Hypothesis/Idea Generation System",
            "problem_domain": "Automated generation of research hypotheses and ideas using scientific literature (likely multi-domain).",
            "problem_description": "Analyze literature to produce candidate hypotheses or research directions, iteratively refine proposals using LLM feedback loops.",
            "problem_complexity": "Open-ended natural language inference and literature synthesis across many domains; complexity depends on corpus size and task framing.",
            "data_availability": "Operates on large literature corpora; the referenced work likely uses open literature datasets but this paper only cites it.",
            "computational_requirements": "LLM inference and iterative prompt cycles; exact resource use not specified here.",
            "problem_structure": "Iterative and creative; requires mechanisms to assess novelty and plausibility of generated hypotheses.",
            "success_metric": "Human evaluation, novelty/impact proxies, or downstream experimental validation in referenced works (not detailed in this paper).",
            "success_rate": "Not provided in this paper (only mentioned in related-work references).",
            "failure_modes": "General LLM limitations (hallucination, lack of experimental grounding) and need for human validation; specifics are in the cited paper.",
            "success_factors": "Good literature coverage, effective prompt / self-refinement strategies, and human-in-the-loop evaluation improve outcomes.",
            "comparative_results": "Mentioned among other contemporary efforts; direct comparisons not performed in this manuscript.",
            "human_baseline": "Not specified here; referenced work may include human evaluations.",
            "uuid": "e2451.5",
            "source_info": {
                "paper_title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2,
            "sanitized_title": "autonomous_chemical_research_with_large_language_models"
        },
        {
            "paper_title": "Self-driving laboratories for chemistry and materials science",
            "rating": 2,
            "sanitized_title": "selfdriving_laboratories_for_chemistry_and_materials_science"
        },
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "rating": 2,
            "sanitized_title": "researchagent_iterative_research_idea_generation_over_scientific_literature_with_large_language_models"
        },
        {
            "paper_title": "Agatha: automatic graph mining and transformer based hypothesis generation approach",
            "rating": 2,
            "sanitized_title": "agatha_automatic_graph_mining_and_transformer_based_hypothesis_generation_approach"
        },
        {
            "paper_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "rating": 2,
            "sanitized_title": "scimon_scientific_inspiration_machines_optimized_for_novelty"
        },
        {
            "paper_title": "Forecasting high-impact research topics via machine learning on evolving knowledge graphs",
            "rating": 2,
            "sanitized_title": "forecasting_highimpact_research_topics_via_machine_learning_on_evolving_knowledge_graphs"
        },
        {
            "paper_title": "Large language models for automated open-domain scientific hypotheses discovery",
            "rating": 1,
            "sanitized_title": "large_language_models_for_automated_opendomain_scientific_hypotheses_discovery"
        },
        {
            "paper_title": "Predicting research trends with semantic and neural networks with an application in quantum physics",
            "rating": 1,
            "sanitized_title": "predicting_research_trends_with_semantic_and_neural_networks_with_an_application_in_quantum_physics"
        }
    ],
    "cost": 0.01864525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders
7 Jan 2025</p>
<p>Xuemei Gu xuemei.gu@mpl.mpg.de 
Max Planck Institute for the Science of Light
Staudtstrasse 291058ErlangenGermany</p>
<p>Mario Krenn mario.krenn@mpl.mpg.de 
Max Planck Institute for the Science of Light
Staudtstrasse 291058ErlangenGermany</p>
<p>Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders
7 Jan 20250C809F4C3A04C4A126E74964855C0AFDarXiv:2405.17044v3[cs.AI]
The rapid growth of scientific literature makes it challenging for researchers to identify novel and impactful ideas, especially across disciplines.Modern artificial intelligence (AI) systems offer new approaches, potentially inspiring ideas not conceived by humans alone.But how compelling are these AI-generated ideas, and how can we improve their quality?Here, we introduce SciMuse, which uses 58 million research papers and a large-language model to generate research ideas.We conduct a large-scale evaluation in which over 100 research group leaders -from natural sciences to humanities -ranked more than 4,400 personalized ideas based on their interest.This data allows us to predict research interest using (1) supervised neural networks trained on human evaluations, and (2) unsupervised zero-shot ranking with large-language models.Our results demonstrate how future systems can help generating compelling research ideas and foster unforeseen interdisciplinary collaborations.</p>
<p>INTRODUCTION</p>
<p>An interesting idea is often at the heart of successful research projects, crucial for their success and impact.However, with the accelerating growth in the number of scientific papers published each year [1][2][3], it becomes increasingly difficult for researchers to uncover novel and interesting ideas.This challenge is even more pronounced for those seeking interdisciplinary collaborations, who have to navigate an overwhelming volume of literature.</p>
<p>Automated systems that extract insights from millions of scientific papers present a promising solution [2,4,5].Recent advances have demonstrated that analyzing relationships between research topics across vast scientific literature can reliably predict future research directions [6][7][8][9][10], forecast the potential impact of emerging work [11,12], and identify unconventional avenues for discovery [13].With the advent of powerful large-language models (LLMs), it is now possible to leverage knowledge from millions of scientific papers to generate concrete research ideas [14][15][16].</p>
<p>Yet, a crucial question remains: Are AI-generated research ideas compelling to experienced scientists?Previous studies have only conducted small-scale evaluations with six natural language processing (NLP) PhD students [14], three social science PhD students [15] and ten PhD students in computer science and biomedicine [16].However, perspectives from experienced researchers -who define and evaluate research projects through grant applications and shape their group's research agendaare essential for assessing the value of new ideas.Involving a larger group of more experienced evaluators could offer deeper insights into what makes a research idea compelling, how to generate and predict them.</p>
<p>Here, we introduce SciMuse, a system designed to suggest new personalized research ideas for individual scientists or collaborations.By using 58 million papers and their citation history, and leveraging automated access to GPT-4 [17], SciMuse formulates comprehensive research suggestions.The suggestions were evaluated by more than 100 research group leaders from the Max Planck Society across natural sciences and technology (e.g., from the Institutes for Biogeochemistry, Astrophysics, Quantum Optics, and Intelligent Systems) as well as social sciences and humanities (e.g., from the Institutes for Geoanthropology, Demographic Research, and Human Development).These experienced researchers rank the interest-level of more than 4,400 research ideas generated by SciMuse.This large dataset not only allows us to identify connections between properties of ideas and their interest-level, but also enables us to accurately predict the level of interest of new ideas with two fundamentally different methods: (1) training supervised neural networks and (2) using LLMs for zero-shot prediction without access to human evaluations, which will be important when expensive human-expert data is unavailable.Our results highlight SciMuse's potential to suggest compelling research directions and collaborations, revealing opportunities that might not be readily apparent and positioning AI as a source of inspiration in scientific discovery [18][19][20][21].</p>
<p>RESULTS</p>
<p>Knowledge graph generation -While we could directly use publicly available large language models such as GPT-4 [17] or Gemini [26] or Claude [27] to suggest new research ideas and collaborations, our control over the generated ideas would be limited to the structure of the prompt.Therefore, we decided to build a large knowledge graph from the scientific literature to identify the personalized research interests of scientists.</p>
<p>The knowledge graph, depicted in Fig. 1(a), consists of vertices, representing scientific concepts, and edges are drawn when two concepts jointly appear in a title or abstract of a scientific paper.The concept list is generated from the titles and abstracts of approximately Fig. 1.SciMuse suggests research ideas or collaborations using a knowledge graph and GPT-4.(a), Knowledge graph generation.Nodes represent scientific concepts extracted from 2.44 million paper titles and abstracts using the RAKE algorithm [22], further refined with custom NLP techniques, manual review, GPT, and Wikipedia (to restore mistakenly removed concepts), resulting in a final list of 123,128 concepts.Edges are formed when two concepts co-occur in titles or abstracts of over 58 million papers from OpenAlex [23], augmented with citation data as a proxy for impact.A mini-knowledge graph illustrates the connections for two example papers [24,25].(b), AI-generated research collaborations.We extract concepts from the publications of Researchers A and B, refine them using GPT-4, and identify relevant sub-networks in the knowledge graph.GPT-4 then uses these concept pairs, along with the researchers' research information, to generate personalized research ideas or collaboration projects.</p>
<p>2.44 million papers from arXiv, bioRxiv, ChemRxiv, and medRxiv, with a data cutoff in February 2023.Rapid Automatic Key-word Extraction (RAKE) algorithm based on statistical text analysis is used to extract candidate concepts [22].Those candidates are further refined using GPT, Wikipedia, and human annotators, resulting in 123,128 concepts in the natural and social sciences.We then use more than 58 million scientific papers from the open-source database OpenAlex [23] to create edges.These edges contain information about the co-occurrence of concepts in scientific papers (in titles and abstracts) and their subsequent citation rates.This new knowledge graph representation was recently introduced in [12] to predict the impact of future research topics.As a result, we have an evolving knowledge graph that captures part of the evolution of science from 1665 (a text by Robert Hooke on the observation of a great spot on Jupiter [29]) to April 2023.Details of the knowledge graph generation are shown in Fig. 1(a) and Supplementary Information.</p>
<p>Personalized research suggestions -We generate personalized research proposals for collaborations between two Max Planck Society group leaders, with one researcher evaluating the AI-suggested proposal.</p>
<p>To generate suggestions (Fig. 1(b)), we first identify each researcher's interests by analyzing all their publications from the past two years.Specifically, we extract their concepts from the titles and abstracts of these papers using the full concept list shown in Fig. 1(a).The extracted concepts are further refined by GPT-4, allowing us to build personalized subgraphs in the knowledge graph for each researcher.</p>
<p>With the researchers' subgraphs, we then generate a prompt for GPT-4 to create a research project (details in the Supplementary Information).In the prompt, we provide the titles of up to seven papers from each researcher and ask GPT-4 to create a research project based on two Each suggestion represents a potential collaboration between the evaluating researcher (Researcher A) and another researcher (Researcher B) from the Max Planck Society, visualized as bi-colored edges (orange for Researcher A, green for Researcher B).A purple circle indicates collaborations within the same institute, and edge transparency reflects the number of evaluated suggestions.Blue dots denote natural sciences (nat) and red dots represent social sciences (soc).(c): Distribution of interest ratings on a scale from 1 ('not interesting') to 5 ('very interesting'), with 394 suggestions rated as very interesting and 713 rated 4. Ratings are further categorized by whether the collaborations are within or across institutes, and by research field affiliation in either the natural or social sciences.</p>
<p>selected scientific concepts.These concepts are chosen in one of three ways: (1) a randomly selected concept pair, (2) the concept pair with the highest predicted impact, or (3) no specific concept pair, relying only on paper titles.We predict the impact (in terms of expected future citations) by adapting the computational methods from [12], applying them to a different and much larger knowledge graph.While the third method does not directly use the knowledge graph features, it serves as a valuable sanity check for our approach (see Supplementary Information).The prompt incorporates self-reflection techniques [30], where GPT-4 generates three ideas, iteratively refines them twice, and then selects the most suitable one as the final result.</p>
<p>Large-scale human evaluation -To evaluate the interest level of AI-generated research ideas, we invited 110 research group leaders, who regularly evaluate research proposals and act upon research ideas, from 54 Max Planck Institutes within the Max Planck Society (one of the largest research organizations worldwide), to participate in the evaluation (see Fig. 2(a) and (b)).Each leader evaluated up to 48 personalized research projects on a scale from 1 ('not interesting') to 5 ('very interesting').Of the 110 researchers, 104 were from natural science institutes and 6 from social science institutes.On average, evaluators had published 59.7 papers (range: 9 to 402) and received 3,759.7 citations (range: 20 to 85,778).In total, we received 4,451 responses.As shown Fig. 3. Analysis of interest levels versus knowledge graph features.We analyzed how eight features of the knowledge graph correlate with researchers' interest levels.After normalizing these features using z-scores, we arranged them from lowest to highest and divided the data into 50 equal groups.For each group, we plotted the average feature value (x-axis) against the average interest level (y-axis) with standard deviations, to identify trends.(a) and (b) show node features, (c)-(e) show node citation metrics, (f ) shows an edge feature, (g) an edge citation metric, and (h) represents semantic distance between researchers' sub-networks (higher values indicate that the researchers' scientific fields are further apart).Data points include all 2,996 responses (blue), the top 50% of concept pairs by predicted impact (green), and the top 25% (red), using the neuralnetwork based impact prediction presented in [12].</p>
<p>in Fig. 2(c), 1,107 projects (nearly 25%) received a rating of 4 or 5, with 394 rated as very interesting.</p>
<p>Properties of interesting research suggestions -On average, we find no significant difference in interest levels between projects generated using random concept pairs, high-impact concept pairs, or without concept pairs.The similarity in results for the sanity test (projects generated without a concept pair used in the knowledge graph) and those using concept pairs enables us to analyze which knowledge graph features most strongly influence the perceived interest of a research project.Identifying these features can help us suggest future research projects with higher interest levels.</p>
<p>We use the 2,996 suggested research projects, which were created using concept pairs from the knowledge graph, and sort them by various knowledge graph features.Then we group them into 50 equal bins and calculated the mean interest and standard deviation for each bin.Fig. 3 shows these correlations and highlights several notable trends.For instance, the vertex degree and PageRank of the first concept, selected from the evaluating researcher's concept list, are strongly negatively correlated with human-assessed interest levels.This indicates that the more connected a concept is within the knowledge graph, the less appealing the research project appears.A similar trend is observed for citation rates: the more frequently a concept has been cited in the past, the less interesting the project is evaluated.Additionally, the semantic distance feature shows a negative correlation in Fig. 3(h), suggesting that research proposals involving researchers from similar fields are perceived as more interesting than those from more distant fields.This finding aligns with Fig. 2(c), where proposals from the same institute are generally rated higher than those from different institutes with distinct research focuses.We present these correlations for all 2,996 responses (blue), as well as for the top 50% and top 25% of concept pairs with the highest predicted impact (green and red, respectively) in Fig. 3, indicating that some correlations are more pronounced for suggestions using highimpact concept pairs.Predicting interest -We set out to predict which suggestions would receive high interest ratings (4 or 5 out of 5) using two fundamentally different methods.First, we trained a neural network on researchers' responses, using knowledge graph properties and human-evaluation rankings, without incorporating the GPT-generated text.Second, we employed GPT in a zero-shot manner to rank the 2,996 suggestions independently of human evaluations.Remarkably, both method showed high prediction accuracy despite the absence of crucial information (see Fig. 4).This suggests that intelligent concept pair selec-Fig.4. Predicting Scientific Interest.We use two distinct methods to predict interest levels: (1) a supervised neural network trained on human evaluations using only knowledge graph data (not the text of the actual suggestion), and (2) GPT in a zero-shot setting, ranking suggestions without getting any feedback from human evaluations.Both methods classify suggestions as highly interesting (ratings of 4 or 5) or not (below 4).The neural network uses 25 knowledge graph features and employs Monte Carlo cross-validation for accuracy estimation.For GPT, we conduct pairwise comparisons using personalized research details and rank suggestions through an ELO-based tournment system.(a), The ROC curve shows prediction accuracy of 64.5% for the neural network and 67.3% for GPT-4o.(b), The precision for top-N suggestions is significantly higher than random selection, with the top-1 precision reaching 70% for the neural network (51.0% for GPT-4o and 52.9% for GPT-3.5) and top-5 precision at 60.4% (46.7% for GPT-4o, 43.7% for GPT-3.5).(c), The probability of having at least one high-interest suggestion among the top N recommendations is significantly higher for the supervised neural network compared to random selection.Practically, evaluation data from experienced researchers may not always be available, thus it is very encouraging that LLMs, even without human evaluation, can rank suggestions effectively such that the highest interesting ones appear first.</p>
<p>tion alone can significantly influence interest rankings in the graph-based approach, while GPT's zero-shot ranking is valuable when human evaluations are unavailable.</p>
<p>For the supervised neural network, we used knowledge graph features to predict whether a research proposal would receive a high rating (4 or 5) or below 4. Given the limited training data -each of the 2,996 data point representing a research group leader's evaluation of a proposal's interest level -we employed a low-data machine learning approach with a small neural network (25 highperforming input features, 50 neurons in a single hidden layer, and one output neuron).Dropout was used for training [31], and Monte Carlo cross-validation [32] (which is also known as repeated random sub-sampling validation) was applied to ensure robust evaluation and maximize the utility of our limited data (see the Supplementary Information).Decision trees [33] were not able to outperform the quality of neural networks (see Supplementary Information).</p>
<p>In the second approach, we tasked GPT-3.5 and GPT-4o to rank all 2,996 suggestions from most to least interesting.Because the suggestions are personalized, we included relevant research details, such as recent paper titles, in the prompts.Specifically, GPT is asked to compare pairs of randomly selected suggestions and determine which is more interesting based on the personalized research interests of the evaluating researcher.This comparison was repeated between 22,000 and 45,000 times (depending on the GPT version), and suggestions were ranked using an ELO system, with each suggestion starting at an initial ELO score of 1400.The model's choices adjusted the rankings, resulting in a final sorted list of suggestions based on their predicted interest level.</p>
<p>For the binary classification task -ranking research ideas in as highly interesting (4 or 5 out of 5) or lowinterest (3 or below) -, both methods achieve an average Area Under the Curve (AUC) of the receiver operating characteristic (ROC) curve [34] of nearly 2/3 (Fig. 4(a)).More importantly, high precision is more relevant for our task, as we want to suggest highly interesting projects within a small subset of overall suggestions.To evaluate this, we calculate the precision for the top-N predicted concept pairs.For small N (e.g., N=3), the supervised approach achieves 66.4% while GPT-4o and GPT-3.5 reached 45.0% and 47.2%, respectively.This means that 66.4% of the top-3 suggestions were rated as highly interesting, significantly higher than random selection (23%), as shown in Fig. 4(b).Additionally, we also measured the probability of finding at least one highly interesting suggestion within the top-N suggestions.As shown in Fig. 4(c), our machine learning method offers a significantly higher probability of identifying interesting suggestions within the first few recommendations compared to random sampling.Surprisingly, GPT -without access to human evaluations -also ranks interesting suggestions much higher than a random approach.This capability is highly valuable in scenarios where human evaluations are costly or unavailable.Furthermore, it is encouraging that newer, more powerful models (e.g., GPT-4o) perform better at predicting human interests than earlier versions like GPT-3.5.</p>
<p>DISCUSSION</p>
<p>We demonstrate the largest human evaluation of AIgenerated research ideas to date, involving over 100 research group leaders across diverse scientific domains, including the natural sciences and humanities.</p>
<p>Beyond understanding the correlations between the properties of the generated research ideas and their interest rankings, we introduce two distinct methods to predict interest levels of AI-generated research ideas.First, we use thousands of human evaluations to train a supervised neural network that can predict interest values based on knowledge graph data alone -without needing the full suggestion text.This approach enables us to select more compelling abstract topics before generating specific ideas.Second, we demonstrate that LLMs can autonomously rank the interest levels of ideas with high quality, even without human evaluation data.This capability is especially valuable when human evaluations are unavailable.Furthermore, we observe that ranking quality improves with more advanced LLMs, which is promising for future developments.As publicly available models such as GPT [17], Gemini 1.5 [26], LLaMa3 [35], and Claude [27] continue to evolve at an accelerated pace [36] -especially when it comes to scientific domain knowledge [37,38] -we expect personalized research ideas to become increasingly targeted and relevant.</p>
<p>The methodologies employed by SciMuse have the potential to inspire novel and unexpected cross-disciplinary research on a large scale.By providing a broad view through the analysis of millions of scientific papers, SciMuse facilitates the discovery of interesting research collaborations between scientists from different fields that might otherwise remain undiscovered.Research in distant fields has been shown to have great potential for impactful, award-winning results [1,2,6,39].Therefore, large scientific organizations, national funding agencies, and other stakeholders may find value in adopting methodologies in the line of SciMuse to foster new highly interdisciplinary and interesting collaborations and ideas that might otherwise remain untapped.This, hopefully, could advance the progress and impact of science at a large scale.</p>
<p>One exciting possibility is in automated scientific experimentation [40].Currently, while large-language models have been integrated into laboratories [41,42], the main idea of the experiment has been provided by human scientists.In the future, one might envision the entire scientific process becoming fully automated -from the generation of an interesting idea, as we demonstrate here, to its automated execution and implementation.</p>
<p>We compiled a list of scientific concepts using metadata from arXiv, bioRxiv, medRxiv, and chemRxiv.The arXiv data is available on Kaggle, while metadata for other preprint sources can be accessed through their respective APIs.Our dataset includes ∼2.44 million prapers, with a cutoff date of February 2023.</p>
<p>For edge generation, we used the OpenAlex database snapshot (available on the OpenAlex bucket) with a cutoff date of April 2023.For more details, refer to the OpenAlex website [23].The original dataset was filtered to entries of journal papers that contain titles, abstracts, and citation data, resulting roughly 92 million papers.From these 92 million papers, 58 million contain at least two concepts of our concept list and can therefore for an edge in the knowledge graph.</p>
<p>B. The concept list</p>
<p>We analyzed the titles and abstracts of ∼2.44 million papers from four preprint datasets using the RAKE algorithm, enhanced with additional stopwords, to extract potential concept candidates.Initial filtering retained two-word concepts (e.g.gouy phase) appearing in at least nine articles and concepts with more than three words (e.g.recurrent neural network ) appearing in six or more, reducing the list to 726,439 concepts.</p>
<p>To further improve the quality of the identified concepts, we developed a suite of automated tools to eliminate domain-independent errors commonly associated with RAKE and performed a manual review to remove inaccuracies like non-conceptual phrases, verbs, and conjunctions.This step refined the list to 368,825 concepts.</p>
<p>Next, we used GPT-3.5 to further refine the concepts, which resulted in the removal of 286,311 concepts.We then employed Wikipedia to restore 40,614 mistakenly removed entries, resulting in a final refined list of 123,128 concepts.</p>
<p>C. Classification of Max Planck Institutes</p>
<p>We classified all 87 Max Planck Institutes into two categories: Class 1, abbreviated as nat, includes natural sciences, technology, mathematics, and medicine (68 institutes), while Class 2, abbreviated as soc, includes social sciences and humanities (19 institutes).The initial classification was done manually based on each institute's title and research field.To validate this, we further used GPT-4o for automatic classification, which perfectly matched with our manual classification.</p>
<p>D. Researcher Statistics</p>
<p>Over 100 highly experienced researchers, spanning fields from the natural sciences to the humanities, participated in evaluating the personalized research ideas.Table .S1 summarizes the researchers' publication and citation statistics as of January 1, 2024, when the evaluations were conducted.On average, the researchers had published 59 papers and received over 3,750 citations.The prompt to refine the researchers' concept list is shown below:</p>
<p>Prompt to Refine the Researchers' Concept List</p>
<p>A scientist has written the following papers: 0) title1 1) title2 2) title3 ... I have a noisy list of the researchers' topics of interest, and I would like your help in filtering them.Please look at the list below and return all concepts that are relevant to the scientist's research (based on their paper titles) and meaningful in the context of their research direction.The concepts can be detailed; I mainly want you to filter out concepts that are not meaningful, words that are not concepts, or concepts that are too general for the direction of the scientist (e.g., "artificial intelligence" might be a meaningful concept for a geologist, but not for a machine learning researcher).Do not change or add any conceptsonly remove or keep them.concept list=[c1, c2, c3, c4, c5, c6, ...]</p>
<p>F. Prompt to GPT-4 for project idea generation</p>
<p>The prompt used to suggest research ideas based on concept pairs from the knowledge graph is described as the follows:</p>
<p>Prompt to GPT-4 for Project Idea Generation Two researchers A and B, with expertise in "con-cept1" and "concept2" respectively, are eager to collaborate on a novel interdisciplinary project that leverages their unique strengths and creates synergy between their fields.</p>
<p>To better understand their backgrounds, here are the titles of recent publications from each researcher: Researcher A: 1: title1 2: title2 3: title3 ... Researcher B: 1: title1 2: title2 3: title3 ... Please suggest a creative and surprising scientific project that combines "concept1" and "con-cept2".In your response, follow this outline: First, explain "concept1" and "concept2" in one short sentence each.</p>
<p>Then, do the following three steps 3 times, improving in each time the response: A) Describe 4 interesting and new scientific contexts, in which those two concepts might appear together in a natural and useful way.B) Criticize the 4 contexts (one short sentence each), on how well the contexts merge the idea of the two concepts.C) Give a 2 sentence summary of your reflections above, on how well one can combine these concepts naturally and interestingly.</p>
<p>Then, start finding a project.Taking your reflections from (A-C) into account, define in your response a project title, followed by a brief explanation of the project's main objective.</p>
<p>Finally, address the following questions (Take the full reflections (A-C) into account): What specific interesting research questions will this project address, that will lead to innovative novel results?[2 bullet points, one sentence each] Rather than relying on a knowledge graph to supply "concept1" and "concept2", it is also possible to direct GPT-4 to extract these concepts from the research paper titles of Researchers A and B, respectively.GPT-4 can then use these identified concepts within the same prompting context to generate innovative research ideas.</p>
<p>G. Interest evaluation for three different generation methods</p>
<p>Fig. S1 presents the interest-level distributions for research suggestions generated using three different methods.The interest levels are notably similar between suggestions generated with and without concepts from the knowledge graph.This similarity enables us to analyze the correlations between knowledge graph properties and interest levels, and to use these properties for predicting the interest level of generated research proposals.(1) no concepts provided by the knowledge graph, (2) random concepts from the researchers' subnetwork, and (3) predicted high-impact concept pairs from the researchers' subnetwork.The figures displays: (a) overall interest levels (numbers within bars show the number of responses for that evaluation), (b) interest levels for ideas without using concepts from the knowledge graph, (c) interest levels with random concept pairs, and (d) interest levels using high-impact concept pairs (predicted by adapting the computational methods from [12], and applying them to a different and much larger knowledge graph).</p>
<p>H. Predicting high interest from knowledge graph features with neural networks</p>
<p>In Fig. 4 (main text), we aim to predict whether a research proposal will be rated with high interest.Specifically, using only data from the knowledge graph (excluding the final text generated by GPT), we predict if a proposal will receive an interest rating of 4 or 5 (on a scale of 1 to 5: not interesting to very interesting) or below 4.This is formulated as a binary classification task.</p>
<p>The input to the neural networks (using PyTorch [44]) consists of network-theoretical features extracted from the knowledge graph.For each concept pair in a re- search project, we compute 144 features.The first 141 features are derived from those used to predict the future impact of concept pairs, as described in [12].These features include node properties (e.g., node degree and PageRank [45]) and edge properties (e.g., Simpson similarity and Sørensen-Dice coefficient [46]).Several features also account for impact information, such as recent citation counts.The remaining three features include the predicted impact and two distance metrics between the researchers' subgraphs (Fig. 1(b)).The first distance metric measures the distance based solely on the concepts present in Researcher A and Researcher B's concept lists.In contrast, the second metric takes into account the entire neighborhood of these subgraphs by calculating semantic distances between all neighboring concepts and the concepts from the subgraphs.These features serve as the input to the neural network for predicting whether a proposal will achieve a high interest rating.</p>
<p>Given the small dataset size (2,996 answers with properties from the knowledge graph), we use a data-efficient learning method -a small neural network with dropout.The input layer consists of the 25 best-performing features (see Table.S2), selected from the total 144 by independently analyzing the feature importance of each and choosing the top 25.The neural network has one hidden layer with 50 neurons and a single output neuron.Mean square error is used as the loss function.</p>
<p>To ensure robust performance estimation for the small dataset, we use Monte Carlo cross-validation.The dataset is repeatedly split into training and validation sets, and the model is trained and evaluated on each split.This approach ensures that the performance metrics are robust and not dependent on a particular split of the data.This iterative process continues until the standard deviation of the mean AUC is less than 10 −2 3 , achieved after 130 iterations.This method provides a reliable estimate of the model's performance, which is crucial for small datasets where individual splits may lead to high variance in the evaluation metrics.</p>
<p>The neural network performance is not specifically sensitive to hyperparameter choices, thus we refrained from hyperparameter optimization, and instead used a reasonable defaults: learning rate=0.003,dropout=20%, weight decay=0.0007,training dataset=75%, validation dataset=15%, test dataset=10%.In Fig. S2, we investigate alternative hyper-parameters of the training process, and find that the results are robust under variations of the hyper-parameters.</p>
<p>I. Predicting high interest from knowledge graph features with decision trees</p>
<p>We experimented with other data-efficient learning methods, specifically with decision trees [33] using [47].However, decision trees did not outperform the neural network predictions, as can be seen in Fig. S3.These values confirm that neural networks are advantageous for the task of ranking research ideas by their interest value in a supervised way, which can also be confirmed in Fig. S5.</p>
<p>J. Zero-shot ranking of research suggestions by GPT</p>
<p>We ranked 2,996 research suggestions -previously evaluated by human experts -using GPT-3.5, GPT-4o, and GPT-4o-mini.For each pair of randomly selected suggestions, we asked the LLMs to rank which one was more interesting, considering the personalized research interests of the evaluating human expert.This pairwise comparison was repeated between 22,000 and 45,000 times (for GPT4o and GPT4o-mini, respectively).We treated this task as a tournament where all 2,996 suggestions compete pairwise against each other.Using the ELO ranking system, each suggestion started with an initial ELO score of 1400.Each comparison by GPT updated the ELO rankings based on the outcome, producing a final sorted list of suggestions from highest to lowest ELO score.We evaluated the ranking quality by calculating the AUC to determine how well the ranked list aligns with the human-expert evaluations of interest levels (Fig. S4).</p>
<p>K. Prediction of Interest with different methods</p>
<p>We show the full data of all five methods (supervised training with neural networks and decision trees, as well as unsupervised zero-shot prediction with GPT3.5, GPT4o and GPT4o-mini), with their corresponding AUC, top-N precision and top-N success probability, in Fig. S5.We see that the neural network outperforms decision trees when trained in a supervised way, and that GPT4o is better than the other tested models, when the ranking is performed in a zero-shot manner without giv-Fig.S4.Zero-Shot ranking of research suggestions by LLMs.The research suggestions are generated using the knowledge graph together with GPT4.They are then ranked using GPT4o, GPT4o-mini and GPT3.5, without feedback from the human evaluation.The human evaluation is used to compute the final quality of the ranking.the ranking is performed in a pair-wise choice where we ask the LLM to select the more interesting one given the research background of the researchers.One match is one pairwise selection.The LLMs perform 10,000 of these pairwise selections, which allows us to compute ELO scores for each generated research idea.</p>
<p>ing any information about the evaluations of humans.</p>
<p>L. Prompt engineering</p>
<p>We have explored manual and automated improvements of the prompts, both for the research question design and the zero-shot prediction.Specifically, we attempted to improve the prompts for the idea generation using GPT-4o.While the prompts were more structured, a small-scale evaluation did not show any improvement in terms of more interesting results.</p>
<p>M. GPT-4o and GPT-o1 for idea generation</p>
<p>We conducted two small-scale tests where GPT-4 and GPT-4o generated ideas using the exact same settings described above.</p>
<p>In the first test, three research group leaders evaluated 180 pairs of questions (one generated by GPT-4 and the other by GPT-4o using the same prompt).They found 31.1% of GPT-4 answers to be more interesting, while 60.56% favored GPT-4o answers (8.3% were draw).In the second test, a research group leader evaluated 11 pairs of questions (one generated by GPT-4o and the other by GPT-o1 with the same prompt), and all 11 ideas generated by GPT-o1 were ranked as more interesting.</p>
<p>These additional small-scale tests suggest that improved models can enhance idea generation, thus directly improving the results of SciMuse.</p>
<p>Prompt for Zero-Shot Ranking of Research Ideas I will present two research ideas.The first idea is for Researchers A1 and B1, and the second idea is for Researchers A2 and B2.Researchers A1 and A2 will evaluate how interesting they find the respective ideas.You will determine which of the two suggestions will be considered more interesting.</p>
<p>Fig. 2 .
2
Fig. 2. Large-scale human evaluation within the Max Planck Society.(a)-(b), The map of Germany, based on the GISCO statistical unit dataset from Eurostat [28], shows the locations of the Max Planck Institutes and the participating group leaders.A total of 4,451 personalized AI-generated research suggestions were evaluated by 110 research group leaders.Each suggestion represents a potential collaboration between the evaluating researcher (Researcher A) and another researcher (Researcher B) from the Max Planck Society, visualized as bi-colored edges (orange for Researcher A, green for Researcher B).A purple circle indicates collaborations within the same institute, and edge transparency reflects the number of evaluated suggestions.Blue dots denote natural sciences (nat) and red dots represent social sciences (soc).(c): Distribution of interest ratings on a scale from 1 ('not interesting') to 5 ('very interesting'), with 394 suggestions rated as very interesting and 713 rated 4. Ratings are further categorized by whether the collaborations are within or across institutes, and by research field affiliation in either the natural or social sciences.</p>
<p>Fig. S1 .
S1
Fig. S1.Interest levels across different generation methods.Research ideas are generated using three methods:(1) no concepts provided by the knowledge graph, (2) random concepts from the researchers' subnetwork, and (3) predicted high-impact concept pairs from the researchers' subnetwork.The figures displays: (a) overall interest levels (numbers within bars show the number of responses for that evaluation), (b) interest levels for ideas without using concepts from the knowledge graph, (c) interest levels with random concept pairs, and (d) interest levels using high-impact concept pairs (predicted by adapting the computational methods from[12], and applying them to a different and much larger knowledge graph).</p>
<p>Fig. S2 .
S2
Fig. S2.Choice of alternative hyper-parameters for training of neural network.We analyse the prediction of interest level quality (in terms of AUC) with different parameters of the neural network, such as different number of features, different number of layers and neurons, learning rate and drop-out rate.We see that the final results are robust under variations of the hyper-parameters.</p>
<p>Fig. S3 .
S3
Fig.S3.Choice of hyper-parameters for training of decision tree.The model is trained using Monte Carlo crossvalidation until a statistical uncertainty of σ=0.001 is reached.We find that no setting of number of features, maximum depth and minimal sample leaf can reach the performance of the data-efficient neural network.</p>
<p>The suggestions are randomly ordered, and you should evaluate each suggestion independently and without bias.### Researcher A1 Context and Suggestion 1: Here are a few papers of Researcher A1: (papersA1) Suggestion 1: [suggestion1] <strong>Summary for Researcher A1</strong>: Provide a one-sentence summary of Suggestion 1 in the context of Researcher A1. ### Researcher A2 Context and Suggestion 2: Here are a few papers of Researcher A2: (papersA2) Suggestion 2: [suggestion2] <strong>Summary for Researcher A2</strong>: Provide a one-sentence summary of Suggestion 2 in the context of Researcher A2.### Evaluation:Based on the summaries and the research interests of A1 and A2, evaluate which suggestion is more likely to be ranked higher in terms of interest.<strong>Result</strong>: If Suggestion 1 is ranked higher by Researcher A1 than Suggestion 2 is by Researcher A2, write 'RESULT: SUGGESTION 1'.Otherwise, write 'RESULT: SUGGESTION 2'.Remember, the suggestions are randomly ordered, and your evaluation should be impartial and based solely on the research interests of A1 and A2.</p>
<p>Fig. S5 .
S5
Fig.S5.Interest predictions with five methods.We reproduce Fig.4(main text), and add results from a supervised decision tree training, and an unsupervised GPT4o-mini.</p>
<p>Table S1 .
S1
Summary statistics of researchers' publications and citations.
Mean Median Min MaxNumber of papers59.736.09402Number of citations 3759.7 1630.020 85778E. Prompt to GPT-4 for concept refinement
ACKNOWLEDGEMENTSThe authors wholeheartedly thank all the researchers who spent the time participating in our study.The authors also thank the organizers of OpenAlex, arXiv, bioRxiv, medRxiv, and chemRxiv for making scientific resources freely accessible.X.G. acknowledges support from the Alexander von Humboldt Foundation.DATA AND CODE AVAILABILITYData for the knowledge graph is accessible on Zenodo at https://doi.org/10.5281/zenodo.13900962[43].Codes and evaluation data for this work are available on GitHub at https://github.com/artificial-scientist-lab/SciMuse.ETHICS STATEMENTThe research was reviewed and approved by the Ethics Council of the Max Planck Society.Supplementary InformationA. Datasets for creating knowledge graph
. S Fortunato, C T Bergstrom, K Börner, J A Evans, D Helbing, S Milojević, A M Petersen, F Radicchi, R Sinatra, B Uzzi, 10.1126/science.aao0185Science of science. 3591852018Science</p>
<p>D Wang, A.-L Barabási, The science of science. Cambridge University Press2021</p>
<p>Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases. L Bornmann, R Haunschild, R Mutz, 10.1057/s41599-021-00903-wHumanities and Social Sciences Communications. 812021</p>
<p>. J A Evans, J G Foster, Metaknowledge , 10.1126/science.1201765Science. 3317212011</p>
<p>Sciscinet: A largescale open data lake for the science of science research. Z Lin, Y Yin, L Liu, D Wang, 10.1038/s41597-023-02198-9Scientific Data. 103152023</p>
<p>Choosing experiments to accelerate collective discovery. A Rzhetsky, J G Foster, I T Foster, J A Evans, 10.1073/pnas.150975711Proc. Natl. Acad. Sci. USA. 112145692015</p>
<p>Predicting research trends with semantic and neural networks with an application in quantum physics. M Krenn, A Zeilinger, 10.1073/pnas.1914370116Proc. Natl. Acad. Sci. USA. 11719102020</p>
<p>Agatha: automatic graph mining and transformer based hypothesis generation approach. J Sybrandt, I Tyagin, M Shtutman, I Safro, 10.1145/3340531.3412684Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. the 29th ACM International Conference on Information &amp; Knowledge Management2020</p>
<p>R Nadkarni, D Wadden, I Beltagy, N A Smith, H Hajishirzi, T Hope, arXiv:2106.09700Scientific language models for biomedical knowledge base completion: an empirical study. 2021</p>
<p>Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network. M Krenn, L Buffoni, B Coutinho, S Eppel, J G Foster, A Gritsevskiy, H Lee, Y Lu, J P Moutinho, N Sanjabi, 10.1038/s42256-023-00735-0Nature Machine Intelligence. 513262023</p>
<p>Surprising combinations of research contents and contexts are related to impact and emerge with scientific outsiders from distant disciplines. F Shi, J Evans, 10.1038/s41467-023-36741-4Nature Communications. 1416412023</p>
<p>X Gu, M Krenn, arXiv:2402.08640Forecasting high-impact research topics via machine learning on evolving knowledge graphs. 2024</p>
<p>Accelerating science with human-aware artificial intelligence. J Sourati, J A Evans, 10.1038/s41562-023-01648-zNature Human Behaviour. 716822023</p>
<p>Q Wang, D Downey, H Ji, T Hope, Scimon: Scientific inspiration machines optimized for novelty, Annual Meeting of the Association for Computational Linguistics. 2024</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Z Yang, X Du, J Li, J Zheng, S Poria, E Cambria, arXiv:2309.027262023</p>
<p>J Baek, S K Jauhar, S Cucerzan, S J Hwang, arXiv:2404.07738Researchagent: Iterative research idea generation over scientific literature with large language models. 2024</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023</p>
<p>On scientific understanding with artificial intelligence. M Krenn, R Pollice, S Y Guo, M Aldeghi, A Cervera-Lierta, P Friederich, G Dos Passos, F Gomes, A Häse, A Jinich, Nigam, 10.1038/s42254-022-00518-3Nature Reviews Physics. 47612022</p>
<p>A computational inflection for scientific discovery. T Hope, D Downey, D S Weld, O Etzioni, E Horvitz, 10.1145/3576896Communications of the ACM. 66622023</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, T Fu, Y Du, W Gao, K Huang, Z Liu, P Chandak, S Liu, P Van Katwyk, A Deac, 10.1038/s41586-023-06221-2Nature. 620472023</p>
<p>The impact of large language models on scientific discovery: a preliminary study using gpt-4. M R Ai4science, M A Quantum, arXiv:2311.073612023</p>
<p>Automatic keyword extraction from individual documents, Text mining: applications and theory. S Rose, D Engel, N Cramer, W Cowley, 10.1002/9780470689646.ch120101</p>
<p>J Priem, H Piwowar, R Orr, arXiv:2205.01833Openalex: A fullyopen index of scholarly works, authors, venues, institutions, and concepts. 2022</p>
<p>. K Wakonig, A Diaz, A Bonnin, M Stampanoni, A Bergamaschi, J Ihli, M Guizar-Sicairos, A Menzel, X-Ray Fourier Ptychography, 10.1126/sciadv.aav0282Science advances. 52822019</p>
<p>Ultrafast x-ray imaging of the light-induced phase transition in vo2. A S Johnson, D Perez-Salinas, K M Siddiqui, S Kim, S Choi, K Volckaert, P E Majchrzak, S Ulstrup, N Agarwal, K Hallman, 10.1038/s41567-022-01848-wNature Physics. 192152023</p>
<p>M Reid, N Savinov, D Teplyashin, D Lepikhin, T Lillicrap, J -B. Alayrac, R Soricut, A Lazaridou, O Firat, J Schrittwieser, arXiv:2403.05530Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. 2024</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Anthropic, Papers with Code. 2024available at https://paperswithcode.com/paper/ the-claude-3-model-family-opus-sonnet-haiku</p>
<p>E Commission, Eurostat gisco -nuts geodata. 2024</p>
<p>A spot in one of the belts of jupiter. R Hooke, 10.1098/rstl.1665.0005Philosophical Transactions of the Royal Society of London. 131665</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, Advances in Neural Information Processing Systems. 362024</p>
<p>Dropout: A simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, Journal of Machine Learning Research. 1519292014</p>
<p>Monte carlo cross validation. Q.-S Xu, Y.-Z Liang, 10.1016/S0169-7439(00)00122-2Chemometrics and Intelligent Laboratory Systems. 5612001</p>
<p>L Breiman, Classification and regression trees. 2017Routledge</p>
<p>Roc graphs: Notes and practical considerations for researchers. T Fawcett, Machine learning. 3112004</p>
<p>Llama 3: Open foundation and fine-tuned chat models. M Ai, 2024</p>
<p>W.-L Chiang, L Zheng, Y Sheng, A N Angelopoulos, T Li, D Li, H Zhang, B Zhu, M Jordan, J E Gonzalez, arXiv:2403.04132Chatbot arena: An open platform for evaluating llms by human preference. 2024</p>
<p>G P Wellawatte, P Schwaller, arXiv:2311.04047Extracting human interpretable structure-property relationships in chemistry using xai and large language models. 2023</p>
<p>Can large language models provide useful feedback on research papers? a large-scale empirical analysis. W Liang, Y Zhang, H Cao, B Wang, D Y Ding, X Yang, K Vodrahalli, S He, D S Smith, Y Yin, 10.1056/AIoa2400196NEJM AI. 1AIoa2400196. 2024</p>
<p>Atypical combinations and scientific impact. B Uzzi, S Mukherjee, M Stringer, B Jones, 10.1126/science.1240474Science. 3424682013</p>
<p>Self-driving laboratories for chemistry and materials science. G Tom, S P Schmid, S G Baird, Y Cao, K Darvish, H Hao, S Lo, S Pablo-García, E M Rajaonson, M Skreta, 10.1021/acs.chemrev.4c00055Chemical Reviews. 12496332024</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, 10.1038/s41586-023-06792-0Nature. 6245702023</p>
<p>Augmenting large language models with chemistry tools. A M Bran, S Cox, O Schilter, C Baldassari, A D White, P Schwaller, 10.1038/s42256-024-00832-8Nature Machine Intelligence. 12024</p>
<p>Interesting scientific idea generation using knowledge graphs and llms: Evaluations with 100 research group leaders [data set. X Gu, 10.5281/zenodo.139009622024</p>
<p>Pytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, Advances in neural information processing systems. 322019</p>
<p>The pagerank citation ranking : Bringing order to the web. L Page, S Brin, R Motwani, T Winograd, 1999Stanford InfoLab</p>
<p>A.-L Barabási, Network Science. Cambridge University Press2016</p>
<p>Scikit-learn: Machine learning in python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Journal of Machine Learning Research. 1228252011</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4760 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4760</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4760</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-266998999</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.06853v6.pdf" target="_blank">Large Language Models Can Learn Temporal Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal concepts and intricate temporal logic. In this paper, we propose TG-LLM, a novel framework towards language-based TR. Instead of reasoning over the original context, we adopt a latent representation, temporal graph (TG) that enhances the learning of TR. A synthetic dataset (TGQA), which is fully controllable and requires minimal supervision, is constructed for fine-tuning LLMs on this text-to-TG translation task. We confirmed in experiments that the capability of TG translation learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we teach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought (CoT) bootstrapping and graph data augmentation. We observed that those strategies, which maintain a balance between usefulness and diversity, bring more reliable CoTs and final results than the vanilla CoT distillation.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4760.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4760.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT bootstrapping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought bootstrapping with contrastive sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method introduced in this paper that uses GPT-3.5/GPT-4 to generate multiple candidate Chain-of-Thought (CoT) reasoning traces, filters only those that yield correct final answers, and performs weighted sampling via a contrastive score to balance usefulness and diversity for supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-13B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-2 family base model (13B parameters) used as the primary fine-tuning backbone; adapters + LoRA are trained for the two-stage pipeline (text-to-TG translation and TG reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>CoT bootstrapping (contrastive sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate K CoTs per example using GPT-3.5/GPT-4, discard CoTs that lead to incorrect final answers, and sample accepted CoTs for SFT according to a contrastive score combining the normalized probability of the correct answer and a plausibility-growth term; sampling (softmax over score) balances usefulness and diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TGQA (primary), TimeQA and TempReason (transfer/evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Temporal reasoning QA datasets: TGQA (synthetic, controllable text→temporal-graph QA), TimeQA and TempReason (Wikipedia-derived temporal QA/benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Human-evaluation and downstream: SFT with CoT-bootstrapping (SFT + bs) reduced certain CoT error types versus plain ICL and vanilla SFT-CoT; human-eval error rates (T1..T4) reported: SFT+bs = [T1:0.06, T2:0.13, T3:0.03, T4:0.08]. Downstream EM/F1/Acc improved over ICL and over vanilla CoT distillation (see paper Figures/Tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Vanilla CoT distillation (SFT-CoT) human-eval error rates: [T1:0.04, T2:0.17, T3:0.13, T4:0.10]; ICL error rates higher (ICL: [0.13,0.13,0.31,0.10]). Quantitative downstream metrics (EM/F1/Acc) are reported in paper showing SFT+bs improves over these baselines (see main tables/figures).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Bootstrapping CoTs with contrastive sampling that enforces correct final answers and balances usefulness/diversity produces more reliable intermediate reasoning traces and improves downstream temporal-reasoning performance compared to in-context CoTs and vanilla CoT distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Not all error types uniformly improved: for example T1 (using wrong info) was slightly higher for SFT+bs (0.06) than SFT (0.04) in the human-evaluation sample, indicating some trade-offs; overall, however, SFT+bs reduced logical inconsistency, external knowledge, and temporal graph errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Learn Temporal Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4760.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4760.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vanilla CoT distillation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla Chain-of-Thought distillation (SFT-CoT baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline approach where CoTs produced by a teacher model are distilled into a student via supervised fine-tuning without the paper's bootstrapping/contrastive sampling or diversity-balancing steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-13B (fine-tuned baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same base model (Llama2-13B) used as a baseline when fine-tuned on CoT examples distilled directly from GPT-3.5 (vanilla distillation).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Vanilla CoT distillation (single/standard CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Collect CoT demonstrations (typically a small set per category) from a teacher LM (GPT-3.5), then supervisedly fine-tune the student LM on these CoTs (no contrastive scoring or explicit diversity balancing).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TGQA (primary) and other TR benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Temporal reasoning QA tasks used to evaluate whether distilled CoTs improve LLM temporal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Human-eval error rates reported for SFT-CoT: [T1:0.04, T2:0.17, T3:0.13, T4:0.10]; downstream metrics are better than ICL but worse than the proposed SFT with bootstrapping and augmentation (paper reports aggregated EM/F1/Acc improvements for the proposed method over SFT-CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>SFT with CoT bootstrapping and graph augmentation (SFT+bs+aug) achieved lower error rates [T1:0.03, T2:0.05, T3:0.04, T4:0.05] and higher downstream scores.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vanilla CoT distillation improves over plain in-context prompting but is outperformed by the paper's bootstrapping + augmentation approach; lack of curated diversity/usefulness balance in distilled CoTs leaves room for hallucinations and logical errors.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Vanilla SFT-CoT had slightly lower T1 (using wrong info) than SFT+bs in the reported human evaluation sample, indicating distilled single-coherence CoTs can sometimes avoid certain mistakes but overall are less robust.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Learn Temporal Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4760.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4760.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICL-CoT (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Context Learning with Chain-of-Thought demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard few-shot prompting where CoT examples are placed in the context (prompt) to elicit intermediate reasoning from a large pre-trained LM at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 and GPT-3.5 (evaluated in ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API-accessed OpenAI models: GPT-3.5-turbo and GPT-4 (gpt-4-1106-preview) used for in-context Chain-of-Thought prompting and as CoT generators for demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>ICL with CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Provide a few demonstrations (including CoTs) in the prompt and ask the model to produce CoT + answer at inference time (no parameter updates).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TGQA / TimeQA / TempReason (ICL evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluation of temporal reasoning in few-shot setting across datasets; measures the LMs' ability to reason via CoT without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports that among ICL methods GPT-4 is strongest; for some TGQA categories GPT-4's best in-context performance reaches ~0.6–0.7 (category-level EM) but overall is insufficient for complex TR. Human-eval CoT error rates for ICL were comparatively high: [T1:0.13, T2:0.13, T3:0.31, T4:0.10].</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>SFT (with CoTs or bootstrapping) outperforms ICL; SFT+bs+aug achieves substantially lower CoT error rates and higher downstream metrics than ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ICL with CoT can elicit reasoning but remains unreliable for temporal reasoning: hallucinated/unfaithful intermediate steps and external-knowledge errors are common; fine-tuning with supervised CoTs (especially bootstrapped/diverse ones) yields stronger and more reliable performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Despite being the best ICL model, GPT-4 still shows only moderate performance in many TGQA categories (~0.6–0.7), demonstrating that powerful pre-trained models with CoT prompts alone are not a complete solution for complex temporal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Learn Temporal Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4760.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4760.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contrastive sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive-scoring and weighted-sampling of CoTs (Best-of-N alternative)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's sampling mechanism for selecting CoTs uses a score combining normalized probability of the correct answer and a plausibility-growth term; sampling via softmax over scores balances usefulness and diversity among accepted CoTs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-13B (trained with sampled CoTs produced by GPT-3.5/GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama2-13B adapters fine-tuned with SFT using the sampled CoTs as supervision; GPT-3.5/GPT-4 used to produce candidate CoTs and demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Contrastive sampling for CoTs</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Compute score(c) = log P(correct answer | context, c) + γ * G(c), where G(c) is a plausibility-growth ratio; accept only CoTs that lead to correct answers, then sample training CoTs with probability proportional to softmax(score). This trades off high-probability usefulness and diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CoT generation for SFT on TGQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Selecting which CoTs to use as SFT targets to teach the student model to produce reliable intermediate reasoning on temporal-graph inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Using contrastive sampling to select CoTs (as part of SFT+bs) led to improved downstream temporal reasoning and reduced CoT error rates compared to Best-of-N / naive selection; exact numeric downstream gains are reported in the paper's main results and ablation tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Best-of-N or naive selection (no contrastive scoring) results in less balanced CoT datasets and worse downstream performance; the paper reports that their contrastive-sampling increases robustness and reduces hallucination compared to conventional Best-of-N strategies (no direct numeric Best-of-N row shown but discussed qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Contrastive scoring + sampling provides a principled way to keep CoTs that are both useful (raise answer probability) and diverse (plausibility growth), producing better SFT data than naive Best-of-N or single-CoT distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Paper does not present a direct numeric comparison to Best-of-N in a table, but notes that contrastive selection is inspired by prior work and qualitatively yields better diversity/usefulness trade-offs; some specific error types (e.g., T1) may still increase in isolated comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Learn Temporal Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4760.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4760.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph data augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporal-graph data augmentation (remove edges / synonyms / global mappings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training-time perturbations applied to temporal graphs to improve robustness of the reasoning model: remove irrelevant edges, replace relations with synonyms, globally remap entity names, and offset times.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-13B (fine-tuned with augmented TGs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama2-13B with adapters fine-tuned on TGs that have been systematically disturbed as augmentation during SFT to make reasoning robust to graph estimation errors at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Graph data augmentation (training-time disturbances)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Introduce controlled disturbances to training TGs: (i) remove irrelevant edges, (ii) replace relations with synonyms, (iii) globally map entity names to random same-type names, (iv) change times by a global offset; used to regularize learning so the model learns underlying logic rather than memorizing surface facts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TGQA (training), evaluation on TGQA/TimeQA/TempReason</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Temporal reasoning QA tasks where the model must reason over (possibly noisy/estimated) temporal graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>SFT combined with CoT bootstrapping + augmentation (SFT+bs+aug) achieved the lowest human-eval CoT error rates reported: [T1:0.03, T2:0.05, T3:0.04, T4:0.05], and improved downstream EM/F1/Acc vs SFT and SFT+bs baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>SFT+bs without augmentation: human-eval [0.06,0.13,0.03,0.08]; vanilla SFT-CoT: [0.04,0.17,0.13,0.10]. Augmentation further reduces logical, external-knowledge and TG errors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Graph data augmentation reduces error rates and improves robustness of TG reasoning models, particularly when combined with diverse CoT supervision; it mitigates discrepancies between training on ground-truth TGs and inference on estimated TGs.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Some augmentation operations require care (paper warns to avoid confusing the LLM); mapping entity names/times necessitates corresponding changes in QA annotations. No explicit failure cases quantified beyond these cautions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Learn Temporal Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4760.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4760.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT-TGR (TG-LLM full pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Fine-Tuning for Temporal Graph Reasoning (full TG-LLM pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's two-stage system: text→temporal-graph translation (SFT) then temporal-graph reasoning via SFT enhanced by CoT bootstrapping and graph augmentation; the full pipeline aims to improve LLM temporal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-13B (SFT-TGR variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama2-13B with two adapter modules trained in parallel: one for text-to-TG translation and one for TG reasoning; training uses TGQA (synthetic) plus bootstrapped CoTs and graph augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Two-stage TG-LLM with CoT bootstrapping + augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>First translate story text into a latent temporal graph (TG) via SFT; then reason over TG with supervised CoTs obtained via bootstrapping and diversified by contrastive sampling and graph augmentations to train robust deliberate reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TGQA (training) → evaluation on TGQA/TimeQA/TempReason (transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>TGQA: synthetic, controllable temporal-graph QA; TimeQA/TempReason: real-world/Wikipedia-derived temporal reasoning benchmarks used for transfer evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports that SFT-TGR substantially outperforms ICL baselines and vanilla SFT baselines on TGQA and transfers well to TimeQA and TempReason; the Llama2-13B based SFT-TGR matches or exceeds GPT-4 on the evaluated datasets (qualitative/aggregate claim; exact per-dataset numbers in main tables). Example human-eval and ER reductions shown (see SFT+bs+aug row).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared to ICL (ICL-SP/ICL-CoT) and SFT-SP/SFT-CoT baselines, SFT-TGR (full) gives higher EM/F1/Acc and reduces CoT error rates; see main tables and Figure 4-5 for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The two-stage TG-LLM paradigm plus diverse, quality-controlled CoT supervision and graph augmentation yields large gains in temporal reasoning, and learned TG-translation + TG-reasoning capabilities generalize to other TR datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CoTs are not uniformly beneficial in naive form (paper notes CoTs sometimes hurt TR due to hallucinated intermediate steps), which motivated the bootstrapping and augmentation; certain isolated error measures (e.g., T1) saw small regressions when introducing bootstrapping alone, resolved when augmentation is added.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Learn Temporal Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Self-Consistent Chain-of-Thought Distillation <em>(Rating: 2)</em></li>
                <li>Large Language Models Are Zero-Shot Reasoners <em>(Rating: 1)</em></li>
                <li>Are Large Language Models Temporally Grounded? <em>(Rating: 1)</em></li>
                <li>TimeQA: A dataset for answering time-sensitive questions <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4760",
    "paper_id": "paper-266998999",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "CoT bootstrapping",
            "name_full": "Chain-of-Thought bootstrapping with contrastive sampling",
            "brief_description": "A method introduced in this paper that uses GPT-3.5/GPT-4 to generate multiple candidate Chain-of-Thought (CoT) reasoning traces, filters only those that yield correct final answers, and performs weighted sampling via a contrastive score to balance usefulness and diversity for supervised fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-13B (fine-tuned)",
            "model_description": "Llama-2 family base model (13B parameters) used as the primary fine-tuning backbone; adapters + LoRA are trained for the two-stage pipeline (text-to-TG translation and TG reasoning).",
            "reasoning_method_name": "CoT bootstrapping (contrastive sampling)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generate K CoTs per example using GPT-3.5/GPT-4, discard CoTs that lead to incorrect final answers, and sample accepted CoTs for SFT according to a contrastive score combining the normalized probability of the correct answer and a plausibility-growth term; sampling (softmax over score) balances usefulness and diversity.",
            "task_name": "TGQA (primary), TimeQA and TempReason (transfer/evaluation)",
            "task_description": "Temporal reasoning QA datasets: TGQA (synthetic, controllable text→temporal-graph QA), TimeQA and TempReason (Wikipedia-derived temporal QA/benchmarks).",
            "performance": "Human-evaluation and downstream: SFT with CoT-bootstrapping (SFT + bs) reduced certain CoT error types versus plain ICL and vanilla SFT-CoT; human-eval error rates (T1..T4) reported: SFT+bs = [T1:0.06, T2:0.13, T3:0.03, T4:0.08]. Downstream EM/F1/Acc improved over ICL and over vanilla CoT distillation (see paper Figures/Tables).",
            "comparison_with_other_method": true,
            "performance_other_method": "Vanilla CoT distillation (SFT-CoT) human-eval error rates: [T1:0.04, T2:0.17, T3:0.13, T4:0.10]; ICL error rates higher (ICL: [0.13,0.13,0.31,0.10]). Quantitative downstream metrics (EM/F1/Acc) are reported in paper showing SFT+bs improves over these baselines (see main tables/figures).",
            "key_findings": "Bootstrapping CoTs with contrastive sampling that enforces correct final answers and balances usefulness/diversity produces more reliable intermediate reasoning traces and improves downstream temporal-reasoning performance compared to in-context CoTs and vanilla CoT distillation.",
            "counter_examples_or_negative_results": "Not all error types uniformly improved: for example T1 (using wrong info) was slightly higher for SFT+bs (0.06) than SFT (0.04) in the human-evaluation sample, indicating some trade-offs; overall, however, SFT+bs reduced logical inconsistency, external knowledge, and temporal graph errors.",
            "uuid": "e4760.0",
            "source_info": {
                "paper_title": "Large Language Models Can Learn Temporal Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Vanilla CoT distillation",
            "name_full": "Vanilla Chain-of-Thought distillation (SFT-CoT baseline)",
            "brief_description": "A baseline approach where CoTs produced by a teacher model are distilled into a student via supervised fine-tuning without the paper's bootstrapping/contrastive sampling or diversity-balancing steps.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama2-13B (fine-tuned baseline)",
            "model_description": "Same base model (Llama2-13B) used as a baseline when fine-tuned on CoT examples distilled directly from GPT-3.5 (vanilla distillation).",
            "reasoning_method_name": "Vanilla CoT distillation (single/standard CoT)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Collect CoT demonstrations (typically a small set per category) from a teacher LM (GPT-3.5), then supervisedly fine-tune the student LM on these CoTs (no contrastive scoring or explicit diversity balancing).",
            "task_name": "TGQA (primary) and other TR benchmarks",
            "task_description": "Temporal reasoning QA tasks used to evaluate whether distilled CoTs improve LLM temporal reasoning.",
            "performance": "Human-eval error rates reported for SFT-CoT: [T1:0.04, T2:0.17, T3:0.13, T4:0.10]; downstream metrics are better than ICL but worse than the proposed SFT with bootstrapping and augmentation (paper reports aggregated EM/F1/Acc improvements for the proposed method over SFT-CoT).",
            "comparison_with_other_method": true,
            "performance_other_method": "SFT with CoT bootstrapping and graph augmentation (SFT+bs+aug) achieved lower error rates [T1:0.03, T2:0.05, T3:0.04, T4:0.05] and higher downstream scores.",
            "key_findings": "Vanilla CoT distillation improves over plain in-context prompting but is outperformed by the paper's bootstrapping + augmentation approach; lack of curated diversity/usefulness balance in distilled CoTs leaves room for hallucinations and logical errors.",
            "counter_examples_or_negative_results": "Vanilla SFT-CoT had slightly lower T1 (using wrong info) than SFT+bs in the reported human evaluation sample, indicating distilled single-coherence CoTs can sometimes avoid certain mistakes but overall are less robust.",
            "uuid": "e4760.1",
            "source_info": {
                "paper_title": "Large Language Models Can Learn Temporal Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "ICL-CoT (few-shot)",
            "name_full": "In-Context Learning with Chain-of-Thought demonstrations",
            "brief_description": "Standard few-shot prompting where CoT examples are placed in the context (prompt) to elicit intermediate reasoning from a large pre-trained LM at inference time.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 and GPT-3.5 (evaluated in ICL)",
            "model_description": "API-accessed OpenAI models: GPT-3.5-turbo and GPT-4 (gpt-4-1106-preview) used for in-context Chain-of-Thought prompting and as CoT generators for demonstrations.",
            "reasoning_method_name": "ICL with CoT",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Provide a few demonstrations (including CoTs) in the prompt and ask the model to produce CoT + answer at inference time (no parameter updates).",
            "task_name": "TGQA / TimeQA / TempReason (ICL evaluation)",
            "task_description": "Evaluation of temporal reasoning in few-shot setting across datasets; measures the LMs' ability to reason via CoT without fine-tuning.",
            "performance": "Paper reports that among ICL methods GPT-4 is strongest; for some TGQA categories GPT-4's best in-context performance reaches ~0.6–0.7 (category-level EM) but overall is insufficient for complex TR. Human-eval CoT error rates for ICL were comparatively high: [T1:0.13, T2:0.13, T3:0.31, T4:0.10].",
            "comparison_with_other_method": true,
            "performance_other_method": "SFT (with CoTs or bootstrapping) outperforms ICL; SFT+bs+aug achieves substantially lower CoT error rates and higher downstream metrics than ICL.",
            "key_findings": "ICL with CoT can elicit reasoning but remains unreliable for temporal reasoning: hallucinated/unfaithful intermediate steps and external-knowledge errors are common; fine-tuning with supervised CoTs (especially bootstrapped/diverse ones) yields stronger and more reliable performance.",
            "counter_examples_or_negative_results": "Despite being the best ICL model, GPT-4 still shows only moderate performance in many TGQA categories (~0.6–0.7), demonstrating that powerful pre-trained models with CoT prompts alone are not a complete solution for complex temporal reasoning.",
            "uuid": "e4760.2",
            "source_info": {
                "paper_title": "Large Language Models Can Learn Temporal Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Contrastive sampling",
            "name_full": "Contrastive-scoring and weighted-sampling of CoTs (Best-of-N alternative)",
            "brief_description": "The paper's sampling mechanism for selecting CoTs uses a score combining normalized probability of the correct answer and a plausibility-growth term; sampling via softmax over scores balances usefulness and diversity among accepted CoTs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-13B (trained with sampled CoTs produced by GPT-3.5/GPT-4)",
            "model_description": "Llama2-13B adapters fine-tuned with SFT using the sampled CoTs as supervision; GPT-3.5/GPT-4 used to produce candidate CoTs and demonstrations.",
            "reasoning_method_name": "Contrastive sampling for CoTs",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Compute score(c) = log P(correct answer | context, c) + γ * G(c), where G(c) is a plausibility-growth ratio; accept only CoTs that lead to correct answers, then sample training CoTs with probability proportional to softmax(score). This trades off high-probability usefulness and diversity.",
            "task_name": "CoT generation for SFT on TGQA",
            "task_description": "Selecting which CoTs to use as SFT targets to teach the student model to produce reliable intermediate reasoning on temporal-graph inputs.",
            "performance": "Using contrastive sampling to select CoTs (as part of SFT+bs) led to improved downstream temporal reasoning and reduced CoT error rates compared to Best-of-N / naive selection; exact numeric downstream gains are reported in the paper's main results and ablation tables.",
            "comparison_with_other_method": true,
            "performance_other_method": "Best-of-N or naive selection (no contrastive scoring) results in less balanced CoT datasets and worse downstream performance; the paper reports that their contrastive-sampling increases robustness and reduces hallucination compared to conventional Best-of-N strategies (no direct numeric Best-of-N row shown but discussed qualitatively).",
            "key_findings": "Contrastive scoring + sampling provides a principled way to keep CoTs that are both useful (raise answer probability) and diverse (plausibility growth), producing better SFT data than naive Best-of-N or single-CoT distillation.",
            "counter_examples_or_negative_results": "Paper does not present a direct numeric comparison to Best-of-N in a table, but notes that contrastive selection is inspired by prior work and qualitatively yields better diversity/usefulness trade-offs; some specific error types (e.g., T1) may still increase in isolated comparisons.",
            "uuid": "e4760.3",
            "source_info": {
                "paper_title": "Large Language Models Can Learn Temporal Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Graph data augmentation",
            "name_full": "Temporal-graph data augmentation (remove edges / synonyms / global mappings)",
            "brief_description": "Training-time perturbations applied to temporal graphs to improve robustness of the reasoning model: remove irrelevant edges, replace relations with synonyms, globally remap entity names, and offset times.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-13B (fine-tuned with augmented TGs)",
            "model_description": "Llama2-13B with adapters fine-tuned on TGs that have been systematically disturbed as augmentation during SFT to make reasoning robust to graph estimation errors at inference time.",
            "reasoning_method_name": "Graph data augmentation (training-time disturbances)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Introduce controlled disturbances to training TGs: (i) remove irrelevant edges, (ii) replace relations with synonyms, (iii) globally map entity names to random same-type names, (iv) change times by a global offset; used to regularize learning so the model learns underlying logic rather than memorizing surface facts.",
            "task_name": "TGQA (training), evaluation on TGQA/TimeQA/TempReason",
            "task_description": "Temporal reasoning QA tasks where the model must reason over (possibly noisy/estimated) temporal graphs.",
            "performance": "SFT combined with CoT bootstrapping + augmentation (SFT+bs+aug) achieved the lowest human-eval CoT error rates reported: [T1:0.03, T2:0.05, T3:0.04, T4:0.05], and improved downstream EM/F1/Acc vs SFT and SFT+bs baselines.",
            "comparison_with_other_method": true,
            "performance_other_method": "SFT+bs without augmentation: human-eval [0.06,0.13,0.03,0.08]; vanilla SFT-CoT: [0.04,0.17,0.13,0.10]. Augmentation further reduces logical, external-knowledge and TG errors.",
            "key_findings": "Graph data augmentation reduces error rates and improves robustness of TG reasoning models, particularly when combined with diverse CoT supervision; it mitigates discrepancies between training on ground-truth TGs and inference on estimated TGs.",
            "counter_examples_or_negative_results": "Some augmentation operations require care (paper warns to avoid confusing the LLM); mapping entity names/times necessitates corresponding changes in QA annotations. No explicit failure cases quantified beyond these cautions.",
            "uuid": "e4760.4",
            "source_info": {
                "paper_title": "Large Language Models Can Learn Temporal Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "SFT-TGR (TG-LLM full pipeline)",
            "name_full": "Supervised Fine-Tuning for Temporal Graph Reasoning (full TG-LLM pipeline)",
            "brief_description": "The paper's two-stage system: text→temporal-graph translation (SFT) then temporal-graph reasoning via SFT enhanced by CoT bootstrapping and graph augmentation; the full pipeline aims to improve LLM temporal reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-13B (SFT-TGR variant)",
            "model_description": "Llama2-13B with two adapter modules trained in parallel: one for text-to-TG translation and one for TG reasoning; training uses TGQA (synthetic) plus bootstrapped CoTs and graph augmentation.",
            "reasoning_method_name": "Two-stage TG-LLM with CoT bootstrapping + augmentation",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "First translate story text into a latent temporal graph (TG) via SFT; then reason over TG with supervised CoTs obtained via bootstrapping and diversified by contrastive sampling and graph augmentations to train robust deliberate reasoning.",
            "task_name": "TGQA (training) → evaluation on TGQA/TimeQA/TempReason (transfer)",
            "task_description": "TGQA: synthetic, controllable temporal-graph QA; TimeQA/TempReason: real-world/Wikipedia-derived temporal reasoning benchmarks used for transfer evaluation.",
            "performance": "Paper reports that SFT-TGR substantially outperforms ICL baselines and vanilla SFT baselines on TGQA and transfers well to TimeQA and TempReason; the Llama2-13B based SFT-TGR matches or exceeds GPT-4 on the evaluated datasets (qualitative/aggregate claim; exact per-dataset numbers in main tables). Example human-eval and ER reductions shown (see SFT+bs+aug row).",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared to ICL (ICL-SP/ICL-CoT) and SFT-SP/SFT-CoT baselines, SFT-TGR (full) gives higher EM/F1/Acc and reduces CoT error rates; see main tables and Figure 4-5 for comparisons.",
            "key_findings": "The two-stage TG-LLM paradigm plus diverse, quality-controlled CoT supervision and graph augmentation yields large gains in temporal reasoning, and learned TG-translation + TG-reasoning capabilities generalize to other TR datasets.",
            "counter_examples_or_negative_results": "CoTs are not uniformly beneficial in naive form (paper notes CoTs sometimes hurt TR due to hallucinated intermediate steps), which motivated the bootstrapping and augmentation; certain isolated error measures (e.g., T1) saw small regressions when introducing bootstrapping alone, resolved when augmentation is added.",
            "uuid": "e4760.5",
            "source_info": {
                "paper_title": "Large Language Models Can Learn Temporal Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-Consistent Chain-of-Thought Distillation",
            "rating": 2,
            "sanitized_title": "selfconsistent_chainofthought_distillation"
        },
        {
            "paper_title": "Large Language Models Are Zero-Shot Reasoners",
            "rating": 1,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Are Large Language Models Temporally Grounded?",
            "rating": 1,
            "sanitized_title": "are_large_language_models_temporally_grounded"
        },
        {
            "paper_title": "TimeQA: A dataset for answering time-sensitive questions",
            "rating": 2,
            "sanitized_title": "timeqa_a_dataset_for_answering_timesensitive_questions"
        }
    ],
    "cost": 0.0166245,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models Can Learn Temporal Reasoning
8 Oct 2024</p>
<p>Siheng Xiong sxiong45@gatech.edu 
Georgia Institute of Technology</p>
<p>Ali Payani apayani@cisco.com 
Cisco Research</p>
<p>Ramana Kompella rkompell@cisco.com 
Cisco Research</p>
<p>Faramarz Fekri faramarz.fekri@ece.gatech.edu 
Georgia Institute of Technology</p>
<p>Large Language Models Can Learn Temporal Reasoning
8 Oct 20249D731E75EB7D5CE2AC3345E07738274AarXiv:2401.06853v6[cs.CL]
While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies.Recent studies have introduced various methods to mitigate these limitations.Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal concepts and intricate temporal logic.In this paper, we propose TG-LLM, a novel framework towards languagebased TR.Instead of reasoning over the original context, we adopt a latent representation, temporal graph (TG) that enhances the learning of TR.A synthetic dataset (TGQA), which is fully controllable and requires minimal supervision, is constructed for fine-tuning LLMs on this text-to-TG translation task.We confirmed in experiments that the capability of TG translation learned on our dataset can be transferred to other TR tasks and benchmarks.On top of that, we teach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought (CoT) bootstrapping and graph data augmentation.We observed that those strategies, which maintain a balance between usefulness and diversity, bring more reliable CoTs and final results than the vanilla CoT distillation. 1</p>
<p>Introduction</p>
<p>As one of the fundamental abilities, temporal reasoning (TR) plays an important role in human perception.It is not just about understanding basic concepts such as ordering or duration; it extends to more intricate aspects, e.g., task planning or causal relation discovery.Recently, large language models (LLMs) (Ouyang et al., 2022;Achiam et al., 2023;Touvron et al., 2023a) have emerged with some reasoning capabilities (Huang and Chang, 2022).However, there is observation that they still can not perform TR sufficiently well (Wang and Figure 1: Our framework (TG-LLM) performs temporal reasoning in two steps: 1) Text-to-Temporal Graph translation: generate (relevant) temporal graph given the context and keyword (extracted from questions); 2) Temporal Graph Reasoning: perform Chain-of-Thought reasoning over the temporal graph.Zhao, 2023;Chu et al., 2023;Qiu et al., 2023), preventing their applications from solving complex real-world problems.In particular, TR requires a combination of various skills including mathematical and logical reasoning as well as commonsense knowledge (Zhou et al., 2019;Vashishtha et al., 2020;Qin et al., 2021).</p>
<p>Recent works mainly adopt general approaches to investigate and improve the TR capability of LLMs.For example, (Wang and Zhao, 2023;Chu et al., 2023;Qiu et al., 2023) benchmark the leading LLMs on different TR tasks with standard Input/Output prompt, few-shot in-context learning (ICL) and Chain-of-Thought (CoT) reasoning (Wei et al., 2022).Similarly, (Wei et al., 2023) designs several specific types of prompts as prompt tuning.(Li et al., 2023;Yuan et al., 2024) introduce predefined Python programs/rule-based templates to perform supervised fine-tuning (SFT).In addition, (Tan et al., 2023a,b) adopt some extra strategies, which include specific pre-training, instruction tuning and reinforcement learning.</p>
<p>Despite the effectiveness of such methods, they either ignore or not explicitly involve the intrinsic nature of TR.Humans perform complex TR on a timeline of events which are aligned with the entities and relations.These temporal concepts (e.g., ordering, duration, frequency, typical time) are then rigorously defined based on the timeline information.In other words, the aligned timeline (more generally, the temporal graph, TG) serves as a latent representation to help humans understand the patterns in TR.However, due to the lack of ground truth, the high-quality TG translation is a challenging task for most TR benchmarks.To solve this problem, we propose a synthetic dataset (TGQA), which is fully controllable and requires minimal supervision.We demonstrate the capability of TG translation learned on our dataset can be transferred to other TR tasks and benchmarks.</p>
<p>Given a reliable TG, the key challenges of teaching TR to LLMs include: (1) How can one introduce the necessary arithmetic and commonsense knowledge involved in TR? Prior work (Lewis et al., 2020) shows that explicitly introducing knowledge into context enhances the performance of LLMs.In this paper, we first identify all the valid time expressions, and then generate related knowledge (e.g., temporal relation and time gap between the timestamps, and the relative order of the gaps).</p>
<p>(2) How can one teach LLM to perform deliberate reasoning?Generally, there exist two roadmaps: (i) translating natural language into logical statements, and using external symbolic engine for reasoning (Pan et al., 2023); (ii) using LLMs directly as the reasoning engine (Zhu et al., 2023).For (i), the difficulty lies in accurate translation (Yang et al., 2023c) and the limited expressive power of formal logic.For (ii), there is no guarantee for the correctness of generated intermediate steps especially with insufficient training data (Yang et al., 2023b).In this paper, we adopt (ii) with the proposed bootstrapping method to generate reliable intermediate steps for supervised fine-tuning.We further improve the model performance with graph data augmentation, which mitigates the data deficiency in TR tasks.</p>
<p>To be specific, our contributions are summarized as follows:</p>
<p>• We propose a new paradigm, TG-LLM, for language-based TR.In this framework, we first translate the context into a latent representation (temporal graph), and then perform reasoning on it.Extensive experiments prove that our novel approach results in superior performance compared to the baselines.</p>
<p>• We design two approaches including Chainof-Thought bootstrapping and graph data augmentation to teach LLM to generate consistent and faithful CoTs, which brings better performance than the vanilla CoT distillation.</p>
<p>• We present a pipeline to create a synthetic dataset (TGQA) for question answering that requires TR.It is fully controllable and requires minimal supervision for text-temporal graph alignment.We show in experiments that fine-tuning on our dataset benefits LLM on other TR tasks and benchmarks.</p>
<p>Dataset Construction</p>
<p>In this section, we present the construction pipeline for TGQA dataset that is fully controllable and re- quires minimal supervision for text-temporal graph alignment.Compared with existing datasets (Chen et al., 2021;Tan et al., 2023a), we have the groundtruth timelines and more diverse categories and types of TR questions (Table 2).More importantly, the pipeline can be used for various scenarios and tasks.We first split the large temporal knowledge graph, YAGO11k (Dasgupta et al., 2018), into subgraphs with a restriction on the number of events, and anonymize the entities to avoid data leakage.Then each story is translated from the subgraph by GPT-3.5 (Ouyang et al., 2022).By using some rule-based templates, we obtain reliable question and answer (QA) pairs from the graph.Finally, to reduce the noise introduced from the misalignment between the subgraph and generated story, we propose a semi-automatic verification method.</p>
<p>Step 1: Graph Splitting &amp; Anonymization.Existing temporal knowledge graphs (Leetaru and Schrodt, 2013;Dasgupta et al., 2018;García-Durán et al., 2018) usually have a large size.To facilitate the learning process, we split YAGO11k into subgraphs for story generation.Specifically, given a certain entity, we find its neighbors within three hops, and extract all the events happening between them.Since we hope LLMs to do reasoning instead of memorization, it is ensured that no overlapping exists between the events in training, validation and test sets.Additionally, we notice the data leakage problem of LLMs, i.e., prior knowledge of the test data has been implicitly obtained from the extensive pre-training.Thus, an anonymization strategy, i.e., changing entity names into random ones of the same type, is adopted.For each relation, we generate a global mapping of entity names with GPT-3.5.To avoid confusion, we adopt obscure names that do not exist in YAGO11k.</p>
<p>Step 2: Graph-based Open QA Creation.In TGQA, each sample is in the form of (temporal graph, story, questions, answers) (Table 1).Based on the given subgraph, we generate a story and multiple QAs.We first ask GPT-3.5 to write a story based on the subgraph with the requirement to include all the events.It is observed that GPT-3.5 tends to ignore the end time of some events in the created story.To solve this problem, we separate the start and end time of the same event in the prompt.On the other hand, to obtain a comprehensive benchmark, we consider all types of temporal reasoning, which include sequencing, duration, frequency, simultaneity, temporal relation, comparative analysis and facts extraction.Given these categories, we design multiple question types (Table 2) with a rule-based Python script to generate the corresponding Qs and As.</p>
<p>Step 3: Quality Control.In TGQA, noise might be introduced from the misalignment between the given subgraph and LLM-generated story.To address this problem, we propose a semi-automatic verification method.Fully manual inspection is expensive, but by first utilizing LLM we can narrow down potential errors, and only manually inspect the set of unanswered questions by LLM.Specifically, given a generated story, GPT-3.5 is queried on the time of each event in the graph.If it cannot give the correct answer, we consider the event as possibly missing in the story, which requires further manual verification.We proved the effectiveness of our semi-automatic pipeline with fully manual verification of the test stories.Note that su-pervision is only required for story-TG alignment verification, since all the QAs are generated from rules.We show dataset statistics in Appendix A, and all the prompts involved in Appendix C.</p>
<p>TG-LLM</p>
<p>Motivated by human perception, we propose a new paradigm called TG-LLM.We first translate the text into a temporal graph (TG), and then guide the LLM to perform deliberate reasoning on it.</p>
<p>Text-to-TG Translation</p>
<p>Although LLM (with ICL) might have such capability, we observed a misalignment between the generated TG and pre-defined QAs, i.e, LLM making mistakes or focusing on irrelevant events.Since TG serves as the foundation of the following deliberate reasoning process, we provide a pipeline for high-quality TG data generation to fine-tune the LLM.Ground-truth TG Generation.For some datasets such as TGQA, a verified TG (corresponding to the story) is provided.We can directly fine-tune the LLM on this task with the ground truth.However, for most real-world applications, the biggest challenge is the lack of ground truth TG.We provide a pipeline for high-quality TG data generation.We first extract all the entities and relations from the QAs to help LLM focus on specific events.We then identify all the valid time expressions in the story.We provide these information to LLM for better alignment in TG construction, and verify the generated TG in a semi-automatic way.</p>
<p>(1) Entity &amp; Relation Extraction: To obtain the entity and relation in pre-defined QAs, we consider two strategies: parsing with rules or using GPT-3.5.Rule-based parsing, if applicable, is more efficient and reliable, which is prioritized in our experiments.</p>
<p>(2) Temporal Info Identification: To identify all the valid time expressions, we first use GPT-3.5 to extract from the story, and then adopt a rule-based Python script for invalid output filtering and normalization.Besides bringing better alignment, the pre-processing of time expressions facilitates the introduction of external knowledge (their temporal relations and time gaps) into TR.We proved in experiments that explicitly introducing this information further improves model performance.</p>
<p>(3) TG Construction &amp; Verification: We generate the TG using GPT-3.5 with ICL.Specifically, we provide several in-context demonstrations which include a story and extracted entities, relations and time expressions as input and the corresponding TG as output (Table 15).For those stories without explicit time expressions, we skip Step (2), and ask GPT-3.5 to provide the temporal order of events.Similar to TGQA, we verify the generated TG by first asking GPT-3.5 the pre-defined QAs, and then manually checking the alignment if GPT-3.5 fails.Supervised Fine-tuning.We first conducted experiments to decide the best in-context format of TG.It is found out that providing a chronological list of events helps LLM perform better TR.In addition, it offers advantages to separate the start and end time of the same event.With these steps, the TG in context is transferred into a timeline with the alignment between entities, relations and times.We perform supervised fine-tuning (SFT) using Llama-2 model (Touvron et al., 2023b) with Low-Rank Adaptation (LoRA) (Hu et al., 2021).</p>
<p>The input and output of the model are the story and aligned timeline, respectively.In experiments, we observe benefits from the SFT on our dataset to other temporal reasoning tasks.</p>
<p>Temporal Graph Reasoning</p>
<p>Given the generated TGs, we teach LLM deliberate reasoning with SFT enhanced by CoT bootstrapping and graph data augmentation.</p>
<p>Bootstrapping Chain of Thoughts</p>
<p>It is observed that SFT on reliable CoTs brings better reasoning performance than that on standard Input/Output prompts (Wang et al., 2023;Ho et al., 2022).Since asking humans to create the CoT data is not scalable, we use LLM to replace humans.This task is non-trivial for reasoning over knowledge graphs (Saparov and He, 2022)
, Q 2 , • • • , Q M }.
For each category Q i , we randomly choose N i samples {(g j , e j , q j , a j )} N i j=1 , where g, e, q, a denote TG, external knowledge, question and answer, respectively, and ask both GPT-3.5 and GPT-4 to provide diverse CoTs {c j } N i j=1 .These CoTs will then be manually verified as ICL examples.Given a new training sample (g j ′ , e j ′ , q j ′ ), we bootstrap K CoTs with final answers {(c j ′ ,k , âj ′ ,k )} K k=1 from GPT-3.5.We first refuse the CoTs leading to incorrect answers, i.e., âj ′ ,k ̸ = a * j ′ , where a * j ′ denotes the correct answer.For the accepted CoTs, we consider a weighted sampling strategy to balance usefulness and diversity.The sampling probability P sample (•) is based on a contrastive learning score (Eq.1).The score design, inspired by (Wang et al., 2023), considers both normalized probability P (•) of the correct answer and plausibility growth G(•) (Eq.2).The definition of G(•) and P (•) are given in Eq. 3 and Eq. 4, respectively.
P sample (c k ) = softmax(score(c k )) (1) score(c k ) = log P a * | q † , c k + γG (c k ) (2) G (c k ) = log P a * | q † , c k P{a ′ ∈A ′ } (a ′ | q † , c k ) (3) log P a | q † , c k = 1 |a| |a| l=0 log P t l | q † , t &lt;l (4)
where c k denotes the current CoT in consideration; q † := {g, e, q}; a ′ denotes a certain wrong answer from the candidate set A ′ ; weight γ is a hyperparameter; t l denotes the l-th token in a, and t &lt;l denotes the sequence of tokens before t l .</p>
<p>Graph Data Augmentation</p>
<p>Compared with other tasks, reasoning suffers more from data insufficiency since more information (such as evidence, arguments, and logics) are involved in the intermediate steps (Huang and Chang, 2022).To address this issue, we propose several graph data augmentation strategies (Figure 3).Our framework involves two steps: text-to-TG translation and temporal graph reasoning.Notably, for the reasoning part, the model is trained over ground-truth/verified TGs but infers on the estimated graphs.This discrepancy actually hurts the robustness of reasoning.Thus, we introduce some disturbances on the TG during training.Note that the type of disturbances should be carefully designed in order to avoid confusing the LLM.We first investigate two types of disturbances: (i) remove irrelevant edges (Eq.5) and (ii) replace edges by using relation synonyms (Eq.6).An edge (event) is considered irrelevant if not involved in both QA and CoT.</p>
<p>F irr ∈ {F : P (c|g {F } , e, q) ≈ P 0 } F ∈g (5)
F ′ ∈ {f R (F ) : P (c|f R (g), e, q) ≈ P 0 } F ∈g (6)
where P 0 := P (c|g, e, q) denotes the original conditional probability, irrelevant event F irr will be randomly removed from g, and disturbed event F ′  Furthermore, to make sure LLM learns the underlying logic of TR instead of just memorizing semantic information, we introduce another two types of disturbances: (iii) globally map all the entity names in training data to some random names of the same type (Eq.7) , and (iv) change the times based on a global offset (Eq.7).For each relation, we generate these random entity names with GPT-3.5 by providing several examples of existing names.
g ′ = f T (f E (g)), e ′ = f T (e), q ′ = f T (f E (q)), c ′ = f T (f E (c)), a ′ = f T (f E (a)) (7)
where f E (•), f T (•) denote the global mapping of entity names and time changing, respectively.</p>
<p>Experiments</p>
<p>We aim to answer the following research questions in our experiments: (1) Can our strategies (CoT bootstrapping and graph data augmentation) bring more reliable reasoning over TGs? (2) Can our twostep framework lead to better TR performance?(3) Do these learned capabilities of TR generalize to other tasks?</p>
<p>Experimental Setup</p>
<p>We demonstrate TG-LLM is a general framework by applying it to, besides TGQA, the two existing datasets, TimeQA (Chen et al., 2021) and TempReason (Tan et al., 2023a), which are constructed using Wikipedia articles, excerpts, and summaries.Examples from other datasets are listed in Appendix B. We thoroughly evaluate our framework on all the datasets with a combination of the metrics, i.e., token-level F1, exact match (EM) and perplexity-based accuracy (Acc).Besides choosing F1 and EM, which are two basic metrics for span-based QA tasks, we consider Acc for LLM evaluation, i.e., selecting from a candidate set the final answer with the lowest perplexity as the prediction.The rationale and detailed construction of the candidate set for all datasets are listed in Appendix B.</p>
<p>We primarily compare our framework with the leading LLMs, i.e., Llama2 (Touvron et al., 2023b), GPT-3.5 (Ouyang et al., 2022) and GPT-4 (Achiam et al., 2023).Specifically, we ran the Llama2 family on local machines (4-bit quantization for Llama2-70B due to limited computational resources), and used APIs provided by OpenAI for GPT models.We evaluated their few-shot ICL performance on the test set.To prove the effectiveness of our method, we also show the model performance with SFT on standard Input/Output prompts (SP) and CoTs from GPT-3.5.The model versions and prompt templates are provided in Appendix B and C, respectively.We also include the T5-based models (Tan et al., 2023a;Yang et al., 2023a) for a comprehensive comparison.Results on TimeQA and TempReason reported in the original papers are used, and these models are fine-tuned and evaluated on TGQA.</p>
<p>Implementation Details</p>
<p>We use Llama2-13B as the baseline due to limited computational resources.We inject two adapters with selectors into the base model for the textto-TG translation and temporal graph reasoning.The adapters are trained in parallel.For inference, we first translate the original story into a temporal graph, and then perform reasoning on it, i.e., the adapters are used in sequence.For data generation, we use GPT-3.5 for story, TG and CoT generation, and the verification of stories and TGs.We use GPT-4 to create the ICL demonstrations of CoT generation, due to its high generation quality.All the prompt templates are given in Appendix C.    4: Main results using different models and strategies.We report exact match (EM), token-level F1 scores, and perplexity-based accuracy (Acc).Note: (1) Results with † are reported in the original papers.We only fine-tune and evaluate the models on our dataset.(2) Results with * are evaluated on 1000 random test samples.</p>
<p>Main Results</p>
<p>Can our strategies bring more reliable reasoning over TGs?We show the comparison between ICL with CoTs, SFT with CoTs, bootstrapping CoTs, and graph data augmentation on TGQA (Figure 4).It can be seen that LLM learns TR better with SFT than ICL.By providing CoTs with bootstrapping and graph data augmentation strategies, the model performance gets further enhanced.Furthermore, inspired by (Wang et al., 2023), we manually check 100 CoTs generated by different strategies (Table 3).Evaluators are asked to classify the errors into four types (using wrong info, logical inconsistency, external knowledge error, and temporal graph error).It can be seen that our strategies reduce all types of error rate.</p>
<p>Can our two-step framework lead to better temporal reasoning performance?We show the comparison between different models and strategies on all datasets (Table 4).First, we observed that among all the LLMs with ICL, GPT-4 has the strongest performance as expected.For the Llama2 family, larger models have better performance due to advanced context understanding and improved generalization.We also found that CoTs not always bring better TR due to unreliable intermediate results and hallucinations.From the results of some alternative strategies, where SFT-SP and SFT-CoT denote supervised fine-tuning on standard Input/Output prompts and vanilla CoT distillation, respectively, we prove the effectiveness of our framework (SFT-TGR).More importantly, our model, which is based on Llama2-13B, shows a comparable or even better performance than GPT-4 on all datasets.It can be seen that the two-step framework brings a substantial performance improvement on different datasets.We hypothesize this performance improvement is because our twostep reasoning process provides an easier path toward answering the temporal questions for LLM.Do these learned capabilities of temporal reasoning generalize to other tasks?We show the comparison between different strategies (ICL with SP/CoT, SFT with TGQA/original data) on the two existing datasets, TimeQA and TempReason (Figure 5).Our framework learns the capabilities of text-to-TG translation and temporal graph reasoning, which brings better TR.More importantly, we observed that SFT on TGQA improves the model performance compared with ICL.It can be concluded that these necessary capabilities in TR are generalizable to different data distributions.Since TGQA is fully controllable and requires minimal supervision, we actually provide a general and effective way of TR capability improvement.</p>
<p>Ablation Study</p>
<p>We ablate different modules to see their contributions to the performance.We show the performance comparison between different configurations (Table 5).To obtain a fair comparison, we use Llama2-13B as the base model for all configurations.From the ablation study, we obtain some insights: (1) LLM can benefit from explicitly presented (temporal) graph which is intuitive, concise and structured.</p>
<p>(2) Given a reliable graph, CoT bootstrapping with contrastive learning brings better performance than vanilla CoT distillation.(3) Data augmentation is necessary for LLMs to perform complex tasks such as temporal reasoning.(4) The introduction of external knowledge such as mathematics and commonsense can further augment the generation.</p>
<p>Related Work</p>
<p>Language-based Temporal Reasoning.Recently, language-based TR has gained substantial research focus (Liu et al., 2023(Liu et al., , 2024b;;Chen et al., 2024a,b;Wang et al., 2024;Jiayang et al., 2024;Xia et al., 2024).The vision here is to help LMs understand temporal concepts and logic such that they can perform more complicate tasks.Existing methods mainly solve this problem with time-aware language modeling.For example, (Rosin et al., 2022;Pereira, 2022;Tan et al., 2023a) propose specific pre-training/fine-tuning strategies for robust TR.On the other hand, (Ning et al., 2019;Zhou et al., 2020a,b;Yang et al., 2020Yang et al., , 2023a) )  knowledge.Although these methods made some progress, representation learning for the underlying structure and logic of TR are either ignored or not explicitly involved.Reasoning towards LMs.Although LLM has exhibited some emergent behaviors (Zhang et al., 2024a;Lai et al., 2024a,b;Lyu et al., 2024), it is still unknown whether they can actually perform reasoning and how strong their capability of reasoning is.Existing methods that try to elicit or enhance reasoning can be divided into two directions: reasoning-involved modeling or hybrid methods.For example, (Wei et al., 2022;Kojima et al., 2022;Yao et al., 2024) use in-context learning where some demonstrations including intermediate reasoning steps are provided in prompt.(Nye et al., 2021;Wang et al., 2023;Ho et al., 2022) fine-tune LMs with the intermediate thinking process before producing final answers.On the other hand, (Schick et al., 2024;Kynoch et al., 2023;Xu et al., 2023) combine LMs with domain-specific external tools, empowering the model to perform more complex tasks that require reasoning and interactions with environment.Reasoning over Knowledge Graphs.Knowledge graphs (KGs) as the foundation representation of semantic and symbolic reasoning have been widely adopted in the past (Huang et al., 2023;Wan et al., 2024;Li et al., 2024).Related work includes symbolic reasoning over temporal KGs (Yang et al., 2022;Xiong et al., 2023Xiong et al., , 2024)), and languagebased reasoning over static KGs (Cheng et al., 2024;Zhang et al., 2024b;Liu et al., 2024a;Zhao et al., 2024;Xu et al., 2024;Wei et al., 2024).In this paper, we build the connection between language-based and symbolic-based TR.This connection brings the potential for extending these KGbased methods to language-based tasks.Different from existing methods (Luo et al., 2023;Zhang et al., 2023;Yuan et al., 2024;Gao et al., 2024), which limit their application to certain tasks, our framework offers enhanced generalization and usability, largely attributed to its innovative use of text-to-graph translation as a precursor to graph reasoning.</p>
<p>Conclusion</p>
<p>TG-LLM, a novel framework for LMs, has been proposed to improve their performance on temporal reasoning.To produce reliable final answers, our framework equips LLMs with the temporal graph and intermediate reasoning steps.Extensive experiments indicate that TG-LLM achieves better performance than existing pipelines.An interesting direction for future work is to extend it to more complex applications such as inductive and abductive reasoning.Due to the graph structure and capability of deliberate reasoning, it is promising to improve the model performance on these tasks as well.</p>
<p>Limitations</p>
<p>Graph-augmented approaches (Jin et al., 2024;Fan et al., 2024;Shang and Huang, 2024;He and Hooi, 2024) including TG-LLM help language models better learn related concepts from the perspective of graph.Although we demonstrate TG-LLM has good performance on understanding temporal relations, it still needs adaptations for temporal commonsense reasoning (Zhou et al., 2019;Qin et al., 2021).Explicit in-context integration of commonsense presents opportunities for this task.Further, we mainly improve the capability of LLMs by introducing a new paradigm and providing more plausible and informative training data.There can be opportunities such as simulating an environment to provide feedback to LLMs (Hao et al., 2023).For example, we can verify the generated TG based on prior knowledge such as the time gap between someone's birth date and death date, i.e., a person's lifespan, should fall into a certain range.In this way, we can further improve the performance.</p>
<p>Figure 2 :
2
Figure 2: In Chain-of-Thought (CoT) bootstrapping, we only accept CoTs that lead to correct final answers and sample them according to their contrastive learning scores to balance usefulness and diversity.</p>
<p>Figure 3 :
3
Figure 3: We further boost the model performance with several graph data augmentation strategies: remove irrelevant edges, use relation synonyms and change entities/times.</p>
<p>Figure 4 :
4
Figure 4: Performance comparison between different CoT generation strategies on TGQA.</p>
<p>Figure 5 :
5
Figure 5: Performance comparison between different strategies on TimeQA and TempReason.To obtain a fair comparison, we use Llama2-13B as the base model for all strategies.The basic strategy used to calculate the performance changes is in-context learning with standard Input/Output prompt (ICL-SP).</p>
<p>Figure 6 :
6
Figure 6: Number of facts distribution in TGQA.</p>
<p>Figure 7 :
7
Figure 7: Types of questions distribution in TGQA.</p>
<p>Table 1 :
1
Each sample from TGQA dataset is in the form of (temporal graph, story, questions, answers).
Temporal Graph [sub; rel; obj; start/end; time]:[1] (John Thompson was born in Weston) starts at 1921;[2] (John Thompson owned Pearl Network) starts at 1942;[3] (Sophia Parker was married to John Thompson) startsat 1947; [4] (John Thompson was married to Sophia Parker)starts at 1947; [5] (Sophia Parker was married to JohnThompson) ends at 1953; [6] (John Thompson was marriedto Sophia Parker) ends at 1953; [7] (John Thompson ownedPearl Network) ends at 1967; [8] (John Thompson died inRiverside) starts at 1988; [9] (Sophia Parker died inLancaster) starts at 1995.Graph-based Story (from GPT-3.5):Once upon a time in the quaint town of Weston, a babyboy named John Thompson was brought into the worldin the year 1921. Growing up, he had a vibrant spiritand an adventurous soul. • • •Graph-based QAs (from rule-based Python script):Q1: Which event started first, (John Thompson ownedPearl Network) or (John Thompson was married toSophia Parker)?A1: (John Thompson owned Pearl Network).Q2: True or false: event (John Thompson owned PearlNetwork) was longer in duration than event (SophiaParker was married to John Thompson)?A2: True.• • •</p>
<p>Table 2 :
2
Reasoning types with the corresponding questions and answers in TGQA.
Reasoning Type Question</p>
<p>. In this section, we propose a bootstrapping pipeline, i.e., given a query, using LLM to generate several CoTs and selecting them as training data with a weighted sampling strategy.Compared with the conventional Best-of-N sampling, our proposal allows more training data diversity.Specifically, we first prepare ICL examples for high-quality CoT generation.To facilitate the learning, the pre-defined QAs are classified into different categories {Q 1</p>
<p>Table 3 :
3
SFT-TGR) 0.797 0.850 0.819 0.664 0.691 0.673 0.631 0.664 0.649 0.424 0.522 0.432 0.356 0.469 0.399
StrategyER-T1 ER-T2 ER-T3 ER-T4ICL0.130.130.310.10SFT0.040.170.130.10SFT + bs0.060.130.030.08SFT + bs + aug0.030.050.040.05
Human evaluation on the generated CoTs by different strategies.ER: error rate; T1: using wrong info; T2: logical inconsistency; T3: external knowledge error; T4: temporal graph error.(Error type explanations are listed in Appendix B.)</p>
<p>AcknowledgementsThis work was supported by a sponsored research award by Cisco Research.Ethics StatementIn this paper, we adopt YAGO11k for fine-tuning the language models.The dataset is publicly available, and is for research purposes only.We also use GPT model to generate text based on YAGO11k, for which OpenAI has been committed to addressing ethical considerations.In addition, we adopt TimeQA and TempReason for evaluation.Both datasets are publicly available, and are for research purposes only.However, they may still contain improper or harmful content.None of such content reflects the opinions of the authors.A Dataset Statistics of TGQAIn this section, we provide statistics for our TGQA dataset.We obtain 400 samples for training, 100 for validation and another 100 for test, with about 30 QA pairs in a single sample.We show the distribution of the number of facts (in one sample) in training (with validation) and test set(Figure 6).It determines the complexity of the TR tasks to some extent.We also show the distribu- Table6: We use a global mapping for entity names from GPT-3.5 to avoid data leakage.External Knowledge 1885 before1893 before 1916 before 1918 before 1922 before 1928 before 1941 1918 -1916 = 2 1928 -1893 = 35 1928 -1922 = 6 1941 -1918 = 23 2 &lt; 6 &lt; 23 &lt; 35Table7: We integrate the necessary mathematics and commonsense in context as external knowledge for TR.tion of question types in training (with validation) and test set (Figure7), where Q0: "Which event occurred first, <Event_A> or <Event_B>?",Q1: "Given the following <N> events: <Event_A>, <Event_B>, <Event_C>, <Event_D>, • • • , which event is the first/second/third/fourth/• • • one in the chronological order?",Q2: "How long did the event <Event_A> last?", Q3: "", Q4: "How much time passed between the start of <Event_A> and the start of <Event_B>?",Q5: "What happened right before/after <Event_A> started?", Q6: "When did the <Event_A> occur?",Q7: "True or false: <Event_A> and <Event_B> happened at the same year?",Q8: "True or false: <Event_A> was still happening when <Event_B> started?".Note that we give eight question categories in Table2, since in a strict sense Q0 and Q1 can be considered as the same type.Among all the question types, Q1 has the largest portion since it has multiple variants.Similarly, Q7 with two variants also has a larger portion.To mitigate question category imbalance, we first calculate the metrics for each category, and then use the average as the final scores.Additionally, we show examples for the global mapping for entity names (Table6) and external knowledge (Table7) used in our dataset.B Experiment DetailsIn this section, we present more experiment details for further research.We include graph data aug-1942, ends at 1967, 1967-1942= 25 (Sophia Parker was married to John Thompson) starts at 1947, ends at 1953, 1953-1947 = 6 251977, 1977 -1952 = 25(Ella Perry was married to James Brown) starts at1957 , ends at 1963, 1963 -1957 = 625 is greater than 6 , thus, the answer is True.A: True.Graph Data Augmentation.We show with an example our graph data augmentation strategies in Table8.For TGs with relation synonym replacement or irrelevant edges removal, there is no need for change on graph-based QAs.To contrast, for TGs with entity names/times mapping, we need to change with the corresponding entity names/times in graph-based QAs to ensure the consistency.Evaluation Tasks &amp; Metrics.Besides TGQA, we consider in experiments the two existing datasets TimeQA(Chen et al., 2021)and TempReason(Tan et al., 2023a).Specifically, TimeQA contains two difficulty levels (Table9).The easy-level split tends to be the information extraction task while the hard-level split involves understanding the relation between different temporal expressions.On the other hand, TempReason contains three levels (L1: Time-Time Relation, L2: Time-Event Relation, L3: Event-Event Relation) and three settings (OBQA: open-book QA, CBQA: close-book QA, ReasonQA: facts-based QA) (Table10).We observed that some stories in TempReason are incomplete which partially leads to the low accuracy of LLMs.We evaluate our framework on all the datasets with the metrics of token-level F1, exact match (EM) and perplexity-based accuracy (Acc).F1 and EM are two basic metrics for span-based QA tasks.However, the free-form prediction of LLMs might hurt their performance under these generation-based metrics.To solve this problem, we introduce perplexity-based accuracy, i.e., selecting from a candidate set the final answer with the lowest perplexity.For questions with multiple correct answers, we follow the strategy proposed in(Chen et al., 2021)to get the best result among them.Since there are multiple question categories in TGQA, we first calculate the metrics for each category, and then use the average as final scores to mitigate question category imbalance.Human evaluation on CoT generation found out four types of error in total (Table3).T1 means that LLM uses wrong information such as wrong start/end time during reasoning.T2 suggests that LLM makes logical errors, e.g., mentioning "Event A ends before Event B starts" in CoT but determining the statement "Event A was still happening when Event B starts" to be true.T3 denotes that LLM makes errors on external knowledge, e.g., claiming that "the date 1978 is after 1983".T4 indicates that there exists errors in the extracted TG TimeQA George Washington (February 22, 1732 -December 14, 1799) was an American Founding Father, military officer, politician and statesman who served as the first   that lead to the wrong conclusion.Candidate Answer Generation.We involve candidate answers in the calculation of Acc.For Tem-pReason, the authors provide negative answers since they perform time-sensitive reinforcement learning.That is, they use the score of the correct answers and wrong answers from the language model as reward to further finetune the model parameter.For TimeQA, we generate the candidates in the following way.Given a story, there are multiple related questions which share the same subject/object entity and relation.The correct answer changes with the query time.For example, "What position did George Washington hold in 1777/1790/1799?"Answer: "Commander in Chief/Presidency/Chancellor".We collect the answers of these related questions as candidates.More generally, if there are no such related questions, we will use all the entities in the corresponding TG as candidates.Model Versions.The versions of the LLMs used in our experiments are listed below.For the Llama2 family, all the model weights can be downloaded from the platform of Hugging Face.For the GPT models, all the model weights can be accessed through the OpenAI APIs.• GPT-3.5 (gpt-3.5-turbo)• GPT-4 (gpt-4-1106-preview)C Example PromptsIn this section, we show example prompts used in our framework.Specifically, Table12shows an example of the graph-based story generation in TGQA; Table13presents an example of the automatic story-temporal graph alignment verification in TGQA; Table14provides an example of the temporal info identification in TimeQA; Table15illustrates an example of the graph construction in TimeQA; Table16gives an example of the automatic temporal graph-QA alignment verification in TimeQA; Table17depicts an illustrative case of the CoTs bootstrapping in TGQA.D Fine-grained Results of TGQAWe provide the fine-grained results of TGQA in Table11.In TGQA, there are nine types of questions as explained in Appendix A. We show all the models with different strategies.Zero-shot performance is also considered to investigate the effect of in-context examples.Note that we do not include zero-shot performance of the Llama2 family since they are not fine-tuned on instructions, i.e., we can not obtain valid zero-shot learning results.For zeroshot learning results, the format of generation is not Table11: Fine-grained results on TGQA using different models and strategies.We report exact match (EM) as the performance metric.Note: (1) Results with * are evaluated on 1000 random test samples.(2) Results with ‡ are parsed by GPT-3.5 during evaluation (might introduce errors).guaranteed.The original accuracy is very low since rule-based parser cannot handle it.Instead, we use GPT-3.5 as parser to extract the answer from the generation (might introduce errors).Also, we only adopt one example in context due to context length limitation (long-context LLM performance could increase with more in-context demonstrations).It can be seen that in-context demonstrations are not necessary for powerful pre-trained models such as GPT-3.5 and GPT-4.Undoubtedly, GPT-4 obtains the best in-context learning performance on most categories.However, for some categories, the best performance of GPT-4 is around 0.6 or 0.7, which is not sufficiently well.On the other hand, we show that our strategies improve the performance in a progressive manner.Note that our model achieves the best overall performance by improving on all the categories.Specifically, deliberate reasoning with CoT bootstrapping brings better performance for most categories.Furthermore, graph data augmentation and external knowledge further enhance the capability of the model while preserving the existing good performance on those categories.Once upon a time, in the vibrant city of Seattle, two remarkable individuals, Molly Adams and Liam Thomas Dawson, were born in the year 1896.Little did they know that their destinies were entwined from the very beginning.Growing up in the same neighborhood, Molly and Liam developed a deep friendship that blossomed into something more as they entered adulthood.In the year 1920, their love story officially began as they exchanged vows and embarked on a journey of companionship that would last for decades.• • •When did the event (Molly Adams was married to Liam Thomas Dawson) end?Table13: Example of the story-temporal graph alignment verification in TGQA.Knox CunninghamSir Samuel Knox Cunningham, 1stBaronet, QC (3 April 1909-29 July 1976) was a Northern Irish barrister, businessman and politician.As an Ulster Unionist politician at a time when the Unionists were part of the Conservative Party, he was also a significant figure in United Kingdom politics as Parliamentary Private Secretary to Harold Macmillan.His nephew was Sir Josias Cunningham.• • •Extract all the time expressions such as'June 1994'June ', '1973', 'late 1980s'. ', 'late 1980s'.Table14: Example of the temporal info identification in TimeQA.Knox Cunningham Sir Samuel Knox Cunningham, 1st Baronet, QC (3 April 1909-29 July 1976) was a Northern Irish barrister, businessman and politician.As an Ulster Unionist politician at a time when the Unionists were part of the Conservative Party, he was also a significant figure in United Kingdom politics as Parliamentary Private Secretary to Harold Macmillan.His nephew was Sir Josias Cunningham.• • •Construct a timeline for Knox Cunningham's position.You should only consider these time points(3 April 1909, 1930s, 1939, 1942, 1943, 1945, 1947, 1949, 1954, 29 July 1976).1918 -1916 = 2 1928 -1893 = 35 1928 -1922 = 6 1941 -1918 = 23 2 &lt; 6 &lt; 23 &lt; 35Answer: Let's think step by step.To determine whether the statement is true or false, we need to compare the durations of (Liam Mitchell was married to Maddox Reynolds) and (Emma Scott was married to Liam Mitchell) .The duration for each event can be calculated as follows:(Liam Mitchell was married to Maddox Reynolds) : (Liam Mitchell was married to Maddox Reynolds) starts at 1922 (Liam Mitchell was married to Maddox Reynolds) endsat 1928 1928 -1922 = 6(Emma Scott was married to Liam Mitchell) : (Emma Scott was married to Liam Mitchell) starts at 1916 (Emma Scott was married to Liam Mitchell) endsat 1918 1918 -1916 = 2Now, we compare the duration of the two events: 6 is greater than 2 , thus, the answer is True .
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Timelinebased sentence decomposition with in-context learning for temporal fact extraction. Jianhao Chen, Haoyuan Ouyang, Junyang Ren, Wentao Ding, Wei Hu, Yuzhong Qu, arXiv:2405.102882024aarXiv preprint</p>
<p>Wenhu Chen, Xinyi Wang, William Yang, Wang , arXiv:2108.06314A dataset for answering time-sensitive questions. 2021arXiv preprint</p>
<p>Selfimprovement programming for temporal knowledge graph question answering. Zhuo Chen, Zhao Zhang, Zixuan Li, Fei Wang, Yutao Zeng, Xiaolong Jin, Yongjun Xu, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)2024b</p>
<p>Call me when necessary: Llms can efficiently and faithfully reason over structured environments. Sitao Cheng, Ziyuan Zhuang, Yong Xu, Fangkai Yang, Chaoyun Zhang, Xiaoting Qin, Xiang Huang, Ling Chen, Qingwei Lin, Dongmei Zhang, arXiv:2403.085932024arXiv preprint</p>
<p>Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Haotian Wang, Ming Liu, Bing Qin, arXiv:2311.17667Timebench: A comprehensive evaluation of temporal reasoning abilities in large language models. 2023arXiv preprint</p>
<p>Hyte: Hyperplane-based temporally aware knowledge graph embedding. Swayambhu Shib Sankar Dasgupta, Partha Nath Ray, Talukdar, Proceedings of the 2018 conference on empirical methods in natural language processing. the 2018 conference on empirical methods in natural language processing2018</p>
<p>Wenqi Fan, Shijie Wang, Jiani Huang, Zhikai Chen, Yu Song, Wenzhuo Tang, Haitao Mao, Hui Liu, Xiaorui Liu, Dawei Yin, arXiv:2404.14928Graph machine learning in the era of large language models (llms). 2024arXiv preprint</p>
<p>Two-stage generative question answering on temporal knowledge graph using large language models. Yifu Gao, Linbo Qiao, Zhigang Kan, Zhihua Wen, Yongquan He, Dongsheng Li, 10.18653/v1/2024.findings-acl.401Findings of the Association for Computational Linguistics ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024and virtual meeting</p>
<p>Learning sequence encoders for temporal knowledge graph completion. Alberto García-Durán, Sebastijan Dumančić, Mathias Niepert, 10.18653/v1/D18-1516Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>Unigraph: Learning a cross-domain graph foundation model from natural language. Yufei He, Bryan Hooi, arXiv:2402.136302024arXiv preprint</p>
<p>Namgyu Ho, Laura Schmid, Se-Young Yun, arXiv:2212.10071Large language models are reasoning teachers. 2022arXiv preprint</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, arXiv:2212.10403Towards reasoning in large language models: A survey. 2022arXiv preprint</p>
<p>Federated graph semantic and structural learning. Wenke Huang, Guancheng Wan, Mang Ye, Bo Du, Proc. Int. Joint Conf. Artif. Intell. Int. Joint Conf. Artif. Intell2023</p>
<p>Eventground: Narrative reasoning by grounding to eventuality-centric knowledge graphs. Cheng Jiayang, Lin Qiu, Chunkit Chan, Xin Liu, Yangqiu Song, Zheng Zhang, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)2024</p>
<p>Graph chain-of-thought: Augmenting large language models by reasoning on graphs. Chulin Bowen Jin, Jiawei Xie, Kashob Zhang, Yu Kumar Roy, Suhang Zhang, Yu Wang, Jiawei Meng, Han, arXiv:2404.071032024arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Recallm: An adaptable memory mechanism with temporal understanding for large language models. Brandon Kynoch, Hugo Latapie, Dwane Van Der Sluis, arXiv:2307.027382023arXiv preprint</p>
<p>Language models are free boosters for biomedical imaging tasks. Zhixin Lai, Jing Wu, Suiyao Chen, Yucheng Zhou, Anna Hovakimyan, Naira Hovakimyan, arXiv:2403.173432024aarXiv preprint</p>
<p>Adaptive ensembles of fine-tuned transformers for llm-generated text detection. Zhixin Lai, Xuesheng Zhang, Suiyao Chen, arXiv:2403.133352024barXiv preprint</p>
<p>Gdelt: Global data on events, location, and tone, 1979-2012. Kalev Leetaru, Philip A Schrodt, ISA annual convention. Citeseer20132</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Enhancing multi-hop knowledge graph reasoning through reward shaping techniques. Chen Li, Haotian Zheng, Yiping Sun, Cangqing Wang, Liqiang Yu, Che Chang, Xinyu Tian, Bo Liu, arXiv:2403.058012024arXiv preprint</p>
<p>Unlocking temporal question answering for large language models using code execution. Xingxuan Li, Liying Cheng, Qingyu Tan, Tou Hwee, Shafiq Ng, Lidong Joty, Bing, arXiv:2305.150142023arXiv preprint</p>
<p>Grounding complex natural language commands for temporal tasks in unseen environments. Jason Xinyu Liu, Ziyi Yang, Ifrah Idrees, Sam Liang, Benjamin Schornstein, Stefanie Tellex, Ankit Shah, Conference on Robot Learning. PMLR2023</p>
<p>Xiaoze Liu, Feijie Wu, Tianyang Xu, Zhuo Chen, Yichi Zhang, Xiaoqian Wang, Jing Gao, arXiv:2404.00942Evaluating the factuality of large language models using large-scale knowledge graphs. 2024aarXiv preprint</p>
<p>Yuchen Liu, Luigi Palmieri, Sebastian Koch, Ilche Georgievski, Marco Aiello, arXiv:2404.03275Delta: Decomposed efficient long-term robot task planning using large language models. 2024barXiv preprint</p>
<p>Chatrule: Mining logical rules with large language models for knowledge graph reasoning. Linhao Luo, Jiaxin Ju, Bo Xiong, Yuan-Fang Li, Gholamreza Haffari, Shirui Pan, arXiv:2309.015382023arXiv preprint</p>
<p>Task-agnostic detector for insertion-based backdoor attacks. Weimin Lyu, Xiao Lin, Songzhu Zheng, Lu Pang, Haibin Ling, Susmit Jha, Chao Chen, 10.18653/v1/2024.findings-naacl.179Findings of the Association for Computational Linguistics: NAACL 2024. Mexico City, MexicoAssociation for Computational Linguistics2024</p>
<p>Qiang Ning, Zhili Feng, Hao Wu, Dan Roth, arXiv:1906.04941Joint reasoning for temporal and causal relations. 2019arXiv preprint</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.00114Show your work: Scratchpads for intermediate computation with language models. 2021arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023arXiv preprint</p>
<p>Attention-focused adversarial training for robust temporal reasoning. Lis Kanashiro, Pereira , Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation Conference2022</p>
<p>Lianhui Qin, Aditya Gupta, Shyam Upadhyay, Luheng He, Yejin Choi, Manaal Faruqui, arXiv:2106.04571Timedial: Temporal commonsense reasoning in dialog. 2021arXiv preprint</p>
<p>Yifu Qiu, Zheng Zhao, Yftah Ziser, Anna Korhonen, Edoardo M Ponti, Shay B Cohen, arXiv:2311.08398Are large language models temporally grounded?. 2023arXiv preprint</p>
<p>Time masking for temporal language models. Ido Guy D Rosin, Kira Guy, Radinsky, Proceedings of the fifteenth ACM international conference on Web search and data mining. the fifteenth ACM international conference on Web search and data mining2022</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, arXiv:2210.012402022arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Advances in Neural Information Processing Systems. 202436</p>
<p>Wenbo Shang, Xin Huang, arXiv:2404.14809arXiv:2306.08952Qingyu Tan, Hwee Tou Ng, and Lidong Bing. 2023a. Towards benchmarking and improving the temporal reasoning capability of large language models. 2024arXiv preprintA survey of large language models on generative graph analytics: Query, learning, and applications</p>
<p>Towards robust temporal reasoning of large language models via a multi-hop qa dataset and pseudoinstruction tuning. Qingyu Tan, Hwee Tou Ng, Lidong Bing, arXiv:2311.098212023barXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023barXiv preprint</p>
<p>Temporal reasoning in natural language inference. Siddharth Vashishtha, Adam Poliak, Yash Kumar Lal, Benjamin Van Durme, Aaron Steven White, 10.18653/v1/2020.findings-emnlp.363Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Federated graph learning under domain shift with generalizable prototypes. Guancheng Wan, Wenke Huang, Mang Ye, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Scott: Self-consistent chain-of-thought distillation. Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, Xiang Ren, arXiv:2305.018792023arXiv preprint</p>
<p>K-link: Knowledgelink graph from llms for enhanced representation learning in multivariate time-series data. Yucheng Wang, Ruibing Jin, Min Wu, Xiaoli Li, Lihua Xie, Zhenghua Chen, arXiv:2403.036452024arXiv preprint</p>
<p>Yuqing Wang, Yun Zhao, arXiv:2310.00835Tram: Benchmarking temporal reasoning for large language models. 2023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Lanning Wei, Jun Gao, Huan Zhao, arXiv:2402.11641Towards versatile graph learning approach: from the perspective of large language models. 2024arXiv preprint</p>
<p>. Yifan Wei, Yisong Su, Huanhuan Ma, Xiaoyan Yu, Fangyu Lei, Yuanzhe Zhang, Jun Zhao, Kang Liu, </p>
<p>Enhancing temporal knowledge graph forecasting with large language models via chain-of-history reasoning. Yuwei Xia, Ding Wang, Qiang Liu, Liang Wang, Shu Wu, Xiaoyu Zhang, arXiv:2310.05157arXiv:2402.14382Menatqa: A new dataset for testing the temporal comprehension and reasoning abilities of large language models. 2024arXiv preprint</p>
<p>TILP: Differentiable learning of temporal logical rules on knowledge graphs. Siheng Xiong, Yuan Yang, Faramarz Fekri, James Clayton Kerce, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Teilp: Time prediction over knowledge graphs via logical reasoning. Siheng Xiong, Yuan Yang, Ali Payani, James C Kerce, Faramarz Fekri, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Binfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng, Yuchen Liu, Ziyu Yao, Dongkuan Xu, arXiv:2308.04030Gentopia: A collaborative for tool-augmented llms. 2023arXiv preprint</p>
<p>Generate-on-graph: Treat llm as both agent and kg in incomplete knowledge graph question answering. Yao Xu, Shizhu He, Jiabei Chen, Zihao Wang, Yangqiu Song, Hanghang Tong, Kang Liu, Jun Zhao, arXiv:2404.147412024arXiv preprint</p>
<p>Once upon a time in graph: Relative-time pretraining for complex temporal reasoning. Sen Yang, Xin Li, Lidong Bing, Wai Lam, arXiv:2310.147092023aarXiv preprint</p>
<p>Sen Yang, Xin Li, Leyang Cui, Lidong Bing, Wai Lam, arXiv:2311.09802Neuro-symbolic integration brings causal and reliable reasoning proofs. 2023barXiv preprint</p>
<p>Yuan Yang, Siheng Xiong, James C Kerce, Faramarz Fekri, arXiv:2206.05051Temporal inductive logic reasoning. 2022arXiv preprint</p>
<p>Harnessing the power of large language models for natural language to first-order logic translation. Yuan Yang, Siheng Xiong, Ali Payani, Ehsan Shareghi, Faramarz Fekri, arXiv:2305.155412023carXiv preprint</p>
<p>Improving event duration prediction via time-aware pre-training. Zonglin Yang, Xinya Du, Alexander Rush, Claire Cardie, arXiv:2011.026102020arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Back to the future: Towards explainable temporal reasoning with large language models. Chenhan Yuan, Qianqian Xie, Jimin Huang, Sophia Ananiadou, Proceedings of the ACM on Web Conference 2024. the ACM on Web Conference 20242024</p>
<p>Unlocking personalized anime recommendations: Langchain and llm at the forefront. Ye Zhang, Kailin Gui, Mengran Zhu, Yong Hao, Haozhan Sun, Journal of Industrial Engineering and Applied Science. 222024a</p>
<p>Making large language models perform better in knowledge graph completion. Yichi Zhang, Zhuo Chen, Wen Zhang, Huajun Chen, arXiv:2310.066712023arXiv preprint</p>
<p>Diet-odin: A novel framework for opioid misuse detection with interpretable dietary patterns. Zheyuan Zhang, Zehong Wang, Shifu Hou, Evan Hall, Landon Bachman, Jasmine White, Vincent Galassi, V Nitesh, Chuxu Chawla, Yanfang Zhang, Ye, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024b</p>
<p>All in one and one for all: A simple yet effective method towards cross-domain graph pretraining. Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng, Jia Li, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>going on a vacation" takes longer than" going for a walk. Ben Zhou, Daniel Khashabi, Qiang Ning, Dan Roth, arXiv:1909.03065A study of temporal commonsense understanding. 2019arXiv preprint</p>
<p>Temporal common sense acquisition with minimal supervision. Ben Zhou, Qiang Ning, Daniel Khashabi, Dan Roth, arXiv:2005.043042020aarXiv preprint</p>
<p>Ben Zhou, Kyle Richardson, Qiang Ning, Tushar Khot, Ashish Sabharwal, Dan Roth, arXiv:2010.12753Temporal reasoning on implicit events from distant supervision. 2020barXiv preprint</p>
<p>Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, Hanjun Dai, arXiv:2310.07064Large language models can learn rules. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
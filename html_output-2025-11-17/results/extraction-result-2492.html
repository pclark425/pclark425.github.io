<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2492 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2492</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2492</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-257776796</p>
                <p><strong>Paper Title:</strong> Machine learning for metabolic pathway optimization: A review</p>
                <p><strong>Paper Abstract:</strong> Optimizing the metabolic pathways of microbial cell factories is essential for establishing viable biotechnological production processes. However, due to the limited understanding of the complex setup of cellular machinery, building efficient microbial cell factories remains tedious and time-consuming. Machine learning (ML), a powerful tool capable of identifying patterns within large datasets, has been used to analyze biological datasets generated using various high-throughput technologies to build data-driven models for complex bioprocesses. In addition, ML can also be integrated with Design–Build–Test–Learn to accelerate development. This review focuses on recent ML applications in genome-scale metabolic model construction, multistep pathway optimization, rate-limiting enzyme engineering, and gene regulatory element designing. In addition, we have discussed some limitations of these methods as well as potential solutions.</p>
                <p><strong>Cost:</strong> 0.03</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2492.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2492.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>iBioFAB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Illinois Biological Foundry for Advanced Biomanufacturing (iBioFAB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated DBTL platform that combines Gaussian Process surrogate modeling with an expected-improvement acquisition function and a robotic execution pipeline to prioritize experimental evaluations that trade off predicted yield and predictive uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards a fully automated algorithm driven platform for biosystems design.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>iBioFAB</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>iBioFAB models the design space with a Gaussian Process (GP) to assign a predictive mean and variance to each candidate. An acquisition function (expected improvement) is used to quantify the tradeoff between predicted outcome (exploitation) and predictive uncertainty (exploration); the selected candidates are executed on a robotic/automated experimental platform (robotic build/test) to close DBTL cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Metabolic pathway optimization / strain engineering (biomanufacturing)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>GP surrogate predicts mean and variance for all design points; acquisition function (expected improvement) ranks candidates by a balance of predicted improvement and uncertainty; top-ranked candidates are selected for experimental evaluation on the automated platform.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not explicitly quantified in FLOPs or wall-time in the review; practical cost measured implicitly as number of experimental evaluations / robotic runs and total DBTL cycles (experimental throughput is the primary resource constraint).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected improvement computed from GP predictive mean and variance (tradeoff between value and uncertainty) — predictive variance is used as an uncertainty proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Expected improvement acquisition function implements an explicit exploitation–exploration tradeoff: candidates with either high predicted mean (exploitation) or high predictive variance (exploration) can be prioritized according to expected improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity-promotion mechanism described in the review beyond the exploration term in the acquisition function (predictive variance encourages sampling in uncertain regions).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Experimental budget / fixed number of evaluations (DBTL cycles) and robotic throughput limits.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Acquisition function ranks candidates so that, under a limited number of experimental slots, those with highest expected improvement are executed; platform throughput determines batch size.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improved product titer / pathway output (e.g., optimized lycopene production used as an application example) — breakthrough measured as higher measured yield than prior designs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Application example: lycopene pathway optimization used to demonstrate the approach; no specific percent improvement vs baseline reported in the review for iBioFAB itself.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not explicitly quantified in the review for iBioFAB; baseline implicitly is unguided or random/naïve experimental sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not specified in numeric terms in the review text for iBioFAB.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported conceptual gains: avoids local optima and reduces experimental overhead by selecting points with expected improvement; specific numerical efficiency gains not reported in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>The review highlights that iBioFAB explicitly encodes the tradeoff between predicted outcome and uncertainty via expected improvement, but also warns that the GP Gaussian predictive assumption may not hold universally.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Using a GP with an acquisition function like expected improvement provides a principled way to allocate a limited experimental budget by balancing likely high performers with uncertain regions that may yield breakthroughs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for metabolic pathway optimization: A review', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2492.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2492.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ART</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated Recommendation Tool (ART)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble-based ML recommendation tool that integrates multiple scikit-learn models and uses parallel-tempering MCMC sampling as an acquisition mechanism to recommend experiments balancing exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A machine learning Automated Recommendation Tool for synthetic biology.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ART (Automated Recommendation Tool)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ART builds an ensemble of predictive models (multiple ML algorithms via scikit-learn) to produce posterior predictive distributions; parallel-tempering MCMC sampling over the ensemble/posterior is used as an acquisition strategy to decide whether to explore uncertain regions or exploit promising candidates. It issues experiment recommendations to DBTL pipelines and was evaluated on simulated and real metabolic engineering datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Metabolic engineering / synthetic biology experimental design (strain and pathway optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Uses ensemble predictions and posterior sampling to score candidates; parallel-tempering MCMC determines exploration vs exploitation by sampling from regions of high posterior probability and/or high uncertainty to produce ranked experimental recommendations under constrained budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not explicitly defined in the review; implicit metrics are number of wet-lab experiments (assay runs) and computational time for ensemble training and MCMC sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Posterior predictive uncertainty derived from ensemble models and MCMC sampling; acquisition implicitly targets candidates that improve posterior knowledge or expected performance.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Parallel-tempering MCMC acquisition samples the posterior predictive space to balance exploring uncertain regions and exploiting high-mean predictions; ensemble modeling reduces risk of model-selection error.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Ensemble modeling plus posterior sampling can produce diverse candidate sets; parallel-tempering encourages exploration across modes of the posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of experiments / DBTL cycle capacity</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Acquisition sampling yields a ranked set of recommendations that are executed up to the available experimental budget; ensemble-based uncertainty helps prioritize under limited trials.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>High predicted experimental outcome (e.g., high yields) and exploration of uncertain/high-potential regions; measured experimentally in downstream papers but no single uniform metric mandated in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Review states ART was demonstrated on simulated datasets and multiple real metabolic engineering projects (biofuels, flavor compounds, fatty acids, tryptophan), but the review does not enumerate numeric improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually against single-model Bayesian optimization and na"ive screening; ensemble approach designed to sidestep model-selection pitfalls.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not quantified with specific numbers in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative: reduces risk of selecting poorly performing models, enables robust recommendations under model uncertainty; no explicit % gains reported in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>ART explicitly addresses the exploration–exploitation tradeoff via MCMC acquisition and attempts to reduce model-selection risk via ensembling; review notes this can improve decision robustness when model assumptions are uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendation: use ensembles + posterior sampling to make more robust allocations of experimental budget when model selection is uncertain; acquisition should account for both predicted performance and model confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for metabolic pathway optimization: A review', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2492.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2492.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MiYa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MiYa ML workflow (YeastFab assembly coupled)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A low-throughput ML-guided workflow (MiYa) for optimizing multigene expression by replacing promoters; uses an ensemble of 1000 ANNs and manually curated initial data to predict best strains for testing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>an efficient machine-learning workflow in conjunction with the YeastFab assembly strategy for combinatorial optimization of heterologous metabolic pathways in Saccharomyces cerevisiae</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MiYa</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MiYa trains an ensemble of 1000 artificial neural networks with random initial weights to avoid overfitting and predict top-performing promoter/gene expression combinations; it operates with low-throughput, high-accuracy experimental measurements and relies on a carefully curated initial dataset (manual screening when products are colorimetric) to bootstrap modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Multistep metabolic pathway optimization in yeast (promoter replacement and combinatorial pathway tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Ensemble predictions rank design points; the highest-ranked candidates (predicted best strains) are selected for low-throughput experimental validation within the available assay capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not explicitly quantified; main resource metric is experimental throughput (number of constructs/strains that can be assayed with high accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not explicitly formalized in terms of expected information; the approach focuses on predicted high-value outputs and ensemble consensus to reduce overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Primarily exploitative — selects those candidates with highest predicted outputs from the ensemble; limited explicit mechanism for exploration is described.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Initial manual sampling of diverse, high-quality examples is used to improve model generalization; no explicit algorithmic diversity-promoting acquisition described.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Low-throughput experimental capacity (limited number of physical assays per cycle)</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Given a small number of experimental slots, the ensemble ranks candidates and the top predictions are executed; emphasis on high-quality initial training data to maximize value per assay.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improved product titer or other measured phenotype (used colored products for screening in examples); breakthrough defined as top predicted/observed performing strains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Review cites that MiYa was used to optimize expression and that ensemble training and curated initial datasets improved model predictions; no specific numeric improvements are given in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Implicit comparison versus unguided trial-and-error or small ensembles; not explicitly contrasted numerically in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not quantified in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative: reduces risk of overfitting and focuses scarce experimental resources on high-probability candidates; no numeric reductions in experiment counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Review notes MiYa's reliance on high-quality initial data and ensemble methods reduces overfitting but that the conservative design space and lack of explicit uncertainty terms can lead to local optima trapping.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>High-quality initial training sets and ensemble predictions can make low-throughput experimental budgets more effective, but explicit uncertainty-aware acquisition would better avoid local optima.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for metabolic pathway optimization: A review', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2492.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2492.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ActiveOpt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ActiveOpt (SVM-based active learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An SVM-based active learning strategy that classifies designs by a performance cutoff and selects for experimental testing the point farthest from the SVM decision hyperplane (highest uncertainty) to improve classification/regression performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ActiveOpt</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ActiveOpt trains an SVM to classify candidate designs as above/below a performance cutoff; active sampling selects the unlabeled candidate farthest from the hyperplane (uncertainty sampling) for experimental validation, after which the model and cutoff are updated iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Microbial chemical production / metabolic pathway optimization (classification of high vs low producers)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Selects the single most uncertain candidate (max distance from decision boundary) to label per iteration, thus directing scarce experiments to places expected to yield highest information for classifier improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not specified in detail; practical cost measured in number of wet-lab experiments/assays performed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uses distance from SVM hyperplane as an uncertainty proxy to choose informative samples (heuristic information gain).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Uncertainty sampling (farthest-from-hyperplane) biases selection toward exploration of ambiguous regions while implicitly incorporating exploitation via the classifier's learned boundary.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity-encouraging mechanism beyond uncertainty-driven selection.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experimental budget per active learning cycle (DBTL cycles implicit).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Iterative selection and retraining until the available experimental budget is exhausted or classifier converges.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Identification of candidate designs that exceed the performance cutoff (class label 'high' producers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Review describes method conceptually; no numeric performance metrics reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Implicit baseline: random sampling or greedy selection; explicit numerical comparisons not provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not quantified in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Method is framed as reducing experiments by focusing on high-information samples, but no numerical efficiency gains reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>ActiveOpt is presented as a practical heuristic balancing information acquisition and resource use; the review notes such single-algorithm frameworks may be limited in scope.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Uncertainty-based sampling (distance from decision boundary) is recommended to prioritize experiments likely to most improve classifier performance under a constrained experimental budget.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for metabolic pathway optimization: A review', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2492.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2492.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLDE / ftMLDE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine-Learning-assisted Directed Evolution (MLDE) and focused-training MLDE (ftMLDE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MLDE couples ML surrogate models to directed evolution by suggesting variants to test iteratively; ftMLDE improves encoding and initial training-set selection (e.g., physicochemical and learned embeddings) to avoid sampling large low-fitness 'holes' and to enhance search efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MLDE / ftMLDE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MLDE trains ML models on measured variant fitness and uses the model to propose new variants for experimental testing; ftMLDE augments MLDE with advanced sequence encodings (physicochemical descriptors, NLP-derived embeddings) and strategic initial sampling that incorporates global sequence information to reduce uninformative low-fitness examples and improve search efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Protein engineering / directed evolution (rate-limiting enzyme engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Iteratively proposes a modest number of variants predicted to have higher fitness for experimental evaluation, thereby focusing wet-lab resources on promising areas of the fitness landscape; ftMLDE designs the initial training set to maximize informativeness per assay.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Measured primarily as number of variants screened / number of experimental assays; in silico cost for training encoders may be nontrivial but not explicitly quantified in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not formalized as mutual information or expected improvement in the review; selection is driven by predicted fitness and improved initial sampling to reduce waste on low-fitness variants.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Model-guided proposals aim to exploit predicted high-fitness regions while ftMLDE's improved initial sampling encourages broader, more informative coverage to avoid local optima (implicit exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>ftMLDE uses initial sampling informed by global sequence databases and diverse encodings to avoid uninformative low-fitness regions, which functionally promotes exploration/diversity in the training set.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Limited number of experimental assays / screening throughput in DE campaigns</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Reduce number of required experimental evaluations by proposing more informative candidate variants and by using encodings that increase model predictive power from fewer labeled examples.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Finding variants with markedly improved fitness (enzyme activity/selectivity); review cites ftMLDE achieving large gains in search efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>ftMLDE reported at least a 12.2-fold improvement in searching the fitness landscape (as cited in the review); MLDE framework shown to be effective at avoiding local traps compared with purely random sampling in referenced studies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random sampling and traditional directed evolution approaches (single-mutation screening); original MLDE vs ftMLDE comparisons emphasize improved encoding/initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>ftMLDE: ≥12.2× improvement in search efficiency relative to baseline MLDE/random sampling as stated in the review (no further numeric detail provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported sample-efficiency gains (ftMLDE ≥12.2-fold); reduces number of experimental evaluations needed to find high-fitness variants.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Review highlights tradeoffs between sampling diverse low-fitness regions (which are mostly uninformative) and focusing on predicted high-fitness variants; ftMLDE addresses this by choosing informative initial samples and richer encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Invest in informative initial training-set design and richer sequence encodings to maximize the information gained per experimental assay and to improve the efficiency of variant discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for metabolic pathway optimization: A review', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2492.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2492.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Low-N UniRep</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Low-N protein engineering using UniRep embeddings and active/MCMC sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-efficient protein engineering framework that uses unsupervised pretraining (UniRep) on millions of sequences to produce embeddings that enable effective model training from very few labeled examples and employs MCMC-based sampling to propose candidate variants and avoid local optima.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Low-N protein engineering with data-efficient deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Low-N UniRep</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>UniRep is an unsupervised deep model trained on large protein sequence corpora to produce fixed-length embeddings that summarize biophysical, structural, and evolutionary information. Low-N protein engineering combines these embeddings with small labeled datasets and MCMC-based candidate sampling (active selection) to propose diverse, high-potential variants while minimizing wet-lab assays.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Protein engineering (data-limited contexts, enzyme optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Minimize the number of wet-lab experiments (low N) by leveraging pretrained sequence representations; MCMC sampling proposes candidates to explore model posterior and avoid local optima.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Primary resource metric is number of labeled experiments (low-N); additional computational cost for embedding inference and MCMC sampling is not quantified in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not formally stated as an explicit mutual-information objective in the review; MCMC sampling used to explore posterior/model uncertainty and propose informative candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>MCMC sampling over model posterior provides exploratory proposals while pretrained embeddings improve exploitation by improving predictive accuracy from few examples.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>MCMC sampling encourages diverse proposals; pretrained embeddings allow more informative generalization from few labeled points.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Severely limited experimental budget (very small number of labeled variants allowable).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Reduce labeled-data needs via pretrained embeddings and choose candidate variants via MCMC sampling to maximize discovery potential per assay.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Discovery of high-fitness variants using very small labeled datasets; measured by improved fitness relative to parental or prior variants in the original studies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Original Low-N work reported successful discovery of improved proteins using small labeled datasets; the review notes data-efficiency but does not provide numeric metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against approaches that require larger labeled datasets or that do not use pretrained embeddings; specifics not provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not quantified in the review text beyond qualitative data-efficiency claims.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Enables engineering with dramatically fewer labeled examples (Low-N); explicit fold-improvements not provided in review summary.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Framework emphasizes that leveraging large unlabeled sequence databases (via UniRep) shifts resource burden from wet-lab assays to offline computation, enabling much smaller experimental budgets while preserving discovery potential.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Pretraining to capture global sequence patterns plus posterior-exploratory sampling (MCMC) is an effective allocation principle when experimental budgets are extremely constrained.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for metabolic pathway optimization: A review', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2492.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2492.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian DBTL (protein engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian optimization–assisted DBTL for protein sequence search (e.g., SCHEMA-guided recombination study)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian-optimization-guided DBTL pipeline applied to chimeric enzyme libraries (SCHEMA recombination) that uses Bayesian acquisition to iteratively select sequences and converges to high-performing sequences with substantial production gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Machine learning-guided acyl-ACP reductase engineering for improved in vivo fatty alcohol production</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian DBTL (protein engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Construct a diverse chimeric sequence library (e.g., SCHEMA-guided recombination), evaluate an initial subset experimentally, fit a Bayesian surrogate model (e.g., GP or probabilistic model), use an acquisition strategy to select next sequences to test, iterate DBTL cycles; sequences converge in sequence-space toward improved catalytic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Enzyme engineering / protein engineering (improving catalytic rates/product formation)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Bayesian acquisition balances model-predicted improvement and uncertainty to pick which chimeric sequences to test next out of a large recombination library, concentrating experimental effort on promising or informative variants.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Primary practical cost is number of experimental constructs/assays and number of DBTL cycles required; computational modeling overhead not explicitly quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Bayesian posterior predictive uncertainty and acquisition functions (not specified which one in review) govern selection; the process aims to maximize expected utility per experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Bayesian optimization acquisition inherently balances exploration (uncertainty) and exploitation (predicted improvement); through cycles, the chosen sequences converge to a promising region in sequence space.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Initial library construction (SCHEMA) creates initial diversity; the Bayesian acquisition may still sample across diverse candidates depending on uncertainty estimates but no separate explicit diversity-promoting term is described.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Limited experimental cycles and number of variants that can be screened</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Use of Bayesian acquisition to prioritize variants that maximize expected improvement or expected utility given limited experimental slots.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Measured gains in catalytic activity/product formation; in the cited example, final enzyme variants produced ~50% more fatty alcohols than parental strains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited result: target strains produced ~50% more fatty alcohols than parental strains after Bayesian DBTL optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Parental enzyme sequences and initial library members; compared against starting strains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>+50% fatty alcohol production compared to parental sequences reported in review summary.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Demonstrated ability to find improved sequences with iterative Bayesian selection; explicit reduction in experimental count vs exhaustive search not quantified in review.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Review emphasizes Bayesian optimization as a way to escape local optima and save experimental resources by explicitly managing the exploration–exploitation tradeoff; specific tradeoff curves are not provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Bayesian DBTL is effective for large combinatorial sequence spaces: prioritize candidates by acquisition functions that trade predicted improvement against uncertainty to converge efficiently on high-performing sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for metabolic pathway optimization: A review', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2492.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2492.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RBS Bayesian pipeline (GPR + UCB)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian optimization pipeline for ribosome binding site (RBS) optimization using Gaussian Process Regression and Upper-Confidence Bound (UCB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPR surrogate models translation initiation rate (TIR) and an Upper-Confidence Bound acquisition (multi-armed bandit style) balances predicted mean and uncertainty to select RBS variants; over four DBTL cycles and 450 variants, RBSs exceeding a benchmark by up to 34% TIR were found.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RBS Bayesian pipeline (GPR + UCB)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Gaussian Process Regression models the mapping from RBS sequence to TIR, providing predictive mean and variance; an UCB acquisition function (mean + kappa * std) is used to score candidates and select variants for batched experimental testing across iterative DBTL cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Gene regulatory element optimization (RBS design to tune translation)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Batched experimental selections across DBTL cycles: choose variants with high UCB scores to balance exploration of uncertain sequences and exploitation of high-mean predictions within limited assay capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of RBS variants synthesized and assayed (450 variants tested across four cycles reported); computational overhead for GPR training is small relative to wet-lab costs and not explicitly quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Predictive variance from the GP is part of the UCB score and thus used as an uncertainty/information metric.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>UCB acquisition balances exploration (variance term) and exploitation (mean term) by scoring candidates as mean + β * std and selecting high-scoring variants.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Batched design and UCB scoring encourage sampling across both promising and uncertain sequences; batched selection can be arranged to enforce some diversity within a batch though the review does not detail a specific diversity constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of experimental variants per DBTL cycle and total cycles (experimental throughput constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Select a fixed number of variants per cycle by ranking UCB scores and executing top candidates subject to batch size limits; iterate until budget or target met.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Translation initiation rate (TIR) relative to benchmark RBS sequences; discovery of RBSs with higher TIR indicates breakthroughs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>450 RBS variants tested across four DBTL cycles; RBSs discovered with TIR up to 34% greater than the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Benchmark RBS sequences / prior best-performing RBSs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Up to +34% TIR improvement over benchmark RBS reported in review.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Found higher-performing RBSs within a limited number of screened variants (450); explicit reduction in experiments vs exhaustive search not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Review highlights that UCB explicitly trades off predicted value and uncertainty; choosing β controls exploration/exploitation and batch composition.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use GP + UCB to prioritize batched experimental slots; tune the UCB parameter to control exploration vs exploitation depending on experimental budget and tolerance for risk.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for metabolic pathway optimization: A review', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2492.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2492.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAPIENs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SAPIENs residual CNN with uncertainty quantification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A residual CNN architecture trained on millions of translation data points to predict GRE (e.g., RBS) activities and to reliably quantify prediction uncertainty for downstream experimental recommendation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SAPIENs</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SAPIENs uses multiple ResNet-like CNNs trained on large high-throughput DNA–phenotype linked datasets (millions of measurements) to predict element activity and to estimate prediction uncertainty, enabling more reliable experimental decisions (e.g., selecting sequences with both high predicted activity and low model uncertainty).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Gene regulatory element design (RBSs and other GREs)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Provide sequence-function predictions with quantified uncertainty so experimenters can prioritize candidates that are both high predicted performers and have acceptable prediction confidence, thereby improving experimental budget allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not explicitly specified; practical measure is number of sequences experimentally assayed; model training requires large data and compute but exact costs not reported in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Model-predicted uncertainty (residual model uncertainty estimates) used as a proxy for expected information when prioritizing candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Uncertainty-aware ranking: favor sequences with high predicted activity but also account for predicted uncertainty to avoid overconfident false positives; specific acquisition function not detailed in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Model trained on massive, diverse datasets which improves generalization; explicit batch-diversity mechanisms for selection are not described in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Limited experimental throughput and capacity to synthesize/assay sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Prioritize sequences that maximize predicted performance subject to acceptable uncertainty; use model uncertainty to avoid wasting budget on overconfident but unreliable predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>High predicted experimental activity with validated experimental gains; breakthrough defined as discovering high-activity GREs that perform well experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Trained on several million translation measurements; reported high predictive performance and portability across contexts in the review but no single numeric metric is provided in the review text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Smaller models or models without uncertainty quantification; explicit baseline numbers not provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Review states SAPIENs successfully identified correlations and quantified uncertainty reliably, but does not give numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>By quantifying uncertainty, SAPIENs can reduce wasted experiments on overconfident wrong predictions; no numeric efficiency gain reported in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Emphasizes that uncertainty quantification helps balance selecting high-performing candidates and avoiding risky recommendations when experimental budgets are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>In high-throughput sequencing-linked phenotype contexts, models that quantify uncertainty allow more informed allocation of experimental resources, prioritizing candidates that maximize expected reliable gain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for metabolic pathway optimization: A review', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2492.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2492.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BoostGAPFILL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BoostGAPFILL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ML-based strategy that integrates machine learning with constraint-based metabolic models to generate hypotheses for gap-filling metabolic reconstructions by constraining candidate reactions based on metabolite patterns; reported precision and recall exceed 60%.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BoostGAPFILL</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BoostGAPFILL uses features derived from incomplete network metabolite patterns and ML classifiers to prioritize candidate reactions/enzyme annotations that could fill gaps in draft genome-scale metabolic models; it produces ranked hypotheses for curation and experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Genome-scale metabolic model construction and curation (model gap-filling)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Prioritize gap-filling hypotheses by ML-predicted likelihood (precision/recall ranking) so curators and experimentalists can focus validation efforts on higher-confidence candidate reactions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Computational ranking cost (ML model inference) is modest; main practical cost is downstream manual curation or targeted experiments to validate proposed reactions — review does not quantify computational FLOPs or monetary cost.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not described as an explicit information-gain objective; ML classifier probabilities are used as confidence scores to prioritize validation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Not formulated as an exploration–exploitation acquisition over experiments; it provides prioritized hypotheses for human/experimental follow-up.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity-promoting mechanism described; selection based on predicted likelihood of correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Human curation time and experimental validation resources (implicit)</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Rank-ordered recommendations allow limited curation/validation effort to focus on high-probability gaps; reported precision/recall >60% implies reduced wasted effort.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Identification of correct gap-filling reactions and improved model predictive power; review cites >60% precision and recall as performance indicators.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>>60% precision and recall reported for BoostGAPFILL in gap-filling tasks (as summarized in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Other gap-filling heuristics or non-ML methods; specific baselines not enumerated in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BoostGAPFILL reported >60% precision and recall, indicating improved hypothesis ranking versus prior undetailed baselines (no direct numeric baseline provided in review).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reduces curation and validation workload by prioritizing higher-confidence gap-filling candidates; quantified by precision/recall but not by reduced number of validation experiments in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Review frames BoostGAPFILL as trading computational hypothesis-ranking for reduced manual curation; does not present a formal cost-information tradeoff curve.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use ML to prioritize gap-filling hypotheses so limited wet-lab or curation effort focuses on the most promising candidates, improving curation efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for metabolic pathway optimization: A review', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A machine learning Automated Recommendation Tool for synthetic biology <em>(Rating: 2)</em></li>
                <li>Towards a fully automated algorithm driven platform for biosystems design. <em>(Rating: 2)</em></li>
                <li>Machine-learning-guided directed evolution for protein engineering <em>(Rating: 2)</em></li>
                <li>Informed training set design enables efficient machine learning-assisted directed protein evolution <em>(Rating: 2)</em></li>
                <li>Low-N protein engineering with data-efficient deep learning <em>(Rating: 2)</em></li>
                <li>Machine learning-guided acyl-ACP reductase engineering for improved in vivo fatty alcohol production <em>(Rating: 2)</em></li>
                <li>Machine learning guided batched design of a bacterial ribosome binding site <em>(Rating: 2)</em></li>
                <li>an efficient machine-learning workflow in conjunction with the YeastFab assembly strategy for combinatorial optimization of heterologous metabolic pathways in Saccharomyces cerevisiae <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2492",
    "paper_id": "paper-257776796",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "iBioFAB",
            "name_full": "Illinois Biological Foundry for Advanced Biomanufacturing (iBioFAB)",
            "brief_description": "An automated DBTL platform that combines Gaussian Process surrogate modeling with an expected-improvement acquisition function and a robotic execution pipeline to prioritize experimental evaluations that trade off predicted yield and predictive uncertainty.",
            "citation_title": "Towards a fully automated algorithm driven platform for biosystems design.",
            "mention_or_use": "mention",
            "system_name": "iBioFAB",
            "system_description": "iBioFAB models the design space with a Gaussian Process (GP) to assign a predictive mean and variance to each candidate. An acquisition function (expected improvement) is used to quantify the tradeoff between predicted outcome (exploitation) and predictive uncertainty (exploration); the selected candidates are executed on a robotic/automated experimental platform (robotic build/test) to close DBTL cycles.",
            "application_domain": "Metabolic pathway optimization / strain engineering (biomanufacturing)",
            "resource_allocation_strategy": "GP surrogate predicts mean and variance for all design points; acquisition function (expected improvement) ranks candidates by a balance of predicted improvement and uncertainty; top-ranked candidates are selected for experimental evaluation on the automated platform.",
            "computational_cost_metric": "Not explicitly quantified in FLOPs or wall-time in the review; practical cost measured implicitly as number of experimental evaluations / robotic runs and total DBTL cycles (experimental throughput is the primary resource constraint).",
            "information_gain_metric": "Expected improvement computed from GP predictive mean and variance (tradeoff between value and uncertainty) — predictive variance is used as an uncertainty proxy.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Expected improvement acquisition function implements an explicit exploitation–exploration tradeoff: candidates with either high predicted mean (exploitation) or high predictive variance (exploration) can be prioritized according to expected improvement.",
            "diversity_mechanism": "No explicit diversity-promotion mechanism described in the review beyond the exploration term in the acquisition function (predictive variance encourages sampling in uncertain regions).",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Experimental budget / fixed number of evaluations (DBTL cycles) and robotic throughput limits.",
            "budget_constraint_handling": "Acquisition function ranks candidates so that, under a limited number of experimental slots, those with highest expected improvement are executed; platform throughput determines batch size.",
            "breakthrough_discovery_metric": "Improved product titer / pathway output (e.g., optimized lycopene production used as an application example) — breakthrough measured as higher measured yield than prior designs.",
            "performance_metrics": "Application example: lycopene pathway optimization used to demonstrate the approach; no specific percent improvement vs baseline reported in the review for iBioFAB itself.",
            "comparison_baseline": "Not explicitly quantified in the review for iBioFAB; baseline implicitly is unguided or random/naïve experimental sampling.",
            "performance_vs_baseline": "Not specified in numeric terms in the review text for iBioFAB.",
            "efficiency_gain": "Reported conceptual gains: avoids local optima and reduces experimental overhead by selecting points with expected improvement; specific numerical efficiency gains not reported in the review.",
            "tradeoff_analysis": "The review highlights that iBioFAB explicitly encodes the tradeoff between predicted outcome and uncertainty via expected improvement, but also warns that the GP Gaussian predictive assumption may not hold universally.",
            "optimal_allocation_findings": "Using a GP with an acquisition function like expected improvement provides a principled way to allocate a limited experimental budget by balancing likely high performers with uncertain regions that may yield breakthroughs.",
            "uuid": "e2492.0",
            "source_info": {
                "paper_title": "Machine learning for metabolic pathway optimization: A review",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "ART",
            "name_full": "Automated Recommendation Tool (ART)",
            "brief_description": "An ensemble-based ML recommendation tool that integrates multiple scikit-learn models and uses parallel-tempering MCMC sampling as an acquisition mechanism to recommend experiments balancing exploration and exploitation.",
            "citation_title": "A machine learning Automated Recommendation Tool for synthetic biology.",
            "mention_or_use": "mention",
            "system_name": "ART (Automated Recommendation Tool)",
            "system_description": "ART builds an ensemble of predictive models (multiple ML algorithms via scikit-learn) to produce posterior predictive distributions; parallel-tempering MCMC sampling over the ensemble/posterior is used as an acquisition strategy to decide whether to explore uncertain regions or exploit promising candidates. It issues experiment recommendations to DBTL pipelines and was evaluated on simulated and real metabolic engineering datasets.",
            "application_domain": "Metabolic engineering / synthetic biology experimental design (strain and pathway optimization)",
            "resource_allocation_strategy": "Uses ensemble predictions and posterior sampling to score candidates; parallel-tempering MCMC determines exploration vs exploitation by sampling from regions of high posterior probability and/or high uncertainty to produce ranked experimental recommendations under constrained budgets.",
            "computational_cost_metric": "Not explicitly defined in the review; implicit metrics are number of wet-lab experiments (assay runs) and computational time for ensemble training and MCMC sampling.",
            "information_gain_metric": "Posterior predictive uncertainty derived from ensemble models and MCMC sampling; acquisition implicitly targets candidates that improve posterior knowledge or expected performance.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Parallel-tempering MCMC acquisition samples the posterior predictive space to balance exploring uncertain regions and exploiting high-mean predictions; ensemble modeling reduces risk of model-selection error.",
            "diversity_mechanism": "Ensemble modeling plus posterior sampling can produce diverse candidate sets; parallel-tempering encourages exploration across modes of the posterior.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of experiments / DBTL cycle capacity",
            "budget_constraint_handling": "Acquisition sampling yields a ranked set of recommendations that are executed up to the available experimental budget; ensemble-based uncertainty helps prioritize under limited trials.",
            "breakthrough_discovery_metric": "High predicted experimental outcome (e.g., high yields) and exploration of uncertain/high-potential regions; measured experimentally in downstream papers but no single uniform metric mandated in the review.",
            "performance_metrics": "Review states ART was demonstrated on simulated datasets and multiple real metabolic engineering projects (biofuels, flavor compounds, fatty acids, tryptophan), but the review does not enumerate numeric improvements.",
            "comparison_baseline": "Compared conceptually against single-model Bayesian optimization and na\"ive screening; ensemble approach designed to sidestep model-selection pitfalls.",
            "performance_vs_baseline": "Not quantified with specific numbers in the review.",
            "efficiency_gain": "Qualitative: reduces risk of selecting poorly performing models, enables robust recommendations under model uncertainty; no explicit % gains reported in the review.",
            "tradeoff_analysis": "ART explicitly addresses the exploration–exploitation tradeoff via MCMC acquisition and attempts to reduce model-selection risk via ensembling; review notes this can improve decision robustness when model assumptions are uncertain.",
            "optimal_allocation_findings": "Recommendation: use ensembles + posterior sampling to make more robust allocations of experimental budget when model selection is uncertain; acquisition should account for both predicted performance and model confidence.",
            "uuid": "e2492.1",
            "source_info": {
                "paper_title": "Machine learning for metabolic pathway optimization: A review",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "MiYa",
            "name_full": "MiYa ML workflow (YeastFab assembly coupled)",
            "brief_description": "A low-throughput ML-guided workflow (MiYa) for optimizing multigene expression by replacing promoters; uses an ensemble of 1000 ANNs and manually curated initial data to predict best strains for testing.",
            "citation_title": "an efficient machine-learning workflow in conjunction with the YeastFab assembly strategy for combinatorial optimization of heterologous metabolic pathways in Saccharomyces cerevisiae",
            "mention_or_use": "mention",
            "system_name": "MiYa",
            "system_description": "MiYa trains an ensemble of 1000 artificial neural networks with random initial weights to avoid overfitting and predict top-performing promoter/gene expression combinations; it operates with low-throughput, high-accuracy experimental measurements and relies on a carefully curated initial dataset (manual screening when products are colorimetric) to bootstrap modeling.",
            "application_domain": "Multistep metabolic pathway optimization in yeast (promoter replacement and combinatorial pathway tuning)",
            "resource_allocation_strategy": "Ensemble predictions rank design points; the highest-ranked candidates (predicted best strains) are selected for low-throughput experimental validation within the available assay capacity.",
            "computational_cost_metric": "Not explicitly quantified; main resource metric is experimental throughput (number of constructs/strains that can be assayed with high accuracy).",
            "information_gain_metric": "Not explicitly formalized in terms of expected information; the approach focuses on predicted high-value outputs and ensemble consensus to reduce overfitting.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Primarily exploitative — selects those candidates with highest predicted outputs from the ensemble; limited explicit mechanism for exploration is described.",
            "diversity_mechanism": "Initial manual sampling of diverse, high-quality examples is used to improve model generalization; no explicit algorithmic diversity-promoting acquisition described.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Low-throughput experimental capacity (limited number of physical assays per cycle)",
            "budget_constraint_handling": "Given a small number of experimental slots, the ensemble ranks candidates and the top predictions are executed; emphasis on high-quality initial training data to maximize value per assay.",
            "breakthrough_discovery_metric": "Improved product titer or other measured phenotype (used colored products for screening in examples); breakthrough defined as top predicted/observed performing strains.",
            "performance_metrics": "Review cites that MiYa was used to optimize expression and that ensemble training and curated initial datasets improved model predictions; no specific numeric improvements are given in the review.",
            "comparison_baseline": "Implicit comparison versus unguided trial-and-error or small ensembles; not explicitly contrasted numerically in the review.",
            "performance_vs_baseline": "Not quantified in the review.",
            "efficiency_gain": "Qualitative: reduces risk of overfitting and focuses scarce experimental resources on high-probability candidates; no numeric reductions in experiment counts reported.",
            "tradeoff_analysis": "Review notes MiYa's reliance on high-quality initial data and ensemble methods reduces overfitting but that the conservative design space and lack of explicit uncertainty terms can lead to local optima trapping.",
            "optimal_allocation_findings": "High-quality initial training sets and ensemble predictions can make low-throughput experimental budgets more effective, but explicit uncertainty-aware acquisition would better avoid local optima.",
            "uuid": "e2492.2",
            "source_info": {
                "paper_title": "Machine learning for metabolic pathway optimization: A review",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "ActiveOpt",
            "name_full": "ActiveOpt (SVM-based active learning)",
            "brief_description": "An SVM-based active learning strategy that classifies designs by a performance cutoff and selects for experimental testing the point farthest from the SVM decision hyperplane (highest uncertainty) to improve classification/regression performance.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "ActiveOpt",
            "system_description": "ActiveOpt trains an SVM to classify candidate designs as above/below a performance cutoff; active sampling selects the unlabeled candidate farthest from the hyperplane (uncertainty sampling) for experimental validation, after which the model and cutoff are updated iteratively.",
            "application_domain": "Microbial chemical production / metabolic pathway optimization (classification of high vs low producers)",
            "resource_allocation_strategy": "Selects the single most uncertain candidate (max distance from decision boundary) to label per iteration, thus directing scarce experiments to places expected to yield highest information for classifier improvement.",
            "computational_cost_metric": "Not specified in detail; practical cost measured in number of wet-lab experiments/assays performed.",
            "information_gain_metric": "Uses distance from SVM hyperplane as an uncertainty proxy to choose informative samples (heuristic information gain).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Uncertainty sampling (farthest-from-hyperplane) biases selection toward exploration of ambiguous regions while implicitly incorporating exploitation via the classifier's learned boundary.",
            "diversity_mechanism": "No explicit diversity-encouraging mechanism beyond uncertainty-driven selection.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed experimental budget per active learning cycle (DBTL cycles implicit).",
            "budget_constraint_handling": "Iterative selection and retraining until the available experimental budget is exhausted or classifier converges.",
            "breakthrough_discovery_metric": "Identification of candidate designs that exceed the performance cutoff (class label 'high' producers).",
            "performance_metrics": "Review describes method conceptually; no numeric performance metrics reported in this review.",
            "comparison_baseline": "Implicit baseline: random sampling or greedy selection; explicit numerical comparisons not provided in the review.",
            "performance_vs_baseline": "Not quantified in the review.",
            "efficiency_gain": "Method is framed as reducing experiments by focusing on high-information samples, but no numerical efficiency gains reported here.",
            "tradeoff_analysis": "ActiveOpt is presented as a practical heuristic balancing information acquisition and resource use; the review notes such single-algorithm frameworks may be limited in scope.",
            "optimal_allocation_findings": "Uncertainty-based sampling (distance from decision boundary) is recommended to prioritize experiments likely to most improve classifier performance under a constrained experimental budget.",
            "uuid": "e2492.3",
            "source_info": {
                "paper_title": "Machine learning for metabolic pathway optimization: A review",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "MLDE / ftMLDE",
            "name_full": "Machine-Learning-assisted Directed Evolution (MLDE) and focused-training MLDE (ftMLDE)",
            "brief_description": "MLDE couples ML surrogate models to directed evolution by suggesting variants to test iteratively; ftMLDE improves encoding and initial training-set selection (e.g., physicochemical and learned embeddings) to avoid sampling large low-fitness 'holes' and to enhance search efficiency.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "MLDE / ftMLDE",
            "system_description": "MLDE trains ML models on measured variant fitness and uses the model to propose new variants for experimental testing; ftMLDE augments MLDE with advanced sequence encodings (physicochemical descriptors, NLP-derived embeddings) and strategic initial sampling that incorporates global sequence information to reduce uninformative low-fitness examples and improve search efficiency.",
            "application_domain": "Protein engineering / directed evolution (rate-limiting enzyme engineering)",
            "resource_allocation_strategy": "Iteratively proposes a modest number of variants predicted to have higher fitness for experimental evaluation, thereby focusing wet-lab resources on promising areas of the fitness landscape; ftMLDE designs the initial training set to maximize informativeness per assay.",
            "computational_cost_metric": "Measured primarily as number of variants screened / number of experimental assays; in silico cost for training encoders may be nontrivial but not explicitly quantified in the review.",
            "information_gain_metric": "Not formalized as mutual information or expected improvement in the review; selection is driven by predicted fitness and improved initial sampling to reduce waste on low-fitness variants.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Model-guided proposals aim to exploit predicted high-fitness regions while ftMLDE's improved initial sampling encourages broader, more informative coverage to avoid local optima (implicit exploration).",
            "diversity_mechanism": "ftMLDE uses initial sampling informed by global sequence databases and diverse encodings to avoid uninformative low-fitness regions, which functionally promotes exploration/diversity in the training set.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Limited number of experimental assays / screening throughput in DE campaigns",
            "budget_constraint_handling": "Reduce number of required experimental evaluations by proposing more informative candidate variants and by using encodings that increase model predictive power from fewer labeled examples.",
            "breakthrough_discovery_metric": "Finding variants with markedly improved fitness (enzyme activity/selectivity); review cites ftMLDE achieving large gains in search efficiency.",
            "performance_metrics": "ftMLDE reported at least a 12.2-fold improvement in searching the fitness landscape (as cited in the review); MLDE framework shown to be effective at avoiding local traps compared with purely random sampling in referenced studies.",
            "comparison_baseline": "Random sampling and traditional directed evolution approaches (single-mutation screening); original MLDE vs ftMLDE comparisons emphasize improved encoding/initialization.",
            "performance_vs_baseline": "ftMLDE: ≥12.2× improvement in search efficiency relative to baseline MLDE/random sampling as stated in the review (no further numeric detail provided here).",
            "efficiency_gain": "Reported sample-efficiency gains (ftMLDE ≥12.2-fold); reduces number of experimental evaluations needed to find high-fitness variants.",
            "tradeoff_analysis": "Review highlights tradeoffs between sampling diverse low-fitness regions (which are mostly uninformative) and focusing on predicted high-fitness variants; ftMLDE addresses this by choosing informative initial samples and richer encodings.",
            "optimal_allocation_findings": "Invest in informative initial training-set design and richer sequence encodings to maximize the information gained per experimental assay and to improve the efficiency of variant discovery.",
            "uuid": "e2492.4",
            "source_info": {
                "paper_title": "Machine learning for metabolic pathway optimization: A review",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Low-N UniRep",
            "name_full": "Low-N protein engineering using UniRep embeddings and active/MCMC sampling",
            "brief_description": "A data-efficient protein engineering framework that uses unsupervised pretraining (UniRep) on millions of sequences to produce embeddings that enable effective model training from very few labeled examples and employs MCMC-based sampling to propose candidate variants and avoid local optima.",
            "citation_title": "Low-N protein engineering with data-efficient deep learning",
            "mention_or_use": "mention",
            "system_name": "Low-N UniRep",
            "system_description": "UniRep is an unsupervised deep model trained on large protein sequence corpora to produce fixed-length embeddings that summarize biophysical, structural, and evolutionary information. Low-N protein engineering combines these embeddings with small labeled datasets and MCMC-based candidate sampling (active selection) to propose diverse, high-potential variants while minimizing wet-lab assays.",
            "application_domain": "Protein engineering (data-limited contexts, enzyme optimization)",
            "resource_allocation_strategy": "Minimize the number of wet-lab experiments (low N) by leveraging pretrained sequence representations; MCMC sampling proposes candidates to explore model posterior and avoid local optima.",
            "computational_cost_metric": "Primary resource metric is number of labeled experiments (low-N); additional computational cost for embedding inference and MCMC sampling is not quantified in the review.",
            "information_gain_metric": "Not formally stated as an explicit mutual-information objective in the review; MCMC sampling used to explore posterior/model uncertainty and propose informative candidates.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "MCMC sampling over model posterior provides exploratory proposals while pretrained embeddings improve exploitation by improving predictive accuracy from few examples.",
            "diversity_mechanism": "MCMC sampling encourages diverse proposals; pretrained embeddings allow more informative generalization from few labeled points.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Severely limited experimental budget (very small number of labeled variants allowable).",
            "budget_constraint_handling": "Reduce labeled-data needs via pretrained embeddings and choose candidate variants via MCMC sampling to maximize discovery potential per assay.",
            "breakthrough_discovery_metric": "Discovery of high-fitness variants using very small labeled datasets; measured by improved fitness relative to parental or prior variants in the original studies.",
            "performance_metrics": "Original Low-N work reported successful discovery of improved proteins using small labeled datasets; the review notes data-efficiency but does not provide numeric metrics here.",
            "comparison_baseline": "Compared against approaches that require larger labeled datasets or that do not use pretrained embeddings; specifics not provided in the review.",
            "performance_vs_baseline": "Not quantified in the review text beyond qualitative data-efficiency claims.",
            "efficiency_gain": "Enables engineering with dramatically fewer labeled examples (Low-N); explicit fold-improvements not provided in review summary.",
            "tradeoff_analysis": "Framework emphasizes that leveraging large unlabeled sequence databases (via UniRep) shifts resource burden from wet-lab assays to offline computation, enabling much smaller experimental budgets while preserving discovery potential.",
            "optimal_allocation_findings": "Pretraining to capture global sequence patterns plus posterior-exploratory sampling (MCMC) is an effective allocation principle when experimental budgets are extremely constrained.",
            "uuid": "e2492.5",
            "source_info": {
                "paper_title": "Machine learning for metabolic pathway optimization: A review",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Bayesian DBTL (protein engineering)",
            "name_full": "Bayesian optimization–assisted DBTL for protein sequence search (e.g., SCHEMA-guided recombination study)",
            "brief_description": "A Bayesian-optimization-guided DBTL pipeline applied to chimeric enzyme libraries (SCHEMA recombination) that uses Bayesian acquisition to iteratively select sequences and converges to high-performing sequences with substantial production gains.",
            "citation_title": "Machine learning-guided acyl-ACP reductase engineering for improved in vivo fatty alcohol production",
            "mention_or_use": "mention",
            "system_name": "Bayesian DBTL (protein engineering)",
            "system_description": "Construct a diverse chimeric sequence library (e.g., SCHEMA-guided recombination), evaluate an initial subset experimentally, fit a Bayesian surrogate model (e.g., GP or probabilistic model), use an acquisition strategy to select next sequences to test, iterate DBTL cycles; sequences converge in sequence-space toward improved catalytic performance.",
            "application_domain": "Enzyme engineering / protein engineering (improving catalytic rates/product formation)",
            "resource_allocation_strategy": "Bayesian acquisition balances model-predicted improvement and uncertainty to pick which chimeric sequences to test next out of a large recombination library, concentrating experimental effort on promising or informative variants.",
            "computational_cost_metric": "Primary practical cost is number of experimental constructs/assays and number of DBTL cycles required; computational modeling overhead not explicitly quantified.",
            "information_gain_metric": "Bayesian posterior predictive uncertainty and acquisition functions (not specified which one in review) govern selection; the process aims to maximize expected utility per experiment.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Bayesian optimization acquisition inherently balances exploration (uncertainty) and exploitation (predicted improvement); through cycles, the chosen sequences converge to a promising region in sequence space.",
            "diversity_mechanism": "Initial library construction (SCHEMA) creates initial diversity; the Bayesian acquisition may still sample across diverse candidates depending on uncertainty estimates but no separate explicit diversity-promoting term is described.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Limited experimental cycles and number of variants that can be screened",
            "budget_constraint_handling": "Use of Bayesian acquisition to prioritize variants that maximize expected improvement or expected utility given limited experimental slots.",
            "breakthrough_discovery_metric": "Measured gains in catalytic activity/product formation; in the cited example, final enzyme variants produced ~50% more fatty alcohols than parental strains.",
            "performance_metrics": "Cited result: target strains produced ~50% more fatty alcohols than parental strains after Bayesian DBTL optimization.",
            "comparison_baseline": "Parental enzyme sequences and initial library members; compared against starting strains.",
            "performance_vs_baseline": "+50% fatty alcohol production compared to parental sequences reported in review summary.",
            "efficiency_gain": "Demonstrated ability to find improved sequences with iterative Bayesian selection; explicit reduction in experimental count vs exhaustive search not quantified in review.",
            "tradeoff_analysis": "Review emphasizes Bayesian optimization as a way to escape local optima and save experimental resources by explicitly managing the exploration–exploitation tradeoff; specific tradeoff curves are not provided in the review.",
            "optimal_allocation_findings": "Bayesian DBTL is effective for large combinatorial sequence spaces: prioritize candidates by acquisition functions that trade predicted improvement against uncertainty to converge efficiently on high-performing sequences.",
            "uuid": "e2492.6",
            "source_info": {
                "paper_title": "Machine learning for metabolic pathway optimization: A review",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "RBS Bayesian pipeline (GPR + UCB)",
            "name_full": "Bayesian optimization pipeline for ribosome binding site (RBS) optimization using Gaussian Process Regression and Upper-Confidence Bound (UCB)",
            "brief_description": "A GPR surrogate models translation initiation rate (TIR) and an Upper-Confidence Bound acquisition (multi-armed bandit style) balances predicted mean and uncertainty to select RBS variants; over four DBTL cycles and 450 variants, RBSs exceeding a benchmark by up to 34% TIR were found.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "RBS Bayesian pipeline (GPR + UCB)",
            "system_description": "Gaussian Process Regression models the mapping from RBS sequence to TIR, providing predictive mean and variance; an UCB acquisition function (mean + kappa * std) is used to score candidates and select variants for batched experimental testing across iterative DBTL cycles.",
            "application_domain": "Gene regulatory element optimization (RBS design to tune translation)",
            "resource_allocation_strategy": "Batched experimental selections across DBTL cycles: choose variants with high UCB scores to balance exploration of uncertain sequences and exploitation of high-mean predictions within limited assay capacity.",
            "computational_cost_metric": "Number of RBS variants synthesized and assayed (450 variants tested across four cycles reported); computational overhead for GPR training is small relative to wet-lab costs and not explicitly quantified.",
            "information_gain_metric": "Predictive variance from the GP is part of the UCB score and thus used as an uncertainty/information metric.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "UCB acquisition balances exploration (variance term) and exploitation (mean term) by scoring candidates as mean + β * std and selecting high-scoring variants.",
            "diversity_mechanism": "Batched design and UCB scoring encourage sampling across both promising and uncertain sequences; batched selection can be arranged to enforce some diversity within a batch though the review does not detail a specific diversity constraint.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of experimental variants per DBTL cycle and total cycles (experimental throughput constraints).",
            "budget_constraint_handling": "Select a fixed number of variants per cycle by ranking UCB scores and executing top candidates subject to batch size limits; iterate until budget or target met.",
            "breakthrough_discovery_metric": "Translation initiation rate (TIR) relative to benchmark RBS sequences; discovery of RBSs with higher TIR indicates breakthroughs.",
            "performance_metrics": "450 RBS variants tested across four DBTL cycles; RBSs discovered with TIR up to 34% greater than the benchmark.",
            "comparison_baseline": "Benchmark RBS sequences / prior best-performing RBSs",
            "performance_vs_baseline": "Up to +34% TIR improvement over benchmark RBS reported in review.",
            "efficiency_gain": "Found higher-performing RBSs within a limited number of screened variants (450); explicit reduction in experiments vs exhaustive search not quantified.",
            "tradeoff_analysis": "Review highlights that UCB explicitly trades off predicted value and uncertainty; choosing β controls exploration/exploitation and batch composition.",
            "optimal_allocation_findings": "Use GP + UCB to prioritize batched experimental slots; tune the UCB parameter to control exploration vs exploitation depending on experimental budget and tolerance for risk.",
            "uuid": "e2492.7",
            "source_info": {
                "paper_title": "Machine learning for metabolic pathway optimization: A review",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SAPIENs",
            "name_full": "SAPIENs residual CNN with uncertainty quantification",
            "brief_description": "A residual CNN architecture trained on millions of translation data points to predict GRE (e.g., RBS) activities and to reliably quantify prediction uncertainty for downstream experimental recommendation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "SAPIENs",
            "system_description": "SAPIENs uses multiple ResNet-like CNNs trained on large high-throughput DNA–phenotype linked datasets (millions of measurements) to predict element activity and to estimate prediction uncertainty, enabling more reliable experimental decisions (e.g., selecting sequences with both high predicted activity and low model uncertainty).",
            "application_domain": "Gene regulatory element design (RBSs and other GREs)",
            "resource_allocation_strategy": "Provide sequence-function predictions with quantified uncertainty so experimenters can prioritize candidates that are both high predicted performers and have acceptable prediction confidence, thereby improving experimental budget allocation.",
            "computational_cost_metric": "Not explicitly specified; practical measure is number of sequences experimentally assayed; model training requires large data and compute but exact costs not reported in the review.",
            "information_gain_metric": "Model-predicted uncertainty (residual model uncertainty estimates) used as a proxy for expected information when prioritizing candidates.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Uncertainty-aware ranking: favor sequences with high predicted activity but also account for predicted uncertainty to avoid overconfident false positives; specific acquisition function not detailed in the review.",
            "diversity_mechanism": "Model trained on massive, diverse datasets which improves generalization; explicit batch-diversity mechanisms for selection are not described in the review.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Limited experimental throughput and capacity to synthesize/assay sequences.",
            "budget_constraint_handling": "Prioritize sequences that maximize predicted performance subject to acceptable uncertainty; use model uncertainty to avoid wasting budget on overconfident but unreliable predictions.",
            "breakthrough_discovery_metric": "High predicted experimental activity with validated experimental gains; breakthrough defined as discovering high-activity GREs that perform well experimentally.",
            "performance_metrics": "Trained on several million translation measurements; reported high predictive performance and portability across contexts in the review but no single numeric metric is provided in the review text.",
            "comparison_baseline": "Smaller models or models without uncertainty quantification; explicit baseline numbers not provided in the review.",
            "performance_vs_baseline": "Review states SAPIENs successfully identified correlations and quantified uncertainty reliably, but does not give numeric comparisons.",
            "efficiency_gain": "By quantifying uncertainty, SAPIENs can reduce wasted experiments on overconfident wrong predictions; no numeric efficiency gain reported in the review.",
            "tradeoff_analysis": "Emphasizes that uncertainty quantification helps balance selecting high-performing candidates and avoiding risky recommendations when experimental budgets are limited.",
            "optimal_allocation_findings": "In high-throughput sequencing-linked phenotype contexts, models that quantify uncertainty allow more informed allocation of experimental resources, prioritizing candidates that maximize expected reliable gain.",
            "uuid": "e2492.8",
            "source_info": {
                "paper_title": "Machine learning for metabolic pathway optimization: A review",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "BoostGAPFILL",
            "name_full": "BoostGAPFILL",
            "brief_description": "An ML-based strategy that integrates machine learning with constraint-based metabolic models to generate hypotheses for gap-filling metabolic reconstructions by constraining candidate reactions based on metabolite patterns; reported precision and recall exceed 60%.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "BoostGAPFILL",
            "system_description": "BoostGAPFILL uses features derived from incomplete network metabolite patterns and ML classifiers to prioritize candidate reactions/enzyme annotations that could fill gaps in draft genome-scale metabolic models; it produces ranked hypotheses for curation and experimental validation.",
            "application_domain": "Genome-scale metabolic model construction and curation (model gap-filling)",
            "resource_allocation_strategy": "Prioritize gap-filling hypotheses by ML-predicted likelihood (precision/recall ranking) so curators and experimentalists can focus validation efforts on higher-confidence candidate reactions.",
            "computational_cost_metric": "Computational ranking cost (ML model inference) is modest; main practical cost is downstream manual curation or targeted experiments to validate proposed reactions — review does not quantify computational FLOPs or monetary cost.",
            "information_gain_metric": "Not described as an explicit information-gain objective; ML classifier probabilities are used as confidence scores to prioritize validation.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Not formulated as an exploration–exploitation acquisition over experiments; it provides prioritized hypotheses for human/experimental follow-up.",
            "diversity_mechanism": "No explicit diversity-promoting mechanism described; selection based on predicted likelihood of correctness.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Human curation time and experimental validation resources (implicit)",
            "budget_constraint_handling": "Rank-ordered recommendations allow limited curation/validation effort to focus on high-probability gaps; reported precision/recall &gt;60% implies reduced wasted effort.",
            "breakthrough_discovery_metric": "Identification of correct gap-filling reactions and improved model predictive power; review cites &gt;60% precision and recall as performance indicators.",
            "performance_metrics": "&gt;60% precision and recall reported for BoostGAPFILL in gap-filling tasks (as summarized in the review).",
            "comparison_baseline": "Other gap-filling heuristics or non-ML methods; specific baselines not enumerated in the review.",
            "performance_vs_baseline": "BoostGAPFILL reported &gt;60% precision and recall, indicating improved hypothesis ranking versus prior undetailed baselines (no direct numeric baseline provided in review).",
            "efficiency_gain": "Reduces curation and validation workload by prioritizing higher-confidence gap-filling candidates; quantified by precision/recall but not by reduced number of validation experiments in the review.",
            "tradeoff_analysis": "Review frames BoostGAPFILL as trading computational hypothesis-ranking for reduced manual curation; does not present a formal cost-information tradeoff curve.",
            "optimal_allocation_findings": "Use ML to prioritize gap-filling hypotheses so limited wet-lab or curation effort focuses on the most promising candidates, improving curation efficiency.",
            "uuid": "e2492.9",
            "source_info": {
                "paper_title": "Machine learning for metabolic pathway optimization: A review",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A machine learning Automated Recommendation Tool for synthetic biology",
            "rating": 2,
            "sanitized_title": "a_machine_learning_automated_recommendation_tool_for_synthetic_biology"
        },
        {
            "paper_title": "Towards a fully automated algorithm driven platform for biosystems design.",
            "rating": 2,
            "sanitized_title": "towards_a_fully_automated_algorithm_driven_platform_for_biosystems_design"
        },
        {
            "paper_title": "Machine-learning-guided directed evolution for protein engineering",
            "rating": 2,
            "sanitized_title": "machinelearningguided_directed_evolution_for_protein_engineering"
        },
        {
            "paper_title": "Informed training set design enables efficient machine learning-assisted directed protein evolution",
            "rating": 2,
            "sanitized_title": "informed_training_set_design_enables_efficient_machine_learningassisted_directed_protein_evolution"
        },
        {
            "paper_title": "Low-N protein engineering with data-efficient deep learning",
            "rating": 2,
            "sanitized_title": "lown_protein_engineering_with_dataefficient_deep_learning"
        },
        {
            "paper_title": "Machine learning-guided acyl-ACP reductase engineering for improved in vivo fatty alcohol production",
            "rating": 2,
            "sanitized_title": "machine_learningguided_acylacp_reductase_engineering_for_improved_in_vivo_fatty_alcohol_production"
        },
        {
            "paper_title": "Machine learning guided batched design of a bacterial ribosome binding site",
            "rating": 2,
            "sanitized_title": "machine_learning_guided_batched_design_of_a_bacterial_ribosome_binding_site"
        },
        {
            "paper_title": "an efficient machine-learning workflow in conjunction with the YeastFab assembly strategy for combinatorial optimization of heterologous metabolic pathways in Saccharomyces cerevisiae",
            "rating": 1,
            "sanitized_title": "an_efficient_machinelearning_workflow_in_conjunction_with_the_yeastfab_assembly_strategy_for_combinatorial_optimization_of_heterologous_metabolic_pathways_in_saccharomyces_cerevisiae"
        }
    ],
    "cost": 0.0300715,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Computational and Structural Biotechnology Journal
27 March 2023</p>
<p>Yang Cheng 
Key Laboratory of Carbohydrate Chemistry and Biotechnology
Ministry of Education
Jiangnan University
214122WuxiChina</p>
<p>Science Center for Future Foods
Ministry of Education
Jiangnan University
214122WuxiChina</p>
<p>Xinyu Bi 
Key Laboratory of Carbohydrate Chemistry and Biotechnology
Ministry of Education
Jiangnan University
214122WuxiChina</p>
<p>Science Center for Future Foods
Ministry of Education
Jiangnan University
214122WuxiChina</p>
<p>Yameng Xu 
Key Laboratory of Carbohydrate Chemistry and Biotechnology
Ministry of Education
Jiangnan University
214122WuxiChina</p>
<p>Science Center for Future Foods
Ministry of Education
Jiangnan University
214122WuxiChina</p>
<p>Yanfeng Liu 
Key Laboratory of Carbohydrate Chemistry and Biotechnology
Ministry of Education
Jiangnan University
214122WuxiChina</p>
<p>Science Center for Future Foods
Ministry of Education
Jiangnan University
214122WuxiChina</p>
<p>Jianghua Li 
Key Laboratory of Carbohydrate Chemistry and Biotechnology
Ministry of Education
Jiangnan University
214122WuxiChina</p>
<p>Science Center for Future Foods
Ministry of Education
Jiangnan University
214122WuxiChina</p>
<p>Guocheng Du 
Key Laboratory of Carbohydrate Chemistry and Biotechnology
Ministry of Education
Jiangnan University
214122WuxiChina</p>
<p>Science Center for Future Foods
Ministry of Education
Jiangnan University
214122WuxiChina</p>
<p>Xueqin Lv 
Key Laboratory of Carbohydrate Chemistry and Biotechnology
Ministry of Education
Jiangnan University
214122WuxiChina</p>
<p>Science Center for Future Foods
Ministry of Education
Jiangnan University
214122WuxiChina</p>
<p>Long Liu longliu@jiangnan.edu.cn 
Key Laboratory of Carbohydrate Chemistry and Biotechnology
Ministry of Education
Jiangnan University
214122WuxiChina</p>
<p>Science Center for Future Foods
Ministry of Education
Jiangnan University
214122WuxiChina</p>
<p>Key Laboratory of Carbohydrate Chemistry and Biotechnology
Ministry of Education
Jiangnan University
214122WuxiChina</p>
<p>Computational and Structural Biotechnology Journal
27 March 2023ED768230A22976E50C1ED7004C01B24B10.1016/j.csbj.2023.03.045Received 15 September 2022 Received in revised form 24 March 2023 Accepted 25 March 2023Machine learning Metabolic pathway optimization Active learning Bayesian optimization Mechanism model Data-driven model
Optimizing the metabolic pathways of microbial cell factories is essential for establishing viable biotechnological production processes.However, due to the limited understanding of the complex setup of cellular machinery, building efficient microbial cell factories remains tedious and time-consuming.Machine learning (ML), a powerful tool capable of identifying patterns within large datasets, has been used to analyze biological datasets generated using various high-throughput technologies to build data-driven models for complex bioprocesses.In addition, ML can also be integrated with Design-Build-Test-Learn to accelerate development.This review focuses on recent ML applications in genome-scale metabolic model construction, multistep pathway optimization, rate-limiting enzyme engineering, and gene regulatory element designing.In addition, we have discussed some limitations of these methods as well as potential solutions.</p>
<p>Introduction</p>
<p>Microbiological fermentation is a green and sustainable approach to produce chemicals, materials, fuels, food, and pharmaceuticals.It can also contribute to solving the global energy crisis and environmental problems [1].However, natural microbials are rarely suitable for directly producing desired chemicals on an industrial scale.To overcome this obstacle, metabolic pathway optimization technologies, including genetic interventions, have been exploited to develop highly efficient microbial cell factories by redistributing the carbon metabolic flow toward desired metabolites [2].</p>
<p>Significant progress has been made in metabolic pathway optimization over the past few decades.For example, replacement of a promoter in Escherichia coli BL21 led to a 4.2-fold increase in 2,3butanediol production (73.8 g/L) in fed-batch fermentation compared with that obtained in a previous study [3].In another case, rubusoside was de novo biosynthesized in Saccharomyces cerevisiae using a systematic engineering strategy, and the titer reached 1368.6 mg/L [4].Moreover, implementation of a global transcriptional machinery engineering strategy led to a 114% increase in L-tyrosine production in E. coli P2 in large-scale fermentation [5].Despite notable applications of metabolic pathway optimization technologies, some potential problems may hinder further development.For instance, due to incomplete understanding of the relationship between the phenotype and genotype of target cells, the conventional trial-and-error approach is often adopted [6].In addition, identifying and testing each combination of different pieces of the genome takes time and effort, and this can significantly hinder the construction and application of microbial cell factories [7].Moreover, despite rapid development in DNA editing technology, the incredible potential of genome-scale engineering has not been fully exploited because conventional research has focused on the redirection of carbon flux in a limited number of metabolic pathways [8][9][10].</p>
<p>Machine learning (ML), a subdiscipline of artificial intelligence, attempts to imitate how the human brain learns and interacts using computers that implement learning algorithms [11][12][13].ML-enabled systems can extract knowledge from previously collected data and use this knowledge to build simulation models.ML has recently been applied in optimizing metabolic pathways due to its excellent modeling ability.On one hand, ML has often replaced conventional statistical methods, including linear regression and partial least squares-based statistical modeling, to build models used to identify features within biological datasets.For instance, DeepEC was developed to predict enzyme commission (EC) numbers using a protein sequence as an input [14].On the other hand, ML can also be integrated into Design-Build-Test-Learn (DBTL) cycles to explore design space more effectively.For example, Zhou et al. proposed an ML-assisted tool to determine the optimal combination of enzyme expression levels [14]; Greenhalgh et al. proposed an ML-based workflow to improve the performance of rate-limiting enzymes [15].Based on this, the present study reviews the recent applications of ML in genome-scale metabolic model (GEM) construction, multistep pathway optimization, rate-limiting enzyme engineering, and gene regulatory element (GRE) designing, which constitute the critical frameworks of metabolic pathway optimization.In addition, we have discussed some limitations of these methods as well as potential solutions.</p>
<p>Application of ML in genome-scale metabolic model (GEM) construction</p>
<p>GEMs computationally describe the metabolic networks of organisms using gene-protein-reaction (GPR) rules [16], which aim to understand the authentic relationships between genotypes and phenotypes [17].Classical GEMs comprise the mathematical metabolic network and the objective function, both of which are subject to stoichiometric constraints in flux balance analysis (FBA) [18,19].However, applying purely stoichiometric constraints usually results in an underdetermined system with infinite solutions [20].To achieve more accurate prediction of metabolic flux distribution, novel modeling concepts that include more cellular processes have been proposed, such as enzyme-constrained genome-scale metabolic models (ecGEMs) [21], macromolecular expression models [18], and whole-cell models [22].Despite considerable advancement in the modeling concept, the deficiencies in quantitative mechanistic representations of our knowledge of molecular processes have limited the development of advanced GEMs.Since its recent emergence as a key data-driven method, ML has been employed to quantitatively depict various subcellular processes (Fig. 1).</p>
<p>Classical GEMs</p>
<p>Since the first GEM of Haemophilus influenzae was reported in 1999, several classical GEMs have been constructed for approximately 6239 organisms [23].Moreover, some classical GEMs for industrial organisms have been upgraded several times as the understanding of GPR relations has been updated; these industrial organisms include S. cerevisiae, E. coli [18], Bacillus subtilis [24], and Corynebacterium glutamicum [25,26].Although classical GEMs only consider the stoichiometric constraints of the metabolic network [22], they have offered many valuable suggestions for metabolic pathway optimization over the past decade [27][28][29].Moreover, the construction workflow of some advanced GEMs, including thermodynamic, enzymatic, and kinetic constraint models, requires classic GEMs to offer high-quality metabolic networks.Thus, building highquality classical GEMs remains critical in the GEM field [17,30,31].</p>
<p>Drafts of classical GEMs are always constructed using the annotation results for the whole genome [32].Thus, employing genome annotation tools with excellent performance is crucial for developing high-quality metabolic networks.To improve the accuracy of the genome annotation step, various ML algorithms are applied to exploit the information in the target genome.For example, to determine minor variations in annotated gene regions between different prokaryote (sub)-species, a novel neural network named DeepRibo was proposed to precisely delineate and annotate expressed genes using features extracted from ribosome profiles and binding site sequence patterns [33].In addition to precisely locating expressed genes, high-quality and high-throughput prediction of enzyme functions is essential for genome annotation.DeepEC, a deep learning-based computational framework, was developed to predict EC numbers of protein sequences with high precision and in a high-throughput manner [34].For the development of DeepEC, three convolutional neural networks were integrated into a single engine that could predict EC numbers.</p>
<p>After construction of the draft model, gaps usually exist due to incomplete knowledge of the metabolic network [35].To ensure adequate GEM quality, it is essential to analyze the metabolic network and fill gaps within it.Recently, several studies have focused on improving the accuracy and efficiency of the gap-filling process.For example, Tolutola et al. proposed an ML-based strategy named BoostGAPFILL [35], which leverages ML methodologies and constraint-based models to generate hypotheses for gap-filling and model refinement.More specifically, reactions that fill gaps are constrained by metabolite patterns in the incomplete network.BoostGAPFILL shows &gt; 60% precision and recall.Similarly, in another study, a series of ML algorithms, including decision trees and logistic regression [36], was applied to identify missing reactions and enzymes in the model.By collecting 123 pathway features from 5610 pathway instances, these ML methods exhibited greater interpretability and extensibility than other pathway prediction algorithms.</p>
<p>Model refinement is also essential to verify gap-filling solutions following the construction of a draft model.In recent years, several automatic construction tools, such as CarveMe [37], Merlin [38], ModelSEED [39], and Pathway Tools [40], have been proposed due to the massive increase in biochemical knowledge and computational annotation of genomes [41].However, curation and refinement of metabolic networks in GEMs are still rare [42].Recently, an MLbased method, called automated metabolic model ensemble-driven uncertainty elimination using statistical learning, has shown that ML can reduce the workload of curation work for draft GEMs.In their research, iterative gap-filling was initially performed to produce random gap-filling solutions [43].Moreover, supervised and unsupervised learning were integrated to determine the uncertainty of the model.This study suggested that combining ML strategies can improve the efficiency of tedious yet essential manual refinement and curation steps.</p>
<p>Enhanced and multiscale models</p>
<p>Compared with classical GEMs, multiscale models consist of multiple heterogeneous models or networks of different cellular layers [22].Typical examples of multiscale models include thermodynamic constraint GEMs [43,44], enzymatic constraint GEMs [21,45], and multiomics-integrated GEMs [46].Each of these has been widely used in biological discoveries, extensive data analysis, and metabolic engineering [22].Recently, various ML strategies have been applied to construct heterogeneous models and networks.For example, expectation maximization (EM)-like algorithms are used to estimate critical parameters in metabolite identification models [47,48], and RF is applied to predict enzyme turnover numbers.Introducing ML algorithms into multiscale models can improve model quality effectively and expand model network dimensionality [49].</p>
<p>Traditional model analysis approaches, such as FBA, search for an optimal growth rate that is constrained only by metabolic network stoichiometric constraints and uptake rates [20].To further enhance the simulation capabilities of GEMs, the cost of expressing enzymes within metabolic networks is an essential constraint to be considered while constructing ecGEMs [50].By adding this enzyme constraint, ecGEMs can accurately simulate maximum growth ability, metabolic shifts, and proteome allocations of various enzymes [51].ecGEMs are constructed using genome-scale enzyme turnover numbers (k cat s), each of which defines a reaction's maximum chemical conversion rate.Nevertheless, the scope of the k cat s dataset is far from the genome scale because k cat s are usually measured via low-throughput assays in vitro [52].In addition, there is a vast difference between k cat s in vivo and in vitro due to incomplete saturation, posttranslational modifications, and allosteric regulation [53].To alleviate this problem, Heckmann et al. proposed a ML method to predict k cat s [50].EC numbers, molecular weight, in silico flux predictions, and assay conditions were integrated to predict k cat s under both in vivo and in vitro conditions.Finally, improved forecasts of proteome allocation were achieved by applying ML models to parameterize GEMs.In addition, to further improve prediction accuracy, the in silico flux predictions were replaced with 13 C fluxomics data to estimate k cat s in vivo [54].Moreover, whether the maximum value of k cat s is robust to genetic perturbations was tested by performing gene knockout experiments and adaptive laboratory evolution.The results indicated that maximum k cat s values in vivo are stable.However, despite improvement in predicting k cat s in vivo, features such as average metabolic flux and catalytic sites obtained from protein structure are typically too complex to obtain from nonmodel organisms.To this end, Lee et al. developed a deep learning approach (DLKcat) to predict k cat s from only substrate structure and protein sequence data.Using this method, k cat s profiles for 343 yeast/fungi species were predicted.Furthermore, an automatic Bayesian-based pipeline was proposed in their study, which enabled the automatic reconstruction of 343 ecGEMs of yeast [55].</p>
<p>In addition, calculating the Gibbs energy of reactions is an essential step in constructing thermodynamic constraints for multiscale models [56].However, metabolite identification is a significant challenge in metabolomics due to the number and diversity of molecules.Metabolites can be rapidly identified via fingerprint identification.Nevertheless, many metabolites require more accurate fingerprints.To this end, ML has been employed in recent studies for the prediction of metabolite fingerprints.FingerID, a classical method, has been proposed to predict corresponding fingerprints from a mass spectrometry (MS) set with supervised ML [57].A support vector machine (SVM) selects fingerprints with integral mass and probability product kernels.FingerID is mainly based on information derived from individual peaks present in the spectra.However, the relationships between different peaks have also been used to predict fingerprints.To further increase model predictive power, CSI: FingerID, an extended version of FingerID, was proposed by combining MS spectra with a corresponding fragmentation tree [58].CSI: FingerID exhibited better predictive accuracy than FingerID in predicting the fingerprints of metabolites.Nevertheless, the optimization of hyperparameters becomes more complex since more inputs are required in CSI: FingerID.Moreover, kernel-based methods are not desirable for dealing with MS spectra since the spectrum comprises only a few peaks.To mitigate these limitations, a new algorithm named SIMPLE was proposed [59].SIMPLE is more efficient and interpretable in predicting metabolite fingerprints.SIMPLE is a generalized additive model that captures information from individual peak and peak interactions.Compared with kernelbased methods, an obvious advantage of SIMPLE is its prediction speed.Moreover, the performance of SIMPLE is correlated with several peaks in the spectrum, whereas that of kernel-based methods explicitly depends on the size of the training dataset.</p>
<p>Transcriptional regulatory networks (TRNs) explain complex life phenotypes at the genomic level of organisms under different environments [60].Integrating TRNs and GEMs can enable a more comprehensive understanding of metabolic regulation and stress response [22].Several conventional methods, such as Pearson correlation, have been employed to infer TRNs.However, these methods require multilevel biological data to accurately predict TRNs [61].Recently, a supervised ML strategy, CNN for coexpression (CNNC), was proposed to infer gene relationship [62].In their study, image representation was used to replace conventional information.More specifically, the model was trained with negative and positive examples of a specific domain, and the prediction could be either binary or multinomial.However, an important feature, time information, is ignored by the CNNC model.Therefore, a hybrid deep learning framework for gene regulatory network inference from single-cell transcriptomic data (DGRNS) was developed to capture time information in expression data [62].DGRNS provides a supervised ML method that can extract both statistical and time-related features.In its workflow, DGRNS first performs a series of pretreatments and constructs correlation vectors that represent gene expression features [17], and this approach therefore improves the accuracy of the inference of TRNs.</p>
<p>Although ML has many successful applications in the construction of GEMs, some potential problems may hamper the further application of ML in GEMs.For example, advanced ML tools are yet to be integrated into conventional construction frameworks.Moreover, constructing a whole-cell model by coupling all mechanism models is challenging because it is laborious to establish mechanistic models for all subcellular processes.</p>
<p>ML-guided multistep pathway optimization</p>
<p>A major task of metabolic pathway optimization is to identify the optimal combination of multiple gene expression levels within a pathway.It is a promising strategy to fully explore the potential genetic design space using high-throughput (HTP) technologies [63].However, the performance of HTP screening depends on accurate and rapid detection methods, which are only available for some biochemicals [64].Moreover, searching the entire genetic design space is a resource-intensive strategy, which may incur high costs.To this end, many computational approaches have been used to understand the metabolic regulation processes of microorganisms and identify genetic interventions necessary to achieve a desired phenotype [65].Nevertheless, due to the complexity of biological systems, mechanistic models, such as GEMs, and kinetic models can only offer suggestions for optimizing metabolic pathways.Optimal genetic interventions remain to be identified using conventional trial-and-error approaches [66].To overcome these challenges, ML and statistical methods have been used to explore the genetic design space more effectively.</p>
<p>Active learning</p>
<p>Active learning reduces resource overhead and human effort by gradually exploring design space to improve model quality [67].Design-Build-Test-Learn (DBTL) cycles are usually tedious and time-consuming because the learn phase of DBTL needs to be better developed [2].To this end, active learning has been introduced to accelerate the DBTL cycle by enhancing the learning phase.For example, MiYa, an ML workflow that works in conjunction with the YeastFab [68] assembly strategy, was proposed to optimize the expression level of numerous genes by replacing their promoters [14].As an active learning method, MiYa adopted a low-throughput experimental strategy to ensure the accuracy of experimental data.To avoid overfitting, 1000 ANN models with random initial weights were used to predict the best strain within the design space.In addition, the quality of the initial dataset has been proven to affect the accuracy of prediction.Since the products are colored, a highquality initial training dataset was constructed via manual screening to improve prediction accuracy (Fig. 2).Similarly, Opgenorth et al. integrated ML into DBTL cycles to optimize dodecanol production in E. coli [69].Nevertheless, the biosynthesis of dodecanol is more complicated because protein abundance within the pathway not only affects the yield but also the purity of the product.Thus, multiobjective modeling methods, including random forest, polynomial, and multilayer perceptron models, have been systematically tested and compared.After two cycles of DBTL, the yield obtained was 6fold greater than that previously reported [58].Furthermore, active learning has been applied in developing classification models.For example, Kumar et al. [65] proposed a strategy based on an SVM, ActiveOpt, to enhance microbial chemical production.In their strategy, data are divided into two classes according to a specific cutoff and is used to train the SVM.The SVM is then applied to find the point farthest from the hyperplane within the design space.If this point can be verified experimentally, it is entered into the dataset and the cutoff is updated.</p>
<p>Although the abovementioned studies successfully integrate active learning with DBTL, several potential problems hamper further application of these methods.For example, the frameworks mentioned above adopt only one ML algorithm and may therefore be applicable in only some cases.Moreover, the design space is often too conservative due to limitations associated with experimental throughput.Furthermore, the methods listed above generally make decisions on where to evaluate in the next round based only on the output value of the predictive model.They can therefore be trapped by local optima when the confidence of the output value is ignored.</p>
<p>Bayesian optimization</p>
<p>Theoretically, active learning aims to improve a model's performance by gradually exploring the areas of the design space with the lowest confidence.However, in reality, researchers generally prefer to use fewer resources to quickly identify optimal solutions.In addition, the accuracy of the predictive model in low output regions of design space is not critical [70].</p>
<p>To escape from local optima and save resources, an exploitation-exploration strategy known as Bayesian optimization has been introduced for metabolic pathway optimization [65].For example, a new metabolic pathway optimization tool, the Illinois Biological Foundry for Advanced Biomanufacturing (iBioFAB), simultaneously considers the expected outcome of each evaluation and the confidence in the desired outcome [71].Gaussian Process was employed to assign a mean and variance to each point within the design space.With increasing size of the training set, the mean value and confidence are gradually adjusted.Next, an acquisition function, termed expected improvement, identifies the point of optimal exploitation by quantifying the tradeoff between output value and confidence.To test the performance of iBioFAB, the lycopene production pathway was optimized by fine-tuning gene expression levels.iBioFAB successfully introduced the exploitation-exploration strategy into metabolic pathway optimization.However, a problem still exists.The prediction distribution is assumed to be Gaussian, which does not apply to all situations [70].To extend the application scope of Bayesian optimization, an automated recommendation tool (ART) for ML approaches was proposed [72].Compared with iBioFAB, ART integrates multiple ML algorithms using the scikit-learn toolbox.Notably, ART enables sidestepping the challenge of model selection using an ensemble model approach (Fig. 2).Parallel-tempering-based Markov chain Monte Carlo-based (MCMC) sampling is used as an acquisition function to determine exploration or exploitation in the decision-making process.Furthermore, the capabilities of ART were demonstrated on simulated datasets and experimental data from real metabolic engineering projects related to the production of renewable biofuels, hops-flavored beer without hops, fatty acids, and tryptophan [69].</p>
<p>Frameworks based on active learning and Bayesian optimization have significantly enhanced the ability of researchers to explore the vastness of design space.Nevertheless, some limitations still exist.For example, the identification of targets in many studies is based on accurate prior knowledge, which limits the application scope of this advanced framework.Moreover, the targets often belong to the same Fig. 2. Main strategies used to accelerate Design-Build-Test-Learn (DBTL) cycles.Two strategies can be applied to accelerate DBTL cycles: active learning and Bayesian optimization.Research using the active learning strategy often avoided overfitting by multiple modeling.In this framework, data points with higher output were considered in subsequent evaluation rounds.In contrast, research using the Bayesian strategy attempted to avoid overfitting by constructing a Bayesian ensemble model in which the tradeoff between output value and confidence could be exploited.pathway, which may strongly restrict the power of advanced ML frameworks to optimize metabolic flux on the genome scale.</p>
<p>Application of ML in rate-limiting enzyme engineering</p>
<p>Natural enzymes are rarely optimal for industrial applications [73].However, their untapped potential can be leveraged to satisfy the demand for diverse biotechnological applications and meet specific biotechnological challenges [74].There is a relationship between the selection function (i.e., fitness) and the sequence of amino acids.This is termed as the fitness landscape and is represented as a surface in a high-dimensional space defined by the function f(sequence) = fitness [75][76][77].Exploration of the protein fitness landscape is the primary task of protein engineering [78].Nevertheless, this job is challenging because the search space grows exponentially with the number of amino acid positions considered.Moreover, functional proteins are extremely rare in the fitness landscape [79].To identify optimal sequences within the landscape, rational protein redesign and directed evolution (DE) have been applied to protein engineering.Rational protein redesign builds a mechanistic model based on molecular dynamics simulations to predict changes in structure and protein fitness caused by specific mutations [80].However, the success of this method depends on accurate protein structure data.In addition, full simulations of the complete protein landscape in silico are resource-intensive [81].Compared with rational protein redesign, DE improves protein fitness by iteratively accumulating positive mutation results, independently of prior knowledge and simulation data in silico [82].Moreover, DE can be conducted with a low screening burden because only single mutations and not combinations of mutations are assessed in each round [83].However, this process ignores the cooperation of different mutations and can be easily trapped by local optima.Increasingly, ML algorithms have been applied to approximate the protein fitness landscape, and they require no prior physical, chemical, or biological knowledge [84].</p>
<p>In addition to accumulating single positive mutations, the DE's ability to explore the fitness landscape can be further enhanced via simultaneous saturation mutagenesis.ML can be employed to assist traditional strategies to explore the fitness landscape.For instance, Wu et al. proposed a ML-assisted directed protein evolution strategy (MLDE) similar to the active learning pipeline mentioned above [80]; this model can offer data points for a researcher to collect according to simulation results, and the tested data points are employed to update the model.This cycle continues until a desired engineering goal is achieved.MDLE serves as an excellent framework for successfully integrating DE with ML, and it has been found to effectively avoid some local fitness traps or long paths to the global optimum (Fig. 3A).However, MLDE still needs to consider a few design considerations.</p>
<p>There are two main questions in exploring the fitness landscape through ML. (1) How to select the suitable encoding strategy?(2) How to handle low-fitness variants in the fitness landscape?Amino acid sequences must be numerically encoded for the training process [85].Mutating amino acids to different ones (i.e., in terms of charge and molecular weight) is more likely to affect the structure and function of a protein than mutating it to residues that more closely resemble the original [86].Thus, researchers generally include such information in ML models via an encoding step to improve the efficiency of the learning phase.Nevertheless, the one-shot encoding strategy adopted in MLDE ignores information regarding the biochemical similarities and differences between amino acids [87].In addition, fitness landscapes tend to be enriched in zero-or extremely low-fitness variants, which provide minimal information regarding the regions of interest in a landscape.For instance, 92% of regions in the empirically determined four-site combinatorial fitness landscape (20 4 = 160,000) had fitness values that were below 1% of the global fitness maximum [87].Thus, the initial sampling of the fitness landscape deserves more consideration.However, MLDE adopted a random sampling protocol to build an initial dataset, which may contain diverse sequences but may provide little information.In any case, the model's accuracy in predicting low-fitness variants is not critical.</p>
<p>To alleviate these challenges, an improved version of MLDE named ftMLDE was proposed to enhance the encoding strategy and initial sampling [85].Several alternate encoding methods, including physicochemical encoding and learning models derived from eight natural language processing models, have been evaluated [88,89].Furthermore, a novel strategy that leverages global sequence information derived from large sequence databases was employed to reduce uninformative holes in MLDE training sets.Compared with the original MLDE, ftMLDE represents at least a 12.2-fold improvement in searching the fitness landscape.Thus, building a highquality model of the fitness landscape of a focal protein appears to be feasible by improving the encoding strategy.Moreover, some studies are trying to reduce the exploration of low-fitness regions of the landscape to save resources.Complex design principles are hidden in the amino acid sequence of natural proteins, and by employing ML to understand these principles, it may be feasible to appropriately predict low-fitness variants in the fitness landscape.An unsupervised deep learning model, UniRep, was recently employed to distill the fundamental features of a protein, including biophysical, structural, and evolutionary information, into a statistical summary (Fig. 3C).Moreover, unsupervised deep learning was performed on &gt; 20 million raw amino acid sequences to distill the general features of all functions [90].Furthermore, a novel framework called Low-N protein engineering has been developed by combining UniRep with active learning.Unlike the studies discussed above, Low-N protein engineering does not rely on high-quality training sets but realizes data dimensionality reduction by incorporating information from numerous proteins to improve the model accuracy; moreover, MCMC-based sampling can help avoid local optimization [90].</p>
<p>In addition, Bayesian optimization has been employed to search chimeric libraries.A recent study successfully enhanced the catalytic rate of alcohol-forming fatty acyl reductases on acyl-ACP substrates using an ML-based protein engineering strategy (Fig. 3B) [15,91].Initially, better enzyme performance was achieved through the rearrangement of different subunits of three enzymes from three different sources.To further explore how gene shuffling can enhance fatty alcohol production, the authors of this study constructed an extensive library of rate-limited domains using SCHEMA structureguided recombination.Next, a Bayesian-based DBTL cycle was used to optimize protein fitness.As the number of cycles increased, the target sequence gradually converged to a specific position in sequence space.In the end, the target strains produced 50% more fatty alcohols than the parental strains.</p>
<p>ML has also been used to reveal the potential underlying protein scaffolds, whereas unsupervised deep learning has been used to learn the semantic grammar within protein sequences deposited in large databases and guide protein engineering.On the other hand, supervised active learning has been applied to model the protein fitness landscape through numerous cycles and identify optimal designs.These strategies can also be combined to further improve protein engineering.However, most of these strategies overemphasize the intelligence of protein engineering, which may lead to situations where we are unable to understand the underlying mechanism of protein engineering.</p>
<p>Application of ML in GRE designing</p>
<p>Metabolic pathway optimization is based on essential GREs, such as promoters, ribosome binding sites (RBSs), and terminators.Hence, one of the main challenges of synthetic biology is to artificially design GREs to meet specific requirements [92].So far, many studies have attempted to build models of the relationship between sequences and the functional properties of proteins.For instance, Jensen et al. used a statistical method to explore the effect of nucleotide position on promoter intensity.Moreover, partial least squares methods were employed to analyze synthetic promoters in E. coli and B. subtilis, respectively [81].Despite advancements in GRE modeling, these studies are hampered by small datasets, use of single-modeling approaches, and imperfect correlations [93].Recently, ML has therefore been employed to resolve these problems.</p>
<p>Promoters</p>
<p>The de novo design of promoters, including both known and novel promoters, has also been facilitated by ML approaches [94].This has subsequently contributed to optimizing metabolic pathways in systems metabolic engineering.Recently, an ML workflow was developed to integrate mutation-construction-screening-characterization (MCSC) engineering cycles and the XgBoost algorithm to identify relationships between promoter sequences and promoter intensity [95].In short, a de novo synthetic promoter library was reconstructed and characterized based on high-strength constitutive promoters and broad dynamic range libraries.Next, ML algorithms were used to analyze the relationships between sequences and functions of promoters.MCSC could effectively extend the dynamic range of promoters and provide high-quality data for ML to build models [96].However, this study was limited by a relatively small library size compared with all possible sequence combinations.In another study, the potential hidden in sequence combinations was analyzed using a generative adversarial network (GAN) to extract features from natural promoters and generate millions of new artificial sequences [97].GAN learned the design principles of the critical regions of a promoter from the natural sequence, such as k-mer frequency, -10 and -35 motifs, and their spacing constraints.Then, a considerable part of the sequence combination space was filtered out using a GAN.Consequently, 70.8% of AI-designed promoters were experimentally demonstrated to be functional, establishing a new strategy for effectively designing brand-new functional promoters [98].</p>
<p>In addition to constitutive promoters, designing inducible promoters from scratch is attractive.For instance, ligand-responsive allosteric transcription factors (aTFs) are essential because they transform biochemical signals into gene expression changes [99].Thus, accurate control of gene expression may be achieved by manipulating an aTF-regulated promoter.Recently, a novel strategy was proposed to design aTF-regulated promoters from scratch.The innovation of this work lies in two aspects: (1) a high-quality dataset was constructed based on the combination of in vivo and in vitro screening and (2) support vector regression was employed to build models to accurately predict induction ratios (Fig. 4A).Interestingly, the inducible promoters identified in this study were based on a minimal constitutive promoter.</p>
<p>RBSs</p>
<p>Promoter engineering focuses on regulating the transcriptional level or stability of RNA, i.e., steps that occur long before translation and protein folding.Unlike promoters, RBSs tune gene expression by directly adjusting translation levels and protein folding [100].Recently, some studies have used ML to elucidate the relationship between RBS sequences and their strength.For example, a Bayesian optimization pipeline was used to improve the translation initiation rate (TIR) of RBSs through DBTL cycles [101].This study used GPR to model the design space in the learn phase.The upper confidence bound multiarmed bandit algorithm was then applied to balance exploitation and exploration in the design phase.By testing a total of 450 RBS variants from four DBTL cycles, RBSs with high TIR values exceeded their RBS benchmark by up to 34%.However, compared with improving RBS strength, achieving a desired output at an appropriate translation level is more attractive for biosensor systems.Hence, another study attempted to identify the optimal combination of RBS sequences in a biosensor system driven by small molecule responsive transcription factors [92].This resulted in the construction and characterization of 120,000 cross-RBSs.These biosensor systems were classified into five categories based on their dynamic range (Fig. 4B).A classification model derived from CNNs can accurately predict dynamic ranges based on RBS sequence combinations in a biosensor system by learning the dataset.</p>
<p>Two steps are required to understand the relationship between protein sequence and function: (1) prepare a training dataset and (2) select an ML algorithm to model the sequence space.A recent study provided these methods simultaneously [93].An innovative HTP approach was initially developed to assign a quantitative functional readout to each specific sequence by constructing a three-component genetic architecture using the same DNA molecule.Thus, GRE sequences and their functional readouts could be read and unambiguously linked from a single sequencing read (Fig. 4C).Further, an approach termed SAPIENs has been developed to quantitatively predict RBS activity and to reliably quantify prediction uncertainty using a residual CNN.Using several million data points from measured translation data, SAPIENs successfully identified a correlation between an RBS sequence and its functional readout [93].Furthermore, this strategy also showed excellent portability, and may therefore be employed to study the relationship between sequence and function for other GREs (Table 1).</p>
<p>Summary and outlook</p>
<p>Various workflows and frameworks based on ML are rapidly emerging and being applied to solve different types of problems.When sufficient available data exist, ML can replace conventional statistical modeling methods to build accurate models that simulate complex nonlinear processes.For instance, various omics datasets can be fully leveraged to build data-driven models that simulate complex cellular processes.Moreover, the relationship between sequence and function can be precisely characterized by ML using corresponding HTP technologies.When there is insufficient data, ML can also be employed to effectively search design space or landscape.For example, active learning and Bayesian optimization have been applied to identify the optimal combination of gene expression levels, to guide protein engineering, and to redesign GREs.This critical review provides an overview of ML technologies that can be used to guide further metabolic pathway optimization.However, despite the advantages of ML technologies, potential challenges and research gaps may limit the further application of ML for metabolic pathway optimization.</p>
<p>These potential challenges and solutions in metabolic pathway optimization are as follows.First, ML has many applications in genome annotation, gap-filling, and metabolic network refinement.However, no framework or software fully integrates all advanced ML applications at the GEMs construction stage and therefore cannot provide explicit guidance for manual refining.In addition, constructing a whole-cell model using coupling mechanism models is difficult because it is laborious to establish mechanistic models for all subcellular processes.To address these problems, ML methods for model construction tasks should be integrated into a framework or software.Moreover, an active ML framework should be combined with non-ML frameworks to further improve model quality by carrying out limited experiments.Furthermore, it may also be feasible to simulate metabolic processes accurately by constructing a digital twin model.To pursue this goal, ML must be more involved with the construction of GEMs instead of estimating parameters for mechanistic models.Moreover, with advancements in multistep pathway optimization frameworks, the potential of new toolkits based on active learning and Bayesian optimization will be further explored.Nevertheless, identifying targets in most of the studies mentioned above depended to some degree on prior knowledge.Moreover, these targets often belong to the same path, which may excessively restrict the power of ML to optimize metabolic flux on the genome scale.To alleviate these challenges, mechanistic models, such as GEMs, can be integrated into the pathway optimization toolkit to provide more convincing targets.Furthermore, ML has successfully identified untapped potential beneath protein scaffolds.However, most of these strategies overemphasize the intelligence of protein engineering, which may lead to our inability to understand the underlying mechanisms involved in particular protein engineering pathways.To cope with these problems, models of protein structures should be closely combined with the other methods listed above.Through multiple rounds of DBTL, we can continuously improve our understanding of the underlying mechanisms of all enzyme interactions.</p>
<p>In addition, we note that studies using ML for metabolic pathway optimization are not fully integrated, which may limit its applications.For example, the targets of multistep pathway optimization can identify based on prior knowledge rather than by computational simulations.Moreover, ML-guided multistep pathway optimization and enzyme engineering have yet to be combined due to the lack of a global perspective.Furthermore, the data generated from DBTL cycles was not sufficiently leveraged to strengthen the interpretability of this kind of "black box" model.Therefore, to further explore the potential of ML for metabolic pathway optimization, GEMs should be considered a core element during construction of an ML-assisted DBTL cycle (Fig. 5).According to our simulation results, GEMs can provide insight into target identification during multistep pathway optimization.Moreover, fluxomics and protein allocation data generated by simulating large and complex networks can be considered as additional input for active learning or Bayesian optimization to improve the performance of predictive models.Meanwhile, the data generated by DBTL cycles can be employed to update GEMs.Subsequently, the updated GEM will identify the highest-priority enzymes that need to be produced using enzyme cost and flux analysis.Coupling multistep pathway optimization and rate-limiting enzyme engineering with the interpretability of GEMs can therefore help optimize metabolic flux on a system level.This will provide global insights into metabolic pathway optimization.Furthermore, this framework can be further upgraded by adopting automated processes that use robots and high-throughput systems.</p>
<p>CRediT authorship contribution statement</p>
<p>Fig. 1 .
1
Fig.1.Machine learning (ML) applications for genome-scale metabolic model (GEM) construction.There are three main categories of ML applications for GEM construction: classical GEMs, enhanced GEMs, and multiscale models.Typical applications are listed in the figure according to the adopted ML algorithm and primary task.</p>
<p>Fig. 3 .
3
Fig. 3. Typical machine learning (ML) applications in rate-limiting enzyme engineering.A. ML-guided directed evolution.The ML model can offer simulated data points for researchers to collect and test, and the results of these tests generate new data that can be used to update the model.This cycle continues until a desired engineering goal is achieved.B. Deep learning-based encoding strategy.Unsupervised deep learning was used to distill the fundamental features of an individual protein, including its biophysical, structural, and evolutionary information, into a statistical summary.C. Bayesian optimization-assisted protein engineering.A target sequence gradually converges to a specific position in sequence space with an increasing number of cycles of optimization.</p>
<p>Fig. 4 .
4
Fig. 4. Typical machine learning (ML) applications as used in gene regulatory element (GRE) design.These strategies are divided into two steps: generating training sets and training models.A. Establishment of a regression model for promoter intensity.A training dataset was built based on in vitro enrichment and in vivo characterization.Support vector regression was then used to build the model.B. Establishment of a classification model for ribosome binding sites (RBSs).A cross-RBS (c-RBS) library was constructed by combining RBSs and biosensor systems.Next, the total dynamic range of these c-RBSs was divided into five sub-libraries.Finally, a convolutional neural network was used to build a model to classify c-RBSs according to dynamic range.C. A large-scale DNA-based phenotypic recording strategy.A three-compartment genetic architecture was used to record GRE phenotypes.Subsequently, a deep learning model relates sequence to function.</p>
<p>Yang Cheng :
Cheng
Conceptualization, Investigation, Writing − original draft.Xinyu Bi: Supervision, Investigation, Writing − original draft.Yameng Xu: Supervision, Writing − review &amp; editing.Yanfeng Liu: Supervision, Project administration, Writing − review &amp; editing.Jianghua Li: Writing − review &amp; editing.Guocheng Du: Writing − review &amp; editing.Xueqin Lv: Supervision, Project administration, Writing − review &amp; editing.Long Liu: Funding acquisition, Writing − review &amp; editing.</p>
<p>AcknowledgmentsThis work was financially supported by the Key Research and Development Program of China (2020YFA0908300), the National Natural Science Foundation of China (32070085), and the Fundamental Research Funds for the Central Universities (JUSRP622004, JUSRP222007).Declaration of Competing InterestThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.Table 1ML tasks and their corresponding algorithms previously applied in metabolic pathway optimization.Learning styleTaskAlgorithmTask descriptionReferenceActive learningClassification/ regression Bayesian ensemble approach• ART provides predictions and recommendations for the next experimental cycle.[72]Bayesian optimization/ Gaussian process (GP)• iBioFAB combines with a fully-automated robotic platform to guide experimental design.[71]Regression GP/Upper-confidence bound (UCB)• GP is trained to make predictions, and UCB optimization is utilized to select informative sequences.[93]Supervised/ unsupervised learning Clustering/ classification K-means/random forest (RF)• K-means clustering assigns cluster membership, and simulation clusters are then used as labels in RF.[43]Classification/ regression Convolutional neural networks (CNNs)• A deep unsupervised representation is applied to automatically learn the representation of amino acids.[88,89]Supervised learningClassificationCNNs• Three CNNs are used as a major engine of DeepEC for predicting EC numbers.[54]CNNs• CNNs infer relationships between different gene expression levels encoded in the image.[61]Recurrent neural networks (RNNs)and CNNs• Supervised method is employed to extract time-related features of the gene regulatory network (GRN).[22]RF• Random forest as a machine-directed evolution strategy is compared with experimental-directed evolution.[89]CNNs• The architecture of MutCompute consists of nine layers divided into two blocks: (1) feature extraction and (2) classification.[93]Regression Support vector regression (SVR)• SVR is trained to predict the presence or absence of each molecular property for the unknown compound.[99]Artificial neural networks (ANNs)•1000 ANNs models are trained with random initial weights to avoid overfitting.[14]Support vector machine (SVM)• SVM is applied to explore the relationship between genotype and phenotype.[65]EVOLVE• EVOLVE is in conjunction with GEM[85]Residual neural network (ResNet)•10 ResNets, each consisting of three residual blocks of two convolutional layers, constitute SAPIENs, which could be explored in RBS activities and sequences.[92]Clustering/ regression RF • Network, structure, biochemistry, and assay conditions are training set to predict in vivo and in vitro data.[84]Unsupervised learning Classification Matrix factorization• Matrix factorization is used to complete the metabolite adjacency matrix.[58]
Recent trends in metabolic engineering of microbial chemical factories. Y Liu, J Nielsen, 10.1016/j.copbio.2019.05.010Curr Opin Biotechnol. 602019</p>
<p>Engineering cellular metabolism. J Nielsen, J D Keasling, 10.1016/J.CELL.2016.02.004Cell. 1642016</p>
<p>Highly efficient biosynthesis of astaxanthin in Saccharomyces cerevisiae by integration and tuning of algal crtZ and bkt. P Zhou, L Ye, W Xie, X Lv, H Yu, 10.1007/s00253-015-6791-yAppl Microbiol Biotechnol. 992015</p>
<p>De novo biosynthesis of rubusoside and rebaudiosides in engineered yeasts. Y Xu, X Wang, C Zhang, X Zhou, X Xu, L Han, 10.1038/s41467-022-30826-2Nat Commun. 1330402022</p>
<p>Rational, combinatorial, and genomic approaches for engineering L-tyrosine production in Escherichia coli. Cns Santos, Xiao W Stephanopoulos, G , 10.1073/pnas.1206346109Proc Natl Acad Sci. 1092012</p>
<p>Recent advances in machine learning applications in metabolic engineering. P Patra, Brd, P Kundu, M Das, A Ghosh, 10.1016/j.biotechadv.2022.108069Biotechnol Adv. 621080692023</p>
<p>Cell-free synthetic biology: Thinking outside the cell. C E Hodgman, M C Jewett, 10.1016/j.ymben.2011.09.002Metab Eng. 142012</p>
<p>. Fig, </p>
<p>Prospective tasks for machine learning technologies in metabolic pathway optimization. </p>
<p>Common principles and best practices for engineering microbiomes. C E Lawson, W R Harcombe, R Hatzenpichler, S R Lindemann, F E Löffler, O Malley, M A , 10.1038/s41579-019-0255-9Nat Rev Microbiol. 172019</p>
<p>Publisher Correction: Morphology and mechanics of fungal mycelium. M R Islam, G Tudryn, R Bucinell, L Schadler, R C Picu, 10.1038/s41598-018-20637-1Sci Rep. 842062018</p>
<p>Ecosystem engineering in space and time. A Hastings, J E Byers, J A Crooks, K Cuddington, C G Jones, J G Lambrinos, 10.1111/j.1461-0248.2006.00997.xEcol Lett. 102007</p>
<p>Machine-directed evolution of an imine reductase for activity and stereoselectivity. E J Ma, E Siirola, C Moore, A Kummer, M Stoeckli, M Faller, 10.1021/acscatal.1c02786ACS Catal. 112021</p>
<p>Comparing deep learning and support vector machines for autonomous waste sorting. G E Sakr, M Mokbel, A Darwich, M N Khneisser, A Hadi, 10.1109/IMCET.2016.7777453IEEE International Multidisciplinary Conference on Engineering Technology (IMCET). 2016. 2016IEEE</p>
<p>Machine learning predicts new anti-CRISPR proteins. S Eitzinger, A Asif, K E Watters, A T Iavarone, G J Knott, J A Doudna, 10.1093/nar/gkaa219Nucleic Acids Res. 482020</p>
<p>an efficient machine-learning workflow in conjunction with the YeastFab assembly strategy for combinatorial optimization of heterologous metabolic pathways in Saccharomyces cerevisiae. Y Zhou, G Li, J Dong, X Xing, J Dai, C Zhang, Miya, 10.1016/j.ymben.2018.03.020Metab Eng. 472018</p>
<p>Machine learning-guided acyl-ACP reductase engineering for improved in vivo fatty alcohol production. J C Greenhalgh, S A Fahlberg, B F Pfleger, P A Romero, 10.1038/s41467-021-25831-wNat Commun. 1258252021</p>
<p>Next-generation machine learning for biological networks. D M Camacho, K M Collins, R K Powers, J C Costello, J J Collins, 10.1016/j.cell.2018.05.015Cell. 1732018</p>
<p>Recent advances on constraint-based models by integrating machine learning. P Rana, C Berry, P Ghosh, S S Fong, 10.1016/j.copbio.2019.11.007Curr Opin Biotechnol. 642020</p>
<p>Reconstructing organisms in silico: genome-scale models and their emerging applications. X Fang, C J Lloyd, B O Palsson, 10.1038/s41579-020-00440-4Nat Rev Microbiol. 182020</p>
<p>What is flux balance analysis. J D Orth, I Thiele, B Ø Palsson, 10.1038/nbt.1614Nat Biotechnol. 282010</p>
<p>Machine and deep learning meet genome-scale metabolic modeling. G Zampieri, S Vijayakumar, E Yaneske, C Angione, 10.1371/journal.pcbi.1007084PLoS Comput Biol. 15e10070842019</p>
<p>Improving the phenotype predictions of a yeast genome-scale metabolic model by incorporating enzymatic constraints. B J Sánchez, C Zhang, A Nilsson, P Lahtvee, E J Kerkhoven, J Nielsen, 10.15252/msb.20167411Mol Syst Biol. 139352017</p>
<p>Multiscale models quantifying yeast physiology: towards a whole-cell model. H Lu, E J Kerkhoven, J Nielsen, 10.1016/j.tibtech.2021.06.010Trends Biotechnol. 402022</p>
<p>Current status and applications of genome-scale metabolic models. C Gu, G B Kim, W J Kim, H U Kim, Lee Sy, 10.1186/s13059-019-1730-3Genome Biol. 201212019</p>
<p>Analyses of extracellular protein production in Bacillus subtilis -I: Genome-scale metabolic model reconstruction based on updated gene-enzyme-reaction data. P Kocabaş, P Çalık, G Çalık, T H Özdamar, 10.1016/j.bej.2017.07.005Biochem Eng J. 1272017</p>
<p>High-Quality Genome-Scale Reconstruction of Corynebacterium glutamicum ATCC 13032. M Feierabend, A Renz, E Zelle, K Nöh, W Wiechert, A Dräger, 10.3389/fmicb.2021.750206Front Microbiol. 122021</p>
<p>A new genome-scale metabolic model of Corynebacterium glutamicum and its application. Y Zhang, J Cai, X Shang, B Wang, S Liu, X Chai, 10.1186/s13068-017-0856-3Biotechnol Biofuels. 101692017</p>
<p>From zero to hero-Design-based systems metabolic engineering of Corynebacterium glutamicum for l-lysine production. J Becker, O Zelder, S Häfner, H Schröder, C Wittmann, 10.1016/j.ymben.2011.01.003Metab Eng. 132011</p>
<p>Improving NADPH availability for natural product biosynthesis in Escherichia coli by metabolic engineering. J A Chemler, Z L Fowler, K P Mchugh, Mag Koffas, 10.1016/j.ymben.2009.07.003Metab Eng. 122010</p>
<p>Metabolic engineering of Escherichia coli for the production of cadaverine: A five carbon diamine. Z-G Qian, X-X Xia, Lee Sy, 10.1002/bit.22918Biotechnol Bioeng. 1082011</p>
<p>Machine learning applications in systems metabolic engineering. G B Kim, W J Kim, H U Kim, Lee Sy, 10.1016/j.copbio.2019.08.010Curr Opin Biotechnol. 642020</p>
<p>Machine learning for metabolic engineering: A review. C E Lawson, J M Martí, T Radivojevic, Svr Jonnalagadda, R Gentz, N J Hillson, 10.1016/j.ymben.2020.10.005Metab Eng. 632021</p>
<p>A protocol for generating a high-quality genome-scale metabolic reconstruction. I Thiele, B Ø Palsson, 10.1038/nprot.2009.203Nat Protoc. 52010</p>
<p>DeepRibo: a neural network for precise gene annotation of prokaryotes by combining ribosome profiling signal and binding site patterns. J Clauwaert, G Menschaert, W Waegeman, 10.1093/nar/gkz061Nucleic Acids Res. 47e362019</p>
<p>Deep learning enables high-quality and highthroughput prediction of enzyme commission numbers. J Y Ryu, H U Kim, S Y Lee, 10.1073/pnas.1821905116Proceedings of the National Academy of Sciences. 1162019</p>
<p>Advances in gap-filling genome-scale metabolic models and model-driven experiments lead to novel metabolic discoveries. S Pan, J L Reed, 10.1016/j.copbio.2017.12.012Curr Opin Biotechnol. 512018</p>
<p>Machine learning methods for metabolic pathway prediction. J M Dale, L Popescu, P D Karp, 10.1186/1471-2105-11-15BMC Bioinforma. 11152010</p>
<p>Fast automated reconstruction of genome-scale metabolic models for microbial species and communities. D Machado, S Andrejev, M Tramontano, K R Patil, 10.1093/nar/gky537Nucleic Acids Res. 462018</p>
<p>Reconstr High-Qual Large-Scale Metab Models merlin. O Dias, M Rocha, E C Ferreira, I Rocha, 10.1007/978-1-4939-7528-0_12018</p>
<p>Highthroughput generation, optimization and analysis of genome-scale metabolic models. C S Henry, M Dejongh, A A Best, P M Frybarger, B Linsay, R L Stevens, 10.1038/nbt.1672Nat Biotechnol. 282010</p>
<p>Pathway Tools version 19.0 update: software for pathway/genome informatics and systems biology. P D Karp, M Latendresse, S M Paley, M Krummenacker, Q D Ong, R Billington, 10.1093/bib/bbv079Brief Bioinform. 172016</p>
<p>gapseq: informed prediction of bacterial metabolic pathways and reconstruction of accurate metabolic models. J Zimmermann, C Kaleta, S Waschina, 10.1186/s13059-021-02295-1Genome Biol. 22812021</p>
<p>Fast automated reconstruction of genome-scale metabolic models for microbial species and communities. D Machado, S Andrejev, M Tramontano, K R Patil, 10.1093/nar/gky537Nucleic Acids Res. 462018</p>
<p>Managing uncertainty in metabolic network structure and improving predictions using EnsembleFBA. M B Biggs, J A Papin, 10.1371/journal.pcbi.1005413PLoS Comput Biol. 13e10054132017</p>
<p>A genome-scale metabolic model of Saccharomyces cerevisiae that integrates expression constraints and reaction thermodynamics. O Oftadeh, P Salvy, M Masid, M Curvat, L Miskovic, V Hatzimanikatis, 10.1038/s41467-021-25158-6Nat Commun. 1247902021</p>
<p>A novel yeast hybrid modeling framework integrating Boolean and enzyme-constrained networks enables exploration of the interplay between signaling and metabolism. L Österberg, I Domenzain, J Münch, J Nielsen, S Hohmann, M Cvijovic, 10.1371/journal.pcbi.1008891PLoS Comput Biol. 17e10088912021</p>
<p>Combining inferred regulatory and reconstructed metabolic networks enhances phenotype prediction in yeast. Z Wang, S A Danziger, B D Heavner, S Ma, J J Smith, S Li, 10.1371/journal.pcbi.1005489PLoS Comput Biol. 13e10054892017</p>
<p>Competitive fragmentation modeling of ESI-MS/ MS spectra for putative metabolite identification. F Allen, R Greiner, D Wishart, 10.1007/s11306-014-0676-4Metabolomics. 112015</p>
<p>Recent advances and prospects of computational methods for metabolite identification: a review with emphasis on machine learning approaches. D H Nguyen, C H Nguyen, H Mamitsuka, 10.1093/bib/bby066Brief Bioinform. 202019</p>
<p>Construction of multiscale genome-scale metabolic models: frameworks and challenges. X Bi, Y Liu, J Li, G Du, X Lv, L Liu, 10.3390/biom12050721Biomolecules. 127212022</p>
<p>Machine learning applied to enzyme turnover numbers reveals protein structural correlates and improves metabolic models. D Heckmann, C J Lloyd, N Mih, Y Ha, D C Zielinski, Z B Haiman, 10.1038/s41467-018-07652-6Nat Commun. 952522018</p>
<p>Mathematical modeling of proteome constraints within metabolism. Y Chen, J Nielsen, 10.1016/j.coisb.2021.03.003Curr Opin Syst Biol. 252021</p>
<p>Metabolic models of protein allocation call for the kinetome. A Nilsson, J Nielsen, B O Palsson, 10.1016/j.cels.2017.11.013Cell Syst. 52017</p>
<p>Global characterization of in vivo enzyme catalytic rates and their correspondence to in vitro k cat measurements. D Davidi, E Noor, W Liebermeister, A Bar-Even, A Flamholz, K Tummler, 10.1073/pnas.1514240113Proc Natl Acad Sci. 1132016</p>
<p>Kinetic profiling of metabolic specialists demonstrates stability and consistency of in vivo enzyme turnover numbers. D Heckmann, A Campeau, C J Lloyd, P V Phaneuf, Y Hefner, M Carrillo-Terrazas, 10.1073/pnas.2001562117Proc Natl Acad Sci. 1172020</p>
<p>Deep learning-based kcat prediction enables improved enzyme-constrained model reconstruction. F Li, L Yuan, H Lu, G Li, Y Chen, Mkm Engqvist, 10.1038/s41929-022-00798-zNat Catal. 52022</p>
<p>eQuilibrator-the biochemical thermodynamics calculator. A Flamholz, E Noor, A Bar-Even, Milo R , 10.1093/nar/gkr874Nucleic Acids Res. 402012</p>
<p>Metabolite identification and molecular fingerprint prediction through machine learning. M Heinonen, H Shen, N Zamboni, J Rousu, 10.1093/bioinformatics/bts437Bioinformatics. 282012</p>
<p>Searching molecular structure databases with tandem mass spectra using CSI:FingerID. K Dührkop, H Shen, M Meusel, J Rousu, S Böcker, 10.1073/pnas.1509788112Proc Natl Acad Sci. 1122015</p>
<p>Sparse Interaction Model over Peaks of moLEcules for fast, interpretable metabolite identification from tandem mass spectra. D H Nguyen, C H Nguyen, H Mamitsuka, Simple, 10.1093/bioinformatics/bty252Bioinformatics. 342018</p>
<p>. Y Cheng, X Bi, Y Xu, Computational and Structural Biotechnology Journal. 212023</p>
<p>Modeling regulatory networks using machine learning for systems metabolic engineering. M S Kwon, B T Lee, S Y Lee, Kim Hu, 10.1016/j.copbio.2020.02.014Curr Opin Biotechnol. 652020</p>
<p>A hybrid deep learning framework for gene regulatory network inference from single-cell transcriptomic data. M Zhao, W He, J Tang, Q Zou, F Guo, 10.1093/bib/bbab568Brief Bioinform. 202223</p>
<p>Deep learning for inferring gene relationships from single-cell expression data. Y Yuan, Z Bar-Joseph, 10.1073/pnas.1911536116Proc Natl Acad Sci. 1162019</p>
<p>Chemical genomic guided engineering of gamma-valerolactone tolerant yeast. S Bottoms, Q Dickinson, M Mcgee, L Hinchman, A Higbee, A Hebert, 10.1186/s12934-017-0848-9Micro Cell Fact. 1752018</p>
<p>Dissecting a complex chemical stress: chemogenomic profiling of plant hydrolysates. J M Skerker, D Leon, M N Price, J S Mar, D R Tarjan, K M Wetmore, 10.1038/msb.2013.30Mol Syst Biol. 96742013</p>
<p>Active and machine learning-based approaches to rapidly enhance microbial chemical production. P Kumar, P A Adamczyk, X Zhang, R B Andrade, P A Romero, P Ramanathan, 10.1016/j.ymben.2021.06.009Metab Eng. 672021</p>
<p>Probabilistic integrative modeling of genomescale metabolic and regulatory networks in Escherichia coli and Mycobacterium tuberculosis. S Chandrasekaran, N D Price, 10.1073/pnas.1005139107Proc Natl Acad Sci. 1072010</p>
<p>Interpretable machine learning to model biomass and waste gasification. S Ascher, X Wang, I Watson, W Sloan, S You, 10.1016/j.biortech.2022.128062Bioresour Technol. 3641280622022</p>
<p>Construction, characterization and application of a genome-wide promoter library in Saccharomyces cerevisiae. T Yuan, Y Guo, J Dong, T Li, T Zhou, K Sun, 10.1007/s11705-017-1621-7Front Chem Sci Eng. 112017</p>
<p>Lessons from Two Design-Build-Test-Learn Cycles of Dodecanol Production in Escherichia coli Aided by Machine Learning. P Opgenorth, Z Costello, T Okada, G Goyal, Y Chen, J Gin, 10.1021/acssynbio.9b00020ACS Synth Biol. 82019</p>
<p>A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise. H J Kushner, 10.1115/1.3653121J Basic Eng. 861964</p>
<p>Towards a fully automated algorithm driven platform for biosystems design. M Hamedirad, R Chao, S Weisberg, J Lian, S Sinha, H Zhao, 10.1038/s41467-019-13189-zNat Commun. 1051502019</p>
<p>A machine learning Automated Recommendation Tool for synthetic biology. T Radivojević, Z Costello, K Workman, Garcia Martin, H , 10.1038/s41467-020-18008-4Nat Commun. 1148792020</p>
<p>Protein engineering via Bayesian optimization-guided evolutionary algorithm and robotic experiments. R Hu, L Fu, Y Chen, J Chen, Y Qiao, T Si, 10.1093/bib/bbac570Brief Bioinform. 202324</p>
<p>Machine learning in enzyme engineering. S Mazurenko, Z Prokop, J Damborsky, 10.1021/acscatal.9b04321ACS Catal. 102020</p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, 10.1038/s41586-021-03819-2Nature. 5962021</p>
<p>Machine-learning-guided directed evolution for protein engineering. K K Yang, Z Wu, F H Arnold, 10.1038/s41592-019-0496-6Nat Methods. 162019</p>
<p>Predicting the structure of large protein complexes using AlphaFold and Monte Carlo tree search. P Bryant, G Pozzati, W Zhu, A Shenoy, P Kundrotas, A Elofsson, 10.1038/s41467-022-33729-4Nat Commun. 1360282022</p>
<p>Computational enzyme redesign: large jumps in function. Y Cui, J Sun, B Wu, 10.1016/j.trechm.2022.03.001Trends Chem. 42022</p>
<p>The road to fully programmable protein catalysis. S L Lovelock, R Crawshaw, S Basler, C Levy, D Baker, D Hilvert, 10.1038/s41586-022-04456-zNature. 6062022</p>
<p>Machine learning-assisted directed protein evolution with combinatorial libraries. Z Wu, Sbj Kan, R D Lewis, B J Wittmann, F H Arnold, 10.1073/pnas.1901979116Proc Natl Acad Sci. 1162019</p>
<p>Highresolution mapping of protein sequence-function relationships. D M Fowler, C L Araya, S J Fleishman, E H Kellogg, J J Stephany, D Baker, 10.1038/nmeth.1492Nat Methods. 72010</p>
<p>Enzyme engineering: reaching the maximal catalytic efficiency peak. M Goldsmith, D S Tawfik, 10.1016/j.sbi.2017.09.002Curr Opin Struct Biol. 472017</p>
<p>Exploring protein fitness landscapes by directed evolution. P A Romero, F H Arnold, 10.1038/nrm2805Nat Rev Mol Cell Biol. 102009</p>
<p>Large scale active-learning-guided exploration for in vitro protein production optimization. O Borkowski, M Koch, A Zettor, A Pandi, A C Batista, P Soudier, 10.1038/s41467-020-15798-5Nat Commun. 1118722020</p>
<p>Informed training set design enables efficient machine learning-assisted directed protein evolution. B J Wittmann, Y Yue, F H Arnold, 10.1016/j.cels.2021.07.008Cell Syst. 122021</p>
<p>Navigating the protein fitness landscape with Gaussian processes. P A Romero, A Krause, F H Arnold, 10.1073/pnas.1215251110Proc Natl Acad Sci. 1102013</p>
<p>Adaptation in protein fitness landscapes is facilitated by indirect paths. N C Wu, L Dai, C A Olson, Lloyd - Smith, J O Sun, R , 10.7554/eLife.1696520165</p>
<p>Interpretable numerical descriptors of amino acid space. A G Georgiev, 10.1089/cmb.2008.0173J Comput Biol. 162009</p>
<p>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. A Rives, J Meier, T Sercu, S Goyal, Z Lin, J Liu, 10.1073/pnas.2016239118Proc Natl Acad Sci. 1182021</p>
<p>Low-N protein engineering with data-efficient deep learning. S Biswas, G Khimulya, E C Alley, K M Esvelt, G M Church, 10.1038/s41592-021-01100-yNat Methods. 182021</p>
<p>SCHEMA-Guide Protein Recomb. J J Silberg, J B Endelman, F H Arnold, S0076-6879(04)88004-22004</p>
<p>Programmable cross-ribosome-binding sites to fine-tune the dynamic range of transcription factorbased biosensor. N Ding, Z Yuan, X Zhang, J Chen, S Zhou, Y Deng, 10.1093/nar/gkaa786Nucleic Acids Res. 482020</p>
<p>Large-scale DNA-based phenotypic recording and deep learning enable highly accurate sequence-function mapping. S Höllerer, L Papaxanthos, A C Gumpinger, K Fischer, C Beisel, K Borgwardt, 10.1038/s41467-020-17222-4Nat Commun. 202011</p>
<p>Heuristic Discovery and Design of Promoter Collections in Non-Model Microbes for Industrial Applications. J Gilman, C Singleton, R K Tennant, P James, T P Howard, T Lux, 10.1021/acssynbio.9b00061ACS Synth Biol. 82019</p>
<p>Precise Prediction of Promoter Strength Based on a De Novo Synthetic Promoter Library Coupled with Machine Learning. M Zhao, Z Yuan, L Wu, S Zhou, Y Deng, 10.1021/acssynbio.1c00117ACS Synth Biol. 112022</p>
<p>T Chen, C X G Guestrin, Boost, 10.1145/2939672.2939785Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningNew York, NY, USAACM2016</p>
<p>Generative adversarial networks: an overview. A Creswell, T White, V Dumoulin, K Arulkumaran, B Sengupta, A A Bharath, 10.1109/MSP.2017.2765202IEEE Signal Process Mag. 352018</p>
<p>Synthetic promoter design in Escherichia coli based on a deep generative network. Y Wang, H Wang, L Wei, S Li, L Liu, X Wang, 10.1093/nar/gkaa325Nucleic Acids Res. 482020</p>
<p>De novo design of programmable inducible promoters. X Liu, Stp Gupta, D Bhimsaria, J L Reed, J A Rodríguez-Martínez, A Z Ansari, 10.1093/nar/gkz772Nucleic Acids Res. 472019</p>
<p>Tuning the performance of synthetic riboswitches using machine learning. A-C Groher, S Jager, C Schneider, F Groher, K Hamacher, B Suess, 10.1021/acssynbio.8b00207ACS Synth Biol. 82019</p>
<p>Machine learning guided batched design of a bacterial ribosome binding site. M Zhang, M B Holowko, H Hayman Zumpe, C S Ong, 10.1021/acssynbio.2c00015ACS Synth Biol. 112022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4640 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4640</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4640</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-267211602</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.13996v1.pdf" target="_blank">Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution</a></p>
                <p><strong>Paper Abstract:</strong> This paper introduces Investigate-Consolidate-Exploit (ICE), a novel strategy for enhancing the adaptability and flexibility of AI agents through inter-task self-evolution. Unlike existing methods focused on intra-task learning, ICE promotes the transfer of knowledge between tasks for genuine self-evolution, similar to human experience learning. The strategy dynamically investigates planning and execution trajectories, consolidates them into simplified workflows and pipelines, and exploits them for improved task execution. Our experiments on the XAgent framework demonstrate ICE's effectiveness, reducing API calls by as much as 80% and significantly decreasing the demand for the model's capability. Specifically, when combined with GPT-3.5, ICE's performance matches that of raw GPT-4 across various agent tasks. We argue that this self-evolution approach represents a paradigm shift in agent design, contributing to a more robust AI community and ecosystem, and moving a step closer to full autonomy.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4640.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4640.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Investigate-Consolidate-Exploit</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-stage strategy for inter-task agent self-evolution that (1) investigates and records planning and execution traces, (2) consolidates them into reusable workflows and finite-automaton pipelines, and (3) exploits those artifacts from external memory to improve efficiency and effectiveness on new tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>XAgent (with ICE applied)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A strategy plugged into the XAgent framework which separates planning and execution via two agent experts; uses LLM(s) to record, prune and consolidate plans into linear workflows and ReACT execution traces into executable pipelines, then retrieves those artifacts at runtime to guide planning and/or directly automate execution.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval-augmented vector memory (episodic/workflow memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Consolidated workflows (linearized successful plan sequences) and pipelines (execution trajectories transformed into finite-automaton JSON) are stored in a Pinecone vector database; keys are goal descriptions or subgoal+milestone descriptions, embedded via OpenAI text-embedding-ada-002, and retrieved by cosine similarity during new-task planning/execution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-step tool-invocation agent tasks (custom 40-task suite)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A manually created set of 40 diverse tool-heavy tasks (e.g., trip planning, data analysis, review writing) split into 20 training tasks used to populate memory and 20 testing tasks used to evaluate inter-task self-evolution; tasks require hierarchical planning and sequences of tool API calls (ReACT style).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Custom 40-task tool-invocation dataset (20 train / 20 test)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported: up to 80% reduction in total model API calls; re-utilization rate of mined pipelines ≈ 50%; overall higher subtask completion rate and fewer plan rectifications when ICE applied. Also reported: when GPT-3.5 is used during the EXPLOIT stage with ICE, performance rivals raw GPT-4 across the evaluated agent tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Standard XAgent baseline (no ICE) had substantially higher API calls and lower completion rates; precise baseline numeric values are not fully legible in the paper, but comparisons indicate large gains when memory is used (text: 'reduce model API calls by up to 80%').</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Execution-ICE (pipelines) is the primary contributor to API-call reduction; combining Planning-ICE (workflows for in-context plan references) and Execution-ICE yields the best results. Ablation (varying number of stored tasks) shows more stored experiences increase subtask completion and reduce API calls; pipeline re-utilization from training → testing is ~50%.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>ICE primarily consolidates successful experiences; failed experiences are not directly consolidated and are reported as challenging to transform into reusable artifacts. Retrieval relies on embedding similarity with a threshold (mismatch causes fallback to standard ReACT), pipeline applicability is imperfect (~50% reuse rate), consolidation depends on GPT-4 for transforming trajectories, and pipelines may not align perfectly with new subgoal semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Disentangling planning and execution traces enables different forms of memory (workflows for in-context plan guidance and pipelines for direct execution). Consolidated execution pipelines drastically reduce runtime API calls and cost, allowing lower-capability models (e.g., GPT-3.5) to match higher-capability models (GPT-4) during exploitation. Scaling memory size improves generalization and efficiency; failed experiences require special handling and remain an open challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4640.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4640.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExecutionICE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Execution Investigate-Consolidate-Exploit (Execution ICE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Subcomponent of ICE that records successful ReACT execution trajectories, prunes and augments them into finite-automaton pipelines, and retrieves/executes those pipelines automatically for similar subgoals in future tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>XAgent execution expert (ReACT-based)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Records stepwise tool-invocation trajectories (thought, tool, inputs, outputs), consolidates fixed successful sequences into automaton-style pipelines with switching/error-handling logic, and permits automated replay or guided parameter filling during new-task execution.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic pipeline memory stored as vectorized JSON (vector DB)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Pipelines (A_Gx) are stored as JSON describing nodes (tool calls) and edges (transition rules/comments). Each pipeline is keyed on subgoal description + milestones, embedded and placed in Pinecone for cosine-similarity retrieval; if similarity passes threshold, the pipeline can be executed as a finite automaton, reducing LLM tool-invocation calls.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Low-level subgoal execution within multi-step agent tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Automating the concrete tool-invocation steps required to satisfy leaf subgoals in a plan (e.g., fetching product details, searching climate news, writing files), replacing or constraining open-ended ReACT generation with deterministic pipeline steps.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Custom 40-task tool-invocation dataset (20 train / 20 test)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Attributed as the primary source of the reported up-to-80% reduction in API calls; pipelines enabled significant tool-call savings and lower rectification times. Reported pipeline re-utilization rate ≈ 50% from training to testing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Without pipelines, execution follows open-ended ReACT trajectories requiring many more model API calls and higher cost; exact numeric baselines in tables are partially unreadable but described as substantially worse.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Execution memory (pipelines) yields larger API-call and cost reductions than planning memory alone; combining both gives best overall performance. Ablation shows storing more execution experiences improves re-utilization and completion rates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Pipelines are derived from successful traces, so failed traces are excluded; pipelines may require pruning and addition of switching logic to be robust; retrieval depends on similarity threshold—if retrieval fails, system falls back to ReACT; some pipelines only partially match new subgoals reducing direct applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Direct re-use of execution traces as constrained automata is more efficient than retrieval-augmented in-context repetition of full trajectories, dramatically saving API calls. Adding explicit switching/error-handling logic during consolidation increases pipeline robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4640.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4640.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PlanningICE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Planning Investigate-Consolidate-Exploit (Planning ICE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Subcomponent of ICE that tracks dynamic plan generation/rectification, prunes unsuccessful branches and linearizes successful subgoal sequences into workflows stored in memory for later in-context retrieval to guide new plan generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>XAgent planning expert (tree-structured planner)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Monitors generation and rectification of hierarchical plan trees (root goal → subgoals → leaf subgoals), prunes failed subgoals and transforms successful subtrees into linearized workflows used as in-context examples during planning for new tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>workflow memory for in-context retrieval (vector DB)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Workflows (W_Gx) are stored keyed by goal descriptions with full linearized sequences of successfully achieved subgoals; embeddings stored in Pinecone, retrieved by cosine similarity and included in the LLM context to reduce rectifications and improve plan quality.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>High-level hierarchical planning for multi-step agent tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decomposing user goals into subgoals and sub-subgoals (tree structure) and generating plans whose leaf subgoals will be executed; workflows provide templates for decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Custom 40-task tool-invocation dataset (20 train / 20 test)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Using planning workflows as in-context references reduced the number of plan rectifications and improved subtask completion rates relative to no-planning-memory baselines (quantitative improvements described but exact table numbers partially unreadable).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Standard XAgent planning without stored workflows required more rectifications and produced lower completion rates (baseline results described qualitatively in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Planning ICE alone improves plan rationality and reduces rectifications; combined with Execution ICE yields additive benefits. Planning ICE provides flexible guidance but still relies on the model's ability to adapt retrieved workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Workflows are more flexible but require the backbone model's attention and adaptation; high-level goal mismatch can make direct reuse difficult; retrieval similarity threshold and embedding quality affect applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Workflows are effective as in-context examples for plan generation and rectification; separating planning memory (workflows) from execution memory (pipelines) enables different re-use modalities (flexible in-context guidance vs. direct automated execution).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4640.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4640.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XAgent (autonomous agent framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent framework that disentangles planning and execution by using two agent experts (planner and executor) and supports tool invocation; used as the experimental platform for evaluating ICE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Xagent: An autonomous agent for complex task solving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>XAgent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Framework separating planning (tree-structured decomposition) and execution (ReACT trajectories) to maximize the flexibility of storing and reusing planning and execution experiences; provides instrumented traces suitable for ICE's INVESTIGATE/CONSOLIDATE stages.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>none by default; in this paper XAgent is augmented with an external vector memory (Pinecone) for ICE</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>When combined with ICE, XAgent stores consolidated workflows and pipelines in an external vector DB keyed by goal/subgoal descriptions for later retrieval and reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex multi-step tasks requiring planning and multi-tool execution</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmark tasks in this study involve hierarchical planning plus sequences of tool calls (handled by XAgent's executor) across domains like trip planning, finance, and writing.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Custom 40-task tool-invocation dataset (20 train / 20 test)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Not applicable to XAgent alone; with ICE applied within XAgent the paper reports large reductions in API calls and improved completion rates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Standard XAgent (without ICE) serves as the baseline; paper reports it uses significantly more API calls and has lower completion rates than ICE-augmented variants.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>XAgent + ICE (both planning and execution memories) outperforms standard XAgent (no memory) on API-cost and task effectiveness metrics; execution memory yields the largest gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>XAgent's vanilla design mixes planning and execution, which reduces re-usability; ICE requires instrumentation and separate experts (planner/executor) to realize full benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Agent architectures that explicitly separate planning and execution are better-suited for inter-task experiential memory and for automated consolidation into reusable artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4640.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4640.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PineconeDB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pinecone vector database</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An external vector database used as the agent's memory store to index and retrieve embeddings of consolidated workflows and pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Pinecone-backed retrieval for ICE</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Stores the keys (goal/subgoal descriptions and milestones) and the values (workflow JSON or pipeline JSON) as vectors for similarity-based retrieval during the EXPLOIT stage.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external vector retrieval store</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Keys are embedded using OpenAI text-embedding-ada-002; retrievals are by cosine similarity with a configured threshold; retrieved artifacts (workflows/pipelines) are used either as in-context references or executed automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Indexing and retrieving consolidated experiences for multi-step agent tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to persist the consolidated artifacts mined from training tasks and to supply them during testing tasks for inter-task self-evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Custom 40-task tool-invocation dataset (20 train / 20 test)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Enables the reported up-to-80% API-call reductions and ≈50% pipeline re-utilization; effectiveness depends on embedding quality and retrieval threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not applicable; without a vector DB, ICE cannot perform similarity-based retrieval and would fall back to pure ReACT or in-context-only strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Vector-store-backed retrieval is central to ICE; embedding + cosine similarity retrieval enables matching of workflows/pipelines to new goals, but retrieval thresholds and embedding model choice affect applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Embedding quality, selection of similarity threshold, and semantically imperfect matches limit reuse; reliance on an external service introduces engineering and privacy considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>A vector DB with goal/subgoal keyed embeddings is an effective mechanism for storing and retrieving inter-task experiences, enabling automated execution pipelines and in-context workflow reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Xagent: An autonomous agent for complex task solving <em>(Rating: 2)</em></li>
                <li>Memorybank: Enhancing large language models with longterm memory <em>(Rating: 2)</em></li>
                <li>Think-in-memory: Recalling and postthinking enable llms with long-term memory <em>(Rating: 2)</em></li>
                <li>Chatdb, Augmenting llms with databases as their symbolic memory <em>(Rating: 1)</em></li>
                <li>Expel: Llm agents are experiential learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4640",
    "paper_id": "paper-267211602",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "ICE",
            "name_full": "Investigate-Consolidate-Exploit",
            "brief_description": "A three-stage strategy for inter-task agent self-evolution that (1) investigates and records planning and execution traces, (2) consolidates them into reusable workflows and finite-automaton pipelines, and (3) exploits those artifacts from external memory to improve efficiency and effectiveness on new tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "XAgent (with ICE applied)",
            "agent_description": "A strategy plugged into the XAgent framework which separates planning and execution via two agent experts; uses LLM(s) to record, prune and consolidate plans into linear workflows and ReACT execution traces into executable pipelines, then retrieves those artifacts at runtime to guide planning and/or directly automate execution.",
            "memory_type": "external retrieval-augmented vector memory (episodic/workflow memory)",
            "memory_description": "Consolidated workflows (linearized successful plan sequences) and pipelines (execution trajectories transformed into finite-automaton JSON) are stored in a Pinecone vector database; keys are goal descriptions or subgoal+milestone descriptions, embedded via OpenAI text-embedding-ada-002, and retrieved by cosine similarity during new-task planning/execution.",
            "task_name": "Multi-step tool-invocation agent tasks (custom 40-task suite)",
            "task_description": "A manually created set of 40 diverse tool-heavy tasks (e.g., trip planning, data analysis, review writing) split into 20 training tasks used to populate memory and 20 testing tasks used to evaluate inter-task self-evolution; tasks require hierarchical planning and sequences of tool API calls (ReACT style).",
            "benchmark_name": "Custom 40-task tool-invocation dataset (20 train / 20 test)",
            "performance_with_memory": "Reported: up to 80% reduction in total model API calls; re-utilization rate of mined pipelines ≈ 50%; overall higher subtask completion rate and fewer plan rectifications when ICE applied. Also reported: when GPT-3.5 is used during the EXPLOIT stage with ICE, performance rivals raw GPT-4 across the evaluated agent tasks.",
            "performance_without_memory": "Standard XAgent baseline (no ICE) had substantially higher API calls and lower completion rates; precise baseline numeric values are not fully legible in the paper, but comparisons indicate large gains when memory is used (text: 'reduce model API calls by up to 80%').",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Execution-ICE (pipelines) is the primary contributor to API-call reduction; combining Planning-ICE (workflows for in-context plan references) and Execution-ICE yields the best results. Ablation (varying number of stored tasks) shows more stored experiences increase subtask completion and reduce API calls; pipeline re-utilization from training → testing is ~50%.",
            "limitations_or_challenges": "ICE primarily consolidates successful experiences; failed experiences are not directly consolidated and are reported as challenging to transform into reusable artifacts. Retrieval relies on embedding similarity with a threshold (mismatch causes fallback to standard ReACT), pipeline applicability is imperfect (~50% reuse rate), consolidation depends on GPT-4 for transforming trajectories, and pipelines may not align perfectly with new subgoal semantics.",
            "key_insights": "Disentangling planning and execution traces enables different forms of memory (workflows for in-context plan guidance and pipelines for direct execution). Consolidated execution pipelines drastically reduce runtime API calls and cost, allowing lower-capability models (e.g., GPT-3.5) to match higher-capability models (GPT-4) during exploitation. Scaling memory size improves generalization and efficiency; failed experiences require special handling and remain an open challenge.",
            "uuid": "e4640.0",
            "source_info": {
                "paper_title": "Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "ExecutionICE",
            "name_full": "Execution Investigate-Consolidate-Exploit (Execution ICE)",
            "brief_description": "Subcomponent of ICE that records successful ReACT execution trajectories, prunes and augments them into finite-automaton pipelines, and retrieves/executes those pipelines automatically for similar subgoals in future tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "XAgent execution expert (ReACT-based)",
            "agent_description": "Records stepwise tool-invocation trajectories (thought, tool, inputs, outputs), consolidates fixed successful sequences into automaton-style pipelines with switching/error-handling logic, and permits automated replay or guided parameter filling during new-task execution.",
            "memory_type": "episodic pipeline memory stored as vectorized JSON (vector DB)",
            "memory_description": "Pipelines (A_Gx) are stored as JSON describing nodes (tool calls) and edges (transition rules/comments). Each pipeline is keyed on subgoal description + milestones, embedded and placed in Pinecone for cosine-similarity retrieval; if similarity passes threshold, the pipeline can be executed as a finite automaton, reducing LLM tool-invocation calls.",
            "task_name": "Low-level subgoal execution within multi-step agent tasks",
            "task_description": "Automating the concrete tool-invocation steps required to satisfy leaf subgoals in a plan (e.g., fetching product details, searching climate news, writing files), replacing or constraining open-ended ReACT generation with deterministic pipeline steps.",
            "benchmark_name": "Custom 40-task tool-invocation dataset (20 train / 20 test)",
            "performance_with_memory": "Attributed as the primary source of the reported up-to-80% reduction in API calls; pipelines enabled significant tool-call savings and lower rectification times. Reported pipeline re-utilization rate ≈ 50% from training to testing tasks.",
            "performance_without_memory": "Without pipelines, execution follows open-ended ReACT trajectories requiring many more model API calls and higher cost; exact numeric baselines in tables are partially unreadable but described as substantially worse.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Execution memory (pipelines) yields larger API-call and cost reductions than planning memory alone; combining both gives best overall performance. Ablation shows storing more execution experiences improves re-utilization and completion rates.",
            "limitations_or_challenges": "Pipelines are derived from successful traces, so failed traces are excluded; pipelines may require pruning and addition of switching logic to be robust; retrieval depends on similarity threshold—if retrieval fails, system falls back to ReACT; some pipelines only partially match new subgoals reducing direct applicability.",
            "key_insights": "Direct re-use of execution traces as constrained automata is more efficient than retrieval-augmented in-context repetition of full trajectories, dramatically saving API calls. Adding explicit switching/error-handling logic during consolidation increases pipeline robustness.",
            "uuid": "e4640.1",
            "source_info": {
                "paper_title": "Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "PlanningICE",
            "name_full": "Planning Investigate-Consolidate-Exploit (Planning ICE)",
            "brief_description": "Subcomponent of ICE that tracks dynamic plan generation/rectification, prunes unsuccessful branches and linearizes successful subgoal sequences into workflows stored in memory for later in-context retrieval to guide new plan generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "XAgent planning expert (tree-structured planner)",
            "agent_description": "Monitors generation and rectification of hierarchical plan trees (root goal → subgoals → leaf subgoals), prunes failed subgoals and transforms successful subtrees into linearized workflows used as in-context examples during planning for new tasks.",
            "memory_type": "workflow memory for in-context retrieval (vector DB)",
            "memory_description": "Workflows (W_Gx) are stored keyed by goal descriptions with full linearized sequences of successfully achieved subgoals; embeddings stored in Pinecone, retrieved by cosine similarity and included in the LLM context to reduce rectifications and improve plan quality.",
            "task_name": "High-level hierarchical planning for multi-step agent tasks",
            "task_description": "Decomposing user goals into subgoals and sub-subgoals (tree structure) and generating plans whose leaf subgoals will be executed; workflows provide templates for decomposition.",
            "benchmark_name": "Custom 40-task tool-invocation dataset (20 train / 20 test)",
            "performance_with_memory": "Using planning workflows as in-context references reduced the number of plan rectifications and improved subtask completion rates relative to no-planning-memory baselines (quantitative improvements described but exact table numbers partially unreadable).",
            "performance_without_memory": "Standard XAgent planning without stored workflows required more rectifications and produced lower completion rates (baseline results described qualitatively in paper).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Planning ICE alone improves plan rationality and reduces rectifications; combined with Execution ICE yields additive benefits. Planning ICE provides flexible guidance but still relies on the model's ability to adapt retrieved workflows.",
            "limitations_or_challenges": "Workflows are more flexible but require the backbone model's attention and adaptation; high-level goal mismatch can make direct reuse difficult; retrieval similarity threshold and embedding quality affect applicability.",
            "key_insights": "Workflows are effective as in-context examples for plan generation and rectification; separating planning memory (workflows) from execution memory (pipelines) enables different re-use modalities (flexible in-context guidance vs. direct automated execution).",
            "uuid": "e4640.2",
            "source_info": {
                "paper_title": "Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "XAgent",
            "name_full": "XAgent (autonomous agent framework)",
            "brief_description": "An agent framework that disentangles planning and execution by using two agent experts (planner and executor) and supports tool invocation; used as the experimental platform for evaluating ICE.",
            "citation_title": "Xagent: An autonomous agent for complex task solving",
            "mention_or_use": "use",
            "agent_name": "XAgent",
            "agent_description": "Framework separating planning (tree-structured decomposition) and execution (ReACT trajectories) to maximize the flexibility of storing and reusing planning and execution experiences; provides instrumented traces suitable for ICE's INVESTIGATE/CONSOLIDATE stages.",
            "memory_type": "none by default; in this paper XAgent is augmented with an external vector memory (Pinecone) for ICE",
            "memory_description": "When combined with ICE, XAgent stores consolidated workflows and pipelines in an external vector DB keyed by goal/subgoal descriptions for later retrieval and reuse.",
            "task_name": "Complex multi-step tasks requiring planning and multi-tool execution",
            "task_description": "Benchmark tasks in this study involve hierarchical planning plus sequences of tool calls (handled by XAgent's executor) across domains like trip planning, finance, and writing.",
            "benchmark_name": "Custom 40-task tool-invocation dataset (20 train / 20 test)",
            "performance_with_memory": "Not applicable to XAgent alone; with ICE applied within XAgent the paper reports large reductions in API calls and improved completion rates.",
            "performance_without_memory": "Standard XAgent (without ICE) serves as the baseline; paper reports it uses significantly more API calls and has lower completion rates than ICE-augmented variants.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "XAgent + ICE (both planning and execution memories) outperforms standard XAgent (no memory) on API-cost and task effectiveness metrics; execution memory yields the largest gains.",
            "limitations_or_challenges": "XAgent's vanilla design mixes planning and execution, which reduces re-usability; ICE requires instrumentation and separate experts (planner/executor) to realize full benefits.",
            "key_insights": "Agent architectures that explicitly separate planning and execution are better-suited for inter-task experiential memory and for automated consolidation into reusable artifacts.",
            "uuid": "e4640.3",
            "source_info": {
                "paper_title": "Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "PineconeDB",
            "name_full": "Pinecone vector database",
            "brief_description": "An external vector database used as the agent's memory store to index and retrieve embeddings of consolidated workflows and pipelines.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Pinecone-backed retrieval for ICE",
            "agent_description": "Stores the keys (goal/subgoal descriptions and milestones) and the values (workflow JSON or pipeline JSON) as vectors for similarity-based retrieval during the EXPLOIT stage.",
            "memory_type": "external vector retrieval store",
            "memory_description": "Keys are embedded using OpenAI text-embedding-ada-002; retrievals are by cosine similarity with a configured threshold; retrieved artifacts (workflows/pipelines) are used either as in-context references or executed automatically.",
            "task_name": "Indexing and retrieving consolidated experiences for multi-step agent tasks",
            "task_description": "Used to persist the consolidated artifacts mined from training tasks and to supply them during testing tasks for inter-task self-evolution.",
            "benchmark_name": "Custom 40-task tool-invocation dataset (20 train / 20 test)",
            "performance_with_memory": "Enables the reported up-to-80% API-call reductions and ≈50% pipeline re-utilization; effectiveness depends on embedding quality and retrieval threshold.",
            "performance_without_memory": "Not applicable; without a vector DB, ICE cannot perform similarity-based retrieval and would fall back to pure ReACT or in-context-only strategies.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Vector-store-backed retrieval is central to ICE; embedding + cosine similarity retrieval enables matching of workflows/pipelines to new goals, but retrieval thresholds and embedding model choice affect applicability.",
            "limitations_or_challenges": "Embedding quality, selection of similarity threshold, and semantically imperfect matches limit reuse; reliance on an external service introduces engineering and privacy considerations.",
            "key_insights": "A vector DB with goal/subgoal keyed embeddings is an effective mechanism for storing and retrieving inter-task experiences, enabling automated execution pipelines and in-context workflow reuse.",
            "uuid": "e4640.4",
            "source_info": {
                "paper_title": "Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Xagent: An autonomous agent for complex task solving",
            "rating": 2,
            "sanitized_title": "xagent_an_autonomous_agent_for_complex_task_solving"
        },
        {
            "paper_title": "Memorybank: Enhancing large language models with longterm memory",
            "rating": 2,
            "sanitized_title": "memorybank_enhancing_large_language_models_with_longterm_memory"
        },
        {
            "paper_title": "Think-in-memory: Recalling and postthinking enable llms with long-term memory",
            "rating": 2,
            "sanitized_title": "thinkinmemory_recalling_and_postthinking_enable_llms_with_longterm_memory"
        },
        {
            "paper_title": "Chatdb, Augmenting llms with databases as their symbolic memory",
            "rating": 1,
            "sanitized_title": "chatdb_augmenting_llms_with_databases_as_their_symbolic_memory"
        },
        {
            "paper_title": "Expel: Llm agents are experiential learners",
            "rating": 1,
            "sanitized_title": "expel_llm_agents_are_experiential_learners"
        }
    ],
    "cost": 0.014522749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution
25 Jan 2024</p>
<p>Cheng Qian 
Tsinghua University</p>
<p>Shihao Liang 
The University of Hong Kong</p>
<p>Yujia Qin 
Tsinghua University</p>
<p>Yining Ye 
Tsinghua University</p>
<p>Xin Cong 
Tsinghua University</p>
<p>Yankai Lin <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#121;&#97;&#110;&#107;&#97;&#105;&#108;&#105;&#110;&#64;&#114;&#117;&#99;&#46;&#101;&#100;&#117;&#46;&#99;&#110;">&#121;&#97;&#110;&#107;&#97;&#105;&#108;&#105;&#110;&#64;&#114;&#117;&#99;&#46;&#101;&#100;&#117;&#46;&#99;&#110;</a> 
Renmin University of China</p>
<p>Yesai Wu 
ModelBest Inc</p>
<p>Zhiyuan Liu 
Tsinghua University</p>
<p>Maosong Sun 
Tsinghua University</p>
<p>Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution
25 Jan 20240CA5C8F60F13E807C917425296F06C07arXiv:2401.13996v1[cs.CL]
This paper introduces Investigate-Consolidate-Exploit (ICE), a novel strategy for enhancing the adaptability and flexibility of AI agents through inter-task self-evolution.Unlike existing methods focused on intra-task learning, ICE promotes the transfer of knowledge between tasks for genuine self-evolution, similar to human experience learning.The strategy dynamically investigates planning and execution trajectories, consolidates them into simplified workflows and pipelines, and exploits them for improved task execution.Our experiments on the XAgent framework demonstrate ICE's effectiveness, reducing API calls by as much as 80% and significantly decreasing the demand for the model's capability.Specifically, when combined with GPT-3.5, ICE's performance matches that of raw GPT-4 across various agent tasks.We argue that this self-evolution approach represents a paradigm shift in agent design, contributing to a more robust AI community and ecosystem, and moving a step closer to full autonomy.</p>
<p>Introduction</p>
<p>The trajectory of human evolution unfolds as a continual process of assimilating and distilling experiences from the past, perpetually advancing the frontiers of human capabilities.This self-directed evolution involves incorporating insights from past failures within tasks to rectify errors, and drawing upon successful experiences across tasks to enhance effectiveness and efficiency.Current language models including the GPT (OpenAI, 2022;2023) and LLaMA (Touvron et al., 2023a;b) series have showcased a remarkable capacity to engage in sophisticated task-solving as agents.While they can leverage tools to address specific capability-related challenges within the task, these agents inherently lack the innate capacity to glean insights from past successes and failures to self-evolve.</p>
<p>Enabling language model-driven agents to assimilate prior experiences, akin to human-like experiential learning, has inspired a spectrum of approaches.These include the integration of post-failure reflections into the model's context (Shinn et al., 2023;Miao et al., 2023;Qian et al., 2023b), adaptive optimization of prompts for the models to accommodate instructions (Zhou et al., 2022;Yang et al., 2023;Pryzant et al., 2023;Hsieh et al., 2023), and retrieval of past contexts for more coherent and effective generation (Zhao et al., 2023;Hu et al., 2023;Liu et al., 2023;Zhong et al., 2023).These investigations primarily focus on intra-task learning, addressing the execution of a specific task.However, they diverge from authentic self-evolution scenarios, which prioritize the transfer of inter-task experiences.</p>
<p>Given humans cultivate cognitive flexibility and problemsolving skills through diverse inter-task experiences, it is similarly significant for agents to generalize past knowledge to tackle new challenges.This contributes to intertask agent self-evolution, which enables the agent's autonomous adaptation and improvement of performance over time.In inter-task scenarios, referencing the previous execution outcomes or directly replicating the entire previous execution process, as seen in intra-task learning methods, proves impractical.This underscores the necessity of disentanglement in experience for effective re-utilization of past experiences.</p>
<p>In the context of current agent designs, a complete experience usually involves two aspects: i) Planning, which entails understanding intricate user task objectives and decomposing them into manageable units; ii) Execution, which entails a sequence of interactions with the environment through tool invocation and feedback processing.The blending of both experiences usually hampers the assimilation of pertinent knowledge, impeding adaptability in tackling diverse challenges.Therefore, advocating for their separation within the agent's self-evolution strategy is crucial to facilitate experience learning and re-utilization.In Preprint Figure 1.An overview of the inter-task agent self-evolution.ICE automatically identifies re-utilizable plans and tool execution trajectories as past experiences for agent self-evolution.The human effort may also be involved in crafting experiences for learning.</p>
<p>Figure 1, we illustrate how plan and execution trajectories as past experiences could respectively be applied for agent self-evolution.Despite this disentanglement, challenges persist in what contents are worth recording as experience, how to standardize their formats for convenient inter-task learning, and when to apply them for future use to raise task effectiveness and efficiency.</p>
<p>In this work, we propose INVESTIGATE-CONSOLIDATE-EXPLOIT (ICE), the first strategy that enables inter-task self-evolution of general agent designs.ICE disentangles task planning records and execution trajectories as the past experiences respectively for inter-task knowledge transfer, thus promoting both efficiency and effectiveness when handling new tasks.Specifically, ICE is divided into three stages: (1) Investigate: To identify experiences that are worth learning and referencing, we track the plan and status of each decomposed goal and extract successful execution trajectories for all the goals being handled.(2) Consolidate: To standardize the format of experiences to make future re-utilization automatic and convenient, we prune the plan into a linearized flow of successfully achieved goals (workflow), transform the mined trajectories into finite automata (pipeline) that enable automated execution for specific purposes, and store them in the database as agent's memory.(3) Exploit: To enhance the efficiency and effectiveness of new tasks by promptly accessing and utilizing previously consolidated experiences, we retrieve the con-solidated plans as in-context references for generating and refining new plans, and directly apply the consolidated trajectories with similar goals for automated execution.</p>
<p>To validate the effectiveness of our approach, we conducted a series of experiments utilizing the XAgent framework (XAgent-Team, 2023) for its clear disentanglement of agent planning and execution, which provides an ideal testbed for validating two self-evolution aspects.Through case studies and controlled experiments, we demonstrate that ICE can i) reduce model API calls by up to 80%, which significantly saves computational resources; ii) diminish the demands on models' intrinsic abilities, which lowers the barrier for agent deployment.Specifically, when paired with GPT-3.5, our ICE strategy can rival the performance of GPT-4 across diverse agent tasks.</p>
<p>Overall, the ICE strategy not only makes agent task completion more effective but also renders its execution more time-efficient and grounding more cost-efficient.All these aspects reflect the success of agent self-evolution.</p>
<p>Preliminaries</p>
<p>The experience accumulated during agent task handling usually involves planning and execution.The planning process is crucial in understanding user intentions and specifying detailed subgoals, while the execution process entails interaction with the environment to implement the goals in the plan.In general, we disentangle the agent's planning and execution experiences to facilitate their re-utilization.</p>
<p>For planning, we aim for a finer granularity in dividing user goals.The utilization of a tree structure serves as a typical and effective method for embodying this hierarchical representation.In the plan tree, the tree root represents the user's ultimate goal G, and each non-root node acts as a subgoal under its parent.For instance, the ultimate user goal G could be broken down into m subgoals G 1 , . . ., G m , while G 1 could be further broken down into n subgoals G 1-1 , . . ., G 1-n .Each subgoal G x (where x = x 1 -x 2 -. . .thus representing any subgoal) may encapsulate meta-information such as milestones that ought to be achieved, suggestions on what to follow or avoid, etc.In this way, the whole plan tree, along with all its metainformation, serves as a holistic planning experience, instructing the agent in rational and effective decomposition of user goals.In addition, the finer granularity provided by the tree structure offers unique advantages, as each subtree can serve as another valid experience for decomposing subgoals, thus maximizing the utility of past experiences.</p>
<p>For execution, we aim to decompose goal accomplishment into multiple steps of tool invocations, and the ReACT reasoning chain precisely meets this requirement.For each goal G, its ReACT execution trajectory is represented by T G = (s 1 , s 2 , . . ., s n ), where T G represents the trajectory with n steps of tool invocations for the goal G.Each step s i (1 ≤ i ≤ n) may encapsulate meta-information such as the thoughts, tool name, tool inputs, and response from the tool, etc.In this way, the whole execution trajectory, along with all its meta-information, serves as a holistic execution experience, guiding the agent to complete a goal effectively and efficiently.The ReACT structure refines the steps to complete a task, enabling greater flexibility in modifying, consolidating, and storing the experiences of achieving a specific goal.</p>
<p>Existing agent frameworks usually mix the planning and execution into one process.However, XAgent (XAgent-Team, 2023) stands out by differentiating these two facets, achieved through the deployment of two distinct agent experts, each responsible for task planning and execution, respectively.The planning system in XAgent is structured as a tree, with all its leaf node subgoals executed through a ReACT trajectory (G x is a leaf node subgoal is equivalent to T Gx exists).These traits all conform with the ideal agent system that maximizes the flexibility and utility of past experiences.In the following, we will mainly introduce our ICE strategy through the XAgent framework.Nevertheless, the core ideas behind our strategies could be generalized to other complex agent designs.</p>
<p>Problem Formulation.Continuing with the symbols we defined above, we formalize the research question as follows: For each goal G i in a multitude of past user goals G = {G 1 , . . ., G m } (1 ≤ i ≤ m), its past experiences involve the plan tree with G i as the root (planning experience) and a set of execution trajectories T = {T G i</p>
<p>x | x = x 1 -x 2 -. ..} (execution experiences).Our goal is to apply all these past experiences from G to improve the effectiveness and efficiency of a new user goal G new / ∈ G.</p>
<p>ICE Self-Evolution Strategy</p>
<p>In this section, we present an in-depth explanation of the ICE self-evolution strategy for planning and execution.We divide the ICE strategy into Planning selfevolution strategy and Execution self-evolution strategy, each composed of three stages corresponding to INVESTI-GATE, CONSOLIDATE, and EXPLOIT.</p>
<p>Planning ICE</p>
<p>The ICE of planning involves dynamic tracking the plan during current task execution, pruning the plan as linearized workflows, and retrieving the workflows from memory as references for new goal decomposition.</p>
<p>Plan-Investigate</p>
<p>The investigation of the plan aims to monitor the entire planning process and each goal's level of completion, facilitating the subsequent identification of valuable planning experiences.This entails tracking: i) the initial plan generation (decomposition of the ultimate goal G into subgoals), ii) any plan rectifications (e.g., splitting subgoal G i into G i-1 and G i-2 , adding subgoal G i+1 after G i , etc.), and iii) the status of each goal or subgoal in the plan.Specifically, a goal or subgoal's status is assessed based on whether the corresponding milestones are achieved by its corresponding execution trajectory.</p>
<p>In Figure 2, we show the INVESTIGATE stage tracks the split of a subgoal (in the red box) after its failure.The expected output is a finalized plan tree detailing all goals, subgoals, and their respective final statuses.</p>
<p>Plan-Consolidate</p>
<p>The consolidation of the plan aims to concatenate the successful goals and simplify the complex tree structure into a linear one easy for the agent to learn and refer to in the future.This entails i) pruning all the failed goals or subgoals in the plan and ii) transforming the successfully achieved goals into a linear structure.</p>
<p>Specifically, we first gather a set of goals and subgoals whose final status is successful to form G s .For each  all the subgoals that are leaf nodes in this subtree, forming
G x ∈ G s (x = x1-x2-. . .), ifW Gx = {G leaf | ∃ T G leaf , leaf = x-y 1 -y 2 -. . .},(1)
where, G leaf represents the leaf node under subgoal G x .We define W Gx as the consolidated workflow of a successfully achieved goal G x .For all the goals or subgoals in G s , we could thus derive a set of workflows W = {W Gx | G x ∈ G s }.All the consolidated workflows in W will then be stored in the agent system's memory, with the description of G x as key and W Gx as value.</p>
<p>In Figure 2, we present in CONSOLIDATE stage the pruning of the failed subgoal and storage of two linearized workflows respectively corresponding to the root goal and a subgoal.The construction of workflows significantly lowers the learning barrier for future reference, and unifies the format of diverse planning experiences.</p>
<p>Plan-Exploit</p>
<p>The exploitation of the plan aims to re-utilize the consolidated workflows as in-context references to improve the effectiveness of plan-making for the new user task goal.This entails retrieving plans of similar goals both during initial plan generation and plan rectifications.</p>
<p>Specifically, during the initial plan generation of a new task goal G new , we first retrieve workflow from the memory with a similar goal description.During the plan rectification after any subgoal fails, we retrieve the workflows that respectively align with i) the description of this failed subgoal G new x (x = x1-. . .x k ) and ii) description of its parent G new y (y = x1-. . .x k−1 ) as a higher-level goal.Leveraging this prior knowledge, we guide the agent system to either i) find improved methods to achieve G new In Figure 2, we reveal how to take reference from the retrieved workflow during initial plan generation in EXPLOIT stage.This ensures an effective and efficient planning process, aiding in the successful execution of subgoals later during self-evolution.</p>
<p>Execution ICE</p>
<p>The ICE of execution aims at utilizing the experiences gained from history to future inter-task execution.Intuitively, compared to the retrieved-augmented generation method, directly re-utilizing the execution trajectories in a restricted decoding manner is more efficient and can mitigate the cognitive burden of the agent.It involves recording the execution trajectories, consolidating the trajectories into pipelines, and the retrieval and execution of pipelines from memory for new task execution.trajectories aims to identify all the trajectories that successfully achieve their corresponding subgoals.Specifically, for all the leaf node subgoals G x in the plan tree, we record T Gx if the final status of G x is a success.This is because completely failed trajectories are more difficult to be directly taken as references during future task execution.Note that though we do not include failed trajectories, we still include errors that occurred during certain steps, as long as they do not cause the whole trajectory to fail.</p>
<p>Execution-Investigate The investigation of the execution</p>
<p>In Figure 3, we present an execution trajectory with repeated and wrong tool calls in certain steps, but we still record it during INVESTIGATE stage as it completes the subgoal climate research.</p>
<p>Execution-Consolidate</p>
<p>The consolidation of execution aims to fix all the necessary steps of tool invocations to achieve a specific goal, thus facilitating automatic execution.This entails i) pruning all the unnecessary tool invocation steps, ii) adding more complex logic (e.g.switching logic) to make the trajectory to success more robust, and iii) explicitly suggesting how to transit to the next tool invocation step (e.g. which tool to use, how to fill in parameters).</p>
<p>We note that for the execution trajectory T Gx = (s 1 , s 2 , . . ., s n ) aiming at subgoal G x , if the tool name and parameters of every s i are fixed, then T Gx can be executed in a finite automaton manner.Therefore, we transform T Gx into the format of an automaton represented by:
A Gx = ((Q, Σ, δ)),(2)
where Q denotes a set of tool invocation nodes, Σ denotes edges representing rules or suggestions under which transitions occur, and δ denotes the transition function defined as δ : Q × Σ → Q.We define A Gx as a consolidated pipeline for G x .Note that for each node in a pipeline, the next step of tool invocation is restricted by its out-degree.This facilitates the agent to make more informed choices efficiently, in contrast to predicting arbitrarily within an open-ended ReACT trajectory.</p>
<p>The transformation from T Gx to A Gx is automatically done by prompting GPT-4 with demonstrations (detailed in Appendix A).All strategies, including filtering repeated or useless nodes and adding switching logic, are explicitly instructed and demonstrated in examples.The rules and suggestions on how to transit from one node to another are also prompted.Finally, we store A Gx into the agent system's memory with G x and its corresponding milestones as the key.</p>
<p>In Figure 3, we present a consolidated pipeline with a switching structure.The previous repeated tool invocation (Node 2.3) is pruned, and the rule for the switching logic is explicitly added to avoid the irreversible failure brought by the wrong tool invocation (Node 2.3).Through consolidation into pipelines, the ReACT trajectories become more efficient and robust, while concurrently allowing for automatic execution.</p>
<p>Execution-Exploit</p>
<p>The exploitation of execution aims to directly apply the consolidated pipelines for use when handling new goals during future task execution.This entails retrieving pipelines according to the similarity of goals and executing the top-one pipeline.</p>
<p>Specifically, for a new subgoal G new</p>
<p>x in a new user task, we retrieve the pipeline with the most similar goal description and milestones.Note that there's a threshold of similarity in retrieval, and the agent will fall back to ReACT execution if the similarity is not enough.If a pipeline is successfully retrieved, then the automaton execution process will start.During execution, the agent will either i) complete the tool parameters if there is only one outgoing edge, or ii) choose from multiple outgoing edges, following the suggestions of the outgoing edge(s) to move to the next node.</p>
<p>In Figure 3, we show a successful exploitation of the stored pipeline to execute a similar subgoal about climate news searching.Flexible uses of pipelines can serve as a substitute for executing ReACT trajectories, thus effectively reducing the model API calls and making the whole execution more time-efficient and cost-efficient.</p>
<p>Experiments</p>
<p>To validate the effectiveness of INVESTIGATE-CONSOLIDATE-EXPLOIT strategy, we conduct experiments on the XAgent framework (XAgent-Team, 2023).Following the settings in ToolLLM (Qin et al., 2024), the agent system has access to high-quality RapidAPIs and external tools.We implement two ICE strategies based on this framework.</p>
<p>Experimental Settings</p>
<p>Data As previous agent benchmarks do not involve complex tool invocation scenarios beneficial for intricate learning experiences, we manually create 40 tasks that encourage the invocation of diverse tools but are still within the agent's capabilities.These encompass a wide range of scenarios including trip planning, data analysis, review writing, etc.We randomly divide the tasks into 20 data points as the training set and the other 20 data points as the testing set.The training set is used for the INVESTIGATE and CONSOLIDATE stages, where the agent dynamically tracks the plans, identifies execution trajectories, and further consolidates them into workflows or pipelines stored in the agent memory.With these past experiences, the agent system then performs the tasks in the testing set to validate our strategy.</p>
<p>Settings After performing the INVESTIGATE and CON-SOLIDATE stages with the workflows and pipelines stored in agent memory, we first test on data points within the training set to ensure the feasibility of ICE in same task self-evolution.Then, we generalize these prior experiences as resources for the self-evolution on tasks of similar dis-tribution in the testing set.We also apply the Pinecone database as the agent's memory (detailed in Appendix B).</p>
<p>Baseline We apply GPT-4 as the backbone model for the XAgent framework.We also involve GPT-4 for the consolidation of execution trajectories into workflows as introduced previously.We compare against several baselines: i) Standard XAgent (No ICE applied, GPT-4 or GPT-3.5 based), ii) Only Planning ICE (GPT-4 based), iii) Only Execution ICE (GPT-4 based), iv) Planning and Execution ICE (GPT-3.5 applied during EXPLOIT stage).</p>
<p>Metrics We objectively measure task performance through several dimensions.Please refer to Appendix B for a more detailed definition.</p>
<p>• API Calls: We count the total model API calls and the number of API calls aiming at tool invocation.API calls explicitly reflect the cost-efficiency and implicitly reflect the time-efficiency.</p>
<p>• Completion Rate: We measure the passing rate of all the subgoals after executing the trajectory, which reflects the effectiveness in comprehensively achieving the user task goals.</p>
<p>• Rectification Times: We record the times of plan rectification, which reflects the effectiveness of plan-making.</p>
<p>• Re-utilization Rate: We record the re-utilization rate of the consolidated pipelines, which reflects the effectiveness of pipeline exploitation.</p>
<p>Results</p>
<p>We present our quantitative experimental results in Table 1 and Applicability of Extracted Pipelines.The re-utilization rate of the mined pipelines from the training set tasks to the testing set is approximately 50%.This rate indicates that the consolidated pipelines resulting from the execution ICE strategy are generally applicable to unseen scenarios, demonstrating our strategy's robustness.</p>
<p>GPT-3.5 vs. GPT-4 Performance.We apply GPT-3.5 during the EXPLOIT stage of ICE to raise efficiency during experience re-utilization.We discover using GPT-3.5 as the backbone model during experience exploitation also exhibits a high completion rate and low rectification times, rivaling the performance of GPT-4.Moreover, it significantly outperforms the standard XAgent across all metrics.These findings suggest that the ICE strategy can significantly reduce the demand for the backbone model's capability when re-utilizing these consolidated experiences, thereby facilitating more cost-efficient agent deployment.</p>
<p>Case Study</p>
<p>To more closely examine the ICE strategy, we go over it with a case study, demonstrating three different scenarios.</p>
<p>Planning Investigate and Consolidate As shown on the left of Figure 4, we construct a scenario about the preparation of a blog post.During this task execution, we dynamically track the status of each subgoal and any rectifications (e.g. the split of subgoal 2 into a smaller one 2.1, the addition of new subgoals 2.2 and 2.3).According to their final status, subgoals 2 and 2.1 are pruned from the tree during consolidation.Therefore, the final workflows are composed of subgoals {1, 2.2, 2.3, 3} for the ultimate user goal, and {2.2, 2.3} for subgoal 2. They would be stored in the plan database for future use.</p>
<p>Execution Investigate and Consolidate</p>
<p>As shown on the right of Figure 4, we construct another scenario about financial investment.For the first subgoal, we record the successful execution trajectory with repeated tool invocations.This trajectory is then consolidated into a static pipeline, where the repeated tool invocations are pruned for simplification, and the rules are explicitly added on edges to facilitate transitions between nodes.With the pipeline's goal and its corresponding milestones as the key, the pipeline is stored in the pipeline database for future use.</p>
<p>Planning and Execution Exploit As shown in Figure 5, the exploitation process involves retrieving workflows with a similar goal description from the plan database to serve as a reference for the initial plan generation of a new user task goal.Subsequently, each subgoal in the plan, along with its corresponding milestones, is utilized to retrieve a pipeline for automatic execution.This integrated approach ensures alignment with previously successful task goal decomposition structures, contributing to more effective and efficient execution of the current task.</p>
<p>Ablation Study</p>
<p>Settings We conduct an ablation study on the number of training tasks used for experience storage.The Standard XAgent setting represents zero prior experience, while previous experiments entail 20 tasks to store experiences.In  Table 3. Results of ablation study on the number of tasks used for the storage of experiences.</p>
<p>this study, We randomly choose 10 tasks from the training set to store experiences, which are then exploited by the same 20 tasks in the testing set.We apply GPT-4 as the base model for XAgent's execution.</p>
<p>Results</p>
<p>In Table 3, we show as the number of tasks used for experience storage increases, the subtask completion rate gradually increases, while API calls significantly decrease.At the same time, more stored experiences could facilitate the pipeline's re-utilization.We discuss ICE's scaling effect in Section 5 in detail.</p>
<p>Preprint</p>
<p>Plan a trip to a foreign country and look for potential job opportunities there.</p>
<p>KEY</p>
<p>Reference Plan Goal: Plan a business trip to a foreign location and gather important information about that place.Milestones 2: … Thought: I have followed the ideas from the previous goal but added more detail to make them as comprehensive and efficient as possible.This plan avoids goals that are likely to fail and instead focuses on achievable goals, bearing in mind the limitations and possible issues that might arise.The refined plan will give a step-by-step guide to planning a trip and job hunting in a foreign country.</p>
<p>Refined</p>
<p>Discussions</p>
<p>The ICE strategies we proposed for self-evolution inspire many future research directions and open questions that we will discuss here.</p>
<p>Self-Evolution through Failed Experiences.Our ICE strategy primarily consolidates successful experiences.However, during the INVESTIGATE stage, errors are inevitable and the information gathered from these failed experiences is also valuable.Successful experiences provide a straightforward path for the agent system to emulate, while failed experiences offer lessons on what to avoid.These failed experiences, however, are challenging to directly apply as they require the system's introspective capability.Future research could address this potential challenge by transforming failed experiences into explicit suggestions or rules, in order to avoid setting unattainable goals in the plan or invoking tools that are likely to fail during the execution.</p>
<p>Experience for In-context Learning or Direct Application.During the EXPLOIT stage, the workflows are used for in-context reference, and the pipelines are directly applied for re-utilization.This approach acknowledges that the user's high-level goal is often not closely similar, making it challenging to directly apply the consolidated workflows to perform decomposition.In contrast, the consolidated pipeline aims to execute a low-level subgoal, which is more likely to be directly re-utilized under a similar high-level goal.Both methods have strengths and weaknesses: i) In-context is more flexible but requires the model's attention to detail for successful adaptation.ii) Direct application is straightforward and more efficient in terms of time and cost, but the consolidated pipeline's goal may still not align perfectly with the current subgoal, making the evaluation of its effectiveness complex.Future research should focus on designing more robust methods to better balance these two approaches, as they constitute the core of self-evolution.</p>
<p>Scaling and Grokking of Agent Self-Evolution.Our experiments have conclusively demonstrated the practicality of transforming prior experiences to facilitate new tasks.As larger memory volume allows for more precise and relevant retrieval, the accumulation of stored experiences can lead to scaling and grokking effects: the agent's execution time and cost can be further reduced, while the tasks to which these experiences could generalize become more complex and diverse.With the whole agent community involved, the collection of past experiences becomes more flexible and easier, as anyone can contribute and share their agent's task execution records for learning.This makes our ICE strategy highly scalable and increasingly beneficial as more past experiences are accumulated.</p>
<p>Related Work</p>
<p>LLM-driven AI Agent.Recent large language models (LLMs) have continuously demonstrated emerging intelligence (Wei et al., 2022a) in generating high-quality texts and codes (Zeng et al., 2022;Chowdhery et al., 2023;Ope-nAI, 2022;2023;Touvron et al., 2023a), performing robust reasoning (Wei et al., 2022b;Gao et al., 2023;Yao et al., 2023;Qian et al., 2023c), and leveraging tools (Schick et al., 2023;Qin et al., 2023;Patil et al., 2023;Qin et al., 2024).These abilities enable LLMs to actively interact with the environment as agents, making plans and grounding actions while processing feedback (Xi et al., 2023;Wang et al., 2023a;Yao et al., 2022;Ye et al., 2023a;b).</p>
<p>Current research into LLM agents involves the construction of robust agent frameworks (Qian et al., 2023b;Cai et al., 2024;Gur et al., 2024), exploration of multi-agent behaviors (Park et al., 2023;Chan et al., 2024;Chen et al., 2024;Li et al., 2023;Qian et al., 2023a), and benchmarks for agent evaluations (Zhou et al., 2023;Liu et al., 2024).Apart from these directions, we investigate the capability of agent self-evolution through ICE.</p>
<p>Memory-Based LLM Enhancement.The implementation of the ICE strategy requires the agent's memory capacity.Early studies including memory-augmented networks (Meng &amp; Huang, 2018;Graves &amp; Wayne) and their computational universality (Schuurmans, 2023) utilized the memory matrix for the model's long-term information storage.Recent works focus on other memorizing mechanisms, such as MemoryBank inspired by Ebbinghaus' forgetting curve theory (Zhong et al., 2023) and Think-in-Memory to imitate human-like recalling and post-thinking ability (Liu et al., 2023).Other external modules are also incorporated into the agent's architecture (Sumers et al., 2023), enhancing LLM through external memory retrieval (Izacard et al., 2023), disentanglement of memory and knowledge (Wang et al., 2023b), and database as symbolic memory (Hu et al., 2023).ICE utilizes external memory to store consolidated plans and pipelines, enabling LLM self-evolution through past experiences.</p>
<p>LLM Self-Improvement.The concept of selfimprovement is implicitly expressed in some current works, but not investigated in depth or uniquely as a strategy for agent designs.For most studies, this is demonstrated as the agent's iterative adaptation through task-execution (Madaan et al., 2023;Sun et al., 2023), code-execution (Qian et al., 2023b), or physical simulation (Song et al., 2023;Wang et al., 2023c) feedback.Other self-evolution strategies take the form of prompt adaptation and optimization (Zhou et al., 2022;Yang et al., 2023;Pryzant et al., 2023;Hsieh et al., 2023), continuous improvement through error-identification and self-reflection (Shinn et al., 2023;Miao et al., 2023;Qian et al., 2023b), and retrieval from database as short or long-term memory (Zhao et al., 2023;Hu et al., 2023;Liu et al., 2023;Zhong et al., 2023).These works primarily emphasize the iterative refinement of a single task within a loop-structured LLM-based framework.In contrast, ICE confronts the challenges associated with inter-task agent self-evolution, offering strategies for the effective exploitation of past experiences.</p>
<p>Conclusion</p>
<p>In this work, we introduce INVESTIGATE-CONSOLIDATE-EXPLOIT (ICE), a novel strategy to facilitate the agent's Preprint inter-task self-evolution.ICE identifies the plan and execution trajectories as valuable past experiences for reference and re-utilization, thereby promoting the agent's continuous improvement.Through experiments on XAgent, we show the ICE strategy can reduce model API calls by up to 80% and significantly lower requirements for model capabilities, thereby enhancing the deployment of agent systems in terms of time efficiency, cost efficiency, and overall task execution effectiveness.Our work contributes to the burgeoning field of research focused on intelligent AI agents, highlighting the potential for complex agents to continuously learn from past experiences and adapt to diverse scenarios.We hope that our research and discussions will inspire a new paradigm in agent design, ultimately contributing to the development of a more robust ecosystem for AI agents.</p>
<p>In-context Examples</p>
<p>Example 1: Query: Fetch the information of a product with sku W003247135 and W003247136.</p>
<p>Execution Trajectory:</p>
<p>Tool Name: RapidAPIEnv_rapi_wayfair_products _detail Tool Arguments: {"sku": "W003247135"} Tool Output: "response1" Tool Name: RapidAPIEnv_rapi_wayfair_reviews _list Tool Arguments: {"sku": "W003247135"} Tool Output: "response2" Tool Name: RapidAPIEnv_rapi_wayfair_products _detail Tool Arguments: {"sku": "W003247136"} Tool Output: "response3" Tool Name: FileSystemEnv_write_to_file Tool Arguments: {"filepath": " blog_post_material.txt","content": "response1 + response2"} Tool Output: "response1 + response2 + response"</p>
<p>Pipeline: "pipeline_name": "product review fetch and write", "pipeline_purpose": "Fetch overview information and details information of a given product.","nodes": [ { "node_name": "start", "tool_name": "Start", "node_type": "Start" }, { "node_name": "end", "tool_name": "End", "node_type": "End" },</p>
<p>Preprint B Experimental Setting Details</p>
<p>We apply the Pinecone vector database as the agent system's memory.For the consolidated workflows, we use the ultimate user's goal or the subgoals as the key and store the whole corresponding workflow.For the consolidated pipelines, we use the subgoal and its milestones as the key and store the pipeline with its name.The pipeline is consolidated into JSON format.The embedding of the key is derived by calling OpenAI text-embedding-ada-002.The retrievals of workflows and pipelines are all based on the embedding's cosine similarity.</p>
<p>For the metrics we apply, the total model API calls refer to all the calls of the backbone model regarding planning and execution, while excluding the callings of text-embedding-ada-002 for embeddings.The calls for tool invocation under the XAgent framework refer to how many times the handle subtask function is called during execution.This function is in charge of deciding how to handle the current subgoal and which tool to invocate, thus representing the real execution steps in the trajectory.The completion rate is measured by if the trajectory ends explicitly with success as the final status with all the milestones in the corresponding subgoal achieved.</p>
<p>Figure 2 .
2
Figure 2. The detailed illustration of Planning ICE.Planning ICE dynamically tracks the plan of the agent system during INVESTIGATE stage, prunes and linearizes the plan into a workflow during CONSOLIDATE stage, and re-utilizes the workflow as a reference for new goals during EXPLOIT stage.</p>
<p>.</p>
<p>All the retrievals are based on the cosine similarity of the embeddings of the goals' descriptions.</p>
<p>To get more information, I should … Tool: climate_news Tool Input: {"place": "Beijing", "date": "1.1"} Tool output: 1. Sunny … Thought: To get the humidity, I should … Tool: climate_news Tool Input: {"humidtity": "True"} Tool output: Key place and date are needed … Thought: The place and date parameters are needed … Tool: climate_news Tool Input: {"place": "Beijing", "date": "1.1", "humidity": "True"} Tool output:</p>
<p>Figure 3 .
3
Figure 3.The detailed illustration of Execution ICE.Execution ICE identifies successful execution trajectories of the agent system during INVESTIGATE stage, extracts potential useful pipelines during CONSOLIDATE stage, and re-utilizes relevant pipelines for new goals during EXPLOIT stage.</p>
<p>Preprint</p>
<p>Prepare for a movie review blog post.Thought: To efficiently address the broad scope, the plan has been divided into three subgoals, each focusing on a specific aspect of the query.Future operations may refine the research output with filtering and analysis techniques.Draft Writing.Draft the movie review for the blog post.Milestones 3: … Thought: Since subgoal 2 failed, we split it into smaller, more focused goal … Subgoal 2.1: Behind-the-Scenes and Trivia Check movie fan forums and dedicated movie trivia websites to collect ... The initial goal was too broad and might fail due to … Since the web search tool is limited, we are adding subgoals that utilize available resources like … Subgoal 2.2: Review Analysis.Analyze existing movie reviews to understand critical reception and key talking points.Milestones 2.2: … Subgoal 2.3: Summary of Cast and Crew Insights.Gather insights on the cast and crew's experiences and contributions to the film.and execution of a financial investment in the cryptocurrency market.Subgoal 1: Market Analysis.Identify promising cryptocurrencies based on current market trends Milestones 1: i)Use search engine to gather information on top-performing cryptocurrencies, ii)… Subgoal 2 … Thought: Beginning with the trending cryptocurrencies API seems to be a logical first step as it should provide a snapshot of what's currently ... Tool: cryptocurrency_markets_trending Tool Input: {} Tool output: cryptocurrencies exhibiting significant volume increases and capturing traders' interest: i) Coin1 … Thought: Since we now have a list of cryptocurrencies with good market performance and potential for growth, … Tool: write_to_file Tool Input: {"path": "analysist.txt","content": "Final Market Analysis Summary:\n\nPromising cryptocurrencies:\n-Coin1 …"} Tool output: write file success The write_to_file tool is used to write the analysis of overlapping cryptocurrencies across trending, top gainers, and most visited lists into a file named 'analysis.txt'…is used to write the final market analysis summary, including future recommendations, into the 'analysis.txt'file… Goal: Identify promising.cryptocurrencies based on current market trends.Milestones: i)Use search engine to gather information on topperforming cryptocurrencies, ii)… tool, cryptocurrency_markets_trending, is used to fetch the list of trending cryptocurrencies…</p>
<p>Figure 4 .
4
Figure 4. Case study on the INVESTIAGTE and CONSOLIDATE stages for the planning (left) and execution (right) self-evolution.</p>
<p>Subgoal 1 :
1
Plan Travel Itinerary.Outline the business trip schedule, including flights, accommodation, and local transportation.Milestones 1: … Subgoal 2: Create a Detailed Trip Agenda.Develop a comprehensive daily itinerary that includes all business appointments, meals, and potential leisure activities.Milestones 2: … … VAL Thought: The original goal consists of two major components, that is planning a trip and finding job opportunities Job opportunities.Research about job opportunities in the destination country.</p>
<p>Plan Subgoal 1 :
1
Plan Detailed Itinerary.Refine the travel itinerary to the destination country considering factors like duration of stay, places to visit and travel costs Milestones: i) Research potential flights and compare prices and schedules using an online travel agency.ii)… Subgoal 2: Research Job Opportunities.Identify potential job opportunities in the destination country and analyze the job market trends Milestones: i) Research job market trends and opportunities in the destination country.iiTool Input: {} Tool output: {"error": "No input parameters provided"} Tool: get_trip_plan Tool Input: {"days": 5,"destination":"Singapore"} Tool output: "Day 1": {"Morning":"Explore the …"} Tool: write_to_file Tool Input: "filepath": "itinerary.txt","content":"Day 1:\nMorning: Explore the…" Tool output: write file success … used to fail due to missing parameters … … …Tool: list_cities_asia Tool Input: {"sort":"desc","sort_by":"overall_score","size":"20", "page":"1"} Tool output: 1. Seoul, South Korea.Overall score: 86…Begin by fetching the list of top Asian cities...End the workflow after …Use the collected data to write an initial draft … Tool: write_to_file Tool Input: "filepath": "Potential_Cities.txt","content": "Potential Cities for Job Opportunities:\n\nAsia:\n-Seoul Tool output: write file success</p>
<p>Figure 5 .
5
Figure 5. Case study on the EXPLOIT stage for the planning (up) and execution (down) self-evolution.</p>
<p>Investigate Consolidate Exploit Goal Plan Data Base Sub Plan 1 Sub Goal 1 Sub Plan 2 Sub Goal 2 Sub Plan 3 Sub Goal 3 Sub Plan 1.1 Sub Goal 1.1 Sub Plan 1.2 Sub Goal 1.2 Root Plan Sub Plan 1 Failed Track Rectification: SPLIT into 1.1 and 1.2 Goal Sub Plan 1 Sub Goal 1 Sub Plan 2 Sub Goal 2 Sub Plan 3 Sub Goal 3 Sub Plan 1.1 Sub Goal 1.1 Sub Plan 1.2 Sub Goal 1.2 Root Plan Sub Plan 1 Sub Goal 1 Sub Plan 1.1 Sub Goal 1.1 Sub Goal 3 Sub Goal 2 Workflow for achieving Prune Goal Root Plan Sub Plan 1 Sub Goal 1 KEY VAL KEY VAL For Root Plan For Sub Plan 1 Sub Goal 1.2 Sub Goal 1.1 Sub Goal 1.1 Sub Goal 1.2 New Goal Root Plan KEY Sub Goal Goal Plan Sub Goal Sub Goal Retrieved Workflow for Reference</p>
<p>it is not a leaf node in the plan tree, we perform the following: First, for the subtree extended from G x , we prune all the subgoals in this subtree whose final status is failure.Next, we sequentially gather
Preprint</p>
<p>Table 2
2
, dividing the results of training and testing sets according to different backbone models and ICE strategies.
Efficiency of ICE in Reducing API Calls. Our findingsindicate that both the planning and execution ICE signifi-cantly reduce the model's API calls. The combination ofboth strategies can reduce total calls by 80%, demonstrat-ing the substantial cost and time efficiency of the ICE strat-egy. A comparative analysis further reveals that the pri-mary source of this reduction is the execution ICE. This isbecause the execution trajectory itself takes up most of themodel API calls originally in the XAgent framework, andthe consolidated pipelines can now serve as a more efficientsubstitute.Effectiveness of ICE in Task Execution and Plan For-mulation. Strategies incorporating ICE generally yield ahigher subtask completion rate than the standard XAgent.
Additionally, the number of rectifications required for the plan is considerably reduced.These results suggest that ICE enhances the effectiveness of subtask execution and the rationality and effectiveness of the plans formulated.</p>
<p>Table 1 .
1
ICE results on the 20 training set tasks to test same task evolution.
ICE StrategyModelAPI Calls (All)API Calls (Tools)Completion Rate (Subtasks, %)Rectifications TimesRe-utilization RateStandard (w/o ICE)GPT-4 GPT-3.52265 4071745 88072.97 25.33107 234--Planning ICEGPT-4177953286.3635-Execution ICEGPT-444331894.44-39.44Planning + ExecutionGPT-4 GPT-3.5540 610384 25890.00 86.966 647.89 35.21</p>
<p>Table 2 .
2
ICE results on the 20 testing set tasks to test self-evolution on tasks of similar distribution.</p>
<p>If one tool call appears more than once in the tool records, try to i) filter them and leave only one tool call node if those tool calls are useless repeated trials, ii) add switch logic as the example does, which means there are multiple out edges from the tool node.-Always add the start node and end node in the nodes, and start edge and end edge in the edges.-Try to simplify the pipeline.Avoid including the wrong tool call trials in the nodes and edges, instead add comments to the edges to state what should be noticed to avoid error happening or add error handle logic such as switch logic.-Do not miss any properties in nodes and edges.Node name, tool name, and node type in nodes.Edge name, edge type, from node to node, and comments in edges.
PreprintAppendixA Demonstrations for PipelineConsolidationSystem prompt and in-context examples used in pipelineconsolidation:System PromptYou are an experienced pipelineextractor who can extract rules andexperiences given an executiontrajectory.You are given an execution trajectorywith tool calls, which contain thetool name and tool input arguments.You need to generate someinformation describing what nodesand edges this pipeline contains:1. some natural language comments andconditions explaining how thecurrent tool call moves to the nexttool call.2. edges between tool calls3. nodes for every tool callHere are two examples:{examples}Note that:-
Preprint{"node_name": "product_detail_1", "tool_name": " RapidAPIEnv_rapi_wayfair _products_detail", "node_type": "ToolServer" }, { "node_name": "review_list", "tool_name": " RapidAPIEnv_rapi_wayfair _reviews_list", "node_type": "ToolServer" }, { "node_name": "product_detail_2", "tool_name": " RapidAPIEnv_rapi_wayfair _products_detail", "node_type": "ToolServer" }, { "node_name": "write_file", "tool_name": " FileSystemEnv_write_to_file", "node_type": "ToolServer" } ], "edges": [ { "edge_name": "start_product_detail ", "edge_type": "data", "from_node": "start", "to_node": "product_detail_1", "comments": [ "The first tool, RapidAPIEnv_rapi_wayfair _products_detail, is used to fetch the product details for the given SKU." ] }, { "edge_name": " product_detail_review_list", "edge_type": "data", "from_node": "product_detail_1", "to_node": "review_list", "comments": [ "The second tool, RapidAPIEnv_rapi_wayfair _reviews_list, is used to fetch the reviews for the same SKU."] }, { "edge_name": " review_list_product_detail_2", "edge_type": "data", "from_node": "review_list", "to_node": "product_detail_2", "comments": [ "The third tool, RapidAPIEnv_rapi_wayfair _products_detail, is used to fetch the reviews for the SKU W003247136." ] }, { "edge_name": " product_detail_2_write_file", "edge_type": "data", "from_node": "product_detail_2", "to_node": "write_file", "comments": [ "The fourth tool, FileSystemEnv_write_to_file , is used to write the fetched product details and reviews into a file named 'blog_post_material.txt'."] }, { "edge_name": "end_pipeline", "edge_type": "data", "from_node": "write_file", "to_node": "end", "comments"Example 2: Query: Fetch the information of a product with sku W003247135.Execution Trajectory:Tool Name: RapidAPIEnv_rapi_wayfair_products _detail Tool Arguments: {"sku": "W003247135"} Tool Output: "fail.Can not find product W003247135.Supported product: W003247136, W003247137, Preprint ..."Tool Name: FileSystemEnv_write_to_file Tool Arguments: {"filepath": " fail_reason_and_suggestions.txt"," content": "Reason: The current available product ids do not include sku W003247135.\ nSuggestions: However, a similar product W003247136 can be obtained ."}Tool Output: "Reason: The current available product ids do not include sku W003247135.\ nSuggestions: However, a similar product W003247136 can be obtained ."Tool Name: RapidAPIEnv_rapi_wayfair_products _detail Tool Arguments: {"sku": "W003247136"} Tool Output: "response 1" Tool Name: RapidAPIEnv_rapi_wayfair_reviews_listTool Arguments: {"sku": "W003247136"} Tool Output: "response 2"Tool Name: FileSystemEnv_write_to_file Tool Arguments: {"filepath": " blog_post_material.txt","content": "response1 + response2"} Tool Output: "response1 + response2" Pipeline: "pipeline_name": "product review fetch and write", "pipeline_purpose": "Fetch overview information and details information of a given product.","nodes": [ { "node_name": "start", "tool_name": "Start", "node_type": "Start" }, { "node_name": "end", "tool_name": "End", "node_type": "End" }, { "node_name": "product_detail", "tool_name": " RapidAPIEnv_rapi_wayfair _products_detail", "node_type": "ToolServer" }, { "node_name": " write_fail_reason_and_suggestions ", "tool_name": " FileSystemEnv_write_to_file", "node_type": "ToolServer" }, { "node_name": "product_detail_retry ", "tool_name": " RapidAPIEnv_rapi_wayfair _products_detail", "node_type": "ToolServer" }, { "node_name": "review_list", "tool_name": " RapidAPIEnv_rapi_wayfair _reviews_list", "node_type": "ToolServer" }, { "node_name": " write_obtained_information", "tool_name": " FileSystemEnv_write_to_file", "node_type": "ToolServer" } ], "edges": [ { "edge_name": "start_product_detail ", "edge_type": "data", "from_node": "start", "to_node": "product_detail", "comments": [ "The first tool, RapidAPIEnv_rapi_wayfair _products_detail, is used to fetch the product details for the given SKU." ] }, { "edge_name": " Preprint product_detail_write_fail _reason_and_suggestions", "edge_type": "data", "from_node": "product_detail", "to_node": " write_fail_reason_and_suggestions ", "comments": [ "Here is a switch logic: If the response from node product_detail is failed, which means the RapidAPIEnv_rapi_wayfair _products_detail tool do not support the product SKU given in the user query, FileSystemEnv_write_to_file , is used to write the failed reason and suggestions into a file named ' fail_reason_and_suggestions .txt'." ] }, { "edge_name": " write_fail_reason_and_suggestions _product_detail_retry", "edge_type": "data", "from_node": " write_fail_reason_and_suggestions ", "to_node": "product_detail_retry", "comments": [ "Retry the RapidAPIEnv_rapi_wayfair _products_detail tool with suggestions written before ."] }, { "edge_name": "product_detail_retry _review_list", "edge_type": "data", "from_node": "product_detail_retry ", "to_node": "review_list", "comments": [ "Use the response from node product_detail_retry to review_list, the RapidAPIEnv_rapi_wayfair _reviews_list tool, to fetch the reviews for the suggested SKU." ] }, { "edge_name": " product_detail_review_list", "edge_type": "data", "from_node": "product_detail", "to_node": "review_list", "comments": [ "product_detail node appears the second time here, so here is another possible option for the switch logic : If the response from node product_detail is successful, then RapidAPIEnv_rapi_wayfair _reviews_list, is used directly to fetch the reviews for the same SKU."] }, { "edge_name": " review_list_write_obtained _information", "edge_type": "data", "from_node": "review_list", "to_node": " write_obtained_information", "comments": [ "The next tool, FileSystemEnv_write_to_file , is used to write the fetched product details and reviews into a file named 'blog_post_material.txt'."] }, { "edge_name": "end_pipeline", "edge_type": "data", "from_node": " write_obtained_information", "to_node": "end", "comments": [] } ]
Large language models as tool makers. T Cai, X Wang, T Ma, X Chen, D Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Chateval: Towards better LLM-based evaluators through multi-agent debate. C.-M Chan, W Chen, Y Su, J Yu, W Xue, S Zhang, J Fu, Z Liu, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. W Chen, Y Su, J Zuo, C Yang, C Yuan, C.-M Chan, H Yu, Y Lu, Y.-H Hung, C Qian, Y Qin, X Cong, R Xie, Z Liu, M Sun, J Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Pal: Program-aided language models. L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Neural turing machines. A Graves, G Wayne, </p>
<p>A real-world webagent with planning, long context understanding, and program synthesis. I Gur, H Furuta, A Huang, M Safdari, Y Matsuo, D Eck, A Faust, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Automatic engineering of long prompts. C.-J Hsieh, S Si, F X Yu, I S Dhillon, arXiv:2311.101172023arXiv preprint</p>
<p>C Hu, J Fu, C Du, S Luo, J Zhao, H Zhao, Chatdb, Augmenting llms with databases as their symbolic memory. 2023</p>
<p>Atlas: Few-shot learning with retrieval augmented language models. G Izacard, P Lewis, M Lomeli, L Hosseini, F Petroni, T Schick, J Dwivedi-Yu, A Joulin, S Riedel, E Grave, Journal of Machine Learning Research. 242512023</p>
<p>Camel: Communicative agents for" mind" exploration of large language model society. G Li, H A A K Hammoud, H Itani, D Khizbullin, B Ghanem, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Think-in-memory: Recalling and postthinking enable llms with long-term memory. L Liu, X Yang, Y Shen, B Hu, Z Zhang, J Gu, G Zhang, 2023</p>
<p>Agentbench: Evaluating LLMs as agents. X Liu, H Yu, H Zhang, Y Xu, X Lei, H Lai, Y Gu, H Ding, K Men, K Yang, S Zhang, X Deng, A Zeng, Z Du, C Zhang, S Shen, T Zhang, Y Su, H Sun, M Huang, Y Dong, J Tang, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Selfrefine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, S Gupta, B P Majumder, K Hermann, S Welleck, A Yazdanbakhsh, P Clark, Thirtyseventh Conference on Neural Information Processing Systems. 2023</p>
<p>Dialogue intent classification with long short-term memory networks. L Meng, M Huang, Natural Language Processing and Chinese Computing. ChamSpringer International Publishing2018</p>
<p>Using llms to zero-shot check their own step-by-step reasoning. N Miao, Y W Teh, T Rainforth, Selfcheck, arXiv:2308.00436OpenAI. Gpt-4 technical report. 2023. 2022. 2023Preprint OpenAI. Chatgpt</p>
<p>Generative agents: Interactive simulacra of human behavior. J S Park, J O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>S G Patil, T Zhang, X Wang, J E Gonzalez, Gorilla, Large language model connected with massive apis. 2023</p>
<p>Automatic prompt optimization with "gradient descent" and beam search. R Pryzant, D Iter, J Li, Y Lee, C Zhu, M Zeng, doi: 10.18653Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>URL. </p>
<p>Communicative agents for software development. C Qian, X Cong, W Liu, C Yang, W Chen, Y Su, Y Dang, J Li, J Xu, D Li, Z Liu, M Sun, 2023a</p>
<p>Creator: Tool creation for disentangling abstract and concrete reasoning of large language models. C Qian, C Han, Y Fung, Y Qin, Z Liu, Ji , H , Findings of the Association for Computational Linguistics: EMNLP 2023. 2023b</p>
<p>Linking toolkit creation and using through chain-of-solving on open-source model. C Qian, C Xiong, Z Liu, Z Liu, Toolink, 2023c</p>
<p>Tool learning with foundation models. Y Qin, S Hu, Y Lin, W Chen, N Ding, G Cui, Z Zeng, Y Huang, C Xiao, C Han, Y R Fung, Y Su, H Wang, C Qian, R Tian, K Zhu, S Liang, X Shen, B Xu, Z Zhang, Y Ye, B Li, Z Tang, J Yi, Y Zhu, Z Dai, L Yan, X Cong, Y Lu, W Zhao, Y Huang, J Yan, X Han, X Sun, D Li, J Phang, C Yang, T Wu, H Ji, Z Liu, M Sun, 2023</p>
<p>Facilitating large language models to master 16000+ real-world apis. Y Qin, S Liang, Y Ye, K Zhu, L Yan, Y Lu, Y Lin, X Cong, X Tang, B Qian, S Zhao, L Hong, R Tian, R Xie, J Zhou, M Gerstein, D Li, Z Liu, M Sun, Toolllm, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Toolformer: Language models can teach themselves to use tools. T Schick, J Dwivedi-Yu, R Dessi, R Raileanu, M Lomeli, E Hambro, L Zettlemoyer, N Cancedda, T Scialom, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Memory augmented large language models are computationally universal. D Schuurmans, 2023</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, K R Narasimhan, S Yao, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. C H Song, J Wu, C Washington, B M Sadler, W.-L Chao, Y Su, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>T R Sumers, S Yao, K Narasimhan, T L Griffiths, arXiv:2309.02427Cognitive architectures for language agents. 2023arXiv preprint</p>
<p>Adaptive planning from feedback with language models. H Sun, Y Zhuang, L Kong, B Dai, C Zhang, Adaplanner, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, 2023a</p>
<p>. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, D Bikel, L Blecher, C C Ferrer, M Chen, G Cucurull, D Esiobu, J Fernandes, J Fu, W Fu, B Fuller, C Gao, V Goswami, N Goyal, A Hartshorn, S Hosseini, R Hou, H Inan, M Kardas, V Kerkez, M Khabsa, I Kloumann, A Korenev, P S Koura, M.-A Lachaux, T Lavril, J Lee, D Liskovich, Y Lu, Y Mao, X Martinet, T Mihaylov, P Mishra, I Molybog, Y Nie, A Poulton, J Reizenstein, R Rungta, K Saladi, A Schelten, R Silva, E M Smith, R Subramanian, X E Tan, B Tang, R Taylor, A Williams, J X Kuan, P Xu, Z Yan, I Zarov, Y Zhang, A Fan, M Kambadur, S Narang, A Rodriguez, R Stojnic, S Edunov, Scialom, 2023bT. Llama 2: Open foundation and fine-tuned chat models</p>
<p>L Wang, C Ma, X Feng, Z Zhang, H Yang, J Zhang, Z Chen, J Tang, X Chen, Y Lin, arXiv:2308.11432A survey on large language model based autonomous agents. 2023aarXiv preprint</p>
<p>Augmenting language models with longterm memory. W Wang, L Dong, H Cheng, X Liu, X Yan, J Gao, F Wei, Thirty-seventh Conference on Neural Information Processing Systems. 2023b</p>
<p>Describe, explain, plan and select: Interactive planning with LLMs enables open-world multitask agents. Z Wang, S Cai, G Chen, A Liu, X Ma, Y Liang, Thirty-seventh Conference on Neural Information Processing Systems. 2023c</p>
<p>Emergent abilities of large language models. J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, Transactions on Machine Learning Research. 2022a</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Xagent: An autonomous agent for complex task solving. Xagent-Team, 2023</p>
<p>Z Xi, W Chen, X Guo, W He, Y Ding, B Hong, M Zhang, J Wang, S Jin, E Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>C Yang, X Wang, Y Lu, H Liu, Q V Le, D Zhou, X Chen, arXiv:2309.03409Large language models as optimizers. 2023arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K R Narasimhan, Y Cao, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K R Narasimhan, Thirtyseventh Conference on Neural Information Processing Systems. 2023</p>
<p>Large language model as autonomous decision maker. Y Ye, X Cong, Y Qin, Y Lin, Z Liu, M Sun, arXiv:2308.125192023aarXiv preprint</p>
<p>Proagent: From robotic process automation to agentic process automation. Y Ye, X Cong, S Tian, J Cao, H Wang, Y Qin, Y Lu, H Yu, H Wang, Y Lin, Z Liu, M Sun, 2023b</p>
<p>Glm-130b: An open bilingual pre-trained model. A Zeng, X Liu, Z Du, Z Wang, H Lai, M Ding, Z Yang, Y Xu, W Zheng, X Xia, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Expel: Llm agents are experiential learners. A Zhao, D Huang, Q Xu, M Lin, Y.-J Liu, G Huang, arXiv:2308.101442023arXiv preprint</p>
<p>Memorybank: Enhancing large language models with longterm memory. W Zhong, L Guo, Q Gao, H Ye, Y Wang, 2023</p>
<p>Webarena: A realistic web environment for building autonomous agents. S Zhou, F Xu, H Zhu, X Zhou, R Lo, A Sridhar, X Cheng, T Ou, Y Bisk, D Fried, U Alon, G Neubig, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023</p>
<p>Large language models are humanlevel prompt engineers. Y Zhou, A I Muresanu, Z Han, K Paster, S Pitis, H Chan, J Ba, The Eleventh International Conference on Learning Representations. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
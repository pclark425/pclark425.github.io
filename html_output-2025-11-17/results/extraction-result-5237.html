<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5237 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5237</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5237</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-267547812</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.05862v1.pdf" target="_blank">Let Your Graph Do the Talking: Encoding Structured Data for LLMs</a></p>
                <p><strong>Paper Abstract:</strong> How can we best encode structured data into sequential form for use in large language models (LLMs)? In this work, we introduce a parameter-efficient method to explicitly represent structured data for LLMs. Our method, GraphToken, learns an encoding function to extend prompts with explicit structured information. Unlike other work which focuses on limited domains (e.g. knowledge graph representation), our work is the first effort focused on the general encoding of structured data to be used for various reasoning tasks. We show that explicitly representing the graph structure allows significant improvements to graph reasoning tasks. Specifically, we see across the board improvements - up to 73% points - on node, edge and, graph-level tasks from the GraphQA benchmark.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5237.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5237.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-based serialization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hand-crafted text-based serialization of graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods that convert graphs into sequential textual descriptions (e.g., lists of nodes, edges, attributes, or serialized triples) so that an LLM can consume the graph as ordinary text; typically hand-designed and used as the predominant mode for structured data inclusion in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Hand-crafted text serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graphs are converted to sequences of lexical tokens by writing nodes and edges (and possibly node/edge attributes) in some textual format or template (e.g., 'Nodes: A, B, C. Edges: A-B, B-C...'). The paper does not define a single canonical algorithm or format but refers to prior work that uses ad-hoc serialization templates and linearizations.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General discrete graphs (relational data, knowledge graphs, social networks); GraphQA-style synthetic graphs are the target in the paper's comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Interpretable to humans; sequential (fits LLM input); ambiguous ordering choices (no canonical serialization order); requires the LLM to decode textual structure back to graph structure (decoding complexity); can be lossy if not all structural info encoded; compactness and expressivity depend on chosen template; generally parameter-free (no extra trainable encoder).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used as the baseline format for LLM graph reasoning evaluations (GraphQA tasks such as node/edge/graph-level reasoning) in prior work and cited baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Evaluated downstream via LLM performance on GraphQA tasks (accuracy for classification-like tasks and MSE for numeric/counting tasks). The paper reports that pure text representations (citing Fatemi et al. 2024) are insufficient for many graph reasoning tasks; GraphToken outperforms text-serialized baselines substantially (see comparisons summarized below). Specific baseline numbers are not enumerated in a single concise table in the text, but the paper states that GraphToken yields up to a 73 percentage-point improvement on GraphQA over text approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared conceptually and experimentally against GraphToken (learned continuous prompt tokens): text-based serializations are easier to interpret but force the LLM to decode structure from text, and they perform much worse on GraphQA tasks. GraphToken (continuous learned tokens) substantially outperforms text-serialized prompting and other textual prompting baselines (ZERO-SHOT, FEW-SHOT, CoT, COT-BAG, SOFT-PROMPT).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No canonical ordering for serialization; can impose heavy decoding burden on the LLM; often insufficient for algorithmic graph reasoning tasks; susceptible to hallucination/failure when LLM fails to reconstruct structure from text; quality heavily dependent on specific template/design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Let Your Graph Do the Talking: Encoding Structured Data for LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5237.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5237.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COT-BAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought with 'Build AGraph' prompt (COT-BAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting-style textual method that extends chain-of-thought prompting for graph tasks by instructing the model to construct an explicit textual graph description (nodes and edges) before reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>COT-BAG textual prompting</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A text-prompt augmentation that appends explicit instructions such as 'Let's construct a graph with the nodes and edges first' to the prompt (often combined with chain-of-thought examples) so the LLM is encouraged to write out the graph structure in text as an intermediate step before answering.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs used in prompting scenarios (GraphQA tasks were used in evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Textual and human-interpretable; intended to produce intermediate structured textual representation as part of the LLM's reasoning; leverages explicit chain-of-thought style reasoning. Does not introduce new model parameters (prompt engineering only).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph reasoning tasks from GraphQA (graph-level, node-level, edge-level tasks such as cycle check, node/edge counts, reachability, shortest path).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Measured by accuracy on GraphQA tasks. In the paper, COT-BAG is included in baselines for Experiment 1; GraphToken substantially outperforms COT-BAG across tasks. Exact per-task COT-BAG numbers are in the paper's Table 1 but the text-level summary is that COT-BAG improves graph-related prompts over naive prompts yet remains well below GraphToken performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared as a prompting/textual baseline to GraphToken and other textual prompting methods (ZERO-SHOT, FEW-SHOT, CoT). COT-BAG helps over vanilla prompting in some cases but is still outperformed by the GraphToken learned-encoding approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on the LLM to reliably produce correct intermediate graph text and then to reason from it; performance limited by LLM's ability to execute graph algorithms from text; does not add structural inductive bias via learned encoders; sensitive to prompt phrasing and examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Let Your Graph Do the Talking: Encoding Structured Data for LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5237.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5237.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SOFT-PROMPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Global static soft prompt (prefix-tuning style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameter-efficient soft-prompt method that learns a single static continuous prompt (a sequence of trainable token embeddings) prepended to all instances; in this paper it is used as a baseline that does not include instance-specific graph information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Global static soft prompt (text-space analog)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A fixed learned sequence of continuous tokens (soft prompt) prepended to the LLM input; in the baseline variant used in the experiments this prompt is shared across problem instances and contains no per-graph information, therefore it cannot convey specific graph structure to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>N/A (no per-instance graph encoding; used as a baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Parameter-efficient (only the soft prompt parameters are trained); does not represent per-instance structured data (so limited expressivity for graph-specific queries); can capture prior biases (e.g., predict majority labels).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>GraphQA benchmark tasks used to compare performance with GraphToken and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Evaluated by accuracy on GraphQA tasks; the paper reports that SOFT-PROMPT sometimes attains the second-best score on some tasks by exploiting label priors (e.g., predicting majority class) but is far below GraphToken when the prompt lacks graph-specific information.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared experimentally. GraphToken (which gives per-instance learned tokens derived from the graph) outperforms the global static soft prompt baseline by large margins across node/edge/graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Cannot communicate instance-specific graph structure; limited usefulness for actual graph reasoning tasks unless combined with graph information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Let Your Graph Do the Talking: Encoding Structured Data for LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5237.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5237.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphToken</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphToken (learned graph prompt function producing graph tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameter-efficient learned GNN-based encoder that maps an input graph to a small set of continuous token embeddings (graph-tokens) which are prepended to a frozen LLM's token embeddings so the LLM can reason over explicit structured information without learning to decode text serialization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Learned soft-token encoding (GraphToken)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A GNN encoder (various GNN architectures evaluated: GCN, GIN, MPNN, HGT, MHA, NodeSet, EdgeSet, etc.) consumes graph structure plus node positional features (Laplacian positional encodings (LPE), learned identity 'IDX', or concatenation LPE+IDX). The encoder produces a fixed-size latent (e.g., 128-d) graph representation which is read out (global pooling, per-node readout, or node-pair concatenation for edges), then projected via a dense 'head' into a sequence of continuous token embeddings (the 'graph tokens') that are prepended to the LLM input. The frozen LLM is trained only via gradients back to the GraphToken encoder (soft-prompt tuning style) by minimizing LLM perplexity on the target answer.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs used in GraphQA (synthetic relational graphs); designed to be general-purpose for relational data including knowledge graphs, social networks, and molecules (motivated but GraphQA synthetic graphs used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Explicit structural representation; parameter-efficient (only the GNN encoder and projection head are trained); operates in the LLM embedding space (continuous, not human-readable text); supports multiple readouts (graph-level pooling, node-level, edge-level via node pairs); can incorporate positional encodings (LPE) and learned identity features (IDX) to break equivariance; embedding dimensionality and projection size are hyperparameters; computationally cheap relative to fine-tuning full LLM; projection head dominates parameter count (paper reports encoder body sizes ~17k–199k parameters and projection head ~11M parameters in Table 3; elsewhere the paper mentions projecting latent 128 into a prompt embedding with ~80k parameters before noting the head dominates at ~11M).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Extensively evaluated on the GraphQA benchmark (graph-level tasks: node count, edge count, cycle check, triangle counting; node-level tasks: node degree, connected nodes; edge-level tasks: reachability, edge existence, shortest path) and in encoder-analysis tasks including out-of-distribution generalization (predicting bipartiteness on exhaustive small graph sets and evaluating embeddings with k-NN/regression; other graph property prediction and MSE metrics across exhaustive 8-node graphs and 15-node trees).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primary evaluation metrics: accuracy for classification-style tasks (e.g., cycle check, reachability), mean-squared error (MSE) for numeric/counting tasks (e.g., triangle count, average clustering coefficient, shortest path MSE). Training objective minimizes LLM perplexity L(A|Q) where Q = E(G)||T. Reported results: GraphToken 'substantially improves' LLM performance on all graph/node/edge tasks in GraphQA Test; the paper claims up to 73 percentage-point improvement on GraphQA benchmark versus text-based or prompting baselines. Per-task numbers are reported in the paper's tables (Table 1, Table 2, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Directly compared experimentally to textual prompting baselines (ZERO-SHOT, FEW-SHOT, CoT, ZERO-COT), COT-BAG, and a SOFT-PROMPT baseline. GraphToken substantially outperforms all these baselines across graph-, node-, and edge-level tasks. The paper also compares many internal variants of GraphToken by changing GNN architectures (GCN, GIN, MPNN, HGT, MHA, NodeSet, EdgeSet) and node features (LPE, IDX, LPE+IDX), showing no single GNN dominates all tasks but that learned positional features (IDX) often help and that linear models excel at simple counting tasks. GraphToken is shown to provide strong generalization in some transfer assessments (e.g., encoders trained on cycle/triangle tasks generalize well to bipartiteness detection).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not human-interpretable (continuous tokens not readable text); performance depends strongly on choice of GNN architecture and node features; projection/head parameter count can dominate total added parameters (not all parameter counts are negligible, e.g., head ~11M); some weaknesses in out-of-distribution settings (trees were often out-of-distribution compared to training graph sets); breaking equivariance via learned identity features helps but raises questions about inductive biases; reliance on a frozen LLM means encoder must align well to the embedding space; open questions remain about applicability to factual grounding and complex multimodal structured data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Let Your Graph Do the Talking: Encoding Structured Data for LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5237.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5237.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Network-produced soft prompts (Levine-style)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Input-to-soft-prompt network (separate trainable encoder producing soft tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A PEFT approach where an auxiliary neural network encodes input examples into continuous soft prompts to be prepended to a frozen LLM; the paper cites Levine et al. (2022) as closely related and extends the idea to graph inputs via a GNN.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Neural-network produced soft prompt (input-conditioned soft prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The method uses a separate trainable neural network that consumes structured input (in this paper, graphs via a GNN) and outputs a sequence of continuous embeddings (soft prompt tokens) that are prepended to the frozen LLM. Training updates only the encoder producing the prompt while LLM weights stay frozen; in GraphToken the encoder is specifically a GNN.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs (GraphToken implements this idea using GNNs and GraphQA graph types).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Instance-specific continuous prompts (thus can encode per-graph structure); parameter-efficient relative to full LLM fine-tuning; not human-readable; flexible dependent on encoder design; can leverage graph positional encodings and readout strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used as the core paradigm for GraphToken and evaluated on GraphQA tasks; also referenced generally in related work as a strategy for producing soft prompts from inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When instantiated as GraphToken, evaluated by accuracy and MSE on GraphQA; training uses LLM perplexity minimization. The paper reports large gains for the graph-conditioned soft prompt (GraphToken) over non-conditioned soft prompts and textual baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>This input-conditioned soft-prompt approach (as implemented by GraphToken) is shown to outperform global static soft prompts and text serialization baselines; the paper positions GraphToken as an instance of this general strategy specialized for graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quality depends on encoder architecture and alignment to LLM embedding space; soft prompts are not human-interpretable; projection/head parameterization can be large; must select appropriate readout (global pooling vs. node-level vs. pairwise) per task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Let Your Graph Do the Talking: Encoding Structured Data for LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5237.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5237.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG (retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) with structured retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval approach that augments LLM prompts with retrieved factual or fresh information (possibly structured) from external sources; the paper cites RAG as a motivation for bringing structured data into prompts but notes structured data is often complex.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Retrieval-augmented textual grounding (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Retrieve relevant factual or fresh data (which may be structured) from external databases or search and append it to the LLM prompt as text for answering; when the retrieved artifact is structured, practitioners often serialize it to text before inclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs, relational databases, web-derived structured information; general structured corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Enables freshness and factual grounding; relies on retrieval accuracy; when structured items are retrieved they typically must be serialized to text before feeding to an LLM; effectiveness depends on retrieval and serialization fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Motivational context for improving LLM factuality and freshness; not directly evaluated in this paper's experiments but referenced as major prior technique for augmenting LLM inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not specifically measured in this paper's experiments; RAG literature commonly evaluates via downstream QA accuracy and retrieval metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper positions GraphToken as an alternative to the typical RAG practice of serializing retrieved structured data to text, arguing that encoding structured data via learned continuous graph tokens can be more effective for graph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>When structured data is serialized for RAG it inherits the limitations of text-based serialization (ordering ambiguity, LLM decoding burden); RAG requires robust retrieval and indexing pipelines; potential mismatch between retrieved format and LLM consumption needs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Let Your Graph Do the Talking: Encoding Structured Data for LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models understand graph structured data? <em>(Rating: 2)</em></li>
                <li>Can language models solve graph problems in natural language? <em>(Rating: 2)</em></li>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>Standing on the shoulders of giant frozen language models <em>(Rating: 2)</em></li>
                <li>The power of scale for parameter-efficient prompt tuning <em>(Rating: 2)</em></li>
                <li>Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5237",
    "paper_id": "paper-267547812",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "Text-based serialization",
            "name_full": "Hand-crafted text-based serialization of graphs",
            "brief_description": "A family of methods that convert graphs into sequential textual descriptions (e.g., lists of nodes, edges, attributes, or serialized triples) so that an LLM can consume the graph as ordinary text; typically hand-designed and used as the predominant mode for structured data inclusion in prompts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Hand-crafted text serialization",
            "representation_description": "Graphs are converted to sequences of lexical tokens by writing nodes and edges (and possibly node/edge attributes) in some textual format or template (e.g., 'Nodes: A, B, C. Edges: A-B, B-C...'). The paper does not define a single canonical algorithm or format but refers to prior work that uses ad-hoc serialization templates and linearizations.",
            "graph_type": "General discrete graphs (relational data, knowledge graphs, social networks); GraphQA-style synthetic graphs are the target in the paper's comparison.",
            "representation_properties": "Interpretable to humans; sequential (fits LLM input); ambiguous ordering choices (no canonical serialization order); requires the LLM to decode textual structure back to graph structure (decoding complexity); can be lossy if not all structural info encoded; compactness and expressivity depend on chosen template; generally parameter-free (no extra trainable encoder).",
            "evaluation_task": "Used as the baseline format for LLM graph reasoning evaluations (GraphQA tasks such as node/edge/graph-level reasoning) in prior work and cited baselines.",
            "performance_metrics": "Evaluated downstream via LLM performance on GraphQA tasks (accuracy for classification-like tasks and MSE for numeric/counting tasks). The paper reports that pure text representations (citing Fatemi et al. 2024) are insufficient for many graph reasoning tasks; GraphToken outperforms text-serialized baselines substantially (see comparisons summarized below). Specific baseline numbers are not enumerated in a single concise table in the text, but the paper states that GraphToken yields up to a 73 percentage-point improvement on GraphQA over text approaches.",
            "comparison_to_other_representations": "Compared conceptually and experimentally against GraphToken (learned continuous prompt tokens): text-based serializations are easier to interpret but force the LLM to decode structure from text, and they perform much worse on GraphQA tasks. GraphToken (continuous learned tokens) substantially outperforms text-serialized prompting and other textual prompting baselines (ZERO-SHOT, FEW-SHOT, CoT, COT-BAG, SOFT-PROMPT).",
            "limitations_or_challenges": "No canonical ordering for serialization; can impose heavy decoding burden on the LLM; often insufficient for algorithmic graph reasoning tasks; susceptible to hallucination/failure when LLM fails to reconstruct structure from text; quality heavily dependent on specific template/design.",
            "uuid": "e5237.0",
            "source_info": {
                "paper_title": "Let Your Graph Do the Talking: Encoding Structured Data for LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "COT-BAG",
            "name_full": "Chain-of-Thought with 'Build AGraph' prompt (COT-BAG)",
            "brief_description": "A prompting-style textual method that extends chain-of-thought prompting for graph tasks by instructing the model to construct an explicit textual graph description (nodes and edges) before reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "COT-BAG textual prompting",
            "representation_description": "A text-prompt augmentation that appends explicit instructions such as 'Let's construct a graph with the nodes and edges first' to the prompt (often combined with chain-of-thought examples) so the LLM is encouraged to write out the graph structure in text as an intermediate step before answering.",
            "graph_type": "General graphs used in prompting scenarios (GraphQA tasks were used in evaluation).",
            "representation_properties": "Textual and human-interpretable; intended to produce intermediate structured textual representation as part of the LLM's reasoning; leverages explicit chain-of-thought style reasoning. Does not introduce new model parameters (prompt engineering only).",
            "evaluation_task": "Graph reasoning tasks from GraphQA (graph-level, node-level, edge-level tasks such as cycle check, node/edge counts, reachability, shortest path).",
            "performance_metrics": "Measured by accuracy on GraphQA tasks. In the paper, COT-BAG is included in baselines for Experiment 1; GraphToken substantially outperforms COT-BAG across tasks. Exact per-task COT-BAG numbers are in the paper's Table 1 but the text-level summary is that COT-BAG improves graph-related prompts over naive prompts yet remains well below GraphToken performance.",
            "comparison_to_other_representations": "Compared as a prompting/textual baseline to GraphToken and other textual prompting methods (ZERO-SHOT, FEW-SHOT, CoT). COT-BAG helps over vanilla prompting in some cases but is still outperformed by the GraphToken learned-encoding approach.",
            "limitations_or_challenges": "Relies on the LLM to reliably produce correct intermediate graph text and then to reason from it; performance limited by LLM's ability to execute graph algorithms from text; does not add structural inductive bias via learned encoders; sensitive to prompt phrasing and examples.",
            "uuid": "e5237.1",
            "source_info": {
                "paper_title": "Let Your Graph Do the Talking: Encoding Structured Data for LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "SOFT-PROMPT",
            "name_full": "Global static soft prompt (prefix-tuning style)",
            "brief_description": "A parameter-efficient soft-prompt method that learns a single static continuous prompt (a sequence of trainable token embeddings) prepended to all instances; in this paper it is used as a baseline that does not include instance-specific graph information.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Global static soft prompt (text-space analog)",
            "representation_description": "A fixed learned sequence of continuous tokens (soft prompt) prepended to the LLM input; in the baseline variant used in the experiments this prompt is shared across problem instances and contains no per-graph information, therefore it cannot convey specific graph structure to the LLM.",
            "graph_type": "N/A (no per-instance graph encoding; used as a baseline).",
            "representation_properties": "Parameter-efficient (only the soft prompt parameters are trained); does not represent per-instance structured data (so limited expressivity for graph-specific queries); can capture prior biases (e.g., predict majority labels).",
            "evaluation_task": "GraphQA benchmark tasks used to compare performance with GraphToken and other baselines.",
            "performance_metrics": "Evaluated by accuracy on GraphQA tasks; the paper reports that SOFT-PROMPT sometimes attains the second-best score on some tasks by exploiting label priors (e.g., predicting majority class) but is far below GraphToken when the prompt lacks graph-specific information.",
            "comparison_to_other_representations": "Compared experimentally. GraphToken (which gives per-instance learned tokens derived from the graph) outperforms the global static soft prompt baseline by large margins across node/edge/graph tasks.",
            "limitations_or_challenges": "Cannot communicate instance-specific graph structure; limited usefulness for actual graph reasoning tasks unless combined with graph information.",
            "uuid": "e5237.2",
            "source_info": {
                "paper_title": "Let Your Graph Do the Talking: Encoding Structured Data for LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GraphToken",
            "name_full": "GraphToken (learned graph prompt function producing graph tokens)",
            "brief_description": "A parameter-efficient learned GNN-based encoder that maps an input graph to a small set of continuous token embeddings (graph-tokens) which are prepended to a frozen LLM's token embeddings so the LLM can reason over explicit structured information without learning to decode text serialization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Learned soft-token encoding (GraphToken)",
            "representation_description": "A GNN encoder (various GNN architectures evaluated: GCN, GIN, MPNN, HGT, MHA, NodeSet, EdgeSet, etc.) consumes graph structure plus node positional features (Laplacian positional encodings (LPE), learned identity 'IDX', or concatenation LPE+IDX). The encoder produces a fixed-size latent (e.g., 128-d) graph representation which is read out (global pooling, per-node readout, or node-pair concatenation for edges), then projected via a dense 'head' into a sequence of continuous token embeddings (the 'graph tokens') that are prepended to the LLM input. The frozen LLM is trained only via gradients back to the GraphToken encoder (soft-prompt tuning style) by minimizing LLM perplexity on the target answer.",
            "graph_type": "General graphs used in GraphQA (synthetic relational graphs); designed to be general-purpose for relational data including knowledge graphs, social networks, and molecules (motivated but GraphQA synthetic graphs used in experiments).",
            "representation_properties": "Explicit structural representation; parameter-efficient (only the GNN encoder and projection head are trained); operates in the LLM embedding space (continuous, not human-readable text); supports multiple readouts (graph-level pooling, node-level, edge-level via node pairs); can incorporate positional encodings (LPE) and learned identity features (IDX) to break equivariance; embedding dimensionality and projection size are hyperparameters; computationally cheap relative to fine-tuning full LLM; projection head dominates parameter count (paper reports encoder body sizes ~17k–199k parameters and projection head ~11M parameters in Table 3; elsewhere the paper mentions projecting latent 128 into a prompt embedding with ~80k parameters before noting the head dominates at ~11M).",
            "evaluation_task": "Extensively evaluated on the GraphQA benchmark (graph-level tasks: node count, edge count, cycle check, triangle counting; node-level tasks: node degree, connected nodes; edge-level tasks: reachability, edge existence, shortest path) and in encoder-analysis tasks including out-of-distribution generalization (predicting bipartiteness on exhaustive small graph sets and evaluating embeddings with k-NN/regression; other graph property prediction and MSE metrics across exhaustive 8-node graphs and 15-node trees).",
            "performance_metrics": "Primary evaluation metrics: accuracy for classification-style tasks (e.g., cycle check, reachability), mean-squared error (MSE) for numeric/counting tasks (e.g., triangle count, average clustering coefficient, shortest path MSE). Training objective minimizes LLM perplexity L(A|Q) where Q = E(G)||T. Reported results: GraphToken 'substantially improves' LLM performance on all graph/node/edge tasks in GraphQA Test; the paper claims up to 73 percentage-point improvement on GraphQA benchmark versus text-based or prompting baselines. Per-task numbers are reported in the paper's tables (Table 1, Table 2, etc.).",
            "comparison_to_other_representations": "Directly compared experimentally to textual prompting baselines (ZERO-SHOT, FEW-SHOT, CoT, ZERO-COT), COT-BAG, and a SOFT-PROMPT baseline. GraphToken substantially outperforms all these baselines across graph-, node-, and edge-level tasks. The paper also compares many internal variants of GraphToken by changing GNN architectures (GCN, GIN, MPNN, HGT, MHA, NodeSet, EdgeSet) and node features (LPE, IDX, LPE+IDX), showing no single GNN dominates all tasks but that learned positional features (IDX) often help and that linear models excel at simple counting tasks. GraphToken is shown to provide strong generalization in some transfer assessments (e.g., encoders trained on cycle/triangle tasks generalize well to bipartiteness detection).",
            "limitations_or_challenges": "Not human-interpretable (continuous tokens not readable text); performance depends strongly on choice of GNN architecture and node features; projection/head parameter count can dominate total added parameters (not all parameter counts are negligible, e.g., head ~11M); some weaknesses in out-of-distribution settings (trees were often out-of-distribution compared to training graph sets); breaking equivariance via learned identity features helps but raises questions about inductive biases; reliance on a frozen LLM means encoder must align well to the embedding space; open questions remain about applicability to factual grounding and complex multimodal structured data.",
            "uuid": "e5237.3",
            "source_info": {
                "paper_title": "Let Your Graph Do the Talking: Encoding Structured Data for LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Network-produced soft prompts (Levine-style)",
            "name_full": "Input-to-soft-prompt network (separate trainable encoder producing soft tokens)",
            "brief_description": "A PEFT approach where an auxiliary neural network encodes input examples into continuous soft prompts to be prepended to a frozen LLM; the paper cites Levine et al. (2022) as closely related and extends the idea to graph inputs via a GNN.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Neural-network produced soft prompt (input-conditioned soft prompt)",
            "representation_description": "The method uses a separate trainable neural network that consumes structured input (in this paper, graphs via a GNN) and outputs a sequence of continuous embeddings (soft prompt tokens) that are prepended to the frozen LLM. Training updates only the encoder producing the prompt while LLM weights stay frozen; in GraphToken the encoder is specifically a GNN.",
            "graph_type": "General graphs (GraphToken implements this idea using GNNs and GraphQA graph types).",
            "representation_properties": "Instance-specific continuous prompts (thus can encode per-graph structure); parameter-efficient relative to full LLM fine-tuning; not human-readable; flexible dependent on encoder design; can leverage graph positional encodings and readout strategies.",
            "evaluation_task": "Used as the core paradigm for GraphToken and evaluated on GraphQA tasks; also referenced generally in related work as a strategy for producing soft prompts from inputs.",
            "performance_metrics": "When instantiated as GraphToken, evaluated by accuracy and MSE on GraphQA; training uses LLM perplexity minimization. The paper reports large gains for the graph-conditioned soft prompt (GraphToken) over non-conditioned soft prompts and textual baselines.",
            "comparison_to_other_representations": "This input-conditioned soft-prompt approach (as implemented by GraphToken) is shown to outperform global static soft prompts and text serialization baselines; the paper positions GraphToken as an instance of this general strategy specialized for graphs.",
            "limitations_or_challenges": "Quality depends on encoder architecture and alignment to LLM embedding space; soft prompts are not human-interpretable; projection/head parameterization can be large; must select appropriate readout (global pooling vs. node-level vs. pairwise) per task.",
            "uuid": "e5237.4",
            "source_info": {
                "paper_title": "Let Your Graph Do the Talking: Encoding Structured Data for LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RAG (retrieval)",
            "name_full": "Retrieval-Augmented Generation (RAG) with structured retrieval",
            "brief_description": "A retrieval approach that augments LLM prompts with retrieved factual or fresh information (possibly structured) from external sources; the paper cites RAG as a motivation for bringing structured data into prompts but notes structured data is often complex.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Retrieval-augmented textual grounding (RAG)",
            "representation_description": "Retrieve relevant factual or fresh data (which may be structured) from external databases or search and append it to the LLM prompt as text for answering; when the retrieved artifact is structured, practitioners often serialize it to text before inclusion.",
            "graph_type": "Knowledge graphs, relational databases, web-derived structured information; general structured corpora.",
            "representation_properties": "Enables freshness and factual grounding; relies on retrieval accuracy; when structured items are retrieved they typically must be serialized to text before feeding to an LLM; effectiveness depends on retrieval and serialization fidelity.",
            "evaluation_task": "Motivational context for improving LLM factuality and freshness; not directly evaluated in this paper's experiments but referenced as major prior technique for augmenting LLM inputs.",
            "performance_metrics": "Not specifically measured in this paper's experiments; RAG literature commonly evaluates via downstream QA accuracy and retrieval metrics.",
            "comparison_to_other_representations": "Paper positions GraphToken as an alternative to the typical RAG practice of serializing retrieved structured data to text, arguing that encoding structured data via learned continuous graph tokens can be more effective for graph reasoning.",
            "limitations_or_challenges": "When structured data is serialized for RAG it inherits the limitations of text-based serialization (ordering ambiguity, LLM decoding burden); RAG requires robust retrieval and indexing pipelines; potential mismatch between retrieved format and LLM consumption needs.",
            "uuid": "e5237.5",
            "source_info": {
                "paper_title": "Let Your Graph Do the Talking: Encoding Structured Data for LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models understand graph structured data?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_understand_graph_structured_data"
        },
        {
            "paper_title": "Can language models solve graph problems in natural language?",
            "rating": 2,
            "sanitized_title": "can_language_models_solve_graph_problems_in_natural_language"
        },
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Standing on the shoulders of giant frozen language models",
            "rating": 2,
            "sanitized_title": "standing_on_the_shoulders_of_giant_frozen_language_models"
        },
        {
            "paper_title": "The power of scale for parameter-efficient prompt tuning",
            "rating": 2,
            "sanitized_title": "the_power_of_scale_for_parameterefficient_prompt_tuning"
        },
        {
            "paper_title": "Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training",
            "rating": 2,
            "sanitized_title": "knowledge_graph_based_synthetic_corpus_generation_for_knowledgeenhanced_language_model_pretraining"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.0162705,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Let Your Graph Do the Talking: Encoding Structured Data for LLMs
8 Feb 2024</p>
<p>Bryan Perozzi <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#98;&#112;&#101;&#114;&#111;&#122;&#122;&#105;&#64;&#97;&#99;&#109;&#46;&#111;&#114;&#103;">&#98;&#112;&#101;&#114;&#111;&#122;&#122;&#105;&#64;&#97;&#99;&#109;&#46;&#111;&#114;&#103;</a>. 
Google Research</p>
<p>Bahare Fatemi 
Google Research</p>
<p>Dustin Zelle 
Google Research</p>
<p>Anton Tsitsulin 
Google Research</p>
<p>Mehran Kazemi 
Google Research</p>
<p>Rami Al-Rfou 
Waymo Research</p>
<p>Jonathan Halcrow 
Google Research</p>
<p>Let Your Graph Do the Talking: Encoding Structured Data for LLMs
8 Feb 20242682B2E69774048A4784AE7A53B69AEAarXiv:2402.05862v1[cs.LG]
How can we best encode structured data into sequential form for use in large language models (LLMs)?In this work, we introduce a parameter-efficient method to explicitly represent structured data for LLMs.Our method, Graph-Token, learns an encoding function to extend prompts with explicit structured information.Unlike other work which focuses on limited domains (e.g., knowledge graph representation), our work is the first effort focused on the general encoding of structured data to be used for various reasoning tasks.We show that explicitly representing the graph structure allows significant improvements to graph reasoning tasks.Specifically, we see across the board improvements -up to 73% points -on node, edge and, graph-level tasks from the GraphQA benchmark.</p>
<p>Introduction</p>
<p>There has been an explosion of recent excitement around using LLMs (Vaswani et al., 2017;Devlin et al., 2018;Radford et al., 2018;Raffel et al., 2020;Brown et al., 2020;Touvron et al., 2023;Zhao et al., 2023) to represent, process, and analyze textual data.These models typically take sequential text as their input but recent work has extended inputs to spatial and temporal modalities (e.g., image (Chen et al., 2022) and video (Arnab et al., 2021)).</p>
<p>Despite their success, current realizations of LLMs have noticeable problems -including a tendency to generate outputs which are untrue or unsupported by their prompt, commonly referred to as hallucinations (Wang et al., 2023a).Another intimately related issue is the problem of freshness, where the knowledge required to answer a query exists only after an LLM's training date (Vu et al., 2023).One mitigation for these problems is through the enrichment of the prompt with additional factual and fresh data.As Kadavath et al. (2022)  encoding, e.g., (Fatemi et al., 2024;Wang et al., 2023b;Stechly et al., 2023), b) This work proposes using GraphToken, a learned graph prompt function to explicitly encode graphs in a parameter efficient way.</p>
<p>with new and supporting information, they are capable of adapting their parametric beliefs to effectively incorporate new evidence.</p>
<p>An automatic way to enrich the input context of an LLM with factual and fresh information is through Retrieval Augmented Generation (RAG) (Khandelwal et al., 2019;Guu et al., 2020).RAG works by augmenting the prompt with additional relevant, factual and fresh information.Sources for RAG might include web searches or private databases.Often this information is in the form of structured datadata that has complex dependencies between different, discrete parts of the whole.For example, private relational databases, social networks, or molecules all have relational information between their discrete data items.</p>
<p>Structured data is ubiquitous in the real world -it surrounds our daily lives -and understanding how to represent this data optimally for its inclusion in LLMs is a crucial research question.The predominant mode of encoding structured data for LLMs is to use various types of hand-crafted, textbased serialization (Guo et al., 2023;Wang et al., 2023b;Stechly et al., 2023) (See Figure 1 (a)).This approach can impose significant decoding complexity for the lan-guage model: from any text description, the model must first correctly decode and understand the structure before it can utilize the information.Recently, Fatemi et al. (2024) demonstrated that pure text representations of structured data are insufficient for graph reasoning with LLMs.They show that LLMs are not able to utilize structure efficiently when posed with common reasoning tasks that are easily answered by classical graph algorithms.This highlights the need to explore better and more expressive ways of representing structured data to an LLM.</p>
<p>In this paper, we propose GraphToken (Figure 1 (b)), a parameter-efficient method for representing structured data for LLMs.Pre-training LLMs on text corpora closely related to the desired reasoning task can enhance performance, but it can be computationally expensive, particularly for larger models.Additionally, fine-tuning requires domainspecific data and human expertise, further increasing costs.</p>
<p>Inspired by recent advancements in parameter-efficient finetuning (Lester et al., 2021;Xu et al., 2023), our method, GraphToken, learns an encoding function that generates fine-tuned soft-token prompts.The soft-token prompt extends a textual prompt with explicit GraphToken encoded structural information, allowing us to train only a trivial number of GraphToken parameters when compared to the total LLM parameter budget.</p>
<p>Our work is the first to develop parameter-efficient encoders specifically for general reasoning tasks on structured data.We demonstrate that explicitly representing structure leads to significant improvement on the comprehensive GraphQA benchmark (Fatemi et al., 2024).</p>
<p>Our Contributions.We propose the following innovations:</p>
<p>• GraphToken, a novel parameter-efficient encoder for structured data inclusion in LLMs.• Extensive experiments on various graph reasoning tasks showing that our method significantly improves LLM capabilities.• Analysis demonstrating that the GraphToken encoder generalizes to both unseen tasks and graphs.</p>
<p>Background</p>
<p>We introduce the related work in LLMs, prompting methods, Graph Neural Networks (GNNs), graph encoders, and graph models combined with LLMs.</p>
<p>Large Language Models</p>
<p>Pre-Trained Large Language Models (LLMs): Language models (Rosenfeld, 2000;Zhao et al., 2023) are probabilistic models that assign probabilities to sequences of words by breaking the probability of a sequence into the product of the probabilities of the next tokens given the previous ones.While earlier models were mainly based on N-gram models (Jurafsky, 2021), newer models adopted neural approaches with the advent of distributed word representations (Bengio et al., 2000;Mikolov et al., 2013).The increased power offered by the neural language models and the increase in model and dataset sizes has led to a new learning paradigm where large language models (LLMs) are pre-trained in an unsupervised way on massive amounts of textual data and are used as base (foundation) models (Devlin et al., 2018;Radford et al., 2019).For each downstream application, the base model is fine-tuned on small amounts of task-specific data to adapt the model to the task.</p>
<p>Parameter-Efficient Fine-Tuning: With the rapid growth in the number of parameters for the state-of-the-art LLMs (Achiam et al., 2023;Team et al., 2023) fine-tuning for each downstream task has become prohibitively expensive in both time and resources.The goal of parameter-efficient finetuning (PEFT) (Xu et al., 2023) is to adapt models to new tasks by updating only a small number of (possibly new) parameters.There are a few dominant PEFT approaches:</p>
<p>• Adapter-based approaches (Houlsby et al., 2019;He et al., 2021) hold the LLM parameters frozen and add new trainable parameters to various parts of the model, with the main differentiating factor between different approaches being where the adapter parameters are added.• LoRA and its variants (Hu et al., 2021;Edalati et al., 2022;Valipour et al., 2022) similarly hold the LLM parameters frozen and add new trainable parameters, however these trainable parameters are added to the frozen LLM parameters such that the fine-tuned LLM is identical in architecture to the initial LLM, but with only those added parameters update.• Partial fine-tuning and partial masking approaches (Zhao et al., 2020;Zaken et al., 2021) only fine-tune or mask a subset of the LLM parameters -no new parameters are introduced.• Finally, soft-prompt approaches (Li &amp; Liang, 2021;Lester et al., 2021) prepend tokens with learnable parameters to the beginning of the LLM input or to the beginning of every LLM layer -like adapter-based and LoRA approaches, they hold the actual LLM parameters frozen.</p>
<p>Our work falls under the umbrella of soft-prompt approaches but can be extended to other PEFT approaches as well.Most relevant to our work is the work of Levine et al. (2022), where the input is fed into a separate trainable neural network to produce the soft-prompt.We extend this to encoding structured data input via a GNN to produce the LLM soft-prompt.</p>
<p>Graph Encoding with Neural Networks</p>
<p>The field of graph representation learning (Chami et al., 2022) seeks ways to represent structured data (i.e., discrete and relation) in a continuous domain -typically for use in a downstream machine learning task.While seminal work like DeepWalk (Perozzi et al., 2014) popularized the node embedding problem, later work utilized GNNs to generalize and learn representations of the entire graph (graph embeddings).Many approaches learning graph representations (node or graph embeddings) have followed (Tsitsulin et al., 2018;Xie et al., 2022).</p>
<p>Graphs and LLMs</p>
<p>The confluence of graph representation learning and reasoning with LLMs is a rapidly growing field of researchlike language, structured data surrounds us but, unlike LLM input, it is not sequential.Some of the first graphs in the literature are knowledge graphs as in (Agarwal et al., 2020), where the retrieval corpus of a retrieval LLM is augmented with text-encoded knowledge graphs.Ye et al. ( 2023) utilize instruction fine-tuned LLMs for node classification.Similarly, Chen et al. (2023b) leverage LLMs to enhance graph learning models by incorporating rich text attributes.Wang et al. (2023b) showed that language models demonstrate preliminary abilities for graph reasoning tasks.Later, Fatemi et al. (2024) proposed GraphQA -a comprehensive benchmark to systematically evaluate models for graph reasoning tasks -finding that graph reasoning tasks are currently difficult and that scaling laws do not seem to apply.Finally, while there is a growing body of work in pre-training, finetuning, and prompt-tuning with GNNs by themselves (Fang et al., 2023;Liu et al., 2023), the research, though conceptually similar, differs crucially from our work.GNN-based approaches lack the textual understanding capabilities that are central to the integration of LLMs with graph learning and reasoning.</p>
<p>GraphToken</p>
<p>When considering how to pass structured data to an LLM there are largely two families of options: (1) encoding it as lexical tokens for LLM embedding as in (Fatemi et al., 2024) or (2) encoding it directly to a continuous representation via a neural network -skipping any LLM token embedding.While representing a graph as a sequence of lexical tokens has benefits in terms of interpretability, there is often no clear choice in what order to sequentially write the structured data.We believe a text encoding of structured data prohibits rich, concise, and expressive representations.Instead, our method eschews representing a graph in text in favor of directly producing -using a GNN as an encoderthe continuous representations for the LLM input.We refer to these new graph encoder learned soft-tokens in the LLM embedding space as "graph tokens."</p>
<p>To maintain the reasoning and language capabilities of the LLM, we freeze its parameters and teach the graph encoder to align its output representations with the LLM embedding space: we learn only those parameters of the graph encoder during the training process.This reduces computational requirements significantly (graph encoder parameters constitute a trivial sum compared to the LLM).During our tests, the LLM is prompted with the output of the graph encoder and a task about the graph, for example: 'Does this graph contain a cycle?'.As such, the quality of the results is purely a function of how well the graph encoder represents the answer to the task and how well the LLM interprets that output.</p>
<p>Architecture</p>
<p>An overview of the architecture is provided in Figure 2. At a high level, our model only has two components.First, the graph encoder takes a graph as input and outputs a fixed number of token embeddings.These tokens are then prepended to the sequence of initial token embeddings in the prompt for an LLM, which is then decoded to produce an answer as text.</p>
<p>Graph Encoder.GNN models range from simple averaging methods to complex models with multi-headed attention.</p>
<p>Thus there are a wide variety of graph representations possible.We suspect that some of these representations are more suited to be consumed by an LLM.Therefore, we conducted a thorough study that includes several well-known graph encoder choices in Section 4.2.Our graph encoder takes the relational structure of the graph as input, using some form of graph positional encoding as node features (either learned, Laplacian, or a combination thereof) -see Section 4.2.2 for details.) Next, we apply a GNN to obtain a representation of the graph, which we read out using one of a few different techniques techniques depending on the task.</p>
<p>• For graph-level tasks (e.g., cycle check) we do global pooling for readout, taking the mean or sum of the representations over all of the nodes.• For node-level tasks (e.g., node degree) we separately output the representation of each node.This can be optionally concatenated with a graph-level pooling.• For edge-level tasks (e.g., edge existence), we use a global representation or the two node-level representations concatenated.</p>
<p>We note that the exact option for readout used (e.g.mean or sum pooling) is a hyper-parameter chosen during model selection.Whichever the readout technique, this representation is then projected onto the space of tokens used by the LLM with a final dense layer.</p>
<p>LLM.</p>
<p>For the experiments in the paper we use PaLM 2(Anil et al., 2023), however, our method generalizes to nearly any LLM in use today.For our purposes, any language model which can accept a sequence of token embeddings and produce text is acceptable, so long as it is possible to compute a gradient with respect to part of the input sequence.</p>
<p>Training procedure</p>
<p>Our training procedure is very similar to that used by soft prompting methods (Lester et al., 2021).The training input consists of triples (G, T, A) where G is a graph structure, T is a statement or question describing the task (e.g., 'Does this graph contain a cycle?' for cycle check) and A is the ground truth answer ('Yes, there exists a cycle in this graph.').</p>
<p>In the forward pass, we compute the augmented query Q = E(G)||T (T ), concatenating the GraphToken encoding of the graph E(G) with the initial embedding of the task textual representation, T (T ).</p>
<p>We train by optimizing the final LLM perplexity (total loglikelihood), L(A | Q), of the expected answer A with respect to the augmented query, Q.We minimize this loss, back-propagating the gradient of L with respect to E(G) to the parameters of the GraphToken encoder -keeping all LLM parameters frozen.We use the Lion optimizer (Chen et al., 2023a) with a learning rate α = 0.05.</p>
<p>Experiments</p>
<p>In this section, we summarize the key experiments conducted with GraphToken.We begin by highlighting some of the most exciting results from our analysis here:</p>
<p>• R1: GraphToken demonstrates superior performance compared to established baselines across a comprehensive range of graph reasoning tasks, including graph-level, node-level, and edge-level tasks.• R2: The performance of different graph convolution architectures varies across tasks.This highlights the importance of carefully choosing the right architecture for the specific graph reasoning problem at hand.• R3: By intentionally breaking equivariance, we enhance GraphToken's graph reasoning capabilities.</p>
<p>Datasets.We conduct our experiments on the graph reasoning tasks proposed in GraphQA (Fatemi et al., 2024).This dataset presents multiple graph reasoning problems with different difficulty levels.These tasks can be categorized as follows.</p>
<p>• Graph-level.node count (counting the number of nodes in a graph), edge count (counting the number of edges in a graph), cycle check (determining whether a graph contains a cycle), and triangle counting (counting the number of triangles in a graph).• Node-level.node degree (calculating the degree of a given node in a graph), and connected nodes (finding all the nodes that are connected to a given node in a graph), • Edge-level.reachability (finding if there is a path from one node to another), edge existence (whether a given edge exists in a graph, and shortest path (finding the length of the shortest path from one node to another).</p>
<p>Setting.We implemented GraphToken in Tensorflow (Abadi et al., 2015) using the TF-GNN library (Ferludin et al., 2023).The LLM used in our experiments is the instruction-fine-tuned Flan (Chung et al., 2022) checkpoint of PaLM 2 S (Anil et al., 2023).Experiments were carried out on Google TPUv3 and TPUv5e (Jouppi et al., 2017).</p>
<p>Table 1.Results comparing GraphToken against prompt engineering and soft prompting on graph reasoning tasks using the GraphQATest benchmark (Fatemi et al., 2024), by simple accuracy.We see that GraphToken substantially improves LLM performance on all graph, node, and edge-level tasks.The first best result for each task is highlighted in bold and the second best result is highlighted with an underline.Model selection was performed by evaluating performance on GraphQA Train</p>
<p>Experiment 1: GraphToken Performance</p>
<p>In this experiment, we rigorously evaluate the performance of GraphToken against the following comprehensive set of established baselines:</p>
<p>• ZERO-SHOT.In this approach, the model is given a task description and immediately asked to produce the desired output.</p>
<p>No additional examples or demonstrations are provided.• FEW-SHOT.This approach provides the model with a few examples of the task and their desired outputs (Brown et al., 2020).Unlike traditional training, these examples are included directly in the prompt, allowing the model to learn and adapt during the inference.• COT.Chain-of-thought (CoT) prompting (Wei et al., 2022) provides examples each showing step-by-step reasoning, teaching the LLM to generate its own thought processes for tackling new tasks.• ZERO-COT.Zero-shot CoT (Kojima et al., 2022) builds upon Chain-of-Thought (CoT) prompting by eliminating the need for training examples.The LLM generates its own step-by-step reasoning process using a simple trigger phrase like "Let's think step by step".• COT-BAG.BAG prompting (Wang et al., 2023b) extends COT to improve the performance of LLMs on graph-related tasks by appending "Let's construct a graph with the nodes and edges first" to the prompt.• SOFT-PROMPT.This approach uses the standard soft prompt tuning of Lester et al. (2021).It optimizes a global static prompt which is shared across problem instances to improve task performance.Unlike our proposed method, it does not have access to the graph information, making the results of this approach equivalent to that of a majority classifier.</p>
<p>Results.The results of this experiment, summarized in Table 1, demonstrate that GraphToken significantly outperforms existing methods on all graph, node, and edgelevel tasks.While SOFT-PROMPT achieves the second best score on some tasks, this is mainly due to its ability to predict majority labels.For example, 82% of the questions in cycle check are about existent cycles.Similarly, 54% of the questions are about non-existent edges in edge existence.</p>
<p>Experiment 2: Encoder Design</p>
<p>From the results in Table 1, we can see that graph encoders can significantly improve a LLM's capability on graph reasoning tasks.However the choice of graph encoders has a significant effect on task performance.Here we study how different architecture choices affect the quality of the graph representation for a language model's use, including the choices of the graph convolution, the features available to the network, and the hyper-parameters.</p>
<p>CHOICE: GRAPH CONVOLUTION</p>
<p>This experiment investigates the impact of graph convolution choice on the performance of GraphToken.We evaluate the following diverse set of encoders:  Result: Table 2 shows the results for each model on GraphQA Test .In general, we see that there is no one model that consistently dominates across graph encoding tasks.Instead, we see that different graph encoder architectures have strengths and weaknesses advantages.</p>
<p>There is one notable pattern however, is that the simple linear GNN models perform quite strongly at their respective counting tasks (i.e.NodeSet does well at node count, EdgeSet does well at edge count).However models with non-linear effects are still capable on these tasks (e.g., MHA does well at node count, and MPNN does well on edge count).</p>
<p>CHOICE: GNN FEATURES</p>
<p>Recently, positional node encodings (Wang et al., 2022;Dwivedi et al., 2023;Lim et al., 2023) were proposed to enhance the expressivity of GNNs.On the other hand, in molecular modeling it has been shown recently that nonequivariant encoders can match or exceed quality of equivariant ones (Wang et al., 2023c).This raises a more general question: do GNNs need to be equivariant in order to generalize, especially with extremely powerful decoders, such as LLMs?</p>
<p>We investigate this question by testing the graph reasoning capability of GraphToken with three distinct node featurization settings:</p>
<p>• LPE: Laplacian positional encodings using the normalized Laplacian matrix, as in (Dwivedi et al., 2023).• IDX: unique identity encoding designed to break the GNN equivariance.• LPE+IDX: a concatenation of the above two strategies.</p>
<p>Setting.The experimental setup is similar to 4.2.Node features of dimensionality d = 4 are evaluated for LPE and IDX featurization.Models using LPE+IDX contains node features of size d = 8.</p>
<p>Result.The outcome of this experiment are show in Figure 3, where we see the difference of all models from the SOFTPROMPT baseline (Lester et al., 2021) when evaluted on GraphQA Test .The core result is that learned positional embeddings (Fig. 3b) generally outperform Laplacian position embeddings (Fig 3a) for most encoders and most tasks.These results show that breaking equivariance surprisingly adds additional capabilities for graph reasoning when powerful decoders are present.Some additional observations include:</p>
<p>• Counting Tasks.Learned features seem to provide essential lift for basic counting tasks (node count, edge count, and node degree) in many encoders.• Combination.In some very interesting cases of task and encoder, the combination of both types of features greatly improved model performance (Fig. 3c).For example, GCN and NodeSet significantly improved at the node count task.• Linear models.NodeSet (an encoder which does not use the graph edges) generally benefited from spectral features as they added previously unseen information about the graph structure.We see that breaking equivariance via learned features (Fig. 3b) generally improve the model performance, but the combination of learned and spectral features (Fig. 3c) proves uniquely powerful for some encoders.</p>
<p>PARAMETER USAGE IN GRAPHTOKEN</p>
<p>Setting: We consider the graph convolution evaluation from Section 4.2.1, using LPE features with dimensionality d = 4.</p>
<p>The graph encoders have a latent space of size 128.We then project this into a prompt embedding with approximately 80, 000 parameters in GraphToken .</p>
<p>Results: Table 3 shows the number of parameters used in the graph encoder.Here 'body' refers to the number of parameters in the graph encoder itself, and 'Head' refers to the parameters in the transformation layer to the higherdimensional LLM token space.</p>
<p>Its also insightful to consider the number of parameters used in each of the models.Table 3 specifies total number of parameters used by each GNN architecture.We note that this size is dominated by the total number of parameters in the projection layer to the token space (roughly 11 million).Out of all non-linear architectures, attention-based ones (MHA and HGT) add the most encoder-based parameters.In general, the size of our graph encoder models varies from 17k to 199k parameters.This is significantly smaller than typical LLMs, which currently often contain tens or hundreds of billions of parameters.For example, the open-source LLama2 language model scales from 7 billion to 70 billion param- eters (Touvron et al., 2023).Meanwhile the closed source PaLM 1 model contains 540 billion parameters (Chowdhery et al., 2022).In light of this, we can see that GraphToken is highly parameter-efficient, and significantly improves the graph reasoning capability of a LLM while barely adding any parameters at all.</p>
<p>Discussion</p>
<p>So far we have examined the performance benefits of Graph-Token, and the design choices necessary when building a graph encoder.However several questions remain: (1) What exactly are the encoders doing, and (2) does it generalize?</p>
<p>In this section we seek to provide some insight (if not answers) to these questions, and lay the foundations for future work.</p>
<p>Graph Encoder Analysis</p>
<p>This section studies the properties learned by GraphToken's graph encoders by directly examining the representations they produce.We study both the in-distribution and outof-distribution properties of these encoders.We consider One benefit of studying graphs is data availability: for smallenough graphs, we can generate all possible graphs exhaustively using geng (McKay et al., 1981).The evaluation goes as follows: First, we train an encoder on a task from GraphQA (e.g.cycle check).Then, to evaluate the cross-task generalizability of the different encoders we train a kNN classifier (or regressor) with k = 5 on the representations of (i) an exhaustive set of connected graphs with 8 nodes (called graph8c in Balcilar et al. (2021)) and (ii) an exhaustive set of tree graphs with 15 nodes.We note that because we are generating a large set of graphs (e.g.there are 11117 graphs of size 8) and only trained on GraphQA Train (1000 instances), the vast majority of the graphs we are using here are unseen.As an illustration, a UMAP (McInnes et al., 2018) visualization of the embeddings for all 8 node graphs using two GNN encoders is presented in Figure 4.</p>
<p>Results.Since we present a lot of experiments and it's hard to cover them all, we focus here on the task of predicting whether a graph is bipartite and outsource the rest to the Appendix.From the basic graph theory we know that, if there is a triangle or an odd cycle in a graph, it can not be bipartite.Therefore, we expect triangle counting and cycle check training objectives to perform well on this task.In Table 4 we can see that this is precisely what happens, with attention-based methods taking the lead.This is an interesting example of generalization from the graph encoders to a new task.</p>
<p>Overall, there is a significant performance gap between different graph encoders, MPNN and attention-based ones being generally the best.We observe significant correlations in performance of in-distribution learning -for instance, GraphToken trained on edge count performs the best on edge count prediction.What is interesting is that node count performs comparably here.This suggests that graph encoders learn some universal features that are applicable to many different downstream tasks.</p>
<p>Future Work</p>
<p>This work opens up an exciting new avenue of exploration for reasoning with structured data and LLMs.Some potential avenues that we consider particularly exciting include:</p>
<p>• This work just considers existing convolutions and measures their effectiveness.An obvious and essential next step is designing graph convolutions that best support LLMs in various graph reasoning tasks.• Evaluating the usefulness of this approach for factual grounding.Can we improve the ability of an LLM to answer questions about the data using prompting over knowledge graphs?Could an LLM answer novel questions about a molecule given a GNN-produced representation of it?• GraphToken improves performance with broken equivariance.Can this result inform other problems with very strong decoder models?• This work examines how a GNN can be used to an enhance LLMs, but what about the reverse?Can we use an LLM to interrogate a GNN to better explain its results or provide higher quality answers?</p>
<p>Conclusions</p>
<p>In this work we have studied the structured data encoding problem for LLMs.Our novel method, GraphToken, learns a graph embedding function through the gradients provided by a frozen LLM.GraphToken is especially well suited for projecting structured data into latent 'prompt space'.It is a parameter-efficient method as it requires only training the graph encoder and does not update LLM parameters.Our extensive experimental analysis across 9 graph reasoning tasks shows that GraphToken greatly improves graph reasoning in LLMs -we observe up to a 73% improvement on the GraphQA benchmark.</p>
<p>There is still much to do!We believe that our approach is fundamental for adapting new structured data sources to LLMs (which are expensive and time consuming to train), and presents a very attractive way of improving fundamental problems in LLMs, including hallucinations, factuality, and freshness.</p>
<p>Figure 1 .
1
Figure1.Graph encoding options for a frozen LLM.a) Fixed encoding, e.g.,(Fatemi et al., 2024;Wang et al., 2023b;Stechly et al., 2023), b) This work proposes using GraphToken, a learned graph prompt function to explicitly encode graphs in a parameter efficient way.</p>
<p>Figure 3 .
3
Figure3.Effect of varying node features used in the graph encoder.Results shown are performance difference from the SOFT PROMPT baseline on GraphQATest.We see that breaking equivariance via learned features (Fig.3b) generally improve the model performance, but the combination of learned and spectral features (Fig.3c) proves uniquely powerful for some encoders.</p>
<p>Figure 4 .
4
Figure 4. UMAP (McInnes et al., 2018) projection of GraphToken embeddings produced by two different encoders, colored by the diameter of a graph.We plot all 8-node graphs.</p>
<p>A visual overview of the architecture of GraphToken.The framework takes a graph and a corresponding question as input.The graph encoder takes the graph and generates graph tokens.The question is tokenized and embedded to question tokens.A frozen LLM leverages the graph and question tokens to generate an answer.
GraphToken EncoderGraphGraph ConvolutionGraph ConvolutionFeature Encoding…Readout…GraphTokensFrozen LLMIs there a cycle in this graph? QuestionIsthereacycleinthisgraph?Embedding lookup+positional encoding…Question TokensYes, there is a cycle in this graph.
OutputFigure 2.</p>
<p>Table 2 .
2
Study of individual graph encoder performance on GraphQATest tasks.Note that there is 'no free lunch' here -no single encoder examined dominates across all tasks.Edge count Cycle check Triangle counting Node degree Connected nodes Reachability Edge existence Shortest path
Graph TasksNode TasksEdge TasksGCN GIN MPNN Method Node count Non-linear 0.746 0.704 0.792 HGT 0.2520.056 0.052 0.368 0.0840.964 0.898 0.956 0.9340.208 0.194 0.348 0.2340.264 0.252 0.962 0.2660.264 0.18 0.25 0.1840.918 0.902 0.934 0.9440.68 0.65 0.648 0.7180.604 0.586 0.638 0.6MHA0.9120.2640.9620.2660.5520.2440.9320.7380.608LinearNode Set Edge Set0.996 0.6180.080 0.4260.948 0.9640.198 0.2280.19 0.220.118 0.0960.942 0.9040.596 0.5920.568 0.568learn different ways of passing messages (based on theattention mask). (Dwivedi &amp; Bresson, 2021)• Heterogeneous Graph Transformer (HGT): Anotheradoption of transformer style attention (we note that itcan be applied to non-heterogeneous graphs as well).(Hu et al., 2020)• Linear Aggregation In addition to the popular en-coders from the literature, we also evaluated a familyof models which use linear aggregation of features, asthis has been shown to work surprisingly well on anumber of tasks (Bojchevski et al., 2020).Setting: The experimental setup is similar to the experi-ment in Section 4.1. Again, GraphQA Train performance wasused for model selection, and we report the correspondingmodel's results on GraphQA Test .
-Node Set: This model simply pools all the node features in the graph together.-EdgeSet: This model simply pools all the edge features together (edge features are defined as the concatenation of its two nodes' features).</p>
<p>Table 3 .
3
Total number of parameters in the graph encoder.
BodyHeadGCN17,1521.1 × 10 7GIN17,1521.1 × 10 7MPNN83,9681.1 × 10 7HGT198,7881.1 × 10 7MHA101,3761.1 × 10 7Node Set04.1 × 10 5Edge Set07.4 × 10 5</p>
<p>Table 4 .
4
Predicting bipartiteness using graph encoders trained for different tasks, measured on all graphs with 8 nodes.Observe that graph encoders trained on cycle check and triangle counting generalize well to bipartiteness detection.Edge count Cycle check Triangle counting Node degree Connected nodes Reachability Edge existence Shortest path
Graph TasksNode TasksEdge TasksGCN GIN MPNN Method Node count Non-linear 53.82 51.09 68.01 HGT 50.0053.28 53.27 71.34 54.3555.46 52.74 56.82 68.5350.00 51.91 76.82 95.0350.00 53.26 60.13 50.2754.64 53.57 60.95 59.8150.00 51.36 61.77 68.8548.48 52.17 62.58 74.5851.60 52.18 54.37 50.00MHA50.2766.3987.0072.1458.7466.3851.6354.1264.45LinearNode Set Edge Set56.55 50.8257.38 50.8256.30 50.8255.74 50.5556.29 50.5456.28 50.5455.73 50.8257.93 50.8256.56 50.549 tasks in total: total number of edges; maximum nodedegree; graph diameter; number of triangles; average localclustering coefficient; largest core number; average shortestpath length; testing planarity; testing bipartiteness.</p>
<p>Xu, L., Xie, H., Qin, S.-Z.J., Tao, X., and Wang, F. L.Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment.arXiv preprint arXiv:2312.12148,2023.Cited on page 2.
Ye, R., Zhang, C., Wang, R., Xu, S., and Zhang, Y.Natural language is all a graph needs. arXiv preprintarXiv:2308.07134, 2023. Cited on page 3.Zaken, E. B., Ravfogel, S., and Goldberg, Y. Bitfit:Simple parameter-efficient fine-tuning for transformer-based masked language-models.arXiv preprintarXiv:2106.10199, 2021. Cited on page 2.Zhao, M., Lin, T., Mi, F., Jaggi, M., and Schütze, H. Mask-ing as an efficient alternative to finetuning for pretrainedlanguage models. arXiv preprint arXiv:2004.12406, 2020.Cited on page 2.Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y.,Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey oflarge language models. arXiv preprint arXiv:2303.18223,2023. Cited on pages 1 and 2.</p>
<p>Table 5 .
5
Average local clustering coefficient MSE measured on all connected graphs with 8 nodes.We highlight the best performance per training task in columns.Edge count Cycle check Triangle counting Node degree Connected nodes Reachability Edge existence Shortest path
Graph TasksNode TasksEdge TasksGCN GIN MPNN Method Node count Non-linear 1.62 2.18 1.03 HGT 2.631.67 2.29 0.95 2.252.12 2.45 1.38 2.084.49 2.60 0.81 1.234.49 2.44 1.50 2.491.73 2.31 1.34 2.174.49 3.73 1.68 1.9016.57 2.88 1.87 1.623.75 3.37 1.47 2.52MHA2.691.011.230.961.561.252.081.591.29LinearNode Set Edge Set2.59 2.222.56 2.222.59 2.222.59 2.222.58 2.242.60 2.232.58 2.222.58 2.222.56 2.23</p>
<p>Table 6 .
6
Degree accuracy on all connected graphs with 8 nodes.We highlight the best performance per training task in columns.Edge count Cycle check Triangle counting Node degree Connected nodes Reachability Edge existence Shortest path
Graph TasksNode TasksEdge TasksGCN GIN MPNN Method Node count Non-linear 57.46 56.86 69.45 HGT 55.2056.65 56.30 69.60 55.7052.46 54.55 67.19 56.5440.09 48.75 71.84 60.1740.09 55.59 64.56 56.6257.42 57.56 67.62 57.6540.09 40.14 61.37 58.0215.73 50.81 58.66 59.0640.26 44.83 63.18 55.46MHA54.8664.3362.8665.6361.6763.2256.9861.6063.97LinearNode Set Edge Set54.66 63.4854.91 63.3754.98 63.0755.06 63.5554.78 63.0854.64 63.3754.50 63.4754.94 63.0654.72 63.44</p>
<p>Table 7 .
7
Diameter Accuracy on all connected graphs with 8 nodes.We highlight the best performance per training task in columns.Edge count Cycle check Triangle counting Node degree Connected nodes Reachability Edge existence Shortest path
Graph TasksNode TasksEdge TasksGCN GIN MPNN Method Node count Non-linear 66.86 66.06 76.92 HGT 63.9767.81 64.87 76.86 65.2466.70 63.97 73.63 66.8837.37 61.09 78.33 70.4537.37 64.98 74.78 65.3068.91 66.43 77.18 68.4537.37 37.80 74.42 69.6452.13 60.65 69.56 68.9755.13 54.82 76.23 66.04MHA63.7674.1776.0074.0373.5074.7168.4569.3272.95Linear</p>
<p>Table 8 .
8
k-Core Accuracy on all connected graphs with 8 nodes.We highlight the best performance per training task in columns.Edge count Cycle check Triangle counting Node degree Connected nodes Reachability Edge existence Shortest path
Graph TasksNode TasksEdge TasksGCN GIN MPNN Method Node count Non-linear 69.49 68.03 87.42 HGT 63.9269.15 65.98 87.54 65.2966.61 64.85 81.81 67.0058.33 62.67 88.63 70.0158.33 66.74 80.30 65.4469.16 67.84 83.48 67.3258.33 58.84 80.08 68.3525.18 63.34 71.01 70.0861.55 59.08 82.05 65.13MHA64.3080.8073.4980.8176.9878.8369.4374.2175.92LinearNode Set Edge Set68.23 66.3068.74 65.7868.50 65.5868.71 66.1568.07 65.7667.99 65.9168.85 65.9468.17 65.7768.70 65.71</p>
<p>Table 9 .
9</p>
<h1>edges Accuracy on all connected graphs with 8 nodes.We highlight the best performance per training task in columns.</h1>
<p>Graph TasksNode TasksEdge TasksMethod Node count Edge count Cycle check Triangle counting Node degree Connected nodes Reachability Edge existence Shortest pathNon-linearGCN GIN MPNN HGT38.91 38.13 86.58 35.6339.19 37.33 86.72 37.4535.94 36.57 53.15 38.2311.60 31.66 84.56 40.3911.60 37.74 52.12 37.1440.24 38.34 66.01 37.8011.60 11.88 50.70 39.682.19 31.45 41.96 39.7414.58 25.92 59.95 36.86MHA35.8555.3245.0453.5247.8949.4439.6942.8446.17LinearNode Set Edge Set40.06 37.9340.14 38.1139.40 38.0540.15 37.9239.97 38.0539.72 37.6739.88 37.6439.79 37.8239.89 37.91</p>
<p>Table 12 .
12</p>
<h1>of triangles MSE on all connected graphs with 8 nodes.We highlight the best performance per training task in columns.Edge count Cycle check Triangle counting Node degree Connected nodes Reachability Edge existence Shortest path</h1>
<p>Graph TasksNode TasksEdge TasksGCN GIN MPNN Method Node count Non-linear 132.94 152.13 8.33 HGT 191.14129.03 168.35 7.51 170.71164.53 182.95 32.08 165.88316.07 201.64 4.56 126.92316.07 169.71 51.90 172.84127.17 156.16 27.18 160.29316.07 251.23 51.04 156.10690.03 200.45 124.89 136.22293.53 251.65 41.73 175.45MHA197.3630.2796.5627.1059.5852.42138.4880.2260.72LinearNode Set Edge Set167.81 181.44168.72 181.21167.33 181.18167.40 181.32167.90 180.86167.96 179.44168.57 181.08169.38 181.68166.13 181.40</p>
<p>Table 13 .
13
Degree Accuracy on all trees with 15 nodes.We highlight the best performance per training task in columns.Edge count Cycle check Triangle counting Node degree Connected nodes Reachability Edge existence Shortest path
Graph TasksNode TasksEdge TasksGCN GIN MPNN Method Node count Non-linear 53.57 60.35 79.37 HGT 54.8855.15 58.79 78.36 55.3355.24 56.36 59.18 55.3425.91 55.11 72.35 58.6525.91 59.88 62.38 54.3354.86 68.04 65.90 58.8425.91 42.01 57.37 57.2711.08 66.72 57.33 57.4336.51 55.25 58.45 55.34MHA59.1761.6160.3857.1854.9961.0052.2958.5653.95LinearNode Set Edge Set65.64 69.5966.32 69.8765.93 69.4466.10 69.4066.13 69.8665.95 69.5666.28 69.3266.22 69.5565.82 69.66</p>
<p>Table 14 .
14
Diameter Accuracy on all trees with 15 nodes.We highlight the best performance per training task in columns.Edge count Cycle check Triangle counting Node degree Connected nodes Reachability Edge existence Shortest path
Graph TasksNode TasksEdge TasksGCN GIN MPNN Method Node count Non-linear 50.77 58.29 54.24 HGT 57.1550.36 54.44 54.68 54.8849.54 52.24 54.97 54.9025.97 49.41 59.29 57.5825.97 51.47 67.65 57.0550.01 59.62 63.80 65.2225.97 24.11 54.13 54.516.77 58.77 52.05 58.7026.64 46.27 59.48 53.07MHA53.9556.6360.4154.6253.3956.0752.8555.1751.70LinearNode Set Edge Set61.89 56.5762.68 56.1962.74 56.2762.36 56.8361.99 56.2561.93 56.5362.34 56.3162.49 56.7262.40 56.84</p>
<p>Table 15 .
15
Shortest path MSE on all trees with 15 nodes.We highlight the best performance per training task in columns.Edge count Cycle check Triangle counting Node degree Connected nodes Reachability Edge existence Shortest path
Graph TasksNode TasksEdge TasksGCN GIN MPNN Method Node count Non-linear 12.95 9.57 4.19 HGT 10.5712.31 10.69 4.54 10.9612.62 11.32 9.82 11.6526.17 11.88 4.92 9.0926.17 11.03 6.87 12.5612.22 8.37 6.10 8.1726.17 19.35 11.06 10.7649.78 9.76 12.10 9.2621.71 14.39 11.01 10.98MHA10.499.889.5111.2212.7510.5213.3110.0912.78LinearNode Set Edge Set10.20 9.9210.05 9.8710.13 9.9210.11 9.9310.17 9.8810.21 9.8810.07 10.0110.18 9.9110.03 9.87
AcknowledgementsWe thank Oleksandr Ferludin, Johannes Gasteiger, Silvio Lattanzi, Vahab Mirrokni and Jan Pfeifer for discussions about the work.A. AppendixA.1.Graph Encoders Notation.We briefly describe the notation we will use.The graph G = (V, E) contains the set of V nodes and E edges.While we will only discuss simple graphs, everything discussed can be extended to heterogeneous graphs w.l.o.g.(Battaglia et al., 2018;Ferludin et al., 2023).Using the notation ofFerludin et al. (2023), a GNN has two primary operations.First, a next state function (NEXTSTATE) which computes the hidden state h v of a node (or edge, m (u,v) ) given information from its neighbors and its previous state, and an aggregation function (EDGEPOOL) which pools information for a node's immediate neighborhood into a fixed size representation.More formally, we can say that the next state of a node is:Then the pooled messages m (i+1) v are defined as follows:Different realizations of the NEXTSTATE and EDGEPOOL functions can implement a wide variety of GNN operations.This can include powerful models which use Transformer style attention instead of the provided graph edges(Dwivedi &amp; Bresson, 2021).The architecture of NodeSet and EdgeSet is shown in Figure5.Other GNN models have graph convolutions before node/edge states are read out.A.2. Additional experimentsWe present additional results for graph encoder analysis.Tables 5-15 present additional results on more graph properties, as well as experiments on tree-structured graphs of size 15.In general, complete graph populations demonstrate significantly better performance than trees -we can attribute that to the fact that GraphToken was trained on diverse sets of data, and trees are somewhat out-of-distribution.Nevertheless, for all considered cases the best overall encoder model achieved better results than naïve set encodings.
TensorFlow: Largescale machine learning on heterogeneous systems. M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen, C Citro, G S Corrado, A Davis, J Dean, M Devin, S Ghemawat, I Goodfellow, A Harp, G Irving, M Isard, Y Jia, R Jozefowicz, L Kaiser, M Kudlur, J Levenberg, D Mané, R Monga, S Moore, D Murray, C Olah, M Schuster, J Shlens, B Steiner, I Sutskever, K Talwar, P Tucker, V Vanhoucke, V Vasudevan, F Viégas, O Vinyals, P Warden, M Wattenberg, M Wicke, Y Yu, X Zheng, 2015Software available from tensorflow.org. Cited on page 4</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.087742023Gpt-4 technical report. arXiv preprintCited on page 2</p>
<p>Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. O Agarwal, H Ge, S Shakeri, R Al-Rfou, arXiv:2010.126882020arXiv preprintCited on page 3</p>
<p>. R Anil, A M Dai, O Firat, M Johnson, D Lepikhin, A Passos, S Shakeri, E Taropa, P Bailey, Z Chen, arXiv:2305.104032023Palm 2 technical report. arXiv preprint</p>
<p>Vivit: A video vision transformer. A Arnab, M Dehghani, G Heigold, C Sun, M Lučić, C Schmid, ICCV. 2021Cited on page 1</p>
<p>Breaking the limits of message passing graph neural networks. M Balcilar, P Héroux, B Gauzere, P Vasseur, S Adam, P Honeine, ICML. 2021Cited on page 8</p>
<p>Relational inductive biases, deep learning, and graph networks. P W Battaglia, J B Hamrick, V Bapst, A Sanchez-Gonzalez, V Zambaldi, M Malinowski, A Tacchetti, D Raposo, A Santoro, R Faulkner, C Gulcehre, F Song, A Ballard, J Gilmer, G Dahl, A Vaswani, K Allen, C Nash, V Langston, C Dyer, N Heess, D Wierstra, P Kohli, M Botvinick, O Vinyals, Y Li, R Pascanu, 2018Cited on page 13</p>
<p>Scaling graph neural networks with approximate pagerank. Y Bengio, R Ducharme, P Vincent, A Bojchevski, J Gasteiger, B Perozzi, A Kapoor, M Blais, B Rózemberczki, M Lukasik, S Günnemann, A neural probabilistic language model. NIPS. 2000. 2020KDD. Cited on page 6</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, NeurIPS. 2020</p>
<p>Machine learning on graphs: A model and comprehensive taxonomy. I Chami, S Abu-El-Haija, B Perozzi, C Re, K Murphy, JMLR. 2022Cited on page 3</p>
<p>PaLI: A jointly-scaled multilingual language-image model. X Chen, X Wang, S Changpinyo, A Piergiovanni, P Padlewski, D Salz, S Goodman, A Grycner, B Mustafa, L Beyer, ICLR2022Cited on page 1</p>
<p>Symbolic discovery of optimization algorithms. X Chen, C Liang, D Huang, E Real, K Wang, Y Liu, H Pham, X Dong, T Luong, C.-J Hsieh, arXiv:2302.066752023aarXiv preprintCited on page 4</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Z Chen, H Mao, H Li, W Jin, H Wen, X Wei, S Wang, D Yin, W Fan, H Liu, arXiv preprint 2307.033932023bCited on page 3</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, P Schuh, K Shi, S Tsvyashchenko, J Maynez, A Rao, P Barnes, Y Tay, N Shazeer, V Prabhakaran, E Reif, N Du, B Hutchinson, R Pope, J Bradbury, J Austin, M Isard, G Gur-Ari, P Yin, T Duke, A Levskaya, S Ghemawat, S Dev, H Michalewski, X Garcia, V Misra, K Robinson, L Fedus, D Zhou, D Ippolito, D Luan, H Lim, B Zoph, A Spiridonov, R Sepassi, D Dohan, S Agrawal, M Omernick, A M Dai, T S Pillai, M Pellat, A Lewkowycz, E Moreira, R Child, O Polozov, K Lee, Z Zhou, X Wang, B Saeta, M Diaz, O Firat, M Catasta, J Wei, K Meier-Hellstern, D Eck, J Dean, S Petrov, N Fiedel, 2022Cited on page 7</p>
<p>Scaling instruction-finetuned language models. H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, E Li, X Wang, M Dehghani, S Brahma, arXiv:2210.114162022arXiv preprintCited on page 4</p>
<p>J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. 2018arXiv preprintCited on pages 1 and 2</p>
<p>A generalization of transformer networks to graphs. V P Dwivedi, X Bresson, 202113</p>
<p>Benchmarking graph neural networks. V P Dwivedi, C K Joshi, A T Luu, T Laurent, Y Bengio, X Bresson, JMLR. 24432023Cited on page 6</p>
<p>A Edalati, M Tahaei, I Kobyzev, V P Nia, J J Clark, M Rezagholizadeh, Krona, arXiv:2212.10650Parameter efficient tuning with kronecker adapter. 2022arXiv preprintCited on page 2</p>
<p>Universal prompt tuning for graph neural networks. T Fang, Y Zhang, Y Yang, C Wang, L Chen, 2023Cited on page 3</p>
<p>Talk like a graph: Encoding graphs for large language models. B Fatemi, J Halcrow, B Perozzi, ICLR. 2024Cited on pages 1, 2, 3, 4, and 5</p>
<p>O Ferludin, A Eigenwillig, M Blais, D Zelle, J Pfeifer, A Sanchez-Gonzalez, W L S Li, S Abu-El-Haija, P Battaglia, N Bulut, J Halcrow, F M G De Almeida, P Gonnet, L Jiang, P Kothari, S Lattanzi, A Linhares, B Mayer, V Mirrokni, J Palowitch, M Paradkar, J She, A Tsitsulin, K Villela, L Wang, D Wong, B Perozzi, Tf-Gnn, Graph neural networks in tensorflow. 2023</p>
<p>Neural message passing for quantum chemistry. J Gilmer, S S Schoenholz, P F Riley, O Vinyals, G E Dahl, 2017Cited on page 5</p>
<p>Can large language models understand graph structured data? an empirical evaluation and benchmarking. J Guo, L Du, H Liu, Gpt4graph, arXiv:2305.150662023arXiv preprintCited on page 1</p>
<p>Retrieval augmented language model pre-training. K Guu, K Lee, Z Tung, P Pasupat, M Chang, ICML. 2020Cited on page 1</p>
<p>On the effectiveness of adapterbased tuning for pretrained language model adaptation. R He, L Liu, H Ye, Q Tan, B Ding, L Cheng, J.-W Low, L Bing, L Si, arXiv:2106.031642021arXiv preprintCited on page 2</p>
<p>Parameter-efficient transfer learning for nlp. N Houlsby, A Giurgiu, S Jastrzebski, B Morrone, Q De Laroussilhe, A Gesmundo, M Attariyan, S Gelly, ICML. 2019Cited on page 2</p>
<p>E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, Lora, arXiv:2106.09685Low-rank adaptation of large language models. 2021arXiv preprintCited on page 2</p>
<p>Heterogeneous graph transformer. Z Hu, Y Dong, K Wang, Y Sun, 2020Cited on page 6</p>
<p>. N P Jouppi, C Young, N Patil, D Patterson, G Agrawal, R Bajwa, S Bates, S Bhatia, N Boden, A Borchers, R Boyle, P.-L Cantin, C Chao, C Clark, J Coriell, M Daley, M Dau, J Dean, B Gelb, T V Ghaemmaghami, R Gottipati, W Gulland, R Hagmann, C R Ho, D Hogberg, J Hu, R Hundt, D Hurt, J Ibarz, A Jaffey, A Jaworski, A Kaplan, H Khaitan, D Killebrew, A Koch, N Kumar, S Lacy, J Laudon, J Law, D Le, C Leary, Z Liu, K Lucke, A Lundin, G Mackean, A Maggiore, M Mahony, K Miller, R Nagarajan, R Narayanaswami, R Ni, K Nix, T Norrie, M Omernick, N Penukonda, A Phelps, J Ross, M Ross, A Salek, E Samadiani, C Severn, G Sizikov, M Snelham, J Souter, D Steinberg, A Swing, M Tan, G Thorson, B Tian, H Toma, E Tuttle, V Vasudevan, R Walter, W Wang, E Wilcox, D H Yoon, 2017datacenter performance analysis of a tensor processing unit. SIGARCH Comput. Archit. News. Cited on page 4</p>
<p>. Dan Jurafsky, </p>
<p>N-gram language models. J H Martin, Speech and Language Processing. 20213rd ed.. Cited on page 2</p>
<p>Language models (mostly) know what they know. S Kadavath, T Conerly, A Askell, T Henighan, D Drain, E Perez, N Schiefer, Z Hatfield-Dodds, N Dassarma, E Tran-Johnson, arXiv:2207.052212022arXiv preprintCited on page 1</p>
<p>U Khandelwal, O Levy, D Jurafsky, L Zettlemoyer, M Lewis, arXiv:1911.00172Generalization through memorization: Nearest neighbor language models. 2019arXiv preprintCited on page 1</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, 2017Cited on page 5</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, NeurIPS. 352022Cited on page 5</p>
<p>The power of scale for parameter-efficient prompt tuning. B Lester, R Al-Rfou, N Constant, 2021Cited on pages 2, 4, 5, and 6</p>
<p>Standing on the shoulders of giant frozen language models. Y Levine, I Dalmedigos, O Ram, Y Zeldes, D Jannai, D Muhlgay, Y Osin, O Lieber, B Lenz, S Shalev-Shwartz, A Shashua, K Leyton-Brown, Y Shoham, 2022Cited on page 2</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. X L Li, P Liang, arXiv:2101.001902021arXiv preprintCited on page 2</p>
<p>Sign and basis invariant networks for spectral graph representation learning. D Lim, J Robinson, L Zhao, T Smidt, S Sra, H Maron, S Jegelka, ICLR. 2023Cited on page 6</p>
<p>Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. Z Liu, X Yu, Y Fang, X Zhang, Proceedings of the ACM Web Conference 2023. the ACM Web Conference 20232023Cited on page 3</p>
<p>Uniform manifold approximation and projection for dimension reduction. L Mcinnes, J Healy, J Melville, Umap, arXiv:1802.034262018arXiv preprint</p>
<p>Practical graph isomorphism. B D Mckay, 1981Cited on page 8</p>
<p>T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781Efficient estimation of word representations in vector space. 2013arXiv preprintCited on page 2</p>
<p>Deepwalk: online learning of social representations. B Perozzi, R Al-Rfou, S Skiena, KDD. 2014Cited on page 3</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018Cited on page 1</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019Cited on page 2</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of Machine Learning Research. 2112020Cited on page 1</p>
<p>Two decades of statistical language modeling: Where do we go from here?. R Rosenfeld, Proceedings of the IEEE. 8882000Cited on page 2</p>
<p>Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. K Stechly, M Marquez, S Kambhampati, arXiv:2310.123972023arXiv preprintCited on page 1</p>
<p>Gemini: a family of highly capable multimodal models. G Team, R Anil, S Borgeaud, Y Wu, J.-B Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, arXiv:2312.11805arXiv:2307.09288Open foundation and finetuned chat models. 2023. 20232arXiv preprint</p>
<p>Self-supervised spectral graph representation learning. A Tsitsulin, D Mottin, P Karras, A Bronstein, E Müller, Sgr, arXiv:1811.062372018arXiv preprintCited on page 3</p>
<p>Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation. M Valipour, M Rezagholizadeh, I Kobyzev, A Ghodsi, Dylora, arXiv:2210.075582022arXiv preprintCited on page 2</p>
<p>Attention is all you need. NeurIPS, 30. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, 2017Cited on page 1</p>
<p>Freshllms: Refreshing large language models with search engine augmentation. T Vu, M Iyyer, X Wang, N Constant, J Wei, J Wei, C Tar, Y.-H Sung, D Zhou, Q Le, T Luong, 2023Cited on page 1</p>
<p>Survey on factuality in large language models: Knowledge, retrieval and domain-specificity. C Wang, X Liu, Y Yue, X Tang, T Zhang, C Jiayang, Y Yao, W Gao, X Hu, Z Qi, Y Wang, L Yang, J Wang, X Xie, Z Zhang, Y Zhang, 2023aCited on page 1</p>
<p>Equivariant and stable positional encoding for more powerful graph neural networks. H Wang, H Yin, M Zhang, P Li, ICLR. 2022Cited on page 6</p>
<p>Can language models solve graph problems in natural language? In NeurIPS. H Wang, S Feng, T He, Z Tan, X Han, Y Tsvetkov, 2023bCited on pages 1, 3, and 5</p>
<p>Y Wang, A A Elhag, N Jaitly, J M Susskind, M A Bautista, arXiv:2311.17932Generating molecular conformer fields. 2023carXiv preprintCited on page 6</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, NeurIPS. 2022Cited on page 5</p>
<p>Selfsupervised learning of graph neural networks: A unified review. Y Xie, Z Xu, J Zhang, Z Wang, Ji , S , IEEE TPAMI. 2022Cited on page 3</p>
<p>K Xu, W Hu, J Leskovec, S Jegelka, arXiv:1810.00826How powerful are graph neural networks?. 2018arXiv preprintCited on page 5</p>            </div>
        </div>

    </div>
</body>
</html>
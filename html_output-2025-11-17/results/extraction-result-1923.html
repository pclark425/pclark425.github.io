<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1923 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1923</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1923</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-41.html">extraction-schema-41</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <p><strong>Paper ID:</strong> paper-276813549</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.04462v1.pdf" target="_blank">PALo: Learning Posture-Aware Locomotion for Quadruped Robots</a></p>
                <p><strong>Paper Abstract:</strong> With the rapid development of embodied intelligence, locomotion control of quadruped robots on complex terrains has become a research hotspot. Unlike traditional locomotion control approaches focusing solely on velocity tracking, we pursue to balance the agility and robustness of quadruped robots on diverse and complex terrains. To this end, we propose an end-to-end deep reinforcement learning framework for posture-aware locomotion named PALo, which manages to handle simultaneous linear and angular velocity tracking and real-time adjustments of body height, pitch, and roll angles. In PALo, the locomotion control problem is formulated as a partially observable Markov decision process, and an asymmetric actor-critic architecture is adopted to overcome the sim-to-real challenge. Further, by incorporating customized training curricula, PALo achieves agile posture-aware locomotion control in simulated environments and successfully transfers to real-world settings without fine-tuning, allowing real-time control of the quadruped robot's locomotion and body posture across challenging terrains. Through in-depth experimental analysis, we identify the key components of PALo that contribute to its performance, further validating the effectiveness of the proposed method. The results of this study provide new possibilities for the low-level locomotion control of quadruped robots in higher dimensional command spaces and lay the foundation for future research on upper-level modules for embodied intelligence.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1923.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1923.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PALO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Posture-Aware Locomotion (PALO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end deep RL framework that trains a posture-aware locomotion policy in simulation with asymmetric actor-critic, adversarial motion priors, curricula, and dynamics domain randomization, then deploys the policy zero-shot on a Unitree A1 quadruped.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Posture-aware quadruped locomotion (6D commands: vx, vy, ωz, body height, pitch, roll)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td>up to 20s per episode (sim: 4000 timesteps at 200 Hz = 20s); dynamic commands resampled every 2s</td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>contact-rich</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>PD-actuated joint control (position targets → torques via PD controller), motor torque magnitude, PD controller gains modeled (kp, kd randomized in sim; real kp=28, kd=0.7), action delay, base and link mass, joint/robot inertia implicitly via rigid-body dynamics, ground friction and restitution, sensor (IMU) noise; velocity disturbance (external force) injected.</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>High-throughput physics-based simulation (Isaac Gym) with dynamics randomization across many actuator/environmental parameters and an inner-loop PD controller (i.e., physics simulation with randomized actuator-level parameters rather than idealized kinematics).</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Terrain traversal success rate per terrain (each terrain tested 5×): 100% success on 8/9 terrains, wooden-stair-ascent had 1 failure out of 5 trials; overall success 44/45 ≈ 97.8%. Additional metric: real-time pitch and roll tracking (qualitative/implied low tracking error from IMU), and simulated command-tracking rewards/errors (plots shown but no numeric RMSE reported).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Training used 4096 parallel agents on one NVIDIA GeForce RTX 4090 GPU for 32.8 hours (reported equivalent of 284 days of real-world experience). Simulation control frequency set to 200 Hz; real robot control frequency 50 Hz.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Dynamics randomization applied to ground friction, restitution, base and link mass, action delay, PD controller gains, motor torque; additionally random velocity disturbances and sensor noise. (Sampling ranges referenced in a Table I in paper but explicit numeric ranges not included in the text.)</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Unitree A1 quadruped robot (≈12 kg, 18 DOF, 12 actuated joints; 3 motors per leg)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>Single observed failure during wooden-stair ascent due to collision with a step triggering a safety protocol; no attribution to specific actuator-dynamics mismatch was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Zero-shot sim-to-real transfer for posture-aware quadruped locomotion was achieved by (1) modeling a PD-actuated inner-loop in simulation and randomizing actuator-level parameters (PD gains, action delay, motor torque, mass) and environment dynamics (friction, restitution), and (2) combining asymmetric actor-critic and adversarial motion priors; this suggests that including actuator inner-loop behavior and variability (action delays and PD-gain/torque uncertainty) in dynamics randomization is critical for robust sim-to-real transfer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Dynamics randomization revisited: A case study for quadrupedal locomotion <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Learning robust perceptive locomotion for quadrupedal robots in the wild <em>(Rating: 1)</em></li>
                <li>Sim-to-real transfer for quadrupedal locomotion via terrain transformer <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1923",
    "paper_id": "paper-276813549",
    "extraction_schema_id": "extraction-schema-41",
    "extracted_data": [
        {
            "name_short": "PALO",
            "name_full": "Posture-Aware Locomotion (PALO)",
            "brief_description": "An end-to-end deep RL framework that trains a posture-aware locomotion policy in simulation with asymmetric actor-critic, adversarial motion priors, curricula, and dynamics domain randomization, then deploys the policy zero-shot on a Unitree A1 quadruped.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "Posture-aware quadruped locomotion (6D commands: vx, vy, ωz, body height, pitch, roll)",
            "task_timescale": "up to 20s per episode (sim: 4000 timesteps at 200 Hz = 20s); dynamic commands resampled every 2s",
            "task_contact_ratio": "contact-rich",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": "PD-actuated joint control (position targets → torques via PD controller), motor torque magnitude, PD controller gains modeled (kp, kd randomized in sim; real kp=28, kd=0.7), action delay, base and link mass, joint/robot inertia implicitly via rigid-body dynamics, ground friction and restitution, sensor (IMU) noise; velocity disturbance (external force) injected.",
            "actuator_parameters_simplified": null,
            "fidelity_level_description": "High-throughput physics-based simulation (Isaac Gym) with dynamics randomization across many actuator/environmental parameters and an inner-loop PD controller (i.e., physics simulation with randomized actuator-level parameters rather than idealized kinematics).",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "Terrain traversal success rate per terrain (each terrain tested 5×): 100% success on 8/9 terrains, wooden-stair-ascent had 1 failure out of 5 trials; overall success 44/45 ≈ 97.8%. Additional metric: real-time pitch and roll tracking (qualitative/implied low tracking error from IMU), and simulated command-tracking rewards/errors (plots shown but no numeric RMSE reported).",
            "sim_vs_real_performance": null,
            "sensitivity_analysis_performed": false,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": true,
            "computational_cost_details": "Training used 4096 parallel agents on one NVIDIA GeForce RTX 4090 GPU for 32.8 hours (reported equivalent of 284 days of real-world experience). Simulation control frequency set to 200 Hz; real robot control frequency 50 Hz.",
            "fidelity_comparison": null,
            "domain_randomization_used": true,
            "domain_randomization_details": "Dynamics randomization applied to ground friction, restitution, base and link mass, action delay, PD controller gains, motor torque; additionally random velocity disturbances and sensor noise. (Sampling ranges referenced in a Table I in paper but explicit numeric ranges not included in the text.)",
            "robot_type": "Unitree A1 quadruped robot (≈12 kg, 18 DOF, 12 actuated joints; 3 motors per leg)",
            "transfer_failure_analysis": "Single observed failure during wooden-stair ascent due to collision with a step triggering a safety protocol; no attribution to specific actuator-dynamics mismatch was reported.",
            "key_finding_for_theory": "Zero-shot sim-to-real transfer for posture-aware quadruped locomotion was achieved by (1) modeling a PD-actuated inner-loop in simulation and randomizing actuator-level parameters (PD gains, action delay, motor torque, mass) and environment dynamics (friction, restitution), and (2) combining asymmetric actor-critic and adversarial motion priors; this suggests that including actuator inner-loop behavior and variability (action delays and PD-gain/torque uncertainty) in dynamics randomization is critical for robust sim-to-real transfer.",
            "uuid": "e1923.0"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 2
        },
        {
            "paper_title": "Dynamics randomization revisited: A case study for quadrupedal locomotion",
            "rating": 2
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2
        },
        {
            "paper_title": "Learning robust perceptive locomotion for quadrupedal robots in the wild",
            "rating": 1
        },
        {
            "paper_title": "Sim-to-real transfer for quadrupedal locomotion via terrain transformer",
            "rating": 1
        }
    ],
    "cost": 0.008483000000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PALO: Learning Posture-Aware Locomotion for Quadruped Robots</p>
<p>Xiangyu Miao 
Jun Sun 
Hang Lai 
Xinpeng Di 
Jiahang Cao 
Yong Yu 
Weinan Zhang 
PALO: Learning Posture-Aware Locomotion for Quadruped Robots
ADA6CC1653EEE00F3134E1A1B818A836
With the rapid development of embodied intelligence, locomotion control of quadruped robots on complex terrains has become a research hotspot.Unlike traditional locomotion control approaches focusing solely on velocity tracking, we pursue to balance the agility and robustness of quadruped robots on diverse and complex terrains.To this end, we propose an end-to-end deep reinforcement learning framework for posture-aware locomotion named PALO, which manages to handle simultaneous linear and angular velocity tracking and real-time adjustments of body height, pitch, and roll angles.In PALO, the locomotion control problem is formulated as a partially observable Markov decision process, and an asymmetric actor-critic architecture is adopted to overcome the sim-to-real challenge.Further, by incorporating customized training curricula, PALO achieves agile postureaware locomotion control in simulated environments and successfully transfers to real-world settings without fine-tuning, allowing real-time control of the quadruped robot's locomotion and body posture across challenging terrains.Through indepth experimental analysis, we identify the key components of PALO that contribute to its performance, further validating the effectiveness of the proposed method.The results of this study provide new possibilities for the low-level locomotion control of quadruped robots in higher-dimensional command spaces and lay the foundation for future research on upper-level modules for embodied intelligence.</p>
<p>I. INTRODUCTION</p>
<p>Recent advancements in artificial intelligence have driven progress in embodied intelligence, unlocking new possibilities for quadruped robots [1].The motion intelligence of quadruped robots is typically divided into high-level motion planning and low-level motion control.Designing locomotion controllers remains a significant challenge due to the robots' high degrees of freedom (DOF) and the need to adapt to diverse structured and unstructured terrains.Traditional approaches rely on model-based control methods [2]- [5], where controllers are manually designed based on the robot's dynamics and control theory.However, these approaches involve complex pipelines and require extensive expert knowledge for accurate modeling and parameter tuning.</p>
<p>In contrast, deep reinforcement learning (RL) provides an end-to-end learning paradigm, enabling agents to directly map observations to actions through interaction with the environment, and reducing reliance on expert intervention [6], [7].Considering the low sampling efficiency and safety risks of directly training in the real world, a more practical 1 Shanghai Jiao Tong University 2 Shanghai Aerospace Control Technology Institute, China *Correspondence to: Jun Sun (sjlovedh@hotmail.com),Weinan Zhang (wnzhang@sjtu.edu.cn)alternative is to train the model in simulation and deploy it in the real world, also known as sim-to-real transfer [8]- [10].</p>
<p>Typically, in quadrupedal locomotion, an RL policy is trained to follow a 3-dimensional (3D) velocity command, including linear velocities along the x-and y-axes and angular velocity around the z-axis in the robot's body frame, as shown in Fig. 2.However, many real-world tasks require precise control of body posture in addition to velocity.For instance, in object transportation, the primary goal is to move from point A to point B, but maintaining object stability is equally important.Suppose a quadruped robot is tasked with delivering a cup of beverage, if it cannot adjust its body posture to prevent spillage, mere point-to-point locomotion would be insufficient, potentially leading to task failure.Similarly, in industrial inspections or terrain exploration, the robot may need to crouch to pass through confined spaces or adjust its orientation for better visibility.Despite these demands, most existing works focus solely on velocity tracking, neglecting explicit posture control [8], [11]- [14].Some even penalize non-standard body orientations, prioritizing stability at the cost of agility [15]- [17].</p>
<p>To enhance the controllability of quadruped robot locomotion, we propose an end-to-end deep reinforcement learning framework for Posture-Aware Locomotion named PALO, which enables adaptive control policies based on proprioceptive feedback and extended command inputs.PALO allows the robot to execute agile, robust, and blind locomotion control under diverse body postures, master various motion skills, and traverse challenging terrains.Notably, the controller is trained entirely in simulation and transferred to realworld deployment without fine-tuning.In real-world experiments with the Unitree A1 quadruped robot, we demonstrate that a single RL policy yielded by PALO can generalize to various velocity and posture command combinations on both structured and unstructured terrains, achieving agile postureaware locomotion.</p>
<p>The key contributions of this research can be summarized</p>
<p>II. RELATED WORK A. Quadrupedal Locomotion with Reinforcement Learning</p>
<p>In recent years, deep reinforcement learning has become increasingly popular for quadrupedal locomotion control due to its ability to learn complex behaviors in dynamic environments.One of the most common focuses in this domain is velocity control.By directly controlling the robot's linear and angular velocity, these methods enable the robot to navigate towards desired destinations [8], [11]- [19].However, pure velocity control often lacks flexibility, especially when navigating complex terrains or performing tasks that require more nuanced body movements.To address this limitation, some studies have expanded the system to include gait control [20]- [23], where the focus is on adjusting the robot's leg movements to generate desired motion patterns, such as walking, trotting, or running.</p>
<p>Despite the progress in gait-adaptive locomotion, there are still challenges.One major issue is the coordination of posture and movement, especially in more dynamic or unpredictable environments.While gait control helps with the rhythm of movement, it does not directly address the robot's posture -specifically the orientation and height of its body during movement.For example, the robot may need to adjust its posture to maintain body stability when traversing slopes or stairs, but traditional gait control does not inherently solve these problems.This is where explicit posture tracking becomes critical.While few studies have explicitly focused on posture tracking in quadruped robots, our approach integrates posture-aware locomotion control using reinforcement learning and customized curricula, allowing the robot to simultaneously adapt its posture and movements to explore complex environments.</p>
<p>B. Adversarial Motion Prior</p>
<p>Designing effective reward functions for reinforcement learning (RL) remains a significant challenge, particularly in domains such as robot locomotion where intricate physical dynamics must be navigated.Crafting rewards that balance task performance with motion quality often demands substantial domain expertise and iterative tuning.Overly complex reward structures, while aiming to encode desired behaviors, can inadvertently lead to unstable or unnatural movement patterns, as agents may exploit reward loopholes through unintended strategies.To address this, researchers have introduced regularization techniques [14]- [16], [24], [25] to constrain policy optimization, promoting structured and energy-efficient gaits.These methods often incorporate penalties for deviations from biomechanically plausible joint motions or incentives for symmetry and periodicity, thereby reducing the need for exhaustive reward engineering.</p>
<p>Recent advances have explored adversarial motion priors (AMP) as a means to implicitly guide policies toward naturalistic behaviors.By leveraging generative adversarial imitation learning (GAIL) frameworks [26], studies such as those by Escontrela et al. [27] and Wu et al. [28], [29] have demonstrated the efficacy of adversarial training for motion synthesis.In this paradigm, a discriminator network is trained to distinguish between agent-generated state transitions and those from an expert dataset, producing a style reward that incentivizes policy behaviors to align with the expert distribution.This approach circumvents the need for manually specified reward terms by instead deriving supervision directly from expert demonstrations, enabling agents to learn coherent, human-like locomotion without explicit kinematic constraints.</p>
<p>C. Sim-to-Real Transfer</p>
<p>Deploying policies trained in simulation to real-world robotic systems continues to face substantial hurdles due to the sim-to-real gap [8]- [10].Discrepancies in dynamic modeling, imperfect terrain representations, unaccounted physical interactions (e.g., friction, material deformation), and mismatches in sensor noise or actuation delays often degrade policy performance when transferring to hardware.These challenges necessitate strategies to bridge the divide between simulated training environments and real-world operational conditions.</p>
<p>A common methodology involves high-fidelity simulation [10], which meticulously replicates real-world physics, sensor behaviors, and environmental textures-including terrain variability, actuator dynamics, and communication delays-to reduce modeling inaccuracies.Alternatively, domain randomization [10], [15], [16], [23]- [25], [27], [28], [30]- [32] deliberately introduces variability during training by perturbing parameters such as robot inertia, joint damping, ground friction, and sensor noise ranges.By exposing policies to diverse simulated conditions, this approach fosters robustness, enabling adaptation to unseen real-world dynamics without overfitting to idealized simulation properties.Together, these strategies aim to narrow the sim-to-real gap, though balancing realism and computational efficiency remains an active area of research.</p>
<p>III. METHODOLOGY</p>
<p>A. Reinforcement Learning Formulation</p>
<p>RL addresses the problem of sequential decision-making.Given that the robot only receives partial environmental information filtered by proprioceptive sensors, we model locomotion control as a Partially Observable Markov Decision Process (POMDP), defined by the tuple (S, O, A, P, r, γ), where S, O, and A are the state, observation, and action spaces, respectively.P : S ×A×S → R is the state transition probability, r : S × A → R is the reward function, and γ ∈ [0, 1) is the discount factor.</p>
<p>An episode begins from an initial state s 0 .At each time step t, the state s t contains a complete description of the environment.During simulator training, the command signal c t is given randomly, whereas during real-world deployment, it is issued by a human operator via remote control.The agent takes an action a t = π(o t , c t ) based on the current observation o t and command signal c t , and interacts with the environment.The environment transitions to the next state s t+1 with probability P(s t+1 | s t , a t ), providing the agent with a reward r t .The objective of RL is to find the optimal policy π θ that maximizes the expected return:
maximize θ E π θ ∞ t=0 γ t r t .
(1)</p>
<p>1) Observation Space: Proprioceptive observation o t ∈ R 50 includes angular velocity around the z-axis in the robot base coordinate system, body pitch and roll angles, gravity projection, 6D commands, DOF positions and linear velocities in the current time step, and DOF positions in the previous time step.The 6D commands consist of linear velocity in the x and y directions, angular velocity around the z-axis, body height, pitch, and roll tracking (see Fig. 2).The robot's posture angles are derived from the Inertial Measurement Unit (IMU) measurements, with the quaternion converted to Euler angles for alignment with commands.</p>
<p>Assuming the robot's quaternion is represented as q = w + xi + yj + zk, where w is the real part, and (x, y, z) are the imaginary parts, the corresponding Euler angles are:
ϕ = atan2(2(wx + yz), 1 − 2(x 2 + y 2 )), θ = asin(2(wy − zx)), ψ = atan2(2(wz + xy), 1 − 2(y 2 + z 2 )),(2)
where ϕ represents the roll angle, θ represents the pitch angle, and ψ represents the yaw angle.</p>
<p>2) Action Space: The action a t ∈ R 12 specifies the target position for each DOF, converted to joint torques via a Proportional-Derivative (PD) controller:
τ = k p (q d − q) + k d ( qd − q),(3)
where q and q d are the current and target DOF positions and (k p , k d ) are the hyperparameters of the PD controller.</p>
<p>3) Reward Function: Our goal is to train an agile and robust blind locomotion controller.The reward function includes terms to encourage locomotion tasks and prevent undesirable behaviors.Inspired by [14,16,34], we use the AMP to replace complex auxiliary reward terms with style rewards, aiming for a simple and effective reward function:
r t = r task t + r style t + r reg t .(4)
The task reward encourages the robot to follow the 6D commands, including linear and angular velocities, body height, pitch, and roll.The reward for linear velocity tracking is defined as:
R v = exp − ∥v actual − v cmd ∥ 2 σ v ,(5)
where v actual and v cmd are the actual and commanded linear velocities.σ v is a scaling factor reflecting the reward's sensitivity to velocity error.The smaller σ v , the more sensitive the reward is to the velocity error.Similarly, the reward for angular velocity tracking is as follows:
R ω = exp − ∥ω actual − ω cmd ∥ 2 σ ω .(6)
Additional reward terms control body posture (height and orientation):
R h = exp − |h actual − h cmd | σ h ,(7)R θ = exp − ∥θ actual − θ cmd ∥ 2 σ θ . (8)
The style reward encourages imitation of expert behaviors, aiding in transferring stable gaits to complex terrains.Following [27], the AMP style reward is defined as:
r style t (s t , s t+1 ) = max[0, 1 − 0.25(D φ (s, s ′ ) − 1) 2 ],(9)
where D φ (s, s ′ ) ∈ [−1, 1] is the discriminator output comparing agent behavior to expert data.Other regularization terms are similar to [28].</p>
<p>B. AMP Motion Data Generation</p>
<p>There are two modes for generating reference expert motion datasets.The first involves using motion capture technology to collect animal motion data, then training robots to mimic these behaviors [27].The second collects motion data directly from quadruped robots.PALO adopts the latter approach.Initially, the robot is trained on flat terrain without involving AMP, using auxiliary reward terms to regularize motion.Once this training stage is complete, AMP data is collected from the robot's learned policy interactions.This stable gait data from flat terrain can be reused for training on more complex terrains.In the next stage, AMP is combined to train the robot in diverse, complex terrains, enabling stable traversal even on challenging surfaces.</p>
<p>C. Asymmetric Actor-Critic</p>
<p>Privileged learning [33] improves policy adaptability by leveraging simulation-only information during training.Recent works leverage a two-stage Teacher-Student paradigm [8], [13], [14], [16], [24], [28], [30], [34], where a teacher policy is trained in simulation using privileged information, and a student policy is distilled for real-world deployment.However, this approach suffers from low data efficiency, and the student policy often underperforms the teacher [17].</p>
<p>Actor-critic algorithms improve efficiency by combining value function approximation with policy gradient.The actor (policy network) interacts with the environment, while the critic (value network) evaluates state values and guides policy updates.To enhance data efficiency under partial observability, we adopt an asymmetric actor-critic architecture [17], [29], [35]- [37], as illustrated in Fig. 3.The critic receives full observations, including privileged information, while the actor relies solely on proprioceptive input.This enables the critic to provide richer learning signals, improving policy performance.</p>
<p>PALO encodes the last five timesteps of observations and actions into a latent vector via a historical encoder.This vector, concatenated with 6D commands, serves as the actor's input to generate 12D target joint positions.These positions are converted into motor torques by a PD controller.Meanwhile, the critic receives privileged inputs, including ground heights, friction, restitution coefficient, base mass, center of mass (CoM) position, contact forces, and Boolean contact indicators.</p>
<p>At each time step t, the actor performs an action based on partial observations, and the environment returns new observations and rewards.The critic evaluates the state using privileged information and provides value estimates for policy updates.This process enables the actor to implicitly leverage privileged information during training, improving robustness under real-world constraints.</p>
<p>D. Training Loss</p>
<p>We use the Proximal Policy Optimization (PPO) algorithm with clipping range [38].Its objective is:
L CLIP (θ ′ ) = E s,a min π θ ′ (a | s) π θ (a | s) A π θ (s, a) , clip π θ ′ (a | s) π θ (a | s) , 1 − ϵ, 1 + ϵ A π θ (s, a) ,(10)
where A π θ (s, a) is the advantage function, which measures the return of taking action a in state s compared to the average expected return.This approach reduces the risk of "policy collapse" by limiting policy updates, and preventing significant performance drops due to large policy changes.</p>
<p>The critic network's loss function minimizes the Mean Square Error (MSE) between estimated and actual returns:
L V F (ϕ) = 1 2 E (s,a)∼π θ (V ϕ (s) − G t ) 2 . (11)
For the AMP discriminator, we adopt the training objective from [27], minimizing the AMP loss function:
L AM P (φ) = E (s,s ′ )∼D (D φ (s, s ′ ) − 1) 2 + E (s,s ′ )∼π θ (s,a) (D φ (s, s ′ ) + 1) 2 + ω gp 2 E (s,s ′ )∼D ∥∇ φ D φ (s, s ′ )∥ 2 ,(12)
where the first two terms help distinguish state transitions from the reference expert dataset and the agent's policy.The last term is a gradient penalty that prevents overfitting and improves the stability of the training [27].</p>
<p>E. Training Curricula</p>
<p>To stabilize early RL, we employ curriculum learning [39] to gradually increase task difficulty based on policy performance.Our curricula include terrain, reward, and command curricula.</p>
<p>1) Terrain Curriculum: We construct diverse simulated terrains -wavy surfaces, rough slopes, ascending and descending stairs, discrete obstacles, and flat rough terrain [40] -to mimic urban and wilderness environments.Initially, all agents start on the easiest terrains.Successful agents progress to more difficult terrains, while those struggling are reset to easier terrains.Adaptability is measured by the distance traveled.Agents traveling more than half the terrain size move to more difficult terrains, while those traveling less are reset to easier terrains.</p>
<p>2) Reward Curriculum: We design a reward curriculum for posture control to gradually increase task complexity.</p>
<p>Early training prioritizes velocity tracking, mid-training gradually introduces body height, pitch, and roll tracking, and later training applies the full reward function for integrated locomotion and posture control.distribution with a mean of 0.5 radians and a standard deviation of 0.25 radians, clipped to [0, π 4 ] radians.Locomotion under 6D command constraints is challenging due to physical limitations.We employ a grid adaptive curriculum [24], dynamically expanding velocity ranges and reducing disturbance intervals when the velocity tracking reward exceeds 80% of its maximum.</p>
<p>F. Domain Randomization</p>
<p>To bridge the sim-to-real gap, we apply domain randomization [41], varying training conditions to cover real-world uncertainties.Domain randomization consists of visual and dynamics randomization [9], [42].Since this paper focuses on low-level blind locomotion control in quadruped robots, we apply dynamics randomization, adjusting robot dynamics, environment parameters, and sensor noise.</p>
<p>We randomize ground friction, restitution, base and link mass, action delay, PD controller gains, and motor torque.Additionally, we introduce random velocity disturbances to simulate external force perturbations, enhancing robustness.The randomized variables and their sampling ranges are detailed in Table I.</p>
<p>IV. RESULTS AND DISCUSSION</p>
<p>In this section, we present the results of our experimental setup and provide a detailed analysis.Our experiments aim to address the following key questions:</p>
<p>• Can PALO enable the robot to effectively track 6D control commands to manage locomotion and posture?• How well does the policy trained in the simulator transfer to real-world environments with complex terrains?• Which components are critical for PALO's overall performance?</p>
<p>To explore these questions, we first trained the robot's locomotion controller in the simulator and then evaluated its command tracking performance.Following this, the policy was transferred to a real robot for deployment in realworld conditions.A series of ablation studies were also conducted to identify the most significant factors affecting the performance of the algorithm.</p>
<p>Regarding hardware details, all real-world experiments were conducted using the Unitree A1 quadruped robot [43].This quadruped robot, weighing approximately 12 kilograms, features 18 DOF, 12 of which are actuated (three motors per leg).</p>
<p>A. Simulation Experiments</p>
<p>We used the Isaac Gym simulator [44] for both training and experimentation, employing the Proximal Policy Optimization (PPO) [38] algorithm with 4096 agents trained in parallel across different terrain types.The training process, conducted on a single NVIDIA GeForce RTX 4090 GPU, took 32.8 hours, equivalent to 284 days of real-world training.The Adam optimizer was used, and the control frequency in the simulator was set to 200 hertz, with each reinforcement learning episode running for up to 4000 timesteps (20 seconds in real-time).Early episode termination would be triggered by collisions.</p>
<p>The robot was tested on various terrains to assess its 6D command tracking capability.The commands received by the robot were represented as a six-tuple: linear velocity along the x-axis (v x ), linear velocity along the y-axis (v y ), angular velocity around the z-axis (ω z ), body height (h) (the desired height difference from the reference height), pitch angle (θ), and roll angle (ϕ).The units for linear velocity, angular velocity, height and angles are meters per second, radians per second, meters, and radians, respectively.</p>
<p>In order to demonstrate the robot's command tracking capability under dynamic conditions, we performed testing primarily on flat terrain, where 6D control commands were dynamically resampled every 2 seconds.The results (shown in Fig. 4) indicate that the robot can effectively track a variety of control commands, adjusting its body posture in realtime whether stationary or moving.This includes combined actions such as walking, crawling, in-place rotation, and posture adjustments.</p>
<p>As observed in the figure, the robot shows excellent tracking performance for v x , v y , yaw rate, pitch, and roll, with very minimal tracking error throughout the entire test.This highlights the robot's robust ability to adapt to a range of control commands and execute them effectively.However, we also noticed that the height command tracking showed some degradation during the interval between 6-10 seconds.During this period, the robot was commanded with unconventional (non-neutral) posture angles that conflicted with the height commands, which requested the robot to crawl.This discrepancy in the height response illustrates the robot's adaptability, as it handled the conflicting command combinations effectively.It also reflects the flexibility and robustness of the reinforcement learning policy, which was able to maintain stable performance even in the presence of command conflicts.While the focus of our analysis is on flat terrain, the robot was also tested on more complex terrains, such as wavy surfaces, slopes, terrain with discrete obstacles, and stairs.The robot demonstrated consistent and robust tracking performance across all environments.On wavy terrain, the robot accurately tracked velocity and posture angle commands, with only slight instability observed in height tracking due to surface fluctuations.On sloped surfaces, the robot maintained a stable body posture, and on stairs, it successfully adapted to height changes, maintaining its posture while climbing and descending.</p>
<p>The primary reason for focusing on flat terrain in this section is its ability to fully capture the robot's performance in dynamic command tracking, providing a comprehensive illustration of the system's capabilities.</p>
<p>B. Real-World Experiments</p>
<p>After training the policy in simulation, we directly deployed it onto the real A1 robot without any additional finetuning.The control frequency was set to 50 Hz, with the PD controller parameters configured as k p = 28 and k d = 0.7.</p>
<p>To evaluate the robot's locomotion capabilities, we conducted tests on nine different terrains: grass, rubber tracks, soft sponge pads, uphill slopes, downhill slopes, sand, wooden stairs (ascending), wooden stairs (descending), and step traversal (both up and down).Each test was repeated five times for consistency.The results demonstrated that the robot successfully navigated all terrains, achieving a 100% success rate except for wooden stair ascent, where only one failure was observed due to a collision with a step, which triggered our predefined safety protocol.A visual overview of the real-world experiments is presented in Fig. 5, which showcases the robot traversing these diverse terrains.Additional demonstrations of posture-aware locomotion tracking can be found in the accompanying video.</p>
<p>In addition to evaluating the robot's locomotion across various terrains, we also collected data on the real-time  tracking of pitch and roll angle commands.Here we focus on demonstrating the robot's body orientation control capabilities, as body height in the real world is difficult to measure due to the absence of specific sensors.The angle measurements are derived from IMU data and converted to Euler angles.The tracking results, shown in Fig. 6, highlight the robot's ability to track target pitch and roll angles in real time.From the figure, we can observe that PALO successfully overcomes the sim-to-real challenge, enabling fast and stable responses to posture commands.The robot shows excellent tracking performance, adapting well to realworld conditions.</p>
<p>C. Ablation Study</p>
<p>Through ablation experiments in the simulation, we assessed the contributions of different components to PALO's performance.Key findings include the significant impact of AMP technology, our tailored training curricula and our sampling strategy for body posture commands on overall performance.</p>
<p>1) Effect of AMP: We compared the performance of policies trained with and without AMP, focusing on average task rewards and body collision counts across flat and complex terrains.</p>
<p>For flat terrain, dynamic commands were used, as described in Sec.IV-A.On more complex terrains, we employed static commands (1.0, 0.0, 0.0, 0.0, 0.0, 0.0) to isolate the effect of AMP.The results were derived from five tests using the same random seed, with mean and standard deviation calculated for each comparison.The average reward data is presented in Fig. 7, illustrating AMP's impact on policy performance across terrain types.While AMP had a minimal effect on flat terrain, it substantially improved performance on complex terrains.Specifically, on terrains with discrete obstacles and stairs, the robot without AMP experienced multiple collisions, often resulting in episode terminations and near-zero task completion rates.However, with AMP, the robot successfully completed all tests without body collisions causing any terminations.</p>
<p>2) Customized Training Curricula: Curriculum learning has been widely used to improve RL policy performance [13], [17], [24], [28], [30], [32], [34].PALO innovates by introducing a staged reward curriculum to improve the robot's ability to navigate complex terrains while controlling its posture.In particular, we evaluate the impact of the reward curriculum on policy performance.</p>
<p>The results show that the reward curriculum has a significant impact on performance across various terrains.For example, when testing on uphill stair terrains, the reward curriculum reduced collision rates and improved mobility.This finding confirms that a carefully designed reward function is critical for optimizing RL policy performance.Specifically, periodically adjusting the composition and magnitude of reward signals during training improves the robot's ability to handle multiple task objectives.</p>
<p>3) Sampling Strategy: We also examined the effects of different command sampling strategies during simulator training.These included:</p>
<p>• Sampling posture control commands from a normal distribution, and sampling velocity commands from a uniform distribution.• Sampling both posture and velocity commands from a uniform distribution.</p>
<p>The results (Fig. 8) indicated that normally distributed sampling for angular velocity and body posture commands led to slightly better performance in orientation tracking compared to uniformly distributed sampling, although the overall differences in average reward were marginal.However, it is worth noting that with the uniform distribution sampling, two episodes were prematurely terminated due to collisions during testing.4) Historical Encoder Architectures: We compared three different historical encoder architectures:</p>
<p>• MLP-based encoder using the last 5 timesteps of observations (MLP-5).• MLP-based encoder using the last 50 timesteps of observations (MLP-50).• 1D-Temporal Convolutional Network using the last 50 timesteps (TCN-50).The results (Fig. 9) revealed that the MLP-5 architecture, with its shorter history of observations, slightly outperformed the others.This suggests that short-term memory is sufficient for controlling low-level blind locomotion, and a simpler network structure may be more effective for this task.</p>
<p>V. CONCLUSIONS</p>
<p>In this work, we present a deep reinforcement learning framework for posture-aware locomotion control of quadruped robots named PALO.Our approach focuses on achieving robust and simultaneous locomotion and posture control across a wide range of terrains, without relying on visual information.This enables quadruped robots to acquire a variety of motion skills, significantly enhancing their adaptability and intelligence.By leveraging PALO, we improve the robot's ability to effectively navigate complex and dynamic environments.</p>
<p>Looking ahead, we aim to extend this work by developing upper-level modules, such as motion planning systems, to facilitate intelligent navigation, obstacle avoidance, and highlevel decision-making.Additionally, we plan to integrate large language models into future iterations of the system, enabling more natural and sophisticated human-robot interaction.This will allow robots to interpret and respond to highdimensional commands, further advancing their autonomy and practical deployment in real-world applications.</p>
<p>Fig. 1 .
1
Fig. 1.Unitree A1 quadruped robot demonstrating posture-aware locomotion control (from left to right: height, pitch, and roll control)</p>
<p>Fig. 2 .
2
Fig. 2. Schematic diagram of the robot's body frame rendered in simulation, showing the coordinate axes (red: x-axis, green: y-axis, blue: z-axis) and local height map sampling points (yellow).The 6D commands correspond to the robot's motion tracking in linear velocity, angular velocity, height, pitch, and roll.</p>
<p>Fig. 3 .
3
Fig.3.Overview of our framework PALO.The orange dashed line represents gradient backpropagation during training, while the gray dashed line indicates observations obtained from the simulator or real-world environment and fed into our framework.Solid arrows denote the data flow within our model.The actor is a shallow MLP that outputs 12D target DOF positions, which are converted to torques via a PD controller.The actor's input combines a 6D command with a history latent vector from a history encoder.The critic is a shallow MLP, and the AMP discriminator is an MLP.</p>
<p>Fig. 4 .
4
Fig. 4. Dynamic 6D command tracking performance on simulated flat terrain.</p>
<p>Fig. 5 .
5
Fig. 5. Real-world deployment of the trained policy on various structured and unstructured terrains.The 3×3 composite image illustrates the robot navigating different terrain types, demonstrating its adaptability.</p>
<p>Fig. 6 .
6
Fig. 6.Real-time tracking of pitch and roll angles.The figure compares target (red dashed line) and actual (blue solid line) angles, demonstrating the robot's posture control capabilities during real-world operation.</p>
<p>Fig. 7 .
7
Fig. 7. Performance comparison between policies trained with and without AMP.</p>
<p>Fig. 8 .
8
Fig. 8. Performance comparison between policies trained with normal distribution sampling and uniform distribution sampling for posture commands.</p>
<p>Fig. 9 .
9
Fig. 9. Performance comparison of different historical encoder architectures.</p>
<p>Real-world embodied ai through a morphologically adaptive quadruped robot. T F Nygaard, C P Martin, J Torresen, K Glette, D Howard, Nature Machine Intelligence. 352021</p>
<p>Animal gaits on quadrupedal robots using motion matching and model-based control. D Kang, S Zimmermann, S Coros, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021</p>
<p>Fast and efficient locomotion via learned gait transitions. Y Yang, T Zhang, E Coumans, J Tan, B Boots, Conference on Robot Learning. PMLR2022</p>
<p>Fastmimic: Model-based motion imitation for agile, diverse and generalizable quadrupedal locomotion. T Li, J Won, J Cho, S Ha, A Rai, Robotics. 123902023</p>
<p>Model predictive control with environment adaptation for legged locomotion. N Rathod, A Bratta, M Focchi, M Zanon, O Villarreal, C Semini, A Bemporad, IEEE Access. 92021</p>
<p>Playing atari with deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, arXiv:1312.56022013arXiv preprint</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 1998MIT press Cambridge1</p>
<p>Simto-real transfer for quadrupedal locomotion via terrain transformer. H Lai, W Zhang, X He, C Yu, Z Tian, Y Yu, J Wang, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Sim-to-real transfer in deep reinforcement learning for robotics: a survey. W Zhao, J P Queralta, T Westerlund, 2020 IEEE symposium series on computational intelligence (SSCI). IEEE2020</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, arXiv:1804.103322018arXiv preprint</p>
<p>A walk in the park: Learning to walk in 20 minutes with model-free reinforcement learning. L Smith, I Kostrikov, S Levine, arXiv:2208.078602022arXiv preprint</p>
<p>Learning agile and dynamic motor skills for legged robots. J Hwangbo, J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, Science Robotics. 42658722019</p>
<p>Learning robust perceptive locomotion for quadrupedal robots in the wild. T Miki, J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, Science robotics. 76228222022</p>
<p>Legged locomotion in challenging terrains using egocentric vision. A Agarwal, A Kumar, J Malik, D Pathak, Conference on robot learning. PMLR2023</p>
<p>Concurrent training of a control policy and a state estimator for dynamic and robust legged locomotion. G Ji, J Mun, H Kim, J Hwangbo, IEEE Robotics and Automation Letters. 722022</p>
<p>Rma: Rapid motor adaptation for legged robots. A Kumar, Z Fu, D Pathak, J Malik, arXiv:2107.040342021arXiv preprint</p>
<p>Dreamwaq: Learning robust quadrupedal locomotion with implicit terrain imagination via deep reinforcement learning. I M A Nahrendra, B Yu, H Myung, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Coupling vision and proprioception for navigation of legged robots. Z Fu, A Kumar, A Agarwal, H Qi, J Malik, D Pathak, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202217283</p>
<p>Guided constrained policy optimization for dynamic quadrupedal robot locomotion. S Gangapurwala, A Mitchell, I Havoutis, IEEE Robotics and Automation Letters. 522020</p>
<p>Cpg-rl: Learning central pattern generators for quadruped locomotion. G Bellegarda, A Ijspeert, IEEE Robotics and Automation Letters. 742022</p>
<p>Multi-expert learning of adaptive legged locomotion. C Yang, K Yuan, Q Zhu, W Yu, Z Li, Science Robotics. 54921742020</p>
<p>Visual cpg-rl: Learning central pattern generators for visually-guided quadruped locomotion. G Bellegarda, M Shafiee, A Ijspeert, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Walk these ways: Tuning robot control for generalization with multiplicity of behavior. G B Margolis, P , Conference on Robot Learning. PMLR2023</p>
<p>Rapid locomotion via reinforcement learning. G B Margolis, G Yang, K Paigwar, T Chen, P , The International Journal of Robotics Research. 4342024</p>
<p>Learning vision-guided quadrupedal locomotion end-to-end with cross-modal transformers. R Yang, M Zhang, N Hansen, H Xu, X Wang, arXiv:2107.039962021arXiv preprint</p>
<p>Generative adversarial imitation learning. J Ho, S Ermon, Advances in neural information processing systems. 201629</p>
<p>Adversarial motion priors make good substitutes for complex reward functions. A Escontrela, X B Peng, W Yu, T Zhang, A Iscen, K Goldberg, P Abbeel, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2022</p>
<p>Learning robust and agile legged locomotion using adversarial motion priors. J Wu, G Xin, C Qi, Y Xue, IEEE Robotics and Automation Letters. 882023</p>
<p>Learning multiple gaits within latent space for quadruped robots. J Wu, Y Xue, C Qi, arXiv:2308.030142023arXiv preprint</p>
<p>Not only rewards but also constraints: Applications on legged robot locomotion. Y Kim, H Oh, J Lee, J Choi, G Ji, M Jung, D Youm, J Hwangbo, IEEE Transactions on Robotics. 2024</p>
<p>Rloc: Terrain-aware legged locomotion using reinforcement learning and optimal control. S Gangapurwala, M Geisert, R Orsolino, M Fallon, I Havoutis, IEEE Transactions on Robotics. 3852022</p>
<p>Robot parkour learning. Z Zhuang, Z Fu, J Wang, C Atkeson, S Schwertfeger, C Finn, H Zhao, arXiv:2309.056652023arXiv preprint</p>
<p>Learning by cheating. D Chen, B Zhou, V Koltun, P Krähenbühl, Conference on robot learning. PMLR2020</p>
<p>Learning quadrupedal locomotion over challenging terrain. J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, Science robotics. 54759862020</p>
<p>Asymmetric actor critic for image-based robot learning. L Pinto, M Andrychowicz, P Welinder, W Zaremba, P Abbeel, arXiv:1710.065422017arXiv preprint</p>
<p>Aacc: Asymmetric actor-critic in contextual reinforcement learning. W Yue, Y Zhou, X Zhang, Y Hua, Z Wang, G Kou, arXiv:2208.023762022arXiv preprint</p>
<p>Resilient legged local navigation: Learning to traverse with compromised perception end-to-end. C Zhang, J Jin, J Frey, N Rudin, M Mattamala, C Cadena, M Hutter, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges. T Lesort, V Lomonaco, A Stoian, D Maltoni, D Filliat, N Díaz-Rodríguez, Information fusion. 582020</p>
<p>Learning to walk in minutes using massively parallel deep reinforcement learning. N Rudin, D Hoeller, P Reist, M Hutter, Conference on Robot Learning. PMLR2022</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2017</p>
<p>Dynamics randomization revisited: A case study for quadrupedal locomotion. Z Xie, X Da, M Van De Panne, B Babich, A Garg, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Unitree robotics. Unitree, 2022</p>
<p>V Makoviychuk, L Wawrzyniak, Y Guo, M Lu, K Storey, M Macklin, D Hoeller, N Rudin, A Allshire, A Handa, arXiv:2108.10470Isaac gym: High performance gpu-based physics simulation for robot learning. 2021arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
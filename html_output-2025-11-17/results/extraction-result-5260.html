<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5260 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5260</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5260</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-263334155</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.00533v4.pdf" target="_blank">SELF: Self-Evolution with Language Feedback</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable versatility across various domains. To further advance LLMs, we propose 'SELF' (Self-Evolution with Language Feedback), a novel approach that enables LLMs to self-improve through self-reflection, akin to human learning processes. SELF initiates with a meta-skill learning process that equips the LLMs with capabilities for self-feedback and self-refinement. Subsequently, the model undergoes an iterative process of self-evolution. In each iteration, it utilizes an unlabeled dataset of instructions to generate initial responses. These responses are enhanced through self-feedback and self-refinement. The model is then fine-tuned using this enhanced data. The model undergoes progressive improvement through this iterative self-evolution process. Moreover, the SELF framework enables the model to apply self-refinement during inference, which further improves response quality. Our experiments in mathematics and general tasks demonstrate that SELF can enhance the capabilities of LLMs without human intervention. The SELF framework indicates a promising direction for the autonomous evolution of LLMs, transitioning them from passive information receivers to active participants in their development.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5260.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5260.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evolution with Language Feedback (SELF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-phase framework that first fine-tunes an LLM to acquire meta-skills (self-feedback and self-refinement) from a small supervised meta-skill corpus, then iteratively self-generates, self-evaluates, self-refines and filters responses from unlabeled prompts to create training data for repeated self-evolution fine-tuning; it also enables online self-refinement at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-7b (primary); additional experiments on OpenL-LaMA (3b) and Vicuna-1.5 reported</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vicuna-7b: an open-source instruction-following model fine-tuned from LLaMA-7b (7B parameters). Experiments also include OpenLLaMA-3b and Vicuna-1.5 to test scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Generate → Self-Feedback → Self-Refinement (meta-skill learning + iterative self-evolution)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Meta-skill learning: supervised fine-tuning on D_meta examples of (prompt, initial response, feedback, refined response) to teach the model to (1) produce natural-language feedback evaluating its own answer and (2) revise its answer using that feedback. Self-evolution: repeatedly (for multiple rounds) generate initial responses to unlabeled prompts, generate feedback and refined answers using the same model, filter refined answers using the model's own feedback (qualified(f)), and fine-tune the model on the cleaned refined (prompt, refined answer) pairs; at inference the model can optionally perform one round of self-refinement (generate feedback and produce a revised answer).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, SVAMP (math); Vicuna testset and Evol-Instruct (general)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K & SVAMP: grade-school math word problems requiring multi-step arithmetic reasoning; Vicuna and Evol-Instruct testsets: general instruction-following and open-domain tasks evaluated by GPT-4 preference judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>After full SELF (meta-skill + three self-evolution rounds) using Vicuna-7b: GSM8K direct generation 29.64% accuracy, GSM8K with inference self-refinement 31.31%; SVAMP direct 49.40%, SVAMP with self-refinement 49.80%. On general tests, Vicuna win rate vs baseline increased (Vicuna testset direct win rate: 72.5% after SELF; with self-refinement 75.0%; Evol-Instruct win rate: 52.8% direct, 55.5% with self-refinement).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline Vicuna + D_QA (pseudo-labeled QA SFT baseline): GSM8K direct 24.49% and SVAMP direct 44.90% (no meta-skill/self-evolution). Base Vicuna (no SELF) had lower scores (example: base Vicuna GSM8K ~16.43% in table).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative gains shown across ablations: SELF (Vicuna + D_meta + three self-evolution rounds) raises GSM8K direct accuracy from 24.49% (Vicuna + D_QA) to 29.64% (+5.15 percentage points) and SVAMP from 44.90% to 49.40% (+4.5 pp). Iterative self-evolution rounds produce incremental improvements (examples: 25.39% → 29.64% on GSM8K across rounds). Meta-skill training alone improves direct generation modestly (e.g., +0.90% GSM8K, +1.9% SVAMP versus D_QA), and inference self-refinement further improves SELF models (e.g., +1.67 pp on GSM8K). Case studies and qualitative analysis (fig.4, A.5) show SELF-enabled models correct errors during refinement where baseline models fail.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: (1) potential plateau in gains across many self-evolution rounds (discussed theoretically in A.1.3); (2) inference-time self-refinement adds computational overhead (multi-turn inference); (3) if meta-skills are not well learned, self-refinement can be marginal or even harmful (baseline models showed marginal or negative self-refinement effects); (4) data filtering via self-feedback improves training-data accuracy but reduces data quantity (e.g., filtered training accuracy increased from 29.89% to 44.10% while test accuracy rose modestly from 26.90% to 27.67% because filtered set shrank), indicating trade-off between data quality and quantity; (5) SELF depends on the quality of the meta-skill corpus (GPT-4 generated D_meta yields better gains than GPT-3.5-generated D_meta).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SELF: Self-Evolution with Language Feedback', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5260.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5260.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inference Self-Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refinement During Inference (one-round)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>At inference the model generates an initial answer, then generates natural-language feedback about that answer and uses it to produce a revised answer; typically executed once in the paper's evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-7b variants (baseline and SELF-finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied both to base Vicuna and models after meta-skill/self-evolution training; refinement is done by the same model (no separate verifier).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refinement (single inference-round)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Procedure: produce initial response r; produce natural-language feedback f evaluating r; produce refined response r' conditioned on (p, r, f). In experiments the model performs one refinement step during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, SVAMP, general testsets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same benchmarks as SELF; self-refinement applied at inference to attempt to improve single-run outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>For Vicuna + SELF: GSM8K improved from 29.64% (direct) to 31.31% (+1.67 pp). SVAMP improved slightly (49.40% → 49.80%). General-domain win rates also improved when applying self-refinement (Vicuna testset from 72.5% → 75.0% for SELF).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>When not applying inference self-refinement, SELF models have the 'direct generation' scores (e.g., 29.64% GSM8K). Baseline models without meta-skill training often did not benefit from self-refinement: example baseline Vicuna + D_QA direct 24.49% → self-refinement 24.44% (slight drop).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Numeric improvements for SELF models (e.g., +1.67 pp GSM8K). Ablation shows that after meta-skill learning self-refinement 'consistently' improves performance across evolution steps, and qualitative case studies show SELF models fix errors during refinement while vanilla models often repeat mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>If the model lacks learned meta-skills, self-refinement can be ineffective or harmful (examples: baseline Vicuna variants showed marginal or negative changes). Self-refinement increases inference cost and does not change model parameters (so recurring errors can persist across instances). Benefit size can be modest relative to training changes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SELF: Self-Evolution with Language Feedback', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5260.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5260.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (majority voting over sampled chain-of-thoughts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An existing decoding strategy where multiple reasoning traces (CoT) are sampled and the most consistent answer (majority vote) is selected to improve reasoning accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-7b (used as comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied at decoding time by sampling multiple chain-of-thought answers and selecting the most frequent final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency (sampling + majority voting)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple CoT outputs via sampling, then take the most common final answer as the prediction. It does not require explicit natural-language feedback or a refinement pass.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (math reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems where multiple reasoning chains are sampled.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>For base Vicuna, self-consistency yielded a notable improvement (reported as +3.13% on GSM8K for the base model). When combined with SELF, further gains were observed (example: combining self-refinement and self-consistency produced GSM8K 29.64% → 32.22%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Without self-consistency the corresponding scores are lower (e.g., base Vicuna direct generation lower by ~3.13 pp vs with self-consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: base Vicuna showed +3.13 percentage points improvement on GSM8K with self-consistency; combining self-consistency with SELF further increased GSM8K accuracy (e.g., 29.64% → 32.22%). The paper notes diminishing returns as models become more certain after self-evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Benefit reduces as model quality increases (diminishing returns), and self-consistency is primarily applicable to tasks with a uniquely correct answer (reasoning tasks) rather than broad open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SELF: Self-Evolution with Language Feedback', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5260.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5260.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prior iterative/self-feedback methods (mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cited prior methods: Self-Refine, SelFee, Reflexion, Ye et al. emergent meta-skills (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites several prior works that explore iterative self-revision, self-feedback, and language-based internal evaluation (e.g., Self-Refine, SelFee, Reflexion), and notes emergent self-refinement/meta-skill behavior in large LLMs such as GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 and other top-tier LLMs (as discussed in citations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>References to larger LLMs (e.g., GPT-4) exhibiting emergent meta-skills; prior works implement iterative self-revision pipelines or agentic RL setups.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative self-revision / self-critique / language-based feedback methods (as in cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>General family: generate initial output, produce critique/feedback (sometimes via another call to an LLM or the same model), use critique to revise output; some works iterate multiple times or use the feedback as training signal.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various (cited papers apply to reasoning, agentic environments, and instruction following)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Cited works cover math reasoning, general instruction following, and agentic online RL scenarios where iterative critique/refinement is used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Mentioned as motivation and prior art: e.g., Ye et al. (2023) observed emergent meta-skills (self-refinement) in GPT-4; other cited works (Madaan et al., An et al., Shinn et al.) demonstrate iterative self-improvement can be effective in specific settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper notes general limitations of online self-improvement methods: increased inference cost, potential to repeat errors (since parameters are unchanged), and dependence on high-capacity models; these are discussed in relation to the cited methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SELF: Self-Evolution with Language Feedback', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Selfee: Iterative self-revising llm empowered by self-feedback generation <em>(Rating: 1)</em></li>
                <li>Self-consistency improves chain of thought reasoning <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Learning from mistakes makes llm better reasoner <em>(Rating: 1)</em></li>
                <li>Training language models with language feedback at scale <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5260",
    "paper_id": "paper-263334155",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "SELF",
            "name_full": "Self-Evolution with Language Feedback (SELF)",
            "brief_description": "A two-phase framework that first fine-tunes an LLM to acquire meta-skills (self-feedback and self-refinement) from a small supervised meta-skill corpus, then iteratively self-generates, self-evaluates, self-refines and filters responses from unlabeled prompts to create training data for repeated self-evolution fine-tuning; it also enables online self-refinement at inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vicuna-7b (primary); additional experiments on OpenL-LaMA (3b) and Vicuna-1.5 reported",
            "model_description": "Vicuna-7b: an open-source instruction-following model fine-tuned from LLaMA-7b (7B parameters). Experiments also include OpenLLaMA-3b and Vicuna-1.5 to test scalability.",
            "reflection_method_name": "Generate → Self-Feedback → Self-Refinement (meta-skill learning + iterative self-evolution)",
            "reflection_method_description": "Meta-skill learning: supervised fine-tuning on D_meta examples of (prompt, initial response, feedback, refined response) to teach the model to (1) produce natural-language feedback evaluating its own answer and (2) revise its answer using that feedback. Self-evolution: repeatedly (for multiple rounds) generate initial responses to unlabeled prompts, generate feedback and refined answers using the same model, filter refined answers using the model's own feedback (qualified(f)), and fine-tune the model on the cleaned refined (prompt, refined answer) pairs; at inference the model can optionally perform one round of self-refinement (generate feedback and produce a revised answer).",
            "num_iterations": 3,
            "task_name": "GSM8K, SVAMP (math); Vicuna testset and Evol-Instruct (general)",
            "task_description": "GSM8K & SVAMP: grade-school math word problems requiring multi-step arithmetic reasoning; Vicuna and Evol-Instruct testsets: general instruction-following and open-domain tasks evaluated by GPT-4 preference judgments.",
            "performance_with_reflection": "After full SELF (meta-skill + three self-evolution rounds) using Vicuna-7b: GSM8K direct generation 29.64% accuracy, GSM8K with inference self-refinement 31.31%; SVAMP direct 49.40%, SVAMP with self-refinement 49.80%. On general tests, Vicuna win rate vs baseline increased (Vicuna testset direct win rate: 72.5% after SELF; with self-refinement 75.0%; Evol-Instruct win rate: 52.8% direct, 55.5% with self-refinement).",
            "performance_without_reflection": "Baseline Vicuna + D_QA (pseudo-labeled QA SFT baseline): GSM8K direct 24.49% and SVAMP direct 44.90% (no meta-skill/self-evolution). Base Vicuna (no SELF) had lower scores (example: base Vicuna GSM8K ~16.43% in table).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative gains shown across ablations: SELF (Vicuna + D_meta + three self-evolution rounds) raises GSM8K direct accuracy from 24.49% (Vicuna + D_QA) to 29.64% (+5.15 percentage points) and SVAMP from 44.90% to 49.40% (+4.5 pp). Iterative self-evolution rounds produce incremental improvements (examples: 25.39% → 29.64% on GSM8K across rounds). Meta-skill training alone improves direct generation modestly (e.g., +0.90% GSM8K, +1.9% SVAMP versus D_QA), and inference self-refinement further improves SELF models (e.g., +1.67 pp on GSM8K). Case studies and qualitative analysis (fig.4, A.5) show SELF-enabled models correct errors during refinement where baseline models fail.",
            "limitations_or_failure_cases": "Reported limitations include: (1) potential plateau in gains across many self-evolution rounds (discussed theoretically in A.1.3); (2) inference-time self-refinement adds computational overhead (multi-turn inference); (3) if meta-skills are not well learned, self-refinement can be marginal or even harmful (baseline models showed marginal or negative self-refinement effects); (4) data filtering via self-feedback improves training-data accuracy but reduces data quantity (e.g., filtered training accuracy increased from 29.89% to 44.10% while test accuracy rose modestly from 26.90% to 27.67% because filtered set shrank), indicating trade-off between data quality and quantity; (5) SELF depends on the quality of the meta-skill corpus (GPT-4 generated D_meta yields better gains than GPT-3.5-generated D_meta).",
            "uuid": "e5260.0",
            "source_info": {
                "paper_title": "SELF: Self-Evolution with Language Feedback",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Inference Self-Refinement",
            "name_full": "Self-Refinement During Inference (one-round)",
            "brief_description": "At inference the model generates an initial answer, then generates natural-language feedback about that answer and uses it to produce a revised answer; typically executed once in the paper's evaluations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vicuna-7b variants (baseline and SELF-finetuned)",
            "model_description": "Applied both to base Vicuna and models after meta-skill/self-evolution training; refinement is done by the same model (no separate verifier).",
            "reflection_method_name": "Self-Refinement (single inference-round)",
            "reflection_method_description": "Procedure: produce initial response r; produce natural-language feedback f evaluating r; produce refined response r' conditioned on (p, r, f). In experiments the model performs one refinement step during inference.",
            "num_iterations": 1,
            "task_name": "GSM8K, SVAMP, general testsets",
            "task_description": "Same benchmarks as SELF; self-refinement applied at inference to attempt to improve single-run outputs.",
            "performance_with_reflection": "For Vicuna + SELF: GSM8K improved from 29.64% (direct) to 31.31% (+1.67 pp). SVAMP improved slightly (49.40% → 49.80%). General-domain win rates also improved when applying self-refinement (Vicuna testset from 72.5% → 75.0% for SELF).",
            "performance_without_reflection": "When not applying inference self-refinement, SELF models have the 'direct generation' scores (e.g., 29.64% GSM8K). Baseline models without meta-skill training often did not benefit from self-refinement: example baseline Vicuna + D_QA direct 24.49% → self-refinement 24.44% (slight drop).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Numeric improvements for SELF models (e.g., +1.67 pp GSM8K). Ablation shows that after meta-skill learning self-refinement 'consistently' improves performance across evolution steps, and qualitative case studies show SELF models fix errors during refinement while vanilla models often repeat mistakes.",
            "limitations_or_failure_cases": "If the model lacks learned meta-skills, self-refinement can be ineffective or harmful (examples: baseline Vicuna variants showed marginal or negative changes). Self-refinement increases inference cost and does not change model parameters (so recurring errors can persist across instances). Benefit size can be modest relative to training changes.",
            "uuid": "e5260.1",
            "source_info": {
                "paper_title": "SELF: Self-Evolution with Language Feedback",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (majority voting over sampled chain-of-thoughts)",
            "brief_description": "An existing decoding strategy where multiple reasoning traces (CoT) are sampled and the most consistent answer (majority vote) is selected to improve reasoning accuracy.",
            "citation_title": "Self-consistency improves chain of thought reasoning",
            "mention_or_use": "use",
            "model_name": "Vicuna-7b (used as comparison)",
            "model_description": "Applied at decoding time by sampling multiple chain-of-thought answers and selecting the most frequent final answer.",
            "reflection_method_name": "Self-Consistency (sampling + majority voting)",
            "reflection_method_description": "Generate multiple CoT outputs via sampling, then take the most common final answer as the prediction. It does not require explicit natural-language feedback or a refinement pass.",
            "num_iterations": null,
            "task_name": "GSM8K (math reasoning)",
            "task_description": "Grade-school math word problems where multiple reasoning chains are sampled.",
            "performance_with_reflection": "For base Vicuna, self-consistency yielded a notable improvement (reported as +3.13% on GSM8K for the base model). When combined with SELF, further gains were observed (example: combining self-refinement and self-consistency produced GSM8K 29.64% → 32.22%).",
            "performance_without_reflection": "Without self-consistency the corresponding scores are lower (e.g., base Vicuna direct generation lower by ~3.13 pp vs with self-consistency).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: base Vicuna showed +3.13 percentage points improvement on GSM8K with self-consistency; combining self-consistency with SELF further increased GSM8K accuracy (e.g., 29.64% → 32.22%). The paper notes diminishing returns as models become more certain after self-evolution.",
            "limitations_or_failure_cases": "Benefit reduces as model quality increases (diminishing returns), and self-consistency is primarily applicable to tasks with a uniquely correct answer (reasoning tasks) rather than broad open-ended tasks.",
            "uuid": "e5260.2",
            "source_info": {
                "paper_title": "SELF: Self-Evolution with Language Feedback",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Prior iterative/self-feedback methods (mentions)",
            "name_full": "Cited prior methods: Self-Refine, SelFee, Reflexion, Ye et al. emergent meta-skills (GPT-4)",
            "brief_description": "The paper cites several prior works that explore iterative self-revision, self-feedback, and language-based internal evaluation (e.g., Self-Refine, SelFee, Reflexion), and notes emergent self-refinement/meta-skill behavior in large LLMs such as GPT-4.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4 and other top-tier LLMs (as discussed in citations)",
            "model_description": "References to larger LLMs (e.g., GPT-4) exhibiting emergent meta-skills; prior works implement iterative self-revision pipelines or agentic RL setups.",
            "reflection_method_name": "Iterative self-revision / self-critique / language-based feedback methods (as in cited works)",
            "reflection_method_description": "General family: generate initial output, produce critique/feedback (sometimes via another call to an LLM or the same model), use critique to revise output; some works iterate multiple times or use the feedback as training signal.",
            "num_iterations": null,
            "task_name": "Various (cited papers apply to reasoning, agentic environments, and instruction following)",
            "task_description": "Cited works cover math reasoning, general instruction following, and agentic online RL scenarios where iterative critique/refinement is used.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Mentioned as motivation and prior art: e.g., Ye et al. (2023) observed emergent meta-skills (self-refinement) in GPT-4; other cited works (Madaan et al., An et al., Shinn et al.) demonstrate iterative self-improvement can be effective in specific settings.",
            "limitations_or_failure_cases": "The paper notes general limitations of online self-improvement methods: increased inference cost, potential to repeat errors (since parameters are unchanged), and dependence on high-capacity models; these are discussed in relation to the cited methods.",
            "uuid": "e5260.3",
            "source_info": {
                "paper_title": "SELF: Self-Evolution with Language Feedback",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Selfee: Iterative self-revising llm empowered by self-feedback generation",
            "rating": 1,
            "sanitized_title": "selfee_iterative_selfrevising_llm_empowered_by_selffeedback_generation"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Learning from mistakes makes llm better reasoner",
            "rating": 1,
            "sanitized_title": "learning_from_mistakes_makes_llm_better_reasoner"
        },
        {
            "paper_title": "Training language models with language feedback at scale",
            "rating": 2,
            "sanitized_title": "training_language_models_with_language_feedback_at_scale"
        }
    ],
    "cost": 0.014318999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SELF: SELF-EVOLUTION WITH LANGUAGE FEED-BACK
1 Feb 2024</p>
<p>Jianqiao Lu jqlu@cs.hku.hk 
The University of Hong Kong</p>
<p>Wanjun Zhong zhongwanjun1@huawei.com 
Huawei Noah's Ark Lab</p>
<p>Wenyong Huang wenyong.huang@huawei.com 
Huawei Noah's Ark Lab</p>
<p>Yufei Wang 
Huawei Noah's Ark Lab</p>
<p>Qi Zhu 
Huawei Noah's Ark Lab</p>
<p>Fei Mi 
Huawei Noah's Ark Lab</p>
<p>Baojun Wang 
Huawei Noah's Ark Lab</p>
<p>Weichao Wang 
Huawei Noah's Ark Lab</p>
<p>Xingshan Zeng 
Huawei Noah's Ark Lab</p>
<p>Lifeng Shang 
Huawei Noah's Ark Lab</p>
<p>Xin Jiang 
Huawei Noah's Ark Lab</p>
<p>Qun Liu 
Huawei Noah's Ark Lab</p>
<p>SELF: SELF-EVOLUTION WITH LANGUAGE FEED-BACK
1 Feb 20245ABE8FDC69A95167C09469EF3962BA24arXiv:2310.00533v4[cs.CL]Meta-Skill Learning Self-Refinement 1st Self-Evolve Meta-Skill Learning
Large Language Models (LLMs) have shown impressive adaptability in various fields, yet the optimal pathway of autonomous model evolution remains underexplored.Drawing inspiration from the self-driven learning process of humans, we introduce SELF (Self-Evolution with Language Feedback), a novel learning framework that empowers LLMs to continually self-improve their abilities.SELF initiates with a meta-skill learning process that equips the LLMs with capabilities for self-feedback and self-refinement.SELF employs language-based feedback for detailed and nuanced evaluations, pinpointing response flaws and suggesting refinements.Subsequently, the model engages in an iterative process of self-evolution: they autonomously generate responses to unlabeled instructions, refine these responses interactively, and use the refined and filtered data for iterative self-training, thereby progressively boosting their capabilities.Moreover, the SELF framework equips the model with the ability to self-refine during inference, leading to further improved response quality.Our experiments on mathematical and general tasks demonstrate that SELF enables the model to continually selfimprove without human intervention.The SELF framework indicates a promising direction for the autonomous evolution of LLMs, transitioning them from passive information receivers to active participants in their development.</p>
<p>INTRODUCTION</p>
<p>Large Language Models (LLMs), like ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023) , stand at the forefront of the AI revolution, demonstrating versatility across tasks.Despite their evident capabilities, the way towards achieving autonomous development of LLMs is still under-explored.</p>
<p>The development of automatically improved LLMs can draw inspiration from human self-driven learning mechanisms.When facing new challenges, humans naturally engage in a learning cycle of initial attempts, introspective feedback, and behavior refinement.This leads to a critical question: "Can LLMs mimic the human learning process, utilizing self-refinement to enhance their inherent capabilities?"Fascinatingly, a recent study (Ye et al., 2023) in top-tier LLMs such as GPT-4 has revealed emergent meta-skills for self-refinement, signaling a promising future direction for the selfevolution of LLMs.Despite this, current methods for LLM development typically rely on a single round of instruction fine-tuning (Wei et al., 2021;Zhou et al., 2023) with meticulously humancrafted datasets and reinforcement learning-based methods (Ouyang et al., 2022) that depend on an external reward model.These strategies not only require extensive resources and ongoing human intervention but also treat LLMs as mere passive repositories of information rather than active learners.These limitations hinder LLMs from tapping into their inherent capabilities, obstructing their progress toward a self-driven, autonomous learning paradigm.Thus, we introduce SELF (Self-Evolution with Language Feedback) framework, designed to unlock the potential for autonomous self-evolution in LLMs. Figure 1 depicts how SELF mimics human-like self-driven learning, emphasizing progressive improvement of model capability Figure 1: Evolutionary Journey of SELF: An initial LLM undergoes successive self-evolution iterations (1st, 2nd, 3rd), enhancing its capabilities and acquiring a self-refinement meta-skill.</p>
<p>with self-evolution training.At the core of SELF are the two meta-skills (self-feedback and selfrefinement), empowering the model to progressively self-evolve by training on its own synthesized data.Additionally, SELF leverages self-generated natural language feedback to offer in-depth analysis and guidance for refining responses, without the need for external rewards or direct human guidance.</p>
<p>Specifically, the SELF framework initiates by teaching LLMs essential meta-skills, namely selffeedback and self-refinement, using a limited set of examples.Once these skills are acquired, the model engages in a cycle of continuous self-evolution, iteratively training with extensive, selfgenerated data.Given a large-scale unlabeled corpus, this data is compiled from initial responses and refined through self-refinement and filtering, with model itself.During this iterative process, the quality of self-evolution training data and model capability are interactively improved, fostering ongoing self-evolution of LLMs.Crucially, in the inference phase, these learned meta-skills enable LLMs to further enhance response quality via self-refinement.In summary, the SELF framework transforms LLMs from passive recipients of data into active learners in self-evolution, and alleviates data scarcity issues by generating large-scale self-curated training datasets.This not only reduces the need for labor-intensive manual intervention but also promotes the continuous self-improvement of LLMs, establishing a more autonomous and efficient training approach.</p>
<p>We evaluate SELF in mathematical and general domains.SELF notably improves the test accuracy on mathematical domains (6.82% on GSM8k (Cobbe et al., 2021) and 4.9% on SVAMP (Patel et al., 2021)), and increases the win rate on general domain (10% on Vicuna testset (Lianmin et al., 2023) and 6.9% on Evol-Instruct testset (Xu et al., 2023)), compared with typical supervised finetuning.There are several insights gained from our experiments.Firstly, SELF can progressively enhance the model capability through self-evolution training.Secondly, the learning of meta-skills, specifically self-feedback and self-refinement, is crucial not only for equipping the model with selfimprovement abilities but also for boosting its direct response generation performance.Finally, the model demonstrates further improvement in its responses through self-refinement during the inference stage.</p>
<p>The main contributions are summarized as follows: (1) SELF empowers LLMs with self-evolving capabilities, allowing for autonomous model evolution, and reducing human intervention.(2) SELF facilitates self-refinement into smaller LLMs, even with challenging math problems.The capability of self-refinement was previously considered an emergent characteristic of top-tier larger LLMs.</p>
<p>(3) Experiments demonstrate the effectiveness of SELF in both mathematical and general domains, confirming its advanced capabilities in self-evolution and self-refinement.</p>
<p>Self-improvement in Inference Self-consistency (Wang et al., 2022a) is a straightforward and effective method to improve LLMs for reasoning tasks.After sampling a variety of reasoning paths, the most consistent answer is selected.During decoding, self-consistency is closely tied to the self-refinement capability of LLMs, on which our method is based.Unlike self-consistency, selfrefinement applies to a broader range of tasks, going beyond reasoning tasks with unique correct answers.Various research efforts have been undertaken to enhance the output quality of LLMs through online self-improvement (Shinn et al., 2023;Madaan et al., 2023;Ye et al., 2023;Chen et al., 2023;Ling et al., 2023).The main idea is to generate an initial output with an LLM.Then, the same LLM provides feedback on its output and employs this feedback to refine its initial output.This process can be iterative until the response quality is satisfied.While simple and effective, online self-improvement necessitates multi-turn inference for refinement, leading to increased inference computational overhead.Most importantly, online self-improvement does not prevent the model from repeating previously encountered errors, as the model's parameters remain unchanged.In contrast, SELF can self-improve with evolution training.</p>
<p>Autonomous Improvements of LLMs "Alignment" (Leike et al., 2018) aims to train agents to act in line with human intentions.Several research efforts (Ouyang et al., 2022;Bai et al., 2022;Scheurer et al., 2023) leverage Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017).RLHF begins with fitting a reward model to approximate human preferences.Subsequently, an LLM is finetuned through reinforcement learning to maximize the estimated human preference of the reward model.Reward Ranked Fine-tuning (RAFT) utilizes a reward model to rank responses sampled from an LLM.Subsequently, it fine-tunes the LLM using highly-ranked responses (Dong et al., 2023).Recent advancements in LLMs have explored Reinforcement Learning (RL) approaches that do not rely on human feedback.RLAIF (Pang et al., 2023) proposes to employ LLMs to label the preference data in replace of human supervision.LLMs are updated progressively through online RL in interacting with the environment in Carta et al. (2023).The connection between conventional RL research and RLHF in LLMs is discussed by Sun (2023).However, scalar rewards in Reinforcement Learning (RL) offer limited insights for evaluating complex reasoning tasks (Lightman et al., 2023), as they fail to specify detailed errors and optimization paths.Recognizing this limitation, the SELF framework suggests utilizing natural language feedback, which effectively guides the self-evolution of LLMs.Unlike scalar rewards, which require a retrained model for each evaluation protocol and dimension, natural language feedback is more flexible, addressing multiple aspects simultaneously.Furthermore, the RLHF process is intricate and computationally intensive, relies on external reward models, and demands manual tuning of hyperparameters for optimal performance.This approach lacks the adaptability to evolve intrinsically with the model itself.</p>
<p>Self-Evolution Training at Iter t (a) Meta-Skill Learning (b) Iterative Self-Evolution Training</p>
<p>Feedback &amp; Refinement Corpus</p>
<p>[Feedback]: The response directly adds the $450 to Dwayne's salary of $1,500 to get a total of $2,050..This is incorrect because it doesn't account for the fact that Brady makes more than Dwayne Judgement: incorrect</p>
<p>[Refinement]: Brady makes $450 more than Dwayne, so Brady makes $1,500 + $450 = $1,950 in a year.Therefore, the total amount that Brady and Dwayne make in a year is $1,950 + $1,500 = $3,450.</p>
<p>Question &amp; Initial Response</p>
<p>[Question]: Brady will make $450 more in a year than Dwayne.If Dwayne makes $1,500 in a year, how much will Brady and Dwayne make combined in a year?</p>
<p>[Initial Answer]: The total amount that Brady and Dwayne will make combined in a year is $450 + $1,500 = $&lt;&lt;450+1500=2050&gt;&gt;2050.Therefore, the combined salary of Brady and Dwayne in a year is $2,050.</p>
<p>Self-Evolving LLM</p>
<p>Learning Meta-Skills of Self-feedback and Self-Refinement</p>
<p>LLM with Meta-Skills (𝑴</p>
<p>Self-Evolution Training data</p>
<p>Iter t</p>
<p>Figure 2: Illustration of SELF.The "Meta-Skill Learning" (left) phase empowers the LLM to acquire meta-skills in self-feedback and self-refinement.The (b)"Self-Evolution" phase (right) utilizes meta-skills for self-evolution training with self-curated data, enabling continuous model enhancement.</p>
<p>As depicted in Fig. 2, the SELF framework enhances model capabilities through a two-stage learning phase: (1) Meta-skill Learning Phase: This phase uses an annotated meta-skill training corpus to fine-tune the model, and equips the model with essential meta-skills for self-feedback and self-refinement with limited supervised examples, laying a foundation for self-evolution.(2) Self-Evolution Phase: With the acquired meta-skills, the model progressively improves through multiple iterations of the self-evolution training process.Each iteration begins with the model itself autonomously creating high-quality training data from unlabeled prompts.Then, the model is finetuned using this data.The process is further illustrated in Alg. 1 in Appendix A.7.</p>
<p>In SELF, Natural Language Feedback plays a crucial role in guiding the evolutionary process.This approach offers a more fine-grained and informative evaluation compared to the traditional method of using a scalar reward.The latter evaluates quality along a single dimension with a numerical value from a reward model.In contrast, natural language feedback provides a detailed and descriptive analysis of the processes involved in a response, which is particularly critical in complex reasoning tasks.This also allows for evaluation across multiple dimensions, offering greater flexibility.Importantly, it guides the refinement process by suggesting directions for improvement.The efficacy of natural language feedback in enhancing evaluation accuracy and model performance is shown in § 4.3.2.</p>
<p>META-SKILL LEARNING</p>
<p>Meta-skill learning targets on instill two essential meta-skills into LLMs for self-evolution.(1) Self-Feedback Ability: This skill enables LLMs to evaluate their responses using natural language feedback.This provides the suggestion for further refinement, thus laying a solid foundation for subsequent self-refinement.Self-feedback also enables the model to filter out low-quality self-evolution training data if a response is judged as unqualified by the model ( § 3.2.1).(2) Self-Refinement Ability: Self-refinement enables the model to optimize its responses based on self-feedback.This ability has two applications: (1) enhancing the quality of the self-evolution training corpus ( § 3.2.1)and ( 2) improving model performance by refining the models' outputs during inference ( § 3.3).</p>
<p>These meta-skills are acquired by fine-tuning the model using the Meta-Skill Training Corpus ( § 3.1.1)with designed training objective ( § 3.1.2).The resulting model is denoted as M meta .</p>
<p>META-SKILL TRAINING CORPUS</p>
<p>The meta-skill training corpus D meta represents the generation, feedback, and refinement process.It is constructed as follows: (1) For each unlabeled prompt p, the initial model M init generates an initial response r.(2) An annotator L provides evaluation feedback f for the initial response r, then produces a refined answer r according to the feedback f .Each instance in D meta takes the form (p, r, f, r), representing the process of response evaluation and refinement.An example instance of D meta is provided in appendix A.6.</p>
<p>TRAINING OBJECTIVE</p>
<p>In the meta-skill learning phase, the objective is to empower the initial model M init to develop meta-skills, resulting in an enhanced model M meta .This process is guided by the cross-entropy loss following the maximum likelihood estimation (MLE) paradigm:
L meta (ϕ) = −E (p,r,f,r)∼Dmeta log τ ϕ (f |p, r) + log τ ϕ (r|p, r, f ) + β log τ ϕ (r|p) ,(1)
where p is prompt, r is the initial model response, f is the feedback to the model response r, and r is the revised response based on feedback f .τ ϕ (y|x) denotes the probability distribution given by the auto-regressive language model with parameters ϕ predicting the response y given the input prompt x.The coefficient β in eq. ( 1) controls a balanced emphasis on direct response generation and the model's capability for self-feedback and self-refinement.</p>
<p>Insight.</p>
<p>Training with D meta aims to achieve two goals: (i) To guide the model in generating feedback (f ) concerning its initial responses (r) (self-feedback) and subsequently employing this feedback to enhance the quality of the final answer (r) (self-refinement).(ii) Training with D meta instructs the model to process problems in a Chain-of-Thought (CoT) manner.This involves evaluating the initial response, integrating the feedback, and then revising the response in a chain process
Ψ(r|p) := r,f τ ϕ (r|p) • τ ϕ (f |p, r) • τ ϕ (r|p, r, f ).</p>
<p>SELF-EVOLUTION TRAINING PROCESS</p>
<p>The model M meta , equipped with meta-skills, undergoes progressive improvement through multiple iterations of the self-evolution training process.Each iteration of the self-evolution process begins with the model autonomously creating high-quality training data ( § 3.2.1)from an unlabeled corpus.</p>
<p>With an unlabeled dataset of prompts, the model generates initial responses and then refines them through self-feedback and self-refinement.These refined responses, superior in quality, are further filtered with self-feedback and utilized as the training data for the model's subsequent self-evolution training ( § 3.2.2).This autonomous self-evolving process interactively improves LLMs as the improved model capability leads to better data quality, which in turn boosts model performance.It also alleviates the data scarcity problem by self-generating data.</p>
<p>SELF-EVOLUTION TRAINING DATA</p>
<p>Let M t evol denotes the model at t th iteration and initialize M 0 evol from M meta .During t th self-evolution iteration , M t−1 evol processes each unlabeled prompt p by first generating an initial response r.r is then refined using the model's self-feedback f , resulting in a self-refined response r.The prompts and their corresponding self-refined responses(p, r) are then incorporated into the t th round selfevolution datasets, denoted as D t evol , for subsequent self-evolution processes.Data Filtering with Self-feedback: To enhance the quality of D t evol , we employ the self-feedback capability of M t−1 evol to filter out data of lower quality.M t−1 evol evaluates the self-refined data, revol , keeping only the responses that meet high-quality standards.The effect is analyzed in § 4.6.</p>
<p>To mitigate the catastrophic forgetting issue of meta-skill, the meta-skill learning data D meta are also included in self-evolution training.At t th iteration, the model undergoes self-evolution training with the updated self-curated data D t evol , improving its performance and aligning it more closely with human values.</p>
<p>MATHEMATICAL MODELING</p>
<p>Main Objective.We denote τ t ϕ as the probability distribution generated by M t evol with parameters ϕ.The self-evolution training loss L t evol (ϕ) is defined as:
L t evol (ϕ) = −E pevol E revol∼Ψ t−1 (revol|pevol) log τ t ϕ (r evol |p evol ) = −E pevol revol Ψ t−1 (r evol |p evol ) log τ t ϕ (r evol |p evol ) ,(2)
where p evol is sampled from unlabeled prompts corpus (detiled in appendix A.3.2) for self-evolution t th round.The joint probability distribution is:
Ψ t−1 (r evol |p evol ) = revol,fevol τ t−1 ϕ (r evol |p evol ) • τ t−1 ϕ (f evol |r evol , p evol ) • τ t−1 ϕ (r evol |f evol .r evol , p evol ) .
(3)</p>
<p>The rationale behind learning from Ψ t−1 (r evol |p evol ) is discussed in appendix A.1.1.</p>
<p>Optimizing eq. ( 2) is equivalent to minimizing the Kullback-Leibler (KL) divergence:
KL(Ψ t−1 (r evol |p evol )||τ t ϕ (r evol |p evol )) = revol Ψ t−1 (r evol |p evol ) log Ψ t−1 (r evol |p evol ) τ t ϕ (r evol |p evol ) = − H(Ψ t−1 (r evol |p evol )) constant for fixed Ψ t−1 − revol Ψ t−1 (r evol |p evol ) log τ t ϕ (r evol |p evol eq. (2)
).</p>
<p>(4)</p>
<p>The optimization of KL divergence is to fine-tune the model parameters ϕ to ensure that the model's predictive probability distribution τ t ϕ aligns with the joint probability of the preceding iteration's chain process (Ψ t−1 ).The goal is to enhance the model's ability to directly produce refined responses (r evol ) in the t th iteration, effectively condensing the intricate process of generation, feedback, and refinement from the (t − 1) th iteration.This advancement demonstrates the model's evolving capability to streamline the complex steps into a more straightforward inference.The potential plateau is discussed in appendix A.1.3.</p>
<p>Further Analysis.Assuming that each self-evolution round is effective, implying that as t increases, the quality of responses sampled from Ψ t improves, optimizing the KL divergence as described in eq. ( 4) is fundamentally a process aimed at enhancing the direct generation of high-quality responses.Before delving deeper, it is crucial to introduce several key concepts.We define a binary variable X to evaluate the quality of responses.A higher probability, p(X = 1 | p evol , revol ), indicates a higher quality of the response revol in relation to the prompt p evol .For the self-evolving model with parameters ϕ at the t th iteration, the model's log-likelihood of producing high-quality responses to a specified prompt is defined as follows:
log p t (X = 1 | p evol ) := log r p(X = 1 | p evol , revol )τ t ϕ (r evol |p evol ) .
By minimizing the KL divergence in eq. ( 4), we can increase log p t (X = 1 | p evol ) by progressively improving its Evidence Lower Bound (ELBO):
log p t (X = 1 | p evol ) = log revol p(X = 1 | p evol , revol )τ t ϕ (r evol |p evol ). = log E Ψ t−1 (revol|pevol) p(X = 1 | p evol , revol )τ t ϕ (r evol |p evol ) Ψ t−1 (r evol |p evol ) ≥ E Ψ t−1 (revol|pevol) log p(X = 1 | p evol , revol )τ t ϕ (r evol |p evol ) Ψ t−1 (r evol |p evol ) = E Ψ t−1 (revol|pevol) [log p(X = 1 | p evol , revol )] − KL(Ψ t−1 (r evol |p evol )||τ t ϕ (r evol |p evol ))
eq. ( 4)</p>
<p>.</p>
<p>The entire self-evolution training process can be viewed as a continuous exploration of inherent model capabilities in generation, self-feedback, and self-refinement, ultimately enhancing the model's ability to generate high-quality responses directly.</p>
<p>Overall Objective.In the iterative self-evolution process, meta-skills, i.e., the ability to selffeedback and self-refinement, is crucial for guiding the evolution process.We incorporate D meta into self-evolution training to mitigate the potential catastrophic forgetting of meta-skills:
L t meta (ϕ) = −E (p,r,f,r)∼Dmeta log τ t ϕ (f |p, r) + log τ t ϕ (r|p, r, f ) .
The total objective for the t th round of self-evolution is:
L t self (ϕ) = L t evol (ϕ) + L t meta (ϕ).</p>
<p>RESPONSE REFINEMENT DURING INFERENCE</p>
<p>Equipped with the meta-skills for self-feedback and self-refinement, the model can conduct selfrefinement during inference.Specifically, the model generates an initial response and then refines it using self-refinement, akin to the method described in § 3.1.Response refinement during inference consistently improves the model's performance as shown in § 4.3.</p>
<p>EXPERIMENT SETTINGS</p>
<p>4.1 EVALUATION Inference Setting.We adopt two inference settings: (1) Direct Response (default): the model directly answers the question with a Zero-shot Chain of Thought (CoT) methodology (Kojima et al., 2022), which is the default setting to evaluate the model capability directly;</p>
<p>(2) Self-Refinement: during inference, the model self-refines its original answer for once, as described in § 3.3.</p>
<p>Benchmarks.We evaluate on two mathematical benchmarks to show the efficacy of SELF on complex reasoning tasks, and further verify the generalizability of SELF on two general benchmarks.</p>
<p>SETUP AND BASELINES</p>
<p>The complete SELF framework includes meta-skill training with D meta , three iterations of selfevolution training, and optional self-refinement during inference.Our evaluation primarily focuses on assessing how self-evolution training can progressively enhance the capabilities of LLMs.For building the meta-skill training corpus D meta , we employ GPT-4 as the language model labeler L due to its proven proficiency in refining responses (An et al., 2023) via the prompt described in appendix A.21 .The data statistic of D meta is shown in appendix A.3.1 and further details of unlabeled corpus construction is described in appendix A.3.2.We note that all model training utilized the same training hyperparameters, as shown in appendix A.4.</p>
<p>We note that the SELF framework is compatible with versatile LLMs.In this study, we perform the experiment with Vicuna-7b (Chiang et al., 2023), a capable open-source instruction-following model fine-tuned from LLaMA-7b (Touvron et al., 2023), will be referred to simply as "Vicuna" in subsequent sections.To verify the generalizability of SELF, we also experiment with OpenL-LaMA Geng &amp; Liu (2023) and Vicuna-1.5 (Chiang et al., 2023) in appendix A.12.All the compared baselines are outlined:</p>
<p>(1) Vicuna + D QA : To demonstrate the improvement brought by SELF and exclude the impact of standard domain-specific supervised fine-tuning (SFT), we set a direct baseline that trained solely on pseudo-labeled question-answer pairs in the meta-skill training corpus.Specifically, we construct D QA , which includes all the (p, r) pairs from D meta , and fine-tune the model as:
L QA (ϕ) = −E (p,r)∼DQA [log τ ϕ (r|p)] .
We refer to this approach as Vicuna + D QA , the most straightforward baseline.The performance gap between Vicuna + D QA and SELF verify the efficacy of the proposed SELF framework, excluding the effect of training on domain-specific QA data.</p>
<p>(2) RLHF: we utilize the RLHF implementation from trlx2 .We apply the same SFT model as the policy model for RLHF, Vicuna + D QA as described above, which is consistent with SELF.The reward model is initialized from Vicuna-7b and is fine-tuned using pair-wise comparison data derived from the meta-skill training corpus D meta ( § 3.1.1),where the refined response r is presumed to be better than the original one r.</p>
<p>(3) Self-Consistency: we compare the self-refinement inference strategy in SELF with the Self-Consistency (Wang et al., 2022a) (i.e., multiple sampling and selecting an answer with majority voting) and examine their combined efficacy.In table 1, we compare SELF against baseline models, as detailed in § 4.2.This comparison elucidates SELF's effectiveness in enhancing LLM performance through self-evolution and offers several key insights:</p>
<p>MAIN RESULT</p>
<p>MATH TEST</p>
<p>(1) Self-Evolution Enhances LLM: Vicuna + SELF significantly outperforms its baseline Vicuna + D QA (24.49%</p>
<p>+5.15% − −−−− → 29.64% on GSM8K and 44.90%</p>
<p>+4.5% − −−− → 49.40% on SVAMP) in direct response setting, showcasing self-evolution is effective in optimizing LLMs.</p>
<p>(2) SELF Instills Self-Refine Capability in LLMs: The integration of self-refinement inference strategy with Vicuna + SELF further boosts performance (29.64% +1.67% − −−−− → 31.31%), while baseline models show marginal or negative effect via self-refinement.We also provide a case analysis for the limited self-refinement ability of baseline models, as shown in fig. 4.This indicates that SELF can instill advanced self-refinement capabilities into small LLMs like Vicuna (7B), although selfrefinement was previously shown as an exclusive ability of large LLMs (Ye et al., 2023) like GPT-4.</p>
<p>(3) SELF can work with Self-Consistency: SELF works effectively with self-consistency, improving accuracy across models.The base Vicuna model, which may have uncertainties in its outputs, shows notable improvement with self-consistency, achieving a +3.13% increase.As the model progresses through self-evolution training and becomes more certain of generating correct math answers, the benefit from self-consistency reduces.Combining self-refinement with self-consistency further elevates performance (e.g., 29.64% +2.58% − −−−− → 32.22% on GSM8K), indicating that these two strategies can complement each other effectively.</p>
<p>(4) Pseudo-Labeled D QA Enhances Performance: The inclusion of pseudo-labeled QA data D QA enhances Vicuna's performance, suggesting that tuning with domain-specific QA data can enhance task-specific problem-solving.</p>
<p>ABLATION STUDY</p>
<p>We conduct ablation experiments on SVAMP and GSM8K datasets to assess the incremental effect of each stage.While baseline models exhibit slight or even adverse effects via self-refinement, the SELF framework endows LLMs with an inherent capability through meta-skill learning and multi-iterations of self-evolution training.As depicted in table 3, our framework facilitates gradual performance improvements through successive SELF stages.Observations are highlighted below:</p>
<p>(1) Meta-skill Training Elevates Performance: Training with the meta-skills dataset D meta as defined in eq. ( 1), and setting β = 1 for a fair comparison with the question-answer dataset D QA , improves direct response performance.Specifically, we observe an increase of +0.90% on the GSM8K dataset and +1.9% on the SVAMP dataset, compared to using the D QA dataset alone.This underscores the interesting finding that arming the model with self-feedback and self-refinement meta-capability implicitly elevates its capacity to generate superior responses directly, even without explicit self-refinement.We offer an insight in appendix A.1.2.</p>
<p>(2) Continuous Improvement through Self-Evolution: The results reveal that three self-evolution rounds consecutively yield performance enhancements (e.g., 25.39% − −−−− → 29.64% on GSM8K).This shows that the model actively self-evolves, refining its performance autonomously without additional manual intervention.</p>
<p>(3) Persistent Efficacy of Self-Refinement: After meta-skill learning, regardless of model variation, executing self-refinement consistently results in notable performance improvements.This shows that the self-refinement meta-capability learned by SELF is robust and consistent across evolution steps.The following insight is highlighted: The combination of self-refinement and self-feedback filtering results in higher self-evolution training data accuracy (+14.21%) and improved fine-tuned model performance (+0.77%).Despite the significant training data accuracy improvement, the performance gain is modest due to the reduced data size (from 4K to 1.8K) after filtering.</p>
<p>ANALYSIS ON DATA FILTERING WITH SELF-FEEDBACK</p>
<p>CONCLUSION</p>
<p>We present SELF (Self-Evolution with Language Feedback), a novel framework that enables LLMs to achieve progressive self-evolution through self-feedback and self-refinement.Unlike conventional methods, SELF transforms LLMs from passive information recipients to active participants in their evolution.The adoption of natural language feedback promotes a more informative and finegrained evaluation.We discuss why it's necessary to optimize τ t ϕ (r evol |p evol ) in the t th round self-evolution by learning from Ψ t−1 (r evol |p evol ), and why we believe samples from Ψ t−1 (r evol |p evol ) are typically of higher quality than those from τ t−1 ϕ (r evol |p evol ) directly.Firstly, similar to the insights analyzed in § 3.1.2,we believe that a process akin to CoT, involving feedback followed by refinement before providing an answer, helps in generating high-quality responses.Secondly, r evol is already a reasonably good output after meta-skill learning and previously (t − 1) rounds of self-evolution.We can assume that the self-feedback f evol is informative, hence revol ∼ τ t−1 ϕ (r evol |p evol , r evol , f evol ) is of higher quality than r evol ∼ τ t−1 ϕ (r evol |p evol ) because it incorporates useful feedback information.If f evol suggests that the initial response r evol does not require refinement, we still proceed through the process of revising from r evol to revol using f evol , but set revol = r evol .By doing so, we ensure that the quality of revol is at least as good as that of r evol .</p>
<p>Moreover, as described in § 3.2.2,we utilize Data Filtering with Self-feedback.In other words, we only keep revol evaluated as qualified, allowing us to emphasize high-quality outputs and further improve τ t ϕ .</p>
<p>A.1.2WHY INTEGRATION OF META-SKILL TRAINING DATA D META LEVATES DIRECT QA</p>
<p>The D meta dataset trains the model to not only modify answers but also to fully grasp a prompt, create feedback, and then develop a revised answer.This approach resembles training the model to think through a problem in a chain-of-thought methodically (CoT) manner, before responding.</p>
<p>The training encompasses a thorough examination of the entire process, which not only betters the model's direct response capability but also enriches its understanding of the logic behind those answers, thereby enhancing its generalization ability.</p>
<p>A.1.3 POTENTIALLY LIMITED PLATEAU OF SELF-EVOLUTION TRAINING</p>
<p>Based on eq. ( 2) and eq.( 3), the model in the t th round is updated to improve direct response quality by incorporating the generate-feedback-refinement process from the (t − 1) th round.This is based on the assumption that the refined response is superior to the initial one generated by M t−1 evol .As illustrated in Fig. 1, the direct generation performance of M t evol (green curve) consistently falls below the self-refinement of M t−1 evol (blue curve).The self-refinement gains in the (t − 1) th round indicate the potential benefit that the t th round self-evolution could bring to direct generation.This</p>
<p>(2) Second/Third rounds: 10K/15K prompts were generated using Self-Instruct method (Wang et al., 2022b)</p>
<p>A.4 TRAINING HYPERPARAMETERS</p>
<p>Our experiments were conducted in a computing environment with 8 NVIDIA V100 GPUs, each having 32GB of memory.All models were fine-tuned in a full-parameter setting.We utilized the AdamW optimizer for model training over 3 epochs, with a batch size of 128.The learning rate was set at 2e-5, including a 3% learning rate warmup period.Below we provide a comprehensive overview of the training hyperparameters employed in table 5.These parameters were uniformly applied across all training methods in our experiments.</p>
<p>A.5 CASE STUDY ANALYSIS</p>
<p>This subsubsection provides an in-depth case study that contrasts the performance of the original Vicuna and Vicuna + SELF models.Illustrated in fig.4, both models perform initial predictions, followed by self-feedback and refinement steps.Notably, Vicuna's refinement fails to correct its initial errors, while Vicuna + SELF effectively utilizes self-feedback and refinement to derive an accurate and logically coherent answer.</p>
<p>A.6 META-SKILL TRAINING CORPUS</p>
<p>The example shown below exemplifies a standard training example from our meta-skill corpus.It illustrates the model's initial response, followed by its self-feedback, and the ensuing refinement.This process demonstrates how the model is trained for self-feedback and self-refinement capabilities.</p>
<p>On Monday, Tiffany had 3 bags of cans.</p>
<p>On the next day, she found 7 more bags of cans.</p>
<p>So, on Tuesday, Tiffany had a total of 3 + 7 = &lt;&lt;3+7=10&gt;&gt;10 bags of cans.</p>
<p>In addition to the 10 bags of cans, she also found 44 bags of bottles.</p>
<p>So, Tiffany had a total of 10 bags of cans + 44 bags of bottles = &lt;&lt;10+44=54&gt;&gt;54 bags in total.</p>
<p>Therefore, Tiffany had 54 bags of cans altogether.</p>
<p>Direct generation</p>
<p>Response Analysis: The response correctly identifies that Tiffany had 10 bags of cans on Monday and 44 bags of bottles on the next day.It then adds these two quantities together to find the total number of bags Tiffany had.</p>
<p>Judgement: Correct</p>
<p>Correct Answer: Therefore, Tiffany had a total of 10 bags of cans and 44 bags of bottles, for a total of 54 bags.Therefore, the correct answer is 54 bags.</p>
<p>On Monday, Tiffany had 3 bags of cans.On the next day, she found 7 more bags of cans.So, she had a total of 3 + 7 = 10 bags of cans on Tuesday.### 10.0 Question On Monday, Tiffany had 3 bags of cans.On the next day, she found 7 more bags of cans.So, she had a total of 3 + 7 = 10 bags of cans on Tuesday.On Monday, she also had 44 bags of bottles.So, overall, Tiffany had 10 bags of cans and 44 bags of bottles on Monday and Tuesday.Therefore, Tiffany had a total of 10 + 44 = 54 bags of cans and bottles.</p>
<p>Response Analysis:</p>
<p>The response correctly identifies that Tiffany had 3 bags of cans on Monday and 7 more on Tuesday, totaling 10 bags.However, the response incorrectly adds the number of bags of cans and bottles together, stating that Tiffany had 10 bags of cans and 44 bags of bottles on Monday and Tuesday.The question only asks for the number of bags of cans, not bags of bottles.</p>
<p>Judgement: incorrect</p>
<p>Correct Answer: On Monday, Tiffany had 3 bags of cans.On Tuesday, she found 7 more bags of cans, for a total of 3 + 7 = 10 bags of cans.Therefore, Tiffany had 10 bags of cans altogether.Both models initially produce answers, followed by self-feedback and self-refinement.Vicuna maintains the incorrect response after refinement, whereas Vicuna+SELF demonstrates enhanced selfrefinement, leading to a correct and logically consistent solution.</p>
<p>Self</p>
<p>Question: Greg has an alarm set to ring three times a day as a reminder.When the alarm goes off, it continues to ring until Greg turns it off.The first time it went off today, it rang four times.The second time it went off, it rang for three times as long as the first time.The third time, it rang for half as long as the second time.How many times did the alarm ring in all?</p>
<p>Original Response: The alarm rang four times the first time, three times as long as the first time the second time, and half as long as the second time the third time.So, the total number of times the alarm rang is 4 + 3<em>3 + 2</em>2 = 24.Feedback: The response correctly interprets the problem.The first time the alarm rang, it rang for 4 times.The second time it rang, it rang for 3 times as long as the first time, which is 3<em>4 = 12 times.The third time it rang, it rang for half as long as the second time, which is 12/2 = 6 times.However, the final calculation is incorrect.The total number of times the alarm rang is not 4 + 3</em>3 + 2*2 = 24, but 4 + 12 + 6 = 22.Judgement: incorrect Refined Response: The alarm rang four times the first time, three times as long as the first time the second time, and half as long as the second time the third time.So, the total number of times the alarm rang is 4 + 12 + 6 = 22.</p>
<p>A.7 ALGORITHM</p>
<p>The "Two-Phase SELF Process" algorithm outlines a method for developing a base language model through a two-staged approach: Meta-Skill Learning and Self-Evolving.The process starts with training on a "Meta-Skill Learning corpus", which consists of data representing the generation, feedback and refinement process.Following this, the model enters the "Self-Evolving Phase", where it undergoes iterative refinements, employing data augmentation in each iteration to produce selfrefined outputs from its previously refined versions.This iterative self-evolution aims to leverage accumulated knowledge and further enhance the model with newly generated data.A.8 DATA FILTERING STANDARDS</p>
<p>We design a boolean function, qualified(f ), to evaluate feedback f across different domains, determining if a response to a specific prompt satisfies essential quality criteria.</p>
<p>In the Math Domain,the function assesses feedback based on the explicit statement of "correctness" in the evaluator's judgment, aligned with the prompt structure in appendix A.2.1.It checks if the word "correct" immediately follows the phrase "judgment:" in the feedback.A presence of "correct" results in qualified(f ) returning 1, meeting the qualification criteria.Absence leads to a return of 0.</p>
<p>For the General Domain, following the structure in appendix A.2.2, qualified(f ) extracts and evaluates a numerical rating from the feedback.If the rating, found after "Rating:", is 7 or higher, the function returns 1, indicating qualification.Ratings below 7 return 0, failing to meet the threshold.A rating of 7 balances quality and training data quantity.</p>
<p>qualified(f ) is key in both domains for filtering and assessing feedback quality, ensuring only highquality responses are used for refined answer generation in self-evolution training.Post data filtering, Ψ t−1 in eq. ( 3) requires an update to Ψ ′t−1 = Ψ t−1 × qualified(f ), adding a quality filter through self-feedback.For clarity, we continue using original formulation as stated in eq.(3) in the main text.</p>
<p>A.9 MULTIPLE V.S. SINGLE SELF-REFINEMENT This study explores the effects of two meta-skill training data organization strategies on model performance: (1) Multiple Self-Refinement (D meta-multi ), involving the sampling of three responses for the model to choose the best for refinement, and (2) Single Self-Refinement (D meta ), where the model generates and refines a single response.</p>
<p>table 6 compares these methods' performances.Both strategies show performance gains with increased training data volume.However, as data volume expands, the multiple-response refinement shows a smaller improvement in direct generation performance (+4.02%) than the single-response method (+5.84%).Considering the simplicity and computational efficiency of the single-response method, which only samples one response during inference, and its better performance than the multiple-response approach, we have opted for the single-response refinement strategy in our experiments., where the model is trained simultaneously with all rounds of self-evolution data, also shows notable enhancements in direct generation (+2.73%) and selfrefinement (+3.94%).In contrast, "Continual Training (D t evol Only)", which trains the model sequentially with self-evolution data from each round, demonstrates more modest gains (+0.38% in direct generation, +0.98% in self-refinement).The relatively lower performance of the latter approach highlights the importance of a mixed data strategy for effective self-evolution training.</p>
<p>Throughout our main text, we have consistently employed the "Restart Training" method.This approach was selected for its superior performance, as evidenced in table 7.In addition, the integration of D meta into the self-evolution training is crucial to prevent the potential catastrophic forgetting of meta-skills.This strategy is essential for preserving the effectiveness and reliability of the selfevolution training process, as highlighted in § 3.2.2.</p>
<p>A.11 SELF VS.SUPERVISED FINE-TUNING ON 7.5K GSM8K TRAINING DATA.</p>
<p>When fine-tuned on the GSM8K 7.5k training set, the Vicuna model achieves an accuracy of 35.70%, which is lower than the SELF method (37.87%).</p>
<p>The experiments in table 8 use 7.5k meta-skill data, ensuring a fair comparison with the supervised fine-tuned model.This approach differs from the one in table 1, where only 3.5k meta-skill data are used.table 8 indicates that, with 7.5k unlabeled training prompts for the meta-skill learning corpus, Vicuna + D QA achieves 28.05%.Post meta-skill learning, direct generation results improve to 31.23%, further increasing to 32.98% after self-refinement.Subsequent self-evolution rounds lead to performance gains, reaching 37.87%(direct generation) and 38.12%(self-refinement) in the second round, outperforming supervised fine-tuning (35.70%).</p>
<p>Continuous Improvement of SELF vs. Supervised Fine-tuning: SELF's primary advantage lies in its ability for continuous improvement and adaptation.In contrast to supervised fine-tuning,</p>
<p>, based on a template shown in appendix A.3.2 with initial 4 to 6 seed examples.General Domain: 15K unlabeled prompts from ShareGPT dialogues were used for self-evolution training data construction.You are an experienced instruction creator.You are asked to develop 3 diverse instructions according to the given examples.Here are the requirements: 1.The generated instructions should follow the task type in the given examples.2. The language used for the generated instructions should be diverse.Given examples: {examples} The generated instructions should be: A. ... B. ... C. ...</p>
<p>Figure 4 :
4
Figure 4: Case study comparing the original Vicuna (left) and Vicuna+SELF (right) on a SVAMP problem.Both models generate direct predictions and undergo self-feedback and self-refinement.Both models initially produce answers, followed by self-feedback and self-refinement.Vicuna maintains the incorrect response after refinement, whereas Vicuna+SELF demonstrates enhanced selfrefinement, leading to a correct and logically consistent solution.</p>
<p>𝒔𝒆𝒍𝒇 𝟎 ) Initial LLM Starting from 𝑴 𝒔𝒆𝒍𝒇 𝟎 , the model Undergoes self-evolution Meta-Skill Training Unlabeled Prompts Iter t
Filtering𝒕'𝟏 𝑴 𝒔𝒆𝒍𝒇via Self-FeedbackInitial ResponseSelf-Refinedfrom 𝑴 𝒔𝒆𝒍𝒇 𝒕'𝟏𝑴 𝒔𝒆𝒍𝒇 𝒕'𝟏Response from 𝑴 𝒔𝒆𝒍𝒇 𝒕'𝟏</p>
<p>Table 1 :
1
Experiment results on GSM8K and SVAMP compare SELF with other baseline methods.
We evaluate the impact of Self-Evolution (SE), Self-Consistency (SC), and Self-Refinement (SR)strategies on model performance.ModelSE SC SR GSM8K(%) SVAMP(%)16.4336.40Vicuna✓19.5640.20✓15.6336.8024.4944.90Vicuna + D QA✓25.7046.00✓24.4445.30✓29.6449.40Vicuna + SELF (Ours)✓✓29.8750.20✓✓31.3149.80✓✓✓32.2251.20</p>
<p>Table 3 :
3
Performance under various training settings of SELF.A checkmark ✓ in a column denotes the additive adoption of the corresponding setting in that training scenario.We present two kinds of inference results: Direct Response (DR) and Self-Refinement (SR), the latter conducts selfrefinement to DR.
SVAMP (%)GSM8K (%)D QAD metaSelf Evol.DRSRDRSR1st 2nd 3rd36.4 36.8 16.43 15.6344.9 45.3 24.49 24.44✓46.8 47.0 25.39 28.28✓47.8 48.0 27.67 29.34✓✓48.9 49.0 28.66 29.87✓✓✓49.4 50.2 29.64 31.31✓✓✓✓</p>
<p>Table 4 :
4
Impact of Data Filtering with Self-Feedback on GSM8K."Training Acc." shows the accuracy of the self-evolution training data post-filtering, and "Test Acc." represents the model's test performance post-training on these filtered data.
Filter Strategy Training Acc. (%) Test Acc. (%)Unfiltered29.8926.90Filtered44.1027.67
Table4presents an analysis of filtering self-evolution training data using self-feedback ( § 3.2.1) on GSM8K, focusing on training data quality and its influence on self-evolution training.The filtering criteria are detailed in appendix A.8.</p>
<p>Through meta-skill learning, SELF equips LLMs with the capability for selffeedback and self-refinement.This empowers the models to evolve their capabilities autonomously, utilizing self-evolution training and online self-refinement.Experiments conducted on benchmarks underscore SELF's capacity to progressively enhance model capabilities while reducing the need for human intervention.SELF represents a leading step in autonomous LLM development, leading to an insight that models are capable of continual learning and self-evolution.Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi.Self-instruct: Aligning language model with self generated instructions.arXiv preprint arXiv:2212.10560,2022b.Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le.Finetuned language models are zero-shot learners.arXiv preprint arXiv:2109.01652,2021.Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo.Selfee: Iterative self-revising llm empowered by self-feedback generation.Blog post, May 2023.URL https://kaistai.github.io/SelFee/.Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al.Lima: Less is more for alignment.arXiv preprint arXiv:2305.11206,2023.
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, andDaxin Jiang. Wizardlm: Empowering large language models to follow complex instructions.arXiv preprint arXiv:2304.12244, 2023.Seonghyeon Ye, A APPENDIXA.1 DISCUSSIONA.1.1 WHY REFINEMENT IS BETTER</p>
<p>Table 5 :
5
Training hyperparameters.
Hyperparameter Global Batch SizeLREpo. Max Length Weight Decay Warmup RatioValue1282 × 10 −53204800.03</p>
<p>The final out-come is an advanced Language Model that has significantly evolved from its original state through multiple self-evolution stages.More details are delineated in Alg. 1.
Algorithm 1: Two-Phase SELF ProcessData: (1) Meta-Skill training data (Dmeta) and (2) unlabeled promptsInput: An initial Language Model MinitResult: A stronger Language Model M T evol after self-evolving// Meta-Skill Learning PhaseData: Meta-Skill learning corpus (Dmeta)Mmeta = Supervised fine tuning(Minit, Dmeta );// Self-Evolving PhaseInitialize M 0 evol with Mmeta;foreach iteration t in 1 to Number of self-evolving iterations T do// Data-AugmentationInitialize D t evol as an empty set;foreach prompt p i evol in t th unlabeled prompts do Generate direct output r i evol using M t−1 evol ; Generate self-refined output ri evol from r i evol using M t−1 evol ; Use M t−1 evol to filter the self-refined output;Add (p i evol , ri evol ) to D t evol , where ri is the refined response;end// Self-Evolution TrainingM t evol = Supervised fine tuning(M t−1 evol , D t evol );end// Training Completereturn Improved Language Model M T evol ;</p>
<p>Table 6 :
6
Performance comparison of single and multiple response refinement with varying volumes of meta-skill training data.The arrow indicates improvement from direct generation to selfrefinement: "direct generation → self-refinement".
Data Size Vicuna + Dmeta Vicuna + Dmeta-multi3.5k25.39 → 28.2825.92 → 27.297.5k31.23 → 32.9829.94 → 32.14A.10 SELF-EVOLUTION TRAINING: CONTINUAL TRAINING V.S. RESTART TRAINING</p>
<p>Table 7 :
7
Analysis about varied self-evolution training methodologies on GSM8K.Restart Training", which combines meta-skill learning corpus with all rounds of self-evolution training data, significantly improves direct generation (+3.18%) and self-refinement (+3.85%).
Training ApproachDirect Generation (%) Self-Refinement (%)Base Model24.4924.49Restart Training27.6729.34Continual Training (Mixed Data)27.2228.43Continual Training (D t evol Only)24.8725.85
""Continual Training (Mixed Data)"</p>
<p>Separate prompts have been designed for the math domain appendix A.2.1 and general domain appendix A.2.2.
https://github.com/CarperAI/trlx
Adhering to the official recommendation https://github.com/arkilpatel/SVAMP/tree/ main, training prompts consist of MAWPS(Koncel-Kedziorski et al., 2016) and ASDiv-A(Miao et al., 2020) 
COMPARISON WITH RLHFIn table 2, we compare the performance of SELF with RLHF.To alleviate the effect led by different amounts of training data and make a fair comparison, for SELF, we only adopt data solely from the initial round of self-evolution training.This ensures the same training data quantity with RLHF and leads to sub-optimal results compared with the one in table 2. As table2shows, RLHF achieves a 25.55% accuracy on GSM8K, which is lower than the 27.67% performed by SELF.We observe that the simple scalar reward of RLHF often fails to identify the correctness of the reasoning process, which limits performance improvements.On the GSM8K test set, for incorrect answers produced by the SFT model (Vicuna + D QA ), the reward model only identifies 24% of them as incorrect, i.e., the reward model assigns lower scalar rewards to incorrect answers compared to correct answers.In contrast, SELF utilizes informative natural language feedback to provide a more accurate assessment.It correctly identifies 72% of incorrect answers.GENERAL TESTWe test the efficacy and generalizability of SELF on general domain benchmarks, explicitly using the Vicuna and Evol-Instruct test sets.Three configurations of the Vicuna model are evaluated: Vicuna, Vicuna + D QA , and Vicuna + SELF.We utilize GPT-4 to evaluate the models' responses on both test sets.We follow the assessment methodology proposed by(Xu et al., 2023), which mitigated the order bias presented in the evaluation procedures.The results are depicted in Figure3.In the figure, blue represents the number of test cases where the model being evaluated is preferred over the baseline model (Vicuna), as assessed by GPT-4.Yellow denotes test cases where both models perform equally, and pink indicates the number of test cases where the baseline model is favored over the model being evaluated.Vicuna Lost Tie Vicuna WonVicuna + !" + SELF (Self-Refinement)Vicuna + !"In the Vicuna testset, SELF increases direct response win rate from 65.0% to 72.5% compared with Vicuna + D QA .After self-refinement, the win rate is further improved to 75.0%.In the Evol-Instruct testset, the win rate of Vicuna + D QA is 48.6%.SELF increases the win rate to approximately 52.8%.Applying self-refinement during inference further improves the win rate to 55.5%.These findings in the general domain highlight the SELF framework's adaptability and robustness, particularly when self-refinement is employed, showcasing its efficacy across varied test domains.also helps determine when to halt the self-evolution process, i.e., the process can be stopped when self-refinement brings no benefit to the direct response.A.2 PROMPT OF GENERATING FEEDBACK AND REFINEMENT FOR META-SKILL CORPUSWe introduce the prompt for generating feedback and refinement in two domains: Math and General.We outline specific prompts designed to guide the evaluation and improvement of responses to questions for building D meta in each domain.A.2.1 MATH DOMAINFor the Math Domain, the prompt instructs evaluators to assess the quality of a response to a math question, provide a step-by-step analysis, and determine its correctness.If the response is incorrect, the evaluator is asked to refine and provide a correct answer.Prompt for feedback and refinement: (Feedback) Please assess the quality of the response to the given question.Here is the question: p.Here is the response: r.Firstly, provide a step-by-step analysis and verification for response starting with "Response Analysis:".Next, judge whether the response correctly answers the question in the format of "judgment: correct/incorrect".(Refinement) If the answer is correct, output it.Otherwise, output a refined answer based on the given response and your assessment.A.2.2 GENERAL DOMAINFor the general test, aligned with the methodology described in section 3, we deploy the following prompt to guide an LLM-based annotator in generating response feedback and refinement.This prompt serves as the foundation for the meta-skill learning corpus and assists in producing selfevolution training data in the general test setting.Prompt for feedback and refinement: (Feedback) Please assess the quality of response to the given question.Here is the question: p.Here is the response: r.Firstly provide an analysis and verification for response starting with "Response Analysis:".Next, then rate the response on a scale of 1 to 10 (1 is worst, 10 is best) in the format of "Rating:" (Refinement) Finally output an improved answer based on your analysis if no response is rated 10.A.3 DATA GENERATIONThe D meta dataset was generated using 3.5k unlabeled prompts from GSM8K and 2K from SVAMP 3 .For general tests, 6K conversations were selected from 90K ShareGPT dialogues to form the general D meta data.A.3.2 UNLABELED PROMPTS FOR SELF-EVOLUTION TRAININGMath Domain: For math tests, unlabeled prompts in self-evolution training were sourced as follows:(1) First round self-evolving phase: 4K leftover prompts from GSM8k and 1K from SVAMP, excluding those used in D meta .SELF doesn't rely on human or external LLM annotations (like GPT3.5/GPT4) for training data in self-evolution training.A.12 SCALABILITY OF SELF FRAMEWORKTo explore how SELF performs with different starting model qualities, we conduct experiments using the OpenLlama-3b model(Geng &amp; Liu, 2023), a smaller LLM along with a stronger LLM, VicunaV1.5(finetunedfrom Llama2-7b)l(Chiang et al., 2023), on the GSM8K dataset.This allows us to assess SELF's adaptability to model quality.Experiments with SELF are based on the first round of self-evolution.The results are as follows: Applicability and Robustness of SELF Framework: The average improvement of 17.32% via direct generation and 16.87% after self-refinement underscores the framework's scalability and efficacy.It reveals a consistent positive impact of the SELF Framework across diverse models.SELF Framework exhibits enhanced performance on more powerful models: As shown in table 9, applying SELF to VicunaV1.5 results in the most significant gains -30.22% in direct generation and 32.43% after self-refinement, surpassing the performance on Vicuna and OpenLlama-3b.This indicates that the effectiveness of the SELF framework improves with the underlying model's capabilities.A.13 IMPACT OF META-SKILL CORPUS QUALITYWe examine the influence of meta-skill learning quality on the self-evolution process with the following results:The presented table 10 demonstrates the remarkable performance improvements achieved by using GPT-4 for generating the meta-skill corpus in our SELF framework, compared to using GPT-3.5-turbo.The table shows significant enhancements in both direct generation and self-refinement across training stages when GPT-4 is utilized.For instance, in the "Vicuna + D meta " stage, direct generation performance increases from 24.84% with GPT-3.5-turbo to 25.39% with GPT-4, marking a gain of 0.55%.Similarly, in the "Vicuna + D meta + SELF Evolution" stage, the self-refinement result improves from 25.47% with GPT-3.5-turbo to 29.34% with GPT-4, showing an enhancement of 3.87%.This analysis highlights the significant impact of utilizing high-quality meta-skill training data on the performance of the Vicuna model within the SELF framework.The shift from GPT-3.5-turbo to GPT-4 for the generation of the meta-skill corpus leads to consistent improvements in both Direct Generation and Self-Refinement metrics.A.14 SINGLE-ROUND VS. ITERATIVE SELF-EVOLUTION TRAININGGiven an equal number of unlabeled prompts, we evaluate the effectiveness of training within a single-round versus iterative training.The former method uses a single model to self-curate training data from all available unlabeled prompts at once.In contrast, the latter method involves dividing the unlabeled prompts into multiple parts.For the iterative approach, the model is initially trained on a portion of the unlabeled prompts and self-curated labels.Following this, the trained model is employed to create new training data based on previously unused prompts.As described in our main text, we divide the unlabeled prompts into three parts, enabling the model to undergo three iterative rounds of self-evolution.11shows that in the "Single-Round" training, the performance is 28.40% for direct generation and 30.55% for self-refinement.In contrast, the iterative approach yields higher scores of 29.64% for direct generation and 31.31% for self-refinement.Advantages of Iterative Training:Iterative training benefits from the enhanced capabilities of LLMs in subsequent rounds, which generate higher-quality training data and lead to improved test performance.
Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, Weizhu Chen, arXiv:2310.20689Learning from mistakes makes llm better reasoner. 2023arXiv preprint</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, arXiv:2204.058622022arXiv preprint</p>
<p>Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer, arXiv:2302.02662Grounding large language models in interactive environments with online reinforcement learning. 2023arXiv preprint</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, arXiv:2304.051282023arXiv preprint</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, March 2023</p>
<p>Deep reinforcement learning from human preferences. Advances in neural information processing systems. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, 201730</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, Tong Zhang, arXiv:2304.06767Raft: Reward ranked finetuning for generative foundation model alignment. 2023arXiv preprint</p>
<p>Openllama: An open reproduction of llama. Xinyang Geng, Hao Liu, May 2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, 10.48550/arXiv.2205.119162022</p>
<p>Association for Computational Linguistics. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, 10.18653/v1/N16-1136Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaJune 2016MAWPS: A math word problem repository</p>
<p>Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, Shane Legg, arXiv:1811.07871Scalable agent alignment via reward modeling: a research direction. 2018arXiv preprint</p>
<p>Vicuna-blog-eval. Zheng Lianmin, Chiang Wei-Lin, Zhuang Siyuan, 2023Ryans</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan. 2023arXiv preprint</p>
<p>Deductive verification of chain-of-thought reasoning. Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su, arXiv:2306.038722023arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, arXiv:2303.176512023arXiv preprint</p>
<p>A diverse corpus for evaluating and developing english math word problem solvers. Chao-Chun Shen-Yun Miao, Keh-Yih Liang, Su, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>. Openai, Chatgpt, OpenAI. Gpt-4 technical report. 2022. 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Language model self-improvement by reinforcement learning contemplation. Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-Hui Chen, Jiacheng Xu, Zongzhang Zhang, Yang Yu, arXiv:2305.144832023arXiv preprint</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsJune 2021</p>
<p>Training language models with language feedback at scale. Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, Ethan Perez, arXiv:2303.167552023arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 2023</p>
<p>Reinforcement learning in the era of llms: What is essential? what is needed? an rl perspective on rlhf, prompting, and beyond. Hao Sun, arXiv:2310.061472023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022aarXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
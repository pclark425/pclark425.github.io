<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5954 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5954</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5954</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-d0edff5402edeab721b7681643e1ff7c2354de4a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d0edff5402edeab721b7681643e1ff7c2354de4a" target="_blank">Leveraging pre-trained language models for mining microbiome-disease relationships</a></p>
                <p><strong>Paper Venue:</strong> BMC Bioinformatics</p>
                <p><strong>Paper TL;DR:</strong> This study establishes that pre-trained language models excel as transfer learners when fine-tuned with domain and problem-specific data, enabling them to achieve state-of-the-art results even with limited training data for extracting microbiome-disease interactions from scientific publications.</p>
                <p><strong>Paper Abstract:</strong> Background The growing recognition of the microbiome’s impact on human health and well-being has prompted extensive research into discovering the links between microbiome dysbiosis and disease (healthy) states. However, this valuable information is scattered in unstructured form within biomedical literature. The structured extraction and qualification of microbe-disease interactions are important. In parallel, recent advancements in deep-learning-based natural language processing algorithms have revolutionized language-related tasks such as ours. This study aims to leverage state-of-the-art deep-learning language models to extract microbe-disease relationships from biomedical literature. Results In this study, we first evaluate multiple pre-trained large language models within a zero-shot or few-shot learning context. In this setting, the models performed poorly out of the box, emphasizing the need for domain-specific fine-tuning of these language models. Subsequently, we fine-tune multiple language models (specifically, GPT-3, BioGPT, BioMedLM, BERT, BioMegatron, PubMedBERT, BioClinicalBERT, and BioLinkBERT) using labeled training data and evaluate their performance. Our experimental results demonstrate the state-of-the-art performance of these fine-tuned models ( specifically GPT-3, BioMedLM, and BioLinkBERT), achieving an average F1 score, precision, and recall of over $$>0.8$$ > 0.8 compared to the previous best of  0.74. Conclusion Overall, this study establishes that pre-trained language models excel as transfer learners when fine-tuned with domain and problem-specific data, enabling them to achieve state-of-the-art results even with limited training data for extracting microbiome-disease interactions from scientific publications.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5954.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5954.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large generative transformer language model used in this study (davinci variant) and fine-tuned to map evidence+question prompts to one of four relation labels between microbes and diseases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Generative transformer LLM (reported in paper as 175 billion parameters) used in a generative/completion setting; accessed and fine-tuned via the OpenAI API (davinci).</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Extract/distill generalizable microbe–disease relationship rules from biomedical text by generating one of four labels (positive, negative, relate, NA) given evidence and a question.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Biomedical literature — microbiome / disease relationships</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Generative prompt-completion approach: prompts composed of an Evidence string and Question string (with fixed stop separator). Tested zero-shot and two-shot few-shot prompting, then fine-tuned via OpenAI API; inference used temperature=0, top_p=1, max_tokens=1.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Categorical relation rules of the form 'microbe X is positively/negatively/associated (relate)/not-associated (NA) with disease Y' distilled from many sentences; i.e., generalizable microbe–disease association rules.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accuracy, weighted average F1 score, precision, recall, per-class precision/recall/F1, 5-fold cross-validation averages; precision-recall analysis for discriminative models (PR AUC reported for BioLinkBERT).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Zero-shot: poor (F1 ~0.50, accuracy 0.48). Two-shot few-shot: only marginal improvement (F1 ~0.56, accuracy 0.57). After fine-tuning, GPT-3 achieved one of the best overall results (5-fold CV mean F1 = 0.810 ± 0.025; accuracy = 0.814 ± 0.021), but exhibited variability across runs and produced occasional empty or ill-formed outputs; per-class performance showed poor NA-class recall (NA F1 ~0.431).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human-curated training data used for fine-tuning (GSC); human re-annotation of NA class by two postdocs; human inspection for evaluating outputs and comparing against MDIDB; calibrating prompts and selecting inference params.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Gold-standard corpus (GSC) of 1100 human-annotated sentences from Wu et al. (2021) describing microbe–disease interactions; corrected NA labels by the authors prior to fine-tuning. Also comparison against MDIDB extractions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Generative variability between runs; occasional empty outputs; outputs not always conforming cleanly to discrete class labels; poor performance in zero-/few-shot without domain fine-tuning; low NA-class performance due to class imbalance and prior labeling errors; interpretability of generative probabilities limited.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Fig. 3 example where GPT-3 in zero-shot returned 'relate' and in few-shot returned 'positive' despite ground truth being 'negative'; after fine-tuning GPT-3 improves substantially but still shows poor NA recall.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging pre-trained language models for mining microbiome-disease relationships', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5954.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5954.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioMedLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioMedLM (2.7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biomedical domain generative language model (reported 2.7B parameters) evaluated for generative relation extraction of microbe–disease pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>BioMedLM (stanford-crfm/BioMedLM)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Domain-specific generative LLM pre-trained on biomedical literature (noted in paper as 2.7B parameters), GPT-2 style backbone used in generative setting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Generate relation labels (positive/negative/relate/NA) from evidence+question prompts to distill microbe–disease association rules.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Biomedical literature — microbiome/disease</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Generative prompt-completion fine-tuning: prompts similar to GPT-3; fine-tuned on A40 GPU with deepspeed for 20 epochs (batch_size=2, grad_accum=2, lr=2e-6). Inference variability observed.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Microbe–disease association labels distilled from text (categorical association rules).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same as other models: accuracy, F1, precision, recall, 5-fold CV.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>BioMedLM produced poor or no sensible outputs in zero-/few-shot; after fine-tuning it achieved competitive precision (reported precision 0.822 ± 0.030) and F1 ~0.804 ± 0.028, with overall accuracy ~0.806 ± 0.028; generative-model instability and alignment to strict labels remained a concern.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Fine-tuning on human-annotated GSC; human reannotation of NA class; manual inspection of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>GSC (1100 sentences) corrected for NA labels; same 5-fold CV evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Did not produce sensible outputs out-of-the-box in zero-/few-shot; generative nature makes outputs and probabilities misaligned with discrete labels; variability across runs; occasional empty outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>BioMedLM fine-tuned showed high precision for some classes but shared the common generative-model issues (variability, label alignment) reported for GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging pre-trained language models for mining microbiome-disease relationships', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5954.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5954.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific generative transformer trained on PubMed abstracts, evaluated here for generative extraction of microbe–disease relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Generative pre-trained transformer for biomedical text generation (paper reports training on ~15M PubMed abstracts, model size cited ~1.5B parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Generate discrete relation labels from evidence+question prompts to extract microbe–disease association rules.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Biomedical literature (microbiome/disease)</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Generative prompt-completion; fine-tuning performed on an NVIDIA RTX 2080 Ti with hyperparameters similar to BioGPT's DDI experiments for relation extraction; prompts structured like GPT-3 prompts with separators.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Categorical rules of association between microbes and diseases distilled from sentence-level evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accuracy, F1, precision, recall, per-class metrics, 5-fold CV.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>BioGPT produced no sensible outputs in zero-/few-shot in this task; after fine-tuning it reached modest discriminative performance (accuracy ~0.732 ± 0.017, F1 ~0.726 ± 0.017) but underperformed relative to top fine-tuned discriminative models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Fine-tuned on human-labeled GSC after NA reannotation; human evaluation of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>GSC (1100 sentences) with corrected NA labeling; 5-fold CV used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Poor zero-/few-shot behavior; less competitive after fine-tuning compared to other models; generative inconsistencies and occasional empty outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Authors report BioGPT often failed to generate usable outputs in zero- and few-shot contexts for this specific relation extraction task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging pre-trained language models for mining microbiome-disease relationships', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5954.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5954.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioLinkBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioLinkBERT-base</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-based discriminative model pre-trained on biomedical text with added document link (citation) information; achieved top discriminative performance when fine-tuned for relation extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>BioLinkBERT-base</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>BERT-derived encoder model pre-trained on biomedical abstracts and augmented with citation-link information to better capture cross-document knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Classify evidence+question pairs into one of four relation labels (positive, negative, relate, NA) to distill structured microbe–disease association rules.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Biomedical literature — microbiome/disease relation extraction</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Discriminative fine-tuning: input = Evidence string [SEP] Question string tokenized (max seq length 512). Fine-tuning hyperparameters: learning rate 5e-5, weight decay 0.01, epochs=7, Adam optimizer, layer-wise LR decay 0.9; 5-fold cross-validation to report metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Structured classification rules mapping textual evidence to discrete microbe–disease association types (positive/negative/relate/NA).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accuracy, weighted average F1 score, precision, recall, per-class precision/recall/F1, 5-fold CV; precision-recall curves and PR-AUC (BioLinkBERT AUC ~0.85).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>BioLinkBERT fine-tuned yielded the best discriminative results: accuracy 0.811 ± 0.029, weighted F1 0.804 ± 0.036, precision 0.813 ± 0.034, recall 0.811 ± 0.028. Per-class metrics strong for positive/negative/relate but poor for NA (NA F1 ~0.457).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Model fine-tuned on human-annotated GSC (with NA reannotations); human curation used in dataset preparation and result inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Corrected GSC (1100 sentences; 178 of original 258 NA re-labeled), 5-fold cross-validation for evaluation; comparisons made against MDIDB outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Poor NA-class performance (low recall and F1) attributed to class imbalance and prior ambiguities in label definitions; BERT-model sequence length limited to 512 tokens; entity recognition/normalization issues in upstream pipeline not addressed here.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Per-class table (Table 3) showing high precision/recall for negative/positive/relate classes but substantially lower NA recall (~0.447 ± 0.186), illustrating difficulty in discerning 'no relation' cases even after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging pre-trained language models for mining microbiome-disease relationships', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5954.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5954.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PubMedBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PubMedBERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-based model pre-trained from scratch on PubMed abstracts used here as a discriminative encoder for relation classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>PubMedBERT</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>BERT-derived encoder pre-trained specifically on PubMed abstracts and full-text to improve biomedical task performance (pretrained model microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext used).</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Discriminative classification of evidence+question into microbe–disease relation labels to produce generalizable association rules.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Biomedical literature (microbiome/disease)</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Discriminative fine-tuning with Evidence+Question input, tokenization, max seq length 512; same optimizer and hyperparameters as other discriminative experiments (lr 5e-5, epochs=7, etc.); evaluated via 5-fold CV.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Text-to-label mapping producing standardized microbe–disease association rules.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accuracy, weighted F1, precision, recall (5-fold cross-validation averages).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>PubMedBERT fine-tuned showed strong performance: accuracy 0.782 ± 0.022, F1 0.778 ± 0.019, precision 0.783 ± 0.021, recall 0.782 ± 0.022 — an improvement over many baselines but not top-performing.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Trained on human-annotated corrected GSC; human reannotation of NA; manual evaluation for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Corrected GSC (1100 sentences) used for fine-tuning and 5-fold CV.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>NA-class detection remains weak across models; restricted to 512 token BERT input window; success depends on quality of labeled corpus and upstream entity normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Model achieved balanced improvements across classes but still struggled with NA cases, reflecting dataset labeling challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging pre-trained language models for mining microbiome-disease relationships', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5954.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5954.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioMegatron</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioMegatron</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large BERT-style biomedical model pre-trained on PubMed and related corpora, fine-tuned here for discriminative relation extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>BioMegatron</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>BERT-family model pre-trained from scratch on PubMed abstracts and free full-text commercial-collection; reported to outperform prior SOTA on several biomedical tasks (architecture: large transformer encoder).</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Classify evidence+question pairs into relation labels to distill microbe–disease association rules from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Biomedical literature — microbiome/disease</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Discriminative fine-tuning with Evidence+Question token pairs (max 512 tokens), same optimizer/hyperparameters as other discriminative models; evaluated via 5-fold CV.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Structured classification rules mapping sentence evidence to microbe–disease association labels.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accuracy, F1, precision, recall (5-fold CV).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>BioMegatron fine-tuned achieved accuracy 0.778 ± 0.008 and F1 0.769 ± 0.013, outperforming basic BERT but below BioLinkBERT and GPT-3 fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Fine-tuned on human-annotated corrected GSC; human reannotation of NA class; manual evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Corrected GSC (1100 sentences), 5-fold cross-validation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>NA-class problems persist; discriminative models limited by sequence length; relies heavily on quality of training labels and entity normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Performed better than generic BERT but did not reach the top metrics; used as part of ablation/benchmarking against other domain encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging pre-trained language models for mining microbiome-disease relationships', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5954.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5954.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-base-uncased</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bert-base-uncased</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General-purpose encoder transformer (BERT-base) used as a baseline discriminative model for relation extraction between microbes and diseases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>BERT (bert-base-uncased)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Bidirectional transformer encoder pretrained on general-domain corpora (Wikipedia, BooksCorpus); used here as a baseline discriminative classifier fine-tuned on domain task data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Baseline discriminative classification of evidence+question pairs into microbe–disease relation labels.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Biomedical literature (applied via transfer fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Fine-tuned discriminatively with Evidence+Question pair tokenized; hyperparameters as for other discriminative models (lr 5e-5, epochs=7, max length 512); 5-fold CV evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Baseline mapping rules from textual evidence to categorical relation labels.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accuracy, F1, precision, recall (5-fold CV).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Fine-tuned BERT baseline achieved accuracy 0.733 ± 0.018 and F1 0.731 ± 0.015—substantially improved after fine-tuning but outperformed by domain-specific models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Trained on corrected human-annotated GSC; human reannotation of NA class.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Corrected GSC of 1100 annotated sentences; 5-fold CV.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Generic pretraining limits domain-specific understanding without fine-tuning; NA-class remains challenging; BERT limited to 512 token input.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Served as an ablation/baseline demonstrating the value of domain-specific pretraining and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging pre-trained language models for mining microbiome-disease relationships', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5954.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5954.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioClinicalBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioClinicalBERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-derived model pre-trained on biomedical and clinical text evaluated here as a discriminative encoder for relation extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>BioClinicalBERT</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Further development of BioBERT pre-trained on a large corpus of biomedical and clinical text aiming to improve clinical/biomedical NLP tasks (BERT-family encoder).</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Discriminative classification of evidence+question pairs into microbe–disease relation labels to derive association rules.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Biomedical/clinical literature (microbiome/disease)</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Discriminative fine-tuning on Evidence+Question pairs; same training hyperparameters as other discriminative models; evaluated with 5-fold CV.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Categorical microbe–disease association rules inferred from sentence evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accuracy, F1, precision, recall (5-fold CV).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>BioClinicalBERT fine-tuned showed lower performance relative to other domain models (accuracy 0.729 ± 0.032, F1 0.724 ± 0.029). Good on some classes but inferior overall compared to BioLinkBERT and GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Fine-tuning and data curation used human-labeled GSC with NA reannotation; manual output inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Corrected GSC (1100 sentences), 5-fold cross-validation evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Underperformed compared to best-performing domain encoders; NA-class detection and entity normalization remain issues.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Lower overall metrics highlight model-to-task mismatch compared to models pre-trained specifically for biomedical citation-linked corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging pre-trained language models for mining microbiome-disease relationships', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mining microbe-disease interactions from literature via a transfer learning model <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>GPT-3 models are poor few-shot learners in the biomedical domain <em>(Rating: 2)</em></li>
                <li>Biogpt: generative pre-trained transformer for biomedical text generation and mining <em>(Rating: 1)</em></li>
                <li>BiomedIm: a domain-specific large language model for biomedical text <em>(Rating: 1)</em></li>
                <li>LinkBERT: Pretraining Language Models with Document Links <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5954",
    "paper_id": "paper-d0edff5402edeab721b7681643e1ff7c2354de4a",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [
        {
            "name_short": "GPT-3",
            "name_full": "Generative Pre-trained Transformer 3",
            "brief_description": "A large generative transformer language model used in this study (davinci variant) and fine-tuned to map evidence+question prompts to one of four relation labels between microbes and diseases.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "GPT-3 (text-davinci-003)",
            "llm_model_description": "Generative transformer LLM (reported in paper as 175 billion parameters) used in a generative/completion setting; accessed and fine-tuned via the OpenAI API (davinci).",
            "task_goal": "Extract/distill generalizable microbe–disease relationship rules from biomedical text by generating one of four labels (positive, negative, relate, NA) given evidence and a question.",
            "domain": "Biomedical literature — microbiome / disease relationships",
            "methodology": "Generative prompt-completion approach: prompts composed of an Evidence string and Question string (with fixed stop separator). Tested zero-shot and two-shot few-shot prompting, then fine-tuned via OpenAI API; inference used temperature=0, top_p=1, max_tokens=1.",
            "type_of_qualitative_law": "Categorical relation rules of the form 'microbe X is positively/negatively/associated (relate)/not-associated (NA) with disease Y' distilled from many sentences; i.e., generalizable microbe–disease association rules.",
            "evaluation_metrics": "Accuracy, weighted average F1 score, precision, recall, per-class precision/recall/F1, 5-fold cross-validation averages; precision-recall analysis for discriminative models (PR AUC reported for BioLinkBERT).",
            "results_summary": "Zero-shot: poor (F1 ~0.50, accuracy 0.48). Two-shot few-shot: only marginal improvement (F1 ~0.56, accuracy 0.57). After fine-tuning, GPT-3 achieved one of the best overall results (5-fold CV mean F1 = 0.810 ± 0.025; accuracy = 0.814 ± 0.021), but exhibited variability across runs and produced occasional empty or ill-formed outputs; per-class performance showed poor NA-class recall (NA F1 ~0.431).",
            "human_involvement": "Human-curated training data used for fine-tuning (GSC); human re-annotation of NA class by two postdocs; human inspection for evaluating outputs and comparing against MDIDB; calibrating prompts and selecting inference params.",
            "dataset_or_corpus": "Gold-standard corpus (GSC) of 1100 human-annotated sentences from Wu et al. (2021) describing microbe–disease interactions; corrected NA labels by the authors prior to fine-tuning. Also comparison against MDIDB extractions.",
            "limitations_or_challenges": "Generative variability between runs; occasional empty outputs; outputs not always conforming cleanly to discrete class labels; poor performance in zero-/few-shot without domain fine-tuning; low NA-class performance due to class imbalance and prior labeling errors; interpretability of generative probabilities limited.",
            "notable_examples": "Fig. 3 example where GPT-3 in zero-shot returned 'relate' and in few-shot returned 'positive' despite ground truth being 'negative'; after fine-tuning GPT-3 improves substantially but still shows poor NA recall.",
            "uuid": "e5954.0",
            "source_info": {
                "paper_title": "Leveraging pre-trained language models for mining microbiome-disease relationships",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "BioMedLM",
            "name_full": "BioMedLM (2.7B)",
            "brief_description": "A biomedical domain generative language model (reported 2.7B parameters) evaluated for generative relation extraction of microbe–disease pairs.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "BioMedLM (stanford-crfm/BioMedLM)",
            "llm_model_description": "Domain-specific generative LLM pre-trained on biomedical literature (noted in paper as 2.7B parameters), GPT-2 style backbone used in generative setting.",
            "task_goal": "Generate relation labels (positive/negative/relate/NA) from evidence+question prompts to distill microbe–disease association rules.",
            "domain": "Biomedical literature — microbiome/disease",
            "methodology": "Generative prompt-completion fine-tuning: prompts similar to GPT-3; fine-tuned on A40 GPU with deepspeed for 20 epochs (batch_size=2, grad_accum=2, lr=2e-6). Inference variability observed.",
            "type_of_qualitative_law": "Microbe–disease association labels distilled from text (categorical association rules).",
            "evaluation_metrics": "Same as other models: accuracy, F1, precision, recall, 5-fold CV.",
            "results_summary": "BioMedLM produced poor or no sensible outputs in zero-/few-shot; after fine-tuning it achieved competitive precision (reported precision 0.822 ± 0.030) and F1 ~0.804 ± 0.028, with overall accuracy ~0.806 ± 0.028; generative-model instability and alignment to strict labels remained a concern.",
            "human_involvement": "Fine-tuning on human-annotated GSC; human reannotation of NA class; manual inspection of outputs.",
            "dataset_or_corpus": "GSC (1100 sentences) corrected for NA labels; same 5-fold CV evaluation.",
            "limitations_or_challenges": "Did not produce sensible outputs out-of-the-box in zero-/few-shot; generative nature makes outputs and probabilities misaligned with discrete labels; variability across runs; occasional empty outputs.",
            "notable_examples": "BioMedLM fine-tuned showed high precision for some classes but shared the common generative-model issues (variability, label alignment) reported for GPT-3.",
            "uuid": "e5954.1",
            "source_info": {
                "paper_title": "Leveraging pre-trained language models for mining microbiome-disease relationships",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "BioGPT",
            "name_full": "BioGPT",
            "brief_description": "A domain-specific generative transformer trained on PubMed abstracts, evaluated here for generative extraction of microbe–disease relations.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "BioGPT",
            "llm_model_description": "Generative pre-trained transformer for biomedical text generation (paper reports training on ~15M PubMed abstracts, model size cited ~1.5B parameters).",
            "task_goal": "Generate discrete relation labels from evidence+question prompts to extract microbe–disease association rules.",
            "domain": "Biomedical literature (microbiome/disease)",
            "methodology": "Generative prompt-completion; fine-tuning performed on an NVIDIA RTX 2080 Ti with hyperparameters similar to BioGPT's DDI experiments for relation extraction; prompts structured like GPT-3 prompts with separators.",
            "type_of_qualitative_law": "Categorical rules of association between microbes and diseases distilled from sentence-level evidence.",
            "evaluation_metrics": "Accuracy, F1, precision, recall, per-class metrics, 5-fold CV.",
            "results_summary": "BioGPT produced no sensible outputs in zero-/few-shot in this task; after fine-tuning it reached modest discriminative performance (accuracy ~0.732 ± 0.017, F1 ~0.726 ± 0.017) but underperformed relative to top fine-tuned discriminative models.",
            "human_involvement": "Fine-tuned on human-labeled GSC after NA reannotation; human evaluation of outputs.",
            "dataset_or_corpus": "GSC (1100 sentences) with corrected NA labeling; 5-fold CV used for evaluation.",
            "limitations_or_challenges": "Poor zero-/few-shot behavior; less competitive after fine-tuning compared to other models; generative inconsistencies and occasional empty outputs.",
            "notable_examples": "Authors report BioGPT often failed to generate usable outputs in zero- and few-shot contexts for this specific relation extraction task.",
            "uuid": "e5954.2",
            "source_info": {
                "paper_title": "Leveraging pre-trained language models for mining microbiome-disease relationships",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "BioLinkBERT",
            "name_full": "BioLinkBERT-base",
            "brief_description": "A BERT-based discriminative model pre-trained on biomedical text with added document link (citation) information; achieved top discriminative performance when fine-tuned for relation extraction.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "BioLinkBERT-base",
            "llm_model_description": "BERT-derived encoder model pre-trained on biomedical abstracts and augmented with citation-link information to better capture cross-document knowledge.",
            "task_goal": "Classify evidence+question pairs into one of four relation labels (positive, negative, relate, NA) to distill structured microbe–disease association rules.",
            "domain": "Biomedical literature — microbiome/disease relation extraction",
            "methodology": "Discriminative fine-tuning: input = Evidence string [SEP] Question string tokenized (max seq length 512). Fine-tuning hyperparameters: learning rate 5e-5, weight decay 0.01, epochs=7, Adam optimizer, layer-wise LR decay 0.9; 5-fold cross-validation to report metrics.",
            "type_of_qualitative_law": "Structured classification rules mapping textual evidence to discrete microbe–disease association types (positive/negative/relate/NA).",
            "evaluation_metrics": "Accuracy, weighted average F1 score, precision, recall, per-class precision/recall/F1, 5-fold CV; precision-recall curves and PR-AUC (BioLinkBERT AUC ~0.85).",
            "results_summary": "BioLinkBERT fine-tuned yielded the best discriminative results: accuracy 0.811 ± 0.029, weighted F1 0.804 ± 0.036, precision 0.813 ± 0.034, recall 0.811 ± 0.028. Per-class metrics strong for positive/negative/relate but poor for NA (NA F1 ~0.457).",
            "human_involvement": "Model fine-tuned on human-annotated GSC (with NA reannotations); human curation used in dataset preparation and result inspection.",
            "dataset_or_corpus": "Corrected GSC (1100 sentences; 178 of original 258 NA re-labeled), 5-fold cross-validation for evaluation; comparisons made against MDIDB outputs.",
            "limitations_or_challenges": "Poor NA-class performance (low recall and F1) attributed to class imbalance and prior ambiguities in label definitions; BERT-model sequence length limited to 512 tokens; entity recognition/normalization issues in upstream pipeline not addressed here.",
            "notable_examples": "Per-class table (Table 3) showing high precision/recall for negative/positive/relate classes but substantially lower NA recall (~0.447 ± 0.186), illustrating difficulty in discerning 'no relation' cases even after fine-tuning.",
            "uuid": "e5954.3",
            "source_info": {
                "paper_title": "Leveraging pre-trained language models for mining microbiome-disease relationships",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "PubMedBERT",
            "name_full": "PubMedBERT",
            "brief_description": "A BERT-based model pre-trained from scratch on PubMed abstracts used here as a discriminative encoder for relation classification.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "PubMedBERT",
            "llm_model_description": "BERT-derived encoder pre-trained specifically on PubMed abstracts and full-text to improve biomedical task performance (pretrained model microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext used).",
            "task_goal": "Discriminative classification of evidence+question into microbe–disease relation labels to produce generalizable association rules.",
            "domain": "Biomedical literature (microbiome/disease)",
            "methodology": "Discriminative fine-tuning with Evidence+Question input, tokenization, max seq length 512; same optimizer and hyperparameters as other discriminative experiments (lr 5e-5, epochs=7, etc.); evaluated via 5-fold CV.",
            "type_of_qualitative_law": "Text-to-label mapping producing standardized microbe–disease association rules.",
            "evaluation_metrics": "Accuracy, weighted F1, precision, recall (5-fold cross-validation averages).",
            "results_summary": "PubMedBERT fine-tuned showed strong performance: accuracy 0.782 ± 0.022, F1 0.778 ± 0.019, precision 0.783 ± 0.021, recall 0.782 ± 0.022 — an improvement over many baselines but not top-performing.",
            "human_involvement": "Trained on human-annotated corrected GSC; human reannotation of NA; manual evaluation for comparisons.",
            "dataset_or_corpus": "Corrected GSC (1100 sentences) used for fine-tuning and 5-fold CV.",
            "limitations_or_challenges": "NA-class detection remains weak across models; restricted to 512 token BERT input window; success depends on quality of labeled corpus and upstream entity normalization.",
            "notable_examples": "Model achieved balanced improvements across classes but still struggled with NA cases, reflecting dataset labeling challenges.",
            "uuid": "e5954.4",
            "source_info": {
                "paper_title": "Leveraging pre-trained language models for mining microbiome-disease relationships",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "BioMegatron",
            "name_full": "BioMegatron",
            "brief_description": "A large BERT-style biomedical model pre-trained on PubMed and related corpora, fine-tuned here for discriminative relation extraction.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "BioMegatron",
            "llm_model_description": "BERT-family model pre-trained from scratch on PubMed abstracts and free full-text commercial-collection; reported to outperform prior SOTA on several biomedical tasks (architecture: large transformer encoder).",
            "task_goal": "Classify evidence+question pairs into relation labels to distill microbe–disease association rules from literature.",
            "domain": "Biomedical literature — microbiome/disease",
            "methodology": "Discriminative fine-tuning with Evidence+Question token pairs (max 512 tokens), same optimizer/hyperparameters as other discriminative models; evaluated via 5-fold CV.",
            "type_of_qualitative_law": "Structured classification rules mapping sentence evidence to microbe–disease association labels.",
            "evaluation_metrics": "Accuracy, F1, precision, recall (5-fold CV).",
            "results_summary": "BioMegatron fine-tuned achieved accuracy 0.778 ± 0.008 and F1 0.769 ± 0.013, outperforming basic BERT but below BioLinkBERT and GPT-3 fine-tuned.",
            "human_involvement": "Fine-tuned on human-annotated corrected GSC; human reannotation of NA class; manual evaluations.",
            "dataset_or_corpus": "Corrected GSC (1100 sentences), 5-fold cross-validation.",
            "limitations_or_challenges": "NA-class problems persist; discriminative models limited by sequence length; relies heavily on quality of training labels and entity normalization.",
            "notable_examples": "Performed better than generic BERT but did not reach the top metrics; used as part of ablation/benchmarking against other domain encoders.",
            "uuid": "e5954.5",
            "source_info": {
                "paper_title": "Leveraging pre-trained language models for mining microbiome-disease relationships",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "BERT-base-uncased",
            "name_full": "Bert-base-uncased",
            "brief_description": "General-purpose encoder transformer (BERT-base) used as a baseline discriminative model for relation extraction between microbes and diseases.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "BERT (bert-base-uncased)",
            "llm_model_description": "Bidirectional transformer encoder pretrained on general-domain corpora (Wikipedia, BooksCorpus); used here as a baseline discriminative classifier fine-tuned on domain task data.",
            "task_goal": "Baseline discriminative classification of evidence+question pairs into microbe–disease relation labels.",
            "domain": "Biomedical literature (applied via transfer fine-tuning)",
            "methodology": "Fine-tuned discriminatively with Evidence+Question pair tokenized; hyperparameters as for other discriminative models (lr 5e-5, epochs=7, max length 512); 5-fold CV evaluation.",
            "type_of_qualitative_law": "Baseline mapping rules from textual evidence to categorical relation labels.",
            "evaluation_metrics": "Accuracy, F1, precision, recall (5-fold CV).",
            "results_summary": "Fine-tuned BERT baseline achieved accuracy 0.733 ± 0.018 and F1 0.731 ± 0.015—substantially improved after fine-tuning but outperformed by domain-specific models.",
            "human_involvement": "Trained on corrected human-annotated GSC; human reannotation of NA class.",
            "dataset_or_corpus": "Corrected GSC of 1100 annotated sentences; 5-fold CV.",
            "limitations_or_challenges": "Generic pretraining limits domain-specific understanding without fine-tuning; NA-class remains challenging; BERT limited to 512 token input.",
            "notable_examples": "Served as an ablation/baseline demonstrating the value of domain-specific pretraining and fine-tuning.",
            "uuid": "e5954.6",
            "source_info": {
                "paper_title": "Leveraging pre-trained language models for mining microbiome-disease relationships",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "BioClinicalBERT",
            "name_full": "BioClinicalBERT",
            "brief_description": "A BERT-derived model pre-trained on biomedical and clinical text evaluated here as a discriminative encoder for relation extraction.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "BioClinicalBERT",
            "llm_model_description": "Further development of BioBERT pre-trained on a large corpus of biomedical and clinical text aiming to improve clinical/biomedical NLP tasks (BERT-family encoder).",
            "task_goal": "Discriminative classification of evidence+question pairs into microbe–disease relation labels to derive association rules.",
            "domain": "Biomedical/clinical literature (microbiome/disease)",
            "methodology": "Discriminative fine-tuning on Evidence+Question pairs; same training hyperparameters as other discriminative models; evaluated with 5-fold CV.",
            "type_of_qualitative_law": "Categorical microbe–disease association rules inferred from sentence evidence.",
            "evaluation_metrics": "Accuracy, F1, precision, recall (5-fold CV).",
            "results_summary": "BioClinicalBERT fine-tuned showed lower performance relative to other domain models (accuracy 0.729 ± 0.032, F1 0.724 ± 0.029). Good on some classes but inferior overall compared to BioLinkBERT and GPT-3.",
            "human_involvement": "Fine-tuning and data curation used human-labeled GSC with NA reannotation; manual output inspection.",
            "dataset_or_corpus": "Corrected GSC (1100 sentences), 5-fold cross-validation evaluation.",
            "limitations_or_challenges": "Underperformed compared to best-performing domain encoders; NA-class detection and entity normalization remain issues.",
            "notable_examples": "Lower overall metrics highlight model-to-task mismatch compared to models pre-trained specifically for biomedical citation-linked corpora.",
            "uuid": "e5954.7",
            "source_info": {
                "paper_title": "Leveraging pre-trained language models for mining microbiome-disease relationships",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mining microbe-disease interactions from literature via a transfer learning model",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "GPT-3 models are poor few-shot learners in the biomedical domain",
            "rating": 2
        },
        {
            "paper_title": "Biogpt: generative pre-trained transformer for biomedical text generation and mining",
            "rating": 1
        },
        {
            "paper_title": "BiomedIm: a domain-specific large language model for biomedical text",
            "rating": 1
        },
        {
            "paper_title": "LinkBERT: Pretraining Language Models with Document Links",
            "rating": 1
        }
    ],
    "cost": 0.015774,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Leveraging pre-trained language models for mining microbiome-disease relationships</h1>
<p>Nikitha Karkera ${ }^{4}$, Sathwik Acharya ${ }^{1,3}$ and Sucheendra K. Palaniappan ${ }^{1,2,4 *}$</p>
<h4>Abstract</h4>
<p>*Correspondence: sucheendra@sbi.jp ${ }^{1}$ The Systems Biology Institute, Tokyo, Japan ${ }^{2}$ Iom Bioworks Pvt Ltd., Bengaluru, India ${ }^{3}$ PES University, Bengaluru, India ${ }^{4}$ SBX Corporation, Tokyo, Japan</p>
<h4>Abstract</h4>
<p>Background: The growing recognition of the microbiome's impact on human health and well-being has prompted extensive research into discovering the links between microbiome dysbiosis and disease (healthy) states. However, this valuable information is scattered in unstructured form within biomedical literature. The structured extraction and qualification of microbe-disease interactions are important. In parallel, recent advancements in deep-learning-based natural language processing algorithms have revolutionized language-related tasks such as ours. This study aims to leverage state-of-the-art deep-learning language models to extract microbe-disease relationships from biomedical literature. Results: In this study, we first evaluate multiple pre-trained large language models within a zero-shot or few-shot learning context. In this setting, the models performed poorly out of the box, emphasizing the need for domain-specific fine-tuning of these language models. Subsequently, we fine-tune multiple language models (specifically, GPT-3, BioGPT, BioMedLM, BERT, BioMegatron, PubMedBERT, BioClinicalBERT, and BioLinkBERT) using labeled training data and evaluate their performance. Our experimental results demonstrate the state-of-the-art performance of these fine-tuned models ( specifically GPT-3, BioMedLM, and BioLinkBERT), achieving an average F1 score, precision, and recall of over $&gt;0.8$ compared to the previous best of 0.74 . Conclusion: Overall, this study establishes that pre-trained language models excel as transfer learners when fine-tuned with domain and problem-specific data, enabling them to achieve state-of-the-art results even with limited training data for extracting microbiome-disease interactions from scientific publications.</p>
<p>Keywords: Microbe-disease relationship extraction, Language models, Fine-tuning, Deep-learning, Transfer learning, Biomedical informatics, Natural language processing</p>
<h2>Introduction</h2>
<p>Microorganisms, in the trillions, are housed and sheltered in the human body. These microorganisms take up residence in various organs, including the gastrointestinal tract, mouth, stomach, skin, urogenital tract, and others. Their presence plays a crucial role in maintaining the host's health and well-being [1]. Collectively, these microbes form what is known as the microbiome. Recent technological advancements have enabled us to study and quantify the microorganisms within our bodies. As a</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>result, we can now establish both correlational and causal relationships between dysbiosis of the microbiome and disease states [2].</p>
<p>Structured knowledge representing the relationship between microorganisms and diseases can greatly contribute in deepening the development of microbiome-based preventive and therapeutic measures. Knowledge repositories that encompass collective information on microbiome-disease associations is usually constructed based on evidence from scientific publications. These manually curated knowledge bases are frequently utilized for downstream analysis and discovery. Several endeavors, such as Amadis [3], Disbiome [4], MicroPhenoDB [5], The Virtual Metabolic Human database [6], MADET [7], gutMDisorder [8, 9], HMDAD [10], mBodyMap [11], have focused on at cataloging and organizing this information. While these knowledge bases are of high quality, their construction is a labor-intensive and expensive process due to the substantial manual effort required for curation. Furthermore, keeping these databases up-to-date poses significant challenges, particularly given the rapid pace of microbiome research and the continuous accumulation of new findings. To provide perspective, a search for the keyword "microbiome" on PubMed returns over 140, 000 abstracts, with more than 27, 000 abstracts published in 2022 alone.</p>
<p>Natural Language Processing (NLP) techniques have emerged as a promising approach to effectively handle the vast amount of scientific literature. These methods enable automated analysis of extensive scientific texts and the extraction of relevant information, which can then be stored in knowledge bases. In recent years, the field of NLP has witnessed substantial advancements owing to the emergence of Large Language Models (LLMs) and Generative AI models [12]. Consequently, there has been a growing interest in leveraging these techniques to tackle problems in the microbiome field as well [13, 14]. Of particular significance is the work presented by Badal et al. [13], which highlights the key challenges that must be addressed to establish meaningful knowledge bases for the microbiome disease problem. This research provides valuable insights into the nuances and intricacies of the problems in this subdomain and serves as a foundation for our work too.</p>
<p>To address the specific challenge of extracting associations between diseases and the microbiome using NLP techniques, the solution skeleton is usually a combination of the following steps:</p>
<ul>
<li>Identifying disease and microbe mentioned in scientific texts. This step typically involves utilizing algorithms such as Named Entity Recognizers (NERs), linguistic taggers, and dictionaries to locate disease and microbe references within each document or sentence.</li>
<li>Establishing the existence of a relationship or association between pairs of diseases and microbes. Relationship extraction algorithms are commonly employed for this task. Extensive research has been conducted in this area, with notable contributions so far [10, 15-19].</li>
<li>Once the presence of a relationship is established, determining the nature of the relationship. For example, investigating whether the presence of a specific bacterium is positively correlated with a particular disease. Specialized relationship</li>
</ul>
<p>extraction algorithms are employed to address this, which is also the primary focus of this paper.</p>
<h1>Related work</h1>
<p>At its core, the current problem is that of relation extraction which has decades of prior work in the Bio-NLP domain [20, 21]. However, the traditional models for relation extraction are now being surpassed by deep learning based NLP models [22] which are shown to be superior in their performance.</p>
<p>In terms of related work to the disease-microbiome extraction task, two main works are related to ours. First, Park et al. [23], proposed an ensemble model for this problem. Their approach involves two steps: first, a relation detection model based on Hierarchical Long Short-Term Memory (LSTM) networks to determine the presence of a diseasemicrobe relationship. Second, they extract the specific relation type by employing a substantial collection of rule sets or patterns, amounting to around 1000. However, this approach has limitations as it requires manual maintenance of the rule list for relation extraction, making it impractical for large-scale efforts.</p>
<p>Next is the work by Wu et al[24] that focuses on a deep-learning strategy for solving this problem. Their approach first involves preparing training datasets. For this, they start by collecting a large corpus of text from PubMed related to microbiome and diseases and subsequently employ Named Entity Recognition (NER) tools to identify microbe and disease entities within the text. Next, they manually create two corpora for microbe-disease interactions: a high-quality gold-standard corpus (GSC) and a silverstandard corpus (SSC) that is known to contain errors. These corpora are then used as training data. Subsequently, they utilize a deep-learning-based relation extraction algorithm [25] to train a deep-learning model using the GSC data which did not yield the best results. Subsequently, they implement a 2-step learning process, where the model is first trained on the error-prone SSC corpus and then fine-tuned using transfer learning on the GSC corpus. The authors report that this 2-step approach significantly improves the accuracy of relationship extraction, achieving an F-score of 0.7381 . For our problem statement, this result represents the current state-of-the-art as reported in the scientific literature. While the approach holds interest, we theorized that the expensive training of deep-learning models could be avoided by directly fine-tuning pre-trained models, in addition to improved accuracy gains.</p>
<h2>Our contribution</h2>
<p>Our paper offers multiple contributions. Firstly, we recognize the significant advancements in the field of large language models such as GPT-3 [26], BERT [27] etc. Leveraging the power of these models, we utilize them in our task to achieve state-of-the-art results. The utilization of deep learning and transformer models allows us to effectively capture complex patterns and relationships within the literature. Secondly, our approach highlights the relevance of deep learning and transformer models in reducing the requirement for large amounts of training data. In contrast to the study by [24] or other deep learning models, our method benefits from transfer learning and task-specific fine-tuning. This advantage enables</p>
<p>us to achieve excellent results with a smaller amount of training data, making our approach efficient and practical. Furthermore, our approach can handle cases where disease-microbe relationships span multiple sentences. Deep learning and transformer models have the capacity to capture long-range dependencies and contextual information, allowing us to effectively address the complexity of such relationships.</p>
<p>To tackle the task of relationship extraction, we adopt two solution strategies. First, we treat it as a classification (discriminative) task. Here, the model receives the evidence describing the relationship between a disease and a microbe as input, along with a question probing their relationship. The model's output is expected to provide the answer from one of the four labels: positive, negative, relate, or NA. This approach enables us to utilize the discriminative power of deep learning and transformer models.</p>
<p>Next, we reformulate the task as a generative text task. In this setup, we provide the model with evidence describing the relationship between a disease and a microbe, as well as a question as a prompt. The model is then tasked with generating the correct label. This generative approach leverages the expressive nature of deep learning and transformer models to generate informative and accurate labels.</p>
<p>For further details on our methodology and experimental setups, we provide comprehensive information in the subsequent sections of our paper, highlighting the specific ways in which deep learning and transformer models contribute to the success of our approach.</p>
<p>To establish baselines, we employ various state-of-the-art models, both domainindependent and domain-specific. Initially, we explore the potential of pre-trained language models as zero-shot and few-shot learners, without fine-tuning, potentially eliminating the need for training data or specialized retraining and fine-tuning. Subsequently, we fine-tune these models using curated training data, obtained from [24], to further enhance their capabilities.</p>
<p>Firstly, among the generative models, BioGPT [28], BioMedLM [29] and GPT-3 [26] were considered. These models are designed to generate/complete the response based on the given question and context(prompt). They were a natural choice for our task, as they are known to perform well in zero-shot or few-shot learning scenarios. Additionally, we incorporated the following language models in the discriminative setting: BERT [30], ClinicalBERT [31], PubMedBERT [32], BioMegatron [33] and BioLinkBERT [34]. With these models, the objective was for the model to classify the input into one of the four predefined labels. Details of our study and the summary of the outcomes are shown in Fig. 1.</p>
<p>Our experiments show that these models fall short in the zero-shot or few-shot setup, highlighting the need for domain-specific fine-tuning and task-specific training data. After fine-tuning, among the generative models, GPT-3 performed well. However, we observed that these models sometimes produced varying outputs for the same prompt, owing to their generative nature, which can pose challenges. Among the discriminative models, the model fine-tuned on BioLinkBERT consistently yielded the best results among the tested models. The next section formally introduces the problem and the solution strategy.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1 Our contributions and study design for extracting disease-microbiome relationships</p>
<h1>Problem formulation and models considered</h1>
<p>Our objective is to extract and determine the relationships between disease and microbe terms from natural language text. Formally, given a scientific text $T$ and a pair of entities $(e 1, e 2)$ occurring in $T$, where $e 1 \in D$ and $e 2 \in M$, with $D$ and $M$ representing the sets of all disease and microbe names, respectively, the task is to predict a label $y$ that represents the relationship between $e 1$ and $e 2$. The label $y$ belongs to the set (positive, negative, relate, $N A$ ).
One approach to mathematically formulate this problem is by using a supervised learning method. In this approach, the model is trained on a dataset consisting of labeled sentences and entities. The model learns a function $f$ that maps the scientific text and entity pair $(T, e 1, e 2)$ to a predicted label $y$, such that $y=f(T, e 1, e 2)$. The function $f$ can be realized using various machine-learning techniques and models.
In the context of microbe-disease relationships, the following labels are defined [24]:</p>
<ul>
<li>(positive): This label indicates a positive correlation between the microbe and the disease. It implies that the microbe can worsen the disease or that its presence increases when the disease occurs.</li>
<li>(negative): This label indicates a negative correlation between the microbe and the disease. It suggests that the microbe can act as a treatment for the disease or that its presence decreases when the disease occurs.</li>
<li>(relate): This label indicates a relationship between the microbe and the disease without additional information about whether it was positive or negative. It signifies that they appear to be associated with each other. In a sense, this label can be considered a super-set of positive and negative labels.</li>
<li>(NA): This label indicates that the microbe and the disease mentioned in the text are not related to each other.</li>
</ul>
<p>In our case, given the same scientific text $T$ and entities of interest ( $e 1, e 2$ ), and a question posed as follows: "What is the relationship between $e 1$ and $e 2$ ?" our models are expected to provide an answer from the set positive, negative, relate, $N A$. The problem formulation in our setting is illustrated in Fig. 2.</p>
<h1>Pre-trained language models considered</h1>
<p>Now, we describe the various pre-trained models that were leveraged for fine-tuning in this study. The choice of models was based on best-in-class for biomedical domain specific tasks.</p>
<h2>Generative setting</h2>
<p>BioMedLM 2.7B [29] is a large language model trained on a dataset of biomedical literature and is based on GPT-2 model. It has 2.7 billion parameters. It is effective for a variety of tasks, including natural language inference, question answering, and text summarization. BioMedLM 2.7B is a valuable tool for researchers and clinicians who need to access and process biomedical information.</p>
<p>BioGPT [28] is a domain-specific generative pre-trained transformer language model for biomedical text generation and mining. It is trained on 15 million PubMed abstracts and has 1.5 billion parameters. It was developed by Microsoft Research and is based on the GPT-2 language model.</p>
<p>GPT-3 [26], or Generative Pre-trained Transformer 3, is a state-of-the-art language model developed by OpenAI. With 175 billion parameters, it exhibits remarkable proficiency in generating coherent and contextually relevant text across various domains. The most competent model available in OpenAI is "text-davinci-003," while there are other models as well. The prompt used for this experiment is detailed in the supplementary website [see Additional file 1]. For all experiments, we changed the Temperature parameter to 0 making the outputs less random.</p>
<h2>SCIENTIFIC EVIDENCE</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Plu0j Dne J2HX Jui 3/25/11a4998757, doi: 10.1373/journal.pone. 0198157 <br> 3/2011 <br> 2018</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Dysbiosis of oral microbiota and its association with salivary immunological biomarkers in autoimmune liver disease</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Novemith Ake ${ }^{1,2}$, Abouihi Takahashi ${ }^{3}$, Masashi Fujita ${ }^{3}$, Amaniethi Hasbumi ${ }^{3}$, Menabu Hayashi ${ }^{1}$, Aen Okai ${ }^{1}$, Atsonasa Othio ${ }^{1}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Affiliations x xyzm3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PMEL DERMANO PMCID PAPERSOFT03 3DB-10-1371@ournal.pone. 0198157</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Free PMC article</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Abstract</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">The gut microbiota has recently been recognized to play a role in the pathogenesis of autoimmune liver disease (ALD), mainly primary biliary cholangitis (PBC) and autoimmune hepatitis (AH). This study aimed to analyze and compare the composition of the oral microbiota of 56 patients with ALD and 10 healthy controls (HCs) and to evaluate its association with salivary immunological biomarkers and gut microbiota. The subjects included 38 patients with PBC and 17 patients with AH diagnosed as non-hepatitis. The control population comprised 19 matched HCs. Patients with fecal samples were collected for analysis of the microbiome by terminal restriction fragment length polymorphism of 18S rDNA. Correlations between immunological biomarkers measured by 31q-19q2 assay, 36q-16q12 and the oral microbiome of patients with PBC and AH were assessed. Patients with PBC showed a significant increase in infiltrates with a concurrent decrease in Streptococcus in the oral microbiota compared with the HCs. Patients with PBC showed significant increases in Eubacterium and Veillonella and a significant decrease in Fusobacterium in the oral microbiota compared with the HCs. Immunological biomarker analysis showed elevated levels of inflammatory cytokines (IL-1 $\beta$, IFN- $\gamma$, TNF- $\alpha$, IL-8) and immunoglobulin A in the saliva of patients with ALD. The relative abundance of infiltrates was positively correlated with the levels of IL-1 $\beta$, IL-6 and immunoglobulin A in saliva and the relative abundance of Lactobacillates in feces. Outbreak of the oral microbiota is associated with inflammatory responses and reflects changes in the gut microbiota of patients with ALD. Outbreak may play an important role in the pathogenesis of ALD.</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Fig. 2 Problem formulation for inferring microbe-disease relationship</p>
<h1>Discriminative setting</h1>
<p>BERT(Bidirectional Encoder Representations from Transformers) model [27] is among the most well-known and early LLMs based on transformer architectures. It was specifically trained on Wikipedia and Google's BooksCorpus. BERT is known to be a very good general-purpose model that works well for most language tasks. In our case, we used BERT first to see if generic models could perform well for our task before resorting to domain-specific adaptations. For our experiments, we used the "Bert-base-uncased" model from the Hugging Face library [35].
PubMedBERT is a BERT-based model pre-trained from scratch using 14 million abstracts from PubMed. It consistently outperforms all the other BERT models in most biomedical NLP tasks, often by a significant margin as reported in [32]. Specifically, microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext is the pretrained model that was utilized.
BioMegatron models are pre-trained from scratch on the PubMed dataset. A large biomedical language model pre-trained on a large literature corpus is an excellent starting point for identifying the microbiome-disease relation type. The pre-training of this model takes PubMed abstracts and full-text commercial-collection (CC) that are free of copyrights. When compared to the prior state-of-the-art (SOTA), BioMegatron significantly outperformed across a variety of tasks. In contrast to models pretrained on wide domain datasets, [33] demonstrates that language models specialized for a particular domain perform at their best.
BioLINK-BERT model is trained on abstract data, similar to PubMedBERT, but with the addition of citation links between articles [34]. Unlike previous works, which only use their raw text for pre-training, academic papers have extensive dependencies on one another through citations (references). Incorporating citation links assists language models in learning the dependencies between papers and the knowledge that spans them.
BioClinicalBERT [36] is a BERT-based language model that has been pre-trained on a dataset of 880 million words of biomedical and clinical text which allows it to better understand and generate text from both domains. It is a further development of BioBERT. BioClinicalBERT is available through the Hugging Face Transformers library. It is designed to improve the performance of biomedical natural language processing (NLP) tasks, such as named entity recognition, and relation extraction.</p>
<h2>Poor performance in zero-shot and few-shot setting</h2>
<p>We first investigated the performance of generative language models in a zero-shot and few-shot learning setting. We evaluated all three models, the details of the prompts used are detailed in the supplementary website [see Additional file 1].
Zero-shot learning allows the model to extract relationships between disease and microbiome without requiring specific training dataset for those relationships. By leveraging the pre-trained knowledge and semantic understanding encoded within the language model, the model can generalize and infer relationships based on the provided input. This approach is particularly valuable when dealing with unseen</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3 Zero-shot and few-shot learning setup for inferring microbe-disease relationship using GPT-3. The Ground truth for this example is "negative". However GPT-3 returns "relate" in zero-shot and "positive" in few-shot setting</p>
<p>Table 1 Performance metrics of the GPT-3 model in the zero-shot and few-shot setting</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>F1 score</th>
<th>Precision</th>
<th>Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3 (zero-shot)</td>
<td>0.48</td>
<td>0.50</td>
<td>0.58</td>
<td>0.48</td>
</tr>
<tr>
<td>GPT-3 (few-shot)</td>
<td>0.57</td>
<td>0.56</td>
<td>0.57</td>
<td>0.57</td>
</tr>
</tbody>
</table>
<p>relationship types, as it enables the model to make predictions even for relationships it has not encountered during training.</p>
<p>On the other hand, few-shot learning extends the capabilities of zero-shot learning by enabling the model to learn from a limited number of labeled examples for a specific relationship. Rather than relying solely on pre-encoded knowledge, few-shot learning allows the model to adapt and make accurate predictions using the additional labeled data. By leveraging both the pre-trained knowledge and the limited labeled examples, few-shot learning enhances the model's ability to generalize and extract relationships, even in scenarios where labeled data is scarce or new relationship types are introduced. We use a "two-shot-learning" setup to infer the microbe-disease relationship. More specifically, two examples of scientific text per class along with its annotation were provided for each of the labels as shown in Fig. 3. A natural language description is also provided along with the prompt for the model to learn about the task as this often improves the model's performance (See [26, 37].</p>
<h1>Experimental results</h1>
<p>In summary, for the zero-shot setting, we found that the performance of the models was poor as shown in Table 1. For GPT-3, the outputs are generated within the four labels, but they don't follow the same casing for every predicted label. It achieved a f1-score of 0.5 with low precision. A detailed analysis of the results showed that the model performs</p>
<p>poorly for the "NA" class. Details can be found in the supplementary website [see Additional file 1]. Surprisingly, BioMedLM and BioGPT did not even produce sensible outputs.</p>
<p>Even in the few-shot setting, we noticed that there were only marginal improvements in the results with GPT-3 (f1-score of 0.56), while no tangible outputs were generated for BioMedLM and BioGPT as shown in Table 1. It was clear that despite the general observation that generative models provide good outcomes in zero or few-shot learning settings, performance still depends on the task-specific domain. Similar outcomes have been established previously [38]. For our problem, using models out of the box was of limited utility, this could also be due to the counterintuitive definition of positive and negative labels. For all experiments in this section, default model parameters were used.</p>
<h1>Dataset for fine-tuning: considerations for improved accuracy</h1>
<p>To fine-tune the various language models for our task, we utilized the human-annotated gold-standard corpus (GSC) from [24]. This dataset consists of 1100 sentence instances that describe interactions between the microbiome and diseases, along with corresponding labels of "positive," "negative," "relate," and "NA." These sentences were selected by employing a semi-automated pipeline to identify diseases and microbes mentioned in articles from PubMed and PubMed Central using the keyword "microbe". Expert annotators then reviewed each sentence and assigned them to one of the four categories. For a comprehensive understanding of how the GSC dataset was constructed, we refer interested readers to [24]. Out of the 1100 sentences, the distribution of classes is depicted in shown in Fig. 4. This dataset served as the basis for fine-tuning our pipeline.</p>
<p>Interestingly, we identified a significant number of labeling errors in the GSC dataset, particularly for the "NA" category. To address this issue, we re-annotated the statements with two postdoctoral-level researchers who re-labeled all the sentences initially marked as "NA". The annotation process was facilitated using the Doccanno tool [39].</p>
<p>Analysis of GSC before and after reannotating "NA" Class
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4 Distribution of relation types for GSC after correcting for issues in "NA" relation</p>
<p>Each researcher independently re-annotated all the "NA" labels, followed by a collaborative review of each other's annotations. Through consensus, a final label was agreed upon for each sentence.</p>
<p>Our analysis revealed that out of the original 258 sentences labeled as "NA" in the GSC dataset, 178 sentences were found to be mislabeled. In the supplementary website [see Additional file 1], we provide illustrative examples and a comprehensive list of the reannotated data. As depicted in Fig. 4, there was a substantial decrease in the number of training data points for the "NA" category, accompanied by an increase in samples for the negative, positive, and relate categories. All further fine-tuning was performed on the corrected dataset. This dataset is available for review in the supplementary website [see Additional file 1].</p>
<h1>Domain specific fine-tuning of different language models</h1>
<p>We fine-tune all the models using the dataset outlined in the preceding section for the task at hand. Notably, the methodology employed by [24] involved a two-step approach. In the first step, an error-prone silver training corpus was utilized to train a relation extraction algorithm [25]. Subsequently, the model trained in the initial step was employed in conjunction with transfer learning techniques for fine-tuning with the GSC dataset. The authors demonstrated that this two-step transfer learning process yielded state-of-the-art (SOTA) results. Although this approach is interesting, we hypothesized the costly training of deep-learning models could be circumvented by directly fine-tuning pre-trained models. In essence, our strategy is to take models that were previously trained on extensive generic or domain-specific free text as the foundation, and subsequently fine-tune them specifically for the targeted problem using a minimal quantity of high-quality training data (in our case, GSC).</p>
<h2>Methodology</h2>
<p>Figure 5 shows the mechanism in which the evidence-question are taken in pairs for the fine-tuning purpose for the discriminative class of models namely BERT [30], BioMegatron [33], PubMedBERT [32], BioClinicalBERT [28] and BioLinkBERT [34]. For this, we resort to a typical tokenizing procedure of the evidence-question pair to produce token IDs and the attention mask. A maximum sequence length of 512 is maintained, as this is what BERT-based models are limited to. For all base models considered, during the finetuning process, a learning rate of $5 e-5$, a weight decay of 0.01 , number of epochs $=7$, and Adam's optimizer with a layer-wise learning rate decay of 0.9 was applied. All models were trained using an NVIDIA GeForce RTX 2080 Ti with 12GB memory, 16 CPUs, and 64GB memory. The fine-tuning process took about 30 min for each model.</p>
<p>Among the generative models, GPT-3 model was fine-tuned using the OpenAI API of the GPT-3 davinci model. Details of the fine-tuning process are available on the OpenAI website (link: https://openai.com/api/). The inference parameters for the finetuned model are $\mathrm{t}=0.0$, top_p $=1$, max_tokens $=1$ with other parameters with its default settings. BioMedLM's stanford-crfm/BioMedLM model was fine-tuned on an A40 GPU instance with deepspeed setting for efficiency. This helped in training the model with 18GB GPU memory utilization. The model was trained for 20 epochs with batch_ size $=2$, gradient_accumulation_steps $=2$, learning_rate $=2 \mathrm{e}-06$ with other parameters</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 5 Illustration and mapping of the evidence and question tokens into the models for our discriminative class of models
kept the same as their default settings. It took around 7hrs to complete 20 epochs. Similarly, BioGPT was fine-tuned on a single NVIDIA GeForce RTX 2080 Ti with 12GB memory. The model was trained using parameters similar to DDI (Drug-Drug Interaction) experiment in BioGPT [28] for Relation extraction purposes.</p>
<h1>Data preparation for fine-tuning</h1>
<p>This section provides details of the data processing and prompt design for the fine-tuning process of different models.</p>
<h2>Discriminative models</h2>
<p>The training data format for fine-tuning these models follows a simple structure. Each example consists of an input and an output. The input is represented by two strings, an Evidence String and a Question String, separated by a delimiter. For example, the Evidence string can be "Additionally, some members of the phylum such as Faecalibacterium prausnitzii, a member of the Clostridiales-Ruminococcaceae lineage have been shown to have anti-inflammatory effects due to the production of the short-chain fatty acid butyrate and have been negatively correlated with inflammatory bowel disease." and Question as "What is the relationship between inflammatory bowel disease and Clostridiales ?". The output corresponds to the target label associated with the given input as shown below.</p>
<p>Input: Evidence string {SEP} Question string
Output: one of the labels $\in{$ positive, negative, relate, na}</p>
<p>This training data format allows for a straightforward mapping between the input evidence and question, and the corresponding output label. By fine-tuning the pre-trained models on the GSC dataset encoded as above, the model can learn to effectively understand the relationship between the evidence and question, and generate accurate labels or predictions based on the input provided.</p>
<h1>Generative models</h1>
<p>The training data format for GPT-3 model consists of a collection of examples, each represented by a prompt and a completion string that corresponds to the label.
The "prompt" key corresponds to the text that serves as the input or context for the model. It contains evidence related to the microbiome and disease relationship. In this format, the prompt text is structured as the evidence string followed by a question string, separated by a line break (" $\backslash \mathrm{n}$ "). The evidence string provides the background or supporting information, while the question string represents the specific question to be answered by the model. For example, a prompt can be:
"Evidence: Additionally, some members of the phylum such as Faecalibacterium prausnitzii, a member of the Clostridiales-Ruminococcaceae lineage have been shown to have anti-inflammatory effects due to production of the short-chain fatty acid butyrate and have been negatively correlated with inflammatory bowel disease107. \n Question: What is the relationship between inflammatory bowel disease107 and Clostridiales ? $\backslash n \backslash n # # # # \backslash$ $n \backslash n$ ". Here, the ending string " $\backslash \mathrm{n} \backslash \mathrm{n} # # # # \backslash \mathrm{n} \backslash \mathrm{n}$ " acts as a fixed separator to the model. For inference, the prompts are designed in the same format as the training dataset including the same separator with the same stop sequence to properly truncate the completion.</p>
<div class="codehilite"><pre><span></span><code>Training Data
{ &quot;prompt&quot;:prompt_text, &quot;completion&quot;: label}
{ &quot;prompt&quot;:prompt_text, &quot;completion&quot;: label}
{&quot;prompt&quot;:prompt_text, &quot;completion&quot;: label}
...
where
prompt_text = &quot;Evidence: Evidence_String\nQuestion: Question_
String\n\n非非非\n\n&quot;
and
label \in {positive, negative, NA, relate}
</code></pre></div>

<p>The training data format allows for multiple examples to be included, each following the same key-value structure. The detailed prompts for BioMedLM and BioGPT which are very similar to the GPT-3 prompts can be found on the supplementary website [see Additional file 1].</p>
<h2>Results</h2>
<p>To mitigate the risk of overfitting, model performance was evaluated using a 5 -fold cross-validation strategy. The curated dataset was divided into five equal parts, referred to as folds. In each iteration, the models were fine-tuned using four folds for training and evaluated on the remaining fold. This process was repeated five times, with each fold serving as the test set once. The average of the five test scores was calculated to</p>
<p>provide the final metrics of the model. We assessed the performance using several metrics, including Accuracy, Weighted Average F1 score, Precision, and Recall, which align with those reported in [24]. The detailed results are presented in Table 2. The results of the study by [24] are shown in the table using the notation $B E R E_{T L}(M D I)$. The reported f1-score reached a peak value of 0.738 along with closely aligned precision and recall scores. These results serve as our baseline.</p>
<p>Coming to the performance of the discriminative fine-tuned models, we observed significant improvements across the entire spectrum. Notably, the model trained on BioLinkBERT-base yielded the best results, achieving an average F1-score of $0.804 \pm 0.0362$ in a 5 -fold cross-validation setup. Detailed information regarding all the models and the fine-tuning parameters can be found on our supplementary website (see Additional file 1). Further, to understand the characteristics of the classifier better, we plotted the precision-recall curves as shown in Fig. 6. Notably, the area under the curve for BioLinkBERT-finetuned outperformed others, reaching 0.85 , indicating the best performance.</p>
<p>Among the generative class of models, we found that the fine-tuned GPT-3 model yielded the best overall results, as shown in Table 2. However, in terms of precision, the model fine-tuned on BioMedLM performed well as shown in Table 2. However, we noticed a few observations regarding the use of these generative models. Firstly, we sometimes noticed variability in the results with each run of the model depending on the parameters used. There were also instances where the model produced empty outputs. Additionally, since these models are generative in nature, the outputs and probabilities generated by the model do not always align with well-defined class labels. This aspect further hinders our comprehension of how these models operate and raises concerns about the reliability of their outputs. Due to these limitations, we were unable to generate a precision-recall curve for GPT-3.</p>
<p>To gain deeper insights into the performance of the classifier and generative model, we analyzed the per-class performance metrics for both the fine-tuned generative models (GPT-3) and the discriminative models (BioLinkBERT model). As expected, the metrics for the negative, positive, and relate classes exhibited satisfactory results. However, we observed poor performance in the NA class for both the finetuned GPT-3 (refer to Table 4) model and the BioLinkBERT model (refer to Table 3). This deficiency in performance also accounts for the lower overall classification</p>
<p>Table 2 Performance metrics different fine-tuned language models</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Accuracy</th>
<th style="text-align: left;">F1 score</th>
<th style="text-align: left;">Precision</th>
<th style="text-align: left;">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Baseline</td>
<td style="text-align: left;">BERE_TL(MDI)</td>
<td style="text-align: left;">NA</td>
<td style="text-align: left;">0.738</td>
<td style="text-align: left;">0.736</td>
<td style="text-align: left;">0.740</td>
</tr>
<tr>
<td style="text-align: left;">Our models <br> (fine-tuned)</td>
<td style="text-align: left;">Bert-base-uncased</td>
<td style="text-align: left;">$0.733 \pm 0.018$</td>
<td style="text-align: left;">$0.731 \pm 0.015$</td>
<td style="text-align: left;">$0.742 \pm 0.02$</td>
<td style="text-align: left;">$0.733 \pm 0.018$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">BioMegatron</td>
<td style="text-align: left;">$0.778 \pm 0.008$</td>
<td style="text-align: left;">$0.769 \pm 0.013$</td>
<td style="text-align: left;">$0.771 \pm 0.013$</td>
<td style="text-align: left;">$0.778 \pm 0.008$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">PubMedBERT</td>
<td style="text-align: left;">$0.782 \pm 0.022$</td>
<td style="text-align: left;">$0.778 \pm 0.019$</td>
<td style="text-align: left;">$0.783 \pm 0.021$</td>
<td style="text-align: left;">$0.782 \pm 0.022$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">BioClinicalBERT</td>
<td style="text-align: left;">$0.729 \pm 0.032$</td>
<td style="text-align: left;">$0.724 \pm 0.029$</td>
<td style="text-align: left;">$0.731 \pm 0.032$</td>
<td style="text-align: left;">$0.729 \pm 0.032$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong>BioLinkBERT-base</strong></td>
<td style="text-align: left;">$\mathbf{0 . 8 1 1} \pm \mathbf{0 . 0 2 9}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 0 4} \pm \mathbf{0 . 0 3 6}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 1 3} \pm \mathbf{0 . 0 3 4}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 1 1} \pm \mathbf{0 . 0 2 8}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">BioMedLM</td>
<td style="text-align: left;">$0.806 \pm 0.028$</td>
<td style="text-align: left;">$0.804 \pm 0.028$</td>
<td style="text-align: left;">$\mathbf{0 . 8 2 2} \pm \mathbf{0 . 0 3 0}$</td>
<td style="text-align: left;">$0.806 \pm 0.028$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">BioGPT</td>
<td style="text-align: left;">$0.732 \pm 0.017$</td>
<td style="text-align: left;">$0.726 \pm 0.017$</td>
<td style="text-align: left;">$0.732 \pm 0.025$</td>
<td style="text-align: left;">$0.736 \pm 0.016$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong>GPT-3</strong></td>
<td style="text-align: left;">$\mathbf{0 . 8 1 4} \pm \mathbf{0 . 0 2 1}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 1 0} \pm \mathbf{0 . 0 2 5}$</td>
<td style="text-align: left;">$0.810 \pm 0.021$</td>
<td style="text-align: left;">$\mathbf{0 . 8 1 4} \pm \mathbf{0 . 0 2 1}$</td>
</tr>
</tbody>
</table>
<p>Bold indicates the performance of the models which gave the best performance</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 6 Precision recall curve for the various fine-tuned pre-trained models used in discriminative setting</p>
<p>Table 3 Per class metrics for BioLinkBERT fine-tuned model</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Class</th>
<th style="text-align: left;">Precision</th>
<th style="text-align: left;">Recall</th>
<th style="text-align: left;">F1-score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NA</td>
<td style="text-align: left;">$0.556 \pm 0.131$</td>
<td style="text-align: left;">$0.447 \pm 0.186$</td>
<td style="text-align: left;">$0.457 \pm 0.096$</td>
</tr>
<tr>
<td style="text-align: left;">Negative</td>
<td style="text-align: left;">$0.827 \pm 0.022$</td>
<td style="text-align: left;">$0.894 \pm 0.043$</td>
<td style="text-align: left;">$0.859 \pm 0.025$</td>
</tr>
<tr>
<td style="text-align: left;">Positive</td>
<td style="text-align: left;">$0.831 \pm 0.015$</td>
<td style="text-align: left;">$0.854 \pm 0.048$</td>
<td style="text-align: left;">$0.841 \pm 0.027$</td>
</tr>
<tr>
<td style="text-align: left;">Relate</td>
<td style="text-align: left;">$0.829 \pm 0.040$</td>
<td style="text-align: left;">$0.798 \pm 0.053$</td>
<td style="text-align: left;">$0.811 \pm 0.025$</td>
</tr>
</tbody>
</table>
<p>Table 4 Per class metrics for GPT-3 fine-tuned model</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Class</th>
<th style="text-align: left;">Precision</th>
<th style="text-align: left;">Recall</th>
<th style="text-align: left;">F1-score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NA</td>
<td style="text-align: left;">$0.570 \pm 0.112$</td>
<td style="text-align: left;">$0.366 \pm 0.108$</td>
<td style="text-align: left;">$0.431 \pm 0.089$</td>
</tr>
<tr>
<td style="text-align: left;">Negative</td>
<td style="text-align: left;">$0.847 \pm 0.047$</td>
<td style="text-align: left;">$0.881 \pm 0.042$</td>
<td style="text-align: left;">$0.863 \pm 0.041$</td>
</tr>
<tr>
<td style="text-align: left;">Positive</td>
<td style="text-align: left;">$0.819 \pm 0.045$</td>
<td style="text-align: left;">$0.866 \pm 0.034$</td>
<td style="text-align: left;">$0.841 \pm 0.022$</td>
</tr>
<tr>
<td style="text-align: left;">Relate</td>
<td style="text-align: left;">$0.820 \pm 0.027$</td>
<td style="text-align: left;">$0.815 \pm 0.022$</td>
<td style="text-align: left;">$0.817 \pm 0.015$</td>
</tr>
</tbody>
</table>
<p>performance. There are two possible reasons for this outcome. Firstly, as previously discussed, the distribution of data in the dataset is imbalanced, with a smaller number of NA samples. Secondly, there may be inherent challenges in defining the classes in the original problem, which could necessitate further investigation and deliberation. However, exploring these concerns is beyond the scope of this paper.</p>
<h1>Comparison of outputs of our approach using a web-based solution</h1>
<p>In the study [24], the authors utilized their best-performing model $\left(B^{2} R E_{T L}\right)$ on a large corpus of text to extract disease-microbiome relationships, which they subsequently released as the MDIDB database.</p>
<p>We aimed to compare the outputs generated by our model with those in the MDIDB database. To accomplish this, we devised a straightforward graph and visualization strategy, as illustrated in the first panel of Fig. 7. The process involved running both models on the original set of evidence statements used in MDIDB and comparing the resulting graphs. In our graph representation, nodes correspond to diseases or microbes, while edges represent established relationships between them. Nodes are colored green when both algorithms agree on the nature of the relationship, and red when they disagree. We also developed a web application for this which is accessible on our supplementary website [see Additional file 1]. The user interface of the tool allows users to select the number of edges they wish to visualize from the larger graph. After specifying, for example, 50 edges in the provided text box, users can click the "generate knowledge graph" button to display the corresponding knowledge graph. Zooming and hovering over the edges of the graph provide information on the differences in predictions between the two models, including evidence text, for both red and green nodes (as depicted in Fig. 7). This approach aims to provide expert researchers with a more comprehensive understanding of the performance of the different models.</p>
<h2>Discussions</h2>
<p>In this paper, we address several crucial aspects concerning the utility of pre-trained language models and their applicability to relevant challenges in the biomedical domain. Specifically, we focus on the task of extracting disease-microbe relationships from scientific publications. To approach this problem, we frame it as a relation extraction task, enabling us to explore the potential of various language models in generative and discriminative paradigms. Our initial investigation involves assessing the capability of
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 7 Comparing the MDIDB knowledge base generated using BERE (TL) model versus our prediction model</p>
<p>generative models, namely GPT-3, BioGPT, and BioMedLM, in a zero-shot or few-shot setting. We sought to determine if these models can perform well on the task with minimal fine-tuning or data preparation. However, we find that their results are of poor quality, highlighting the need for domain-specific adaptations to enhance their usefulness.</p>
<p>Interestingly, we discover that GPT-3 performs the best when fine-tuned. Subsequently, we explore the performance of discriminative models, specifically the BERT-based models and their domain-specific adaptations such as BioMegatron, PubMedBERT, BioClinicalBERT, and BioLinkBERT. As expected, fine-tuning these models yields state-of-the-art results for the task. We also observe that the quality of the training data significantly influences the accuracy improvements achieved. Our work serves as a foundation for further research on adapting and leveraging language models in the field of biomedicine. In conclusion, we have demonstrated that language models in both generative and discriminative settings are viable candidates for fine-tuning and constructing models that yield SOTA results for the microbiome-disease relationship extraction task.</p>
<p>There are several avenues for future exploration. For instance, investigating a broader range of models, including Galactica [40], LLaMA [41], GPT-4 [42], etc., could provide valuable insights for these tasks. Additionally, as we are in the era of models like ChatGPT [43], it would be interesting to explore the possibility of fine-tuning similar conversational models. As a preliminary experiment, we briefly examined ChatGPT using their publicly available service, and the results can be found in the supplementary website [see Additional file 1] of this paper. While the initial findings appear promising, we observed that the model produced different outputs for the same prompt, raising concerns about the reliability of the generated responses. These observations align with our experiences during the fine-tuning of GPT-3, indicating the need for further refinement in this area. Such investigations could lead to exciting new research directions.</p>
<p>Furthermore, we identify limitations in entity recognition and normalization within the GSC dataset. Addressing these issues requires additional work to refine the end-toend pipeline and build accurate and trustworthy knowledge bases. Another important aspect of this research is that once reliable knowledge bases are established, they can serve as a foundation for formulating hypotheses regarding potentially new diseasemicrobe associations, thus fostering new knowledge and discoveries [44, 45]. Previous studies, such as those conducted by [46, 47], have explored similar approaches. In addition, while we considered the current work purely as a NLP task, augmenting with other heterogeneous knowledge networks (such as in [48, 49]) can further improve the prediction ability of the models.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">MDIDB</th>
<th style="text-align: left;">Microbe-Disease Interactions Database</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TL</td>
<td style="text-align: left;">Transfer Learning</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA</td>
<td style="text-align: left;">Large Language Model Meta AI</td>
</tr>
<tr>
<td style="text-align: left;">SOTA</td>
<td style="text-align: left;">State-Of-The-Art</td>
</tr>
</tbody>
</table>
<h1>Supplementary Information</h1>
<p>The online version contains supplementary material available at (https://doi.org/10.1186/s12859-023-05411-z).</p>
<h2>Additional file 1: Supplementary material.</h2>
<h2>Acknowledgements</h2>
<p>The authors thank Prof. Hiroaki Kitano from The Systems Biology Institute for his constant support and the ONRG Grant for the Nobel Turing challenge to The Systems Biology Institute. The authors also thank Dr Nicolas Shinada from The Systems Biology Institute for his support in re-annotating the GSC dataset. Sathwik Acharya was a undergraduate student intern from PES University India for 6 months at the Systems Biology Institute during the period of this research.</p>
<h2>Author contributions</h2>
<p>SKP designed and led the study. SA, NK and SKP developed the methods. SA and NK implemented the algorithms, performed experiments and analysed of the data. NK, SKP and SA wrote the manuscript. All authors reviewed and approved the final manuscript.</p>
<h2>Funding</h2>
<p>The study is partly funded by ONR Global Grant—N62909-21-1-2032.</p>
<h2>Availability of data and materials</h2>
<p>All data, model and code used in this project is available at the supplementary companion website https://bit.ly/micro biomeLLM</p>
<h2>Declarations</h2>
<h2>Ethics approval and consent to participate</h2>
<p>Not applicable.</p>
<h2>Consent for publication</h2>
<p>Not applicable.</p>
<h2>Competing interests</h2>
<p>The authors declare no competing interests.
Received: 22 May 2023 Accepted: 13 July 2023
Published online: 19 July 2023</p>
<h2>References</h2>
<ol>
<li>Sommer F, Bäckhed F. The gut microbiota-masters of host development and physiology. Nat Rev Microbiol. 2013;11(4):227-38.</li>
<li>Lozupone CA, Stombaugh JI, Gordon JI, Jansson JK, Knight R. Diversity, stability and resilience of the human gut microbiota. Nature. 2012;489(7415):220-30.</li>
<li>Li L, Jing Q, Yan S, Liu X, Sun Y, Zhu D, Wang D, Hao C, Xue D. Amadis: a comprehensive database for association between microbiota and disease. Front Physiol. 2021;12: 697059.</li>
<li>Janssens Y, Nielandt J, Bronselaer A, Debunne N, Verbeke F, Wynendaele E, Van Immerseel F, Vandewynckel Y-P, De Tré G, De Spiegeleer B. Disbiome database: linking the microbiome to disease. BMC Microbiol. 2018;18(1):50. https://doi.org/10.1186/s12866-018-1197-5</li>
<li>Yao G, Zhang W, Yang M, Yang H, Wang J, Zhang H, Wei L, Xie Z, Li W. MicroPhenoDB associates metagenomic data with pathogenic microbes, microbial core genes, and human disease phenotypes. Genom Proteom Bioinform. 2020;18(6):760-72. https://doi.org/10.1016/j.gpb.2020.11.001.</li>
<li>Noronha A. The virtual metabolic human database: integrating human and gut microbiome metabolism with nutrition and disease. Nucleic Acids Res. 2019;47(D1):614-24. https://doi.org/10.1093/nar/gky992.</li>
<li>Zhang J, Chen X, Zou J, Li C, Kang W, Guo Y, Liu S, Zhao W, Mou X, Huang J, Ke J. MADET: a manually curated knowledge base for microbiomic effects on efficacy and toxicity of anticancer treatments. microbiology spectrum. 2022;10(6):02116-22. https://doi.org/10.1128/spectrum.02116-22</li>
<li>Qi C, Cai Y, Qian K, Li X, Ren J, Wang P, Fu T, Zhao T, Cheng L, Shi L, Zhang X. gutMDisorder v2.0: a comprehensive database for dysbiosis of gut microbiota in phenotypes and interventions. Nucleic Acids Res. 2022. https://doi. org/10.1093/nar/gkac871</li>
<li>
<p>Cheng L, Qi C, Zhuang H, Fu T, Zhang X. gutMDisorder: a comprehensive database for dysbiosis of the gut microbiota in disorders and interventions. Nucleic Acids Res. 2020;48(D1):554-60. https://doi.org/10.1093/nar/ gkz843.</p>
</li>
<li>
<p>Ma W, Zhang L, Zeng P, Huang C, Li J, Geng B, Yang J, Kong W, Zhou X, Cui Q. An analysis of human microbe- disease associations. Brief Bioinform. 2017;18(1):85-97.</p>
</li>
<li>Jin H, Hu G, Sun C, Duan Y, Zhang Z, Liu Z, Zhao X-M, Chen W-H. mbodymap: a curated database for microbes across human body and their associations with health and diseases. Nucleic Acids Res. 2022;50(D1):808-16.</li>
<li>Jo A. The promise and peril of generative AI. Nature. 2023;614(1):214-6.</li>
<li>Badal VD, Wright D, Katso Y, Kim H-C, Swafford AD, Knight R, Hsu C-N. Challenges in the construction of knowledge bases for human microbiome-disease associations. Microbiome 2019;7(1):1-15. Publisher: BioMed Central.</li>
<li>Wang Q, Xu R. Automatic extraction, prioritization and analysis of gut microbial metabolites from biomedical literature. Sci Rep. 2020;10(1):1-10.</li>
<li>Lim KMK, Li C, Chng KR, Nagarajan N. MInter: automated text-mining of microbial interactions. Bioinformatics. 2016;32(19):2981-7.</li>
<li>Ahmed SAJA, Bapatdhar N, Kumar BP, Ghosh S, Yachie A, Palaniappan SK. Large scale text mining for deriving useful insights: a case study focused on microbiome. Front Physiol. 2022;13</li>
<li>Xu H, Li X, Zheng C, Liu K, Liu S, Zeng Y, Song Z, Cui S, Xu Y. Gdrebase: a comprehensive, indexed and updated knowledge base for relations between human gut microbes and diseases. 2022.</li>
<li>Qu J, Zhao Y, Yin J. Identification and analysis of human microbe-disease associations by matrix decomposition and label propagation. Front Microbiol. 2019;10</li>
<li>Peng L, Shen L, Liao L, Liu G, Zhou L. RNMFMDA: a microbe-disease association identification method based on reliable negative sample selection and logistic matrix factorization with neighborhood regularization. Front Microbiol. 2020;11.</li>
<li>Konstantinova N. Review of relation extraction methods: What is new out there? In: Analysis of Images, Social Networks and Texts: Third International Conference, AIST 2014, Yekaterinburg, Russia, April 10-12, 2014, Revised Selected Papers 2014;3:15-28</li>
<li>Nédellec C, Bossy R, Kim J-D, Kim J-J, Ohta T, Pyysalo S, Zweigenbaum P. Overview of bionlp shared task 2013. In: Proceedings of the BioNLP Shared Task 2013 Workshop. 2013:1-7.</li>
<li>Wang H, Qin K, Zakari RY, Lu G, Yin J. Deep neural network-based relation extraction: an overview. Neural Comput Appl. 2022;1-21.</li>
<li>Park Y, Lee J, Moon H, Choi YS, Rho M. Discovering microbe-disease associations from the literature using a hierarchical long short-term memory network and an ensemble parser model. Sci Rep. 2021;11(1):1-12</li>
<li>Wu C, Xiao X, Yang C, Chen J, Yi J, Qiu Y. Mining microbe-disease interactions from literature via a transfer learning model. BMC Bioinform. 2021;22(1):1-15.</li>
<li>Hong L, Lin J, Li S, Wan F, Yang H, Jiang T, Zhao D, Zeng J. A novel machine learning framework for automated biomedical relation extraction from large-scale literature repositories. Nat Mach Intell. 2020;2(6):347-55.</li>
<li>Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell A, et al. Language models are few-shot learners. Adv Neural Inf Process Syst. 2020;33:1877-901.</li>
<li>Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. In Advances in neural information processing systems 2017:30.</li>
<li>Luo R, Sun L, Xia Y, Qin T, Zhang S, Poon H, Liu T-Y. Biogpt: generative pre-trained transformer for biomedical text generation and mining. Brief Bioinform. 2022;23(6)</li>
<li>Venigalla A, Frankle J, Carbin M. BiomedIm: a domain-specific large language model for biomedical text. MosaicML. Accessed: Dec 2022;23</li>
<li>Devlin J, Chang M-W, Lee K, Toutanova K. Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 2018.</li>
<li>Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, Kang J. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics. 2020;36(4):1234-40.</li>
<li>Gu Y, Tinn R, Cheng H, Lucas M, Usuyama N, Liu X, Naumann T, Gao J, Poon H. Domain-specific language model pretraining for biomedical natural language processing. 2020. arXiv:2007.15779</li>
<li>Shin H-C, Zhang Y, Bakhturina E, Puri R, Patwary M, Shoeybi M, Mani R. Brigadoon: Larger biomedical domain language model. arXiv preprint arXiv:2010.06060. 2020.</li>
<li>Yasunaga M, Leskovec J, Liang P. LinkBERT: Pretraining Language Models with Document Links. 2022. arXiv preprint arXiv:2203.15827</li>
<li>Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, Cistac P, Rault T, Louf R, Funtowicz M, et al. Huggingface's transformers: state-of-the-art natural language processing. arXiv preprint arXiv:1910.03771. 2019.</li>
<li>Alsentzer E, Murphy JR, Boag W, Weng W-H, Jin D, Naumann T, McDermott M. Publicly available clinical bert embeddings. arXiv preprint arXiv:1904.03323 2019.</li>
<li>Reynolds L, McDonell K. Prompt programming for large language models: beyond the few-shot paradigm. 2021:1-7</li>
<li>Moradi M, Blagec K, Haberl F, Samwald M. Gpt-3 models are poor few-shot learners in the biomedical domain. arXiv preprint arXiv:2109.02555 2021.</li>
<li>Nakayama H, Kubo T, Kamura J, Taniguchi Y, Liang X. doccano: text annotation tool for human. Software available from https://github.com/doccano/doccano 2018.</li>
<li>Taylor R, Kardas M, Cucurulli G, Scialom T, Hartshorn A, Saravia E, Poulton A, Kerkez V, Stojnic R. Galactica: a large language model for science. arXiv preprint arXiv:2211.09085 2022.</li>
<li>Touvron H, Lavril T, Izacard G, Martinet X, Lachaux M-A, Lacroix T, Rozière B, Goyal N, Hambro E, Azhar F, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. 2023.</li>
<li>OpenAI: GPT-4 Technical Report 2023. arXiv:2303.08774</li>
<li>OpenAI: ChatGPT: Optimizing Language Models for Dialogue. https://openai.com/blog/chatgpt/. 2022.</li>
<li>Kitano H. Nobel turing challenge: creating the engine for scientific discovery. NPJ Syst Biol Appl. 2021;7(1):1-12.</li>
<li>
<p>Kitano H. Artificial intelligence to win the nobel prize and beyond: creating the engine for scientific discovery. AI Mag. 2016;37(1):39-49.</p>
</li>
<li>
<p>Bao W, Jiang Z, Huang D-S. Novel human microbe-disease association prediction using network consistency projection. BMC Bioinform. 2017;18(16):173-81.</p>
</li>
<li>Huang Y-A, You Z-H, Chen X, Huang Z-A, Zhang S, Yan G-Y. Prediction of microbe-disease association from the integration of neighbor and graph with collaborative recommendation model. J Transl Med. 2017;15(1):1-11.</li>
<li>Zhao B-W, Wang L, Hu P-W, Wong L, Su X-R, Wang B-Q, You Z-H, Hu L. Fusing higher and lower-order biological information for drug repositioning via graph representation learning. IEEE Trans Emerg Comput. 2023;</li>
<li>Zhao B-W, You Z-H, Hu L, Guo Z-H, Wang L, Chen Z-H, Wong L. A novel method to predict drug-target interactions based on large-scale graph representation learning. Cancers. 2021;13(9):2111.</li>
</ol>
<h1>Publisher's Note</h1>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
<h2>Ready to submit your research? Choose BMC and benefit from:</h2>
<ul>
<li>fast, convenient online submission</li>
<li>thorough peer review by experienced researchers in your field</li>
<li>rapid publication on acceptance</li>
<li>support for research data, including large and complex data types</li>
<li>gold Open Access which fosters wider collaboration and increased citations</li>
<li>maximum visibility for your research: over 100M website views per year</li>
</ul>
<p>At BMC, research is always in progress.
Learn more biomedcentral.com/submissions</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Abbreviations</p>
<p>GPT Generative Pre-trained Transformer
BERT Bidirectional Encoder Representations from Transformers
MADET Microbiomics of Anticancer Drug Efficacy and Toxicity
HMDAD Human Microbe-Disease Association Database
NLP Natural Language Processing
NER Named Entity Recognizer
LLMs Large Language Models
LSTM Long Short-Term Memory
GSC Gold Standard Corpus
SSC Silver Standard Corpus
CC Commercial-Collection
GPU Graphics Processing Unit
DDI Drug-Drug Interaction&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4965 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4965</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4965</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-16af44cd417766bab61e4ec9ebd4566cfa2a3b93</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/16af44cd417766bab61e4ec9ebd4566cfa2a3b93" target="_blank">What Makes Chain-of-Thought Prompting Effective? A Counterfactual Study</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4965.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4965.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-62B with Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PaLM-62B (a 62B-parameter PaLM decoder-only transformer) evaluated with standard Chain-of-Thought (CoT) few-shot prompts that include intermediate reasoning 'thought' steps before answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-62B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 62B, a large decoder-only Transformer language model used in this study for in-context few-shot and chain-of-thought prompting experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Few-shot prompts include explicit intermediate 'thought' steps (t_i) for each example: the model generates a chain of intermediate reasoning tokens before the final answer. The examples in the prompt are presented with a consistent pattern/template.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-8K; SPORTS (BIG-bench); DATE (BIG-bench); SORTING</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM-8K: grade-school math word problems; SPORTS: verify plausibility of athlete-activity statements; DATE: compute date offsets; SORTING: sort single-digit integers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM-8K accuracy 27.37%; SPORTS accuracy 93.67%; DATE accuracy 45.18%; SORTING accuracy 60.6% (reported CoT solve rates)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>DIRECT (no intermediate thoughts) — GSM-8K 10.11%; SPORTS 71.08%; DATE 31.61%; SORTING 46.0%</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Standard CoT (consistent intermediate steps/templates) substantially improves performance over DIRECT prompting across tasks (large gains on GSM-8K, DATE, SPORTS, SORTING). CoT is robust to changing low-level symbols (e.g., digits → abstract tokens) but sensitive to preserving a consistent pattern and readable text style.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>On some tasks, incorrect intermediate content (wrong equations) only slightly degrades performance (e.g., GSM-8K: pat_wrong 24.39% vs CoT 27.37%). CoT's gains are task-dependent; when prompts break pattern consistency or text structure, performance can drop sharply.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4965.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4965.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-pat_inconsistent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-62B with inconsistent pattern few-shot prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The same model but few-shot examples contain inconsistent thought formats/templates across examples (multiple different reasoning-style templates rather than a uniform pattern).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-62B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 62B as above.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Inconsistent-pattern prompting (C_pat_inconsistent)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Few-shot examples deliberately vary the structure of intermediate 'thought' steps across examples (e.g., different ordering or phrasing of the same logical elements), producing a diversity of reasoning surface forms in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-8K; SPORTS; DATE; SORTING</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as for CoT above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM-8K 21.46%; SPORTS 79.01%; DATE 34.19%; SORTING 45.0% (reported C_pat_inconsistent solve rates)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT (consistent patterns) — GSM-8K 27.37%; SPORTS 93.67%; DATE 45.18%; SORTING 60.6%</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Introducing diversity/inconsistency in the few-shot thought formats (i.e., multiple different reasoning templates) degrades performance relative to a consistent CoT prompt across tasks — often substantially (notably SPORTS and GSM-8K). This supports that consistency of pattern (a 'similar' reasoning presentation) helps model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The magnitude of degradation varies by task; for some symbolic tasks changes are smaller. Overall, inconsistent presentation is harmful rather than beneficial in these experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4965.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4965.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-pat_only</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-62B with pattern-only CoT prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Few-shot examples are reduced to only the essential pattern information (e.g., just equations or distilled pattern statements) and omit surrounding contextual text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-62B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 62B as above.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Pattern-only prompting (C_pat_only)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Prompts keep only the structural/pattern elements of thoughts (e.g., equations or minimal pattern statements) without the fuller textual context; maintains a single, consistent pattern but reduces contextual detail.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-8K; SPORTS; DATE; SORTING</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM-8K 10.01%; SPORTS 74.13%; DATE 33.52%; SORTING 46.0% (reported C_pat_only solve rates)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT (full thoughts) — GSM-8K 27.37%; SPORTS 93.67%; DATE 45.18%; SORTING 60.6%</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Preserving only the abstract pattern without contextual text substantially reduces performance for many tasks (especially GSM-8K and SPORTS), indicating that pattern alone is often insufficient; contextual text in the thought examples contributes materially to success.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>For SORTING the pattern-only prompt produced similar performance to DIRECT (46.0%), suggesting some symbolic tasks may require primarily the pattern and less context.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4965.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4965.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-pat_wrong</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-62B with wrong-pattern (misleading) CoT prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Few-shot examples maintain the same thought template/pattern but include incorrect factual or structural content (e.g., wrong equations, erroneous associations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-62B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 62B as above.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Wrong-pattern prompting (C_pat_wrong)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Prompts keep a consistent thought-pattern/template but populate it with incorrect or misleading intermediate results or facts (e.g., incorrect equations), preserving surface similarity while corrupting correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-8K; SPORTS; DATE; SORTING</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM-8K 24.39%; SPORTS 46.02%; DATE 44.84%; SORTING 64.8% (reported C_pat_wrong / pat_arong values)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT (correct patterns) — GSM-8K 27.37%; SPORTS 93.67%; DATE 45.18%; SORTING 60.6%</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Maintaining a consistent reasoning template but inserting incorrect content has task-dependent effects: for GSM-8K and DATE the drop is small (CoT vs pat_wrong similar), indicating the model can still infer task objective; for SPORTS, incorrect factual associations catastrophically reduce performance; for SORTING, surprising improvements were observed (wrong ordering in prompts improved solve rate), indicating that in some tasks 'wrong' examples can inadvertently help clarify the task.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The fact that pat_wrong can nearly match CoT on math and date but collapse on sports shows that correctness of few-shot facts matters when those facts are central to understanding the task; additionally, sorting showed a counterexample where wrong-pattern increased accuracy (64.8% > 60.6%).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4965.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4965.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-DIRECT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-62B with Direct (no-thought) few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Few-shot examples contain only question→answer pairs (no intermediate reasoning), i.e., standard few-shot prompting without CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-62B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 62B as above.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>DIRECT prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Standard few-shot prompting where examples show only inputs and final outputs (no intermediate chain-of-thought).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-8K; SPORTS; DATE; SORTING</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM-8K 10.11%; SPORTS 71.08%; DATE 31.61%; SORTING 46.0% (reported DIRECT solve rates)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT (with intermediate thoughts) — GSM-8K 27.37%; SPORTS 93.67%; DATE 45.18%; SORTING 60.6%</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DIRECT prompting is outperformed by CoT across examined tasks, often substantially; the presence of intermediate reasoning examples helps either by communicating the task intent or by eliciting background knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>In a few settings (e.g., some sorting pattern-only variants) DIRECT-like performance is comparable, indicating some tasks can be solved with minimal examples.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4965.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4965.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-symb_abs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-62B with abstracted symbols in CoT prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CoT few-shot examples where concrete symbols (digits, names, dates) are replaced with abstract placeholders (Greek letters or placeholders like ACTIVITY).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-62B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 62B as above.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Symbol-abstracted CoT (C_symb_abs)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Keep CoT structure and consistent patterns but substitute concrete symbols in examples with abstract placeholders; question remains in original (non-abstract) form.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-8K; SPORTS; DATE; SORTING</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM-8K 25.70%; SPORTS 92.11%; DATE 37.41%; SORTING 61.8% (reported C_symb_abs solve rates)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT (concrete symbols) — GSM-8K 27.37%; SPORTS 93.67%; DATE 45.18%; SORTING 60.6%</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Replacing concrete symbols in few-shot examples with abstract placeholders has little to modest impact on performance — models often attend to the same tokens and reach similar answers. This indicates that exact symbol identity is less important than pattern/structure and task intent.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>A small fraction of outputs for GSM-8K still included abstract symbols in answers; the impact varies by dataset (DATE and SPORTS showed larger sensitivity than GSM-8K for some abstract substitutions).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4965.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4965.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-symb_ood</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-62B with out-of-distribution symbol prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CoT few-shot examples where symbols are replaced by out-of-distribution variants (e.g., integers replaced with fractions, dates beyond year 3000, uncommon names/actions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-62B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 62B as above.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>OOD-symbol CoT (C_symb_ood)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Prompts keep CoT structure but instantiate examples with symbol tokens that are atypical relative to the model's expected distribution (fractions, far-future dates, fabricated names), testing generalization from examples to original question symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-8K; SPORTS; DATE; SORTING</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM-8K (fractions) 25.7% (reported); SPORTS (fabricated names/activities) 79.72%; DATE (far-future dates) 44.50%; SORTING (larger integers) 80.0% (reported C_symb_ood values)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT (in-distribution symbols) — GSM-8K 27.37%; SPORTS 93.67%; DATE 45.18%; SORTING 60.6%</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using OOD symbols has task-dependent effects: little effect for GSM-8K and DATE, large detrimental effect for SPORTS (because OOD symbols obscured task understanding), but in SORTING using larger integers (an OOD variant) improved solve rate notably (80.0% vs CoT 60.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>OOD symbol prompts can either hurt (SPORTS) or help (SORTING), showing that symbol distribution interacts with how the task is communicated.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4965.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4965.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-CCoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-62B with Concise Chain-of-Thought (CCoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distilled/concise variant of CoT that preserves essential pattern and required information while removing extraneous tokens from few-shot 'thought' examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-62B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 62B as above.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Concise Chain-of-Thought (CCoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Construct few-shot thoughts that are minimal but still convey the necessary pattern/steps (shorter, more compact thoughts). The idea is to retain the pattern and ability to elicit missing commonsense while using fewer tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-8K; DATE; SORTING (evaluated across models)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported relative improvements vs CoT: PaLM-62B GSM: +6.2% (relative); DATE: +14.8% (relative); SORTING: +9% (relative). (Table 5 reports relative performance differences of CCoT over CoT.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT (full thoughts) — baseline CoT solve rates (see PaLM-CoT entry).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A concise, pattern-preserving CoT (CCoT) can match or outperform standard CoT while using fewer tokens; suggests that the critical element is conveying task intent/pattern succinctly rather than long verbose thoughts.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Exact magnitudes vary by model and task; some model/task combinations (noted in appendix) have different relative gains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? <em>(Rating: 2)</em></li>
                <li>Complementary Explanations for Effective In-Context Learning <em>(Rating: 2)</em></li>
                <li>Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters <em>(Rating: 2)</em></li>
                <li>Training Language Models to Follow Instructions with Human Feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4965",
    "paper_id": "paper-16af44cd417766bab61e4ec9ebd4566cfa2a3b93",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "PaLM-CoT",
            "name_full": "PaLM-62B with Chain-of-Thought prompting",
            "brief_description": "PaLM-62B (a 62B-parameter PaLM decoder-only transformer) evaluated with standard Chain-of-Thought (CoT) few-shot prompts that include intermediate reasoning 'thought' steps before answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-62B",
            "model_description": "PaLM 62B, a large decoder-only Transformer language model used in this study for in-context few-shot and chain-of-thought prompting experiments.",
            "reasoning_method_name": "Chain-of-Thought (CoT)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Few-shot prompts include explicit intermediate 'thought' steps (t_i) for each example: the model generates a chain of intermediate reasoning tokens before the final answer. The examples in the prompt are presented with a consistent pattern/template.",
            "task_name": "GSM-8K; SPORTS (BIG-bench); DATE (BIG-bench); SORTING",
            "task_description": "GSM-8K: grade-school math word problems; SPORTS: verify plausibility of athlete-activity statements; DATE: compute date offsets; SORTING: sort single-digit integers.",
            "performance": "GSM-8K accuracy 27.37%; SPORTS accuracy 93.67%; DATE accuracy 45.18%; SORTING accuracy 60.6% (reported CoT solve rates)",
            "comparison_with_other_method": true,
            "performance_other_method": "DIRECT (no intermediate thoughts) — GSM-8K 10.11%; SPORTS 71.08%; DATE 31.61%; SORTING 46.0%",
            "key_findings": "Standard CoT (consistent intermediate steps/templates) substantially improves performance over DIRECT prompting across tasks (large gains on GSM-8K, DATE, SPORTS, SORTING). CoT is robust to changing low-level symbols (e.g., digits → abstract tokens) but sensitive to preserving a consistent pattern and readable text style.",
            "counter_examples_or_negative_results": "On some tasks, incorrect intermediate content (wrong equations) only slightly degrades performance (e.g., GSM-8K: pat_wrong 24.39% vs CoT 27.37%). CoT's gains are task-dependent; when prompts break pattern consistency or text structure, performance can drop sharply.",
            "uuid": "e4965.0"
        },
        {
            "name_short": "PaLM-pat_inconsistent",
            "name_full": "PaLM-62B with inconsistent pattern few-shot prompts",
            "brief_description": "The same model but few-shot examples contain inconsistent thought formats/templates across examples (multiple different reasoning-style templates rather than a uniform pattern).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-62B",
            "model_description": "PaLM 62B as above.",
            "reasoning_method_name": "Inconsistent-pattern prompting (C_pat_inconsistent)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Few-shot examples deliberately vary the structure of intermediate 'thought' steps across examples (e.g., different ordering or phrasing of the same logical elements), producing a diversity of reasoning surface forms in the prompt.",
            "task_name": "GSM-8K; SPORTS; DATE; SORTING",
            "task_description": "Same as for CoT above.",
            "performance": "GSM-8K 21.46%; SPORTS 79.01%; DATE 34.19%; SORTING 45.0% (reported C_pat_inconsistent solve rates)",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT (consistent patterns) — GSM-8K 27.37%; SPORTS 93.67%; DATE 45.18%; SORTING 60.6%",
            "key_findings": "Introducing diversity/inconsistency in the few-shot thought formats (i.e., multiple different reasoning templates) degrades performance relative to a consistent CoT prompt across tasks — often substantially (notably SPORTS and GSM-8K). This supports that consistency of pattern (a 'similar' reasoning presentation) helps model performance.",
            "counter_examples_or_negative_results": "The magnitude of degradation varies by task; for some symbolic tasks changes are smaller. Overall, inconsistent presentation is harmful rather than beneficial in these experiments.",
            "uuid": "e4965.1"
        },
        {
            "name_short": "PaLM-pat_only",
            "name_full": "PaLM-62B with pattern-only CoT prompts",
            "brief_description": "Few-shot examples are reduced to only the essential pattern information (e.g., just equations or distilled pattern statements) and omit surrounding contextual text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-62B",
            "model_description": "PaLM 62B as above.",
            "reasoning_method_name": "Pattern-only prompting (C_pat_only)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Prompts keep only the structural/pattern elements of thoughts (e.g., equations or minimal pattern statements) without the fuller textual context; maintains a single, consistent pattern but reduces contextual detail.",
            "task_name": "GSM-8K; SPORTS; DATE; SORTING",
            "task_description": "Same as above.",
            "performance": "GSM-8K 10.01%; SPORTS 74.13%; DATE 33.52%; SORTING 46.0% (reported C_pat_only solve rates)",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT (full thoughts) — GSM-8K 27.37%; SPORTS 93.67%; DATE 45.18%; SORTING 60.6%",
            "key_findings": "Preserving only the abstract pattern without contextual text substantially reduces performance for many tasks (especially GSM-8K and SPORTS), indicating that pattern alone is often insufficient; contextual text in the thought examples contributes materially to success.",
            "counter_examples_or_negative_results": "For SORTING the pattern-only prompt produced similar performance to DIRECT (46.0%), suggesting some symbolic tasks may require primarily the pattern and less context.",
            "uuid": "e4965.2"
        },
        {
            "name_short": "PaLM-pat_wrong",
            "name_full": "PaLM-62B with wrong-pattern (misleading) CoT prompts",
            "brief_description": "Few-shot examples maintain the same thought template/pattern but include incorrect factual or structural content (e.g., wrong equations, erroneous associations).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-62B",
            "model_description": "PaLM 62B as above.",
            "reasoning_method_name": "Wrong-pattern prompting (C_pat_wrong)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Prompts keep a consistent thought-pattern/template but populate it with incorrect or misleading intermediate results or facts (e.g., incorrect equations), preserving surface similarity while corrupting correctness.",
            "task_name": "GSM-8K; SPORTS; DATE; SORTING",
            "task_description": "Same as above.",
            "performance": "GSM-8K 24.39%; SPORTS 46.02%; DATE 44.84%; SORTING 64.8% (reported C_pat_wrong / pat_arong values)",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT (correct patterns) — GSM-8K 27.37%; SPORTS 93.67%; DATE 45.18%; SORTING 60.6%",
            "key_findings": "Maintaining a consistent reasoning template but inserting incorrect content has task-dependent effects: for GSM-8K and DATE the drop is small (CoT vs pat_wrong similar), indicating the model can still infer task objective; for SPORTS, incorrect factual associations catastrophically reduce performance; for SORTING, surprising improvements were observed (wrong ordering in prompts improved solve rate), indicating that in some tasks 'wrong' examples can inadvertently help clarify the task.",
            "counter_examples_or_negative_results": "The fact that pat_wrong can nearly match CoT on math and date but collapse on sports shows that correctness of few-shot facts matters when those facts are central to understanding the task; additionally, sorting showed a counterexample where wrong-pattern increased accuracy (64.8% &gt; 60.6%).",
            "uuid": "e4965.3"
        },
        {
            "name_short": "PaLM-DIRECT",
            "name_full": "PaLM-62B with Direct (no-thought) few-shot prompting",
            "brief_description": "Few-shot examples contain only question→answer pairs (no intermediate reasoning), i.e., standard few-shot prompting without CoT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-62B",
            "model_description": "PaLM 62B as above.",
            "reasoning_method_name": "DIRECT prompting",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Standard few-shot prompting where examples show only inputs and final outputs (no intermediate chain-of-thought).",
            "task_name": "GSM-8K; SPORTS; DATE; SORTING",
            "task_description": "Same as above.",
            "performance": "GSM-8K 10.11%; SPORTS 71.08%; DATE 31.61%; SORTING 46.0% (reported DIRECT solve rates)",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT (with intermediate thoughts) — GSM-8K 27.37%; SPORTS 93.67%; DATE 45.18%; SORTING 60.6%",
            "key_findings": "DIRECT prompting is outperformed by CoT across examined tasks, often substantially; the presence of intermediate reasoning examples helps either by communicating the task intent or by eliciting background knowledge.",
            "counter_examples_or_negative_results": "In a few settings (e.g., some sorting pattern-only variants) DIRECT-like performance is comparable, indicating some tasks can be solved with minimal examples.",
            "uuid": "e4965.4"
        },
        {
            "name_short": "PaLM-symb_abs",
            "name_full": "PaLM-62B with abstracted symbols in CoT prompts",
            "brief_description": "CoT few-shot examples where concrete symbols (digits, names, dates) are replaced with abstract placeholders (Greek letters or placeholders like ACTIVITY).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-62B",
            "model_description": "PaLM 62B as above.",
            "reasoning_method_name": "Symbol-abstracted CoT (C_symb_abs)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Keep CoT structure and consistent patterns but substitute concrete symbols in examples with abstract placeholders; question remains in original (non-abstract) form.",
            "task_name": "GSM-8K; SPORTS; DATE; SORTING",
            "task_description": "Same as above.",
            "performance": "GSM-8K 25.70%; SPORTS 92.11%; DATE 37.41%; SORTING 61.8% (reported C_symb_abs solve rates)",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT (concrete symbols) — GSM-8K 27.37%; SPORTS 93.67%; DATE 45.18%; SORTING 60.6%",
            "key_findings": "Replacing concrete symbols in few-shot examples with abstract placeholders has little to modest impact on performance — models often attend to the same tokens and reach similar answers. This indicates that exact symbol identity is less important than pattern/structure and task intent.",
            "counter_examples_or_negative_results": "A small fraction of outputs for GSM-8K still included abstract symbols in answers; the impact varies by dataset (DATE and SPORTS showed larger sensitivity than GSM-8K for some abstract substitutions).",
            "uuid": "e4965.5"
        },
        {
            "name_short": "PaLM-symb_ood",
            "name_full": "PaLM-62B with out-of-distribution symbol prompts",
            "brief_description": "CoT few-shot examples where symbols are replaced by out-of-distribution variants (e.g., integers replaced with fractions, dates beyond year 3000, uncommon names/actions).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-62B",
            "model_description": "PaLM 62B as above.",
            "reasoning_method_name": "OOD-symbol CoT (C_symb_ood)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Prompts keep CoT structure but instantiate examples with symbol tokens that are atypical relative to the model's expected distribution (fractions, far-future dates, fabricated names), testing generalization from examples to original question symbols.",
            "task_name": "GSM-8K; SPORTS; DATE; SORTING",
            "task_description": "Same as above.",
            "performance": "GSM-8K (fractions) 25.7% (reported); SPORTS (fabricated names/activities) 79.72%; DATE (far-future dates) 44.50%; SORTING (larger integers) 80.0% (reported C_symb_ood values)",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT (in-distribution symbols) — GSM-8K 27.37%; SPORTS 93.67%; DATE 45.18%; SORTING 60.6%",
            "key_findings": "Using OOD symbols has task-dependent effects: little effect for GSM-8K and DATE, large detrimental effect for SPORTS (because OOD symbols obscured task understanding), but in SORTING using larger integers (an OOD variant) improved solve rate notably (80.0% vs CoT 60.6%).",
            "counter_examples_or_negative_results": "OOD symbol prompts can either hurt (SPORTS) or help (SORTING), showing that symbol distribution interacts with how the task is communicated.",
            "uuid": "e4965.6"
        },
        {
            "name_short": "PaLM-CCoT",
            "name_full": "PaLM-62B with Concise Chain-of-Thought (CCoT)",
            "brief_description": "A distilled/concise variant of CoT that preserves essential pattern and required information while removing extraneous tokens from few-shot 'thought' examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-62B",
            "model_description": "PaLM 62B as above.",
            "reasoning_method_name": "Concise Chain-of-Thought (CCoT)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Construct few-shot thoughts that are minimal but still convey the necessary pattern/steps (shorter, more compact thoughts). The idea is to retain the pattern and ability to elicit missing commonsense while using fewer tokens.",
            "task_name": "GSM-8K; DATE; SORTING (evaluated across models)",
            "task_description": "Same as above.",
            "performance": "Reported relative improvements vs CoT: PaLM-62B GSM: +6.2% (relative); DATE: +14.8% (relative); SORTING: +9% (relative). (Table 5 reports relative performance differences of CCoT over CoT.)",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT (full thoughts) — baseline CoT solve rates (see PaLM-CoT entry).",
            "key_findings": "A concise, pattern-preserving CoT (CCoT) can match or outperform standard CoT while using fewer tokens; suggests that the critical element is conveying task intent/pattern succinctly rather than long verbose thoughts.",
            "counter_examples_or_negative_results": "Exact magnitudes vary by model and task; some model/task combinations (noted in appendix) have different relative gains.",
            "uuid": "e4965.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
            "rating": 2
        },
        {
            "paper_title": "Complementary Explanations for Effective In-Context Learning",
            "rating": 2
        },
        {
            "paper_title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters",
            "rating": 2
        },
        {
            "paper_title": "Training Language Models to Follow Instructions with Human Feedback",
            "rating": 1
        }
    ],
    "cost": 0.0194015,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>What makes Chain-of-Thought Prompting Effective? A Counterfactual Study</h1>
<p>Aman Madaan ${ }^{\text {a }}$ Katherine Hermann ${ }^{\text { }}$ Amir Yazdanbakhsh ${ }^{\text { } *}$<br>${ }^{a}$ Language Technologies Institute, Carnegie Mellon University<br>${ }^{\text { }}$ Google DeepMind<br>amadaan@cs.cmu.edu {hermannk, ayazdan}@google.com</p>
<h4>Abstract</h4>
<p>The effectiveness of Chain-of-thought prompting ( COT ) has been widely recognized, but the underlying mechanisms behind its success, the reason why it just works for a wide range of tasks, remains an open question. To investigate this, we employ a counterfactual prompting approach, systematically manipulating elements of examples used in a few-shot prompt, and testing the consequences on model behavior. This allows us to understand the relative contributions of prompt elements such as symbols (digits, entities) and patterns (equations, sentence structure) on in-context learning. Our experiments with three different large language models (LLMs) reveal several key findings. First, the specific symbols used in the prompt do not significantly impact the model's performance. However, consistent patterns in examples and specifying text in style frequently found on the web are crucial. Second, our findings suggest that the necessity of accurate few-shot examples depends on their role in communicating task understanding. We identify tasks where inaccurate few-shot examples hurt and, surprisingly, tasks where they improve performance. Additionally, we find that the intermediate steps in COT may not necessarily facilitate learning how to solve a task, but instead efficiently convey task understanding (what) to the model. Furthermore, CoT leverages LLMs to fill in missing commonsense information, particularly helping difficult reasoning problems and long-tail questions ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have demonstrated remarkable performance in various complex tasks using a small number of examples-a paradigm known as few-shot learning (Brown et al., 2020;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Chowdhery et al., 2022). This progress has been significantly boosted by chain-of-thought prompting (CoT) and its variants (Wei et al., 2022b; Kojima et al., 2022; Zhou et al., 2022), which have been proven to further enhance the LLMs' capabilities (Ling et al., 2017; Nye et al., 2021; Cobbe et al., 2021; Patel et al., 2021; BIG-bench Collaboration, 2022).</p>
<p>Despite its demonstrated effectiveness, the underlying mechanisms behind COT still need to be fully understood. A common explanation draws a parallel with human thinking, in which individuals often reflect on a problem before arriving at a solution (Ling et al., 2017; Wei et al., 2022b,a). While this analogy is intuitive, it does not fully explain the reasons for COT's success, including when and how the COT mechanism operates. Since LLMs are trained to predict the next token in a given context, there might be a more systematic explanation behind the successes and failures of COT. This study aims to explore the mechanism behind COT, providing insights into its operation.</p>
<p>Our approach involves modifying different components of the examples utilized in the few-shot prompt, and assessing the impact of these changes on the final performance (Figure 1). Specifically, we pinpoint the key elements of an example in fewshot prompting as: Symbols (e.g., digits, dates) and Patterns (e.g., equations, templates, sentence structure). We then apply counterfactual prompting (Goyal et al., 2019) where all components except one are held constant- for instance, replacing symbols like numbers with Greek letters. The effect of each component is then assessed by comparing the performance differences between prompt variations. Our experimental approach spans four diverse reasoning tasks and is implemented across three major language models-PaLM, GPT-3, and CODEX, yielding several surprising findings:
(1) Our study reveals that the specific symbols employed in the prompt have minimal impact on the</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We evaluate the effectiveness of chain-of-thought (CoT) prompting by conducting experiments on diverse datasets, and modifying various aspects of the prompt. By identifying symbols and patterns (templates) specific to each dataset, we create counterfactual prompts and compare their output (y<sub>cf</sub>) to that of the original CoT prompt (y). Furthermore, we analyze the attention patterns used by the model when reasoning about the same question using the original and counterfactual prompts, to gain a deeper understanding of the underlying mechanisms.</p>
<p>model's performance. For instance, substituting numbers in the prompt (e.g., 1, 2, 3) with Greek alphabets such as α, β, γ does not significantly affect the model's performance (Section 3). Nevertheless, maintaining a consistent pattern in examples and specifying a text style commonly seen on the web is crucial (Sections 4 and 5).</p>
<p>2 Our findings reveal the nuanced role that the accuracy of few-shot examples plays in task understanding within prompts. We discover that the significance of this accuracy is contingent on how much it contributes to comprehending the task at hand. For instance, in tasks such as mathematical reasoning, the presence of incorrect equations doesn't impede performance significantly, primarily because the task objective—solving a mathematical problem—remains clear, regardless of these errors. Conversely, in tasks like sports reasoning, embedding incorrect information in the prompt can obscure task comprehension, negatively impacting performance. This analysis allows us to differentiate between tasks where the accuracy of few-shot examples is pivotal for understanding, and those where it is less critical. Interestingly, we also unearth instances where including incorrect information paradoxically enhances performance by inadvertently facilitating task comprehension (Section 4).</p>
<p>3 Crucially, we find that symbols and patterns work in unison to bolster CoT in two primary ways: by generating missing information (e.g., through extraction of commonsense knowledge), and reinforcing task understanding (Ouyang et al., 2022) (e.g., by outlining the specific methodology for generating answers). We posit that the successful interplay of symbols and patterns, as facilitated by CoT prompts, plays a more central role in task success than the model's inherent reasoning capabilities (Section 6).</p>
<h2>2 Counterfactual Prompting for CoT</h2>
<p><strong>Chain-of-thought prompting.</strong> In the few-shot prompting setup, the input to the model is a prompt, which consists of <em>k</em> in-context examples in the form of <input → *x\<sub>i\</sub>*, output → *y\<sub>i\</sub>*> tuples, each of which is related to the target task.</p>
<p>Chain-of-thought prompting (CoT, as proposed by Wei et al. (2022b)) includes an additional intermediate step in the form of a "thought" <em>t<sub>i</sub></em>, creating triplets 〈<em>x<sub>i</sub></em>, <em>t<sub>i</sub></em>, <em>y<sub>i</sub></em>〉. The "thought" <em>t<sub>i</sub></em> describes the intermediate steps and/or results required to derive the output <em>y<sub>i</sub></em> from <em>x<sub>i</sub></em>.</p>
<p>For example, given a question such as <em>John had 6 apples and gave half of them away. How many does he have now?</em>, instead of directly generating the answer (3), CoT first generates a reasoning step, such as <em>John had 6 apples and gave away half. Half of 6 = 6 / 2 = 3</em>. The final answer is then conditioned on this intermediate rationale, and is expected to improve the overall performance of the LLM on the task.</p>
<p><strong>Counterfactual prompting.</strong> The primary objective of our study is to understand CoT through counterfactual prompting. Each counterfactual prompt <em>C(p)</em> alters only one particular aspect of the in-context examples from the original prompt <em>p</em>, while retaining the question asked to the model in its original form. For example, in GSM-8K, a dataset of math word problems (Table 1), we might manipulate the symbol type instantiated in the few-shot examples that appear within a prompt by systematically swapping digits (e.g., 1, 2, 3) for Greek letters (e.g., α, β, γ). This enables us to ask: <em>what would the model's performance have</em></p>
<p>4 MATHEMATICAL $&gt;$ Solve a grade-school level math reasoning problems
Question: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?
Thought: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. $5+4=9$.
Symbols: Numbers: 5, 4, 9
Patterns: Equations: $5+4=9$. The equations typically appear at the end of the thought, and are almost always involved in generating the final answer.
4 COMMONSENSE - (SPORTS) Verify the accuracy of a statement linking an athlete with a sport.
Question: Is the following sentence plausible? "Jamal Murray was perfect from the line."
Thought: Jamal Murray is a basketball player. Being perfect from the line is part of basketball.
Symbols: Person and activity: Jamal Murray, Being perfect from the line
Patterns: Consistent sentence structure PERSON belongs to SPORT. ACTIVITY belongs to SPORT, where belongs to is a phrase that connects a sports personality with an activity. The answer is yes if both the person and the activity are associated with the same sport.
4 COMMONSENSE - (DATE) Reason about dates
Question: It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY?
Thought: Today is 04/19/1969. 24 hours later is one day after today, which would be 04/20/1969. The answer is 04/20/1969. Symbols: Dates: 04/19/1969, 04/20/1969
Patterns: Reasoning flows in two steps: initial calculation (Today is 04/19/1969...), followed by generation of output (The answer is...)
4 SYMBOLIC $&gt;$ (SORTING) Sort integers between 1-9
Question: 3, 1, 2, 7, 8, 5, 6, 9, 4
Thought: $1&lt;2&lt;\ldots&lt;9$
Symbols: Numbers: 2, 4, 9
Patterns: Smaller number &lt; larger number $(1&lt;2)$
Table 1: Symbols and Patterns for different tasks.
been if all the numbers in the prompt were replaced with symbols?. By comparing performance on this new version of the prompt $C_{\text {symb_abs }}(p)$ with that of performance on the original prompt $p$, we can learn about the role which using actual digits plays in task performance ${ }^{2}$.
Symbols and Patterns. In this study, for each dataset, we factor prompts into three distinct components: symbols, patterns, and other surface-level features. For example, in the GSM-8K dataset, symbols are numerical digits (e.g., $5,4,2,13$ ) and patterns (templates) are mathematical equations (e.g., $1+2=3$ ). These definitions are chosen to align with the specific characteristics of each dataset to maximize the potential for exciting analysis. Furthermore, our choice of tasks allows us to experiment with various elements of a prompt, such as exploring cases where tasks require explicit patterns (e.g., equations) and implicit patterns (e.g., sentence structures). A detailed description of these components for each task is provided in Table 7 and the corresponding sections of the paper. We also include all the prompts in the Appendix.
Tasks. We select tasks for our study based on two criteria: (i) Tasks for which CoT presents ample</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>improvements over DIRECT prompting as reported in Wei et al. (2022b), and (ii) Tasks that are varied enough to allow us to analyze different symbols, patterns, and text. Consequently, we pick mathematical tasks (GSM-8K, Cobbe et al. (2021)), commonsense tasks (date and sports understanding, BIG-bench Collaboration (2022)), and symbolic tasks (Sorting) as the tasks for our study. For more details on these tasks and datasets, please see Appendix B and Appendix-Table 7. Table 1 provides examples from each dataset.</p>
<p>Prompts and metrics. We utilize the same prompts as Wei et al. (2022b) as the base prompts, and modify them for all counterfactual experiments (5-8 examples per prompt). All of the tasks we consider have a single correct answer, and are thus readily evaluated with automated metrics. We use solve rate and accuracy interchangeably to refer to performance on the task.</p>
<p>Models. We use LLMs such as PaLM, GPT-3, and CODEX (code-davinci-002), as these models have been shown to perform chain-of-thought reasoning (Wei et al., 2022a) successfully. To provide clear and concise results, we present the results of our experiments using PaLM-62b in the main text. However, to ensure that our findings are not specific to a single model, we also conduct experiments on</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question / Thought</th>
<th style="text-align: center;">Prompt Type</th>
<th style="text-align: center;">Solve Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\triangleleft$ MATHEMATICAL $\triangleright($ DIRECT $=10.11 \%, \operatorname{CoT}=27.37 \%)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Thought: Shawn started with $\alpha$ toys. If he got $\beta$ toys each from his mom and dad, then that is $\lambda$ more toys. $\alpha+\lambda=\pi$. <br> Thought: Shawn started with 5.5 toys. If he got 2.5 toys each from his mom and dad, then that is 5 more toys. $5.5+5=10.5$.</td>
<td style="text-align: center;">$C_{\text {symb_abs }}(p)$ (Table 25)</td>
<td style="text-align: center;">$25.70 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\triangleleft$ COMMONSENSE $\triangleleft$ (SPORTS) (DIRECT $=71.08 \%, \operatorname{CoT}=93.67 \%)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Thought: Jamal Murray is a basketball player. Being ACTIVITY is part of basketball. <br> Thought: Adair Foster is a basketball player. Juggling the paper cups is part of basketball.</td>
<td style="text-align: center;">$\begin{aligned} &amp; C_{\text {symb_abs }}(p) \text { (Table 28) } \ &amp; C_{\text {symb_ood }}(p) \text { (Table 32) } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 92.11 \% \ &amp; 79.72 \% \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">$\triangleleft$ COMMONSENSE $\triangleleft$ (DATE) (DIRECT $=31.61 \%, \operatorname{CoT}=45.18 \%)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Thought: Today is DATE. 24 hours later is one day after today, which would be DATE. <br> Thought: Today is 04/30/3069. 24 hours later is one day after today, which would be 04/31/3069.</td>
<td style="text-align: center;">$\begin{aligned} &amp; C_{\text {symb_abs }}(p) \text { (Table 24) } \ &amp; C_{\text {symb_ood }}(p) \text { (Table 31) } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 37.41 \% \ &amp; 44.50 \% \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">$\triangleleft$ SYMBOLIC $\triangleleft$ (SORTING) (DIRECT $=46.0 \%, \operatorname{CoT}=60.6 \%)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Thought: $\varsigma&lt;\phi&lt;\gamma&lt;\delta&lt;\zeta&lt;\chi&lt;\varepsilon&lt;\pi&lt;v$ <br> Thought: $11&lt;23&lt;34&lt;48&lt;56&lt;63&lt;72&lt;85&lt;95$</td>
<td style="text-align: center;">$\begin{aligned} &amp; C_{\text {symb_abs }}(p) \text { (Table 26) } \ &amp; C_{\text {symb_ood }}(p) \text { (Table 33) } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 61.8 \% \ &amp; 80.0 \% \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Results for counterfactual prompts that replace symbols with abstract symbols or OOD placeholders. DIRECT refers to standard few-shot prompting, without intermediate steps.
a diverse set of publicly available models such as GPT-3 and CODEX. Our results are consistent across all models, as presented in Appendix H.</p>
<h2>3 Role of Symbols</h2>
<h3>3.1 Method</h3>
<p>Symbols refer to token sequences in the prompt which the model employs to reason about the objective of a task and derive the solution. In this work, we adopt natural definitions of symbols for each task. Specifically, we designate numbers in GSM-8K and Sorting, dates in Date (e.g., 01/01/2021), and athletes (e.g., Michael Jordan) and activities (e.g. dunked the ball) in SPORTS as symbols.</p>
<p>To understand how symbols help the model understand a target task, we devised a collection of counterfactual prompts manipulating symbols of various types. This subsection outlines the methods for two main experiments conducted using these counterfactual prompts for symbols: (a) "Abstract" $\rightarrow$ substituting the symbols with abstract values and (b) "Out-of-Distribution" $\rightarrow$ replacing the symbols with the ones that were not encountered during the training phase.
Abstract symbols ${ }<em _symb_abs="{symb_abs" _text="\text">{x}^{\prime} C</em>(p)}<em x="x">{x}^{\prime}$. In this variant, we substitute symbols with an abstract placeholder. For example in SPORTS, we use the placeholder ACTIVITY to represent a sports activity "scoring a three pointer".
OOD symbols ${ }</em>$. These prompts investigate the effects of incorporating out-of-
distribution (OOD) symbols within prompts. Specifically, we examine the consequences of replacing integers with fractions in GSM-8K, substituting sportspersons with random names in SPORTS, and altering dates to dates beyond the year 3000 AD in DATE.}^{\prime} C_{\text {symb_ood }}(p)_{x}^{\prime</p>
<p>In our experiments, we manipulate the symbols only in the few-shot examples that appear within a prompt. The question asked to the model at the end of the prompt remains the unaltered original in all conditions. This approach ensures that the model must generalize from the modified symbol type used in the few-shot examples to the original symbol type used in the question, allowing us to evaluate the model's ability to extrapolate its understanding from manipulated context.</p>
<h3>3.2 Results</h3>
<p>This subsection discusses the results from the experiments described above, as presented in Table 2. The statistical significance test for each counterfactual experiment is detailed in Appendix H.
Abstract symbols ${ }<em _symb_abs="{symb_abs" _text="\text">{x}^{\prime} C</em>$. The results in Table 2 illustrate that substituting symbols with abstract placeholders has little to no impact on model performance. In addition, we observe that in most cases the answers does not contain abstract symbols, however, the exact fraction is dataset dependent. For example, outputs for GSM-8K contain abstract symbols (Greek alphabets) in $12.91 \%$ of the cases, whereas abstract symbols (placeholders for names and activities) are not present in the out-}}(p)_{x}^{\prime</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Average attention per token ( $0^{\text {th }}$ layer, averaged across heads) for the same question using (a) vanilla CoT prompt $p$ and (b) $C_{\text {symb_abs }}(p)$. Both $p$ and $C_{\text {symb_abs }}(p)$ generate the correct answer, relatively attending to same tokens. The phenomenon holds for higher layers (Appendix G).
put for SPORTS. Overall, in cases where the model does generate Greek alphabet symbols, the model regardless reaches the correct answer in $36.5 \%$ of the cases.
OOD symbols $\left[C_{\text {symb_ood }} \mid p\right]$. Table 2 demonstrate that substituting symbols with OOD symbols does not have a significant effect on the model's performance when solving GSM-8K and DATE. However, for SPORTS, using OOD symbols significantly influences the model's task understanding, leading to a noticeable decrease in performance. In the Sorting task, we observed a significant improvement in the task solve rate (from $60.6 \%$ to $80.0 \%$ ) by using larger integers in the prompts. These results underscore the importance of providing the model with clear and diverse understanding of the target task.</p>
<p>However, for SPORTS, using OOD symbols significantly influences the model's task understanding, leading to a noticeable decrease in performance. We change a known player's name, Jamal Murray, to a fabricated name, Adair Foster. We also switch a typical basketball action, Being ACTIVITY, with something unrelated like Juggling the paper cups. These changes make it hard to tell that the task is about figuring out if a sports person and an activity are related, leading to a drop in accuracy (Table 2).
Attention analysis. Beyond corpus-level statistics like accuracy and agreement scores, it is useful to examine whether the model's behavior is consistent for the same input question across different versions of a prompt (instantiated with different symbol types). To understand this, we analyze the attention patterns for randomly sampled questions using both CoT and $C_{\text {symb_abs }}(p)$. The results, presented in Figure 2, show the attention patterns for a random question from GSM-8K using $p$ and $C_{\text {symb_abs }}(p)$. The attention patterns are similar for
both prompt instantiations, indicating a consistent reasoning mechanism. We include details on the attention score calculation, per-layer heatmaps, and the limitations of relying solely on attention analysis for model interpretation in Appendix G.</p>
<h2>4 Role of Patterns</h2>
<p>In this study, we define a pattern as a template that is present in all examples within a given task. The role of patterns is mainly to guide the task towards reaching its objective and hence can manifest differently depending on the task. For example, in GSM-8K patterns are structural, whereas in SPORTS patterns represent specific rules.</p>
<h3>4.1 Method</h3>
<p>We have identified the patterns for each task and they are summarized as follows:</p>
<ul>
<li>GSM-8K $\rightarrow$ Equations (e.g., $2+2=4$ ).</li>
<li>SPORTS $\rightarrow$ The pattern is a consistent thought structure in the following form: "person is a sport ${ }<em 2="2">{1}$ player: activity is part of sport ${ }</em>}$ ". The answer is yes, if sport ${ <em 2="2">{1}$ and sport ${ }</em>$ are the same.</li>
<li>DATE $\rightarrow$ As in SPORTS, the pattern here is the consistent thought structure. Each thought contains two parts: (a) 〈calculation〉 in which the information from input (e.g., question) is restated, and intermediate results are derived (e.g., "One day after 06/01/1943 is 06/02/1943") and (b) 〈output〉 in which the final answer is generated based on the intermediate results (e.g., "10 days before today is 05/23/1943").</li>
<li>Sorting $\rightarrow$ The thought lists numbers in a sorted order (1 less than 2).
We alter different aspects of patterns within the few-shot prompts to study their importance. Similar to our approach with symbols, all these alterations are made strictly within the prompts,</li>
</ul>
<p>while keeping the input questions unaltered (i.e., no changes are made to the task).</p>
<p>Inconsistent pattern. In $C_{\text {pat_inconsistent }}(p)$ we assess the sensitivity of model performance to the usage of inconsistent patterns. For GSM8 K , we construct $C_{\text {pat_inconsistent }}(p)$ by exclusively removing everything except equations. However, in SPORTS, patterns are implicit in the sentence structure (person is a sport ${ }<em 2="2">{1}$ player, activity is part of sport ${ }</em>}{ }^{\prime \prime}$ ), making it challenging to create a $C_{\text {pat_inconsistent }}(p)$ scenario. To overcome this, we devise a prompt that incorporates multiple variations of thought. For example, in some cases, we phrase the thought by listing activity first: " activity is part of sport ${ <em 1="1">{2}$, person is a sport ${ }</em>(p)$ setup. We apply similar techniques to DATE and Sorting.}$ player." This methodology effectively eliminates the model's reliance on specific patterns, essentially creating a virtual equivalent of the $C_{\text {pat_inconsistent }</p>
<p>Pattern-only. In $C_{\text {pat_only }}(p)$ prompts, we modify the thoughts by preserving solely the essential information conveyed by the patterns. For example, in GSM-8K, the pattern-only prompts exclusively contain mathematical equations. In SPORTS, the pattern strives to establish a connection between a person and an activity, based on whether they involve the same sport (in affirmative cases) or different sports (in negative cases). The $C_{\text {pat_only }}(p)$ prompts retain this information by distilling the thought to "both are part of the same/different sport". Similarly, in DATE, we construct thoughts that retain the calculation and answer generation. For example, the statement the date today is 04/19/1969, there are 24 hours in a day is transformed into today $=04 / 19 / 1969,24$ hours $=$ day, where the second expression only provides the answer equation.</p>
<p>Wrong pattern. In $C_{\text {pat_wrong }}(p)$, we examine prompts that include misleading or incorrect information while following the standard pattern. For instance, we use incorrect equations for GSM-8K, erroneous date calculations for DATE, and improper ordering for Sorting. Similarly, for SPORTS, we associate a sportsperson and activity with a randomly chosen sport, instead of the correct one. The goal of this experiment is to evaluate the role factual information in the prompt plays in model's ability to generate correct responses.</p>
<h3>4.2 Results</h3>
<p>Inconsistent pattern. The use of inconsistent patterns in the $C_{\text {pat_inconsistent }}(p)$ method had a noticeable impact on performance. For instance, in mathematical tasks, the solve rate was $21.46 \%$ (Table 3), significantly lower than the $27.37 \%$ achieved by CoT. In SPORTS tasks, the solve rate was $79.01 \%$, as compared to CoT's $93.67 \%$. Despite being able to derive relevant facts such as "Nick Foles is a football player" and "the puck is a part of ice hockey," the model failed to utilize these facts to produce correct answers.
Pattern-only. Results from the $C_{\text {pat_only }}(p)$ method demonstrated that preserving only the patterns in prompts led to a reduced performance. For mathematical tasks, the solve rate was only $10.01 \%$ (Table 3), significantly lower than the $27.37 \%$ solve rate of CoT. Similarly, in SPORTS tasks, the solve rate achieved was $74.13 \%$, as opposed to the $93.67 \%$ solve rate of COT. This underscores the importance of the contextual information that accompanies the patterns for optimal performance.</p>
<p>Wrong pattern. Introducing incorrect patterns in the $C_{\text {pat_wrong }}(p)$ method led to varying impacts on performance depending on the task. In mathematical tasks, $C_{\text {pat_wrong }}(p)$ achieved a solve rate of $24.39 \%$, nearly the same as the $27.37 \%$ solve rate of CoT (Table 3). Likewise, for Date tasks, the solve rate achieved by $C_{\text {pat_wrong }}(p)$ was $44.84 \%$, closely comparable to COT's $45.18 \%$. However, for SPORTS tasks, the solve rate sharply declined to $46.02 \%$, which was considerably lower than the $93.67 \%$ solve rate of COT. These results indicate that incorrect patterns can greatly skew the model's understanding, especially for tasks like SPORTS where correct associations between the subject and activity are crucial.</p>
<h2>5 Additional Surface-level Manipulations</h2>
<p>In addition to symbols and patterns, we delve into surface-level manipulations of text. These manipulations encompass changes to tokens that do not directly contribute to task-specific semantics but may nonetheless impact a language model's understanding and performance. In this section, we scrutinize the effects of these surface-level alterations in our prompts and examine their influence on the outcomes.
Text with altered grammatical style. First, we examine the impact of Yodish, a syntactically valid</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question / Thought</th>
<th style="text-align: center;">Prompt Type</th>
<th style="text-align: center;">Solve Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\triangle$ MATHEMATICAL $\boldsymbol{\sim}(\text { DIRECT }=10.11 \%, \mathrm{CoT}=27.37 \%)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Thought: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys.</td>
<td style="text-align: center;">$C_{\text {pat_inconsistent }}(p)$ (Table 39)</td>
<td style="text-align: center;">$21.46 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Thought: $5+4=9$.</td>
<td style="text-align: center;">$C_{\text {pat_only }}(p)$ (Table 40)</td>
<td style="text-align: center;">$10.01 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Thought: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. $\mathbf{5 + 4 = 7}$.</td>
<td style="text-align: center;">$C_{\text {pat_arong }}(p)$ (Table 37)</td>
<td style="text-align: center;">$24.39 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\triangle$ COMMONSENSE $\boldsymbol{\sim}$ (SPORTS) (DIRECT $=71.08 \%, \mathrm{CoT}=93.67 \%)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Thought: Jamal Murray and being perfect from the line are both part of basketball.</td>
<td style="text-align: center;">$C_{\text {pat_inconsistent }}(p)$ (Table 45)</td>
<td style="text-align: center;">$79.01 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Thought: Both are part of the same sport.</td>
<td style="text-align: center;">$C_{\text {pat_only }}(p)$ (Table 41)</td>
<td style="text-align: center;">$74.13 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Thought: Jamal Murray is a soccer player. Being perfect from the line is part of soccer.</td>
<td style="text-align: center;">$C_{\text {pat_arong }}(p)$ (Table 46)</td>
<td style="text-align: center;">$46.02 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\triangle$ COMMONSENSE $\boldsymbol{\sim}$ (DATE) (DIRECT $=31.61 \%, \mathrm{CoT}=45.18 \%)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Thought: Today is 04/19/1969.</td>
<td style="text-align: center;">$C_{\text {pat_inconsistent }}(p)$ (Table 44)</td>
<td style="text-align: center;">$34.19 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Thought: calculation: Today $=04 / 19 / 1969.24$ hours $=1$ day. output: $04 / 19 / 1969+1=$ $04 / 20 / 1969$.</td>
<td style="text-align: center;">$C_{\text {pat_only }}(p)$ (Table 42)</td>
<td style="text-align: center;">$33.52 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Thought: calculation: Today is 04/19/1969. 24 hours later is one day after today, which output: would be 03/20/1969.</td>
<td style="text-align: center;">$C_{\text {pat_arong }}(p)$ (Table 36)</td>
<td style="text-align: center;">$44.84 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\triangle$ SYMBOLIC $\boldsymbol{\sim}$ (SORTING) (DIRECT $=46.0 \%, \mathrm{CoT}=60.6 \%)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Thought $9&gt;8&gt;7&gt;6&gt;5&gt;4&gt;3&gt;2&gt;1$</td>
<td style="text-align: center;">$C_{\text {pat_inconsistent }}(p)$ (Table 43)</td>
<td style="text-align: center;">$45.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Thought: - (similar to DIRECT)</td>
<td style="text-align: center;">$C_{\text {pat_only }}(p)$</td>
<td style="text-align: center;">$46.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Thought: $1&lt;2&lt;3&lt;4&lt;7&lt;6&lt;5&lt;8&lt;9$</td>
<td style="text-align: center;">$C_{\text {pat_arong }}(p)$ (Table 47)</td>
<td style="text-align: center;">$64.8 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: The accuracy of patterns is not always important, but their absence could be catastrophic. Please note that the $C_{\text {pat_inconsistent }}(p)$ prompts have examples in multiple formats, and we only show one here due to space constraints.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question / Thought</th>
<th style="text-align: center;">Solve Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\triangle$ MATHEMATICAL $\boldsymbol{\sim}(\text { DIRECT }=10.11 \%, \mathrm{CoT}=27.37 \%)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">With 5 toys, Shawn started. 2 toys each from his mom and dad, if he got, then that is 4 more toys. 5</td>
<td style="text-align: center;">$23.22 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$+4=9$.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\triangle$ SPORTS $\boldsymbol{\sim}$ (DIRECT $=71.08 \%, \mathrm{CoT}=93.67 \%)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">A basketball player, Jamal Murray is. Perfect from the line, is part of basketball being.</td>
<td style="text-align: center;">$68.26 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\triangle$ DATE $\boldsymbol{\sim}$ (DIRECT $=31.61 \%, \mathrm{CoT}=45.18 \%)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">04/19/1969, today is. Later is one day after today, 24 hours, 04/20/1969, which would be.</td>
<td style="text-align: center;">$30.75 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4: Modifying the style of text in the prompts to Yodish severely impacts DATE and SPORTS, which rely on the model to generate output in a specific format.
but non-standard style of English, on the model's performance (Kaminski, 2011; Pullum, 2005). In Yodish, the XSV sentence structure is prevalent, where $X$ is a phrase that complements the verb $V$, and $S$ is the subject. For example, the sentence "Bryce Harper is a baseball player" would be rearranged in Yodish as "A baseball player, Bryce Harper is". This style presents a greater challenge for the model, as it is less frequently encountered in typical training data. This makes it a valuable test case for evaluating how textual structure influences model performance. We experiment with three variations of prompts: (a) $C_{\text {text_yodathoughts }}(p)$ : thoughts, (b) $C_{\text {text_yodaquestions }}(p)$ : questions, and
(c) $C_{\text {text_yoda }}(p)$ : both questions and thoughts in Yodish. As shown in Table 4, this style has varying effects on model performance, from moderate (GSM-8K) to significantly negative (SporTS and DATE). For example, in SPORTS, the use of Yodish encourages the model to generate the sport (the object) at the start of the sentence. This structure, while grammatically correct, forces the model to process information in a manner closer to direct prompting, as the model has to output the answer before the reasoning process.</p>
<p>Using CODEX as the base model, we performed additional experiments with other forms of less common patterns in standard English grammar: (a) passive voice and (b) nested clause. Specifically, we modified the prompts to rewrite the original thought (e.g., Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. $5+4=9$.) in passive voice ( 5 toys were initially had by Shawn. 4 more toys were received by him, 2 each from his mom and dad. $5+4$ is 9) and using nested clauses (Given that Shawn had 5 toys, and considering he received 4 more from his parents, the total is 9 toys).</p>
<p>Both these variations led to a drop in performance. For passive voice the solve rates for GSM-8K, SPORTS, and DATE dropped to $53.0 \%$ ( $12.6 \%), 90.3 \%$ (-8.0\%), and $65.9 \%$ (-3.3\%) respec-</p>
<p>tively. For thoughts written with nested clauses, the solve rates decreased to $55.5 \%$ (-10.1\%) for GSM-8к, $90.3 \%$ (-8.0\%) for SPORTS, and $66.4 \%$ $(-2.8 \%)$ for DATE. These results indicate that even within standard English grammar using less common variations cause discernible drops in task solve rates.
Shuffled and random thoughts. Finally, we test the impact of altering the order or context of the text. We experiment with random thoughts ( $C_{\text {text_rand }}(p)$ ), where each thought is replaced by a semantically correct but randomly chosen thought from another example, and shuffled thoughts, where we either shuffle the words within a thought ( $C_{\text {text_inter_shuf }}(p)$ ) or across thoughts ( $C_{\text {text_intra_shuf }}(p)$ ). These experiments were designed to assess the model's dependency on the logical sequence and context of the text. The considerable decrease in performance indicates the model's reliance on coherent and contextually relevant information. The performance decrease was significant across all models, suggesting a universal dependency on coherent and contextually appropriate text among these models.</p>
<h2>6 What makes Chain-of-Thought prompting work?</h2>
<p>In this section, we summarize our findings and present some key takeaways.</p>
<p>Takeaway I. CoT helps in reinforcing task understanding.</p>
<p>Symbols and patterns can be significantly altered as long as they communicate task intent (what has to be done). In some cases (such as Sorting), deviating from standard patterns may be beneficial if they more effectively communicate the task. While few-shot prompting is often called in-context learning, our findings indicate that prompts serve more as a means of reminding the model of the task that needs to be solved.</p>
<p>Takeaway II. CoT helps in eliciting commonsense knowledge.</p>
<p>Examples in a CoT prompt share a key property: they help to fill in the information in the prompt. For instance, in $\boldsymbol{Q} 3$ of Appendix-Table 16, the model with CoT infuses commonsense knowledge</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">PaLM- <br> 62B</th>
<th style="text-align: left;">GPT-3</th>
<th style="text-align: left;">CODEX</th>
<th style="text-align: left;">PaLM- <br> 540B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GSM (Table 73)</td>
<td style="text-align: left;">$+6.2 \%$</td>
<td style="text-align: left;">$+11.7 \%$</td>
<td style="text-align: left;">$-4.7 \%$</td>
<td style="text-align: left;">$+5.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">DATE (Table 74)</td>
<td style="text-align: left;">$+14.8 \%$</td>
<td style="text-align: left;">$+12.8 \%$</td>
<td style="text-align: left;">$+1.1 \%$</td>
<td style="text-align: left;">$+5.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SPORTS <br> ble 75)</td>
<td style="text-align: left;">(Ta-</td>
<td style="text-align: left;">$+16.6 \%$</td>
<td style="text-align: left;">$+0.2 \%$</td>
<td style="text-align: left;">$+2.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SORTING <br> ble 23)</td>
<td style="text-align: left;">(ta-</td>
<td style="text-align: left;">$+9 \%$</td>
<td style="text-align: left;">$+273 \%$</td>
<td style="text-align: left;">$+268 \%$</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparison (realtive performance difference) of COT and CCoT solve rates.
about animals (e.g., "5 dogs have 4 legs each ... 2 cats have 4 legs each ... 10 birds have 2 legs each"). For DATE, the model articulates the exact date for "Christmas Eve" in the generated thought $\left(\boldsymbol{\sim} \mathbf{Q 6} \boldsymbol{\leftarrow} \boldsymbol{\mathcal { T }}_{\mathrm{t}}^{\prime \prime} \mathrm{CoT}\right.$, Appendix-Table 16). Further, examples typically repeat known information (e.g., name of the athlete) before generating new, helpful information (name of the sport) conditioned on known information. We find that hampering this property of the prompt, either by removing patterns (Section 4) or altering text structure (Section 5), hurts performance (Appendix-Table 16).</p>
<p>Takeaway III. COT helps difficult examples and long-tail questions.</p>
<p>We analyze cases where COT is exclusively correct (i.e., COT is correct and DIRECT prompting is wrong). For GSM-8к, we find that the average number of entities in questions solved exclusively by COT is 3.98 compared to the overall average of 3.62 , a statistically significant difference (difference of means t-test $p=0.04$ ). Similarly, in the SPORTS dataset, we find that COT is exclusively correct for rare entities: the average number of Google search results for activities for which COT exclusively yields correct answers is $\approx 52 \times$ lower compared to the ones for which $C_{\text {pat_only }}(p)$ exclusively lands correct answers Appendix-Table 18.</p>
<p>CCOT: Concise Chain Of Thought A central observation of our study is that when few-shot examples effectively convey the task, LLMs can successfully harness them. Moreover, a consistent pattern within the prompts that aligns with the task significantly enhances the model's ability to generate the correct response. To investigate whether a consistent pattern and the ability to fill in missing information are sufficient, we create a concise version of the Chain of Thought (CoT) prompts,</p>
<p>named CCoT, that retain the essential information while removing unnecessary tokens.</p>
<p>For GSM-8к, we randomly select questions from the training set whose thoughts are shorter than CoT. For SPORTS, a thought such as Jamal Murray is a basketball player. Being perfect from the line is part of basketball was streamlined to Ja mal Murray $\rightarrow$ basketball. perfect from the line $\rightarrow$ basketball. Similarly, in Today is 04/19/1969. 24 hours later is one day after today, which would be 04/20/1969 was converted to Today is 04/19/1969. 24 hours (one day) later is 04/20/1969. Table 5 shows that CCoT outperforms CoT while using prompts with fewer tokens. The task solve rate of CCoT remains relatively high as we scale the model to the large version, highlighting the efficiency of CCoT. Additionally, we find that CCoT reduces the input and output tokens by 1.39 and 1.58 times, respectively. We provide additional results and links to each prompt in Table 19.</p>
<h2>7 Related Work and Discussion</h2>
<p>This paper intersects with a growing body of work on prompting and large language model reasoning (Brown et al., 2020; Chowdhery et al., 2022; Scao et al., 2022; Zhang et al., 2022; Dasgupta et al., 2022).
Role of accurate few-shot examples. Min et al. (2022) find that label correctness is not crucial for the success of the models, and even random labels might lead to competitive performance. Building on this work, Kim et al. (2022) find that the role of the correctness of the labels might be taskdependent. A concurrent body of work has also explored the reasons behind the effectiveness of chain-of-thought-prompting and shows that even wrong CoT prompts can lead to strong performance (Ye et al., 2022; Wang et al., 2022). Our findings complement and concur with the findings of these works and is mutually reinforcing, but go beyond the notion of the accuracy of examples. Specifically, we manipulate various aspects of symbols and patterns (with correctness being one of the aspects) to examine their role in the success of CoT. Further, in addition to comparing the final results (outcome), we also focus on the mechanism (attention patterns). While Wang et al. (2022) primarily evaluates the effectiveness of CoT for reasoning and question-answering tasks by introducing drastic changes to prompts to illustrate invalid reasoning, our work adopts a broader approach.</p>
<p>We introduce counterfactual studies encompassing both subtle and significant modifications to COT. Specifically, we assess CoT's effectiveness under two scenarios: (1) where the reasoning flow remains but with incorrect symbols and patterns, and (2) where the reasoning flow is intentionally disrupted, such as through prompt shuffling or the introduction of random texts.</p>
<p>In a related vein, Ye et al. (2022) investigates the effects of incorrect calculations and word omission/masking in CoT. Our work extends this by exploring the influence of out-of-distribution (OOD) symbols, inconsistent patterns, exclusive use of symbols or patterns, and varied grammatical styles. Few-shot learning or few-shot reminding? Our results resonate with the work of Reynolds and McDonell (2021); Ouyang et al. (2022), who found that one of the key roles played by the prompt is to remind the model of the underlying task. Finally, Xie et al. (2021) show that in-context learning enables a large model to infer a shared concept between the examples, possibly leading to better task understanding. Our studies on the role of prompt, especially examples where wrong examples lead to better output (e.g., for Sorting), provide further empirical evidence for these findings. Finally, in concurrence with Razeghi et al. (2022)'s finding that pre-training term frequencies partly account for the success of few-shot methods, our experiments on SPORTS shows that COT method is helps difficult questions involving personalities and activities less commonly found on the web.</p>
<h2>8 Conclusions</h2>
<p>Our study suggests that the underlying mechanisms behind the effectiveness of Chain-of-thought prompting ( COT ) may be more complex than previously thought. We find that even large substitutions, like replacing the digits in few-shot examples with Greek letters, do not affect model performance. However, simple grammar or word-order changes can have catastrophic effects. These results, along with other findings, suggest that the effectiveness of COT may stem from its ability to efficiently convey task understanding (what) to the LLM. Our results indicate that a combination of consistent, easy-to-mimic patterns (templates) and a strong LLM that can fill missing commonsense is the recipe for effective COT. We hope to use this research to develop better prompting techniques and more robust language models for various tasks.</p>
<h2>Limitations</h2>
<p>This work investigates mechanisms that enable the effectiveness of chain of thought techniques in large language models. However, this study does not delve into the underlying interactions between the layers or devise theoretical formulations of the models' reasoning capability, mainly due to the complexity and depth of these models, which hinder faithful probing. Instead, we leverage counterfactual probing, a tractable approach for understanding the behavior of large language models.
Limitations of counterfactual prompting. Counterfactual examples can provide valuable insights into the behavior of language models, as they allow for identifying and collecting prompts that are critical for generating respective outputs. However, it is essential to note that relying solely on counterfactual examples can be misleading (Laugel et al., 2019; Slack et al., 2021). In this work, we focus on counterfactual examples that exhibit consistent and systematic performance divergence to better understand the failure modes and strengths of the model. We also analyze attention patterns to supplement our findings. We neither rely on the results that do not exhibit such characteristics, nor reject prompts that pose contradictory observations. We discuss additional limitations of our approach in Section 8. Spurious Correlations While this approach has its advantages, there are limitations to consider. The counterfactual approach assumes that the model's behavior can be understood by analyzing its output given a specific input. However, there may be uncharted and baffling artifacts that the model could be exploiting (McCoy et al., 2019; Geirhos et al., 2020), leading to potentially misleading observations. For instance, there is a potential for spurious correlations between symbols, patterns, text, and the outcome, which can lead to false conclusions. Our exhaustive empirical study addresses some of these concerns by providing in-depth analysis and methodical measures to ground our hypotheses. Additionally, the discrete and multiplicative nature of language understanding tasks implies that no study can be completely thorough.
Limited Task and Dataset Scope This work is also limited to a subset of common tasks and datasets, including math (Cobbe et al., 2021), commonsense reasoning (BIG-bench Collaboration, 2022), and symbolic reasoning. Our conclusions may not apply to other reasoning tasks. Despite these limitations, we hope that this work sheds light on the
ability of large language models to solve complex reasoning tasks.
Model availability. In our experiments, we use three different language models: PaLM, GPT-3 (text-davinci-002), and Codex (code-davinci002). While PaLM is not publicly available at the time of submission, the provided source code is compatible with OpenAI API v0.23.0 and can work with any OpenAI model. However, using closed models like PaLM may limit our results' reproducibility and hinder our findings' generalizability to other models. Additionally, our results may not be directly comparable to other studies that use different models, as the behavior of models may vary across architectures and training datasets. This limitation should be considered when interpreting the results of our study.</p>
<h2>Acknowledgements</h2>
<p>We thanks the anonymous reviewers for their useful comments and suggestions. We would like to extend our gratitude towards Kathy Meier-Hellstern, Denny Zhou, Victor Veitch, Saleem Abdulrasool, Shruthi Sukumar, Milad Hashemi, Douglas Eck, Christian Szegedy, Cliff Young, Yiming Yang, and James Laudon for their invaluable feedback and support. We also thank the PaLM team and our extended team at Google Research, Brain Team (now Google DeepMind) who enabled this research and helped us conduct our experiments.</p>
<h2>References</h2>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms. In ACL.</p>
<p>BIG-bench Collaboration. 2022. Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models. arXiv preprint arXiv:2206.04615.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In NeurIPS.</p>
<p>Nadia Burkart and Marco F Huber. 2021. A Survey on the Explainability of Supervised Machine Learning. JAIR.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021a. Evaluating Large Language Models Trained on Code. arXiv:2107.03374 [cs]. ArXiv: 2107.03374.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,</p>
<p>William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021b. Evaluating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374.</p>
<p>Hyunyoung Choi and Hal Varian. 2012. Predicting the Present with Google Trends. Economic record, 88:29.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math Word Problems. arXiv preprint arXiv:2110.14168.</p>
<p>Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and psychological measurement.</p>
<p>Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James L McClelland, and Felix Hill. 2022. Language Models Show Human-like Content Effects on Reasoning. arXiv preprint arXiv:2207.07051.</p>
<p>Amir Feder, Katherine A. Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret E. Roberts, Brandon M. Stewart, Victor Veitch, and Diyi Yang. 2021. Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond. arXiv preprint arXiv:2109.00725.</p>
<p>Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. 2020.</p>
<p>Shortcut Learning in Deep Neural Networks. Nature Machine Intelligence, 2(11):665-673.</p>
<p>Jeremy Ginsberg, Matthew H Mohebbi, Rajan S Patel, Lynnette Brammer, Mark S Smolinski, and Larry Brilliant. 2009. Detecting Influenza Epidemics using Search Engine Query Data. Nature, 457(7232):10121014.</p>
<p>Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Counterfactual Visual Explanations. In ICML.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The Curious Case of Neural Text Degeneration. In ICLR.</p>
<p>Alon Jacovi and Yoav Goldberg. 2020. Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness? In ACL.</p>
<p>Sarthak Jain and Byron C. Wallace. 2019. Attention is not Explanation. In NAACL.</p>
<p>Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho, Thomas B. Jablin, George Kurian, James Laudon, Sheng Li, Peter Ma, Xiaoyu Ma, Thomas Norrie, Nishant Patil, Sushma Prasad, Cliff Young, Zongwei Zhou, and David Patterson. 2021. Ten Lessons from Three Generations Shaped Google's TPUv4i: Industrial Product. In ISCA.</p>
<p>Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. 2017. In-Datacenter Performance Analysis of a Tensor Processing Unit. In ISCA.</p>
<p>Michael Kaminski. 2011. Yoda-Speak: A Study of Yoda's Speaking Pattern and Their Frequencies. The Secret History of Star Wars.</p>
<p>Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna M. Wallach, and Jennifer Wortman</p>
<p>Vaughan. 2020. Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning. In CHI.</p>
<p>Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, Kang Min Yoo, and Taeuk Kim. 2022. GroundTruth Labels Matter: A Deeper Look into Input-Label Demonstrations. arXiv preprint arXiv:2205.12685.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large Language Models are Zero-Shot Reasoners. arXiv preprint arXiv:2205.11916.</p>
<p>Taku Kudo and John Richardson. 2018. SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing. In EMNLP-Demo Track.</p>
<p>Thibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, and Marcin Detyniecki. 2019. The Dangers of Post-hoc Interpretability: Unjustified Counterfactual Explanations. In IJCAI.</p>
<p>Teven Le Scao and Alexander M Rush. 2021. How Many Data Points is a Prompt Worth? In NAACL.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems. arXiv preprint arXiv:1705.04146.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021a. What Makes Good In-Context Examples for GPT-3? arXiv:2101.06804 [cs]. ArXiv: 2101.06804.</p>
<p>Pengfei Liu, Jinlan Fu, Yang Xiao, Weizhe Yuan, Shuaichen Chang, Junqi Dai, Yixin Liu, Zihuiwen Ye, and Graham Neubig. 2021b. ExplainaBoard: An Explainable Leaderboard for NLP. In IJCNLP.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021c. Pretrain, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. arXiv preprint arXiv:2107.13586.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically Ordered Prompts and Where to Find Them: Overcoming FewShot Prompt Order Sensitivity. In ACL.</p>
<p>Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In NeurIPS.</p>
<p>R Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. arXiv preprint arXiv:1902.01007.</p>
<p>Quinn McNemar. 1947. Note on the Sampling Error of the Difference between Correlated Proportions or Percentages. Psychometrika.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? arXiv preprint arXiv:2202.12837.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. 2021. Reframing Instructional Prompts to GPTk's Language. arXiv preprint arXiv:2109.07830.</p>
<p>Ramaravind K Mothilal, Amit Sharma, and Chenhao Tan. 2020. Explaining Machine Learning Classifiers Through Diverse Counterfactual Explanations. In FAT*.</p>
<p>Guoshun Nan, Jiaqi Zeng, Rui Qiao, Zhijiang Guo, and Wei Lu. 2021. Uncovering Main Causalities for Long-tailed Information Extraction. In EMNLP.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show your Work: Scratchpads for Intermediate Computation with Language Models. arXiv preprint arXiv:2112.00114.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training Language Models to Follow Instructions with Human Feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP Models Really Able to Solve Simple Math Word Problems? arXiv preprint arXiv:2103.07191.</p>
<p>Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. 2021. Synchromesh: Reliable Code Generation from Pre-trained Language Models. In ICLR.</p>
<p>Rafael Poyiadzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and Peter Flach. 2020. FACE: Feasible and Actionable Counterfactual Explanations. In AAAI.</p>
<p>Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, and Zachary C. Lipton. 2020. Learning to Deceive with Attention-Based Explanations. In ACL.</p>
<p>Geoffrey K. Pullum. 2005. YODA'S Syntax the Tribune Analyzes; Supply more Details I Will! YODA'S Syntax the Tribune Analyzes; Supply more Details I Will! Accessed: 2022-08-15.</p>
<p>Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of Pretraining Term Frequencies on Few-shot Reasoning. arXiv preprint arXiv:2202.07206.</p>
<p>Laria Reynolds and Kyle McDonell. 2021. Prompt Programming for Large Language Models: Beyond the Few-shot Paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems.</p>
<p>Marco Túlio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why Should I Trust You?": Explaining the Predictions of Any Classifier. In SIGKDD.</p>
<p>Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2021. Learning to Retrieve Prompts for In-context Learning. arXiv preprint arXiv:2112.08633.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176B-Parameter Open-Access Multilingual Language Model. arXiv preprint arXiv:2211.05100.</p>
<p>Dylan Slack, Anna Hilgard, Himabindu Lakkaraju, and Sameer Singh. 2021. Counterfactual Explanations can be Manipulated. NeurIPS.</p>
<p>Ilia Stepin, Jose M Alonso, Alejandro Catala, and Martín Pereira-Fariña. 2021. A Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Artificial Intelligence. IEEE Access.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In NeurIPS.</p>
<p>Sahil Verma, John Dickerson, and Keegan Hines. 2020. Counterfactual Explanations for Machine Learning: A Review. arXiv preprint arXiv:2010.10596.</p>
<p>Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. 2022. Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters. arXiv preprint arXiv:2212.10001.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent Abilities of Large Language Models. arXiv preprint arXiv:2206.07682.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b. Chain of Thought Prompting Elicits Reasoning in Large Language Models. arXiv preprint arXiv:2201.11903.</p>
<p>Sarah Wiegreffe and Yuval Pinter. 2019. Attention is not not Explanation. In EMNLP-IJCNLP.</p>
<p>Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An Explanation of In-context Learning as Implicit Bayesian Inference. In ICLR.</p>
<p>Xi Ye, Srinivasan Iyer, Asli Celikyilmaz, Ves Stoyanov, Greg Durrett, and Ramakanth Pasunuru. 2022. Complementary Explanations for Effective In-Context Learning. arXiv preprint arXiv:2211.13892.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open Pre-trained Transformer Language Models. arXiv preprint arXiv:2205.01068.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. arXiv preprint arXiv:2205.10625.</p>
<h2>Part I</h2>
<h2>Appendix</h2>
<h2>A Reproducing the Results with Publicly Available Models</h2>
<p>We take the following steps to enable the reproducibility of our work.
Controlling for randomness due to the order of examples. We run each experiment with multiple random seeds to control for randomness because of the order of examples in the prompt. We report the average and standard deviation of the results across all the random seeds. Additionally, we conduct statistical significance tests (McNemar's test (McNemar, 1947)) to compare the results across different prompts. Finally, we evaluate the agreement in output generated by different models using Cohen's kappa ( $\kappa$ ) metric.
Experiments with publicly available models. We experiment with three different language models: PaLM, GPT-3 (text-davinci-002), and CODEX (code-davinci-002). PaLM is not publicly available as of submission time, but the provided source code is compatible with OpenAI API v0.23.0, and can work with any OpenAI models. Finally, CODEX is free to use as of submission time that further helps with the reproducibility of the results.</p>
<p>All the prompts are included in the prompt_lib/prompts/ directory in the code repository.</p>
<h2>B Details on Studied Reasoning Tasks</h2>
<p>In this work, we evaluate counterfactual prompting on the following reasoning tasks:</p>
<ol>
<li>Mathematical We experiment with GSM-8K (Cobbe et al., 2021) (1319 samples). The dataset contains math word problems geared toward an average middle-school curriculum.</li>
<li>$\triangle$ COMMONSENSE We use date understanding (DATE, 349 samples) and sports understanding (Sports, 980 samples) as representative tasks for commonsense reasoning, both derived from BIG-bench Collaboration (2022).</li>
<li>Symbolic We experiment with sorting (Sorting, 500 samples) a list of singledigit integers. We do not associate explicit instruction (e.g., sort these numbers) with the questions. Instead, we frame the questions as a challenging setup in which the model should
figure out the task and the requisite information to solve it.</li>
</ol>
<h2>C Computational Resources and Models</h2>
<p>In this work, we neither train any of the PaLM models, nor performs finetuning. We solely perform inference on PaLM variants using TPU v4 (Jouppi et al., 2021, 2017). For PaLM-62B, we use $4 \times 4 \times 4$ TPU v4 configuration, whereas, for PaLM-540B we use $4 \times 4 \times 16$ mesh configuration. To account for the variation in results caused by the order of examples in the prompt, we conduct each experiment three times, each with different seeds, and report the average task solve rate. Following Wei et al. (2022b), we evaluate each task using accuracy i.e. fraction of examples where the output matched the expected result.
Public large language models. We use OpenAI $\mathrm{API}^{3}$ to conduct experiments with GPT-3 (text-davinci-002) and CODEX (code-davinci-002).</p>
<h2>D FAQ</h2>
<p>Q: Are the definitions of symbols and patterns universal? A: It is possible that there can be other ways to define symbols, patterns, and text in the context of Chain-of-thought prompting (CoTp). Our characterization of these components is not meant to be universal, and there may be additional properties and perspectives that future research can explore. However, the goal of our study is not to provide a universal definition but to make practical and reasonable distinctions that allow us to manipulate each component individually while keeping the others fixed. This approach allows us to better understand the underlying structure of prompts in different contexts. We acknowledge that the impact of symbols and patterns on CoTp success is taskdependent, and our results provide a diverse set of insights. Finally, while there is not a universal definition of symbols and patterns, for any given task we have aimed to provide a clear and reasonable characterization.</p>
<p>Q: What is the TLDR? A: The effectiveness of COT in few-shot learning with large language models is due to two factors: it helps reinforce task understanding and fills in the missing information. The traditional notion of in-context learning may need to be reevaluated as the model may be using the few-shot examples to be reminded of the task (Reynolds and McDonell, 2021).</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 6: Examples of "what if" questions that we seek to answer in this work.
Q1. What if we replace all the symbols in the prompt with abstract placeholders, can the required task still be discerned?
Q2. What if the examples in the prompt were incorrect, will it affect the correctness of the outputs?
Q3. What if we remove all patterns from the input, will CoT continue to be effective?
Q4. What if the linguistic style of the prompt was different than that of the questions, will it hamper the performance?</p>
<p>Table 7: Examples of tasks used in this work. The $\Rightarrow \mathcal{Q} \mapsto$ question, $\leftrightarrow \mathcal{T} \mapsto$ thought, and $\triangleleft \mathcal{A} \mapsto$ answer are separately highlighted. For the complete list of vanilla CoT for each category refer to Table 20, Table 22, Table 21, and Table 23, respectively.</p>
<h1>4 MATHEMATICAL $\square$</h1>
<p>$\Rightarrow \mathcal{Q} \mapsto$ Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?
$\bullet \mathcal{T} \mapsto$ Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. $5+4=9$.
$\triangle \rightarrow \rightarrow$ The answer is 9 .
4 COMMONSENSE $\rightarrow$ (SPORTS)
$\Rightarrow \mathcal{Q} \mapsto$ Is the following sentence plausible? "Jamal Murray was perfect from the line."
$\bullet \mathcal{T} \mapsto$ Jamal Murray is a basketball player. Being perfect from the line is part of basketball.
$\triangle \rightarrow \rightarrow$ The answer is yes.
4 COMMONSENSE $\rightarrow$ (DATE)
$\Rightarrow \mathcal{Q} \mapsto$ It is $4 / 19 / 1969$ today. What is the date 24 hours later in MM/DD/YYYY?
$\bullet \mathcal{T} \mapsto$ Today is $04 / 19 / 1969.24$ hours later is one day after today, which would be $04 / 20 / 1969$.
$\triangle \rightarrow \rightarrow$ The answer is $04 / 20 / 1969$.
4 SYMBOLIC $\rightarrow$ (SORTING)
$\Rightarrow \mathcal{Q} \mapsto 2,4,3,8,9,6,7,1$.
$\bullet \mathcal{T} \mapsto 1&lt;2&lt;3&lt;4&lt;5&lt;6&lt;7&lt;8&lt;9$.
$\triangle \rightarrow \rightarrow$ The answer is $1,2,3,4,5,6,7,8,9$</p>
<p>Q: What are some key findings? How can they help understand few-shot prompting better? A: We summarize key takeaways in Section 6. One of the main conclusions is the shift in perspective on how prompts should be viewed - rather than being used as a method of teaching the model, they should be seen as a way to remind the model of the task at hand. A helpful analogy is that of search engines; just as query expansion is a useful tool for retrieving relevant results, CoT is helpful in extracting meaningful answers from a model.</p>
<p>Q: Why the long appendix? A: Our study includes experimentation with over 20 counterfactual prompts for 4 datasets and 3 models. Due to space limitations and for clarity purposes, only a subset of the results is included in the main paper. The Appendix includes all the results and additional insights, such as additional attention analysis. Every task reveals unique insights about CoT and prompting. However, the empirical results alone may not convey all necessary information, so a comprehensive qualitative analysis is included in the Appendix. Additionally, the prompts take up a significant portion of space, which are also in-
cluded in the Appendix.
Q: Where are additional details on Attention patterns? A: Appendix G shows additional details, including details on the attention score calculation. Note that the attention analysis was only possible for PaLM, because that was the only model we had weight-level access to.</p>
<h2>E Extended Background</h2>
<p>Chain of thought prompting. This work broadly investigates the premise of in-context few-shot prompting in large language models (LLM). In these methods, the input to the model is a prompt $p$ consisting of $k$ in-context examples in the form of $\langle$ input $\mapsto x_{i}$, output $\mapsto y_{i}\rangle$ tuples ${ }^{4}$. Each $\left\langle x_{i}\right.$, $y_{i}\rangle$ alludes to the target task. For example, in math solving problems (Cobbe et al., 2021), an input is math question (If three apples were added to a basket which had two apples, how many apples are in the basket now?), and the output supplies the answer (5). Wei et al. (2022b) additionally sup-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>plement in-context few-shot prompting with Chain Of Thought (CoT) method, improving the performance of LLM in solving several reasoning tasks. In particular, COT additionally prefixes each output with a thought, creating triplets $\left\langle x_{i}, t_{i}, y_{i}\right\rangle$. The "chain of thought" $t_{i}$ describes the intermediate steps and/or results required to derive the output $y_{i}$ from $x_{i}$. Therefore, the prompt is assembled in the form of $p \equiv\left\langle x_{1} \cdot t_{1} \cdot y_{1}\right\rangle\left|\left\langle x_{2} \cdot t_{2} \cdot y_{2}\right\rangle\right| \ldots\left|\left\langle x_{k} \cdot t_{k} \cdot y_{k}\right\rangle\right.$, where "." and " $|$ " are indicator symbols. The role of $\cdot$ is to separate elements of an example, whereas | indicates the boundary of an example. The intuition behind chain of thought prompting is that catering the outputs/answers with intermediate steps/results present additional in-context information to the model (Ling et al., 2017; Amini et al., 2019; Chen et al., 2021a; Cobbe et al., 2021; Nye et al., 2021). This additional in-context information presumably improves accuracy in solving various reasoning tasks.</p>
<p>At inference time, COT appends an unseen question $\hat{x}$ to the prompt $p$ and supplies the extended prompt to a LLM. The model completes the prompt to generate a relevant thought $\hat{t}$ and an answer $\hat{y}$. To assess the performance of LLM, CoT only compares the post-processed generated answer with the ground truth. Gauging the correctness of the generated thought $\hat{t}$ is not straightforward because ground truth thoughts are unavailable. Nonetheless, the generated thought can be further analyzed to infer the possible mechanisms, allowing an analogy with the human thought process, with which the model attains the answer.</p>
<p>Counterfactual explanation. Counterfactual explanations seek to explain the behavior of a model by conducting "what if" analysis on examples for which the expected outputs of the model is known (Mothilal et al., 2020; Stepin et al., 2021; Verma et al., 2020; Poyiadzi et al., 2020; Goyal et al., 2019; Feder et al., 2021). Specifically, let $(x, y)$ be a tuple where $x$ is the input to a model $\mathcal{M}$ that estimates an output distribution $p(\cdot \mid x)$, and $y \sim p(\cdot \mid x)$. Counterfactual explanations utilize variants $C_{f}(x, b, a)$ of the inputs that differ from the original input $x$ in all except one feature $f$. Here, $b$ and $a$ denote the before and after values of the feature $f$ in $x$. For instance, consider an image $x$ of a camel with a brown background labeled correctly by a classifier. A counterfactual $C_{b q}(x$, brown, green $)$ example is an identical image with only a different background color, green,
in this example. By virtue of comparing $p(\cdot \mid x)$ with $p\left(\cdot \mid C_{b q}(x\right.$, brown, green)) for a sufficiently large sample of images, one may infer certain facts about the classifier, for example its reliance on the background color.</p>
<h2>F Extended Related Work and Discussion</h2>
<p>Broadly, this paper intersects with a growing body of work on prompting and large language model reasoning (Brown et al., 2020; Chowdhery et al., 2022; Scao et al., 2022; Zhang et al., 2022). Below, we review the most relevant work in these directions.
Least to most prompting. Zhou et al. (2022) help the model generate a chain of thought by first asking the model to generate the sub-questions for the given problem. Next, the model is asked to answer the sub-questions, and finally, the sub-questions, along with sub-answers, are combined to generate the final result. This work is closely related to Kojima et al. (2022), the latter distinguished by generating the rationale from a large language model directly. We posit that Zhou et al. (2022) derives its key strengths from its ability to generate useful sub-steps. This resonates with our finding that the key contribution of CoT is the extraction of meaningful sub-steps.
Prompt selection. Several works have recently explored the design of the prompt-a process often called "prompt engineering" (Le Scao and Rush, 2021; Liu et al., 2021c). The methods include dynamically creating prompts based on the question (Liu et al., 2021a; Rubin et al., 2021; Poesia et al., 2021), formatting the prompt as a list or questions (Mishra et al., 2021; Rubin et al., 2021), improving order of examples in the prompt ( Lu et al., 2022), and providing instructions in the task (Ouyang et al., 2022). Unlike these techniques, COT is relatively robust to minor changes in the prompt design. Thus, the findings of our work might be more generally applicable.
Explaining model behavior using counterfactual prompts and attention. As noted by (Jacovi and Goldberg, 2020), an explanation of a deep learning system typically serves two different purposes: i) plausibility, which aims to provide an interpretation of system outputs that is convincing for humans, and ii) faithfulness, which aims to capture the actual reasoning process of a model. Our study requires both and uses different means to achieve them. We utilize counterfactual prompts</p>
<p>to interpret the system outputs to aid human understanding. This is similar to using posthoc analysis tools (Ribeiro et al., 2016; Lundberg and Lee, 2017; Liu et al., 2021b), which also focus on analyzing outputs without concern for the details of the model. To get a glimpse of the model's inner workings, we leverage attention (Vaswani et al., 2017), a ubiquitous mechanism in NLP. While the broader question on the utility of attention for posthoc analysis is still open (Jain and Wallace, 2019; Pruthi et al., 2020), there is some evidence to show that attention can act as an explanation (Wiegreffe and Pinter, 2019). Finally, the utility of any explanation mechanism is closely tied to the users and application domain (Kaur et al., 2020; Burkart and Huber, 2021). As our analysis shows, attention adds intuition and insights to the empirical findings.</p>
<p>Counterfactual explanations seek to explain the behavior of a model by performing a what if analysis on examples (Mothilal et al., 2020; Stepin et al., 2021; Verma et al., 2020; Poyiadzi et al., 2020; Goyal et al., 2019). While counterfactuals can be misleading due to artifacts (e.g., see (Laugel et al., 2019; Slack et al., 2021)), they offer a tractable solution for probing large models like PaLM and GPT-3. Notably, unlike fine-tuned methods, the most important examples for generating the model output are readily available. Thus, counterfactual inputs that show a consistent and systematic change in the model performance are more likely to reflect the model's behavior.</p>
<h2>G Attention Analysis</h2>
<p>While attention mechanisms have proven invaluable for enhancing the performance of deep neural networks, they should be used with caution when interpreting how a model works. The interpretations derived from attention weights are, at best, approximate indicators of the model's decision process and should not be over-interpreted as a precise description of the underlying mechanisms.</p>
<p>The broader question on the utility of attention for posthoc analysis is still open (Jain and Wallace, 2019; Pruthi et al., 2020), with some evidence to show that attention can act as an explanation (Wiegreffe and Pinter, 2019). Finally, the utility of any explanation mechanism is closely tied to the users and application domain (Kaur et al., 2020; Burkart and Huber, 2021). Our analysis shows that attention provides concurring evidence that adds intuition and insights to the empirical findings of this
work. Note that while we conduct empirical experiments with PaLM, GPT-3, and CODEX, we only conduct attention-related ablations with PaLM as the GPT-3 and CODEX were only available to us via API.
Attention for autoregressive models. Consider a sentence: my dog loved the toy. Modern NLP methods divide each sentence into tokens, a decision dictated by the underlying tokenization library. PaLM uses SentencePiece (Kudo and Richardson, 2018) for tokenization. For simplicity, we assume a tokenizer that divides the sentence into tokens based on the whitespace. This yields the following list of tokens: [my, dog, loves, treats].</p>
<p>Let BOS be a special beginning of sequence token present in all sentences, and $p_{\theta}$ be a language model with the parameters $\theta$. Decoder-only language models such as PaLM estimate the likelihood of a sequence such as my dog loved the toy using an autoregressive factorization or the chainrule:</p>
<p>$$
\begin{aligned}
&amp; p_{\theta}(\text { BOS }, m y, \text { dog, loves }, \text { treats })= \
&amp; p_{\theta}(m y \mid \text { BOS }) \
&amp; * p_{\theta}(\text { dog } \mid \text { BOS, } m y) \
&amp; * p_{\theta}(\text { loves } \mid \text { BOS }, m y, \text { dog }) \
&amp; * p_{\theta}(\text { treats } \mid \text { BOS }, m y, \text { dog, loves })
\end{aligned}
$$</p>
<p>Estimating these conditional probabilities (e.g., $p_{\theta}(m y \mid$ BOS $)$ ) requires a stack of transformer layers, each containing an attention module. Thus, this factorization also implies that tokens attend to the left (Figure 4), with a token $w_{i}$ at location $i$ attending to all tokens $w_{&lt;i}$.</p>
<p>Let $w_{s}$ be the source token (current input to the model). The set of target tokens, or tokens that $w_{s}$ will attend to, thus are: $w_{0}, w_{1}, \ldots, w_{s-1}$. PaLM-62B has 64 layers, each containing the selfattention mechanism with 32 heads. Focusing on a single layer and head, let $a_{s t}$ be the attention score from $w_{s}$ to $w_{t}$, where $\sum_{t=0}^{s-1} a_{s t}=1$.
Analyzing the important components of a COT prompt. We leverage attention scores as an additional signal to help uncover the important components of a prompt. To this end, we calculate the attention scores from the source tokens that are part of the $\mathbf{Q}^{\prime}, \mathbf{T}^{\prime}$, or $\mathbf{A}^{\prime}$ to the target prompt question $\mathbf{Q}<em _mathbf_i="\mathbf{i">{\mathbf{i}}$, thought $\mathbf{T}</em>$ (Figure 3). Note that the same prompt is used for all the questions in the test set. Thus across questions, the set of target tokens remains the same.}}$, and answer $\mathbf{A}_{\mathbf{i}</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Structure of a typical chain of thought prompt. The prompt contains a handful of QTA examples, each containing three parts: 1) The question (Q), the thought (T) that spells out the reasoning process to derive the answer, and finally, 3) Answer (A) the final answer. In the Figure, the prompt contains two such QTA examples. During inference, a test question $\mathbf{Q}^{\prime}$ is appended to the prompt, and the model is expected to complete it by generating a thought $\mathbf{T}^{\prime}$ and the answer $\mathbf{A}^{\prime}$, presumably leveraging the two QTA examples in the input.</p>
<p>Our goal in attention analysis is to uncover important tokens and spans used by PaLM to solve a task. Since the distribution of attention scores $a_{s}$ is typically long-tailed, recording the attention score between every pair of source-target tokens might lead to noise and spurious patterns (Nan et al., 2021). To remedy this, we take inspiration from nucleus sampling (Holtzman et al., 2019) and set all values below the $k^{t h}$ largest attention value to 0 (we use $k=10$ ).</p>
<p>Let $Q_{j}^{\prime}$ be the $j^{t h}$ question in the test set $\mathcal{Q}$ of the questions to be evaluated. Recall that the same prompt is used for all the questions, and we calculate the attention scores from the source tokens (tokens in the inference question) to the target tokens (those in the prompt). Let $a_{s t}$ be the attention from token $w_{s}$ to $w_{t}$.</p>
<p>We calculate the attention importance $\mathbf{I}<em t="t">{t}$ of a token $w</em>$ of inference questions.}$ in the prompt as the average max attention it has received across the set $\mathcal{Q</p>
<p>$$
\mathbf{I}<em j="1">{t}=\frac{\sum</em> \max }^{|\mathcal{Q}|<em j="j">{s=1}^{\left|Q</em>
$$}^{\prime}\right|+\left|A_{j}^{\prime}\right|+\left|T_{j}^{\prime}\right|} a_{s t}}{|\mathcal{Q}|</p>
<ol>
<li>The spectrum plots show a comparison of $\mathbf{I}<em _symb_abs="{symb_abs" _text="\text">{t}$ for all tokens in the prompt for two different prompts: vanilla COT prompt and $C</em>(p)$ prompt.}</li>
<li>The pattern vs. text prompts group the target tokens by their type: the tokens that belong to a pattern vs. tokens belonging to the text. The attention importance values are then shown.</li>
<li>The bos by layer plots investigate the total attention importance for the bos token.</li>
</ol>
<h2>G. 1 Per-layer Attention Analysis</h2>
<p>The main draft provides spectrum plots averaged over heads and layers. Figure 11 shows the same question for three different datasets averaged across layers. Figure 5, Figure 6, and Figure 7 provide
the same plots, per layer. We find that the spectrum of $I_{s}$ values is identical between $\operatorname{CoT}(\mathrm{p})$ and $C_{\text {symb_abs }}(p)$ across layers, showing that averaging is not leading to spurious correlations.</p>
<h2>G. 2 Specialized Attention Heads</h2>
<p>Fine-tuned models can be expected to learn attention patterns that facilitate solving a task. Does the same hold for few-shot models? To our knowledge, the question of attention in a few-shot setup has not been explored. Surprisingly, we find that the model consistently uses certain heads and layers for attending over certain semantic parts of the inputs. We find such specialized head-layer pairs manually, and plot the average $I_{t}$ for 100 questions for them in Figure 9. The $I_{t}$ values show a clear tendency for the head to favor either past tense (would, yesterday) or future tense (will). Analyzing a largelanguage model's attention patterns in detail is an interesting future work.</p>
<p>Symbiosis in attention scores We have explored different semantic components of prompts, namely patterns (including symbols) and text. A logical next question is whether patterns or text confer differential importance. While importance can be measured via various approaches, we use attention scores as a reasonable proxy. For GSM-8k (where the distinction between patterns and text is clear), we calculate attention mass on patterns and text across several layers and average it over their attention heads. Figure 11 compares these average scores, normalized between patterns and text. Our findings show that the model pays approximately equal attention to both, indicating similar importance. These results concur with our findings that text and patterns contribute equally to the success of COT.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Auto-regressive language models: the tokens are generated as a sequence, with each token attending to the preceding tokens.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Average attention per token for a randomly sampled question using standard CoT prompt $p$ (above) and $C_{\text {sypth_abs }}(p)$ for GSM-8K across layers. Near identical attention pattern shows that few-shot models are relatively indifferent to the exact symbols, but are sensitive to patterns.</p>
<h2>H Results on CODEX, GPT-3, PaLM-540B and Statistical Significance Test</h2>
<p>We show results from four models: CODEX (Chen et al., 2021b), GPT-3 (Brown et al., 2020), and two variants of PaLM (Chowdhery et al., 2022) (PaLM62B and PaLM-540B). Note that we could not get results on all variations of prompts for GPT-3 because of usage limits by OpenAI. Such cases are indicated with a hyphen (-). Similarly, due to the rate limitations, we experimented with two seeds for all variations on CODEX and had to use a single seed for some variations. The findings are shown in Table 8 (GSM-8K), Table 9 (DATE), Table 10 (SPORTS), and Table 11 (SORTING). We find that all the findings hold across models: correctness of patterns is immaterial, abstract and OOD symbols are still helpful, and the sensitivity to text is proportional to the degree of randomness. Finally, CCoT matches or outperforms CoT despite being $20 \%$ shorter.</p>
<h2>H. 1 Significance tests for PaLM-62B</h2>
<p>In this section, we present detailed results for experiments on PaLM-62B. Each experiment was repeated thrice using three different values of the random seed. We use McNemar's test (McNemar, 1947) to calculate the statistical significance of differences in the performance of a given Counterfactual prompt with $\operatorname{CoT}(\mathrm{p})$, and Cohen's kappa (Cohen, 1960) to measure the degree of agreement between the outputs generated by a counterfactual prompt and $\operatorname{CoT}(\mathrm{p})$.</p>
<h2>I Additional Experiments</h2>
<h2>I. 1 Constructing Effective Intermediate Thoughts</h2>
<p>Heeding our preceding findings, this section underscores few concrete venues in which the symbiosis of patterns and text contribute to the construction of effective thoughts, consequently leading to the success of COT. To enable a systematic analysis, we first identify samples in which $\operatorname{CoT}(\mathrm{p})$ yields</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Average attention per token for a randomly sampled question using standard CoT prompt $p$ (above) and $C_{\text {symb_abs }}(p)$ for DATE across layers. Near identical attention pattern shows that few-shot models are relatively indifferent to the exact symbols, but are sensitive to patterns.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Average attention per token for a randomly sampled question using standard CoT prompt $p$ (above) and $C_{\text {symb_abs }}(p)$ for SPORTS across layers. Near identical attention pattern shows that few-shot models are relatively indifferent to the exact symbols, but are sensitive to patterns.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">CODEX</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT-3</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PaLM-62B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PaLM-540B</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">SD</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">SD</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">SD</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">SD</td>
</tr>
<tr>
<td style="text-align: center;">DIRECT</td>
<td style="text-align: center;">20.8\%</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">16.1\%</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">10.1\%</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">9.6\%</td>
<td style="text-align: center;">3.2</td>
</tr>
<tr>
<td style="text-align: center;">COT(p) (Table 20)</td>
<td style="text-align: center;">65.6\%</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">46.9\%</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">27.4\%</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">60.8\%</td>
<td style="text-align: center;">0.6</td>
</tr>
<tr>
<td style="text-align: center;">COT</td>
<td style="text-align: center;">62.5\%</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">52.2\%</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">29.1\%</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">63.2\%</td>
<td style="text-align: center;">1.1</td>
</tr>
<tr>
<td style="text-align: center;">$C_{\text {symb_ood }}(p)$ (Table 30)</td>
<td style="text-align: center;">66.2\%</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">55.3\%</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">25.7\%</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">60.7\%</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr>
<td style="text-align: center;">$C_{\text {symb_abs }}(p)$ (Table 25)</td>
<td style="text-align: center;">56.5\%</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">49.4\%</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">28.2\%</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">59.0\%</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: center;">$C_{\text {pat wrong }}(p)$ (Table 37)</td>
<td style="text-align: center;">65.5\%</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">52.4\%</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">24.4\%</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">63.5\%</td>
<td style="text-align: center;">0.7</td>
</tr>
<tr>
<td style="text-align: center;">$C_{\text {pat_inconsistent }}(p)$ (Table 39)</td>
<td style="text-align: center;">33.3\%</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">37.8\%</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">21.5\%</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">53.9\%</td>
<td style="text-align: center;">1.5</td>
</tr>
<tr>
<td style="text-align: center;">$C_{\text {text_indelthoughts }}(p)$ (Table 52)</td>
<td style="text-align: center;">60.8\%</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">23.2\%</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">57.4\%</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: center;">$C_{\text {text_intra_shut }}(p)$ (Table 61)</td>
<td style="text-align: center;">33.0\%</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.0\%</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">45.6\%</td>
<td style="text-align: center;">3.8</td>
</tr>
<tr>
<td style="text-align: center;">$C_{\text {text_inter_shut }}(p)$ (Table 64)</td>
<td style="text-align: center;">29.7\%</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">10.8\%</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">37.2\%</td>
<td style="text-align: center;">3.1</td>
</tr>
<tr>
<td style="text-align: center;">$C_{\text {text_dirf_entities }}(p)$ (Table 50)</td>
<td style="text-align: center;">59.0\%</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">49.8\%</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">16.6\%</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">51.1\%</td>
<td style="text-align: center;">3.3</td>
</tr>
</tbody>
</table>
<p>Table 8: All results for GSM-8K across four models: CODEX, GPT-3, PaLM-62B, and PaLM-540B.
correct answer, whereas both $C_{\text {pat_inconsistent }}(p)$ and $C_{\text {pat_only }}(p)$ are wrong. Analyzing these samples assist us in identifying probable systematic differences across these methods.</p>
<p>CoT is more effective in solving questions with
more patterns. In general, questions with more patterns require more intermediate steps to arrive at correct answers. Thus, COT is expected to help more for such cases. We test this hypothesis by glancing into the GSM-8K dataset. The num-</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ The number of $\left\langle x_{i}, y_{i}\right\rangle$ tuples depend on the maximum input sequence length of the model, typically $k \leqslant 10$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
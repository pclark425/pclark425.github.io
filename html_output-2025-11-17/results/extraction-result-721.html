<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-721 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-721</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-721</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-251066622</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2207.12876v2.pdf" target="_blank">Repeated Environment Inference for Invariant Learning</a></p>
                <p><strong>Paper Abstract:</strong> We study the problem of invariant learning when the environment labels are unknown. We focus on the invariant representation notion when the Bayes optimal conditional label distribution is the same across different environments. Previous work conducts Environment Inference (EI) by maximizing the penalty term from Invariant Risk Minimization (IRM) framework. The EI step uses a reference model which focuses on spurious correlations to efficiently reach a good environment partition. However, it is not clear how to find such a reference model. In this work, we propose to repeat the EI process and retrain an ERM model on the \textit{majority} environment inferred by the previous EI step. Under mild assumptions, we find that this iterative process helps learn a representation capturing the spurious correlation better than the single step. This results in better Environment Inference and better Invariant Learning. We show that this method outperforms baselines on both synthetic and real-world datasets.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e721.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e721.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REIIL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Repeated Environment Inference for Invariant Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative environment-inference strategy that repeats EIIL's environment inference step and retrains an ERM reference model on the inferred majority environment to produce a more spurious-feature-focused reference model and improve downstream invariant learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>REIIL (Repeated Environment Inference for Invariant Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>REIIL iteratively applies Environment Inference (EI) using an ERM reference model, then retrains an ERM classifier on the majority environment produced by that EI step, repeating this process for n iterations (n≈9 used in experiments). The intuition/conjecture is that retraining on the majority environment amplifies the spurious association in the reference model (i.e., better approximates a model that depends heavily on spurious features), which in turn yields a better environment partition when EI (which maximizes an IRM penalty) is reapplied; the final environment partition is then used for invariant learning (e.g., IRM) or reweighted ERM.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CMNIST / CBMNIST synthetic image benchmarks (training distributional shifts)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Simulated image classification environments where spurious cues (color in Colored MNIST; CIFAR backgrounds in CBMNIST) are strongly correlated with labels in training but change at test time. These are static synthetic datasets (not interactive or open-ended)—the paper constructs two training environments with different rates of 'shuffled' (background/color-mismatched) examples to emulate unstable anti-causal mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Iterative environment inference to produce a reference model biased toward spurious features, plus subsequent invariant learning or reweighting on inferred environment partitions; essentially a representation-level focus on separating spurious vs invariant samples via repeated EI.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Anti-causal spuriously correlated features (image background textures, color) and generally irrelevant variables correlated with label in training (selection bias / unstable mechanisms).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Uses EI's IRM-based penalty maximization (gradient-of-risk penalty) with a reference model to partition data into environments that maximize violation of invariance; repetition sharpens the partition so that samples that lack the spurious cue concentrate in the minority environment.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Indirect: by producing environment partitions that maximize invariance violation for spurious-focused reference models, REIIL enables downstream invariant learning objectives (e.g., IRM) to enforce invariance across those inferred environments and thus reduce reliance on spurious cues; no explicit statistical refutation test is proposed beyond improved downstream generalization and partition diagnostics (accuracy drops on minority env, shuffled-sample concentration).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Qualitative: REIIL consistently outperforms EIIL and other baselines on CBMNIST variants (substantial gains on CBMNIST [I] and [II]) and matches or slightly improves stability on CMNIST; REIIL often achieves lower validation and test error than EIIL in the reported experiments. (Numerical metrics are presented in paper figures/tables but are dataset-specific; no single aggregated numeric summary provided in text.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Qualitative baseline: EIIL (single-step EI) and plain ERM/IRM; in many CBMNIST cases ERM outperforms native IRM but REIIL improves over EIIL and sometimes over IRM; in CMNIST gains are small because ERM already approximates a spurious model well.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>CBMNIST uses 10 distinct CIFAR background types mapped to digits; majority of training samples (~98.5%) exhibit the spurious background-label correlation in CBMNIST[I].</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Repeating environment inference and retraining ERM on the majority inferred environment produces a reference model that relies more on spurious features, yielding better environment partitions and improved downstream invariant learning; this is particularly effective when most training samples exhibit a strong spurious correlation and the initial ERM reference model is not already strongly spurious-focused. The paper shows dynamics where minority-environment accuracy under the reference model decreases and the percentage of 'shuffled' (causal-only) samples in the minority environment increases across iterations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e721.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e721.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EIIL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Environment Inference for Invariant Learning (EIIL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-step method that infers environment labels from an input reference classifier by maximizing an IRM-style penalty, then uses the inferred labels for invariant learning (e.g., IRM).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Environment inference for invariant learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>EIIL (Environment Inference for Invariant Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>EIIL fixes a reference classifier Φ (typically ERM), optimizes a soft assignment q to partition data into environments by maximizing the IRM penalty C_EI(Φ,q) = ||∇_w|_{w=1.0} R_e(w·Φ,q)||^2, converts soft assignments to a binary partition, and then trains an invariant learning objective (e.g., IRM) using those inferred environments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CMNIST / CBMNIST synthetic image benchmarks (as used in this paper and Creager et al. 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Static synthetic image datasets with constructed training/test environment differences via manipulating spurious features (color/background). Not interactive or open-ended.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Environment partitioning via maximizing IRM gradient penalty with a spurious-focused reference model; relies on obtaining a reference model that emphasizes spurious correlations so that EI finds partitions that separate samples by presence/absence of spurious cues.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Anti-causal spurious correlations such as color in CMNIST and background textures in CBMNIST; selection bias/unstable P(feature|label) mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detection via optimization: EIIL identifies partitions that produce high IRM penalty (i.e., large violation of conditional invariance across partitions) under the provided reference model, effectively detecting where model predictions disagree with invariance assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Enables downstream invariant learning (IRM) to be applied on inferred environment splits; does not itself provide statistical refutation beyond producing splits that make spurious patterns salient to invariant learners.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Reported as a baseline: EIIL improves over naive methods in settings where the reference model captures spurious features, but performance degrades when the reference model fails to emphasize spurious cues. In this paper, EIIL is outperformed by REIIL in most tested CBMNIST setups.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>If the reference model does not focus on spurious features (i.e., is not biased), EIIL often produces suboptimal partitions and poor downstream invariant learning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>EIIL depends critically on the choice of reference classifier: it works well when the reference model is spurious-feature-focused, but can fail otherwise; REIIL was proposed to produce better reference models via repetition.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e721.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e721.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRM (IRMv1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant Risk Minimization (practical IRMv1 variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An objective for learning representations whose optimal classifier is invariant across environments, operationalized with a gradient-based penalty that constrains the classifier to have invariant optimality across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Invariant risk minimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>IRM / IRMv1</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>IRM defines an objective that minimizes empirical risk across environments plus a penalty that enforces that a classifier w composed with representation Φ is optimal in each environment; the practical IRMv1 uses a penalty ||∇_w|_{w=1.0} R_e(w·Φ)||^2 summed across environments and weighted by λ. The penalty tests whether scaling the classifier changes risk gradients differently across environments, encouraging representations where P(y|Φ(x)) is invariant.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multiple environment training datasets (CMNIST, CBMNIST) where environment labels are known or inferred</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Static datasets with environment labels (either ground-truth or inferred via EI/EIIL/REIIL); not interactive.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Regularization-based invariance enforcement: penalizes representations whose optimal linear classifier varies across environments, thereby discouraging dependence on environment-specific (spurious) features.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations that differ across environments (unstable anti-causal features, selection bias, background/color cues).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>IRM penalty itself acts as detector of invariance violation: a large gradient penalty indicates representation predictions depend on environment-specific features.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>By enforcing invariance across multiple environments, IRM implicitly refutes predictors that rely on environment-specific spurious signals; if a predictor's optimal classifier cannot be made common across environments, IRM penalizes it.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>IRM can succeed when true invariant P(y|φ(x)) exists and environments are informative; in some experiments (including synthetic IRM benchmarks) IRM finds best causal solutions. In CMNIST, IRM can help but performance depends on environment partition quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>When environment labels are poor or target domains are dissimilar, IRM can fail or be outperformed by ERM; IRM requires good environment labels and sufficient diversity across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>IRM provides a principled penalty for discouraging spurious-environment-specific predictors, but its success depends on having informative environment labels and sufficiently similar target domains; it may fail if environment diversity is insufficient or partitions are poor.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e721.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e721.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REI-WERM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weighted ERM on REIIL/EI-inferred splits (REI-WERM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reweighting baseline where samples are weighted per inferred environment (samples within each environment weighted equally) and ERM is trained on these weighted samples; found empirically to match IRM performance in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>REI-WERM (Weighted ERM on inferred environment splits)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>After obtaining environment partitions (from EIIL or REIIL), assign equal weight to samples within each environment (effectively upweighting minority-environment samples relative to raw data frequency) and train an ERM model on this reweighted dataset. The paper calls this WERM and reports REI-WERM when used with REIIL splits.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CMNIST / CBMNIST (using inferred environment partitions)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Static synthetic datasets; the method operates on partitions inferred by EI/REIIL rather than interactive environments.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Reweighting: balances the influence of environments so that minority (likely more causal) samples have proportionally larger impact; reduces dominance of spurious-majority samples.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations arising from majority-environment features (backgrounds, color) and selection bias.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Implicit: by equalizing environment contributions, samples from the spurious-majority environment are downweighted relative to their raw frequency (and minority/causal-rich environment samples are upweighted).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Empirical: REI-WERM (WERM trained on REIIL splits) matched the performance of IRM in some CBMNIST settings and provided surprisingly strong results, indicating that simple reweighting on good inferred partitions can rival explicit invariance regularizers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baseline ERM (no reweighting) often performs worse when spurious correlations dominate training; WERM on poor partitions (e.g., from weak reference models) may not help.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simple reweighting of inferred environments can be a competitive and effective way to mitigate spurious-majority dominance; success depends on quality of inferred partitions (REIIL-produced partitions improved WERM performance).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e721.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e721.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adversarial Invariant Learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial Invariant Learning (minimax worst-case environment inference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that jointly infers adversarial (worst-case) environment partitions and trains predictors robust to those partitions via a minimax formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adversarial invariant learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Adversarial Invariant Learning (Ye et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Formulates invariant learning as a minimax problem that alternates between inferring a worst-case environment partition (adversary) and training predictors to be robust to that partition; aims to find predictors that minimize risk under the most damaging environment splits.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Not applied in this paper's experiments (referenced in related work); generally targeted at multi-domain classification problems.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Typically static dataset partitions but inferred adversarially; not an interactive or experimental design environment.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Adversarial partitioning to expose spurious dependencies and training robust predictors via minimax optimization; effectively forces models to perform well even when data is partitioned to maximize their reliance on spurious features.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Unstable features and spurious correlations that can be exploited by models; selection bias and environment-specific cues.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Adversarial optimization to find partitions that maximize model vulnerability (i.e., expose spurious reliance); not a statistical test per se but an optimization-based detection.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>By training against adversarial partitions, the method attempts to eliminate predictors that rely on spurious environment-specific signals.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Not evaluated in this paper's experiments; cited as a related approach that jointly infers partitions and trains robust models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as an alternative approach that infers worst-case partitions adversarially; relevant to spurious-correlation robustness but not empirically compared in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e721.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e721.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Upweighting (Just-Train-Twice)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Upweighting underperforming samples (Just Train Twice approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage strategy that trains a model, identifies underperforming groups/samples, then upweights them in a second training pass to improve worst-group performance and mitigate spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Just train twice: Improving group robustness without training group information</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Upweighting underperforming samples (Liu et al., 2021 'Just Train Twice')</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train a first-pass ERM model, use its per-sample or per-group performance to identify underperforming subsets (likely corresponding to minority/causal groups), then upweight those samples in a second ERM training pass to improve robustness; does not require explicit environment/group labels.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Referenced as a general technique for dataset/group robustness; not applied here experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applies to static datasets; not an interactive experimental environment.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Reweighting strategy that increases influence of samples that the initial model handles poorly (often samples where spurious correlations do not hold), thereby reducing overall reliance on spurious majority patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Selection bias / spurious-majority correlations where minority/causal samples are underrepresented or poorly predicted by ERM.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Identification of underperforming samples/groups via first-pass model errors (empirical error-based detection).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Not typically downweighting; primarily upweights underperforming (often causal) samples in a second training pass.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Reported in cited work to substantially improve worst-group accuracy in many benchmarks. In this paper it is cited as inspiration and not directly evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Compared to plain ERM, the two-pass upweighting approach often improves worst-group metrics where ERM fails due to spurious-majority dominance (as shown in the cited literature).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as a simple, practical method that can improve worst-case group performance without group labels; conceptually related to REIIL's goal of amplifying minority/causal samples' influence but differs in mechanism (empirical upweighting vs iterative environment inference).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Environment inference for invariant learning <em>(Rating: 2)</em></li>
                <li>Invariant risk minimization <em>(Rating: 2)</em></li>
                <li>Just train twice: Improving group robustness without training group information <em>(Rating: 2)</em></li>
                <li>Adversarial invariant learning <em>(Rating: 2)</em></li>
                <li>The risks of invariant risk minimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-721",
    "paper_id": "paper-251066622",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "REIIL",
            "name_full": "Repeated Environment Inference for Invariant Learning",
            "brief_description": "An iterative environment-inference strategy that repeats EIIL's environment inference step and retrains an ERM reference model on the inferred majority environment to produce a more spurious-feature-focused reference model and improve downstream invariant learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "REIIL (Repeated Environment Inference for Invariant Learning)",
            "method_description": "REIIL iteratively applies Environment Inference (EI) using an ERM reference model, then retrains an ERM classifier on the majority environment produced by that EI step, repeating this process for n iterations (n≈9 used in experiments). The intuition/conjecture is that retraining on the majority environment amplifies the spurious association in the reference model (i.e., better approximates a model that depends heavily on spurious features), which in turn yields a better environment partition when EI (which maximizes an IRM penalty) is reapplied; the final environment partition is then used for invariant learning (e.g., IRM) or reweighted ERM.",
            "environment_name": "CMNIST / CBMNIST synthetic image benchmarks (training distributional shifts)",
            "environment_description": "Simulated image classification environments where spurious cues (color in Colored MNIST; CIFAR backgrounds in CBMNIST) are strongly correlated with labels in training but change at test time. These are static synthetic datasets (not interactive or open-ended)—the paper constructs two training environments with different rates of 'shuffled' (background/color-mismatched) examples to emulate unstable anti-causal mechanisms.",
            "handles_distractors": true,
            "distractor_handling_technique": "Iterative environment inference to produce a reference model biased toward spurious features, plus subsequent invariant learning or reweighting on inferred environment partitions; essentially a representation-level focus on separating spurious vs invariant samples via repeated EI.",
            "spurious_signal_types": "Anti-causal spuriously correlated features (image background textures, color) and generally irrelevant variables correlated with label in training (selection bias / unstable mechanisms).",
            "detection_method": "Uses EI's IRM-based penalty maximization (gradient-of-risk penalty) with a reference model to partition data into environments that maximize violation of invariance; repetition sharpens the partition so that samples that lack the spurious cue concentrate in the minority environment.",
            "downweighting_method": null,
            "refutation_method": "Indirect: by producing environment partitions that maximize invariance violation for spurious-focused reference models, REIIL enables downstream invariant learning objectives (e.g., IRM) to enforce invariance across those inferred environments and thus reduce reliance on spurious cues; no explicit statistical refutation test is proposed beyond improved downstream generalization and partition diagnostics (accuracy drops on minority env, shuffled-sample concentration).",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Qualitative: REIIL consistently outperforms EIIL and other baselines on CBMNIST variants (substantial gains on CBMNIST [I] and [II]) and matches or slightly improves stability on CMNIST; REIIL often achieves lower validation and test error than EIIL in the reported experiments. (Numerical metrics are presented in paper figures/tables but are dataset-specific; no single aggregated numeric summary provided in text.)",
            "performance_without_robustness": "Qualitative baseline: EIIL (single-step EI) and plain ERM/IRM; in many CBMNIST cases ERM outperforms native IRM but REIIL improves over EIIL and sometimes over IRM; in CMNIST gains are small because ERM already approximates a spurious model well.",
            "has_ablation_study": true,
            "number_of_distractors": "CBMNIST uses 10 distinct CIFAR background types mapped to digits; majority of training samples (~98.5%) exhibit the spurious background-label correlation in CBMNIST[I].",
            "key_findings": "Repeating environment inference and retraining ERM on the majority inferred environment produces a reference model that relies more on spurious features, yielding better environment partitions and improved downstream invariant learning; this is particularly effective when most training samples exhibit a strong spurious correlation and the initial ERM reference model is not already strongly spurious-focused. The paper shows dynamics where minority-environment accuracy under the reference model decreases and the percentage of 'shuffled' (causal-only) samples in the minority environment increases across iterations.",
            "uuid": "e721.0"
        },
        {
            "name_short": "EIIL",
            "name_full": "Environment Inference for Invariant Learning (EIIL)",
            "brief_description": "A two-step method that infers environment labels from an input reference classifier by maximizing an IRM-style penalty, then uses the inferred labels for invariant learning (e.g., IRM).",
            "citation_title": "Environment inference for invariant learning",
            "mention_or_use": "use",
            "method_name": "EIIL (Environment Inference for Invariant Learning)",
            "method_description": "EIIL fixes a reference classifier Φ (typically ERM), optimizes a soft assignment q to partition data into environments by maximizing the IRM penalty C_EI(Φ,q) = ||∇_w|_{w=1.0} R_e(w·Φ,q)||^2, converts soft assignments to a binary partition, and then trains an invariant learning objective (e.g., IRM) using those inferred environments.",
            "environment_name": "CMNIST / CBMNIST synthetic image benchmarks (as used in this paper and Creager et al. 2021)",
            "environment_description": "Static synthetic image datasets with constructed training/test environment differences via manipulating spurious features (color/background). Not interactive or open-ended.",
            "handles_distractors": true,
            "distractor_handling_technique": "Environment partitioning via maximizing IRM gradient penalty with a spurious-focused reference model; relies on obtaining a reference model that emphasizes spurious correlations so that EI finds partitions that separate samples by presence/absence of spurious cues.",
            "spurious_signal_types": "Anti-causal spurious correlations such as color in CMNIST and background textures in CBMNIST; selection bias/unstable P(feature|label) mechanisms.",
            "detection_method": "Detection via optimization: EIIL identifies partitions that produce high IRM penalty (i.e., large violation of conditional invariance across partitions) under the provided reference model, effectively detecting where model predictions disagree with invariance assumptions.",
            "downweighting_method": null,
            "refutation_method": "Enables downstream invariant learning (IRM) to be applied on inferred environment splits; does not itself provide statistical refutation beyond producing splits that make spurious patterns salient to invariant learners.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Reported as a baseline: EIIL improves over naive methods in settings where the reference model captures spurious features, but performance degrades when the reference model fails to emphasize spurious cues. In this paper, EIIL is outperformed by REIIL in most tested CBMNIST setups.",
            "performance_without_robustness": "If the reference model does not focus on spurious features (i.e., is not biased), EIIL often produces suboptimal partitions and poor downstream invariant learning performance.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "EIIL depends critically on the choice of reference classifier: it works well when the reference model is spurious-feature-focused, but can fail otherwise; REIIL was proposed to produce better reference models via repetition.",
            "uuid": "e721.1"
        },
        {
            "name_short": "IRM (IRMv1)",
            "name_full": "Invariant Risk Minimization (practical IRMv1 variant)",
            "brief_description": "An objective for learning representations whose optimal classifier is invariant across environments, operationalized with a gradient-based penalty that constrains the classifier to have invariant optimality across environments.",
            "citation_title": "Invariant risk minimization",
            "mention_or_use": "use",
            "method_name": "IRM / IRMv1",
            "method_description": "IRM defines an objective that minimizes empirical risk across environments plus a penalty that enforces that a classifier w composed with representation Φ is optimal in each environment; the practical IRMv1 uses a penalty ||∇_w|_{w=1.0} R_e(w·Φ)||^2 summed across environments and weighted by λ. The penalty tests whether scaling the classifier changes risk gradients differently across environments, encouraging representations where P(y|Φ(x)) is invariant.",
            "environment_name": "Multiple environment training datasets (CMNIST, CBMNIST) where environment labels are known or inferred",
            "environment_description": "Static datasets with environment labels (either ground-truth or inferred via EI/EIIL/REIIL); not interactive.",
            "handles_distractors": true,
            "distractor_handling_technique": "Regularization-based invariance enforcement: penalizes representations whose optimal linear classifier varies across environments, thereby discouraging dependence on environment-specific (spurious) features.",
            "spurious_signal_types": "Spurious correlations that differ across environments (unstable anti-causal features, selection bias, background/color cues).",
            "detection_method": "IRM penalty itself acts as detector of invariance violation: a large gradient penalty indicates representation predictions depend on environment-specific features.",
            "downweighting_method": null,
            "refutation_method": "By enforcing invariance across multiple environments, IRM implicitly refutes predictors that rely on environment-specific spurious signals; if a predictor's optimal classifier cannot be made common across environments, IRM penalizes it.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "IRM can succeed when true invariant P(y|φ(x)) exists and environments are informative; in some experiments (including synthetic IRM benchmarks) IRM finds best causal solutions. In CMNIST, IRM can help but performance depends on environment partition quality.",
            "performance_without_robustness": "When environment labels are poor or target domains are dissimilar, IRM can fail or be outperformed by ERM; IRM requires good environment labels and sufficient diversity across environments.",
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "IRM provides a principled penalty for discouraging spurious-environment-specific predictors, but its success depends on having informative environment labels and sufficiently similar target domains; it may fail if environment diversity is insufficient or partitions are poor.",
            "uuid": "e721.2"
        },
        {
            "name_short": "REI-WERM",
            "name_full": "Weighted ERM on REIIL/EI-inferred splits (REI-WERM)",
            "brief_description": "A reweighting baseline where samples are weighted per inferred environment (samples within each environment weighted equally) and ERM is trained on these weighted samples; found empirically to match IRM performance in some settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "REI-WERM (Weighted ERM on inferred environment splits)",
            "method_description": "After obtaining environment partitions (from EIIL or REIIL), assign equal weight to samples within each environment (effectively upweighting minority-environment samples relative to raw data frequency) and train an ERM model on this reweighted dataset. The paper calls this WERM and reports REI-WERM when used with REIIL splits.",
            "environment_name": "CMNIST / CBMNIST (using inferred environment partitions)",
            "environment_description": "Static synthetic datasets; the method operates on partitions inferred by EI/REIIL rather than interactive environments.",
            "handles_distractors": true,
            "distractor_handling_technique": "Reweighting: balances the influence of environments so that minority (likely more causal) samples have proportionally larger impact; reduces dominance of spurious-majority samples.",
            "spurious_signal_types": "Spurious correlations arising from majority-environment features (backgrounds, color) and selection bias.",
            "detection_method": null,
            "downweighting_method": "Implicit: by equalizing environment contributions, samples from the spurious-majority environment are downweighted relative to their raw frequency (and minority/causal-rich environment samples are upweighted).",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Empirical: REI-WERM (WERM trained on REIIL splits) matched the performance of IRM in some CBMNIST settings and provided surprisingly strong results, indicating that simple reweighting on good inferred partitions can rival explicit invariance regularizers.",
            "performance_without_robustness": "Baseline ERM (no reweighting) often performs worse when spurious correlations dominate training; WERM on poor partitions (e.g., from weak reference models) may not help.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Simple reweighting of inferred environments can be a competitive and effective way to mitigate spurious-majority dominance; success depends on quality of inferred partitions (REIIL-produced partitions improved WERM performance).",
            "uuid": "e721.3"
        },
        {
            "name_short": "Adversarial Invariant Learning",
            "name_full": "Adversarial Invariant Learning (minimax worst-case environment inference)",
            "brief_description": "A method that jointly infers adversarial (worst-case) environment partitions and trains predictors robust to those partitions via a minimax formulation.",
            "citation_title": "Adversarial invariant learning",
            "mention_or_use": "mention",
            "method_name": "Adversarial Invariant Learning (Ye et al., 2021)",
            "method_description": "Formulates invariant learning as a minimax problem that alternates between inferring a worst-case environment partition (adversary) and training predictors to be robust to that partition; aims to find predictors that minimize risk under the most damaging environment splits.",
            "environment_name": "Not applied in this paper's experiments (referenced in related work); generally targeted at multi-domain classification problems.",
            "environment_description": "Typically static dataset partitions but inferred adversarially; not an interactive or experimental design environment.",
            "handles_distractors": true,
            "distractor_handling_technique": "Adversarial partitioning to expose spurious dependencies and training robust predictors via minimax optimization; effectively forces models to perform well even when data is partitioned to maximize their reliance on spurious features.",
            "spurious_signal_types": "Unstable features and spurious correlations that can be exploited by models; selection bias and environment-specific cues.",
            "detection_method": "Adversarial optimization to find partitions that maximize model vulnerability (i.e., expose spurious reliance); not a statistical test per se but an optimization-based detection.",
            "downweighting_method": null,
            "refutation_method": "By training against adversarial partitions, the method attempts to eliminate predictors that rely on spurious environment-specific signals.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Not evaluated in this paper's experiments; cited as a related approach that jointly infers partitions and trains robust models.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Mentioned as an alternative approach that infers worst-case partitions adversarially; relevant to spurious-correlation robustness but not empirically compared in this paper.",
            "uuid": "e721.4"
        },
        {
            "name_short": "Upweighting (Just-Train-Twice)",
            "name_full": "Upweighting underperforming samples (Just Train Twice approach)",
            "brief_description": "A two-stage strategy that trains a model, identifies underperforming groups/samples, then upweights them in a second training pass to improve worst-group performance and mitigate spurious correlations.",
            "citation_title": "Just train twice: Improving group robustness without training group information",
            "mention_or_use": "mention",
            "method_name": "Upweighting underperforming samples (Liu et al., 2021 'Just Train Twice')",
            "method_description": "Train a first-pass ERM model, use its per-sample or per-group performance to identify underperforming subsets (likely corresponding to minority/causal groups), then upweight those samples in a second ERM training pass to improve robustness; does not require explicit environment/group labels.",
            "environment_name": "Referenced as a general technique for dataset/group robustness; not applied here experimentally.",
            "environment_description": "Applies to static datasets; not an interactive experimental environment.",
            "handles_distractors": true,
            "distractor_handling_technique": "Reweighting strategy that increases influence of samples that the initial model handles poorly (often samples where spurious correlations do not hold), thereby reducing overall reliance on spurious majority patterns.",
            "spurious_signal_types": "Selection bias / spurious-majority correlations where minority/causal samples are underrepresented or poorly predicted by ERM.",
            "detection_method": "Identification of underperforming samples/groups via first-pass model errors (empirical error-based detection).",
            "downweighting_method": "Not typically downweighting; primarily upweights underperforming (often causal) samples in a second training pass.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Reported in cited work to substantially improve worst-group accuracy in many benchmarks. In this paper it is cited as inspiration and not directly evaluated.",
            "performance_without_robustness": "Compared to plain ERM, the two-pass upweighting approach often improves worst-group metrics where ERM fails due to spurious-majority dominance (as shown in the cited literature).",
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Cited as a simple, practical method that can improve worst-case group performance without group labels; conceptually related to REIIL's goal of amplifying minority/causal samples' influence but differs in mechanism (empirical upweighting vs iterative environment inference).",
            "uuid": "e721.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Environment inference for invariant learning",
            "rating": 2,
            "sanitized_title": "environment_inference_for_invariant_learning"
        },
        {
            "paper_title": "Invariant risk minimization",
            "rating": 2,
            "sanitized_title": "invariant_risk_minimization"
        },
        {
            "paper_title": "Just train twice: Improving group robustness without training group information",
            "rating": 2,
            "sanitized_title": "just_train_twice_improving_group_robustness_without_training_group_information"
        },
        {
            "paper_title": "Adversarial invariant learning",
            "rating": 2,
            "sanitized_title": "adversarial_invariant_learning"
        },
        {
            "paper_title": "The risks of invariant risk minimization",
            "rating": 1,
            "sanitized_title": "the_risks_of_invariant_risk_minimization"
        }
    ],
    "cost": 0.014938749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Repeated Environment Inference for Invariant Learning</p>
<p>Aayush Mishra 
Anqi Liu 
Repeated Environment Inference for Invariant Learning</p>
<p>We study the problem of invariant learning when the environment labels are unknown. We focus on the invariant representation notion when the Bayes optimal conditional label distribution is the same across different environments. Previous work conducts Environment Inference (EI) by maximizing the penalty term from Invariant Risk Minimization (IRM) framework. The EI step uses a reference model which focuses on spurious correlations to efficiently reach a good environment partition. However, it is not clear how to find such a reference model. In this work, we propose to repeat the EI process and retrain an ERM model on the majority environment inferred by the previous EI step. Under mild assumptions, we find that this iterative process helps learn a representation capturing the spurious correlation better than the single step. This results in better Environment Inference and better Invariant Learning. We show that this method outperforms baselines on both synthetic and real-world datasets.</p>
<p>Introduction</p>
<p>In conventional machine learning, training data is assumed to be independently and identically distributed (iid) as the test data. This assumption is usually violated in the real world. Samples used in training might not be representative of the whole data distribution in many applications. Therefore, performance suffers when machine learning algorithms are deployed in new domains or environments where spurious correlations learned from the training data do not hold. To solve this problem, recent literature advocates focusing on extracting causal relationships (Peters et al., 2017;Pearl, 2019) from data. However, causal discovery and inference in high-dimensional data remain a big challenge in practice (Schölkopf et al., 2021).</p>
<p>Published at the ICML 2022 Workshop on Spurious Correlations, Invariance, and Stability. Baltimore, Maryland, USA. Copyright 2022 by the author(s).</p>
<p>Recent works (Arjovsky et al., 2019;Ilse et al., 2020;Chuang et al., 2020) have proposed to learn stable correlations that are invariant across different domains because invariance has shown strong links with causality (Heinze-Deml et al., 2018;Peters et al., 2016). However, invariant learning methods usually require predefined environment labels, which are not always available. For example, fair machine learning can be regarded as an invariant learning problem where different sub-populations correspond to different environments, and the sensitive features are usually unavailable or even hard to define (Corbett-Davies &amp; Goel, 2018). On the other hand, environment labels play an important role in invariant learning. Figure 1 shows how the performance of IRM (Arjovsky et al., 2019) increases when training samples are grouped differently (Experiment B) from the ground-truth group labels (Experiment A). To create two training environments, we randomly shuffle the background for digits with different probabilities. IRM benefits from the regrouping as the new grouping reflects a better differentiation between the two environments. See Figure 1. p. 1% and 2% in both environments, respectively; Experiment B: Samples with shuffled background are all grouped in one environment. The test environment has completely different backgrounds from training in both cases. IRM's performance is significantly higher in Experiment B. More details in Section 4.1. Creager et al. (2021) proposed EIIL, enabling invariant learning when environment labels are unavailable. It conducts Environment Inference (EI) using the representation learned by a reference model by maximizing the penalty term in the Invariant Risk Minimization (IRM) framework.</p>
<p>Moreover, it is shown that EIIL usually works when the reference model captures "spurious" correlations. However, it is unclear how to find such a model besides learning an ERM model from all the training data.</p>
<p>In this paper, we find that environment partition obtained in a single EI step after ERM is often sub-optimal for invariant learning. We propose a simple repeated EI strategy to improve the quality of environment inference and, consequently, the performance of Invariant Learning. Our contribution can be summarized as follows:</p>
<ol>
<li>
<p>We propose a novel method that generates environment labels by repeating the EI step and training the ERM model on the majority environment. We call it Repeated Environment Inference for Invariant Learning (REIIL).</p>
</li>
<li>
<p>Under mild assumptions, we show that our strategy better facilitates downstream invariant learning.</p>
</li>
<li>
<p>Our approach outperforms previous methods, including EIIL on previously used and a newly introduced dataset.</p>
</li>
</ol>
<p>Related Work</p>
<p>IRM and Environment Inference Arjovsky et al. (2019) propose a training objective for learning the invariant representations, under the assumption that the Bayes optimal conditional P (y|φ(x)) remains invariant across domains. However, recent works (Rosenfeld et al., 2020;Kamath et al., 2021) have shown that IRM fails when target domains are not sufficiently similar to training domains. Besides, it also requires good environment labels beforehand. Several methods have been proposed to infer the environment labels before or along with the invariant learning. EIIL (Creager et al., 2021) takes a two-step strategy and first partitions the training data into environments using a biased reference model before using environment-based invariant learning. However, it is unclear how to get a perfectly biased reference model to obtain a good environment partition. Adversarial Invariant Learning (Ye et al., 2021) uses a minimax game to robustly train the predictors while inferring the worstcase environment partition. In this paper, we follow the EIIL setup and study how to better conduct environment inference for downstream invariant learning.</p>
<p>Other Types of Invariance in Domain Generalization</p>
<p>Besides IRM, other definitions of invariance exist in the domain generalization literature. Dating back to early works in domain adaptation (Gretton et al., 2009;Sun &amp; Saenko, 2016), marginal feature alignment motivates the learning of the domain-invariant features, that is P (φ(x)) being invariant across domains (Ganin et al., 2016). However, the shortcoming of these methods is also obvious: it ignores the relationship between the features and the labels, so it still suffers under label shift (Zhao et al., 2019;. Other works utilize tools like Information Bottleneck and propose learning objectives based on the mutual information . Li et al. (2018) focus on conditional domain invariant representation and make sure that the P (φ(x)|y) remains invariant across domains. Moreover, focusing on only one type of invariance may be inadequate (Shui et al., 2022), and choosing the right type of invariant learning algorithm can be tricky.</p>
<p>Some works also propose using domain-specific features along with domain-invariant features to improve performance in domain generalization tasks (Ding &amp; Fu, 2017;Chattopadhyay et al., 2020;Bui et al., 2021). However, it is hard to say whether exploiting domain-specific features will always help in unseen domains without theoretical guarantees. Our paper aims to learn the invariant P (y|φ(x)) distribution when environment labels are unavailable.</p>
<p>Representation Learning under Domain Shift Other representation learning techniques have also been found helpful under domain shift. For example, Liu et al. (2021) proposes that upweighting underperforming samples in a second iteration of training is sufficient to improve worst-case group performance. Kirichenko et al. (2022) suggest that last layer retraining of Neural Networks with interesting samples might be sufficient for robustness against spurious correlations. Contrastive Learning approaches Kim et al., 2021) have also shown promise for domain adaptation and generalization. However, these methods do not explicitly learn any invariance and are usually hard to analyze. In our paper, we focus on the more principled IRM setting and overcome the difficulty of obtaining biased models for environment inference.</p>
<p>Method</p>
<p>IRM method and Environment Inference</p>
<p>The practical version of IRM (called IRMv1) has the following objective,
min Φ:X →Y e∈Etr R e (Φ) + λ · ∇ w|w=1.0 R e (w · Φ) 2 (1)
where samples are taken from multiple environments and this additional information is used to regularize training. As multiple environments are not always available, EIIL proposes an environment inference step to produce the required environment labels. It uses a fixed reference classifier Φ and the IRM penalty term to do this. The EI step maximizes the following with respect to soft environment assignment q,
C EI (Φ, q) = ∇ w|w=1.0R e (w · Φ, q) 2 ,(2)
where R e (Φ, q) = 1 N i q i (e)l(Φ(x i ), y i ). This soft assignment is used to produce hard environment labels which are then used for invariant learning.</p>
<p>In summary, EIIL takes the following steps:</p>
<ol>
<li>
<p>Takes an input reference modelΦ.</p>
</li>
<li>
<p>Maximizes C EI to get environment partition. q * = arg max q C EI (Φ, q).</p>
</li>
<li>
<p>Minimizes C IL (any Invariant Learning objective of choice) to get the final model.
Φ * = arg min Φ C IL (Φ, q * ).
Note that here q is used to produce a binary partition, and we assume this binary partitioning throughout this paper.</p>
</li>
</ol>
<p>EIIL works under the assumption that the reference model (Φ) focuses only on the spurious features. It is only wheñ Φ = Φ Spurious , does the environment inference (EI) process maximally violates the Invariance Principle (EIC):
E[y|Φ(x) = h, e1] = E[y|Φ(x) = h, e2], ∀h ∈ H, e1, e2 ∈ E obs
The authors use Φ ERM as an approximation for Φ Spurious , but this does not always work. As underlined in the paper explicitly via various experiments, it remains unexplored to find better reference models (which would be worse than ERM in terms of generalization performance) to fully exploit the EI step in EIIL.</p>
<p>Proposed method</p>
<p>We present an assumption about the training distribution and a conjecture about the resulting EI step using a model trained from such a training distribution. We then reason how finding a model that satisfies the conjecture under the assumption would help the downstream invariant learning. We finally propose a strategy to approximate such a model. Assumption 3.1. Assume a data generating graph X → Y → Z, where X and Z are observed features, Y is the target feature and the anti-causal mechanism is unstable (Subbaswamy et al., 2022). A training dataset is sampled following this graph where anti-causal features are assumed to be more informative than the causal features, i.e.,
I tr (Y ; Z) &gt; I tr (Y ; X) &gt; 0(3)
where I measures mutual information between features.</p>
<p>Conjecture 3.2. There exists a model trained on such training datasets (tr) such that when it is used as a reference model for the EI step (2), the resulting majority environment (e maj ) satisfies the following:
I emaj (Y ; Z)−I emaj (Y ; X) ≥ I tr (Y ; Z)−I tr (Y ; X) (4)
Remarks: By induction, if we find such a model after one EI step, there exists another such model in the next step, when the majority environment is the training dataset. This process repeats until a point of diminishing returns.</p>
<p>Therefore, we propose a simple repetition of reference model training on a biased subset of samples to get a more biased reference model better approximating Φ Spurious . We use ERM to approximate such a model in each step. The strategy can be defined in the following steps:</p>
<ol>
<li>
<p>Perform EI step from EIIL using the ERM reference model trained on the whole dataset.</p>
</li>
<li>
<p>Instead of training an invariant learning algorithm directly on the obtained partition, retrain an ERM model on the majority environment obtained from the previous step. Repeat this step for n iterations.</p>
</li>
<li>
<p>Use the finally obtained partition for downstream invariant learning methods.</p>
</li>
</ol>
<p>The repetition of the EI step is based on the proposed conjecture that majority subsets obtained would show even stronger correlation between spurious and target labels and, the ERM model trained on it would iteratively approximate Φ Spurious .</p>
<p>Experimental Results</p>
<p>We show the effectiveness of our proposed method on various datasets under different challenging conditions and find that it recovers good environment splits and provides boosted performance in invariant learning. We also provide empirical evidence to support our conjecture.</p>
<p>Datasets</p>
<p>Colored MNIST (CMNIST) was originally introduced in the IRM paper (Arjovsky et al., 2019). It has a synthetic binary classification task where color is introduced as an anti-causal spurious correlation.</p>
<p>CIFAR-Background MNIST (CBMNIST)</p>
<p>We introduce this new dataset, which is based on the simple concept of putting MNIST (LeCun et al., 1998) digits on CIFAR (Krizhevsky et al., 2009) backgrounds which would act as spurious correlations. The classification task for the resultant images remains same as the MNIST digit classification task.</p>
<p>CBMNIST [I] was created in the following manner:</p>
<p>• One random CIFAR image from each class is chosen to act as the background for MNIST images. This results in 10 different backgrounds with different visual properties.</p>
<p>• Each MNIST class is mapped with a randomly selected CIFAR class. This mapping between classes acts as the anti-causal spurious correlation in the training set. For the test set, a completely different random mapping of classes is chosen such that there is no overlap of digitbackground combinations with the training set.</p>
<p>• Training data is split into two environments with 25000 samples each (from 60000 MNIST training samples). Test data has the remaining 10000 samples.</p>
<p>• All training samples are filled with the mapped CIFAR backgrounds. In environment 1, backgrounds of ∼1% of samples are shuffled. Shuffled here means a random background which is not equal to the one in the train or test mapping of that class (one of the remaining 8), is applied to this sample. Similarly, in environment 2, ∼2% samples have their background shuffled.</p>
<p>The resulting dataset has ∼98.5% samples exhibiting a strong anti-causal relationship between the target label and the background. The rest ∼1.5% samples have shuffled backgrounds so the only invariant features in them are the digit shapes. This supports our dataset assumption (3.1) strongly, which is required for our conjecture (3.2) to hold.</p>
<p>CBMNIST [II]</p>
<p>has a slight variation of the concept of shuffled. Here shuffled samples have their background switched randomly to exactly one other pre-decided background rather than 8 in CBMNIST [I]. In this way, even the ∼1.5% shuffled samples exhibit a spurious correlation with the background and a particular digit is only ever seen with at most 2 different backgrounds in the training set. The test set remains the same.</p>
<p>CBMNIST [III]</p>
<p>has the percentage of samples having shuffled backgrounds change from CBMNIST [I]. We changed the numbers 1% and 2% to 10% and 20% respectively, simulating a case where even simple ERM can focus on invariant causal features because of lots of background variations. It is unclear whether our assumption (3.1) holds in this case. Figure 2. CBMNIST Data generating process. Here X f is the foreground or digit shape (from MNIST) that directly causes the class label Y of a sample. X b is the spuriously correlated background (from CIFAR) which is caused by the label Y but the mechanism generating this distribution P (X b |Y ) is unstable and is represented with a colored dashed edge inspired from (Subbaswamy et al., 2022). Note that these variables are abstract and need to be extracted from images.</p>
<p>Implementation details</p>
<p>We reuse the code [github link], model architectures and hyperparameters from EIIL, which follows IRM for CM-NIST experiments. For CMNIST, we increase the number of training steps to 900. For CBMNIST, we use an MLP with a similar structure but instead of down sampling as in CMNIST, we use all the 32 × 32 × 3 input dimensions to help the model learn any complex background features. For REIIL, we found that in n = 9 iterations, the performance usually saturates. Note that for EIIL and REIIL, the environment labels are not used in the training. We also used L2 regularization on the parameters for all methods.  Figure 2 shows the data generating procss.</p>
<p>Result Analysis</p>
<p>BETTER INVARIANT LEARNING</p>
<p>Test performance on CMNIST and CBMNIST variants can be seen in Figure 4. In both datasets, REIIL improves over EIIL and outperforms others in most cases.</p>
<p>In CMNIST, the performance gains are insignificant because Φ ERM approximates Φ Spurious quite well in the first EIIL step itself. However, REIIL's performance is more stable than EIIL's across five runs.</p>
<p>In CBMNIST [I], the performance gains are quite substantial. Because the shuffled samples are scarce and poorly split, IRM fails to beat even ERM. But REIIL consistently performs better than other methods. For CBMNIST [II], the results were quite similar. Interestingly, in both these cases, ERM outperforms the native IRM, and using the EI step proves critical for performance gains.</p>
<p>ERM emerges as the best-performing model in CBMNIST [III], probably because the higher percentage of shuffled background samples can regularize the ERM model's training. The initial reference model is also very far from Φ Spurious , making the performance of EIIL and even REIIL worse than IRM. REIIL still beats EIIL, but it cannot reach the performance of even IRM due to the poor starting point. It highlights the importance of a good reference model for the success of Environment Inference (2).</p>
<p>One thing to note is the superior performance of the reweighting method to invariant Learning methods. Inspired from (Liu et al., 2021), we trained a Weighted ERM (WERM) model on the EI obtained environment splits (REI-WERM). WERM weighs samples within each environment equally rather than weighing each sample equally in the whole dataset as in ERM. This model surprisingly matched the performance of IRM. We defer investigating this further for future work.  </p>
<p>DYNAMICS OF REPEATED EI</p>
<p>If datasets follow our assumption (3.1) and we find a model satisfying Conjecture (3.2) using ERM, we claim that repeating the EI step produces better environment splits such that the spurious features are more informative about the labels in the majority environment (for example, corresponding to the non-shuffled images in CBMNIST [I]). This would imply that the minority environment's test accuracy would be low under the reference model trained using the majority environment. It would also imply that most of the causally informative samples (in the case of CBMNIST, these would be shuffled samples) would fall in the minority environment. Figure 5 shows the accuracies of the reference models in each step for the samples in the minority environment and the percentage of shuffled samples assigned in the minority environment with each EI step. We see that accuracies keep decreasing while the proportion of shuffled samples increases in the minority environment. Note that the training accuracies in the majority environment always reach close to 1, implying that the ERM model successfully trains to convergence in each step. For CMNIST, shuffled samples would correspond to those which do not exhibit the colorbased spurious correlation. This effect is not as pronounced as in CBMNIST [I], but follows a similar pattern.</p>
<p>Therefore, we can conclude that an ERM model trained on the majority environment obtained from the previous EI step better captures spurious features (background or color) and produces a better environment partition. </p>
<p>Discussion</p>
<p>What if our assumption is violated? It was shown by (Gulrajani &amp; Lopez-Paz, 2020) that carefully trained ERM models to work quite well and often outperform Domain Generalization methods. We also see that ERM outperforms other methods in CBMNIST [III]. Our assumption is supposed to hold in more challenging situations where ERM usually fails. However, in practice, it may not be easy to test whether our assumption is valid, especially when the variables in the causal graph are hard to extract directly from data as in the image data. We conducted experiments on synthetic data (Appendix A) where there is no clear optimal data split due to the significant noise levels. We find that REIIL usually achieves the best test error. However, it may not recover the models close to the true ones. There also exists a model selection problem as the parameter tuning according to the validation data may make the model farther from the ground truth one. We defer more discussion to Appendix A and leave further exploration to future work.</p>
<p>What if there are more than two environments? For simplicity, we only investigate environment inference for two environments in the paper. We are aware that a good environment split is hard to define with multiple types of spurious correlations. It is unclear whether a binary partition is sufficient. We defer further investigation of these more complicated scenarios in invariant learning for the future.</p>
<p>Conclusion</p>
<p>In this paper, we propose a novel strategy to find a more biased reference model for environment inference. By repeating the EI step iteratively and training the reference model on the majority environment, our method helps find a better environment partition for downstream Invariant Learning tasks. Our assumption often holds in practice when most training samples exhibit strong spurious correlations and simple ERM methods fail to generalize. We conduct experiments on both CMNIST and our newly designed CBMNIST to demonstrate the effectiveness of our method.</p>
<p>A. Synthetic Data Experiments</p>
<p>We use all variants of the synthetic data defined in (Arjovsky et al., 2019) as well as the synthetic data defined in (Creager et al., 2021) to test REIIL. The data is split into 3 environments with noises 0.2, 2, and 5 respectively. In the previous implementations, they trained ERM on all three environments, while training IRM/EIIL only on the first two and using the last one as a validation set to pick the best IRM regularizer. We change this setup by introducing a validation set explicitly using noise = 3.5 and treating the last environment with noise = 5 as the test set.</p>
<p>The IRM data experiment results can be summarized in Table 1. In almost all experiments, IRM finds the best solutions. Interestingly, REIIL almost always finds models with the lowest validation and test errors (always lower than EIIL), but that does not translate to the corresponding solutions having lower Causal and Non-Causal Errors compared with the ground truth solutions.</p>
<p>We find that the validation set in both previous and current settings had high noise (5 and 3.5), which is not representative of the real causal graph (which would have no noise). Therefore, we infer that selecting a model based on a lower validation error does not necessarily yield the best solutions. But in a learning setting with a fixed number of samples, finding a model with lower validation error is usually the best we can do. REIIL often achieves the lowest validation error, which is always lower than EIIL, suggesting that repeating the EI step improves performance. We found similar results with the synthetic data defined in EIIL, where REIIL usually found models with the lowest validation and test errors. </p>
<p>Figure 1 .
1Left: Data distributions in two environments in experiment A and B; Right: IRM test accuracy in experiment A and B; Experiment A: CBMNIST [I] samples with shuffled background w.</p>
<p>Figure 3 .
3CBMNIST [I]  samples. We use CIFAR images as the background for MNIST digits.</p>
<p>Figure 4 .
4Comparison with baseline methods on CMNIST and CBMNIST.</p>
<p>Figure 5 .
5Accuracy of reference model drops for the minority environment and percentage of shuffled samples increase in the minority environment with REIIL iterations.</p>
<p>Discussion This questions the traditional methods of model and hyperparameter selection based on validation sets. In this case, the validation set was biased, and it is clear that the model which performs best for this set does not yield the causal solution. Even if the validation set was unbiased, it might not generalize to biased sets because the solution does not account for the noise explicitly.46) 12.61 (1.28) 12.29 (2.23) 11.68 (1.06) ERM test 16.39 (0.76) 16.28 (1.14) 16.95 (1.13) 16.01 (1.04) 20.09 (0.95) 22.42 (2.00) 22.47 (5.43) Table 1. Results of synthetic data experiments. val and test denote validation and test set errors of the model after training. CE and NCE denote Causal and Non-Causal errors. (standard deviation across 3 runs in brackets)Method/Type FOU 
FOS 
FEU 
FES 
POU 
POS 
PEU 
PES </p>
<p>ERM val 9.50 (0.26) 
9.29 (0.23) 
10.02 (0.40) 9.34 (0.47) 
10.89 (0.20.45 (2.06) 
IRM val 5.11 (0.09) 
7.66 (0.17) 
14.84 (0.48) 13.96 (0.68) 6.50 (0.57) 
8.74 (0.43) 
16.16 (1.62) 
16.26 (1.38) 
IRM test 5.50 (0.19) 
12.52 (0.81) 27.12 (1.52) 26.18 (2.09) 9.06 (1.06) 
13.67 (1.33) 30.23 (3.69) 
30.22 (2.75) 
EIIL val 10.38 (0.26) 7.57 (0.35) 
14.01 (3.30) 18.32 (1.39) 10.07 (0.98) 12.38 (2.58) 19.01 (5.68) 
18.78 (5.10) 
EIIL test 17.67 (0.64) 9.51 (1.51) 
26.06 (8.6) 
32.75 (3.43) 18.06 (2.50) 19.89 (4.91) 35.37 (11.46) 35.92 (11.33) 
REIIL val 7.17 (0.45) 
6.58 (0.88) 
8.97 (0.51) 
8.96 (0.33) 
8.75 (0.49) 
10.40 (1.91) 11.81 (2.51) 
11.72 (1.42) 
REIIL test 10.77 (0.72) 8.31 (1.96) 
14.72 (1.29) 15.63 (1.36) 14.06 (1.50) 17.09 (4.13) 21.41 (6.00) 
20.86 (2.97) </p>
<p>ERM CE 0.11 (0.01) 
0.12 (0.00) 
0.44 (0.02) 
0.45 (0.00) 
0.12 (0.02) 
0.14 (0.00) 
0.44 (0.01) 
0.47 (0.01) 
ERM NCE 0.11 (0.00) 
-
0.44 (0.01) 
-
0.11 (0.01) 
-
0.42 (0.02) 
-
IRM CE 0.01 (0.00) 
0.09 (0.01) 
0.25 (0.01) 
0.29 (0.01) 
0.01 (0.00) 
0.04 (0.01) 
0.22 (0.02) 
0.29 (0.01) 
IRM NCE 0.01 (0.00) 
-
0.32 (0.00) 
-
0.01 (0.00) 
-
0.33 (0.01) 
-
EIIL CE 0.12 (0.02) 
0.02 (0.00) 
0.40 (0.05) 
0.46 (0.02) 
0.10 (0.03) 
0.09 (0.03) 
0.31 (0.10) 
0.42 (0.05) 
EIIL NCE 0.10 (0.02) 
-
0.45 (0.01) 
-
0.05 (0.04) 
-
0.39 (0.08) 
-
REIIL CE 0.05 (0.00) 
0.02 (0.00) 
0.49 (0.04) 
0.45 (0.02) 
0.05 (0.04) 
0.09 (0.02) 
0.42 (0.06) 
0.47 (0.02) 
REIIL NCE 0.04 (0.01) 
-
0.51 (0.02) 
-
0.04 (0.03) 
-
0.47 (0.05) 
-</p>
<p>Department of Computer Science, Johns Hopkins University. Correspondence to: Aayush Mishra <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#97;&#109;&#105;&#115;&#104;&#114;&#50;&#52;&#64;&#106;&#104;&#46;&#101;&#100;&#117;">&#97;&#109;&#105;&#115;&#104;&#114;&#50;&#52;&#64;&#106;&#104;&#46;&#101;&#100;&#117;</a>.</p>
<p>. M Arjovsky, L Bottou, I Gulrajani, Lopez-Paz, arXiv:1907.02893D. Invariant risk minimization. arXiv preprintArjovsky, M., Bottou, L., Gulrajani, I., and Lopez- Paz, D. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.</p>
<p>Exploiting domain-specific features to enhance domain generalization. M.-H Bui, T Tran, A Tran, Phung , D , Advances in Neural Information Processing Systems. 342021Bui, M.-H., Tran, T., Tran, A., and Phung, D. Exploiting domain-specific features to enhance domain generaliza- tion. Advances in Neural Information Processing Systems, 34, 2021.</p>
<p>Learning to balance specificity and invariance for in and out of domain generalization. P Chattopadhyay, Y Balaji, J Hoffman, European Conference on Computer Vision. SpringerChattopadhyay, P., Balaji, Y., and Hoffman, J. Learning to balance specificity and invariance for in and out of domain generalization. In European Conference on Com- puter Vision, pp. 301-318. Springer, 2020.</p>
<p>Estimating generalization under distribution shifts via domain-invariant representations. C.-Y Chuang, A Torralba, S Jegelka, arXiv:2007.03511arXiv preprintChuang, C.-Y., Torralba, A., and Jegelka, S. Estimating gen- eralization under distribution shifts via domain-invariant representations. arXiv preprint arXiv:2007.03511, 2020.</p>
<p>The measure and mismeasure of fairness: A critical review of fair machine learning. S Corbett-Davies, S Goel, arXiv:1808.00023arXiv preprintCorbett-Davies, S. and Goel, S. The measure and mismea- sure of fairness: A critical review of fair machine learning. arXiv preprint arXiv:1808.00023, 2018.</p>
<p>Environment inference for invariant learning. E Creager, J.-H Jacobsen, R Zemel, International Conference on Machine Learning. PMLRCreager, E., Jacobsen, J.-H., and Zemel, R. Environment inference for invariant learning. In International Con- ference on Machine Learning, pp. 2189-2200. PMLR, 2021.</p>
<p>Deep domain generalization with structured low-rank constraint. Z Ding, Y Fu, IEEE Transactions on Image Processing. 271Ding, Z. and Fu, Y. Deep domain generalization with struc- tured low-rank constraint. IEEE Transactions on Image Processing, 27(1):304-313, 2017.</p>
<p>Domain-adversarial training of neural networks. The journal of machine learning research. Y Ganin, E Ustinova, H Ajakan, P Germain, H Larochelle, F Laviolette, M Marchand, V Lempitsky, 17Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096-2030, 2016.</p>
<p>Covariate shift by kernel mean matching. A Gretton, A Smola, J Huang, M Schmittfull, K Borgwardt, B Schölkopf, Dataset shift in machine learning. 34Gretton, A., Smola, A., Huang, J., Schmittfull, M., Borg- wardt, K., and Schölkopf, B. Covariate shift by kernel mean matching. Dataset shift in machine learning, 3(4): 5, 2009.</p>
<p>I Gulrajani, D Lopez-Paz, arXiv:2007.01434search of lost domain generalization. arXiv preprintGulrajani, I. and Lopez-Paz, D. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020.</p>
<p>Invariant causal prediction for nonlinear models. C Heinze-Deml, J Peters, N Meinshausen, Journal of Causal Inference. 62Heinze-Deml, C., Peters, J., and Meinshausen, N. Invariant causal prediction for nonlinear models. Journal of Causal Inference, 6(2), 2018.</p>
<p>Domain invariant variational autoencoders. M Ilse, J M Tomczak, C Louizos, M Welling, Diva, Medical Imaging with Deep Learning. PMLRIlse, M., Tomczak, J. M., Louizos, C., and Welling, M. Diva: Domain invariant variational autoencoders. In Medical Imaging with Deep Learning, pp. 322-348. PMLR, 2020.</p>
<p>Does invariant risk minimization capture invariance. P Kamath, A Tangella, D Sutherland, N Srebro, International Conference on Artificial Intelligence and Statistics. PMLRKamath, P., Tangella, A., Sutherland, D., and Srebro, N. Does invariant risk minimization capture invariance? In International Conference on Artificial Intelligence and Statistics, pp. 4069-4077. PMLR, 2021.</p>
<p>Selfsupervised contrastive regularization for domain generalization. D Kim, Y Yoo, S Park, J Kim, J Lee, Selfreg, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionKim, D., Yoo, Y., Park, S., Kim, J., and Lee, J. Selfreg: Self- supervised contrastive regularization for domain general- ization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9619-9628, 2021.</p>
<p>Last layer re-training is sufficient for robustness to spurious correlations. P Kirichenko, P Izmailov, A G Wilson, arXiv:2204.02937arXiv preprintKirichenko, P., Izmailov, P., and Wilson, A. G. Last layer re-training is sufficient for robustness to spurious correla- tions. arXiv preprint arXiv:2204.02937, 2022.</p>
<p>Learning multiple layers of features from tiny images. A Krizhevsky, G Hinton, Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.</p>
<p>Gradientbased learning applied to document recognition. Proceedings of the IEEE. Y Lecun, L Bottou, Y Bengio, P Haffner, 86LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278-2324, 1998.</p>
<p>Invariant information bottleneck for domain generalization. B Li, Y Shen, Y Wang, W Zhu, C J Reed, J Zhang, D Li, K Keutzer, H Zhao, arXiv:2106.06333arXiv preprintLi, B., Shen, Y., Wang, Y., Zhu, W., Reed, C. J., Zhang, J., Li, D., Keutzer, K., and Zhao, H. Invariant information bottleneck for domain generalization. arXiv preprint arXiv:2106.06333, 2021.</p>
<p>Deep domain generalization via conditional invariant adversarial networks. Y Li, X Tian, M Gong, Y Liu, T Liu, K Zhang, D Tao, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Li, Y., Tian, X., Gong, M., Liu, Y., Liu, T., Zhang, K., and Tao, D. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 624-639, 2018.</p>
<p>Just train twice: Improving group robustness without training group information. E Z Liu, B Haghgoo, A S Chen, A Raghunathan, P W Koh, S Sagawa, P Liang, C Finn, International Conference on Machine Learning. PMLRLiu, E. Z., Haghgoo, B., Chen, A. S., Raghunathan, A., Koh, P. W., Sagawa, S., Liang, P., and Finn, C. Just train twice: Improving group robustness without training group information. In International Conference on Machine Learning, pp. 6781-6792. PMLR, 2021.</p>
<p>The seven tools of causal inference, with reflections on machine learning. J Pearl, Communications of the ACM. 623Pearl, J. The seven tools of causal inference, with reflections on machine learning. Communications of the ACM, 62 (3):54-60, 2019.</p>
<p>Causal inference by using invariant prediction: identification and confidence intervals. J Peters, P Bühlmann, N Meinshausen, Journal of the Royal Statistical Society: Series B (Statistical Methodology). 785Peters, J., Bühlmann, P., and Meinshausen, N. Causal in- ference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Soci- ety: Series B (Statistical Methodology), 78(5):947-1012, 2016.</p>
<p>Elements of causal inference: foundations and learning algorithms. J Peters, D Janzing, B Schölkopf, The MIT PressPeters, J., Janzing, D., and Schölkopf, B. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017.</p>
<p>The risks of invariant risk minimization. E Rosenfeld, P Ravikumar, A Risteski, arXiv:2010.05761arXiv preprintRosenfeld, E., Ravikumar, P., and Risteski, A. The risks of invariant risk minimization. arXiv preprint arXiv:2010.05761, 2020.</p>
<p>Towards causal representation learning. B Schölkopf, F Locatello, S Bauer, N R Ke, N Kalchbrenner, A Goyal, Y Bengio, Schölkopf, B., Locatello, F., Bauer, S., Ke, N. R., Kalch- brenner, N., Goyal, A., and Bengio, Y. Towards causal representation learning, 2021. URL https://arxiv. org/abs/2102.11107.</p>
<p>On the benefits of representation regularization in invariance based domain generalization. C Shui, B Wang, C Gagné, Machine Learning. Shui, C., Wang, B., and Gagné, C. On the benefits of representation regularization in invariance based domain generalization. Machine Learning, pp. 1-21, 2022.</p>
<p>A unifying causal framework for analyzing dataset shift-stable learning algorithms. A Subbaswamy, B Chen, S Saria, Journal of Causal Inference. 101Subbaswamy, A., Chen, B., and Saria, S. A unifying causal framework for analyzing dataset shift-stable learning al- gorithms. Journal of Causal Inference, 10(1):64-89, 2022.</p>
<p>Deep coral: Correlation alignment for deep domain adaptation. B Sun, K Saenko, European conference on computer vision. SpringerSun, B. and Saenko, K. Deep coral: Correlation alignment for deep domain adaptation. In European conference on computer vision, pp. 443-450. Springer, 2016.</p>
<p>Cross-domain contrastive learning for unsupervised domain adaptation. R Wang, Z Wu, Z Weng, J Chen, G.-J Qi, Y.-G Jiang, IEEE Transactions on Multimedia. Wang, R., Wu, Z., Weng, Z., Chen, J., Qi, G.-J., and Jiang, Y.-G. Cross-domain contrastive learning for unsupervised domain adaptation. IEEE Transactions on Multimedia, 2022.</p>
<p>Adversarial invariant learning. N Ye, J Tang, H Deng, X.-Y Zhou, Q Li, Z Li, G.-Z Yang, Z Zhu, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEEYe, N., Tang, J., Deng, H., Zhou, X.-Y., Li, Q., Li, Z., Yang, G.-Z., and Zhu, Z. Adversarial invariant learning. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 12441-12449. IEEE, 2021.</p>
<p>On learning invariant representations for domain adaptation. H Zhao, R T Des Combes, K Zhang, Gordon , G , International Conference on Machine Learning. PMLRZhao, H., Des Combes, R. T., Zhang, K., and Gordon, G. On learning invariant representations for domain adaptation. In International Conference on Machine Learning, pp. 7523-7532. PMLR, 2019.</p>
<p>Fundamental limits and tradeoffs in invariant representation learning. H Zhao, C Dan, B Aragam, T S Jaakkola, G J Gordon, P Ravikumar, arXiv:2012.10713arXiv preprintZhao, H., Dan, C., Aragam, B., Jaakkola, T. S., Gordon, G. J., and Ravikumar, P. Fundamental limits and trade- offs in invariant representation learning. arXiv preprint arXiv:2012.10713, 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4198 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4198</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4198</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-280265849</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2507.10522v1.pdf" target="_blank">DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology</a></p>
                <p><strong>Paper Abstract:</strong> We introduce DeepResearchEco, a novel agentic LLM-based system for automated scientific synthesis that supports recursive, depth- and breadth-controlled exploration of original research questions—enhancing search diversity and nuance in the retrieval of relevant scientific literature. Unlike conventional retrieval-augmented generation pipelines, DeepResearch enables user-controllable synthesis with transparent reasoning and parameter-driven configurability, facilitating high-throughput integration of domain-specific evidence while maintaining analytical rigor. Applied to 49 ecological research questions, DeepResearch achieves up to a 21-fold increase in source integration and a 14.9-fold rise in sources integrated per 1,000 words. High-parameter settings yield expert-level analytical depth and contextual diversity. Source code available at: https://github.com/sciknoworg/deep-research.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4198.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4198.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepResearch Eco</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepResearch Eco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recursive, agentic LLM-based literature-synthesis system that issues SERP-style subqueries, retrieves top documents (ORKG Ask or Firecrawl), prompts an LLM to produce concise 'learnings' and follow-up questions, and synthesizes the accumulated insights into structured, JSON-validated reports; designed to extract mechanistic descriptions, quantitative findings, temporal thresholds, and cross-study patterns from ecological literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepResearch Eco</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DeepResearch is an orchestrated, multi-agent pipeline with four sub-agents (generate SERP queries, search, summarize result, generate report) operating in a recursive loop controlled by user-set breadth and depth parameters. At each recursion, generate_serp_queries uses the LLM to create search-compatible queries and goals; search executes queries in parallel against configurable providers (ORKG Ask for scholarly metadata and Firecrawl for full-text web content); summarize_result merges top-k documents (default top 10) into a prompt that asks the LLM to extract up to three concise 'learnings' (information-dense insights) and up to three follow-up questions; these learnings drive subsequent recursive queries. At termination, generate_report synthesizes all learnings and visited URLs into a final Markdown report validated against a JSON schema. The workflow explicitly extracts numeric findings, mechanistic causal chains, temporal thresholds, and taxonomic mentions through targeted prompting and curated detection vocabularies; evaluation uses lexical and embedding similarity metrics and a multi-dimensional domain-specific quality scoring pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT o3; GPT o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Ecology (literature across restoration ecology, invasive species management, microbial ecology, pollination ecology, interdisciplinary ecology)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>49 research questions => 196 generated syntheses; per-synthesis source counts reported from 9.1 to 192.9 sources depending on configuration (configs: d∈{1,4}, b∈{1,4})</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Empirical quantitative relationships and process-level mechanistic relationships (percent effect statements, temporal thresholds, multi-step causal chains) and meta-level scaling relationships (power-law scaling of system source-utilization vs. parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Examples extracted or synthesized by DeepResearch include: (i) 'Extensification packages suppress herbage or milk output by 10-40%' (quantitative effect size); (ii) 'species richness recovery occurs within 5-6 years' and 'functional diversity lags require ≥10 years' (temporal thresholds); (iii) mechanistic causal chain: 'Nutrient withdrawal shifts competitive hierarchies from fast-growing tall grasses to stress-tolerators by (i) reducing soil NO3- and NH4+, (ii) decreasing leaf N content, (iii) opening ground-layer light niches' (multi-step causal pathway); (iv) system-level empirical relationship discovered in the paper's meta-analysis: source utilization scales with depth-breadth parameters following a power-law fit (R^2 = 0.97), increasing from 9.1±1.7 sources (d1_b1) to 192.9±31.2 sources (d4_b4), a 21.2-fold increase.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Hybrid retrieval-and-generation: dense semantic retrieval (ORKG Ask or web-scale Firecrawl) to collect top-k titles/abstracts or full text; LLM prompting on merged document text to produce concise 'learnings' (text-mined insights) and follow-up questions; vocabulary- and regex-based detectors (curated ecology vocabularies, causal connectives, temporal regexes) applied to LLM outputs and source text to identify mechanistic terms, numerical findings, statistics, taxonomic entities, and temporal precisions; aggregated into final synthesized reports.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Intrinsic and extrinsic evaluation: alignment/similarity metrics (ROUGE-L F1, SciBERT-based BERTScore F1, Word Mover's Distance on SciBERT embeddings) across configurations; domain-specific multi-dimensional quality scoring using curated vocabularies and detection heuristics (research depth, breadth, ecological specificity, scientific rigor, innovation capacity, information density); comparisons across model/configuration pairs (o3 vs o3-mini, d/b settings); qualitative case exemplars; claims of approaching expert-level integration are supported by these metrics and by manual examples but no large-scale external ground-truth benchmarking of individual extracted laws was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics include: ROUGE-L F1 (typically low, ~0.12-0.16 across cross-config comparisons), SciBERT BERTScore F1 (≈0.54-0.61 within/between models), WMD-based similarity (≈0.54-0.61). Source utilization: d1_b1 ~9.1±1.7 sources to d4_b4 ~192.9±31.2 sources (21.2×). Word count increased modestly from 1,579 to 2,234 words (41.5%); information density (sources per 1,000 words) improved 14.9-fold. Composite quality scores progressed from 0.405 (low config) to 0.478 (all configs mean) with best config d4_b4 = 0.577. Taxonomic precision reached 1.0±0.0 in d4_b4. Statistical sophistication metric increased from 1.02 to 1.20 across configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported as an explicit percent of 'laws correctly extracted' relative to an independent gold standard; closest reported indicators: taxonomic precision = 1.0 for d4_b4 (perfect on the metric used), and multi-step causal chains were 3.2× more frequent in d4 vs d1, but no external correctness rate for extracted quantitative laws provided.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Limits and failure modes reported include: LLMs can overlook or misuse details in complex reasoning; model-specific generation patterns produce cross-model variability; ROUGE-L underestimates semantic equivalence; reconciling conflicting temporal information across many sources is challenging; high-parameter configurations incur large computational/resource costs; no full external ground-truth validation of quantitative law correctness was performed and risk of hallucination remains (authors note use of ChatGPT only for stylistic edits).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared internal configurations (two reasoning models: GPT o3 and GPT o3-mini; depths d∈{1,4}; breadths b∈{1,4}); similarity and quality metrics reported across these. Compared conceptually to prior feed-forward pipelines (PaperQA, PaperQA2, ORKG Ask, LLMs4Synthesis) showing that DeepResearch's recursive approach yields greater source integration and different capability profiles, but no formal head-to-head accuracy benchmark against human experts or quantified baseline extraction-of-laws accuracy was presented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4198.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4198.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ORKG Ask</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ORKG Ask: a neuro-symbolic scholarly search and exploration system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scholarly search API/service that provides semantic search over a large index (70+ million publications), returning structured metadata (titles, abstracts, URLs) and serving as a retrieval backend for LLM-based synthesis pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Orkg ask: a neuro-symbolic scholarly search and exploration system</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ORKG Ask (semantic search API)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ORKG Ask performs vector/semantic retrieval over a large scholarly corpus and returns structured responses (titles, abstracts, URLs). In DeepResearch, ORKG Ask is configured as the semantic search provider: for each SERP-style query it returns top-ranked publications whose titles and abstracts are merged and passed to the LLM summarization sub-agent for extraction of learnings. ORKG Ask supports evidence-based synthesis by providing curated scholarly inputs rather than raw web text.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain scholarly literature (used here for ecological literature)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>ORKG index >70 million publications (search returns top-k per query; DeepResearch experiments retrieved on average up to ~192.9 sources per synthesis for high-parameter runs)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Provides source material (titles/abstracts) enabling extraction of empirical relationships, quantitative results, and reported statistical findings from the literature; not itself an LLM extracting laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Not an extractor itself; used to supply documents from which DeepResearch extracted examples such as percent effect sizes and temporal thresholds (see DeepResearch examples).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Semantic retrieval (vector similarity ranking) returning structured metadata (title, abstract, url); extraction of quantitative relationships is performed downstream by the LLM summarization agent.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not described in detail in this paper beyond being evaluated as a retrieval backend; DeepResearch validates downstream synthesis outputs with similarity and domain-specific quality metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>ORKG Ask provides ranked results; specific retrieval metrics are not reported here. DeepResearch reports downstream metrics (see DeepResearch entry).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>As a retrieval provider, limitations depend on index coverage and returned metadata (abstracts may omit numeric details present in full text); DeepResearch also used Firecrawl when full text was desired.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used in contrast to Firecrawl (web full-text provider) to illustrate trade-offs between structured scholarly retrieval and broad web search; no formal retrieval baseline comparison reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4198.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4198.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA: Retrieval-augmented generative agent for scientific research</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular agentic pipeline for LLM-assisted scientific question answering that retrieves papers via Google Scholar queries, builds embedding-based chunk databases, and summarizes or filters chunks before prompting an LLM to generate answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Paperqa: Retrievalaugmented generative agent for scientific research.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PaperQA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PaperQA retrieves relevant papers (via Google Scholar queries with keywords and year ranges), constructs embedding-based chunk indices, retrieves chunks with maximal marginal relevance, summarizes or marks chunks irrelevant, and feeds the distilled content to an LLM for final answer generation. The pipeline is modular (retrieval, chunking, reranking, summarization, generation) and is cited as prior art in agentic literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific literature (used for question answering and synthesis tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper (PaperQA cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Designed to support extraction of factual findings and summarized relationships from papers via chunking and LLM summarization; the paper here cites it as related work rather than reporting concrete law extraction examples.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Embedding-based chunking and retrieval, contextual summarization of chunks by LLMs, followed by LLM generation for answers. Numeric relationships would be obtained via summarization of retrieved chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>PaperQA is referenced for its modular retrieval-summarization pipeline; this paper does not report its validation details.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Mentioned as a feedforward pipeline with limited recursion/controllability compared to DeepResearch; potential limitations include chunking and reranking errors and semantic noise.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>PaperQA is referenced as prior work to which DeepResearch contrasts its recursive, depth/breadth-controllable approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4198.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4198.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA2 (multi-agent scientific synthesis framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of PaperQA implementing a multi-agent framework that separates retrieval and generation into specialized agents (paper search, citation traversal, gather-evidence, generation) to support collection, expansion, summarization, and synthesis of scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PaperQA2 decomposes the scientific synthesis workflow into distinct agents: a paper search agent (reformulates queries and fetches PDFs), a citation traversal agent (expands corpus via citation networks), a gather-evidence agent (retrieves and summarizes chunks via dense retrieval and reranking), and a generation agent (synthesizes answers from ranked evidence). This modular multi-agent design is cited as influential background motivating DeepResearch's multi-agent recursive orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific literature and question answering</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Supports extraction and summarization of evidence that could include quantitative relationships via chunk summarization; referenced rather than used.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Multi-agent retrieval, citation expansion, chunk retrieval + summarization, LLM-based generation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not detailed in this paper; referenced as related system.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Described as largely feedforward (single-pass) and less explicitly configurable/recursive than DeepResearch.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as related work context to highlight differences with DeepResearch's recursive loop and explicit depth/breadth controls.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4198.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4198.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs4Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs4Synthesis: Leveraging large language models for scientific synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that structures synthesis tasks into paper-wise, methodological, and thematic categories and uses LLMs for structured synthesis and evaluation (including use of LLM-as-a-judge for assessment).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llms4synthesis: Leveraging large language models for scientific synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLMs4Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLMs4Synthesis organizes synthesis tasks into structured categories (paper-wise, methodological, thematic) and uses LLMs to generate syntheses guided by structured prompts. It also explores evaluation approaches such as LLM-as-a-judge and optimization via RL with AI feedback for factuality and clarity. Cited as related work that emphasizes structured, evaluated synthesis using LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific synthesis applications</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Supports structured synthesis which could include extraction of quantitative relationships; cited as related methodology rather than demonstrated extraction examples in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Structured prompting and categorization of synthesis tasks; LLM-based summarization and judged evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>LLM-as-a-judge evaluation and use of reinforcement learning with AI feedback reported in the LLMs4Synthesis work (cited); no direct validation details reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Mentioned as part of the landscape of synthesis frameworks; not evaluated head-to-head here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Positioned in related work that informs DeepResearch's design choices around structured outputs and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4198.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4198.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autonomous multi-agent framework proposed to automate steps from idea generation to experimental design and publication writing, demonstrating potential for closed-loop scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI Scientist: Towards fully automated open-ended scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Scientist (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AI Scientist is described as a framework aiming to automate the end-to-end scientific workflow including ideation, experimental planning, and write-up using agentic components; cited as evidence of research interest in autonomous discovery systems that extend beyond retrieval-and-synthesis to experimental cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific discovery and automation (broad)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Proposed to enable discovery of novel scientific relationships in silico, although in this paper AI Scientist is referenced conceptually and no concrete law-extraction examples from that system are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Multi-agent autonomous workflows combining idea generation, planning, and experiment design; specifics left to original AI Scientist work.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Referenced as related work; validation details not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Cited to illustrate the frontier of autonomous discovery; not directly evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used to position DeepResearch within the broader landscape of agentic scientific systems that may perform discovery beyond literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Paperqa: Retrievalaugmented generative agent for scientific research. <em>(Rating: 2)</em></li>
                <li>Llms4synthesis: Leveraging large language models for scientific synthesis. <em>(Rating: 2)</em></li>
                <li>Orkg ask: a neuro-symbolic scholarly search and exploration system <em>(Rating: 2)</em></li>
                <li>The AI Scientist: Towards fully automated open-ended scientific discovery. <em>(Rating: 1)</em></li>
                <li>Language agents achieve superhuman synthesis of scientific knowledge. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4198",
    "paper_id": "paper-280265849",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "DeepResearch Eco",
            "name_full": "DeepResearch Eco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
            "brief_description": "A recursive, agentic LLM-based literature-synthesis system that issues SERP-style subqueries, retrieves top documents (ORKG Ask or Firecrawl), prompts an LLM to produce concise 'learnings' and follow-up questions, and synthesizes the accumulated insights into structured, JSON-validated reports; designed to extract mechanistic descriptions, quantitative findings, temporal thresholds, and cross-study patterns from ecological literature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DeepResearch Eco",
            "system_description": "DeepResearch is an orchestrated, multi-agent pipeline with four sub-agents (generate SERP queries, search, summarize result, generate report) operating in a recursive loop controlled by user-set breadth and depth parameters. At each recursion, generate_serp_queries uses the LLM to create search-compatible queries and goals; search executes queries in parallel against configurable providers (ORKG Ask for scholarly metadata and Firecrawl for full-text web content); summarize_result merges top-k documents (default top 10) into a prompt that asks the LLM to extract up to three concise 'learnings' (information-dense insights) and up to three follow-up questions; these learnings drive subsequent recursive queries. At termination, generate_report synthesizes all learnings and visited URLs into a final Markdown report validated against a JSON schema. The workflow explicitly extracts numeric findings, mechanistic causal chains, temporal thresholds, and taxonomic mentions through targeted prompting and curated detection vocabularies; evaluation uses lexical and embedding similarity metrics and a multi-dimensional domain-specific quality scoring pipeline.",
            "model_name": "GPT o3; GPT o3-mini",
            "model_size": null,
            "scientific_domain": "Ecology (literature across restoration ecology, invasive species management, microbial ecology, pollination ecology, interdisciplinary ecology)",
            "number_of_papers": "49 research questions =&gt; 196 generated syntheses; per-synthesis source counts reported from 9.1 to 192.9 sources depending on configuration (configs: d∈{1,4}, b∈{1,4})",
            "law_type": "Empirical quantitative relationships and process-level mechanistic relationships (percent effect statements, temporal thresholds, multi-step causal chains) and meta-level scaling relationships (power-law scaling of system source-utilization vs. parameters).",
            "law_examples": "Examples extracted or synthesized by DeepResearch include: (i) 'Extensification packages suppress herbage or milk output by 10-40%' (quantitative effect size); (ii) 'species richness recovery occurs within 5-6 years' and 'functional diversity lags require ≥10 years' (temporal thresholds); (iii) mechanistic causal chain: 'Nutrient withdrawal shifts competitive hierarchies from fast-growing tall grasses to stress-tolerators by (i) reducing soil NO3- and NH4+, (ii) decreasing leaf N content, (iii) opening ground-layer light niches' (multi-step causal pathway); (iv) system-level empirical relationship discovered in the paper's meta-analysis: source utilization scales with depth-breadth parameters following a power-law fit (R^2 = 0.97), increasing from 9.1±1.7 sources (d1_b1) to 192.9±31.2 sources (d4_b4), a 21.2-fold increase.",
            "extraction_method": "Hybrid retrieval-and-generation: dense semantic retrieval (ORKG Ask or web-scale Firecrawl) to collect top-k titles/abstracts or full text; LLM prompting on merged document text to produce concise 'learnings' (text-mined insights) and follow-up questions; vocabulary- and regex-based detectors (curated ecology vocabularies, causal connectives, temporal regexes) applied to LLM outputs and source text to identify mechanistic terms, numerical findings, statistics, taxonomic entities, and temporal precisions; aggregated into final synthesized reports.",
            "validation_approach": "Intrinsic and extrinsic evaluation: alignment/similarity metrics (ROUGE-L F1, SciBERT-based BERTScore F1, Word Mover's Distance on SciBERT embeddings) across configurations; domain-specific multi-dimensional quality scoring using curated vocabularies and detection heuristics (research depth, breadth, ecological specificity, scientific rigor, innovation capacity, information density); comparisons across model/configuration pairs (o3 vs o3-mini, d/b settings); qualitative case exemplars; claims of approaching expert-level integration are supported by these metrics and by manual examples but no large-scale external ground-truth benchmarking of individual extracted laws was reported.",
            "performance_metrics": "Reported metrics include: ROUGE-L F1 (typically low, ~0.12-0.16 across cross-config comparisons), SciBERT BERTScore F1 (≈0.54-0.61 within/between models), WMD-based similarity (≈0.54-0.61). Source utilization: d1_b1 ~9.1±1.7 sources to d4_b4 ~192.9±31.2 sources (21.2×). Word count increased modestly from 1,579 to 2,234 words (41.5%); information density (sources per 1,000 words) improved 14.9-fold. Composite quality scores progressed from 0.405 (low config) to 0.478 (all configs mean) with best config d4_b4 = 0.577. Taxonomic precision reached 1.0±0.0 in d4_b4. Statistical sophistication metric increased from 1.02 to 1.20 across configurations.",
            "success_rate": "Not reported as an explicit percent of 'laws correctly extracted' relative to an independent gold standard; closest reported indicators: taxonomic precision = 1.0 for d4_b4 (perfect on the metric used), and multi-step causal chains were 3.2× more frequent in d4 vs d1, but no external correctness rate for extracted quantitative laws provided.",
            "challenges_limitations": "Limits and failure modes reported include: LLMs can overlook or misuse details in complex reasoning; model-specific generation patterns produce cross-model variability; ROUGE-L underestimates semantic equivalence; reconciling conflicting temporal information across many sources is challenging; high-parameter configurations incur large computational/resource costs; no full external ground-truth validation of quantitative law correctness was performed and risk of hallucination remains (authors note use of ChatGPT only for stylistic edits).",
            "comparison_baseline": "Compared internal configurations (two reasoning models: GPT o3 and GPT o3-mini; depths d∈{1,4}; breadths b∈{1,4}); similarity and quality metrics reported across these. Compared conceptually to prior feed-forward pipelines (PaperQA, PaperQA2, ORKG Ask, LLMs4Synthesis) showing that DeepResearch's recursive approach yields greater source integration and different capability profiles, but no formal head-to-head accuracy benchmark against human experts or quantified baseline extraction-of-laws accuracy was presented.",
            "uuid": "e4198.0",
            "source_info": {
                "paper_title": "DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "ORKG Ask",
            "name_full": "ORKG Ask: a neuro-symbolic scholarly search and exploration system",
            "brief_description": "A scholarly search API/service that provides semantic search over a large index (70+ million publications), returning structured metadata (titles, abstracts, URLs) and serving as a retrieval backend for LLM-based synthesis pipelines.",
            "citation_title": "Orkg ask: a neuro-symbolic scholarly search and exploration system",
            "mention_or_use": "use",
            "system_name": "ORKG Ask (semantic search API)",
            "system_description": "ORKG Ask performs vector/semantic retrieval over a large scholarly corpus and returns structured responses (titles, abstracts, URLs). In DeepResearch, ORKG Ask is configured as the semantic search provider: for each SERP-style query it returns top-ranked publications whose titles and abstracts are merged and passed to the LLM summarization sub-agent for extraction of learnings. ORKG Ask supports evidence-based synthesis by providing curated scholarly inputs rather than raw web text.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Cross-domain scholarly literature (used here for ecological literature)",
            "number_of_papers": "ORKG index &gt;70 million publications (search returns top-k per query; DeepResearch experiments retrieved on average up to ~192.9 sources per synthesis for high-parameter runs)",
            "law_type": "Provides source material (titles/abstracts) enabling extraction of empirical relationships, quantitative results, and reported statistical findings from the literature; not itself an LLM extracting laws.",
            "law_examples": "Not an extractor itself; used to supply documents from which DeepResearch extracted examples such as percent effect sizes and temporal thresholds (see DeepResearch examples).",
            "extraction_method": "Semantic retrieval (vector similarity ranking) returning structured metadata (title, abstract, url); extraction of quantitative relationships is performed downstream by the LLM summarization agent.",
            "validation_approach": "Not described in detail in this paper beyond being evaluated as a retrieval backend; DeepResearch validates downstream synthesis outputs with similarity and domain-specific quality metrics.",
            "performance_metrics": "ORKG Ask provides ranked results; specific retrieval metrics are not reported here. DeepResearch reports downstream metrics (see DeepResearch entry).",
            "success_rate": null,
            "challenges_limitations": "As a retrieval provider, limitations depend on index coverage and returned metadata (abstracts may omit numeric details present in full text); DeepResearch also used Firecrawl when full text was desired.",
            "comparison_baseline": "Used in contrast to Firecrawl (web full-text provider) to illustrate trade-offs between structured scholarly retrieval and broad web search; no formal retrieval baseline comparison reported in this paper.",
            "uuid": "e4198.1",
            "source_info": {
                "paper_title": "DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "PaperQA",
            "name_full": "PaperQA: Retrieval-augmented generative agent for scientific research",
            "brief_description": "A modular agentic pipeline for LLM-assisted scientific question answering that retrieves papers via Google Scholar queries, builds embedding-based chunk databases, and summarizes or filters chunks before prompting an LLM to generate answers.",
            "citation_title": "Paperqa: Retrievalaugmented generative agent for scientific research.",
            "mention_or_use": "mention",
            "system_name": "PaperQA",
            "system_description": "PaperQA retrieves relevant papers (via Google Scholar queries with keywords and year ranges), constructs embedding-based chunk indices, retrieves chunks with maximal marginal relevance, summarizes or marks chunks irrelevant, and feeds the distilled content to an LLM for final answer generation. The pipeline is modular (retrieval, chunking, reranking, summarization, generation) and is cited as prior art in agentic literature synthesis.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General scientific literature (used for question answering and synthesis tasks)",
            "number_of_papers": "Not specified in this paper (PaperQA cited as related work).",
            "law_type": "Designed to support extraction of factual findings and summarized relationships from papers via chunking and LLM summarization; the paper here cites it as related work rather than reporting concrete law extraction examples.",
            "law_examples": "",
            "extraction_method": "Embedding-based chunking and retrieval, contextual summarization of chunks by LLMs, followed by LLM generation for answers. Numeric relationships would be obtained via summarization of retrieved chunks.",
            "validation_approach": "PaperQA is referenced for its modular retrieval-summarization pipeline; this paper does not report its validation details.",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Mentioned as a feedforward pipeline with limited recursion/controllability compared to DeepResearch; potential limitations include chunking and reranking errors and semantic noise.",
            "comparison_baseline": "PaperQA is referenced as prior work to which DeepResearch contrasts its recursive, depth/breadth-controllable approach.",
            "uuid": "e4198.2",
            "source_info": {
                "paper_title": "DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "PaperQA2",
            "name_full": "PaperQA2 (multi-agent scientific synthesis framework)",
            "brief_description": "An extension of PaperQA implementing a multi-agent framework that separates retrieval and generation into specialized agents (paper search, citation traversal, gather-evidence, generation) to support collection, expansion, summarization, and synthesis of scientific literature.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "PaperQA2",
            "system_description": "PaperQA2 decomposes the scientific synthesis workflow into distinct agents: a paper search agent (reformulates queries and fetches PDFs), a citation traversal agent (expands corpus via citation networks), a gather-evidence agent (retrieves and summarizes chunks via dense retrieval and reranking), and a generation agent (synthesizes answers from ranked evidence). This modular multi-agent design is cited as influential background motivating DeepResearch's multi-agent recursive orchestration.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General scientific literature and question answering",
            "number_of_papers": null,
            "law_type": "Supports extraction and summarization of evidence that could include quantitative relationships via chunk summarization; referenced rather than used.",
            "law_examples": "",
            "extraction_method": "Multi-agent retrieval, citation expansion, chunk retrieval + summarization, LLM-based generation.",
            "validation_approach": "Not detailed in this paper; referenced as related system.",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Described as largely feedforward (single-pass) and less explicitly configurable/recursive than DeepResearch.",
            "comparison_baseline": "Used as related work context to highlight differences with DeepResearch's recursive loop and explicit depth/breadth controls.",
            "uuid": "e4198.3",
            "source_info": {
                "paper_title": "DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "LLMs4Synthesis",
            "name_full": "LLMs4Synthesis: Leveraging large language models for scientific synthesis",
            "brief_description": "A framework that structures synthesis tasks into paper-wise, methodological, and thematic categories and uses LLMs for structured synthesis and evaluation (including use of LLM-as-a-judge for assessment).",
            "citation_title": "Llms4synthesis: Leveraging large language models for scientific synthesis.",
            "mention_or_use": "mention",
            "system_name": "LLMs4Synthesis",
            "system_description": "LLMs4Synthesis organizes synthesis tasks into structured categories (paper-wise, methodological, thematic) and uses LLMs to generate syntheses guided by structured prompts. It also explores evaluation approaches such as LLM-as-a-judge and optimization via RL with AI feedback for factuality and clarity. Cited as related work that emphasizes structured, evaluated synthesis using LLMs.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General scientific synthesis applications",
            "number_of_papers": null,
            "law_type": "Supports structured synthesis which could include extraction of quantitative relationships; cited as related methodology rather than demonstrated extraction examples in this paper.",
            "law_examples": "",
            "extraction_method": "Structured prompting and categorization of synthesis tasks; LLM-based summarization and judged evaluation.",
            "validation_approach": "LLM-as-a-judge evaluation and use of reinforcement learning with AI feedback reported in the LLMs4Synthesis work (cited); no direct validation details reported here.",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Mentioned as part of the landscape of synthesis frameworks; not evaluated head-to-head here.",
            "comparison_baseline": "Positioned in related work that informs DeepResearch's design choices around structured outputs and evaluation.",
            "uuid": "e4198.4",
            "source_info": {
                "paper_title": "DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "AI Scientist",
            "name_full": "AI Scientist: Towards fully automated open-ended scientific discovery",
            "brief_description": "An autonomous multi-agent framework proposed to automate steps from idea generation to experimental design and publication writing, demonstrating potential for closed-loop scientific discovery.",
            "citation_title": "The AI Scientist: Towards fully automated open-ended scientific discovery.",
            "mention_or_use": "mention",
            "system_name": "AI Scientist (framework)",
            "system_description": "AI Scientist is described as a framework aiming to automate the end-to-end scientific workflow including ideation, experimental planning, and write-up using agentic components; cited as evidence of research interest in autonomous discovery systems that extend beyond retrieval-and-synthesis to experimental cycles.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General scientific discovery and automation (broad)",
            "number_of_papers": null,
            "law_type": "Proposed to enable discovery of novel scientific relationships in silico, although in this paper AI Scientist is referenced conceptually and no concrete law-extraction examples from that system are provided.",
            "law_examples": "",
            "extraction_method": "Multi-agent autonomous workflows combining idea generation, planning, and experiment design; specifics left to original AI Scientist work.",
            "validation_approach": "Referenced as related work; validation details not provided here.",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Cited to illustrate the frontier of autonomous discovery; not directly evaluated here.",
            "comparison_baseline": "Used to position DeepResearch within the broader landscape of agentic scientific systems that may perform discovery beyond literature synthesis.",
            "uuid": "e4198.5",
            "source_info": {
                "paper_title": "DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
                "publication_date_yy_mm": "2025-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Paperqa: Retrievalaugmented generative agent for scientific research.",
            "rating": 2,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        },
        {
            "paper_title": "Llms4synthesis: Leveraging large language models for scientific synthesis.",
            "rating": 2,
            "sanitized_title": "llms4synthesis_leveraging_large_language_models_for_scientific_synthesis"
        },
        {
            "paper_title": "Orkg ask: a neuro-symbolic scholarly search and exploration system",
            "rating": 2,
            "sanitized_title": "orkg_ask_a_neurosymbolic_scholarly_search_and_exploration_system"
        },
        {
            "paper_title": "The AI Scientist: Towards fully automated open-ended scientific discovery.",
            "rating": 1,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "Language agents achieve superhuman synthesis of scientific knowledge.",
            "rating": 1,
            "sanitized_title": "language_agents_achieve_superhuman_synthesis_of_scientific_knowledge"
        }
    ],
    "cost": 0.01607975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DeepResearch Eco : A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology</p>
<p>Jennifer D'souza jennifer.dsouza@tib.eu 
TIB Leibniz Information Centre for Science and Technology
HannoverGermany</p>
<p>Endres Keno Sander endres.keno.sander@stud.uni-hannover.de 
Leibniz University Hannover
Germany</p>
<p>Andrei Aioanei aaioanei@proton.me 
TIB Leibniz Information Centre for Science and Technology
HannoverGermany</p>
<p>DeepResearch Eco : A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology
88025719B8BD79F0D584FD2916B8CD41
We introduce DeepResearch Eco , a novel agentic LLM-based system for automated scientific synthesis that supports recursive, depth-and breadth-controlled exploration of original research questions-enhancing search diversity and nuance in the retrieval of relevant scientific literature.Unlike conventional retrieval-augmented generation pipelines, DeepResearch enables user-controllable synthesis with transparent reasoning and parameterdriven configurability, facilitating high-throughput integration of domain-specific evidence while maintaining analytical rigor.Applied to 49 ecological research questions, DeepResearch achieves up to a 21-fold increase in source integration and a 14.9-fold rise in sources integrated per 1,000 words.High-parameter settings yield expert-level analytical depth and contextual diversity.Source code available at: https://github.com/sciknoworg/deep-research.</p>
<p>Introduction</p>
<p>Science requires extreme attention to detail, and large language models (LLMs) can overlook or misuse details when faced with challenging reasoning problems [1,2].Ensuring factual accuracy in LLM-generated content has thus become a key challenge.The current paradigm for eliciting factually grounded responses from LLMs is to use retrieval-augmented generation (RAG) [3,4], which supplements the model's knowledge with relevant documents from external sources.By leveraging retrieval, such agentic pipelines can explore scientific literature at a much higher throughput than human scientists-enabling comprehensive surveys that were previously impractical.However, scaling up literature exploration in this manner raises new questions about how to balance breadth (covering many sources) versus depth (deeply analyzing the evidence from each source) to produce high-quality scientific syntheses.</p>
<p>In this work, we introduce DeepResearch Eco , an agentic LLM-based system for complex scientific question answering and literature synthesis, which in this work is tested against research questions in the ecological sciences.DeepResearch employs a recursive retrieval and generation loop guided by explicit, user-controllable depth and breadth parameters.This design enables iterative broad exploration of the topic followed by targeted deep dives, effectively marrying a wide-ranging literature survey with in-depth analysis.Unlike prior feed-forward pipelines, our approach surfaces intermediate reasoning steps (e.g., search subqueries and extracted "learnings") and uses them to refine subsequent queries, yielding a transparent and traceable knowledge workflow.We integrate two variants of LLM reasoning models within this framework to assess the robustness and generality of the generated research reports across different model capabilities.The result is a flexible methodology that can be tuned to either quickly scan numerous publications or rigorously drill down into specific evidence, all within an automated agentic workflow.</p>
<p>Specifically, we explore the following empirically driven research questions in this work: RQ1: How similar are the reports generated by DeepResearch across different depth and breadth settings, using two variants of reasoning models, when evaluated with ROUGE (word-based) and embedding-based semantic similarity metrics?RQ2: How do depth and breadth parameters in automated research systems affect the quality and diversity of synthesized scientific knowledge in ecology?RQ3: Can high-parameter configurations in LLM-based systems achieve domain-specific synthesis capabilities that match or exceed expert-level integration, especially in ecological research contexts?To answer these questions, we conduct extensive experiments using DeepResearch on ecological science problems, analyzing both quantitative metrics and qualitative aspects of the generated outputs.In summary, our contributions are threefold: (1) we present a novel recursive, breadth-vs-depth controllable LLM workflow for automated scientific literature review; (2) we provide an in-depth evaluation of how exploration depth and breadth impact the quality and diversity of knowledge synthesis (finding, for example, that a high-depth configuration can automatically integrate information from 111 sources-nearly 6× more than a shallow setting-and increase coverage of key concepts by 25%); and (3) we demonstrate that carefully configured, highparameter runs can approach expert-level integration in ecology, achieving an order-of-magnitude higher information density in outputs without loss of rigor or specificity.All code and data are released under an open-source MIT license 1 to facilitate reproducibility and future research.</p>
<p>Related Work</p>
<p>Recent developments in LLMs have led to the emergence of agentic workflows for scientific question answering and synthesis.This section reviews key systems that shape the landscape of LLM-based scientific discovery, covering agentic pipelines, human-aligned synthesis frameworks, and multi-agent reasoning systems.Scientific Search and Synthesis.A growing line of work focuses on using LLMs to automate scientific search and synthesis, combining document retrieval with intelligent summarization and reasoning capabilities.</p>
<p>PaperQA [5] introduced a modular agentic pipeline for LLM-assisted scientific question answering.It begins by retrieving relevant papers through Google Scholar using keyword and year-range queries, then constructs an embedding-based chunk database.For each user question, relevant text chunks are retrieved using maximal marginal relevance.These chunks are then summarized or marked as irrelevant, helping mitigate semantic noise and parsing errors.Finally, an LLM generates a response, using its own knowledge and optionally the summarized content.PaperQA2 [1] builds on this model by introducing a full-fledged multi-agent framework.Retrieval and generation are separated into distinct agents: a paper search agent reformulates the user query, fetches PDFs, and converts them to text; a citation traversal agent expands the corpus through citation networks; a gather-evidence agent retrieves and summarizes text chunks via dense retrieval, reranking, and contextual summarization; and a generation agent synthesizes answers from the top-ranked evidence.PaperQA2 also powers use cases beyond question answering, including Wikipedia-style summarization (WikiCrow) and contradiction detection (ContraCrow), the latter benchmarking whether scientific claims contradict prior literature.</p>
<p>ORKG Ask [6] offers a complementary approach rooted in scholarly infrastructure.It combines semantic search over a 70+ million article index (from CORE [7]) with knowledge extraction via LLMs.The search interface returns top-ranked articles using vector similarity (via Nomic embeddings), and the LLM generates a synthesis of the top 5 results.This is augmented by LLMs4Synthesis [8], a framework that structures synthesis tasks into paper-wise, methodological, and thematic categories.Syntheses are generated using structured prompts and evaluated using GPT-4 as an LLM-as-a-judge.This highlights an important emerging research direction: leveraging LLM-as-a-judge [9] as a scalable and effective approach for evaluating scientific tasks.RLAIF (reinforcement learning with AI feedback) is further applied to optimize open-source models (e.g., Mistral-7B) for factuality and clarity.Iterative, Structured, and Human-Aligned Research Workflows.Beyond retrieval and synthesis, another line of research focuses on designing LLM systems that mirror human cognitive workflows-emphasizing iterative refinement, structured reasoning, and alignment with scientific practices.</p>
<p>Nova [10] and IdeaSynth [11] enable iterative refinement of research ideas.Nova leverages planning and information retrieval to diversify generated ideas, addressing the tendency of LLMs to produce repetitive outputs.IdeaSynth organizes ideas as canvas nodes that evolve through literature-grounded feedback loops, facilitating deeper exploration across multiple stages of ideation.Semantic Canvas [12] complements these efforts by introducing constraint-guided input filtering and semantic navigation, which improves output relevance and encourages user engagement.</p>
<p>Other systems focus on structuring the research ideation process.Chain of Ideas (CoI) [13] arranges literature into developmental chains to reflect how research areas evolve over time, supporting progressive insight development.Scideator [14] promotes creativity by recombining research facets-such as purpose, mechanism, and evaluation-using novelty heuristics to suggest original directions.Both systems emphasize structured representations of knowledge to align with how researchers typically generate and refine ideas.Multi-Agent Systems for End-to-End Scientific Discovery.Recent work explores autonomous multi-agent systems that aim to replicate the full scientific workflow-from ideation to publication.Going beyond search and synthesis, fully autonomous multi-agent systems have been proposed to tackle scientific discovery holistically.The AI Scientist framework [15] automates the entire pipeline from idea generation to experimental design and publication writing.VirSci [16] coordinates teams of virtual agents that generate, critique, and revise scientific proposals collaboratively.These systems demonstrate the potential of distributed agentic reasoning and underscore the growing interest in autonomous research systems.These systems underscore the growing interest in distributed agentic reasoning for scientific discovery and highlight the feasibility of closed-loop, autonomous scientific workflows.Positioning DeepResearch.While prior systems like PaperQA2 and ORKG Ask emphasize modularity and scalability, they follow largely feedforward retrieval-to-generation pipelines, often with limited configurability or recursion.In contrast, DeepResearch introduces a recursive, user-controllable exploration loop governed by explicit depth and breadth parameters.One of the essential facets of true deep research is the ability to have recursive calls to repeatedly drill down on the nuances of the question posed by researchers.This enables progressively focused or diversified reasoning, which single-pass architectures do not address.Moreover, DeepResearch surfaces intermediate reasoning steps-such as SERP-style subqueries, structured "learnings," and follow-up questions-enhancing transparency and researcher oversight.Its ability to integrate multiple search modalities and enforce structured, schema-conformant outputs positions it as a flexible tool for both exploratory synthesis and machine-readable knowledge workflows.These distinctions highlight DeepResearch's unique contribution to the emerging paradigm of agentic, iterative, and human-aligned scientific research systems.</p>
<p>Method</p>
<p>Deep Research</p>
<p>The Deep Research system orchestrates a recursive, multi-agent workflow for automated literature exploration and synthesis, as shown in Figure 1.The system is initialized with a user-defined research question and two parameters-breadth and depth-that determine how the exploration unfolds.The breadth parameter controls how many diverse SERP-style queries are generated at each level, allowing the system to branch into multiple directions.The depth parameter governs the number of recursive layers, each of which pushes the investigation deeper by refining queries based on prior learnings.Before execution begins, the environment is configured by selecting an LLM backend and a search client.full-text results in markdown, and ORKG Ask, which queries a scholarly corpus of over 80 million publications to return structured metadata including titles, abstracts, and links.The core loop is composed of four sub-agents.The generate serp queries sub-agent converts the input research question (or a follow-up from a previous round) into a set of search-compatible queries, each accompanied by a research goal.These are passed to the search sub-agent, which retrieves the top results using the selected provider.The results are then processed by the summarize result sub-agent, which merges relevant content (titles and abstracts or full text) and prompts the LLM to generate summary "learnings" and new follow-up questions.This cycle continues until the specified depth is reached.All accumulated insights are then handed off to the generate report sub-agent, which synthesizes the findings into a comprehensive Markdown report, complete with citations and structured using a validated JSON schema.Each sub-agent operates independently but in coordination, and the entire orchestration is governed by a shared system prompt that ensures coherence across the research workflow.</p>
<p>Sub-agents</p>
<p>generate serp queries.This sub-agent takes an actual research question (e.g., "What are the effects of invasive species in grasslands?") and prompts the LLM to generate SERP-style queries-i.e., search engine-compatible (SERP = search engine results page) queries-which are typically: 1) declarative or keyword-based sentences, and 2) optimized for information retrieval rather than naturalness.For example, the original question may yield the query "impact of invasive species on native grassland biodiversity."</p>
<p>On its first invocation, the sub-agent receives the user's research question, optionally enriched with feedback.In subsequent recursive calls, its input consists of the previous research goal and a set of follow-up questions, along with accumulated learnings passed from the summarize result sub-agent.The number of queries generated defaults to 3 but is configurable via the breadth parameter.With each increase in recursion depth, the number of generated queries is halved (using integer division, breadth // 2), thus progressively narrowing the scope of research exploration.Note that at this stage, the LLM is prompted not only to generate SERP-style queries, but also to produce an accompanying research goal for each query, which helps guide subsequent iterations of the research process.</p>
<p>search.This sub-agent executes each SERP-style query using one of two configurable search providers.The first is the Firecrawl API, which performs web-scale search and returns full-text web content in markdown format for up to ten retrieved pages.This mode enables broad coverage of unstructured online sources such as blogs, scientific literature, or news articles.The second is the ORKG Ask API, which queries a scholarly index of over 70 million scientific publications and returns a structured response comprising titles, abstracts, and URLs for the top-ranked results.While Firecrawl supports general-purpose web research, ORKG Ask is optimized for evidence-based synthesis from scientific literature.In both cases, the sub-agent operates asynchronously and executes queries in parallel to maximize efficiency.Retrieved content is passed unfiltered to the summarization sub-agent, and all URLs are retained for transparency and citation in downstream reporting.</p>
<p>summarize result.This sub-agent processes the raw output from the search sub-agent.For each query, it takes the top 10 returned documents (by default) and extracts their textual content.In the case of the Firecrawl provider, this content consists of markdown-formatted full text; for ORKG Ask, it is a combination of publication titles and abstracts.These are merged into a single prompt and passed to the LLM, along with the original query that triggered the search.The LLM is then instructed to produce two outputs: (i) a list of up to 3 "learnings, " meaning concise and information-dense summary insights derived from the content, and (ii) a list of up to 3 follow-up questions for further exploration.Both values are configurable via parameters.These outputs are used to inform recursive querying (generate SERP queries) and accumulate findings for the final report.The agent prompt used for this summarization is shown below.</p>
<p>generate report.This sub-agent synthesizes all accumulated learnings from previous search and summarization rounds into a comprehensive Markdown report.It takes as input the original user research question or, in the case of a recursive call, a composed prompt containing the research goal and follow-up questions.Alongside this prompt, it receives the list of learnings-information-dense insights extracted by the summarize result sub-agent-and the URLs of visited documents.The LLM is instructed to generate a detailed narrative that weaves together all findings, aiming for the length and coherence of a multi-page literature overview.The final report includes a Sources section automatically appended, listing all retrieved document URLs for transparency and traceability.The output is strictly validated against a JSON schema that enforces the presence of a single field: reportMarkdown.The exact prompt passed to the language model is shown below, illustrating how the composed query and accumulated learnings are structured to guide report generation.</p>
<p>Results and Discussion</p>
<p>This section presents our experiments with DeepResearch on ecological research questions, analyzing outcomes both quantitatively and qualitatively.</p>
<p>Experimental Settings</p>
<p>Dataset</p>
<p>We compiled a corpus of 49 ecological research questions from nine fellows of the interdisciplinary group "Mapping Evidence to Theory in Ecology." 2 The questions were collected via a Google Form with prompts such as: Your research question, Relevant ecological sub-discipline, and Purpose of the question.The dataset is publicly available at https://github.com/sciknoworg/deep-research/blob/main/data/49-questions.csv.</p>
<p>The questions span a wide range of ecological sub-domains, including restoration ecology, invasive species management, microbial ecology, and pollination ecology, as well as interdisciplinary areas involving sociology and geology.In terms of intent, 16 questions aim to explore existing hypotheses, another 16 seek to generate new ideas, and 13 aim to collect evidence.A few respondents were motivated by the need for practical insights or broad knowledge overviews.This distribution highlights the exploratory and generative nature of early-stage or interdisciplinary ecological research.</p>
<p>Experimental Setup</p>
<p>We conducted experiments using two OpenAI models: GPT o3 and GPT o3-mini.These models were selected for their ability to produce structured, schema-conformant outputs, supporting fields such as learnings, follow-up questions, and research goals extracted from unstructured LLM responses.Both models are also advertised as reasoning-capable, an essential feature for multi-step scientific synthesis.</p>
<p>The semantic search component is powered by the ORKG Ask API. 3 We evaluated the system across eight configurations defined by a Cartesian product of two reasoning models (o3-mini and o3), two synthesis depths ( ∈ {1, 4})-i.e., the number of recursive synthesis steps, where each step involves one full query-response cycle, and two breadth values ( ∈ {1, 4})-i.e., the number of subqueries issued per step.Each configuration generated 49 structured markdown reports saved with the filename pattern: <index><em><model></em><engine>_d<depth>_b<breadth>.md.All the markdown reports are available at https://github.com/sciknoworg/deep-research/tree/main/data/ecology-reports/orkg-ask.</p>
<p>Quantitative Evaluations</p>
<p>To compare DeepResearch outputs across settings, we align reports generated under different configurations by their shared indices.Let   and   denote two configuration groups (e.g., different model-depth-breadth settings), each containing 50 reports indexed by question ID .We define the aligned subset of indices as:   = {  :  exists in both   and   }.</p>
<p>This ensures that for each  ∈   , the same research question is compared under both configurations.All similarity metrics are computed over these aligned report pairs and averaged across the set   .</p>
<p>Metrics</p>
<p>We assess report similarity using three complementary metrics:</p>
<ol>
<li>
<p>ROUGE-L F 1 .ROUGE-L is a lexical metric that measures the longest common subsequence (LCS) between two texts.It reflects surface-level overlap in word order and phrasing.Given token sequences  and , with ℓ = LCS(, ), the precision, recall, and F 1 score are:
𝑃 LCS = ℓ |𝐴| , 𝑅 LCS = ℓ |𝐵| 𝐹 1 LCS = 2 𝑃 LCS 𝑅 LCS 𝑃 LCS + 𝑅 LCS .
We compute ROUGE-L using the rouge_score library with stemming enabled.While effective for capturing surface similarity, ROUGE-L does not account for paraphrasing or semantic equivalence.</p>
</li>
<li>
<p>BERTScore (SciBERT F 1 ).BERTScore compares token-level embeddings from a pre-trained language model to measure semantic similarity.Using SciBERT, we split each report into chunks of up to 510 tokens, ensuring compatibility with the model's 512-token input limit.For each chunk pair (  ,   ), we compute cosine similarity-based precision, recall, and F 1 :
𝑃 chunk = 1 |𝐴| |𝐴| ∑︁ 𝑖=1 max 𝑗 cos(a 𝑖 , b 𝑗 ), 𝑅 chunk = 1 |𝐵| |𝐵| ∑︁ 𝑗=1 max 𝑖 cos(a 𝑖 , b 𝑗 )
We then average chunk-level F 1 scores across aligned reports to obtain document-level similarity.Unlike ROUGE-L, BERTScore captures paraphrasing and semantic alignment even when wording differs.</p>
</li>
</ol>
<p>Word Mover's Distance (WMD).</p>
<p>WMD computes the minimal cumulative distance required to "transport" words from one document to another in embedding space.Each word is represented by a SciBERT embedding h  , and distances are computed as (,  ′ ) = 1 − cos(h  , h  ′ ).WMD solves the following optimal transport problem:
WMD(𝐴, 𝐵) = min 𝜋∈Π(𝐴,𝐵) ∑︁ 𝑤∈𝐴 ∑︁ 𝑤 ′ ∈𝐵 𝜋(𝑤, 𝑤 ′ ) 𝑑(𝑤, 𝑤 ′ ),
where Π(, ) denotes valid transport plans between the empirical word distributions of  and .</p>
<p>To match our similarity scale, we report 1 − WMD(, ), where higher values indicate greater similarity.Computation is performed using Gensim's WmdSimilarity on precomputed SciBERT embeddings.</p>
<p>Comparison: ROUGE-L emphasizes exact token sequence overlap, BERTScore captures contextual semantic similarity via embedding proximity, and WMD quantifies semantic dissimilarity as the transport cost between word embeddings.Together, these metrics offer a complementary, multi-faceted perspective on report similarity.</p>
<p>Results</p>
<p>To address RQ1-How similar are the reports generated by DeepResearch across different depth and breadth settings, using two variants of reasoning models, when evaluated with ROUGE (word-based) and embedding-based semantic similarity metrics?-we present results in Figure 2. The figure contains three 8×8 heatmaps showing pairwise similarity between the four o3 configurations (rows/columns 1-4) and the four o3-mini configurations (rows/columns 5-8).Each cell reports the average similarity across aligned reports with the same index.Darker shading indicates stronger similarity (higher ROUGE-L or BERTScore; lower WMD).Self-consistency.In all three heatmaps, the main diagonal-where each configuration is compared to itself-is the darkest, reflecting perfect alignment.ROUGE-L F1 and BERTScore F1 are both 1.0, and WMD similarity is also 1.0 (i.e., WMD = 0).This confirms that the similarity metrics behave as expected in the identity case.</p>
<p>Within-model consistency.The upper-left 4×4 block shows consistency across o3 configurations with different depth and breadth settings.BERTScore values average around 0.56, WMD similarity around 0.56, while ROUGE-L is lower, around 0.14.Similarly, the bottom-right 4×4 block for o3-mini configurations shows even higher internal consistency: BERTScore averages around 0.61, WMD similarity around 0.61, and ROUGE-L around 0.16.</p>
<p>The comparatively lower ROUGE-L scores are expected, as ROUGE evaluates surface-level token overlap and does not account for paraphrasing or semantic equivalence.In contrast, BERTScore and WMD rely on contextual embeddings, capturing semantic similarity even when lexical expressions differ.These embedding-based metrics thus better reflect the meaning-preserving variations typical in LLM-generated outputs.</p>
<p>Cross-model similarity.The off-diagonal blocks (rows 1-4 vs. columns 5-8 and vice versa), representing comparisons across o3 and o3-mini, are visibly lighter.Average BERTScore drops to approximately 0.54, WMD similarity to 0.54, and ROUGE-L to 0.12.Even the best-aligned configuration pair-depth 4, breadth 4 for both models-exhibits weaker similarity than within-model comparisons.</p>
<p>Summary.These results indicate that both o3 and o3-mini produce internally consistent outputs across different recursive configurations, with o3-mini showing slightly stronger stability.However, alignment between the two models is consistently weaker, suggesting that model-specific generation patterns persist despite identical prompts and retrieval settings.This highlights the influence of model architecture on the structure and wording of scientific outputs.</p>
<p>Qualitative Evaluations</p>
<p>To systematically evaluate synthesis quality across multiple dimensions, we developed a scoring framework.Our approach builds on existing frameworks for assessing scientific synthesis quality [17,18] while incorporating domain-specific considerations for ecological research.</p>
<p>Theoretical Foundation</p>
<p>Our quality assessment framework is motivated by three key principles from the literature on automated research systems: (i) human alignment (Chain of Ideas [13]), emphasizing depth, breadth, and rigor; (ii) iterative refinement (Nova [10], IdeaSynth [11]), highlighting sophisticated reasoning and broad literature integration; and (iii) collaborative knowledge integration (VirSci [16]), assessing the ability to draw connections across sources.</p>
<p>Metric Design</p>
<p>We assess the deep research generated report quality using six complementary metrics, selected to balance ecological relevance, analytical depth, and scalability for automated evaluation.Each metric targets a distinct quality axis, grounded in identifiable linguistic or structural signals and weighted by domain relevance and signal reliability.Scores are normalized to the [0,1] range using empirical thresholds from our 196-report dataset and aggregated via weighted sums reflecting their relative importance.For each metric, we define the detection strategy, assumptions, normalization scheme, and weight rationale, informed by curated vocabularies and empirical distributions.Research Depth Parameter Assessment.Research depth quantifies the mechanistic sophistication and analytical precision of synthesis outputs, distinguishing surface-level description from processlevel understanding.We define three key components: Mechanistic understanding is assessed via a curated list of 15 ecology-specific process indicators (Appendix A), such as "feedback, " "nutrient cycling, " and "trophic cascade."Matches are counted via case-insensitive substring search.Causal reasoning captures explicit cause-effect statements using predefined connectives ("because," "due to"), result indicators ("results in, " "induces"), and mechanistic verbs ("drives, " "regulates").This reflects an LLM's capacity to reason about ecological processes.Temporal precision measures the proportion of specific temporal references, such as quantified intervals ("within 6 months, " "every 3 years") and dated events ("1990-2020"), identified via regular expressions.</p>
<p>The combined score is:
𝑆_𝑑𝑒𝑝𝑡ℎ = 0.4 • min (︂ 𝑀 _𝑚𝑒𝑐ℎ 20 ,1)︂ + 0.3 • min (︂ 𝑀 _𝑐𝑎𝑢𝑠𝑎𝑙 10 ,1)︂ + 0.3 • 𝑀 _𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙 (1)
Research Breadth Parameter Assessment.Breadth evaluates the diversity of evidence synthesized across spatial, ecological, and methodological axes.It reflects generalizability and the capacity to identify patterns across contexts.We compute five normalized sub-scores: Geographic coverage (  ): count of unique biogeographic zones (e.g., "Tropical, " "Boreal") from a list of 20.Intervention diversity (  ): number of unique management practices matched to a taxonomy of 17 interventions.Biodiversity dimensions (  ): presence of terms related to taxonomic, functional, phylogenetic, and spatial diversity.Ecosystem services (  ): matches against a vocabulary aligned with the Millennium Ecosystem Assessment.Spatial scale (  ): presence of explicit scale terms ("local," "regional," "continental") and area measures.Combined:
𝑆_𝑏𝑟𝑒𝑎𝑑𝑡ℎ =; 0.25 • min (︂ 𝐺_𝑟𝑒𝑔𝑖𝑜𝑛𝑠 8 ,1)︂ + 0.25 • min (︂ 𝐼_𝑡𝑦𝑝𝑒𝑠 12 ,1)︂ + 0.25 • min (︂ 𝐷_𝑑𝑖𝑚𝑠 8 ,1)︂ + 0.15 • min (︂ 𝐸_𝑠𝑒𝑟𝑣𝑖𝑐𝑒𝑠 10 ,1
)︂ + 0.10 • min
(︂ 𝑆_𝑠𝑐𝑎𝑙𝑒𝑠 6 ,1
)︂</p>
<p>Domain-Specific Quality Assessment.This ecology-specific dimension captures alignment with pressing research themes: Conservation focus: frequency of conservation-related terms ("biodiversity, " "restoration, " "habitat loss").Climate relevance: mentions of climate-related terms across scales.Ecological complexity: use of system-level terms ("synergistic, " "nonlinear, " "interconnected").Combined:
𝑆_𝑒𝑐𝑜𝑙𝑜𝑔𝑖𝑐𝑎𝑙 = 0.4•min (︂ 𝐶_𝑐𝑜𝑛𝑠𝑒𝑟𝑣𝑎𝑡𝑖𝑜𝑛 8 ,1)︂ +0.3•min (︂ 𝐶_𝑐𝑙𝑖𝑚𝑎𝑡𝑒 6 ,1)︂ +0.3•min (︂ 𝐸_𝑐𝑜𝑚𝑝𝑙𝑒𝑥𝑖𝑡𝑦 5 ,1)︂(3)
Scientific Rigor Assessment.This metric assesses evidentiary and methodological integrity across three axes: Statistical sophistication detects the use of inferential statistics and analysis techniques, reflecting quantitative depth.Citation practices are evaluated by presence of parenthetical (e.g., "(Smith et al., 2021)") or narrative citations.Uncertainty acknowledgment rewards explicit discussion of limitations ("unknown, " "limited evidence, " "unclear").</p>
<p>Combined score:
𝑆_𝑟𝑖𝑔𝑜𝑟 = 0.4 • min (︂ 𝑅_𝑠𝑡𝑎𝑡𝑖𝑠𝑡𝑖𝑐𝑎𝑙 5 , 1 )︂ + 0.4 • min (︂ 𝐶_𝑓 𝑜𝑟𝑚𝑎𝑙 20 , 1 )︂ + 0.2 • min (︂ 𝑈 _𝑎𝑐𝑘𝑛𝑜𝑤𝑙𝑒𝑑𝑔𝑚𝑒𝑛𝑡 5, 1 )︂ (4)
Innovation Capacity Assessment.We assess novelty using three linguistic signals: Speculative statements use hedging and conjecture ("might," "could," "hypothetical").Novelty indicators include self-declared innovation terms ("novel," "pioneering," "emerging").Gap identification detects explicit acknowledgment of unanswered questions ("research gap, " "understudied").Combined:
𝑆_𝑖𝑛𝑛𝑜𝑣𝑎𝑡𝑖𝑜𝑛 = 0.4 • min (︂ 𝐼_𝑠𝑝𝑒𝑐𝑢𝑙𝑎𝑡𝑖𝑣𝑒 3 ,1)︂ + 0.3 • min (︂ 𝐼_𝑖𝑛𝑑𝑖𝑐𝑎𝑡𝑜𝑟𝑠 3 ,1)︂ + 0.3 • min (︂ 𝐺_𝑟𝑒𝑠𝑒𝑎𝑟𝑐ℎ 3, 1 )︂ (5)
Information Density and Taxonomic Precision.Information density reflects synthesis efficiency:
𝑆_𝑑𝑒𝑛𝑠𝑖𝑡𝑦 = min (︂ 𝑁 _𝑠𝑜𝑢𝑟𝑐𝑒𝑠 𝑊 _𝑐𝑜𝑢𝑛𝑡/1000 • 1 50 ,1
)︂</p>
<p>Together, these dimensions enable a multifaceted, reproducible evaluation of synthesis quality grounded in both ecological expertise and computational feasibility.To facilitate reproducibility, we publicly release our qualitative evaluation pipeline and the accompanying taxonomies at https: //github.com/sciknoworg/deep-research/blob/main/scripts/README.md.</p>
<p>Results</p>
<p>Analysis of 196 syntheses across 49 ecological questions shows that depth and breadth parameters strongly shape synthesis quality, with clear implications for automated research system design.Depth Parameter Effects.In addressing RQ2 on how depth and breadth parameters shape synthesis quality and diversity, we first examine the role of depth in enhancing analytical sophistication.Increasing depth parameters transforms synthesis from surface-level generalizations to mechanistic understanding.Moving from d1 to d4 yields a 5.9-fold increase in source utilization (18.9 to 111.1 sources) without increasing content length, enabling denser, more analytical outputs.</p>
<p>At low depth (d1), syntheses are descriptive but lack causal insight.For example, a grassland analysis notes: "Extensification packages suppress herbage or milk output by 10-40%," reporting outcomes without explanation.In contrast, high-depth (d4) synthesis offers mechanistic accounts: "Nutrient withdrawal shifts competitive hierarchies from fast-growing tall grasses to stress-tolerators by (i) reducing soil NO − 3 and NH + 4 , (ii) decreasing leaf N content, and (iii) opening ground-layer light niches... " -tracing clear ecological pathways and system dynamics.</p>
<p>Research depth is formally assessed via three components that capture analytical sophistication: mechanistic understanding, causal reasoning, and temporal precision.Mechanistic understanding is measured using a curated vocabulary of 15 ecology-specific terms (Appendix A) such as "feedback," "nutrient cycling, " and "energy flow, " detected via case-insensitive substring matching.Causal reasoning is assessed through scientific connectives ("because," "due to," "leads to," "triggers," "regulates," etc.), identifying both simple and multi-step causal explanations.Temporal precision quantifies the ratio of specific time references (e.g., "5-10 years, " "within 6 months") to all temporal mentions, using regular expressions to distinguish precise from vague durations.</p>
<p>Measured via Equation 1, empirical results reflect this assessment: though raw depth scores for d1 and d4 are similar (0.494 vs. 0.500), d4 outputs contain over three times more multi-step causal chains, revealing deeper reasoning.Temporal specificity also improves: low-depth syntheses use vague terms like "several years" or "long-term, " while d4 outputs report concrete thresholds ("5-6 years" for species recovery, "≥10 years" for diversity lags).Although temporal precision scores remain close (0.583 for  1  1 vs. 0.549 for  4  4 ), this reflects the challenge of reconciling more diverse temporal information in high-depth synthesis.The ability of d4 configurations to integrate broader evidence while maintaining precision demonstrates robust synthesis capabilities under information load.</p>
<p>Conclusion and Future Work</p>
<p>In this work, we presented DeepResearch Eco , a recursive, agentic workflow for controllable scientific synthesis, validated on 49 ecological research questions.Increasing depth and breadth parameters improves analytical rigor, evidence diversity, and ecological specificity.For instance, in Question 8-"Is there evidence that climate change and land use interact to alter biodiversity of grasslands?"-the d=1, b=1 report offers a brief generalization, while the d=4, b=4 report integrates cross-regional evidence, mechanistic pathways, and system feedbacks.Similarly, for Question 41-"What is the most common effect of fertilization on grassland plant diversity?"-thed=1, b=1 report notes a general decline, whereas the d=4, b=4 report details competitive shifts, functional group changes, and long-term nutrient effects.These cases exemplify how DeepResearch enables structured, transparent, and expert-like synthesis with tunable analytical control.</p>
<p>Future work will focus on evaluating DeepResearch across additional domains beyond ecology, such as materials science and social science, to further demonstrate its generality and adaptability.We also plan to address current limitations by implementing an interactive agent for researcher feedback integration, enabling guided refinement across recursive steps.Support for multimodal synthesisincluding figures and tables-will be explored to enhance utility in data-rich fields.Finally, we envision collaborative agentic workflows in which multiple agents co-explore subtopics or perspectives, enabling distributed synthesis across teams or disciplines.These enhancements will reinforce our commitment to scalable, human-aligned, and reproducible AI-assisted research.0.8% from d1_b1 to intermediate configurations but jump 16.1% from intermediate to d4_b4, indicating discrete capability transitions rather than smooth improvement curves.Third, the super-linear efficiency indicates that computational investment yields disproportionate returns in synthesis quality, altering the cost-benefit calculations for deploying automated research systems (Figure 5).</p>
<p>B.4. Domain-Specific Quality Validation</p>
<p>Beyond general synthesis metrics, our analysis reveals systematic patterns in ecology-specific quality dimensions that validate the system's domain expertise and demonstrate parameter-dependent specialization capabilities.These domain-specific assessments provide critical evidence that the system achieves not merely generic text synthesis but ecological knowledge integration that scales with computational investment.Conservation focus (a component of   in Equation 3) demonstrates clear parameterdependent variation, with breadth-enhanced configurations achieving superior performance (d1_b4: 9.42 ± 9.73; d4_b4: 9.33 ± 10.00) compared to depth-focused alternatives (d4_b1: 8.67 ± 8.89).This pattern reflects the inherently multi-scale, multi-stakeholder nature of conservation challenges that require integration of diverse management approaches, regional conservation strategies, and crossjurisdictional policy frameworks.The superior performance of breadth-enhanced configurations aligns with conservation biology's need to synthesize evidence across geographic regions, taxonomic groups, and intervention strategies to develop effective preservation strategies.</p>
<p>Climate relevance (another component of   in Equation 3) exhibits progressive enhancement with parameter optimization, increasing systematically from 5.88 ± 6.46 (d1_b1) to 7.33 ± 7.73 (d4_b4), representing a 25% improvement in climate integration capability.This enhancement transcends simple keyword counting to demonstrate cross-domain synthesis, as high-parameter configurations successfully identify and integrate climate considerations across diverse research contexts.The progressive improvement validates that parameter enhancement enables deeper integration of specialized research domains.</p>
<p>Ecosystem services coverage reveals nuanced patterns that illuminate the differential effects of depth versus breadth parameters.Coverage ranges from 1.32 to 1.49 across configurations, with depthenhanced configurations achieving optimal performance (d4_b1: 1.49 ± 1.40).The finding suggests that while breadth helps identify diverse services across systems, depth enables understanding of the mechanisms underlying service generation.</p>
<p>Statistical sophistication (a key component of   in Equation 4) shows progressive enhancement across parameter configurations (1.02 to 1.20), with d4_b4 achieving optimal integration of quantitative research methodologies.This improvement reflects not merely increased detection of statistical terms but enhanced capacity to synthesize quantitative findings across vastly expanded literature sets.The 53% increase in quantitative information density (related to   in Equation 6) from d1_b1 (12.16 ± 9.05) to d4_b4 (18.55 ± 9.43) demonstrates the system's enhanced content analysis capabilities that emerge under high-parameter conditions, enabling the system to identify, extract, and integrate numerical findings that would be overlooked by simpler synthesis approaches.</p>
<p>Taxonomic precision emerges as the most distinctive quality indicator, with d4_b4 configurations achieving perfect performance (1.0 ± 0.0) compared to variable results in other configurations (0.47-0.53).This improvement reflects the system's enhanced capacity to correctly identify and reference specific taxonomic entities when processing comprehensive literature sets.The pattern suggests that taxonomic accuracy benefits from the combination of broad geographic coverage (exposing the system to diverse taxa) and analytical processing (enabling correct taxonomic placement and nomenclature).</p>
<p>Ecological complexity metrics demonstrate stability across parameter configurations despite exponentially increasing source loads, with d4_b4 achieving the highest score (1.22 ± 1.36) while processing 21.2-fold more sources than the baseline.This maintained performance under information loads vali-dates the system's capacity for knowledge integration at scale, suggesting that parameter enhancement enables not just broader coverage but sustained analytical depth even as information complexity increases.</p>
<p>As illustrated in Figure 3, the overall quality score progression from 0.405 to 0.478 across configurations demonstrates consistent enhancement with parameter increases, following a logarithmic improvement pattern with diminishing returns.The d4_b4 configuration achieves an 18% quality improvement over d1_b1, providing empirical justification for the 21.2-fold increase in computational requirements.This cost-benefit relationship, while showing diminishing returns, still validates high-parameter deployment for applications demanding comprehensive, high-quality synthesis outputs, particularly in domains where synthesis quality directly impacts conservation outcomes or policy decisions (Figure 6).</p>
<p>The practical implications of these scaling relationships become clear when examining cost-benefit trade-offs (Figure 6), which reveals that optimal configuration choice depends critically on application requirements and resource constraints.</p>
<p>Figure 1 :
1
Figure 1: Deep Research Orchestration Workflow.The user provides a research question and feedback, along with recursion parameters -breadth (b) and depth (d) -to guide the exploration.The workflow recursively calls four sub-agents: (1) generate serp queries to formulate search-optimized sub-queries and research goals, (2) search to retrieve content from configurable APIs (e.g., ORKG Ask or Firecrawl), (3) summarize result to extract structured learnings and follow-up questions, and (4) generate report to produce a final markdown report.The process iterates until the maximum depth is reached.</p>
<p>Figure 2 :
2
RQ1: Similarity of reports generated by o3-mini and o3 across four depth-breadth settings.Darker cells indicate higher similarity (ROUGE-L/BERTScore) or lower distance (WMD).</p>
<p>Figure 3 :
3
Figure 3: Decomposition of quality improvements across six key dimensions.Error bars represent standard deviations across 49 ecological research questions.The domain-specific score shows strong performance for comprehensive configurations (d4_b4: 0.9+), while information density remains the primary driver of differentiation across configurations.Overall quality scores (composite of all dimensions): best configuration d4_b4 achieved 0.577, with mean across all configurations of 0.511.</p>
<p>Figure 4 :
4
Figure 4: Individual parameter effects on source utilization showing equivalent depth and breadth contributions.Bar chart comparing averaged effects of depth and breadth parameters independently: Depth 1 (18.9 sources), Depth 4 (111.1 sources), Breadth 1 (19.2 sources), and Breadth 4 (110.8sources).Both depth (5.9-fold increase) and breadth (5.8-fold increase) parameters demonstrate nearly identical individual effects when averaged across the complementary parameter.This equivalence indicates that depth and breadth contribute equally to synthesis capability when considered independently, validating the balanced parameter design.The synergistic combination of both parameters (d4_b4: 192.9 sources) exceeds the sum of individual effects, demonstrating super-linear scaling behavior.</p>
<p>Figure 5 :
5
Figure 5: Exponential scaling of synthesis capabilities with depth-breadth parameters.(Left) Source utilization demonstrates super-linear scaling from 9.1 ± 1.7 sources (d1_b1) to 192.9 ± 31.2 sources (d4_b4), representing a 21.2-fold increase.(Center) Word count shows modest 41.5% increase (1,579 to 2,234 words), indicating enhanced information integration rather than verbosity.(Right) Information density (sources per 1,000 words) exhibits 14.9-fold improvement, demonstrating that higher parameter configurations achieve fundamentally superior synthesis efficiency.Error bars represent standard deviations across 49 ecological research questions (n=196 documents total).</p>
<p>Figure 6 :
6
Figure 6: Configuration optimization analysis showing cost-benefit trade-offs and efficiency frontiers.Four-panel analysis of parameter configuration performance: (A) Quality vs. cost trade-off using source count as computational cost proxy, revealing d4_b4's superior quality despite highest resource requirements; (B) Quality efficiency (quality per source) showing d1_b1's highest efficiency for resource-constrained applications; (C) Marginal quality improvement relative to baseline (d1_b1), demonstrating diminishing returns with d4_b4 providing 18% quality improvement; (D) Composite ranking combining quality (70%) and efficiency (30%) weights to identify optimal configurations for different use cases.Analysis based on 196 documents across 49 ecological research questions, providing empirical foundation for resource allocation decisions.</p>
<p>Two search modes are currently supported: Firecrawl, which enables open web search and returns
summarize resultInitial user researchgenerate SERP queriesTitle + Abstract merged from top k resultsGenerate N learnings and N follow-up questionsq. q. or recursion-wise follow-up[user research question &amp; feedback] or [prev. research goal + follow-up questions &amp; learnings][User research1searchquestion] questions] [Feedback2Deep Research depth: d breadth: bQuery available APIs (e.g., ORKG ASK)results Top k</p>
<p>Table 1
1
Source utilization statistics across 49 ecological questions (n=196 documents)
Configuration Mean Sources Std Dev Min Maxd1_b19.11.7010d1_b428.77.01040d4_b129.34.71839d4_b4192.931.293244
https://github.com/sciknoworg/deep-research
https://www.uni-bielefeld.de/einrichtungen/zif/groups/previous/mapping-evidence/
https://api.ask.orkg.org/docs#tag/Semantic-Neural-Search/operation/semantic_search_index_search_get
AcknowledgmentsWe thank the ecologists who participated in our survey and contributed the 49 research questions that underpin this study.All participants were research fellows in the interdisciplinary ZiF research group Mapping Evidence to Theory in Ecology, hosted by the Center for Interdisciplinary Research (ZiF: Zentrum für interdisziplinäre Forschung) at Bielefeld University and led by PI Tina Heger.The first author was a fellow in this group and conducted the survey as part of the ORKG Ask project.More information on the research group is available at https://www.uni-bielefeld.de/einrichtungen/zif/groups/ongoing/mappingevidence/.We also gratefully acknowledge support from the SCINEXT project (BMBF, Grant ID: 01IS22070) and the TIB Leibniz Information Centre for Science and Technology.Breadth Parameter Effects.Breadth expansion shifts synthesis from localized analyses to globally integrated perspectives.Moving from b1 to b4 results in a 5.8-fold increase in source utilization (19.2 to 110.8), demonstrating that breadth parameters expand diversity of evidence without inflating content length.This shift manifests most clearly in geographic coverage.Low-breadth configurations (b1) average 3.7 regions, typically focused on temperate zones in Europe and North America.In contrast, b4 outputs integrate evidence from an average of 4.9 distinct regions across multiple continents-e.g., "North America, Europe, Asia, and Australia"-surfacing biogeographic variation in species response, management effectiveness, and system constraints.Such contextualization enables nuanced recommendations that are otherwise invisible in regionally constrained syntheses.Methodological diversity also improves with breadth.Low-breadth syntheses average 2.6 intervention types, often reflecting single-strategy evaluations.In contrast, high-breadth configurations incorporate an average of 3.2 distinct approaches.For example, a b4 synthesis on Phragmites control evaluates chemical (glyphosate, imazapyr), mechanical (mowing, excavation), biological (goat grazing), and hydrological (salinity manipulation) methods.This comparative framing enhances decision support by revealing trade-offs and synergies across intervention types.Applying the breadth metric (Equation2) reveals broader gains beyond geography and methodology.High-breadth (d4_b4) syntheses exhibit stronger integration of biodiversity dimensions (e.g., combining functional, phylogenetic, and spatial perspectives), more comprehensive treatment of ecosystem services (including provisioning, regulating, and cultural functions), and finer resolution of spatial scale considerations (e.g., from plot-level to continental).These collectively elevate the generalizability and ecological realism of the synthesis.Quantitatively, the breadth score rises from 0.376 (d4_b1) to 0.473 (d4_b4), affirming that breadth enables systematic identification of cross-regional patterns while accounting for boundary conditions.The increase reflects not just a higher number of sources but a richer, more multidimensional integration of evidence, supporting more robust ecological inference and transferable insights for policy and practice.Domain, Rigor, Innovation, and Density Quality Validation. Figure3presents a comprehensive decomposition of quality improvements across all six dimensions, revealing how each component responds to depth-breadth parameter configurations and demonstrating the empirical validation of our multi-dimensional quality framework.To address RQ3-whether high-parameter configurations enable domain-aware synthesis comparable to expert-level integration-we analyzed performance across four advanced quality metrics: domain specificity (  ), scientific rigor (  ), innovation capacity (  ), and information density (  ).The findings support a clear pattern: high-parameter setups (notably d4_b4) consistently outperform lower-depth/breadth configurations across all measures.Domain-specificity metrics reveal that breadth-enhanced configurations (d1_b4, d4_b4) better capture conservation-oriented themes and climate relevance, aligning with the inherently cross-scale nature of ecological policy and management.For example, conservation term frequency rises to 9.33 ± 10.00 in d4_b4, compared to 8.67 ± 8.89 in depth-focused d4_b1-consistent with the weighting of conservation and climate indicators in Equation3. Climate integration shows a 25% gain from d1_b1 to d4_b4, underscoring the role of parameter scaling in cross-domain awareness.Interestingly, ecosystem service coverage peaks in d4_b1, suggesting that depth facilitates mechanistic unpacking of service generation, while breadth ensures representational completeness.Rigorous synthesis practices also improve with parameter scaling.Statistical sophistication, a key component of   (Equation4), increases from 1.02 to 1.20 across configurations, reflecting greater incorporation of inferential analysis.Citation quality and uncertainty acknowledgment co-evolve, resulting in robust evidence presentation that mirrors academic standards.Innovation capacity (Equation5) benefits from enhanced parameterization through more frequent identification of knowledge gaps and speculative framing-signals that often underpin novel research trajectories.The most pronounced efficiency gain, however, lies in information density (Equation6), which improves 14.9-fold from d1_b1 to d4_b4 despite only modest word count increases.This validates that high-parameter configurations not only scale information volume but also preserve analytical quality and specificity.Taken together, these results confirm that LLM-based systems, when carefully configured, can approximate expert-levelDeclaration on Generative AIChatGPT was used solely to support stylistic refinement and selective text shortening.All original text and substantive content were authored by the three co-authors.A. Domain-Specific Vocabulary for Quality AssessmentThe following vocabulary was used for automated detection of ecology-specific concepts in our quality assessment framework.Terms were selected based on frequency analysis of high-impact ecology papers, expert consultation, and validation against ecology textbook indices.A.1. Mechanistic Terms"mechanism", "pathway", "feedback", "trophic", "nutrient cycling", "energy flow", "predation", "competition", "mutualism", "succession", "disturbance", "resilience", "adaptation", "selection pressure", "gene flow", "decomposition", "mineralization", "nitrification", "photosynthesis", "respiration", "herbivory", "facilitation", "inhibition"A.2. Management Interventions"fertilizer", "stocking", "mowing", "grazing", "irrigation", "organic", "controlled burn", "restoration", "reforestation", "afforestation", "rewilding", "habitat creation", "invasive species control", "predator control", "captive breeding", "protected area", "translocation"The complete vocabulary is available in machine-readable JSON format at: https://github.com/sciknoworg/deep-research/blob/main/scripts/vocab/ecology_dictionaries.jsonB. Detailed Qualitative AnalysisB.1. Depth Parameter EffectsAnalysis of depth parameter variation reveals a fundamental transition in analytical sophistication that transcends simple quantitative scaling.Enhancement from d1 to d4 produces a 5.9-fold increase in average source utilization (from 18.9 to 111.1 sources) while maintaining comparable content length, indicating that depth parameters enable qualitatively different synthesis modes characterized by enhanced analytical penetration rather than mere scope expansion.The transformation in mechanistic understanding proves most evident when comparing synthesis outputs across depth levels.Low-depth configurations (d1) typically provide broad generalizations with minimal mechanistic detail, employing descriptive language that reports empirical relationships without explaining underlying processes.A representative d1 grassland analysis exemplifies this pattern: "Extensification packages suppress herbage or milk output by 10-40%."While this statement provides useful quantitative information about management outcomes, it offers no insight into the causal pathways or ecological mechanisms driving these effects.In stark contrast, high-depth configurations (d4) deliver comprehensive mechanistic frameworks that explicitly trace causal sequences from initial interventions through intermediate processes to ultimate outcomes.The same grassland management question addressed at d4 provides detailed process-level explanations: "Nutrient withdrawal shifts competitive hierarchies from fast-growing tall grasses toward stress-tolerators by: (i) reducing soil NO − 3 and NH + 4 availability, (ii) decreasing leaf N content and photosynthetic capacity in dominants, (iii) opening ground-layer light niches through reduced canopy closure, enabling germination of small-seeded forbs." This progression from empirical observation to mechanistic understanding represents a qualitative shift in synthesis capability, with d4 documents consistently capturing biochemical pathways, ecological feedbacks, and system dynamics that remain entirely implicit or absent in lower-depth analyses.Temporal precision emerges as another critical differentiator across depth levels.Low-depth syntheses employ vague temporal descriptors such as "several years, " "long-term, " or "historically, " providing little guidance for practical implementation or hypothesis testing.High-depth configurations transform this temporal vagueness into precise quantitative thresholds essential for ecological management and prediction.D4 syntheses consistently specify exact timeframes: species richness recovery occurs within "5-6 years, " functional diversity lags require "≥10 years, " and diversity-productivity trade-offs emerge at "ca. 18-22 years." This precision extends beyond simple duration reporting to include process-specific temporal sequences, seasonal timing requirements, and critical intervention windows.The apparent stability in temporal precision metrics (0.583 for  1  1 versus 0.549 for  4  4 , as measured by the temporal component in Equation1) initially seems counterintuitive but reflects a phenomenon: as source integration increases 21-fold, maintaining comparable precision becomes increasingly challenging due to the need to reconcile conflicting temporal information across diverse studies.This pattern indicates that high-depth configurations successfully integrate temporal information from expanded source sets while preserving precision, demonstrating robust temporal synthesis capabilities under high information loads.Causal reasoning sophistication shows marked enhancement with depth parameter increases.While d1 and d4 configurations achieve similar raw depth scores (0.494 versus 0.500), calculated using Equation1), qualitative analysis reveals that d4 documents contain 3.2 times more multi-step causal sequences, linking distal causes through proximate mechanisms to ultimate ecological outcomes.This multiplication of causal chains indicates not merely more causal statements but fundamentally more sophisticated causal reasoning that captures the complex, indirect pathways characteristic of ecological systems.The depth enhancement enables synthesis outputs to move beyond simple cause-effect pairs to construct integrated causal networks that better represent ecological reality.B.2. Breadth Parameter EffectsBreadth parameter drives a systematic expansion from geographically and methodologically constrained analyses to globally comprehensive syntheses that capture the full spectrum of ecological variation.Quantitative analysis shows that progression from b1 to b4 produces a 5.8-fold increase in source utilization (from 19.2 to 110.8 sources on average) while maintaining proportional content expansion, indicating that breadth parameters facilitate the integration of diverse evidence rather than superficial coverage expansion.Geographic coverage transformation represents the most immediately evident manifestation of breadth enhancement.Low-breadth configurations (b1) typically focus on specific biogeographic regions, averaging 3.7 geographic regions, with a heavy emphasis on well-studied European or North American temperate systems.These syntheses often present detailed insights into regional management practices but offer limited applicability beyond their focal geography.The concentration on familiar systems reflects both source availability bias and the computational constraints of limited breadth parameters that prevent comprehensive geographic integration.High-breadth configurations (b4) achieve substantially enhanced global perspective, with geographic coverage expanding to an average of 4.9 regions while systematically incorporating evidence from multiple continents.A representative b4 synthesis demonstrates this transformation by integrating findings from "North America, Europe, Asia, and Australia," explicitly recognizing biogeographic variation in species responses, management effectiveness, and ecological constraints.This expanded geographic scope enables identification of context-dependencies and boundary conditions that remain entirely invisible in regionally-focused analyses, providing managers with nuanced understanding of when and where specific interventions prove effective.Methodological diversity shows parallel enhancement with breadth parameters.Low-breadth syntheses average only 2.6 intervention types, typically focusing on single management approaches or closely related intervention clusters.This methodological constraint limits the ability to compare alternative strategies or identify optimal intervention combinations.High-breadth configurations expand intervention coverage to 3.2 categories on average, systematically integrating diverse management philosophies and implementation approaches.A representative b4 Phragmites management synthesis exemplifies this comprehensiveness by evaluating "chemical (glyphosate, imazapyr), mechanical (mowing, excavation), biological (goat grazing), and hydrological (salinity manipulation) control methods," providing practitioners with comparative assessment across the intervention spectrum rather than advocacy for single approaches.The breadth enhancement particularly influences synthesis generalizability through systematic identification of context-dependencies and biogeographic patterns.Higher breadth configurations consistently achieve superior breadth scores (0.473 for d4_b4 versus 0.376 for d4_b1, calculated using Equation2), reflecting enhanced capacity to identify cross-regional patterns while explicitly acknowledging boundary conditions and regional variations.This dual capability-recognizing both generalities and exceptions-proves essential for developing robust management recommendations that maintain validity across diverse implementation contexts.B.3. Source Utilization and Synthesis EfficiencyThe relationship between parameter configuration and synthesis capability follows a precise power law ( 2 = 0.97), revealing systematic scaling properties that transcend simple linear effects.Our analysis of 196 synthesis documents demonstrates that maximum parameter configurations (d4_b4) achieve a 21.2-fold increase in source utilization compared to baseline (d1_b1), with mean source counts escalating from 9.1 ± 1.7 to 192.9 ± 31.2 sources per synthesis.This exponential scaling pattern indicates that higher parameter configurations enable qualitatively different modes of information integration.The efficiency implications prove particularly striking when examining the relationship between source utilization and content generation.While source utilization increases 21.2-fold, word count expands only 41.5% (from 1,579 to 2,234 words), yielding a dramatic 14.9-fold enhancement in information density measured as sources per 1,000 words (Equation6).This disproportionate scaling demonstrates that parameter enhancement drives analytical depth rather than content inflation, with high-parameter configurations achieving superior knowledge integration while maintaining proportional document length.Three critical characteristics define this scaling behavior.First, the relationship exhibits consistency across ecological domains, with coefficients of variation remaining stable (0.16-0.19) across all 50 research questions spanning grassland management, invasive species control, and biodiversity conservation.This domain-independent scaling suggests that the observed patterns reflect fundamental properties of the synthesis system rather than artifacts of particular research areas.Second, the scaling demonstrates clear threshold effects, with minimal improvements occurring until both parameters exceed moderate values ( ≥ 2,  ≥ 2), after which dramatic gains materialize.Overall quality scores increase by only
M D Skarlinski, S Cox, J M Laurent, J D Braza, M Hinks, M J Hammerling, M Ponnapati, S G Rodriques, A D White, arXiv:2409.13740Language agents achieve superhuman synthesis of scientific knowledge. 2024arXiv preprint</p>
<p>Large legal fictions: Profiling legal hallucinations in large language models. M Dahl, V Magesh, M Suzgun, D E Ho, Journal of Legal Analysis. 162024</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W -T. Yih, T Rocktäschel, Advances in neural information processing systems. 332020</p>
<p>Retrieval augmentation reduces hallucination in conversation. K Shuster, S Poff, M Chen, D Kiela, J Weston, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>
<p>J Lála, O O'donoghue, A Shtedritski, S Cox, S G Rodriques, A D White, 10.48550/arXiv.2312.07559arXiv:2312.07559Paperqa: Retrievalaugmented generative agent for scientific research. 2023arXiv preprint</p>
<p>Orkg ask: a neuro-symbolic scholarly search and exploration system. A Oelen, M Y Jaradeh, S Auer, arXiv:2412.049772024arXiv preprint</p>
<p>Core: A global aggregation service for open access papers. P Knoth, D Herrmannova, M Cancellieri, L Anastasiou, N Pontika, S Pearce, B Gyawali, D Pride, Scientific Data. 103662023</p>
<p>Llms4synthesis: Leveraging large language models for scientific synthesis. H Babaei Giglou, J Souza, S Auer, Proceedings of the 24th ACM/IEEE Joint Conference on Digital Libraries. the 24th ACM/IEEE Joint Conference on Digital Libraries2024</p>
<p>Yescieval: Robust llm-as-a-judge for scientific question answering. J Souza, H B Giglou, Q Münch, arXiv:2505.142792025arXiv preprint</p>
<p>X Hu, H Fu, J Wang, Y Wang, Z Li, R Xu, Y Lu, Y Jin, L Pan, Z Lan, arXiv:2410.14255An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Nova2024arXiv preprint</p>
<p>K Pu, K Feng, T Grossman, T Hope, B D Mishra, M Latzke, J Bragg, J C Chang, P Siangliulue, arXiv:2410.04025Ideasynth: Iterative research idea development through evolving and composing idea facets with literature-grounded feedback. 2024arXiv preprint</p>
<p>T Sandholm, S Dong, S Mukherjee, J Feland, B A Huberman, arXiv:2411.03575Semantic navigation for ai-assisted ideation. 2024arXiv preprint</p>
<p>L Li, W Xu, J Guo, R Zhao, X Li, Y Yuan, B Zhang, Y Jiang, Y Xin, R Dang, arXiv:2410.13185Chain of ideas: Revolutionizing research via novel idea development with llm agents. 2024arXiv preprint</p>
<p>Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. M Radensky, S Shahid, R Fok, P Siangliulue, T Hope, D S Weld, arXiv:2409.146342024arXiv preprint</p>
<p>C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2408.06292The AI Scientist: Towards fully automated open-ended scientific discovery. 2024arXiv preprint</p>
<p>H Su, R Chen, S Tang, X Zheng, J Li, Z Yin, W Ouyang, N Dong, arXiv:2410.09403Two heads are better than one: A multi-agent system has the potential to improve scientific idea generation. 2024arXiv preprint</p>
<p>An effective framework for measuring the novelty of scientific articles through integrated topic modeling and cloud model. Z Wang, H Zhang, J Chen, H Chen, Journal of Informetrics. 181015872024</p>
<p>A practical guide to question formation, systematic searching and study screening for literature reviews in ecology and evolution. Y Z Foo, R E O'dea, J Koricheva, S Nakagawa, M Lagisz, 10.1111/2041-210X.13654Methods in Ecology and Evolution. 122021</p>            </div>
        </div>

    </div>
</body>
</html>
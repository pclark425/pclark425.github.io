<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5314 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5314</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5314</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-265610018</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.01276v2.pdf" target="_blank">Toward best research practices in AI Psychology</a></p>
                <p><strong>Paper Abstract:</strong> Language models have become an essential part of the burgeoning field of AI Psychology. I discuss 14 methodological considerations that can help design more robust, generalizable studies evaluating the cognitive abilities of language-based AI systems, as well as to accurately interpret the results of these studies.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5314.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5314.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WSC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Winograd Schema Challenge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of minimally different sentence pairs in which pronoun referents are resolved using commonsense/world knowledge rather than simple syntactic cues; designed to probe commonsense reasoning in language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Winograd Schema Challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLMs (various reported in the literature)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained language models referenced collectively in the literature; the paper does not specify architectures, training corpora, or exact models for the reported WSC results.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Winograd Schema Challenge (WSC)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>language / commonsense reasoning / world knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Pairs of sentences that differ minimally such that a pronoun's antecedent must be inferred using background/world knowledge (e.g., 'The trophy did not fit in the suitcase because it was too small' → 'suitcase'). Items are intended to be immune to simple syntactic/selectional heuristics and dataset memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported in this paper: by 2020 LLMs achieved over 90% accuracy on the original WSC items (citation given in the paper). The paper emphasizes this high performance was later shown to be inflated by heuristic shortcuts and dataset contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>The paper reports high LLM accuracy (>90%) on the WSC as reported in prior work, but cautions that this does not necessarily reflect deep human-like understanding because of dataset artifacts and heuristics; no direct, controlled human-vs-model quantitative comparison is provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>The paper highlights major limitations: many WSC items can be solved by selectional restriction or frequency/association heuristics rather than genuine commonsense reasoning; many original items are publicly available so models can memorize them; >10% of original items had simple association-based solutions; thus LLM success on the WSC can be driven by shortcuts, dataset contamination, and frequency artifacts rather than robust world knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward best research practices in AI Psychology', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5314.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5314.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WinoGrande</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WinoGrande (large Winograd-style dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large, crowd-sourced Winograd-style dataset that was automatically filtered (using language model embeddings) to reduce simple co-occurrence/association solutions and scale up the number of items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An adversarial winograd schema challenge at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLMs (evaluated in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large language models evaluated by the WinoGrande authors and subsequent work; architectures and sizes are not specified in this review paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>WinoGrande (Winograd-style large dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>language / commonsense reasoning / world knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Thousands of Winograd-like items created via crowd-sourcing and then filtered automatically to reduce high co-occurrence associations (estimated via embeddings) so that simple association heuristics would be less effective.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>The paper reports that the embedding-based filtering used in WinoGrande substantially reduced model performance compared to performance on the original WSC, although this review does not provide exact numeric scores.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Filtering that reduced association cues led to lower LLM accuracy versus earlier reported high scores on the small original WSC, suggesting earlier apparent model success was partially due to dataset artifacts; no direct quantitative human comparisons are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>WinoGrande's scale improved generalizability but introduced quality-control problems (typos, grammatical errors, unwarranted assumptions). The paper emphasizes a quality vs. quantity tradeoff: larger datasets reduce memorization but can introduce noisy or flawed items that complicate interpretation of model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward best research practices in AI Psychology', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5314.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5314.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elazar baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline analyses and artifact detection in the Winograd schema (Elazar et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analyses that introduce baseline conditions (e.g., removing candidate referents or the first clause) to test whether model preferences derive from simple frequency/association biases rather than context-sensitive reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Back to square one: Artifact detection, training and commonsense disentanglement in the Winograd schema.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLMs (evaluated in Elazar et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained language models analyzed for artifact-driven preferences; the review does not specify exact model names or sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Winograd Schema baseline conditions (reduced-sentence baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>diagnostic checks for language/commonsense reasoning tests</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Baseline manipulations such as removing candidate referents or removing the first clause so that, if models rely on associative frequency rather than context, they will still show a preference for one referent in these reduced contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Elazar et al. (as reported in this paper) found that LLMs performed above chance on these baseline (reduced) sentences, indicating the presence of association/frequency biases; numeric values are not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>The baseline results suggest model preferences in the WSC are partially driven by frequency/association artifacts rather than contextual commonsense reasoning; the paper uses this to argue that apparent high LLM accuracy overestimates genuine reasoning ability.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>These baseline analyses reveal that LLMs can exploit spurious dataset artifacts (frequency/co-occurrence patterns) to get correct answers even when critical contextual information is removed, demonstrating a qualitative difference from the intended human-like inference process the test targets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward best research practices in AI Psychology', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5314.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5314.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>General LLM cognitive evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>General mentions of cognitive psychology tests applied to LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper surveys the rapid growth of AI Psychology, noting many studies that evaluate LLMs on tasks such as working memory, logical reasoning, planning, social reasoning, creativity, and personality, but it does not provide detailed quantitative results for these evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Collective reference to large language models tested across diverse cognitive assessments; specific model architectures, training data, and sizes are not specified in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Various (working memory, logical reasoning, planning, social reasoning, creativity, personality assessments, vision-language tests)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>memory, reasoning, planning, social cognition, creativity, personality, multimodal perception</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>A range of cognitive assessments adapted from human psychological tests to probe different cognitive abilities in LLMs; administration varies by study and task (e.g., language-only prompts, image+text prompts for vision-language models).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>The paper notes many such evaluations have been run and reported in the literature, but does not supply model-level quantitative results within this review (no specific accuracy/scores are provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>The review urges that model-to-human comparisons should recreate similar testing conditions and that direct comparisons often require collecting human data for those exact items; it does not present direct quantitative comparisons for these varied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>The paper emphasizes methodological caveats across these evaluations: training-data contamination, shortcut strategies available to LLMs (statistical co-occurrence, grammatical cues), issues with prompt/culture effects, reliance on well-known test items, and the need for control conditions—i.e., qualitative differences in how LLMs and humans may achieve similar surface-level performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward best research practices in AI Psychology', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The Winograd Schema Challenge. <em>(Rating: 2)</em></li>
                <li>An adversarial winograd schema challenge at scale. <em>(Rating: 2)</em></li>
                <li>Back to square one: Artifact detection, training and commonsense disentanglement in the Winograd schema. <em>(Rating: 2)</em></li>
                <li>How reasonable are commonsense reasoning tasks: A case-study on the Winograd schema challenge and SWAG. <em>(Rating: 1)</em></li>
                <li>The defeat of the winograd schema challenge. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5314",
    "paper_id": "paper-265610018",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "WSC",
            "name_full": "Winograd Schema Challenge",
            "brief_description": "A set of minimally different sentence pairs in which pronoun referents are resolved using commonsense/world knowledge rather than simple syntactic cues; designed to probe commonsense reasoning in language.",
            "citation_title": "The Winograd Schema Challenge.",
            "mention_or_use": "mention",
            "model_name": "unspecified LLMs (various reported in the literature)",
            "model_description": "Large pre-trained language models referenced collectively in the literature; the paper does not specify architectures, training corpora, or exact models for the reported WSC results.",
            "model_size": null,
            "cognitive_test_name": "Winograd Schema Challenge (WSC)",
            "cognitive_test_type": "language / commonsense reasoning / world knowledge",
            "cognitive_test_description": "Pairs of sentences that differ minimally such that a pronoun's antecedent must be inferred using background/world knowledge (e.g., 'The trophy did not fit in the suitcase because it was too small' → 'suitcase'). Items are intended to be immune to simple syntactic/selectional heuristics and dataset memorization.",
            "llm_performance": "Reported in this paper: by 2020 LLMs achieved over 90% accuracy on the original WSC items (citation given in the paper). The paper emphasizes this high performance was later shown to be inflated by heuristic shortcuts and dataset contamination.",
            "human_baseline_performance": null,
            "performance_comparison": "The paper reports high LLM accuracy (&gt;90%) on the WSC as reported in prior work, but cautions that this does not necessarily reflect deep human-like understanding because of dataset artifacts and heuristics; no direct, controlled human-vs-model quantitative comparison is provided in this paper.",
            "notable_differences_or_limitations": "The paper highlights major limitations: many WSC items can be solved by selectional restriction or frequency/association heuristics rather than genuine commonsense reasoning; many original items are publicly available so models can memorize them; &gt;10% of original items had simple association-based solutions; thus LLM success on the WSC can be driven by shortcuts, dataset contamination, and frequency artifacts rather than robust world knowledge.",
            "uuid": "e5314.0",
            "source_info": {
                "paper_title": "Toward best research practices in AI Psychology",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "WinoGrande",
            "name_full": "WinoGrande (large Winograd-style dataset)",
            "brief_description": "A large, crowd-sourced Winograd-style dataset that was automatically filtered (using language model embeddings) to reduce simple co-occurrence/association solutions and scale up the number of items.",
            "citation_title": "An adversarial winograd schema challenge at scale.",
            "mention_or_use": "mention",
            "model_name": "unspecified LLMs (evaluated in cited work)",
            "model_description": "Large language models evaluated by the WinoGrande authors and subsequent work; architectures and sizes are not specified in this review paper.",
            "model_size": null,
            "cognitive_test_name": "WinoGrande (Winograd-style large dataset)",
            "cognitive_test_type": "language / commonsense reasoning / world knowledge",
            "cognitive_test_description": "Thousands of Winograd-like items created via crowd-sourcing and then filtered automatically to reduce high co-occurrence associations (estimated via embeddings) so that simple association heuristics would be less effective.",
            "llm_performance": "The paper reports that the embedding-based filtering used in WinoGrande substantially reduced model performance compared to performance on the original WSC, although this review does not provide exact numeric scores.",
            "human_baseline_performance": null,
            "performance_comparison": "Filtering that reduced association cues led to lower LLM accuracy versus earlier reported high scores on the small original WSC, suggesting earlier apparent model success was partially due to dataset artifacts; no direct quantitative human comparisons are reported here.",
            "notable_differences_or_limitations": "WinoGrande's scale improved generalizability but introduced quality-control problems (typos, grammatical errors, unwarranted assumptions). The paper emphasizes a quality vs. quantity tradeoff: larger datasets reduce memorization but can introduce noisy or flawed items that complicate interpretation of model performance.",
            "uuid": "e5314.1",
            "source_info": {
                "paper_title": "Toward best research practices in AI Psychology",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Elazar baselines",
            "name_full": "Baseline analyses and artifact detection in the Winograd schema (Elazar et al.)",
            "brief_description": "Analyses that introduce baseline conditions (e.g., removing candidate referents or the first clause) to test whether model preferences derive from simple frequency/association biases rather than context-sensitive reasoning.",
            "citation_title": "Back to square one: Artifact detection, training and commonsense disentanglement in the Winograd schema.",
            "mention_or_use": "mention",
            "model_name": "unspecified LLMs (evaluated in Elazar et al.)",
            "model_description": "Pre-trained language models analyzed for artifact-driven preferences; the review does not specify exact model names or sizes.",
            "model_size": null,
            "cognitive_test_name": "Winograd Schema baseline conditions (reduced-sentence baselines)",
            "cognitive_test_type": "diagnostic checks for language/commonsense reasoning tests",
            "cognitive_test_description": "Baseline manipulations such as removing candidate referents or removing the first clause so that, if models rely on associative frequency rather than context, they will still show a preference for one referent in these reduced contexts.",
            "llm_performance": "Elazar et al. (as reported in this paper) found that LLMs performed above chance on these baseline (reduced) sentences, indicating the presence of association/frequency biases; numeric values are not provided in this review.",
            "human_baseline_performance": null,
            "performance_comparison": "The baseline results suggest model preferences in the WSC are partially driven by frequency/association artifacts rather than contextual commonsense reasoning; the paper uses this to argue that apparent high LLM accuracy overestimates genuine reasoning ability.",
            "notable_differences_or_limitations": "These baseline analyses reveal that LLMs can exploit spurious dataset artifacts (frequency/co-occurrence patterns) to get correct answers even when critical contextual information is removed, demonstrating a qualitative difference from the intended human-like inference process the test targets.",
            "uuid": "e5314.2",
            "source_info": {
                "paper_title": "Toward best research practices in AI Psychology",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "General LLM cognitive evaluations",
            "name_full": "General mentions of cognitive psychology tests applied to LLMs",
            "brief_description": "The paper surveys the rapid growth of AI Psychology, noting many studies that evaluate LLMs on tasks such as working memory, logical reasoning, planning, social reasoning, creativity, and personality, but it does not provide detailed quantitative results for these evaluations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "various LLMs (unspecified)",
            "model_description": "Collective reference to large language models tested across diverse cognitive assessments; specific model architectures, training data, and sizes are not specified in this review.",
            "model_size": null,
            "cognitive_test_name": "Various (working memory, logical reasoning, planning, social reasoning, creativity, personality assessments, vision-language tests)",
            "cognitive_test_type": "memory, reasoning, planning, social cognition, creativity, personality, multimodal perception",
            "cognitive_test_description": "A range of cognitive assessments adapted from human psychological tests to probe different cognitive abilities in LLMs; administration varies by study and task (e.g., language-only prompts, image+text prompts for vision-language models).",
            "llm_performance": "The paper notes many such evaluations have been run and reported in the literature, but does not supply model-level quantitative results within this review (no specific accuracy/scores are provided here).",
            "human_baseline_performance": null,
            "performance_comparison": "The review urges that model-to-human comparisons should recreate similar testing conditions and that direct comparisons often require collecting human data for those exact items; it does not present direct quantitative comparisons for these varied tasks.",
            "notable_differences_or_limitations": "The paper emphasizes methodological caveats across these evaluations: training-data contamination, shortcut strategies available to LLMs (statistical co-occurrence, grammatical cues), issues with prompt/culture effects, reliance on well-known test items, and the need for control conditions—i.e., qualitative differences in how LLMs and humans may achieve similar surface-level performance.",
            "uuid": "e5314.3",
            "source_info": {
                "paper_title": "Toward best research practices in AI Psychology",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The Winograd Schema Challenge.",
            "rating": 2,
            "sanitized_title": "the_winograd_schema_challenge"
        },
        {
            "paper_title": "An adversarial winograd schema challenge at scale.",
            "rating": 2,
            "sanitized_title": "an_adversarial_winograd_schema_challenge_at_scale"
        },
        {
            "paper_title": "Back to square one: Artifact detection, training and commonsense disentanglement in the Winograd schema.",
            "rating": 2,
            "sanitized_title": "back_to_square_one_artifact_detection_training_and_commonsense_disentanglement_in_the_winograd_schema"
        },
        {
            "paper_title": "How reasonable are commonsense reasoning tasks: A case-study on the Winograd schema challenge and SWAG.",
            "rating": 1,
            "sanitized_title": "how_reasonable_are_commonsense_reasoning_tasks_a_casestudy_on_the_winograd_schema_challenge_and_swag"
        },
        {
            "paper_title": "The defeat of the winograd schema challenge.",
            "rating": 1,
            "sanitized_title": "the_defeat_of_the_winograd_schema_challenge"
        }
    ],
    "cost": 0.009824749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TOWARD BEST RESEARCH PRACTICES IN AI PSYCHOLOGY A PREPRINT
October 30, 2024</p>
<p>Anna Ivanova a.ivanova@gatech.edu 
Georgia Institute of Technology</p>
<p>TOWARD BEST RESEARCH PRACTICES IN AI PSYCHOLOGY A PREPRINT
October 30, 2024F4B28D4374D0E031FF12D2E28C3E2246arXiv:2312.01276v2[cs.AI]
Language models have become an essential part of the burgeoning field of AI Psychology.I discuss 14 methodological considerations that can help design more robust, generalizable studies evaluating the cognitive abilities of language-based AI systems, as well as to accurately interpret the results of these studies.</p>
<p>Cognitive evaluations of AI models</p>
<p>Ever since the Turing test, the idea of having a dialogue with a machine to probe its cognitive abilities ("thought") has been inextricably associated with the field of artificial intelligence (AI).In addition to its intuitive simplicity, this idea naturally aligns with everyday practice in psychology: language-based assessments are the bread and butter of many psychologists' toolkits.If researchers want to know what is happening in the mind of a human, the easiest approach is to ask.Today, advances in linguistic abilities of large language models (LLMs)-and AI systems that might incorporate LLMs as one of their components-make it possible to seamlessly test these models on languagebased assessments originally designed for people.This is an unprecedented advance: to date, the only entities who could flexibly use human language were, well, human.Now, however, we are faced with artificial systems that can process linguistic information, generate novel texts, and respond to questions.How do we assess the cognitive capabilities of these systems?Easy access to chat-based LLM interfaces (such as ChatGPT) makes it possible for anyone to run a "cognitive test" on an AI system.This advance has led to an explosive growth of AI psychology, with papers assessing LLMs' working memory capacity, logical reasoning, planning abilities, social reasoning, creativity, and even personality traits.Advances in vision-language models now also make it possible to test AI systems on cognitive assessments that incorporate pictures and videos.</p>
<p>Although running such assessments can be fairly straightforward, interpreting the results is not.In fact, AI Psychology today is faced with a plethora of methodological questions.What factors should we consider when assessing model performance?How can we adapt our stimuli to reduce the prevalence of "hacks", i.e., heuristics that the model might employ to achieve high task performance without using the cognitive skill being assessed?If a model passes a human test for a cognitive ability X, does it mean that it indeed possesses X?</p>
<p>To begin answering these questions, I provide a list of 14 do's and don'ts to consider when designing, conducting, and interpreting the results of an AI Psychology study (see also 1 and 2</p>
<p>Experimental Design</p>
<p>When designing or adapting a cognitive test for evaluating an AI model: for LLMs include word associations based on their statistical co-occurrence ("racecar"/"fast"), although there might be many others, such as grammatical cues or more abstract structural patterns in text.Humans, too, routinely use task strategies not initially considered by the experimenters, but the kinds of alternative strategies available to humans and to AI systems are not necessarily the same.Considering possible shortcut paths to solutions can help both design shortcut-proof items and identify edge cases where shortcut-based models are likely to fail 3 .</p>
<p>DO incorporate careful control conditions.</p>
<p>Once you identify alternative strategies by which a model might solve the task, the next step is to design control conditions to help distinguish these possible strategies.For instance: did the model choose answer a over b simply because a is more frequent?Will the model show the same preference for answer a over b even when the question is omitted?</p>
<ol>
<li>
<p>DO NOT rely on well-known (or minimally changed) test items.When tested on a famous test item (such as "The trophy did not fit the suitcase because it was too small"; Box 1), the model has likely seen it multiple times during training and therefore has a high chance of relying on answer memorization; thus, its performance may not generalize to new, non-famous test items.Even if the experimenter makes a minor change to the original item, such as replacing "trophy" with "prize", a model might still be able to complete the test based on simple association between the old and the new item.</p>
</li>
<li>
<p>DO NOT overly trust crowd-sourced / automatically generated items.The ability of AI models to process large amounts of queries-way more than any human-creates a temptation of evaluating their performance on thousands of test items, obtained from online human workers, created by automatic item generation scripts, or even generated by AI models themselves.However, if the quality of these items is not rigorously evaluated, such tests may be meaningless.AI-generated items present their own set of challenges: a model might (a) generate test items that resemble samples from the training data or, more broadly, (b) generate patterns that have a high likelihood under the model and might therefore facilitate its performance.The systematic assessment of such biases in AI-generated materials remains to be done.</p>
</li>
</ol>
<p>DO evaluate models under conditions similar to humans when comparing their performance.</p>
<p>When conducting direct model-to-human comparisons, the default should be to recreate the same testing conditions for humans and models 4 .If a human needs instructions to perform well on a test, the model should also receive those instructions.If a human performs well on a task with no in-the-moment training, a model with human-level performance would also need to perform the task with no in-context training.If divergences during evaluation occur (for either theoretical or practical reasons), they need to be justified and highlighted when presenting the results.</p>
<ol>
<li>DO examine the effect of culture-specific aspects of the prompt on model behavior.The language in which the test is presented, the dialect, the vocabulary used, and even the use of capitalization and punctuation might all affect AI model performance as it leverages these cues to mimic specific users online.</li>
</ol>
<p>Interpreting the results</p>
<p>When interpreting the results of a cognitive assessment:</p>
<ol>
<li>
<p>DO compare model and human performance.We often assume humans will perform well on a specific test even when we don't have direct evidence for it.If these exact test items and question/prompt wordings have not been tested on humans before, they should be.9. DO be explicit about the experimental settings when reporting the results.Computational work should, ideally, have perfect replicability.For that to be achievable, all the materials and code needed to run the experiment should be made available online.For models offered by commercial third parties, there is no guarantee that they will continue to offer access to the model, so some backup arrangements may need to be made.Finally, a third party may roll out a model update that would change experimental results; in such cases, the date of the experiment needs to be reported as well.</p>
</li>
<li>
<p>DO NOT overly trust LLM item scoring.The flip side of testing a model on thousands of items is the need to evaluate responses to all these items.In some cases, determining correct answers is straightforward (e.g., a-d for a multiple-choice question); however, scoring free response items is hard work.Many studies are opting for LLM-based item evaluation; however, such approaches might be circular because the items that are hard for the model being evaluated might also be hard for the model doing the evaluation.At a minimum, the model being tested and the model doing the evaluation should belong to different classes; and even then, evaluator model performance needs to be rigorously checked by comparing it with human evaluations across a range of item difficulties.</p>
</li>
<li>
<p>DO NOT assume that model responses reflect "universal" human behavior.Even when directly comparing AI models and humans, it is important to be specific about which human population serves as a reference.Human psychology suffers from over-reliance on WEIRD (white, educated, industrialized, rich, democratic) participant pools; AI Psychology suffers from similar issues 5 because WEIRD individuals disproportionately contribute to the training data.Thus, calls to replace human data in psychology studies with AI-generated responses need to account for the strong demographic and cultural biases that these models bring.Similarly, model performance in English (the most represented language on the web) might not be reflective of model performance in other languages.</p>
</li>
<li>
<p>DO NOT assume that models solve the task in the same way as humans.Even if both humans and models do well on a particular test, it does not mean that they solve it in the same way.Some approaches to evaluating similarities and differences in the mechanisms underlying human and model task performance include comparing their error patterns, generalization to new tasks, and the internal (neural) representations required to accomplish the task.</p>
</li>
<li>
<p>DO check whether a model generalizes beyond a single test.Even when we control for possible shortcuts, a model might still find a loophole that allows it to perform well on a particular test.However, the more diverse tests we include (and the more we control for the shortcuts in each), the harder it will be to attribute model performance to serendipitous factors.If a model does well on 20 logical reasoning tasks that vary in their content and format, it is more plausible that it possesses logical reasoning than if it performs well on one such task.</p>
</li>
<li>
<p>DO NOT jump to conclusions.The discourse around AI models has become very polarized, with both extreme enthusiasm based on a few isolated examples and extreme skepticism based on isolated failures.What the field needs is careful evaluation of specific model capabilities with a clear acknowledgement of advances and limitations.Moreover, results reported for one model may not generalize to other models, so a single study's conclusions should be qualified accordingly.</p>
</li>
</ol>
<p>Case study: Winograd Schema Challenge</p>
<p>In 2012, Levesque and colleagues proposed a test for diverse kinds of basic world knowledge 6 (named after an initial example by Terry Winograd).The test includes minimally different item pairs with pronouns whose referent can be inferred based on general world knowledge:</p>
<p>(1) Q: The trophy doesn't fit into the brown suitcase because it's too small.What is too small?A: The suitcase</p>
<p>(2) Q: The trophy doesn't fit into the brown suitcase because it's too large.What is too large?A: The trophy A set of such minimally differing sentence pairs, tackling diverse world knowledge phenomenaphysical properties, biology, social situations, etc., -was then compiled into the Winograd Schema Challenge (WSC).Although initially challenging, the WSC was largely solved by 2020, with LLMs achieving over 90% accuracy 7 .However, it turned out that this success did not reflect models' deep knowledge of the world but rather the flaws in the test itself.</p>
<p>In the original paper introducing the challenge 6 , the authors cautioned against examples that could be solved via simple heuristics.They cite two such heuristics: selectional restrictions and frequency effects.The selectional restrictions heuristics can be illustrated with the example: "The women stopped taking the pills because they were pregnant/carcinogenic".Here, only an animate entity can be pregnant and only an inanimate entity can be carcinogenic, leading to an unambiguous association between the adjectives and the corresponding nouns without the need to tap into deeper world knowledge.The frequency heuristics applies to cases like: "The racecar zoomed by the school bus because it was going so fast/slow": a simple association between "racecar" and "fast" will suffice to determine the referent.Finally, the authors note that the examples need to be, in their words, Google-proof, i.e., not present in online text corpora used to train the models.</p>
<p>Despite the field's awareness of these heuristics, constructing a heuristics-free dataset was hard.In the original WSC dataset, over 10% of the items turned out to have simple association-based solutions 8 .To address this issue, Sakaguchi et al 7 constructed a large dataset called WinoGrande, where the examples were first crowd-sourced online and then automatically filtered to reduce cooccurrence-based associations as estimated through language model embeddings (rather than human intuition).This filtering substantially reduced model performance, suggesting that the association heuristic indeed inflates model accuracy.</p>
<p>Soon after, Elazar et al 9 showed that adding even more stringent quality controls leads to significant decreases in model performance on the WSC.The authors introduced two baseline cases to account for raw frequency of possible continuations: sentences with candidate referents excluded ("doesn't fit into because it's too large/small") and sentences with the first half excluded ("because it's too large/small").The assumption is that for these reduced sentences, there should be no consistent preference for "suitcase" vs. "trophy", and if there is, it reflects an association bias.It turned out that LLMs performed above chance on the WSC even in those baseline conditions, indicating previously undetected association biases.</p>
<p>The final issue raised by the WSC story is the quality vs. quantity tradeoff in test design.The initial WSC set includes less than 300 examples, all hand-crafted by scholars.Now that these examples are freely available online, performance on this set is no longer reflective of the models' general capabilities.The authors of WinoGrande used a different approach: to obtain thousands of novel items, they asked human workers to come up with many different examples and then used automatic filtering techniques.This approach is more scalable but suffers from numerous quality issues, e.g.typos ("wit" instead of "with"), grammatical errors ("more brighter color"), and unwarranted assumptions ("good at math" means "likely to be a professor").Overall, the tradeoff between result generalizability (which is harder for small datasets) and quality control (which is harder for large datasets) remains an important issue to consider in test design.</p>
<p>Kocijan et al 10 formulate several lessons from the WSC saga, the most important of which is perhaps: "We need to be careful not to rely on a perceived connection between tasks and methods".Just because we think that a certain task requires a cognitive ability X, doesn't mean that it actually does.</p>
<p>Testing open vs. closed models</p>
<p>Some of the DO'S discussed above -checking the training data for contamination with test items, verifying that the model has not been fine-tuned on the exact task being tested, creating conditions that will allow the study to be replicated in the future on the exact same model -are essentially impossible in the case of closed models.Thus, there is an argument to stop running AI Psychology assessments on closed models altogether given that, from a scientific perspective, the results are potentially non-reproducible and difficult to interpret.</p>
<p>Will researchers indeed stop running cognitive tests on closed AI models?Probably not.Although scientifically questionable, evaluating the behavior of closed, industry-standard models has important practical implications, including understanding which real-life tasks they can (and cannot) be reliably used for, what AI models are in principle capable of achieving (given that closed models often exhibit state-of-the-art performance), and what safety risks they may present.Thus, going forward, cognitive evaluations of AI systems might be used in two separate settings: (a) basic scientific inquiry of cognitive capacities of AI systems -which should prioritize open models, and (b) applied studies tackling questions related to model performance, safety, and user impact -which can be applied both to open and to closed models with the caveat that closed model results might not replicate or generalize.</p>
<p>As with any rapidly growing research area, the scientific practices and norms in AI Psychology get defined on the fly, often through trial and error.I hope that this discussion of the DO'S and DON'TS of AI Psychology will help distill some of the lessons learned and improve the robustness and validity of future work aiming to probe the cognitive capacities of AI systems.</p>
<p>determine what the model might have learned about your test during training consider alternative strategies a model might use to arrive at the correct answer incorporate careful control conditions evaluate models under conditions similar to humans examine the effect of culture- specific aspects of the prompt on model behavior</p>
<p>).I do not touch on the philosophical question of whether it is at all appropriate to ascribe mental capacities to a machine; my goal here is simply to clarify the methodological criteria that determine the inferences we can(not) make based on a model's responses to a questionnaire or a cognitive test.
y'allyoucompare model and humanperformance85% 87%</p>
<p>be explicit about the experimental settings when reporting the results check whether a model generalizes beyond a single test rely on well-known (or minimally changed) test items
The trophy is…The prize is…</p>
<p>overly trust crowd- sourced / automatically generated items My nam is Mark overly trust LLM item scoring assume that model responses reflect "universal" human behavior jump to conclusions A+ F- DO DON'T assume that models solve the task in the same way as humans
Figure 1: The do's and don'ts of AI model evaluation on cognitive tests.</p>
<ol>
<li>DO determine what the model might have learned about your test during training. The</li>
<li>DOtwo mostimportant issues to consider are: (a) was the model directly trained on your task? (b) did the model"see" examples from your test during its training? In the worst case, a and b might occur together,i.e., the model was trained on the task of interest using the same items as those in the current study.</li>
</ol>
<p>consider alternative strategies a model might use to arrive at the correct answer.</p>
<p>Even if the model has not directly learned your task during training or finetuning, it might still use a different strategy from the strategy you, the experimenter, have presupposed.The most prominent shortcuts</p>
<p>AcknowledgementsI gratefully acknowledge the funding support from the University System of Georgia.Many thanks to Aalok Sathe, Ben Lipkin, Carina Kauf, Greta Tuckute, Kyle Mahowald, and the reviewers for their constructive comments.Competing interestsThe author declares no competing interests.
Baby steps in evaluating the capacities of large language models. M C Frank, Nature Reviews Psychology. 22023</p>
<p>How do we know how smart AI systems are?. M Mitchell, 2023</p>
<p>Embers of autoregression show how large language models are shaped by the problem they are trained to solve. R T Mccoy, S Yao, D Friedman, M D Hardy, T L Griffiths, Proceedings of the National Academy of Sciences. 121e23224201212024</p>
<p>Can language models handle recursively nested grammatical structures? a case study on comparing models and humans. A K Lampinen, arXiv:2210.153032022arXiv preprint</p>
<p>. M Atari, M J Xue, P S Park, D Blasi, Henrich, J. Which humans? PsyArXiv. 2023</p>
<p>The Winograd Schema Challenge. H Levesque, E Davis, L Morgenstern, Thirteenth international conference on the principles of knowledge representation and reasoning. 2012</p>
<p>An adversarial winograd schema challenge at scale. K Sakaguchi, R L Bras, C Bhagavatula, Y Choi, Winogrande, Communications of the ACM. 642021</p>
<p>How reasonable are commonsense reasoning tasks: A case-study on the Winograd schema challenge and SWAG. P Trichelair, A Emami, A Trischler, K Suleman, J C K Cheung, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong; ChinaAssociation for Computational Linguistics2019</p>
<p>Back to square one: Artifact detection, training and commonsense disentanglement in the Winograd schema. Y Elazar, H Zhang, Y Goldberg, D Roth, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics2021Online and Punta Cana</p>
<p>The defeat of the winograd schema challenge. V Kocijan, E Davis, T Lukasiewicz, G Marcus, L Morgenstern, Artificial Intelligence. 1039712023</p>            </div>
        </div>

    </div>
</body>
</html>
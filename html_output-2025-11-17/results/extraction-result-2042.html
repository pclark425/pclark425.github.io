<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2042 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2042</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2042</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-50.html">extraction-schema-50</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <p><strong>Paper ID:</strong> paper-281830055</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.02752v1.pdf" target="_blank">The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback</a></p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning (RL) has demonstrated potential in enhancing the reasoning capabilities of large language models (LLMs), but such training typically demands substantial efforts in creating and annotating data. In this work, we explore improving LLMs through RL with minimal data. Our approach alternates between the LLM proposing a task and then attempting to solve it. To minimize data dependency, we introduce two novel mechanisms grounded in self-awareness: (1) self-aware difficulty prediction, where the model learns to assess task difficulty relative to its own abilities and prioritize challenging yet solvable tasks, and (2) self-aware limit breaking, where the model recognizes when a task is beyond its capability boundary and proactively requests external data to break through that limit. Extensive experiments on nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra data demonstrate the efficacy of self-aware RL and underscore the promise of self-evolving agent training.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2042.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2042.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-aware RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-aware Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-evolving RL paradigm introduced in this paper where an LLM generator produces tasks and predicts their difficulty relative to its current solver, and the system selectively queries a stronger external solver to break capability limits; combines LLM-generated curriculum, difficulty-aligned reward, novelty scoring, and a task-filtered external guidance mechanism to minimize external data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Qwen2.5-Coder-3B (generator/solver); Qwen2.5-Coder-32B (external solver)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td>3B (generator/solver), 32B (external solver)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>The curriculum is generated by an LLM generator agent that (a) emits new tasks (python code snippets in this implementation) and (b) predicts a success rate µ(x) for how many correct solutions the solver will produce out of N rollouts. The predicted success is trained via a difficulty-prediction reward R_dp = 1 - |μ(x) - µ(x)|, where μ(x) is the empirically observed success rate from sampled rollouts. Tasks are filtered by a task-utility score combining difficulty (1 - µ(x)) and novelty (token-level perplexity under the solver policy). Utility scores are normalized via z-scores over a FIFO task buffer and converted to a probability p(x) (via a parametrized mapping using γ and τ) to select high-utility tasks for external guidance. When selected and unsolved by the solver's rollouts, the pipeline queries an external solver for a correct solution (limit breaking) and uses that minimal external data to update the solver. Generator and solver also receive format rewards; learning uses an RLVR algorithm (REINFORCE++ in the implementation).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Code generation with verifiable execution (python executor) for training; evaluated on mathematical reasoning and code-generation benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Training domain uses verifiable, deterministic execution outcomes (python interpreter) enabling binary outcome rewards. Evaluation spans mathematical reasoning datasets (multi-step, compositional, symbolic reasoning requiring chain-of-thought and multi-step derivations) and coding benchmarks (function synthesis with unit-test style verification). Tasks are compositional and require multi-step reasoning; the training domain supports verifiability and deterministic reward but the evaluation domains include out-of-distribution (code) and out-of-domain (math) reasoning requiring generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Generator conditions on the solver's current capabilities implicitly via predicted success rate µ(x) and uses a recent-task buffer B (FIFO) to compute relative utility z-scores; solver conditions include policy perplexity and rollout success statistics used to compute ground-truth μ(x).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td>Explicit novelty score computed as negative average log-probability (token-level perplexity) of the task under the current solver policy; novelty combined with difficulty into a utility vector and compared to other recent tasks via z-scoring to prioritize novel tasks for external guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>python code interpreter for verifiable reward, task filter (utility-based), external stronger solver (Qwen2.5-Coder-32B) for limit breaking, REINFORCE++ RLVR algorithm, format-check reward.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>Reported improvement compared to base pre-trained model: mathematical reasoning average increased from 21.0 (baseline Qwen2.5-Coder-3B math average) to 32.3 (self-aware RL) — a reported ≈53.8% relative gain on math benchmarks; coding average increased by ≈5.3% (from 51.2 to 53.9). Self-aware RL queried external guidance on 157/12,800 = 1.23% of generated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td>Compared to baseline AZR (a self-evolving baseline) the math average improved from 22.3 (AZR) to 32.3 (self-aware RL); ablation without limit breaking (self-aware RL −) achieved math average 28.0, showing that difficulty prediction alone provides substantial gains (≈25.6% over AZR in their reported analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td>Base pre-trained model (no self-generated curriculum / no RL) math avg reported as 21.0 and code avg 51.2 in Table 1; self-aware RL substantially improves math over this no-curriculum baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td>Not reported as an explicit count of unique tasks, but the training used 12,800 generated tasks (of which 157 received external labels); task buffer used for relative utility; measured rollout accuracy evolution and utility separation between selected vs unselected tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Self-aware RL improved performance on out-of-distribution code-generation benchmarks and out-of-domain mathematical reasoning benchmarks: e.g., large relative gains on math benchmarks (MATH500 +29.8% absolute in table entries, AMC'23 +77.8% relative in some reported per-benchmark numbers) and smaller gains on coding benchmarks; indicates improved general reasoning transfer but no detailed zero-shot/few-shot breakdown beyond benchmark scores.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training runs reported on a server with 4 NVIDIA H-200 GPUs for experiments; ran for 200 steps in appendix details. No dollar or per-example inference cost reported. External guidance usage was deliberately minimal (1.23% of tasks) to reduce human/supervised data cost.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Initial difficulty prediction is poor (≈0.2 accuracy) and requires >50 training steps to calibrate; limit breaking is disabled for first 50 steps. Overusing external guidance (large τ) harms performance by corrupting solver reasoning patterns (ablation shows an optimal τ ≈ 0.1; larger τ yields diminishing/negative returns). Without proper utility ranking (randomized/shuffled utility), external guidance yields negligible improvements. The approach relies on verifiable reward environments (python execution), so domains without verifiable outcome signals may be harder to apply.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>Performs especially well on mathematical reasoning benchmarks (substantial relative gains reported), while coding domain shows modest improvements because the base model was already strong in coding; suggests the curriculum helps domains requiring general multi-step reasoning more than already-strong coding abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablations reported: (1) Removing limit breaking (self-aware RL −) still gives large gains over baseline (math avg from 22.3 AZR to 28.0), showing difficulty prediction alone is beneficial; (2) Varying τ (frequency of external guidance) — best performance at τ=0.1; too small still helps, too large degrades gains; (3) Shuffling utility ranking abolishes benefit from limit breaking (shuffled utility gives negligible additional improvement), demonstrating importance of utility ranking; (4) Difficulty prediction accuracy improves from ~0.2 to >0.6 after ~50 steps, justifying the training curriculum schedule.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>LLM-generated curriculum that is self-aware (predicting task difficulty relative to the model and combining difficulty with novelty to prioritize tasks) produces large improvements in downstream multi-step reasoning (≈53.8% relative avg gain on mathematical benchmarks) while using very little external supervision (1.23% of tasks). Difficulty prediction alone yields substantial gains; selective, utility-ranked external guidance (limit breaking) provides additional improvements, but excessive external guidance harms learned reasoning patterns. Proper utility ranking and calibrated frequency (τ) are critical components.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2042.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2042.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-evolving RL (related works)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-evolving Reinforcement Learning (prior works)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods in related work where agents autonomously generate tasks (or environments/world models) for self-improvement with minimal human oversight; examples include generator agents creating coding tasks, co-evolving unit testers, and simulated world models to drive exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated / generator agent (varies by prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Prior approaches (Absolute Zero Reasoner, WebEvolver, generator-agent frameworks) generate tasks automatically (e.g., coding tasks or simulated web navigation goals) to create training curricula, but typically do not condition task generation on the agent's own capability estimates; therefore they can produce tasks that are too easy or too hard and waste training budget.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Varied in cited works: code generation, web navigation, GUI learning, and general agentic environments</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Often open-ended, diverse task spaces (web navigation, GUI interaction, code generation); tasks can be compositional and require exploration, but many prior methods lack capability-aware filtering so curricula may be misaligned with agent learning frontier.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Examples include co-evolving unit testers, simulated world models, or separate generator/solver agent pairs in prior works.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Paper notes that prior self-evolving methods generate tasks without awareness of the agent's current capabilities, leading to inefficient curricula that are often too trivial or too difficult and thus inefficient for learning.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Prior self-evolving curricula reduce human labeling but suffer from misalignment with agent capability; paper's contribution is to add self-awareness (difficulty prediction and limit breaking) to address this misalignment and reduce wasted training.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Absolute Zero Reasoner <em>(Rating: 2)</em></li>
                <li>WebEvolver: Enhancing web agent self-improvement with coevolving world model <em>(Rating: 2)</em></li>
                <li>Co-evolving LLM coder and unit tester via reinforcement learning <em>(Rating: 2)</em></li>
                <li>WebRL: Training LLM web agents via self-evolving online curriculum reinforcement learning <em>(Rating: 2)</em></li>
                <li>Self-evolving curriculum for llm reasoning <em>(Rating: 2)</em></li>
                <li>Adaptive Difficulty Curriculum Learning <em>(Rating: 1)</em></li>
                <li>Balanced online difficulty filtering for reasoning oriented reinforcement learning <em>(Rating: 1)</em></li>
                <li>ZeroGUI: Automating online GUI learning at zero human cost <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2042",
    "paper_id": "paper-281830055",
    "extraction_schema_id": "extraction-schema-50",
    "extracted_data": [
        {
            "name_short": "Self-aware RL",
            "name_full": "Self-aware Reinforcement Learning",
            "brief_description": "A self-evolving RL paradigm introduced in this paper where an LLM generator produces tasks and predicts their difficulty relative to its current solver, and the system selectively queries a stronger external solver to break capability limits; combines LLM-generated curriculum, difficulty-aligned reward, novelty scoring, and a task-filtered external guidance mechanism to minimize external data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated",
            "llm_model_name": "Qwen2.5-Coder-3B (generator/solver); Qwen2.5-Coder-32B (external solver)",
            "llm_model_size": "3B (generator/solver), 32B (external solver)",
            "curriculum_description": "The curriculum is generated by an LLM generator agent that (a) emits new tasks (python code snippets in this implementation) and (b) predicts a success rate µ(x) for how many correct solutions the solver will produce out of N rollouts. The predicted success is trained via a difficulty-prediction reward R_dp = 1 - |μ(x) - µ(x)|, where μ(x) is the empirically observed success rate from sampled rollouts. Tasks are filtered by a task-utility score combining difficulty (1 - µ(x)) and novelty (token-level perplexity under the solver policy). Utility scores are normalized via z-scores over a FIFO task buffer and converted to a probability p(x) (via a parametrized mapping using γ and τ) to select high-utility tasks for external guidance. When selected and unsolved by the solver's rollouts, the pipeline queries an external solver for a correct solution (limit breaking) and uses that minimal external data to update the solver. Generator and solver also receive format rewards; learning uses an RLVR algorithm (REINFORCE++ in the implementation).",
            "domain_name": "Code generation with verifiable execution (python executor) for training; evaluated on mathematical reasoning and code-generation benchmarks",
            "domain_characteristics": "Training domain uses verifiable, deterministic execution outcomes (python interpreter) enabling binary outcome rewards. Evaluation spans mathematical reasoning datasets (multi-step, compositional, symbolic reasoning requiring chain-of-thought and multi-step derivations) and coding benchmarks (function synthesis with unit-test style verification). Tasks are compositional and require multi-step reasoning; the training domain supports verifiability and deterministic reward but the evaluation domains include out-of-distribution (code) and out-of-domain (math) reasoning requiring generalization.",
            "state_conditioning": true,
            "state_conditioning_details": "Generator conditions on the solver's current capabilities implicitly via predicted success rate µ(x) and uses a recent-task buffer B (FIFO) to compute relative utility z-scores; solver conditions include policy perplexity and rollout success statistics used to compute ground-truth μ(x).",
            "novelty_mechanism": true,
            "novelty_mechanism_details": "Explicit novelty score computed as negative average log-probability (token-level perplexity) of the task under the current solver policy; novelty combined with difficulty into a utility vector and compared to other recent tasks via z-scoring to prioritize novel tasks for external guidance.",
            "complementary_systems": "python code interpreter for verifiable reward, task filter (utility-based), external stronger solver (Qwen2.5-Coder-32B) for limit breaking, REINFORCE++ RLVR algorithm, format-check reward.",
            "performance_llm_curriculum": "Reported improvement compared to base pre-trained model: mathematical reasoning average increased from 21.0 (baseline Qwen2.5-Coder-3B math average) to 32.3 (self-aware RL) — a reported ≈53.8% relative gain on math benchmarks; coding average increased by ≈5.3% (from 51.2 to 53.9). Self-aware RL queried external guidance on 157/12,800 = 1.23% of generated tasks.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": "Compared to baseline AZR (a self-evolving baseline) the math average improved from 22.3 (AZR) to 32.3 (self-aware RL); ablation without limit breaking (self-aware RL −) achieved math average 28.0, showing that difficulty prediction alone provides substantial gains (≈25.6% over AZR in their reported analysis).",
            "performance_no_curriculum": "Base pre-trained model (no self-generated curriculum / no RL) math avg reported as 21.0 and code avg 51.2 in Table 1; self-aware RL substantially improves math over this no-curriculum baseline.",
            "has_curriculum_comparison": true,
            "task_diversity_metrics": "Not reported as an explicit count of unique tasks, but the training used 12,800 generated tasks (of which 157 received external labels); task buffer used for relative utility; measured rollout accuracy evolution and utility separation between selected vs unselected tasks.",
            "transfer_generalization_results": "Self-aware RL improved performance on out-of-distribution code-generation benchmarks and out-of-domain mathematical reasoning benchmarks: e.g., large relative gains on math benchmarks (MATH500 +29.8% absolute in table entries, AMC'23 +77.8% relative in some reported per-benchmark numbers) and smaller gains on coding benchmarks; indicates improved general reasoning transfer but no detailed zero-shot/few-shot breakdown beyond benchmark scores.",
            "computational_cost": "Training runs reported on a server with 4 NVIDIA H-200 GPUs for experiments; ran for 200 steps in appendix details. No dollar or per-example inference cost reported. External guidance usage was deliberately minimal (1.23% of tasks) to reduce human/supervised data cost.",
            "failure_modes_limitations": "Initial difficulty prediction is poor (≈0.2 accuracy) and requires &gt;50 training steps to calibrate; limit breaking is disabled for first 50 steps. Overusing external guidance (large τ) harms performance by corrupting solver reasoning patterns (ablation shows an optimal τ ≈ 0.1; larger τ yields diminishing/negative returns). Without proper utility ranking (randomized/shuffled utility), external guidance yields negligible improvements. The approach relies on verifiable reward environments (python execution), so domains without verifiable outcome signals may be harder to apply.",
            "long_horizon_performance": null,
            "specialized_domain_performance": "Performs especially well on mathematical reasoning benchmarks (substantial relative gains reported), while coding domain shows modest improvements because the base model was already strong in coding; suggests the curriculum helps domains requiring general multi-step reasoning more than already-strong coding abilities.",
            "ablation_studies": "Ablations reported: (1) Removing limit breaking (self-aware RL −) still gives large gains over baseline (math avg from 22.3 AZR to 28.0), showing difficulty prediction alone is beneficial; (2) Varying τ (frequency of external guidance) — best performance at τ=0.1; too small still helps, too large degrades gains; (3) Shuffling utility ranking abolishes benefit from limit breaking (shuffled utility gives negligible additional improvement), demonstrating importance of utility ranking; (4) Difficulty prediction accuracy improves from ~0.2 to &gt;0.6 after ~50 steps, justifying the training curriculum schedule.",
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "LLM-generated curriculum that is self-aware (predicting task difficulty relative to the model and combining difficulty with novelty to prioritize tasks) produces large improvements in downstream multi-step reasoning (≈53.8% relative avg gain on mathematical benchmarks) while using very little external supervision (1.23% of tasks). Difficulty prediction alone yields substantial gains; selective, utility-ranked external guidance (limit breaking) provides additional improvements, but excessive external guidance harms learned reasoning patterns. Proper utility ranking and calibrated frequency (τ) are critical components.",
            "uuid": "e2042.0"
        },
        {
            "name_short": "Self-evolving RL (related works)",
            "name_full": "Self-evolving Reinforcement Learning (prior works)",
            "brief_description": "A family of methods in related work where agents autonomously generate tasks (or environments/world models) for self-improvement with minimal human oversight; examples include generator agents creating coding tasks, co-evolving unit testers, and simulated world models to drive exploration.",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generator_type": "LLM-generated / generator agent (varies by prior work)",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "Prior approaches (Absolute Zero Reasoner, WebEvolver, generator-agent frameworks) generate tasks automatically (e.g., coding tasks or simulated web navigation goals) to create training curricula, but typically do not condition task generation on the agent's own capability estimates; therefore they can produce tasks that are too easy or too hard and waste training budget.",
            "domain_name": "Varied in cited works: code generation, web navigation, GUI learning, and general agentic environments",
            "domain_characteristics": "Often open-ended, diverse task spaces (web navigation, GUI interaction, code generation); tasks can be compositional and require exploration, but many prior methods lack capability-aware filtering so curricula may be misaligned with agent learning frontier.",
            "state_conditioning": false,
            "state_conditioning_details": null,
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": "Examples include co-evolving unit testers, simulated world models, or separate generator/solver agent pairs in prior works.",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "Paper notes that prior self-evolving methods generate tasks without awareness of the agent's current capabilities, leading to inefficient curricula that are often too trivial or too difficult and thus inefficient for learning.",
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Prior self-evolving curricula reduce human labeling but suffer from misalignment with agent capability; paper's contribution is to add self-awareness (difficulty prediction and limit breaking) to address this misalignment and reduce wasted training.",
            "uuid": "e2042.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Absolute Zero Reasoner",
            "rating": 2
        },
        {
            "paper_title": "WebEvolver: Enhancing web agent self-improvement with coevolving world model",
            "rating": 2
        },
        {
            "paper_title": "Co-evolving LLM coder and unit tester via reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "WebRL: Training LLM web agents via self-evolving online curriculum reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Self-evolving curriculum for llm reasoning",
            "rating": 2
        },
        {
            "paper_title": "Adaptive Difficulty Curriculum Learning",
            "rating": 1
        },
        {
            "paper_title": "Balanced online difficulty filtering for reasoning oriented reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "ZeroGUI: Automating online GUI learning at zero human cost",
            "rating": 1
        }
    ],
    "cost": 0.01206575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback
3 Oct 2025</p>
<p>Hangfan Zhang 
Pennsylvania State University</p>
<p>Siyuan Xu 
Pennsylvania State University</p>
<p>Zhimeng Guo zhimeng@psu.edu 
Pennsylvania State University</p>
<p>Huaisheng Zhu 
Pennsylvania State University</p>
<p>Shicheng Liu 
Pennsylvania State University</p>
<p>Xinrun Wang xrwang@smu.edu.sg 
Management University
Singapore</p>
<p>Qiaosheng Zhang zhangqiaosheng@pjlab.org.cnyang.chen.csphd@gmail.com 
Shanghai Artificial Intelligence Laboratory</p>
<p>Yang Chen 
Shanghai Artificial Intelligence Laboratory</p>
<p>Peng Ye yepeng@pjlab.org.cn 
Shanghai Artificial Intelligence Laboratory</p>
<p>Lei Bai bailei@pjlab.org.cn 
Shanghai Artificial Intelligence Laboratory</p>
<p>Shuyue Hu hushuyue@pjlab.org.cn 
Shanghai Artificial Intelligence Laboratory</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback
3 Oct 2025DE8C2950EE94BE6441DD051E25505C2FarXiv:2510.02752v1[cs.CL]
Reinforcement learning (RL) has demonstrated potential in enhancing the reasoning capabilities of large language models (LLMs), but such training typically demands substantial efforts in creating and annotating data.In this work, we explore improving LLMs through RL with minimal data.Our approach alternates between the LLM proposing a task and then attempting to solve it.To minimize data dependency, we introduce two novel mechanisms grounded in self-awareness: (1) self-aware difficulty prediction, where the model learns to assess task difficulty relative to its own abilities and prioritize challenging yet solvable tasks, and (2) self-aware limit breaking, where the model recognizes when a task is beyond its capability boundary and proactively requests external data to break through that limit.Extensive experiments on nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra data demonstrate the efficacy of self-aware RL and underscore the promise of self-evolving agent training.</p>
<p>Introduction</p>
<p>Reinforcement Learning (RL) has emerged as a crucial approach for enhancing the reasoning abilities of large language models (LLMs), especially in tasks like mathematical problem solving and code generation, where the correctness of generated outputs can be rigorously verified (Lambert et al., 2024;Guo et al., 2025;Jaech et al., 2024;Team et al., 2025).However, these improvements often come at the cost of requiring vast amounts of high-quality data.The data curation processes, which heavily rely on human experts to create tasks and annotate solutions, are prohibitively costly and time-consuming, creating a significant bottleneck in LLM advancements.</p>
<p>One promising solution is to empower LLMs to generate their own tasks, enabling selfimprovement through RL (Zhao et al., 2025;Qi et al., 2025;Yang et al., 2025;Wang et al., 2025;Fang et al., 2025).By actively participating in their own learning process-generating tasks and attempting to solve them-LLMs can establish a self-improving loop of data creation and model refinement, significantly reducing the need for manually curated data.While initial efforts have shown promise, this line of research faces two critical challenges:</p>
<p>First, the model may fail to generate appropriately challenging tasks.If the tasks are too easy, the model fails to engage in deeper reasoning and exploration.Conversely, if the tasks are too difficult, the model receives little useful feedback about what went wrong or how to improve.This issue is further complicated by the dynamic nature of the self-improvement loop, as the model's evolving capabilities require that task difficulty be continuously adjusted.Second, the self-improving loop is prone to stagnation, failing to continuously proposing challenging tasks.While the model can improve through RL, the self-improving loop is fundamentally constrained by the base model's inherent capabilities.As the loop progresses, the difficulty of the generated tasks may plateau, once the model has exhausted its current knowledge.Consequently, the model may encounter a point of stagnation, unable to advance further or surpass its inherent capability limits.</p>
<p>This paper argues that the key to addressing these challenges lies in self-awareness-the ability to understand one's own strengths, weaknesses, and evolving capabilities.A task that is difficult for the base model may later become trivial as its abilities improve.Therefore, the model must be acutely aware of its current abilities to accurately assess the difficulty of its generated tasks in relation to its state.Furthermore, overcoming the model's capability limits and escaping stagnation also require a clear understanding of its own capability boundaries.</p>
<p>To this end, this paper introduces a novel paradigm for improving LLMs with minimal data: self-aware RL.This paradigm features two key mechanisms.First, through self-aware difficulty prediction, the model not only generates a task but also predicts the difficulty of the task considering its current state.To achieve accurate predictions, the model learns to align its difficulty estimates with its actual success rate.This enables the creation of an adaptive curriculum that prioritizes problems with an appropriate level of difficulty, tailored to the current capabilities of the LLM.Second, through self-aware limit breaking, the model occasionally proactively seeks external guidance, such as requesting a correct solution, when it identifies a generated task to be highly valuable for improvement but hardly solvable given its current capability.This is achieved by having the model assess the novelty (via perplexity) and difficulty (via its own prediction) of a generated task.This allows the LLM to surpass its inherent capability limits while minimizing reliance on external data, resorting to it only when necessary.</p>
<p>For evaluation, we train the LLMs with self-aware RL equipped with python code interpreter that provides verifiable outcome signals, and then evaluate the LLMs across a range of reasoning tasks the models have not encountered during our training.In our experiments, we implement self-aware RL based on Qwen2.5-Coder-3B, and consider nine benchmarks across mathematical reasoning and code generation in the evaluation.Our results demonstrate that self-aware RL can significantly improve the pre-trained model's performance on both out-of-distribution tasks (code generation benchmarks) and out-of-domain tasks (mathematical reasoning benchmarks), achieving a relative performance gain of 53.8% on average on mathematical reasoning benchmarks.Specifically, self-aware RL got significant improvements on a lot of widely used benchmarks: 29.8% on MATH500, 77.8% on AMC'23, 82.4% on OlympiadBench, and 22.3% on LiveCodeBench.We further analyze the training behavior, and conduct fine-grained ablation studies to learn the impact of different components.</p>
<p>To summarize, our key contributions are as follows:</p>
<p>• We establish connections between self-awareness and the self-improvement of LLMs, highlight-ing that self-awareness is a crucial mechanism for efficiently enhancing LLMs, while reducing reliance on external data.</p>
<p>• We present a novel RL paradigm for improving LLMs with minimal data, enabling the model to (i) generate appropriately difficult tasks that align with its current abilities, and (ii) proactively seek external guidance to surpass its inherent capability limits when necessary.</p>
<p>• We conduct extensive experiments across nine benchmarks in mathematical reasoning and code generation, showing that our approach substantially boosts the base model's performance and generalization abilities, thereby validating the effectiveness of the self-aware RL paradigm.</p>
<p>Related Works</p>
<p>Reinforcement Learning with Verifiable Reward (RLVR).RLVR is an emerging paradigm (Guo et al., 2025;Lambert et al., 2024;Jaech et al., 2024;Team et al., 2025) in LLM alignment. Tulu3 (Lambert et al., 2024) was the first LLM tuned with RLVR to improve math and instruction following capabilities.Deepseek-R1 (Shao et al., 2024;Guo et al., 2025) was trained with Group Relative Policy Optimization (GRPO), a novel RLVR algorithm which uses inner group relative reward to replace the value model in traditional RL algorithms, boosting the training efficiency.Many following works tried to stabilize GRPO training.DAPO (Yu et al., 2025) used a higher clipping threshold and proposed dynamic sampling to better utilize the positive reward signal.GSPO (Zheng et al., 2025) updated the token-level importance sampling in GRPO to sequence-level, mitigating the bias introduced by imbalanced response lengths.Dr. GRPO Liu et al. (2025) identified that GRPO tends to overly penalize shorter, incorrect responses, which misguides the LLM to produce longer yet incorrect responses.Similar to GSPO, Dr.GRPO fixed this issue by considering a sequence-level reward instead of a token-level one.Due to the significant performance gain brought by RLVR, it is widely adopted in most advanced Large Reasoning Models (LRM) (Team et al., 2025).However, while RLVR does not require human crafted responses for imitation learning like in Supervised Fine Tuning (SFT), a large set of high-quality tasks are still necessary to incentivize diverse and effective reasoning patterns.</p>
<p>Self-evolving RL.To overcome the dependency on vast data, self-evolving RL is proposed to create agents that autonomously enhance their capabilities with minimal human oversight.Wang et al. (2025) proposed to build a unit tester co-evolving with the code generation agent to improve the overall generation quality.WebEvolver (Fang et al., 2025) introduced a dynamic world model simulating web navigation to help exploration in the web environment.Absolute Zero Reasoner (Zhao et al., 2025) proposed a generator agent automatically generating coding tasks to train the policy model without any input data.Similarly, self-challenging agent Zhou et al. (2025) designed a geneator agent that can interact with the environment to simulate a user query as the task fed into a customer agent.WebRL (Qi et al., 2025) proposed to generate new tasks from unsuccessful attempts of a web agent, expanding the data coverage to improve the decision-making capability of the agent.ZeroGUI (Yang et al., 2025) adopted VLM-based automatic task generation to produce diverse training goals.However, these approaches generate tasks without awareness of the agent's own capabilities, leading to inefficient training curricula.The generated tasks are often misaligned with the agent's learning frontier, being either too trivial or difficult.</p>
<p>Curriculum RL in LLM Reasoning.Curriculum learning, which involves structuring training data from easy to hard, has emerged as a powerful technique for cultivating advanced reasoning in LLMs.Typical curriculum learning methods focus on dynamically selecting or filtering problems from a larger pool to maintain an optimal level of difficulty.Recently, curriculum learning has also caught much attention from LLM community due to the reliance on proper utilization of available data in LLM training.Balanced online difficulty filtering (Bae et al., 2025) proposed a novel mechanism, balanced filtering, that focuses on tasks with medium level of hardness to maximize the effectiveness of GRPO.Adaptive Difficulty Curriculum Learning (Zhang et al., 2025) imitates human learning strategy and periodically re-estimates the difficulty within upcoming data batches to keep aligned with model's capabilities.Self-Evolving Curriculum (Chen et al., 2025) learns an additional curriculum policy concurrently with the RL fine-tuning process to maximize the learning progress.Curriculum Reinforcement Fine-tuning (Deng et al., 2025) proposed a difficulty-aware reward design ensuring steady progression of model capabilities.By guiding models through a structured learning path, these methods effectively foster the development of complex, multi-step reasoning capabilities.Even though, these curriculum learning approaches directly improve typical RL methods.The conjunction of curriculum learning and self-evolving RL is largely underexplored.</p>
<p>In this paper, we explore a solution combining the advantages of both, showing that the curriculum design is especially important in self-evolving RL training.</p>
<p>Preliminaries</p>
<p>In this paper, we denote the LLM parameterized by θ as a policy model π θ , which is also used to refer to an LLM agent driven by π θ for simplicity.We use x to denote the task that a solver agent solves with response y, where x and y are both sequences of tokens with t-th token in x denoted as x t , and the length of x denoted as |x|.</p>
<p>REINFORCE++ (Hu et al., 2025)is an advanced RL algorithm for LLM post-training.Its learning objective on a sampled pair (x, y) is given by:
J (θ|x, y) = 1 |y| |y| t=1 min w t (π θ ) Ât , clip (w t (π θ ), 1 − ε, 1 + ε) Ât (1)
where w t (θ) is the importance sampling ratio, and Ât denotes the normalized advantage term:
Ât = r − mean batch (r) std batch (r) (2)
4 Method</p>
<p>The general framework of self-aware RL can be viewed in Figure 1.We design self-aware RL as a self-evolving training loop, where a generator agent and a solver agent iteratively generate and solve new tasks to improve themselves.Due to the stability and accessibility of coding environment, we implement self-aware RL on code generation tasks, while any other environment that can check the correctness of the agent's response also applies, e.g., the verifiable rewards gym adopted by Kimi-K2 (Team et al., 2025).We use the python code interpreter to produce verifiable reward signals.Given a code snippet, the solver agent is asked to predict the outcome after execution, and a matching outcome will receive a positive outcome reward.Therefore, the generator agent</p>
<p>Generator Solver</p>
<p>Difficulty Prediction ()</p>
<p>External Solver</p>
<p>DP Reward Outcome Reward Format Reward</p>
<p>Task Filter</p>
<p>Policy Update</p>
<p>Figure 1: The overview of self-aware RL.The generator agent first generates the task, with a predicted success rate µ(x) (Section 4.1).The solver agent will then generates reasoning paths.If none of these generated reasoning paths are correct, which means no update are available, the task will be filtered by a task filter to determine whether it is of enough value to be processed by an external solver (Section 4.2).Finally, the collected difficulty prediction reward, outcome reward, and format reward will be aggregated to calculate the policy update (Section 4.3).</p>
<p>can generate a new code snippet with accessible execution outcome.With the generated tasks, self-aware RL improves the solver agent with typical RLVR methods.Specifically, self-aware RL incorporates difficulty prediction and limit breaking to improve the self-awareness of the agent.LLM agents based on pre-trained LLMs are not inherently self-aware, which hinders efficient self-evolution.With a better understanding of their own capability boundaries, agents can generate properly challenging tasks, which has been validated to be beneficial to RL training (Bae et al., 2025;Chen et al., 2025).Meanwhile, fully on policy RL training can be extremely challenging in some cases where the agent is strictly limited by its inherent capability ceiling.External guidance is necessary here to break the limitation.However, it is not efficient to seek external guidance blindly on every unsolvable tasks, which degrades to naive supervised fine-tuning (SFT) requiring heavy human inspections.Self-awareness can assist in identifying highly valuable tasks where external guidance is necessary and effective, mitigating the need for vast, well-grounded supervision.</p>
<p>• Self-aware difficulty prediction guides the generator agent to learn to predict the difficulty of the generated tasks.</p>
<p>• Self-aware limit breaking identifies cases where the solver are limited by its inherent reasoning capability, and breaks it with external guidance.</p>
<p>Self-aware difficulty prediction</p>
<p>Problems at a proper level of difficulty can maximize the learning progress (Chen et al., 2025;Bae et al., 2025).However, guiding the generator to produce suitable tasks requires a good understanding of the LLM's own capability which entails non-trivial challenges.LLMs are rarely trained to improve the capability of understanding their own boundary as it is not feasible to do so in large scale pre-training with preset human inspection.Consequently, while being a great task solver, LLMs are not naturally good at understanding themselves.Our intuition is empirically confirmed through evaluating pre-trained LLMs' performance in predicting task solving success rates.As discussed in Figure 3, we found that the base model is initially bad at predicting the task solving success rates, i.e., LLMs are not inherently aware of its own capability boundary.Therefore, we propose self-aware difficulty prediction to enhance the generator agent's capability of understanding its own knowledge boundaries and generate proper tasks benefiting the self-evolving loop.First, we enable self-aware self-aware RL by asking the generator agent to explicitly reason about the complexity of the generated task and predict how many trials would be correct among N rollouts generated by the solver agent.For instance, assume that the generator agent predicts that for a generated task (we denote the task as x) to be solved, the solver agent can generate 5 correct answers out of N = 8 rollouts, we will have the predicted success rate µ(x) = 5 8 .The predicted success rate µ(x) should be aligned with the ground truth success rate μ(x).When rollouts are sampled from the solver agent, difficulty prediction collects the sampled rollouts and calculate the ground truth success rate, which is further utilized in the reward signal for the generator.Specifically, given the actual success rate μ(x), and the predicted success rate µ(x), the reward function is:
R dp (x) = 1 − |μ(x) − µ(x)|(3)
When the generator predicts a close enough success rate, it will receive a high reward from it.</p>
<p>Self-aware limit breaking</p>
<p>Even with proper tasks, agents are still limited by their own capability.When the solver fails to solve a task with multiple trials, it will not get any improvement since no positive guidance is acquired.Previous practices to deal with this challenge have focused on injecting new knowledge and abilities into the LLM during training, a process that commonly involves human inspection.</p>
<p>As LLM training is extremely costly, intervention from human experts will not be frequent, which results in a widely adopted training paradigm: data collection, training, evaluation, and repeat.This less dynamic and timely intervention makes the benefit from external guidance take effect much later than the time point when the LLM actually requires the guidance.</p>
<p>To mitigate this issue, self-aware RL adopts self-aware limit breaking to offer effective guidance immediately when the solver agent fails to solve a valuable task.As shown in Figure 1, for any task, when all reasoning paths sampled from the solver agent are incorrect, limit breaking will first check the task utility representing whether the task is significant in improving the agent.Once validated, external guidance will be obtained by querying a stronger external solver.Correct guidance will be adopted to improve the solver agent, breaking the inherent ability limitation.</p>
<p>We first introduce how to identify valuable tasks.For clarity, we use an indicator variable I x to denote whether a task is selected as valuable enough to acquire external guidance.The selection is a random event that occurs with a probability p(x) depending on the task itself.Formally, I x is a Bernoulli random variable defined as
I x = 1, with probability p(x) 0, with probability 1 − p(x)
To obtain suitable p(x), we design a task utility mechanism that assigns different levels of utility score to tasks reflecting their significance.For task utility, we consider two dimensions: the difficulty and the novelty.For difficulty, we utilize the predicted success rate from difficulty prediction as this score reflects the general review of complexity from the generator agent.We inversely use 1 − µ(x) to measure the difficulty level of task x.To measure the novelty of a task, we utilize the token-level perplexity from the solver agent.The utility score ω of task x can be formulated as follows:
ω difficulty (x) = 1 − µ(x) ω novelty (x) = − 1 |y| |y| i=1 logπ θ old (y i |x, y &lt;i )(4)
A higher novelty score indicates that the current policy is more confused about the task description, which can be viewed as a signal the solver agent is not familiar with the task.While we only consider two kinds of utility measurement here, it is possible to take more measurement into consideration when transferred to other domains.</p>
<p>In order to minimize the amount of acquired external guidance used to save cost, we only acquire external guidance on filtered-out high-utility tasks.Given that the tasks are dynamically generated and the agent's capability may differ between runs, it is natural to calculate the utility relatively by comparing a task to other tasks in that run.To achieve this, we maintain a task buffer B that records recent tasks seen by the solver agent.The buffer is implemented as a FIFO queue, which only stores a fixed number of recently generated tasks.
B = {x 1 , x 2 , ..., x |B| }(5)
Tasks with top-level utility will be assigned a higher p(x).To achieve this, we first obtain the z-score of the task utility among the recorded tasks, and assign a higher p(x) to tasks with higher z-score.This procedure is implemented as follows:
ω(x) = [ω difficulty (x), ω novelty (x)] z(x) = 1 |ω| |ω| i=1 ω(x) i − E x∼B [ω(x) i ] std x∼B [ω(x) i ] (6) p(x) = Φ γ z(x) + Φ −1 (τ ) 1 + 1 γ 2 (7)
where ω(x) i denotes the i-th dimension of ω(x).In this case, ω(x) 0 = ω difficulty (x), and ω(x) 1 = ω novelty (x).Φ is the cumulative distribution function (CDF) of standard normal distribution, γ &gt; 0 is a sharpness factor and τ is the probability factor, which makes E[p(x)] = τ .By tuning τ , we can control the number of tasks that require external guidance.A higher sharpness factor γ will enlarge p(x) for a task with high z(x).</p>
<p>Self-aware RL training pipeline</p>
<p>Finally, we can combine all components to build the self-aware RL training pipeline.For the generator agent, we use R dp as the reward.Similar to typical RLVR settings, we include a format reward R format ∈ {0, 1} to guide the agent responses following the difficulty prediction template.</p>
<p>A positive R format indicates that the agent's response passes the format check.We formulate the reward for the generator agent as follows:
R generator = R format R dp + R format (8)
For the solver agent, we apply typical RLVR method, and adopt the binary outcome reward R outcome ∈ {0, 1} as the reward signal.A positive outcome reward indicates that the agent successfully solves the task.We also adopt a format reward for the solver agent, with a different dialogue template to the generator agent.We formulate the reward for the solver agent as follows:
R solver = R format R outcome + R format (9)
For a training sample (x, y) including one generated task from the generator agent and one sampled response from the solver agent, the learning objective is formulated in Equation 1.Therefore, the learning objective for self-aware RL is:
J self-aware RL (θ) = E [J (θ|x, y)] , y ∼ π 1−Ix θ π Ix θ external (10)
The training pipeline is compatible with different RL algorithms.In our implementation, we adopt REINFORCE++ (Hu et al., 2025) as the RL algorithm for verification.</p>
<p>Experiment</p>
<p>Experimental Setup</p>
<p>Datasets We conduct our evaluation on 6 mathematic reasoning datasets and 3 coding benchmarks: MATH500 (Hendrycks et al., 2021), AIME (AIME'24 and AIME'25), Minerva (Lewkowycz et al., 2022), AMC'23, OlympiadBench (He et al., 2024), MBPP++, HumanEval++ (Liu et al., 2023), and LiveCodeBench (Jain et al., 2024).Training Setup We implement self-aware RL based on verl (Sheng et al., 2024), an open-sourced reinforcement learning pipeline widely used in developing LLM RLVR frameworks.We made necessary adjustment to make it compatible with our GPU servers, with detailed hyperparameter configuration listed in Appendix A.1.We validate self-aware RL on a widely used open-sourced LLM, Qwen2.5-Coder-3B(Hui et al., 2024) given its superior coding capability which serves as a good starting point.We use Qwen2.5-Coder-32B as the external policy model.By default, we set hyperparameter in limit breaking as τ = 0.1 and γ = 5.Due to the inaccurate difficulty prediction at the beginning, we diable limit breaking for the first 50 steps, which is further explained in Section 5.3.We compare self-aware RL to baselines on nine benchmarks in Table 1.Our results indicate that self-aware RL significantly outperform previous baselines.Specifically, we observe that self-aware RL outperforms Qwen2.5-Coder-3B by 53.8% on average on mathematic benchmarks, and by 5.3% on coding benchmarks.The improvement of self-aware RL on coding benchmarks is not as significant as that on mathematical benchmarks as Qwen2.5-Coder-3B is originally strong in coding tasks, leaving little space for further improvement.The obvious improvement on mathematic benchmarks can be attributed to the acquisition of stronger general reasoning capability during RL training with complex reasoning trajectories.Notably, self-aware RL only queried external guidance on 157 out of 12,800 (=1.23%) tasks.Training reward As shown in Figure 2, self-aware RL received higher training reward in comparison to the baseline AZR, which demonstrates the performance improvement brought by self-aware RL.Note that the training reward of self-aware RL is lower at the first , which can be explained by that we adopted different dialogue template, and the policy model needs further tuning to get fitted.After 50 steps, self-aware RL received higher consistently increasing training reward.Difficulty prediction We record the difficulty prediction accuracy measured by R dp in Figure 3.</p>
<p>Experimental Results</p>
<p>Analysis</p>
<p>Observe that the pre-trained model at the first step performs poorly on predicting its own accuracy on the generated tasks, which supports our intuition discussed in Section 4.1 that LLMs are not trained to understand their own capability boundaries.After training with self-aware difficulty prediction for around 50 steps, the accuracy has significantly improved from 0.2 to over 0.6.Based on this observation, in our implementation we did not enable self-aware limit breaking until the 50-th step, since precise measurement of task utility requires accurate prediction on the difficulty of generated tasks.Utility of selected tasks</p>
<p>Selected Filtered</p>
<p>Figure 5: Utility score z(x) of selected and unselected tasks.Selected tasks should be of high utility, and will be proceeded to the external solver.While unselected tasks are of lower utility and are discarded.</p>
<p>Rollout accuracy</p>
<p>In Figure 4 we recorded the accuracy of rollouts sampled from the solver agent.</p>
<p>The initially high accuracy indicates that the generator agent did not create sufficiently challenging tasks.As training progresses, the accuracy gradually decreases and stabilizes around 0.6.This observation supports our intuition from two perspectives: (i) the pre-trained base model are not good at generating appropriate tasks without specific training, and (ii) after training with self-aware RL, the generator can adaptively generate tasks at a suitable difficulty level.Utility of selected tasks and unselected tasks In self-aware limit breaking, the solver agent will be trained on selected high utility tasks to break its capability upper bound.We compare the utility of selected and unselected tasks in Figure 5, which shows that the utility score of selected tasks are significantly higher than that of unselected tasks.This observation demonstrates that the task filter in self-aware limit breaking successfully distinguishes high utility tasks from low utility ones.</p>
<p>Ablation study</p>
<p>We conduct fine-grained ablation study to validate the effectiveness of each components in the design of self-aware RL.In the ablation study we consider three dimensions: (i) the effectiveness of difficulty prediction and limit breaking; (ii) the frequency of querying external guidance in limit breaking; and (iii) the effectiveness of utility ranking of limit breaking.We first compare self-aware RL to self-aware RL w/o limit breaking denoted as self-aware RL − in Table 2. Observe that self-aware RL − can still outperform the AZR baseline by 25.6% on mathematical reasoning benchmarks, which is still a considerable improvement.This ablation study indicates the agent can benefit from being trained to improve the self-awareness on how likely it can successfully solve the task, which perfectly support out intuition that an LLM can generate problems more suitable for improving itself if it knows itself better.Ablating the amount of external guidance In limit breaking, we propose to seek for external guidance on high-quality yet challenging tasks that cannot be solved directly to break through its intrinsic capability ceilings.We vary the hyperparameter τ controlling the frequency of external guidance to learn the behavior of self-aware RL with different level of external guidance.As shown in Table 3, we achieved the highest performance gain when we set τ = 0.1.For a smaller τ , the improvement is still obvious.However, we observe that with a larger τ , the improvement from external guidance becomes ignorable.This can be explained by that overly learning from external guidance will corrupt the original reasoning pattern of the solver, which may requires further adjustment to stabilize the training process.The effectiveness of utility ranking In limit breaking, problems are ranked according to its utility score and high-utility problems are considered to be critical in improving the agent.We validate the effectiveness of utility ranking by randomly shuffling the sorted problems.Table 4 shows that random shuffled utility can only achieve ignorable improvement from 28.0% to 28.4% on mathematic benchmarks, and from 53.3% to 53.6% on coding benchmarks.Without proper utility ranking, limit breaking cannot gain obvious improvement by querying external guidance casually.</p>
<p>Conclusion</p>
<p>In this paper, we tackled the In this paper, we propose self-aware RL, a self-evolving framework that enables an agent to efficiently guide its own learning by leveraging self-awareness.By learning to be self-aware and recognize its own capability limits to proactively request minimal external data, self-aware RL overcomes the common failure modes of naive self-training.Our experiments validate this approach, showing that self-aware RL achieves a 53.2% relative performance gain while using less than 1% of the extra data required by conventional methods.These findings underscore the potential of self-evolving agents to reduce reliance on large-scale human annotation.This work suggests a shift from simply building larger models on large datasets, to creating smarter learners that understand their own knowledge boundaries and adapt to it.Future research could extend this self-aware learning framework to more complex, interactive domains.</p>
<h3>Task: Create a Python Code Snippet (where custom classes are allowed, which should be defined at the top of the code snippet) with one Matching Input Using the reference code snippets provided below as examples, design a new and unique Python code snippet that demands deep algorithmic reasoning to deduce one possible input from a given output.Your submission should include both a code snippet and test input pair, where the input will be plugged into the code snippet to produce the output, which that function output be given to a test subject to come up with any input that will produce the same function output.This is meant to be an I. -Executability, your code should be executable given your input -Difficulty in predicting the output from your provided input and code snippet.Focus on either algorithmic reasoning or logic complexity.For example, you can define complex data structure classes and operate on them like trees, heaps, stacks, queues, graphs, etc, or use complex control flow, dynamic programming, recursions, divide and conquer, greedy, backtracking, etc -Creativity, the code needs to be sufficiently different from the provided reference snippets -Restricted usage of certain keywords and packages, you are not allowed to use the following words in any form, even in comments: raise First, carefully devise a clear plan: e.g., identify how your snippet will be challenging, distinct from reference snippets, and creative.Then, write the final code snippet and its inputs.</h3>
<p>B The Use of Large Language Models(LLMs)</p>
<p>During the preparation of this paper, we use LLMs as a writing assistant to polish up some expressions.Its role was strictly limited to improving grammar, clarity, and readability.The LLM was not used for any substantive aspect of this research, such as the generation of core ideas, the development of the methodology, code implementation, data analysis, or the formulation of conclusions.</p>
<p>Figure 2 :
2
Figure 2: training Reward of self-aware RL stably increases as the training continues.The reward is lower at the first few steps since the dialogue template is more complicated in comparison to the baseline.After the agent has been fitted to the new dialogue template, the training reward of self-aware RL quickly increases and surpasses the baseline reward.</p>
<p>Figure 3 :
3
Figure 3: Accuracy of difficulty prediction.The generator agent driven by the pre-trained base model performs poorly on the task of difficulty prediction (Section 4.1), which is shown by the low accuracy at the first step.After being tuned for 50 steps, the generator agent performs much better.Note that the accuracy shown in this figure is measured by the difficulty prediction reward in Equation 3.</p>
<p>Figure 4 :
4
Figure4: Accuracy of rollouts generated by the solver agent.The accuracy is initially high, reflecting that the generator did not generate challenging tasks without training.As the training continues, the difficulty of generated tasks increases and the rollout accuracy gradually decreases, was finally stabilized around 0.6.</p>
<p>Table 1 :
1
Comparing self-aware RL against baselines.
MethodMATH500 Minerva AIME Olympiad AMC'23 Math Avg HumanEval++ MBPP++ LCB Code AvgCoder-3B49.017.60.015.922.521.067.766.719.351.2AZR43.621.01.720.425.022.371.366.920.853.0self-aware RL63.625.43.329.040.032.370.767.523.653.9</p>
<p>Table 2 :
2
Ablation study: comparing self-aware RL to self-aware RL w/o limit breaking (denoted as self-aware RL − ).
MethodMATH500 Minerva AIME24+25 Olympiad AMC23 Math Avg HumanEval++ MBPP++ LCB Code AvgAZR43.621.01.720.425.022.371.366.920.853.0self-aware RL −59.623.93.325.827.528.070.166.922.953.3self-aware RL63.625.43.329.040.032.370.767.523.653.9</p>
<p>Table 3 :
3
Ablation study: varying the amount of external guidance in limit breaking.
τMATH500 Minerva AIME24+25 Olympiad AMC23 Math Avg HumanEval++ MBPP++ LCB Code Avg059.623.93.325.827.528.070.166.922.953.30.0559.624.61.730.735.030.371.366.723.253.70.163.625.43.329.040.032.370.767.523.653.90.257.223.20.023.935.027.971.367.522.153.6</p>
<p>Table 4 :
4
Ablation study: validating the effectiveness of utility ranking.
ϕMATH500 Minerva AIME24+25 Olympiad AMC23 Math Avg HumanEval++ MBPP++ LCB Code Avgself-aware RL −59.623.93.325.827.528.070.166.922.953.3self-aware RL63.625.43.329.040.032.370.767.523.653.9Shuffled59.223.91.727.130.028.472.067.721.253.6</p>
<p>Q. test.### Code Requirements: -Name the entry function 'f' (e.g., 'def f(...): ...'), you can have nested definitions inside 'f' -Ensure the function returns a value -Include at least one input parameter -Make the function deterministic -Make the snippet require state tracking across multiple data transformations, ensuring the task requires long multi step reasoning -AVOID THE FOLLOWING: * Random functions or variables * Date/time operations * I/O operations (reading files, network requests) * Printing or logging * Any external state -Ensure execution completes within 10 seconds on a modern CPU -All imports and class definitions should be at the very top of the code snippet -The snippet should end with a return statement from the main function 'f', anything after
"'input'John', 'age': 20, 'city': 'New York'"'### Evaluation Criteria:will be removed### Input Requirements:-Provide exactly one test input for your function-Format multiple arguments with commas between them-Remember to add quotes around string arguments### Formatting:-Format your code with: "'pythondef f(...):# your code herereturn ..."'-Format your input with: "'inputarg1, arg2, ..."'### Example Format:"'pythondef f(name: str, info: dict):# code logic herereturn result"'
A Additional Experimental SetupA.1 Training setupOur experiments are conducted on a server with 4 NVIDIA H-200 GPUs.We run the experiment for 200 steps.The code environment is implemented based on the QWQ python executor(Team, 2025).We list the key hyperparameter configuration of verl as follows.A.2 Chat templatesHere we provide chat templates used in our implementation.A part of our implementation is based on AZR(Zhao et al., 2025), therefore the task generation template is similar to that used in AZR.Template for difficulty prediction:### Difficulty Prediction Requirements:-At the end of your generation, you need to review your code and provide a difficulty prediction for the code.-The difficulty prediction should be a number between 0 and 8, where 0 is the easiest and 8 is the hardest.-The review of your code should be in the <review>...</review> tags.The review should focus on analyzing the difficulty of the code based on the complexity of the code, the number of steps required to solve the problem, the creativity of the code, as well as how powerful the current solver is.-You need to control the difficulty of the generated tasks to be medium.A difficulty level at 4 or 5 would be good.-The difficulty prediction should be wrapped in <difficulty prediction> and </difficulty prediction> tags.It should be strictly a number between 0 and 8. Otherwise, you will be penalized.-Therefore, your response should be formatted like <think>...</think> <answer>...</answer> <review>...</review> <difficulty prediction>...</difficulty prediction> Template for task generation:
S Bae, J Hong, M Y Lee, H Kim, J Nam, D Kwak, arXiv:2504.03380Online difficulty filtering for reasoning oriented reinforcement learning. 2025arXiv preprint</p>
<p>X Chen, J Lu, M Kim, D Zhang, J Tang, A Piché, N Gontier, Y Bengio, E Kamalloo, arXiv:2505.14970Self-evolving curriculum for llm reasoning. 2025arXiv preprint</p>
<p>Boosting the generalization and reasoning of vision language models with curriculum reinforcement learning. H Deng, D Zou, R Ma, H Luo, Y Cao, Y Kang, arXiv:2503.070652025arXiv preprint</p>
<p>T Fang, H Zhang, Z Zhang, K Ma, W Yu, H Mi, D Yu, arXiv:2504.21024Webevolver: Enhancing web agent self-improvement with coevolving world model. 2025arXiv preprint</p>
<p>D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, X Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. C He, R Luo, Y Bai, S Hu, Z L Thai, J Shen, J Hu, X Han, Y Huang, Y Zhang, arXiv:2402.140082024arXiv preprint</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021arXiv preprint</p>
<p>Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models. J Hu, J K Liu, H Xu, W Shen, arXiv:2501.032622025arXiv preprint</p>
<p>B Hui, J Yang, Z Cui, J Yang, D Liu, L Zhang, T Liu, J Zhang, B Yu, K Lu, arXiv:2409.12186Qwen2. 5-coder technical report. 2024arXiv preprint</p>
<p>A Jaech, A Kalai, A Lerer, A Richardson, A El-Kishky, A Low, A Helyar, A Madry, A Beutel, A Carney, arXiv:2412.16720Openai o1 system card. 2024arXiv preprint</p>
<p>N Jain, K Han, A Gu, W.-D Li, F Yan, T Zhang, S Wang, A Solar-Lezama, K Sen, I Stoica, arXiv:2403.07974Livecodebench: Holistic and contamination free evaluation of large language models for code. 2024arXiv preprint</p>
<p>N Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V Miranda, A Liu, N Dziri, S Lyu, arXiv:2411.15124Tulu 3: Pushing frontiers in open language model post-training. 2024arXiv preprint</p>
<p>Solving quantitative reasoning problems with language models. A Lewkowycz, A Andreassen, D Dohan, E Dyer, H Michalewski, V Ramasesh, A Slone, C Anil, I Schlag, T Gutman-Solo, Advances in neural information processing systems. 352022</p>
<p>Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. J Liu, C S Xia, Y Wang, L Zhang, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Understanding r1-zero-like training: A critical perspective. Z Liu, C Chen, W Li, P Qi, T Pang, C Du, W S Lee, M Lin, arXiv:2503.207832025arXiv preprint</p>
<p>Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. Z Qi, X Liu, I Iong, H Lai, X Sun, W Zhao, Y Yang, X Yang, J Sun, S Yao, 2025. 2025</p>
<p>Z Shao, P Wang, Q Zhu, R Xu, J Song, X Bi, H Zhang, M Zhang, Y Li, Y Wu, arXiv:2402.03300Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>G Sheng, C Zhang, Z Ye, X Wu, W Zhang, R Zhang, Y Peng, H Lin, C Wu, arXiv:2409.19256Hybridflow: A flexible and efficient rlhf framework. 2024arXiv preprint</p>
<p>K Team, Y Bai, Y Bao, G Chen, J Chen, N Chen, R Chen, Y Chen, Y Chen, Y Chen, arXiv:2507.20534Kimi k2: Open agentic intelligence. 2025arXiv preprint</p>
<p>Qwq-32b: Embracing the power of reinforcement learning. Q Team, 2025</p>
<p>Co-evolving llm coder and unit tester via reinforcement learning. Y Wang, L Yang, Y Tian, K Shen, M Wang, 2025. 2025</p>
<p>C Yang, S Su, S Liu, X Dong, Y Yu, W Su, X Wang, Z Liu, J Zhu, H Li, arXiv:2505.23762Zerogui: Automating online gui learning at zero human cost. 2025arXiv preprint</p>
<p>Dapo: An open-source llm reinforcement learning system at scale. Q Yu, Z Zhang, R Zhu, Y Yuan, X Zuo, Y Yue, W Dai, T Fan, G Liu, L Liu, arXiv:2503.144762025arXiv preprint</p>
<p>Learning like humans: Advancing llm reasoning capabilities via adaptive difficulty curriculum learning and expert-guided selfreformulation. E Zhang, X Yan, W Lin, T Zhang, Q Lu, arXiv:2505.083642025arXiv preprint</p>
<p>Absolute zero: Reinforced self-play reasoning with zero data. A Zhao, Y Wu, Y Yue, T Wu, Q Xu, M Lin, S Wang, Q Wu, Z Zheng, G Huang, arXiv:2505.033352025arXiv preprint</p>
<p>C Zheng, S Liu, M Li, X.-H Chen, B Yu, C Gao, K Dang, Y Liu, R Men, A Yang, arXiv:2507.18071Group sequence policy optimization. 2025arXiv preprint</p>
<p>Y Zhou, S Levine, J Weston, X Li, S Sukhbaatar, arXiv:2506.01716Self-challenging language model agents. 2025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6228 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6228</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6228</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-40c318400809abf5e50aba5a5a80c8012a7715d5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/40c318400809abf5e50aba5a5a80c8012a7715d5" target="_blank">GPTScore: Evaluate as You Desire</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A novel evaluation framework, GPTScore, which utilizes the emergent abilities of generative pre-trained models to score generated texts simply by natural language instructions to overcome several long-standing challenges in text evaluation.</p>
                <p><strong>Paper Abstract:</strong> Generative Artificial Intelligence (AI) has enabled the development of sophisticated models that are capable of producing high-caliber text, images, and other outputs through the utilization of large pre-trained models.Nevertheless, assessing the quality of the generation is an even more arduous task than the generation itself, and this issue has not been given adequate consideration recently.This paper proposes a novel evaluation framework, GPTScore, which utilizes the emergent abilities (e.g., in-context learning, zero-shot instruction) of generative pre-trained models to score generated texts. There are 19 pre-trained models explored in this paper, ranging in size from 80M (e.g., Flan-T5-small) to 175B (e.g., GPT3).Experimental results on four text generation tasks, 22 evaluation aspects, and corresponding 37 datasets demonstrate that this approach can effectively allow us to achieve what one desires to evaluate for texts simply by natural language instructions.This nature helps us overcome several long-standing challenges in text evaluation–how to achieve customized, multi-faceted evaluation without model training. We make our code publicly available.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6228.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6228.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPTScore-Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPTScore (LLM-as-a-judge) applied to Text Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPTScore uses conditional generation probabilities from pre-trained LMs (GPT3 / OPT / Flan-T5 / GPT2 families) as evaluation scores for summarization aspects and compares those scores to dataset human judgments via correlation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Text summarization (SummEval, REALSumm, NEWSROOM, QAGS_XSUM)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Multiple GPTScore variants built on GPT3 family (text-ada-001, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-003) and models from OPT, Flan-T5, GPT2/GPT-J</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Uses existing human judgments from summarization datasets (e.g., SummEval, REALSumm); for cost reasons authors sampled ~40 examples per summarization dataset; human labels cover aspects such as coherence, consistency, fluency, relevance, factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Spearman (primary) and Pearson correlations between automatic (GPTScore) scores and human judgments; comparisons also against baselines (ROUGE variants, BERTScore, MoverScore, PRISM, BARTScore and fine-tuned BARTScore variants).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Instruction-augmented GPTScore (IST) significantly improves correlation relative to vanilla prompting across summarization aspects; many GPT3- and Flan-T5-based GPTScore variants with instruction outperform supervised/fine-tuned baselines (e.g., BARTScore+CNN+Para) on multiple aspects; instruction+demonstration sometimes further improves results. Some GPT3 variants (e.g., text-davinci-003) do not uniformly outperform older variants (text-davinci-001) — davinci-003 often underperforms d01 in many evaluation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Model behavior is opaque: internal structure, training data and fine-tuning (incl. human-feedback) are not disclosed, making explanations for some performance patterns (e.g., d003 < d001) unavailable; cost of API use limits sample sizes; choice of scoring conditioning (src->hypo vs ref->hypo) matters and must match human protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>1) GPT3-text-davinci-003 (tuned with human feedback) performs worse than text-davinci-001 across many summarization settings without clear explanation. 2) Smaller models without proper instruction/demonstration perform worse; adding demonstrations can hurt if demonstrations are poorly chosen. 3) Demonstration gains plateau (little to no gain beyond K>4).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Provide explicit task & aspect instructions (instruction prompting) and appropriate in-context demonstrations; align scoring conditioning to the human-annotation protocol (e.g., src->hypo vs ref->hypo); tune number and choice of exemplars (K) — avoid single biased example for small models; combine aspect definitions with correlated aspects to boost signal; prefer cheaper GPT3 variants (e.g., curie) when they achieve comparable performance; use significance testing to validate improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPTScore: Evaluate as You Desire', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6228.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6228.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPTScore-Data2Text</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPTScore (LLM-as-a-judge) applied to Data-to-Text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPTScore evaluates generated descriptions from tables (BAGEL, SFRES) across informativeness, naturalness, and fluency by measuring LLM conditional probabilities under tailored instruction prompts and comparing to human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Data-to-Text generation (BAGEL, SFRES)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT3 family variants (text-ada/babbage/curie/davinci variants) plus GPT2, OPT, Flan-T5 backbones used for GPTScore variants</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Uses annotated aspect scores from BAGEL and SFRES datasets; authors randomly sampled ~100 samples per data-to-text dataset for evaluation; human labels cover informativeness (INF), naturalness (NAT), and fluency (FLU).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Spearman correlations between GPTScore outputs and human judgments, averaged across aspects and model families; compared against baselines including BARTScore+CNN+Para and other automatic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Instruction (IST) improves correlations vs vanilla; combining instruction with demonstrations (IDM) often yields further gains, but gains are sensitive to exemplar choice. Many instruction+demonstration GPTScore variants outperform fine-tuned BARTScore+CNN+Para on NAT and FLU for SFRES and BAGEL in many settings. However, in some cases (average across backbones) IDM performed worse than IST due to exemplar selection.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Performance highly dependent on demonstration selection; small models can be hurt by few or biased examples; the source format (structured table) complicates src->hypo scoring so ref->hypo is used — this choice may not perfectly mirror human annotation processes.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>1) When demonstrations are poorly chosen for BAGEL/SFRES, the average performance of models with IDM can be worse than instruction-only (IST). 2) Small-model degradation when K=1 demonstration due to example one-sidedness.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Carefully curate demonstrations and tune K (number of in-context exemplars); prefer instruction+demonstration only when exemplar selection is validated; choose scoring conditioning (ref->hypo) appropriate for structured inputs; evaluate cheaper/smaller GPT3 variants with IDM since they can sometimes surpass larger models with proper exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPTScore: Evaluate as You Desire', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6228.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6228.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPTScore-Dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPTScore (LLM-as-a-judge) applied to Dialogue Response Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPTScore maps dialogue evaluation aspects (turn-level and dialogue-level: interestingness, engagement, relevance, coherence, etc.) to instructions and measures LLM conditional likelihoods, then correlates these scores with human FED annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-domain dialogue evaluation (FED turn- and dialogue-level datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Primarily GPT3-based variants (text-ada/babbage/curie/davinci-001/davinci-003) used as GPTScore backbones; authors focused on GPT3 family for dialogue experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Uses FED dataset human judgments (turn-level and dialogue-level, multiple aspects). For cost reasons authors subsampled dialogue data (100 samples per dialogue/data-to-text dataset); FED provides annotator labels across ~9-11 aspects per dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Dataset-level Spearman correlations between GPTScore outputs and FED human judgments; comparisons with fine-tuned dialogue evaluators (FED baseline, DynaEval) and other automatic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>GPT3-based GPTScore achieved strong generalization: some small GPT3 variants (e.g., text-ada-001) achieved comparable performance to fine-tuned baselines (FED and DynaEval) despite far fewer parameters and no fine-tuning. Surprisingly, GPT3-davinci-001 outperformed GPT3-davinci-003 substantially on the FED datasets (d01 > d03), e.g., a large average gap reported (d01 outperforming d03 by large margins in some settings). Combining aspect definitions (adding correlated aspects) improved performance of smaller models to exceed that of larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Opaque model differences across GPT3 variants (d003 worse than d001) lack explanation; instruction design and aspect composition strongly influence results; dataset-level evaluation strategy required for dialogue (different from sample-level used elsewhere), so alignment of scoring protocol to human annotation is essential; potential for injected biases from training/human-feedback datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>1) Significant performance divergence between models of identical size but different tuning (d01 vs d03) leading to inconsistent evaluator behavior. 2) Without instruction or with suboptimal prompts, GPTScore performance degrades compared to instruction-enhanced versions. 3) Aspect-definition mismatch can reduce correlation (need to compose/augment aspect definitions).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use instruction prompts and in-context demonstrations; craft aspect definitions and—where beneficial—augment them with highly-correlated aspect definitions to boost concordance with human judgments; select the dataset-level aggregation strategy consistent with human annotation practice; validate evaluator choice (model variant and prompt) for dialogue specifically rather than assuming larger models are always better.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPTScore: Evaluate as You Desire', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6228.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6228.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPTScore-MT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPTScore (LLM-as-a-judge) applied to Machine Translation (MQM-2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPTScore evaluates translation outputs (Chinese->English MQM-2020) for accuracy, fluency, and MQM quality by scoring hypothesis conditioned on reference (ref->hypo) and correlating with MQM human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Machine Translation (MQM-2020 Chinese->English)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT3 family (various text-ada/babbage/curie/davinci variants) and GPT2/OPT/Flan-T5 backbones used for comparative GPTScore variants.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Uses MQM-2020 human annotations (accuracy, fluency, MQM scores); authors subsampled the dataset for evaluation and aligned scoring conditioning to reference (ref->hypo) since source language differs from hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Sample-level Spearman correlations between GPTScore scores and MQM human ratings; compared with baselines (ROUGE, BERTScore, PRISM, BARTScore variants).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Instruction improves performance across ACC, FLU, and MQM; combining instruction with demonstrations (IDM) brings additional gains for many model families; GPT3-c01 (curie) often matches or is comparable to the more expensive GPT3-davinci variants, making cheaper variants viable choices for MT evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Need to adapt scoring conditioning to cross-lingual setting (ref->hypo); small-model performance sensitive to number and quality of demonstrations; opaque differences across GPT3 variants remain unexplained; limited language coverage in the evaluated LMs (authors did not include GPT-3.5/ChatGPT/GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>1) Small demonstration counts (K) can lead to unstable gains and sometimes degrade performance for small models (e.g., K=1). 2) Some models plateau quickly with more examples (K>4) yielding diminishing returns.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use instruction prompting and include a modest number of well-chosen demonstrations (empirically K up to ~4 shows benefit); choose ref->hypo scoring for MT to match human annotation; select cost-efficient model variants (e.g., GPT3-curie) when they empirically match larger models; perform significance testing and ablations to confirm gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPTScore: Evaluate as You Desire', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BARTScore: Evaluating Generated Text as Text Generation <em>(Rating: 2)</em></li>
                <li>Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation <em>(Rating: 2)</em></li>
                <li>Experts, errors, and context: A large-scale study of human evaluation for machine translation <em>(Rating: 2)</em></li>
                <li>Unsupervised evaluation of interactive dialog with dialogpt (FED) <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6228",
    "paper_id": "paper-40c318400809abf5e50aba5a5a80c8012a7715d5",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "GPTScore-Summarization",
            "name_full": "GPTScore (LLM-as-a-judge) applied to Text Summarization",
            "brief_description": "GPTScore uses conditional generation probabilities from pre-trained LMs (GPT3 / OPT / Flan-T5 / GPT2 families) as evaluation scores for summarization aspects and compares those scores to dataset human judgments via correlation metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Text summarization (SummEval, REALSumm, NEWSROOM, QAGS_XSUM)",
            "llm_judge_model": "Multiple GPTScore variants built on GPT3 family (text-ada-001, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-003) and models from OPT, Flan-T5, GPT2/GPT-J",
            "human_evaluation_setup": "Uses existing human judgments from summarization datasets (e.g., SummEval, REALSumm); for cost reasons authors sampled ~40 examples per summarization dataset; human labels cover aspects such as coherence, consistency, fluency, relevance, factuality.",
            "metrics_compared": "Spearman (primary) and Pearson correlations between automatic (GPTScore) scores and human judgments; comparisons also against baselines (ROUGE variants, BERTScore, MoverScore, PRISM, BARTScore and fine-tuned BARTScore variants).",
            "reported_differences": "Instruction-augmented GPTScore (IST) significantly improves correlation relative to vanilla prompting across summarization aspects; many GPT3- and Flan-T5-based GPTScore variants with instruction outperform supervised/fine-tuned baselines (e.g., BARTScore+CNN+Para) on multiple aspects; instruction+demonstration sometimes further improves results. Some GPT3 variants (e.g., text-davinci-003) do not uniformly outperform older variants (text-davinci-001) — davinci-003 often underperforms d01 in many evaluation settings.",
            "llm_specific_limitations": "Model behavior is opaque: internal structure, training data and fine-tuning (incl. human-feedback) are not disclosed, making explanations for some performance patterns (e.g., d003 &lt; d001) unavailable; cost of API use limits sample sizes; choice of scoring conditioning (src-&gt;hypo vs ref-&gt;hypo) matters and must match human protocol.",
            "notable_failure_cases": "1) GPT3-text-davinci-003 (tuned with human feedback) performs worse than text-davinci-001 across many summarization settings without clear explanation. 2) Smaller models without proper instruction/demonstration perform worse; adding demonstrations can hurt if demonstrations are poorly chosen. 3) Demonstration gains plateau (little to no gain beyond K&gt;4).",
            "mitigation_strategies": "Provide explicit task & aspect instructions (instruction prompting) and appropriate in-context demonstrations; align scoring conditioning to the human-annotation protocol (e.g., src-&gt;hypo vs ref-&gt;hypo); tune number and choice of exemplars (K) — avoid single biased example for small models; combine aspect definitions with correlated aspects to boost signal; prefer cheaper GPT3 variants (e.g., curie) when they achieve comparable performance; use significance testing to validate improvements.",
            "uuid": "e6228.0",
            "source_info": {
                "paper_title": "GPTScore: Evaluate as You Desire",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "GPTScore-Data2Text",
            "name_full": "GPTScore (LLM-as-a-judge) applied to Data-to-Text",
            "brief_description": "GPTScore evaluates generated descriptions from tables (BAGEL, SFRES) across informativeness, naturalness, and fluency by measuring LLM conditional probabilities under tailored instruction prompts and comparing to human ratings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Data-to-Text generation (BAGEL, SFRES)",
            "llm_judge_model": "GPT3 family variants (text-ada/babbage/curie/davinci variants) plus GPT2, OPT, Flan-T5 backbones used for GPTScore variants",
            "human_evaluation_setup": "Uses annotated aspect scores from BAGEL and SFRES datasets; authors randomly sampled ~100 samples per data-to-text dataset for evaluation; human labels cover informativeness (INF), naturalness (NAT), and fluency (FLU).",
            "metrics_compared": "Spearman correlations between GPTScore outputs and human judgments, averaged across aspects and model families; compared against baselines including BARTScore+CNN+Para and other automatic metrics.",
            "reported_differences": "Instruction (IST) improves correlations vs vanilla; combining instruction with demonstrations (IDM) often yields further gains, but gains are sensitive to exemplar choice. Many instruction+demonstration GPTScore variants outperform fine-tuned BARTScore+CNN+Para on NAT and FLU for SFRES and BAGEL in many settings. However, in some cases (average across backbones) IDM performed worse than IST due to exemplar selection.",
            "llm_specific_limitations": "Performance highly dependent on demonstration selection; small models can be hurt by few or biased examples; the source format (structured table) complicates src-&gt;hypo scoring so ref-&gt;hypo is used — this choice may not perfectly mirror human annotation processes.",
            "notable_failure_cases": "1) When demonstrations are poorly chosen for BAGEL/SFRES, the average performance of models with IDM can be worse than instruction-only (IST). 2) Small-model degradation when K=1 demonstration due to example one-sidedness.",
            "mitigation_strategies": "Carefully curate demonstrations and tune K (number of in-context exemplars); prefer instruction+demonstration only when exemplar selection is validated; choose scoring conditioning (ref-&gt;hypo) appropriate for structured inputs; evaluate cheaper/smaller GPT3 variants with IDM since they can sometimes surpass larger models with proper exemplars.",
            "uuid": "e6228.1",
            "source_info": {
                "paper_title": "GPTScore: Evaluate as You Desire",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "GPTScore-Dialogue",
            "name_full": "GPTScore (LLM-as-a-judge) applied to Dialogue Response Generation",
            "brief_description": "GPTScore maps dialogue evaluation aspects (turn-level and dialogue-level: interestingness, engagement, relevance, coherence, etc.) to instructions and measures LLM conditional likelihoods, then correlates these scores with human FED annotations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Open-domain dialogue evaluation (FED turn- and dialogue-level datasets)",
            "llm_judge_model": "Primarily GPT3-based variants (text-ada/babbage/curie/davinci-001/davinci-003) used as GPTScore backbones; authors focused on GPT3 family for dialogue experiments.",
            "human_evaluation_setup": "Uses FED dataset human judgments (turn-level and dialogue-level, multiple aspects). For cost reasons authors subsampled dialogue data (100 samples per dialogue/data-to-text dataset); FED provides annotator labels across ~9-11 aspects per dataset.",
            "metrics_compared": "Dataset-level Spearman correlations between GPTScore outputs and FED human judgments; comparisons with fine-tuned dialogue evaluators (FED baseline, DynaEval) and other automatic metrics.",
            "reported_differences": "GPT3-based GPTScore achieved strong generalization: some small GPT3 variants (e.g., text-ada-001) achieved comparable performance to fine-tuned baselines (FED and DynaEval) despite far fewer parameters and no fine-tuning. Surprisingly, GPT3-davinci-001 outperformed GPT3-davinci-003 substantially on the FED datasets (d01 &gt; d03), e.g., a large average gap reported (d01 outperforming d03 by large margins in some settings). Combining aspect definitions (adding correlated aspects) improved performance of smaller models to exceed that of larger models.",
            "llm_specific_limitations": "Opaque model differences across GPT3 variants (d003 worse than d001) lack explanation; instruction design and aspect composition strongly influence results; dataset-level evaluation strategy required for dialogue (different from sample-level used elsewhere), so alignment of scoring protocol to human annotation is essential; potential for injected biases from training/human-feedback datasets.",
            "notable_failure_cases": "1) Significant performance divergence between models of identical size but different tuning (d01 vs d03) leading to inconsistent evaluator behavior. 2) Without instruction or with suboptimal prompts, GPTScore performance degrades compared to instruction-enhanced versions. 3) Aspect-definition mismatch can reduce correlation (need to compose/augment aspect definitions).",
            "mitigation_strategies": "Use instruction prompts and in-context demonstrations; craft aspect definitions and—where beneficial—augment them with highly-correlated aspect definitions to boost concordance with human judgments; select the dataset-level aggregation strategy consistent with human annotation practice; validate evaluator choice (model variant and prompt) for dialogue specifically rather than assuming larger models are always better.",
            "uuid": "e6228.2",
            "source_info": {
                "paper_title": "GPTScore: Evaluate as You Desire",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "GPTScore-MT",
            "name_full": "GPTScore (LLM-as-a-judge) applied to Machine Translation (MQM-2020)",
            "brief_description": "GPTScore evaluates translation outputs (Chinese-&gt;English MQM-2020) for accuracy, fluency, and MQM quality by scoring hypothesis conditioned on reference (ref-&gt;hypo) and correlating with MQM human labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Machine Translation (MQM-2020 Chinese-&gt;English)",
            "llm_judge_model": "GPT3 family (various text-ada/babbage/curie/davinci variants) and GPT2/OPT/Flan-T5 backbones used for comparative GPTScore variants.",
            "human_evaluation_setup": "Uses MQM-2020 human annotations (accuracy, fluency, MQM scores); authors subsampled the dataset for evaluation and aligned scoring conditioning to reference (ref-&gt;hypo) since source language differs from hypothesis.",
            "metrics_compared": "Sample-level Spearman correlations between GPTScore scores and MQM human ratings; compared with baselines (ROUGE, BERTScore, PRISM, BARTScore variants).",
            "reported_differences": "Instruction improves performance across ACC, FLU, and MQM; combining instruction with demonstrations (IDM) brings additional gains for many model families; GPT3-c01 (curie) often matches or is comparable to the more expensive GPT3-davinci variants, making cheaper variants viable choices for MT evaluation.",
            "llm_specific_limitations": "Need to adapt scoring conditioning to cross-lingual setting (ref-&gt;hypo); small-model performance sensitive to number and quality of demonstrations; opaque differences across GPT3 variants remain unexplained; limited language coverage in the evaluated LMs (authors did not include GPT-3.5/ChatGPT/GPT-4).",
            "notable_failure_cases": "1) Small demonstration counts (K) can lead to unstable gains and sometimes degrade performance for small models (e.g., K=1). 2) Some models plateau quickly with more examples (K&gt;4) yielding diminishing returns.",
            "mitigation_strategies": "Use instruction prompting and include a modest number of well-chosen demonstrations (empirically K up to ~4 shows benefit); choose ref-&gt;hypo scoring for MT to match human annotation; select cost-efficient model variants (e.g., GPT3-curie) when they empirically match larger models; perform significance testing and ablations to confirm gains.",
            "uuid": "e6228.3",
            "source_info": {
                "paper_title": "GPTScore: Evaluate as You Desire",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BARTScore: Evaluating Generated Text as Text Generation",
            "rating": 2
        },
        {
            "paper_title": "Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation",
            "rating": 2
        },
        {
            "paper_title": "Experts, errors, and context: A large-scale study of human evaluation for machine translation",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised evaluation of interactive dialog with dialogpt (FED)",
            "rating": 2
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2
        }
    ],
    "cost": 0.016462499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GPTScore: Evaluate as You Desire</h1>
<p>Jinlan $\mathbf{F u}^{1}$, See-Kiong $\mathbf{N g}^{1}$, Zhengbao Jiang ${ }^{2}$, Pengfei Liu ${ }^{3}$,<br>${ }^{1}$ National University of Singapore, ${ }^{2}$ Carnegie Mellon University<br>${ }^{3}$ Shanghai Jiao Tong University<br>jinlanjonna@gmail.com, pengfei@sjtu.edu.cn</p>
<h4>Abstract</h4>
<p>Generative Artificial Intelligence (AI) has enabled the development of sophisticated models that are capable of producing high-caliber text, images, and other outputs through the utilization of large pre-trained models. Nevertheless, assessing the quality of the generation is an even more arduous task than the generation itself, and this issue has not been given adequate consideration recently. This paper proposes a novel evaluation framework, GPTSCORE, which utilizes the emergent abilities (e.g., in-context learning, zero-shot instruction) of generative pre-trained models to score generated texts. There are 19 pre-trained models explored in this paper, ranging in size from 80M (e.g., Flan-T5-small) to 175B (e.g., GPT3). Experimental results on four text generation tasks, 22 evaluation aspects, and corresponding 37 datasets demonstrate that this approach can effectively allow us to achieve what one desires to evaluate for texts simply by natural language instructions. This nature helps us overcome several long-standing challenges in text evaluation-how to achieve customized, multi-faceted evaluation without model training. We make our code publicly available. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>The advent of generative pre-trained models, such as GPT3 (Brown et al., 2020), has precipitated a shift from analytical AI to generative AI across multiple domains (Sequoia, 2022). Take text as an example: the use of a large pre-trained model with appropriate prompts (Liu et al., 2021) has achieved superior performance in tasks defined both in academia (Sanh et al., 2021) and scenarios from the real world (Ouyang et al., 2022). While text generation technology is advancing rapidly, techniques for evaluating the quality of these texts lag far behind. This is especially evident in the following ways:</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of text evaluation approaches.
(1) Existing studies (Ghazarian et al., 2022; Ye et al., 2021) evaluate text quality with limited aspects (e.g., fluency) (Fig. 1-(a)), which are usually customized prohibitively, making it harder for users to evaluate aspects as they need (Freitag et al., 2021). (2) A handful of studies (Yuan et al., 2021; Scialom et al., 2021; Zhong et al., 2022; Li et al., 2021; Mehri and Eskénazi, 2020a) have examined multi-aspect evaluation but lack carefully studied aspects' definitions and their relationship. Moreover, the specific aspect evaluations are empirically bound with metric variants (Fig. 1-(b)). (3) Rely on annotated samples and model training. Most of the above methods necessitate complicated supervised training or costly manual annotation of samples (Fig. 1-(a,b)). This makes these methods hard to use in industrial settings and adapt to new evaluation aspects required by users.</p>
<p>In this paper, we demonstrated the talent of the super large pre-trained language model (e.g., GPT-3) in achieving multi-aspect, customized, and training-free evaluation (Fig. 1-(c)). Essentially, it skillfully utilizes the pre-trained model's zeroshot instruction (Chung et al., 2022) and in-context learning (Brown et al., 2020; Min et al., 2022) ability to deal with complex and ever-changing evaluation needs while solving multiple evaluation challenges that have plagued many years. Specifically, given a text generated from a specific context (e.g., source text in text summarization) and a desirable evaluation aspect (e.g., fluency), the high-level idea of the proposed framework is that</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The framework of GPTSCORE. We include two evaluation aspects <em>relevance (REL)</em> and <em>informative (INF)</em> in this figure and use the evaluation of <em>relevance (REL)</em> of the text summarization task to exemplify our framework.</p>
<p>The higher-quality text for a specific aspect will be more likely generated than unqualified ones, where the "likely" can be measured by the conditional generation probability.</p>
<h3>How to perform an evaluation as the user desires?</h3>
<p>As illustrated in Fig. 2, to capture users' true desires, an <em>evaluation protocol</em> ² will be initially established based on (a) the <em>task specification</em>, which typically outlines how the text is generated (e.g., generate a response for a human based on the conversation); (b) <em>aspect definition</em> that documents the details of desirable evaluation aspects (e.g., the response should be intuitive to understand); (c) <em>demonstrated samples</em>: a handful of well-labeled samples are required to teach the model which sample is qualified. Subsequently, each evaluation sample will be presented with the evaluated protocol with optionally moderate exemplar samples, which could facilitate the model's learning. Lastly, a <strong>generative pre-trained</strong> model will be used to calculate how likely the text could be generated based on the above evaluation protocol, thus giving rise to our model's name: GPTSCORE. Given the plethora of pre-trained models, we instantiate our framework with different backbones: GPT2 (Radford et al., 2019), OPT (Zhang et al., 2022b), Flan-T5 (Chung et al., 2022), and GPT3 (instruction-based (Ouyang et al., 2022)) due to their superior capacity for <em>zero-shot instruction</em> and their aptitude for <em>in-context learning</em>.</p>
<p>Experimentally, we ran through almost all common natural language generation tasks in NLP, and the results showed the power of this new paradigm. The main observations are listed as follows: (1) GPTScore performs better when instructed by the definition of task and aspect. Furthermore, incorporating suitable exemplified samples with in-context learning will further enhance the process. (2) Different evaluation aspects exhibit certain correlations. By incorporating definitions with other highly correlated aspects (e.g., interesting and engaging), the performance of the smaller model (GPT3-curie, 6.7B) can surpass the larger model (GPT3-davinci, 175B). (3) The GPTscore performs better than fine-tuned models across tasks such as text summarization, data-to-text, and dialogue response generation. (4) The performance of <em>GPT3-text-davinci-003</em>, which is tuned based on human feedback, is inferior to <em>GPT3-text-davinci-001</em> in the majority of the evaluation settings.</p>
<p>Our main <strong>contributions</strong> in this paper are:</p>
<ol>
<li>We propose a newly generated text scoring framework, GPTScore, which utilizes the emergent ability of large language models to achieve multi-aspect, customized, and training-free evaluation.</li>
<li>We comprehensively explore GPTScore, studying 19 language models (ranging in size from 80M to 175B) and four popular text generation tasks. Experiments demonstrate that training-free GPTScore outperforms fine-tuning models and achieves higher human correspondence.</li>
<li>We design and demonstrate the feasibility of custom evaluation for a new aspect by the GPTScore framework with training-free.</li>
<li>We summarize some observations of the opaque GPT3 family and other backbone models in the evaluation and try to give explanations.</li>
</ol>
<h3>2 Related Work</h3>
<p><strong>Similarity-based Metrics</strong> measures the similarity between the generated text and the reference text. It includes two types: (1) lexical overlap-based</p>
<p><sup>2</sup>To better understand how to design the evaluation protocols, we give all the evaluation protocols for the different tasks and aspects studied in this work in the Appendix F.</p>
<p>metrics, e.g., BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004); (2) embedding-based metrics, e.g., BERTScore (Zhang et al., 2020) and MoverScore (Zhao et al., 2019).</p>
<p>Single-aspect Evaluator refers to evaluators designed to evaluate the quality of a specific aspect or overall of the generated text. For example, DEAM (Ghazarian et al., 2022) and QuantiDCE (Ye et al., 2021) were proposed for the evaluation of the coherence of the dialogue system; several evaluators (Cao et al., 2020; Durmus et al., 2020; Wang et al., 2020a) are designed for the evaluation of the consistency of text summarization.</p>
<p>Multi-aspect Evaluator refers to one evaluator handle several evaluation aspects by using different input and output text pair (Yuan et al., 2021), different prompt designed by the aspect name (Zhong et al., 2022; Mehri and Eskénazi, 2020a), (Mehri and Eskénazi, 2020b), different formulas (Scialom et al., 2021). Unlike (Zhong et al., 2022; Mehri and Eskénazi, 2020a) which only consider the vague aspect description, we fully considered exhaustive aspect definition and their relationship.</p>
<p>Emergent Ability Recent works have revealed various emergent abilities of generative pre-trained language models, such as, in-context learning (Min et al., 2022), chain-of-thought reasoning (Wei et al., 2022), and zero-shot instruction (Ouyang et al., 2022). These abilities allow large language models to achieve good performance without training.</p>
<p>What is the difference between our work and BARTScore? Yuan et al. (2021) demonstrates the feasibility of using the probability of text generation as a text quality score, and fine-tuning is required to achieve better performance. However, the model fine-tuning cost a lot, and it is hard for us to fine-tune a scoring model for each task and each domain. In this work, we focus on proposing a new framework that allows the generated text evaluation to achieve customizable, multi-faceted, and train-free evaluation. To achieve this target, (1) we utilized the emergent ability of language models, such as in-context learning, zero-shot instruction et al., to build the GPTScore. (2) By studying the GPTScore framework on 19 language models covering four backbones, we demonstrate that (a) GPTScore outperforms the fine-tuned BARTScore; (b) GPTScore can be customized for a new evaluation aspect with a labeled handful of samples while BARTScore cannot do this.</p>
<h2>3 Generative Pretraining Score (GPTScore)</h2>
<p>The core idea of GPTSCORE is that a generative pre-training model will assign a higher probability of high-quality generated text following a given instruction and context. In our method, the instruction is composed of the task description $d$ and the aspect definition $a$. Specifically, suppose that the text to be evaluated is $\boldsymbol{h}=\left{h_{1}, h_{2}, \cdots, h_{m}\right}$, the context information is $\mathcal{S}$ (e.g., source text or reference text), then GPTSCORE is defined as the following conditional probability:</p>
<p>$$
\begin{aligned}
&amp; \operatorname{GPTScore}(\boldsymbol{h} \mid d, a, \mathcal{S})= \
&amp; \sum_{t=1}^{m} w_{t} \log p\left(h_{t} \mid \boldsymbol{h}_{&lt;t}, T(d, a, \mathcal{S}), \theta\right)
\end{aligned}
$$</p>
<p>where $w_{t}$ is the weight of the token at position $t$. In our work, we treat each token equally. $T(\cdot)$ is a prompt template that defines the evaluation protocol, which is usually task-dependent and specified manually through prompt engineering.</p>
<p>Few-shot with Demonstration The generative pre-trained language model can better perform tasks when prefixed with a few annotated samples (i.e., demonstrations). Our proposed framework is flexible in supporting this by extending the prompt template $T$ with demonstrations.</p>
<p>Choice of Prompt Template Prompt (Liu et al., 2021; Fu et al., 2022) templates define how task description, aspect definition, and context are organized. In this work, for the GPT3-based model, we opt for prompts that are officially provided by OpenAI. ${ }^{3}$ For instruction-based pre-trained models, we use prompts from NaturalInstruction (Wang et al., 2022) since it's the main training source for those instruction-based pre-train models. Taking the evaluation of the fluency of the text summarization task as an example, based on the prompt provided by OpenAI, ${ }^{4}$ the task prompt is "{Text} Tl;dr {Summary}", the definition of fluency is "Is the generated text well-written and grammatical?" (in Tab. 1), and then the final prompt template is "Generate a fluent and grammatical summary for the following text: {Text} Tl;dr {Summary}", where demonstrations could be introduced by repeating instantiating "{Text} Tl;dr {Summary} " In Appendix F, we list the prompts for various aspects of all tasks studied in this work and</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Aspect</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Semantic Coverage (COV)</td>
<td style="text-align: center;">Summ</td>
<td style="text-align: center;">How many semantic content units from the reference text are covered by the generated text?</td>
</tr>
<tr>
<td style="text-align: center;">Factuality (FAC)</td>
<td style="text-align: center;">Summ</td>
<td style="text-align: center;">Does the generated text preserve the factual statements of the source text?</td>
</tr>
<tr>
<td style="text-align: center;">Consistency (CON)</td>
<td style="text-align: center;">Summ, Diag</td>
<td style="text-align: center;">Is the generated text consistent in the information it provides?</td>
</tr>
<tr>
<td style="text-align: center;">Informativeness (INF)</td>
<td style="text-align: center;">Summ, D2T, Diag</td>
<td style="text-align: center;">How well does the generated text capture the key ideas of its source text?</td>
</tr>
<tr>
<td style="text-align: center;">Coherence (COH)</td>
<td style="text-align: center;">Summ, Diag</td>
<td style="text-align: center;">How much does the generated text make sense?</td>
</tr>
<tr>
<td style="text-align: center;">Relevance (REL)</td>
<td style="text-align: center;">Diag, Summ, D2T</td>
<td style="text-align: center;">How well is the generated text relevant to its source text?</td>
</tr>
<tr>
<td style="text-align: center;">Fluency (FLU)</td>
<td style="text-align: center;">Diag, Summ, D2T, MT</td>
<td style="text-align: center;">Is the generated text well-written and grammatical?</td>
</tr>
<tr>
<td style="text-align: center;">Accuracy (ACC)</td>
<td style="text-align: center;">MT</td>
<td style="text-align: center;">Are there inaccuracies, missing, or unfactual content in the generated text?</td>
</tr>
<tr>
<td style="text-align: center;">MQM</td>
<td style="text-align: center;">MT</td>
<td style="text-align: center;">How is the overall quality of the generated text?</td>
</tr>
<tr>
<td style="text-align: center;">Interest (INT)</td>
<td style="text-align: center;">Diag</td>
<td style="text-align: center;">Is the generated text interesting?</td>
</tr>
<tr>
<td style="text-align: center;">Engagement (ENG)</td>
<td style="text-align: center;">Diag</td>
<td style="text-align: center;">Is the generated text engaging?</td>
</tr>
<tr>
<td style="text-align: center;">Specific (SPE)</td>
<td style="text-align: center;">Diag</td>
<td style="text-align: center;">Is the generated text generic or specific to the source text?</td>
</tr>
<tr>
<td style="text-align: center;">Correctness (COR)</td>
<td style="text-align: center;">Diag</td>
<td style="text-align: center;">Is the generated text correct or was there a misunderstanding of the source text?</td>
</tr>
<tr>
<td style="text-align: center;">Semantically</td>
<td style="text-align: center;">Diag</td>
<td style="text-align: center;">Is the generated text semantically appropriate?</td>
</tr>
<tr>
<td style="text-align: center;">appropriate (SEM)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Understandability (UND)</td>
<td style="text-align: center;">Diag</td>
<td style="text-align: center;">Is the generated text understandable?</td>
</tr>
<tr>
<td style="text-align: center;">Error Recovery (ERR)</td>
<td style="text-align: center;">Diag</td>
<td style="text-align: center;">Is the system able to recover from errors that it makes?</td>
</tr>
<tr>
<td style="text-align: center;">Diversity (DIV)</td>
<td style="text-align: center;">Diag</td>
<td style="text-align: center;">Is there diversity in the system responses?</td>
</tr>
<tr>
<td style="text-align: center;">Depth (DEP)</td>
<td style="text-align: center;">Diag</td>
<td style="text-align: center;">Does the system discuss topics in depth?</td>
</tr>
<tr>
<td style="text-align: center;">Likeability (LIK)</td>
<td style="text-align: center;">Diag</td>
<td style="text-align: center;">Does the system display a likeable personality?</td>
</tr>
<tr>
<td style="text-align: center;">Flexibility (FLE)</td>
<td style="text-align: center;">Diag</td>
<td style="text-align: center;">Is the system flexible and adaptable to the user and their interests?</td>
</tr>
<tr>
<td style="text-align: center;">Inquisitiveness (INQ)</td>
<td style="text-align: center;">Diag</td>
<td style="text-align: center;">Is the system inquisitive throughout the conversation?</td>
</tr>
</tbody>
</table>
<p>Table 1: The definition of aspects evaluated in this work. Semantic App. denotes semantically appropriate aspect. Diag, Summ, D2T, and MT denote the dialogue response generation, text summarization, data to text and machine translation, respectively. "MQM" is the short name of Multidimensional Quality Metrics.
leave a more comprehensive exploration on prompt engineering as a future work.</p>
<p>Selection of Scoring Dimension GPTSCORE exhibits different variants in terms of diverse choices of texts being calculated. For example, given a generated hypothesis, we can calculate GPTSCORE either based on the source text (i.e., $\operatorname{src}-&gt;h y p o, p($ hypo|src $)$ ) or based on the gold reference (i.e., ref-&gt;hypo, $p($ hypo|ref)). In this paper, the criteria for choosing GPTSCORE variants are mainly designed to align the protocol of human judgments (Liu et al., 2022) that are used to evaluate the reliability of automated metrics. We will detail this based on different human judgment datasets in the experiment section.</p>
<h2>4 Experimental Settings</h2>
<h3>4.1 Meta Evaluation</h3>
<p>Meta evaluation aims to evaluate the reliability of automated metrics by calculating how well automated scores $\left(y_{\text {auto }}\right)$ correlate with human judgment $\left(y_{\text {human }}\right)$ using correlation functions $g\left(y_{\text {auto }}, y_{\text {human }}\right)$ such as spearman correlation. In this work, we adopt two widely-used correlation measures: (1) Spearman correlation $(\rho)$ (Zar, 2005) and (2) Pearson correlation $(r)$ (Mukaka, 2012).</p>
<h3>4.2 Tasks, Datasets, and Aspects</h3>
<p>To achieve a comprehensive evaluation, in this paper, we cover a broad range of natural language generation tasks: Dialogue Response Generation, Text Summarization, Data-to-Text, and Machine</p>
<p>Translation, which involves 37 datasets and 22 evaluation aspects in total. Tab. 8 summarizes the tasks, datasets, and evaluation aspects considered by each dataset. The definition of different aspects can be found in Tab. 1. More detailed illustrations about the datasets can be found in Appendix D.
(1) Dialogue Response Generation aims to generate an engaging and informative response based on the dialogue history. We adopt the FED (Mehri and Eskénazi, 2020a) datasets and consider both turn-level and dialogue-level evaluations. (2) Text Summarization is a task of automatically generating informative summary for a given long text. We adopt SummEval (Bhandari et al., 2020), REALSumm (Bhandari et al., 2020), NEWSROOM (Grusky et al., 2018), and QAGS_XSUM (Wang et al., 2020b) datasets. (3) Data-to-Text aims to generate a fluent and factual description for a given table. We consider BAGEL (Mairesse et al., 2010) and SFRES (Wen et al., 2015) datasets. (4) Machine Translation aims to translate a sentence from one language to another. We consider a subdatasets of Multidimensional Quality Metrics (MQM) (Freitag et al., 2021), namely, MQM-2020 (Chinese-&gt;English).</p>
<h3>4.3 Scoring Models</h3>
<p>ROUGE (Lin, 2004) is a popular automatic generation evaluation metric. We consider three variants ROUGE-1, ROUGE-2, and ROUGE-L. PRISM (Thompson and Post, 2020) is a referencebased evaluation method designed for machine translation with pre-trained paraphrase systems.</p>
<p>BERTScore (Zhang et al., 2020) uses contextual representation from BERT to calculate the similarity between the generated text and the reference text. MoverScore (Zhao et al., 2019) considers both contextual representation and Word Mover's Distance (Kusner et al., 2015). DynaEval (Zhang et al., 2021) is a unified automatic evaluation framework for dialogue response generation tasks on the turn level and dialogue level. BARTScore (Yuan et al., 2021) is a text-scoring model based on BART (Lewis et al., 2020) without fine-tuning. BARTScore+CNN and BARTScore+CNN+Para are the variants of BARTScore, the former is finetuned on the CNNDM dataset (Hermann et al., 2015), and the latter is fine-tuned on CNNDM and Paraphrase2.0 (Hu et al., 2019). GPTSCORE is our evaluation method, designed based on 19 pre-trained language models, covering GPT3, OPT, Flan-T5, and GPT2 backbones. Tab. 2 shows model variants used in this paper and their number of parameters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">GPT3</th>
<th style="text-align: right;">Param. OPT</th>
<th style="text-align: left;"></th>
<th style="text-align: right;">Param.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT3-a01 (text-ada-001)</td>
<td style="text-align: right;">350 M</td>
<td style="text-align: left;">OPT350M</td>
<td style="text-align: right;">350 M</td>
</tr>
<tr>
<td style="text-align: left;">GPT3-b01 (text-babbage-001)</td>
<td style="text-align: right;">1.3 B</td>
<td style="text-align: left;">OPT-1.3B</td>
<td style="text-align: right;">1.3 B</td>
</tr>
<tr>
<td style="text-align: left;">GPT3-c01 (text-curic-001)</td>
<td style="text-align: right;">6.7 B</td>
<td style="text-align: left;">OPT-6.7B</td>
<td style="text-align: right;">6.7 B</td>
</tr>
<tr>
<td style="text-align: left;">GPT3-d01 (text-davinci-001)</td>
<td style="text-align: right;">175 B</td>
<td style="text-align: left;">OPT-13B</td>
<td style="text-align: right;">13 B</td>
</tr>
<tr>
<td style="text-align: left;">GPT3-d03 (text-davinci-003)</td>
<td style="text-align: right;">175 B</td>
<td style="text-align: left;">OPT-66B</td>
<td style="text-align: right;">66 B</td>
</tr>
<tr>
<td style="text-align: left;">Flan-T5</td>
<td style="text-align: right;">Param. GPT2</td>
<td style="text-align: left;"></td>
<td style="text-align: right;">Param.</td>
</tr>
<tr>
<td style="text-align: left;">FT5-small</td>
<td style="text-align: right;">80 M</td>
<td style="text-align: left;">GPT2-M</td>
<td style="text-align: right;">355 M</td>
</tr>
<tr>
<td style="text-align: left;">FT5-base</td>
<td style="text-align: right;">250 M</td>
<td style="text-align: left;">GPT2-L</td>
<td style="text-align: right;">774 M</td>
</tr>
<tr>
<td style="text-align: left;">FT5-L</td>
<td style="text-align: right;">770 M</td>
<td style="text-align: left;">GPT2-XL</td>
<td style="text-align: right;">1.5 B</td>
</tr>
<tr>
<td style="text-align: left;">FT5-XL</td>
<td style="text-align: right;">3 B</td>
<td style="text-align: left;">GPT-J-6B</td>
<td style="text-align: right;">6 B</td>
</tr>
<tr>
<td style="text-align: left;">FT5-XXL</td>
<td style="text-align: right;">11 B</td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
<p>Table 2: A summary of pre-trained language models studied in this work. Param. denotes Parameter.</p>
<h3>4.4 Scoring Dimension</h3>
<p>Specifically, (1) For aspects INT, ENG, SPC, REL, COR, SEM, UND, and FLU of FED-Turn datasets from the open domain dialogue generation task, we choose the $s r c-&gt;h y p o$ variant since the human judgments of the evaluated dataset (i.e., FED-Turn) are also created based on the source. (2) For aspects COH, CON, and INF from SummEval and Newsroom, since data annotators labeled the data based on source and hypothesis texts, we choose $s r c-&gt;h y p o$ for these aspects. (3) For aspects INF, NAT, and FLU from the data-to-text task, we choose ref-&gt;hypo. Because the source text of the data-to-text task is not in the standard text format, which will be hard to handle by the scoring function.</p>
<h3>4.5 Evaluation Dataset Construction</h3>
<p>Unlike previous works (Matiana et al., 2021; Xu et al., 2022a,b; Castricato et al., 2022) that only consider the overall text quality, we focus on evaluating multi-dimensional text quality. In this work, we studied 37 datasets according to 22 evaluation aspects. Since each sample needs to evaluate the generated text of dozens of systems, to reduce the API cost of GPT3, we randomly sample 40 samples for each text summarization dataset and 100 samples for each dialogue response generation and data-to-text dataset. For example, in the Newsroom dataset with 60 samples, 40 samples (accounting for $60 \%$ of the samples) are randomly selected to construct the evaluation set.</p>
<h2>5 Experiment Results</h2>
<p>In this work, we focus on exploring whether language models with different structures and sizes can work in the following three scenarios. (a) vanilla (VAL): with non-instruction and non-demonstration; (b) instruction (IST): with instruction and non-demonstration; (c) instruction+demonstration (IDM): with instruction and demonstration. We studied four text generation tasks introduced in Sec. 4.2. Due to the limited space, we moved the results and analysis of the machine translation task into the Appendix A.</p>
<p>Significance Tests To examine the reliability and validity of the experiment results, we conducted the significance test based on bootstrapping. ${ }^{5}$ Our significance test is to check (1) whether the performance of IST (IDM) is significantly better than VAL, and values achieved with the IST (IDM) settings will be marked $\dagger$ if it passes the significant test (p-value $&lt;0.05$ ). (2) whether the performance of IDM is significantly better than IST, if yes, mark the value with IDM setting with $\ddagger$.</p>
<p>Average Performance Due to space limitations, we keep the average performance of GPT3-, GPT2, OPT-, and FT5-based models. The full results of various variants can be found in Appendix G.</p>
<h3>5.1 Text Summarization</h3>
<p>The evaluation results of 28 ( 9 baseline models (e.g., ROUGE-1) and 19 variants of GPTScore (e.g., GPT3-d01) scoring functions for the text summarization task on SummEval and RealSumm datasets are shown in Tab. 3. Due to the space limitation,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">CON</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FLU</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">REL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">COH</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-1</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-2</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BERTSc</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">MoverSc</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PRISM</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BARTSc</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">+CNN</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">+CNN+Pa</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-a01</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">$40.5^{\dagger}$</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">$39.8^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-b01</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">$41.4^{\dagger}$</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">$39.1^{\dagger}$</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">$33.4^{\dagger}$</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">$45.2^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-c01</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">$45.1^{\dagger}$</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">$39.5^{\dagger}$</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">$33.2^{\dagger}$</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">40.8</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-d01</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">$47.5^{\dagger}$</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">$41.0^{\dagger}$</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">$34.3^{\dagger}$</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">40.1</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-d03</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">$38.1^{\dagger}$</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">43.4</td>
</tr>
<tr>
<td style="text-align: center;">GPT2-M</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">$35.3^{\dagger}$</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">$30.7^{\dagger}$</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">$39.2^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2-L</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">$34.4^{\dagger}$</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">$31.5^{\dagger}$</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">$28.1^{\dagger}$</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">$39.8^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2-XL</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">$36.1^{\dagger}$</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">$33.1^{\dagger}$</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">$39.9^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J-6B</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">$42.8^{\dagger}$</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">$37.4^{\dagger}$</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">$31.9^{\dagger}$</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">$39.5^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT350m</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">$35.5^{\dagger}$</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">$31.4^{\dagger}$</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">$37.6^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-1.3B</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">$42.0^{\dagger}$</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">$35.9^{\dagger}$</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">$34.2^{\dagger}$</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">$37.8^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-6.7B</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">$45.7^{\dagger}$</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">$37.6^{\dagger}$</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">$36.8^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-13B</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">$45.2^{\dagger}$</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">$37.3^{\dagger}$</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">$34.7^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-66B</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">$45.3^{\dagger}$</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">$38.0^{\dagger}$</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">$33.7^{\dagger}$</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">$35.9^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5-small</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">$38.0^{\dagger}$</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">$28.0^{\dagger}$</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">$35.4^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5-base</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">$37.2^{\dagger}$</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">$31.2^{\dagger}$</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">$39.9^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5-L</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">$42.5^{\dagger}$</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">$41.6^{\dagger}$</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">$35.3^{\dagger}$</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">$45.1^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5-XL</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">$43.6^{\dagger}$</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">$42.1^{\dagger}$</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">$34.4^{\dagger}$</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">$47.0^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5-XXL</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">$42.4^{\dagger}$</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">$34.3^{\dagger}$</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">$45.6^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">40.2</td>
</tr>
</tbody>
</table>
<p>Table 3: Spearman correlation of different aspects on SummEval dataset. VAL and IST are the abbreviations of vanilla and instruction, respectively. Values with $\dagger$ denote the evaluator with instruction significantly outperforms vanilla. Values in bold are the best performance in a set of variants (e.g., GPT3 family).
we move the results of the REALSumm, NEWSROOM, and QXSUM datasets to the Appendix G. Fig. 3 shows the evaluation results of five GPT3 variant models on four text summarization datasets, where QXSUM uses the Pearson correlation and other datasets use the Spearman correlation metric. The main observations are summarized as follows:
(1) Evaluator with instruction significantly improves the performance. For the 4 aspects of SummEval datasets, 19 instruction-enhanced variants of GPTScore significantly outperform models without instruction (values with $\dagger$ in Tab. 3) and almost 7 unsupervised baseline methods. (2) Most GPT3- and FT5-based models equipped with instructions outperform supervised methods. For example, equipped with instructions, FT5L, FT5-XL, and FT5-XXL significantly outperform the supervised model BARTSc+CNN+Pa for all
four aspects of the SummEval dataset. (3) As for the GPT3-based models, (a) the performance of GPT3-d01 is barely significantly better than GPT3-c01, which tries to balance power and speed. (b) GPT3-d03 performs better than GPT3-d01 significantly. Both conclusions have passed the significance test at $p&lt;0.05$ and can be seen in Fig. 3.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">SummEval</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RSumm</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CON</td>
<td style="text-align: center;">FLU</td>
<td style="text-align: center;">REI</td>
<td style="text-align: center;">COH</td>
<td style="text-align: center;">COV</td>
</tr>
<tr>
<td style="text-align: center;"><img alt="img-2.jpeg" src="img-2.jpeg" /></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 3: Experimental results for GPT3-based variants in text summarization task. Here, blue, orange, green, pink, and cyan dot denote that GPTSCORE is built based on GPT3-a01 ( ), GPT3-b01 ( ), GPT3-c01 ( ), GPT3-d01 ( ), and GPT3-d03 ( ), respectively. The red lines ( - ) denote the average performance of GPT3-based variants.</p>
<h3>5.2 Data to Text</h3>
<p>We consider the BAGEL and SFRES datasets for the evaluation of data to text task. The average Spearman correlations of the GPT3-based, GPT2based, OPT-based, and FT5-based models are listed in Tab. 4. VAL, IST, and IDM denote the vanilla, using instruction, and using both instruction and demonstration settings, respectively. Due to the space limitations, detailed results for each evaluator are moved to Appendix G (Tab. 15 and Tab. 16). The main observations are listed as follows:
(1) Instruction (IST) improves performance, and combining it with demonstrations (IDM) further enhances it. In Tab. 4, the average performance on the three aspects is significantly improved when adapting to the instruction, and the performance of using demonstration on NAT and FLU has further significantly improved. (2) Many GPTScore variants enhanced by instruction and demonstration (IDM) outperform the fine-tuned</p>
<p>model, namely BARTSCORE+CNN+Para. For example, in Tab. 16 regarding NAT and FLU of the SFRES dataset, most of the 19 variants of GPTScore with instructions and demonstrations outperform fine-tuned BARTSCORE+CNN+Para. (3) The choice of samples for demonstration impacts the evaluation performance a lot. On the BAGEL and SFRES datasets, when equipped with instruction and demonstration (IDM), the average performance of the four backbones performs much worse than backbone equipped with instruction (IST) only. (4) Equipped with instruction and demonstration, the performance of a GPT3 family model with a small model size can surpass that of large models. In Fig. 4, the performance of GPT3-c01 with IDM always outperforms GPT3d03, which holds for both datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">INF</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NAT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FLU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VALIST</td>
<td style="text-align: center;">IDM</td>
<td style="text-align: center;">VALIST</td>
<td style="text-align: center;">IDM</td>
<td style="text-align: center;">VALIST IDM</td>
</tr>
<tr>
<td style="text-align: center;">BAGEL</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT3</td>
<td style="text-align: center;">$35.438 .3^{\dagger} 43.6^{1, \pm}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$21.726 .5^{\dagger} 36.9^{1, \pm} 3$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$30.532 .9^{\dagger} 43.4^{1, \pm}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2</td>
<td style="text-align: center;">$40.843 .2^{\dagger} 40.2$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$31.433 .0^{\dagger} 33.5^{1, \pm} 3$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$36.739 .3^{\dagger} 41.3^{1, \pm}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">$38.739 .3^{\dagger} 38.6$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$31.430 .033 .7^{1, \pm} 3$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$37.737 .1^{\dagger} 41.5^{1, \pm}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5</td>
<td style="text-align: center;">41.541 .539 .1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$26.529 .7^{\dagger} 28.6^{\dagger}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$38.141 .1^{\dagger} 40.3^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">$39.140 .6^{\dagger} 40.3^{\dagger}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$27.729 .8^{\dagger} 33.2^{1, \pm} 3$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$35.837 .6^{\dagger} 41.6^{1, \pm}$</td>
</tr>
<tr>
<td style="text-align: center;">SFRES</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT3</td>
<td style="text-align: center;">$30.425 .131 .5^{1, \pm}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$25.030 .4^{\dagger} 26.5^{\dagger}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">31.230 .926 .1</td>
</tr>
<tr>
<td style="text-align: center;">GPT2</td>
<td style="text-align: center;">$22.525 .1^{\dagger} 20.5$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$31.031 .9^{\dagger} 37.0^{1, \pm} 2$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$20.033 .1^{\dagger} 36.2^{1, \pm}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">$25.226 .9^{\dagger} 24.3$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$26.230 .0^{\dagger} 36.6^{1, \pm} 2$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$21.325 .6^{\dagger} 30.6^{1, \pm}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5</td>
<td style="text-align: center;">24.021 .919 .7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$34.334 .6^{\dagger} 36.8^{1, \pm} 2$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">22.017 .819 .7 ${ }^{\ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">25.524 .724 .0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$29.131 .7^{\dagger} 34.2^{1, \pm} 2$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$23.626 .8^{\dagger} 28.2^{1, \pm}$</td>
</tr>
</tbody>
</table>
<p>Table 4: The average of Spearman correlation of the models based on GPT3, GPT2, OPT, and FT5 on BAGEL and SFRES datasets in the data-to-text task.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">BAGEL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SFRES</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">INF</td>
<td style="text-align: center;">NAT</td>
<td style="text-align: center;">FLU</td>
<td style="text-align: center;">INF</td>
<td style="text-align: center;">NAT</td>
<td style="text-align: center;">FLU</td>
</tr>
<tr>
<td style="text-align: center;"><img alt="img-3.jpeg" src="img-3.jpeg" /></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 4: Experimental results for GPT3-based variants in the data-to-text task. Here, blue, orange, green, pink, and cyan dots denote that GPTSCORE is built based on GPT3-a01 ( $\odot$ ), GPT3-b01 ( $\odot$ ), GPT3-c01 ( $\odot$ ), GPT3d01 ( $\odot$ ), and GPT3-d03 ( $\odot$ ). The red lines ( - ) denote the average performance of GPT3-based variants.</p>
<h3>5.3 Dialogue Response Generation</h3>
<p>To test if GPTSCORE can generalize to more aspects, we choose dialogue response generation task as a testbed, which usually requires evaluating generated texts from a variety of dimensions (i.e., "interesting" and "fluent"). To reduce the computational cost, we focus on GPT3-based metrics only since they have achieved superior performance as we observed in the previous experiments. Tab. 5 shows the perfromance in dialogue response generation task, where both baseline FED and DE (DynaEval) are fine-tuned on a large dialogue corpus. The main observations are listed as follows:
(1) The performance of GPT3-d01 is much better than GPT3-d03, even though both of them have the same model size. The average Spearman correlation of GPT3-d01 outperforms GPT3-d03 by $\mathbf{4 0 . 8}$ on the FED Turn-level dataset, and 5.5 on the FED dialogue-level. (2) The GPT3based model demonstrate stronger generalization ability. BART-based models failed in evaluating the dialogue generation task. The GPT3a01 with 350 M parameters achieved comparable performance to FED and DE models, which are fine-tuned on dialogue corpus.</p>
<h2>6 Ablation Study</h2>
<h3>6.1 Effectiveness of Demonstration</h3>
<p>To investigate the relationship between the demonstration sample size (denote as K) and the evaluation performance, we choose the machine translation task and the GPT3-based variants with model sizes ranging from 350 M to 175 B for further study.</p>
<p>The change of Spearman correlation on the MQM-2020 dataset with different demonstration sample size are shown in Fig. 5. The main observations are summarized as follows: (1) The utilization of demonstration significantly improves the evaluation performance, which holds for these three aspects. (2) There is an upper bound on the performance gains from the introduction of the demonstration. For example, when $\mathrm{K}&gt;4$, the performance of ACC is hard to improve further. (3) When demonstration has only a few samples (such as $\mathrm{K}=1$ ), small models (e.g., GPT3-a01) are prone to performance degradation due to the one-sidedness of the given examples.</p>
<h3>6.2 Partial Order of Evaluation Aspect</h3>
<p>To explore the correlation between aspects, we conducted an empirical analysis with INT (interesting)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Aspect</th>
<th style="text-align: center;">Baseline</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPTScore</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT</td>
<td style="text-align: center;">BTC</td>
<td style="text-align: center;">BTCP</td>
<td style="text-align: center;">FED</td>
<td style="text-align: center;">DE</td>
<td style="text-align: center;">a01</td>
<td style="text-align: center;">b01 c01 d01 d03</td>
</tr>
<tr>
<td style="text-align: center;">FED dialogue-level</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">COH</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">$-14.9$</td>
<td style="text-align: center;">$-18.9$</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">15.022 .5 56.913 .4</td>
</tr>
<tr>
<td style="text-align: center;">ERR</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">$-12.2$</td>
<td style="text-align: center;">$-13.7$</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">16.821 .3 45.79.40</td>
</tr>
<tr>
<td style="text-align: center;">CON</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">$-6.7$</td>
<td style="text-align: center;">$-10.2$</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">33.79.9</td>
<td style="text-align: center;">18.432 .918 .1</td>
</tr>
<tr>
<td style="text-align: center;">DIV</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">$-2.5$</td>
<td style="text-align: center;">$-13.9$</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">14.95 .2021 .5 62.8</td>
<td style="text-align: center;">$-6.6$</td>
</tr>
<tr>
<td style="text-align: center;">DEP</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">$-6.6$</td>
<td style="text-align: center;">$-17.6$</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">9.0012 .928 .2 66.934 .1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LIK</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">$-6.3$</td>
<td style="text-align: center;">$-11.8$</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">26.222 .032 .1 63.418 .4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">UND</td>
<td style="text-align: center;">$-11.5-17.6$</td>
<td style="text-align: center;">$-18.2$</td>
<td style="text-align: center;">$-0.3$</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">40.040 .0 52.419 .6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FLE</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">$-10.2$</td>
<td style="text-align: center;">$-10.3$</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">32.744 .934 .6 51.57.20</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">INF</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">$-7.5$</td>
<td style="text-align: center;">$-10.5$</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">6.808 .018 .8 60.231 .7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">INQ</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">$-0.6$</td>
<td style="text-align: center;">$-14.8$</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">44.238 .749 .2 50.3 -10.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">$-8.5$</td>
<td style="text-align: center;">$-14.0$</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">25.321 .328 .6 54.313 .5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FED turn-level</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">INT</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">$-3.3$</td>
<td style="text-align: center;">$-10.1$</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">16.66.430 .8 50.122 .4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ENG</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">$-2.5$</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">10.26.229 .4 49.635 .5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SPE</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">$-7.9$</td>
<td style="text-align: center;">$-16.2$</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">33.716 .131 .721 .415 .1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">REL</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">8.610 .323 .8 45.238 .0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">COR</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">29.711 .227 .0 43.442 .8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SEM</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">$-9.4$</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">6.88.123 .144 .440 .5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">UND</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">6.6614 .823 .4 36.531 .1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FLU</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">$-13.4$</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">16.55.714 .016 .036 .7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">16.19.925 .438 .332 .8</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 5: Spearman correlation of different aspects on the FED turn- and dialogue-level datasets. $B T, B T C, B T C P$, and $D E$ denote BARTSCORE, BARTSCORE+CNN, BARTSCORE+CNN+Para, and the DynaEval model. Values in bold indicate the best performance.
on the dialogue response generation task of the FED-Turn dataset. Specifically, take INT as the target aspect and then combine the definitions of other aspects with the definition of INT as the final evaluation protocols. The x -axis of Fig. 6-(a) is the aspect order achieved based on the Spearman correlation between INT and that aspect's human score. Fig. 6-(b) is the Spearman correlation o INT as the modification of the INT definition, and the scoring function is GPT3-c01 with 6.7B parameters.</p>
<p>The following table illustrates the definition composition process, where Sp denotes Spearman.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">X Aspect</th>
<th style="text-align: center;">Aspect Definition</th>
<th style="text-align: center;">Sp</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1 INT</td>
<td style="text-align: center;">Is this response interesting to the 30.8 conversation?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3 INT, ENG, SPE</td>
<td style="text-align: center;">Is this an interesting response that 48.6 is specific and engaging?</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Specifically, the definition of INT is "Is this response interesting to the conversation? " at $\mathrm{x}=1$ in Fig. 6-(b). When INT combines with ENG, SPE (at $\mathrm{x}=3$ in Fig. 6-(b)), its definition can be "Is this an interesting response that is specific and engaging?". And the new aspect definition boosts the performance from 30.8 (at $\mathrm{x}=1$ in Fig. 6-(b)) to</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Results of the GPT3 family models with different numbers of examples ( K ) in the demonstration on the MQM-2020 dataset. Here, blue, orange, green, red, and cyan lines denote that GPTSCORE is built based on GPT3-a01 ( $\Delta$ ), GPT3-b01 ( $\&amp;$ ), GPT3-c01 ( $\odot$ ), GPT3d01 ( $\square$ ), and GPT3-d03 ( + ), respectively.
48.6 (at $x=3$ in Fig. 6-(b)). The best performance of $51.4(\mathrm{x}=5$ in Fig. 6-(b)) is achieved after combining five aspects (INT, ENG, SPE, COR, REL), which already exceeded $\mathbf{5 0 . 1}$ of the most potent scoring model GPT3-d01 (175B) with aspect definition built only on INT. Therefore, by combining definitions with other highly correlated aspects, the performance of the smaller model (GPT3curie, 6.7B) can outperform the bigger model (GPT3-davinci, 175B).</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: (a) Descending order of Spearman correlation between INT and other aspects' human scoring. (b) The Spearman correlation of INT changes as its aspect definition is modified in combination with other aspects.</p>
<h2>7 Conclusion</h2>
<p>In this paper, we propose to leverage the emergent abilities from generative pre-training models to address intricate and ever-changing evaluation requirements. The proposed framework, GPTSCORE, is studied on multiple pre-trained language models with different structures, including the GPT3 (175B). GPTSCORE has multiple benefits: customizability, multi-faceted evaluation, and train-free, which enable us to flexibly craft a metric that can support 22 aspects on 37 datasets without any learning process yet attain competitive performance. Furthermore, demonstrate that GPTScore achieves the goal of "evaluate as you desire". This work opens a new way to audit generative AI by utilizing generative AI.</p>
<h2>8 Limitations</h2>
<p>The limitations of this work include: (1) The pretrained language models considered in our work were released before GPT-3.5 (included), while some recently released popular LLMs (such as ChatGPT and GPT-4) are not studied in this work. (2) GPT3-text-davinci-003 performs worse than GPT3-text-davinci-001, which holds in many evaluation settings. However, we cannot explain this conclusion well until OpenAI discloses the model and training in more details. (3) Due to the cost limitation of using the OpenAI API, we only consider evaluating four traditional NLP generation tasks. The evaluation of some complex text generation tasks (e.g., story generation, a long text generation task) can be studied in the future.</p>
<p>There are some risks associated with modelbased evaluation. For example, (1) GPT3-based models work well as text evaluators, but their internal structure, training data, and training process are opaque, which makes it hard to explain the phenomenon of model performance. (2) Training and human feedback datasets influence language model behavior a lot. During the evaluation process, language models may inject bias and risky behaviors that are hard to identify, sending dangerous information to humans. The safety and risk of model-based evaluation should be carefully studied, and we will comprehensively explore the risk for our GPTScore like (Wang et al., 2023) in the future.</p>
<h2>Acknowledgements</h2>
<p>We thank Chen Zhang for helpful discussion and feedback. This research / project is supported by the National Research Foundation, Singapore under its Industry Alignment Fund - Pre-positioning (IAF-PP) Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.</p>
<h2>References</h2>
<p>Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, and Quoc V. Le. 2020. Towards a human-like opendomain chatbot. CoRR, abs/2001.09977.</p>
<p>Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Re-
evaluating evaluation in text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 9347-9359. Association for Computational Linguistics.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. CoRR, abs/2005.14165.</p>
<p>Meng Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit Cheung. 2020. Factual error correction for abstractive summarization models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 6251-6258. Association for Computational Linguistics.</p>
<p>Louis Castricato, Alexander Havrilla, Shahbuland Matiana, Michael Pieler, Anbang Ye, Ian Yang, Spencer Frazier, and Mark O. Riedl. 2022. Robust preference learning for storytelling via contrastive reinforcement learning. CoRR, abs/2210.07792.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Esin Durmus, He He, and Mona T. Diab. 2020. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 5055-5070. Association for Computational Linguistics.</p>
<p>Markus Freitag, George F. Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021. Experts, errors, and context: A large-scale study of human evaluation for machine translation. CoRR, abs/2104.14478.</p>
<p>Jinlan Fu, See-Kiong Ng, and Pengfei Liu. 2022. Polyglot prompt: Multilingual multitask promptraining. arXiv preprint arXiv:2204.14264.</p>
<p>Sarik Ghazarian, Nuan Wen, Aram Galstyan, and Nanyun Peng. 2022. DEAM: dialogue coherence evaluation using amr-based semantic manipulations. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 771-785. Association for Computational Linguistics.</p>
<p>Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 708-719. Association for Computational Linguistics.</p>
<p>Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 16931701.
J. Edward Hu, Abhinav Singh, Nils Holzenberger, Matt Post, and Benjamin Van Durme. 2019. Large-scale, diverse, paraphrastic bitexts via sampling and clustering. In Proceedings of the 23rd Conference on Computational Natural Language Learning, CoNLL 2019, Hong Kong, China, November 3-4, 2019, pages 44-54. Association for Computational Linguistics.</p>
<p>Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger. 2015. From word embeddings to document distances. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 957-966. JMLR.org.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 7871-7880. Association for Computational Linguistics.</p>
<p>Zekang Li, Jinchao Zhang, Zhengcong Fei, Yang Feng, and Jie Zhou. 2021. Conversations are not flat: Modeling the dynamic information flow across dialogue utterances. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 128-138. Association for Computational Linguistics.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586.</p>
<p>Yixin Liu, Alexander R Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, et al. 2022. Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. arXiv preprint arXiv:2212.07981.</p>
<p>François Mairesse, Milica Gasic, Filip Jurcícek, Simon Keizer, Blaise Thomson, Kai Yu, and Steve J. Young. 2010. Phrase-based statistical language generation using graphical models and active learning. In ACL 2010, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, July 1116, 2010, Uppsala, Sweden, pages 1552-1561. The Association for Computer Linguistics.</p>
<p>Shahbuland Matiana, J. R. Smith, Ryan Teehan, Louis Castricato, Stella Biderman, Leo Gao, and Spencer Frazier. 2021. Cut the CARP: fishing for zero-shot story evaluation. CoRR, abs/2110.03111.</p>
<p>Shikib Mehri and Maxine Eskénazi. 2020a. Unsupervised evaluation of interactive dialog with dialogpt. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGdial 2020, 1st virtual meeting, July 1-3, 2020, pages 225-235. Association for Computational Linguistics.</p>
<p>Shikib Mehri and Maxine Eskénazi. 2020b. USR: an unsupervised and reference free evaluation metric for dialog generation. CoRR, abs/2005.00456.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? CoRR, abs/2202.12837.</p>
<p>Mavuto M Mukaka. 2012. A guide to appropriate use of correlation coefficient in medical research. Malawi medical journal, 24(3):69-71.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Bo Pang, Erik Nijkamp, Wenjuan Han, Linqi Zhou, Yixian Liu, and Kewei Tu. 2020. Towards holistic and automatic evaluation of open-domain dialogue generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 3619-3629. Association for Computational Linguistics.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, pages 311-318. ACL.</p>
<p>Maja Popovic. 2015. chrf: character n-gram f-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, WMT@EMNLP 2015, 17-18 September 2015, Lisbon, Portugal, pages 392-395. The Association for Computer Linguistics.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Ricardo Rei, Craig Stewart, Ana C. Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. CoRR, abs/2009.09025.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. 2022. BLOOM: A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100.</p>
<p>Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick Gallinari. 2021. Questeval: Summarization asks for fact-based evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 6594-6604. Association for Computational Linguistics.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. 2020. BLEURT: learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 7881-7892. Association for Computational Linguistics.</p>
<p>Team Sequoia. 2022. Generative ai: A creative new world. https://www.sequoiacap.com/article/ generative-ai-a-creative-new-world/.</p>
<p>Brian Thompson and Matt Post. 2020. Automatic machine translation evaluation in many languages via
zero-shot paraphrasing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 90-121. Association for Computational Linguistics.</p>
<p>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020a. Asking and answering questions to evaluate the factual consistency of summaries. CoRR, abs/2004.04228.</p>
<p>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020b. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 5008-5020. Association for Computational Linguistics.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. 2022. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. URL https://arxiv. org/abs/2204.07705.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. CoRR, abs/2201.11903.</p>
<p>Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Peihao Su, David Vandyke, and Steve J. Young. 2015. Semantically conditioned lstm-based natural language generation for spoken dialogue systems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 1711-1721. The Association for Computational Linguistics.</p>
<p>Wenda Xu, Xian Qian, Mingxuan Wang, Lei Li, and William Yang Wang. 2022a. Sescore2: Retrieval augmented pretraining for text generation evaluation. CoRR, abs/2212.09305.</p>
<p>Wenda Xu, Yi-Lin Tuan, Yujie Lu, Michael Saxon, Lei Li, and William Yang Wang. 2022b. Not all errors are equal: Learning text generation metrics using stratified error synthesis. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 6559-6574. Association for Computational Linguistics.</p>
<p>Zheng Ye, Liucun Lu, Lishan Huang, Liang Lin, and Xiaodan Liang. 2021. Towards quantifiable dialogue coherence evaluation. In Proceedings of the 59th Annual Meeting of the Association for Computational</p>
<p>Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 2718-2729. Association for Computational Linguistics.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34:27263-27277.</p>
<p>Jerrold H Zar. 2005. Spearman rank correlation. Encyclopedia of biostatistics, 7.</p>
<p>Chen Zhang, Yiming Chen, Luis Fernando D'Haro, Yan Zhang, Thomas Friedrichs, Grandee Lee, and Haizhou Li. 2021. Dynaeval: Unifying turn and dialogue level evaluation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 5676-5689. Association for Computational Linguistics.</p>
<p>Chen Zhang, Luis Fernando D'Haro, Qiquan Zhang, Thomas Friedrichs, and Haizhou Li. 2022a. Finedeval: Fine-grained automatic dialogue-level evaluation. CoRR, abs/2210.13832.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022b. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 563-578. Association for Computational Linguistics.</p>
<p>Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. 2022. Towards a unified multidimensional evaluator for text generation. CoRR, abs/2210.07197.</p>
<h2>A Machine Translation</h2>
<p>Scoring Dimension For aspects ACC, FLU, and MQM from the machine translation task, we also choose ref-&gt;hypo. Because the source text of the machine translation is a different language from the translated text (hypo). In this work, we mainly consider the evaluation of the English text. In the future, we can consider designing a scoring function based on BLOOM (Scao et al., 2022) that can evaluate texts in a cross-lingual setting.</p>
<p>The average sample-level Spearman $(\rho)$ scores of GPT3-based, GPT2-based, OPT-based, and FT5based models on the MQM-2020 machine translation dataset are shown in Tab. 6, where values with $\dagger$ denote that the evaluator equipped with IST (or IDM) significantly outperforms the VAL setting, and $\ddagger$ indicate that the evaluator equipped with IDM (the combination of IST and DM) significantly outperforms the IST setting. The Spearman correlations for the GPT3-based variants are shown in Fig. 7. For the full evaluation results of 28 models (including 9 baseline scoring models, such as ROUGE-1) can be found in Tab. 14. Following Thompson and Post (2020) and Yuan et al. (2021), we treat the evaluation of machine translation as the paraphrasing task. The main observations are listed as follows:
(1) The introduction of instruction (IST) significantly improve the performance in three different aspects of ACC, FLU, and MQM. In Tab. 6, the average performance of 19 GPTSCORE based evaluators with instruction (IST) significantly outperforms vanilla (VAL). (2) The combination of instruction and demonstration (IDM) brings gains for the evaluator with different model structures. In Tab. 6, the performance of GPT3, GPT2, OPT, and FT5 improves a lot when instruction and demonstration (IDM) are introduced. (3) The evaluator built based on GPT3-c01 achieves comparable performance with GPT3-d01 and GPT3d03. This can be found in Fig. 7. Since the GPT3d01 and GPT3-d03 are most expensive variant of GPT3, the cheaper and comparative GPT3-c01 is a good choice for machine translation task.</p>
<h2>B Evaluation Strategy</h2>
<p>Evaluation strategies define different aggregation methods when we calculate the correlation scores. Specifically, suppose that for each source text $s_{i}, i \in[1,2, \cdots, n]$ (e.g., documents in text summarization task or dialogue histories for dialogue</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">ACC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FLU</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MQM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VALIST</td>
<td style="text-align: center;">IDM</td>
<td style="text-align: center;">VALIST</td>
<td style="text-align: center;">IDM</td>
<td style="text-align: center;">VALIST IDM</td>
</tr>
<tr>
<td style="text-align: center;">GPT3</td>
<td style="text-align: center;">27.227 .1</td>
<td style="text-align: center;">$29.7^{1, \pm}$</td>
<td style="text-align: center;">11.310 .4</td>
<td style="text-align: center;">16.4 $4^{1, \pm}$</td>
<td style="text-align: center;">$30.331 .2^{\dagger} 32.3^{1, \pm}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2</td>
<td style="text-align: center;">25.827 .0 ${ }^{\dagger}$</td>
<td style="text-align: center;">30.3 $3^{1, \pm} 9.8$</td>
<td style="text-align: center;">$10.8^{\dagger} 15.8^{1, \pm}$</td>
<td style="text-align: center;">30.130 .3 ${ }^{\dagger} 33.5^{1, \pm}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">28.729 .4 ${ }^{\dagger}$</td>
<td style="text-align: center;">$30.3^{1, \pm} 10.0$</td>
<td style="text-align: center;">12.2 $2^{\dagger} 16.3^{1, \pm}$</td>
<td style="text-align: center;">32.534 .6 ${ }^{\dagger}$</td>
<td style="text-align: center;">35.1 $1^{1, \pm}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5</td>
<td style="text-align: center;">27.727 .8 ${ }^{\dagger}$</td>
<td style="text-align: center;">$28.3^{1, \pm} 9.6$</td>
<td style="text-align: center;">$11.0^{\dagger} 15.4^{1, \pm}$</td>
<td style="text-align: center;">31.032 .3 ${ }^{\dagger} 32.3$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Avg. $27.427 .8^{\dagger} 29.7^{1, \pm} 10.211 .1^{\dagger} 16.0^{1, \pm} 31.032 .1^{\dagger} 33.3^{1, \pm}$
Table 6: The average Spearman correlation of the GPT3based, GPT2-based, OPT-based, and FT5-based models in machine translation task of MQM-2020 dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">MQM-2020</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ACC</td>
<td style="text-align: center;">FLU</td>
<td style="text-align: center;">MQM</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; 30 \ &amp; 25 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 29 \ &amp; 10 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 40 \ &amp; 35 \ &amp; 30 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 35 \ &amp; 30 \ &amp; 25 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \frac{\text { of }}{2} \ &amp; \frac{\text { of }}{10} \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 10 \ &amp; \frac{\text { of }}{10} \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 25 \ &amp; 20 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 25 \ &amp; 20 \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>Figure 7: Experimental results for GPT3-based variants in the machine translation task. Here, blue, orange, green, pink, and cyan dot denote that GPTSCORE is built based on GPT3-a01 ( ), GPT3-b01 ( ), GPT3c01 ( ), GPT3-d01 ( ), and GPT3-d03 ( ), respectively. The red lines ( - ) denote the average performance of GPT3-based variants.
generation task), there are $J$ system outputs $\boldsymbol{h}<em _auto="{auto" _text="\text">{i, j}$, where $j \in[1,2, \cdots, J] . f</em>$ is the gold human scoring function. For a given evaluation aspect $a$, the meta-evaluation metric $F$ can be formulated as follows.}}$ is an automatic scoring function (e.g., ROUGE (Lin, 2004)), and $f_{\text {human }</p>
<p>Sample-level defines that a correlation value is calculated for each sample separately based on outputs of multiple systems, then averaged across all samples.</p>
<p>$$
\begin{aligned}
F_{f_{\text {auto }}, f_{\text {human }}}^{\text {sample }}=\frac{1}{n} &amp; \sum_{i=1}^{n}\left(g\left(\left[f_{\text {auto }}\left(\boldsymbol{h}<em _auto="{auto" _text="\text">{i, 1}\right), \cdots, f</em>}}\left(\boldsymbol{h<em _human="{human" _text="\text">{i, J}\right)\right]\right. \
&amp; {\left.\left[f</em>}}\left(\boldsymbol{h<em _human="{human" _text="\text">{i, 1}\right), \cdots, f</em>\right)\right]\right)\right)
\end{aligned}
$$}}\left(\boldsymbol{h}_{i, J</p>
<p>where $g$ can be instantiated as Spearman or Pearson correlation.</p>
<p>Dataset-level indicates that the correlation value is calculated on system outputs of all $n$ samples.</p>
<p>$$
\begin{aligned}
F_{f_{\text {auto }}, f_{\text {human }}}^{\text {data }}= &amp; g\left(\left[f_{\text {auto }}\left(\boldsymbol{h}<em _auto="{auto" _text="\text">{1,1}\right), \cdots, f</em>}}\left(\boldsymbol{h<em _human="{human" _text="\text">{n, J}\right)\right]\right. \
&amp; {\left.\left[f</em>}}\left(\boldsymbol{h<em _human="{human" _text="\text">{1,1}\right), \cdots, f</em>\right)\right]\right)
\end{aligned}
$$}}\left(\boldsymbol{h}_{n, J</p>
<p>In this work, we select the evaluation strategy for a specific task based on previous works (Yuan et al., 2021; Zhang et al., 2022a). We use the samplelevel evaluation strategy for text summarization, data-to-text, and machine translation tasks. For the dialogue response generation task, the dataset-level evaluation strategy is utilized.</p>
<h2>C Metric Comparison</h2>
<p>Tab. 7 summarize several popular generated text evaluation methods.</p>
<h2>D Tasks, Datasets, and Aspects</h2>
<p>To achieve a more comprehensive evaluation, in this paper, we cover a broad range of natural language generation tasks: Dialogue Response Generation, Text Summarization, Data-to-Text, and Machine Translation, which involves 9 datasets and 22 evaluation aspects in total. Tab. 8 summarizes the tasks, datasets, and evaluation aspects considered by each dataset. The definition of different aspects can be found in Tab. 1.</p>
<p>Dialogue Response Generation aims to automatically generate an engaging and informative response based on the dialogue history. (1) FED (Mehri and Eskénazi, 2020a) collects 124 conversations, including both human-machine (Meena (Adiwardana et al., 2020), Mitsuku ${ }^{6}$ ) and human-human dialogues, and manually annotated 9 and 11 evaluation aspects at the turn- and dialoguelevel, respectively.</p>
<p>Text Summarization is a task of automatically generating an informative and fluent summary for a given long text. Here, we consider the following four datasets covering 6 evaluation aspects: semantic coverage, informativeness, relevance, fluency, coherence, and factuality. (1) SummEval (Bhandari et al., 2020) collects human judgments on 16 model-generated summaries on the CNN/Daily Mail dataset, covering aspects of coherence, consistency, fluency, and relevance. (2) REALSumm (Bhandari et al., 2020) evaluates the reliability of automatic metrics by measuring the pyramid recall of text generated by 25 systems. (3) NEWSROOM (Grusky et al., 2018) covers news, sports, entertainment, finance, and other topics and evaluates the quality of summaries generated by 7</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metrics</th>
<th style="text-align: center;">Custom</th>
<th style="text-align: center;">Function $(f)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Additional text $(S)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Training-free Application</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Representation</td>
<td style="text-align: center;">Formulation</td>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ROUGE (Lin, 2004)</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Token</td>
<td style="text-align: center;">Matching</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Required</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">SUM</td>
</tr>
<tr>
<td style="text-align: center;">BLEU (Papineni et al., 2002)</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Token</td>
<td style="text-align: center;">Matching</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Required</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">MT</td>
</tr>
<tr>
<td style="text-align: center;">CHRF (Popovic, 2015)</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Character</td>
<td style="text-align: center;">Matching</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Required</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">MT</td>
</tr>
<tr>
<td style="text-align: center;">BERTScore (Zhang et al., 2020)</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Matching</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Required</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">MUL(2)</td>
</tr>
<tr>
<td style="text-align: center;">MoverScore (Zhao et al., 2019)</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Matching</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Required</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">MUL(4)</td>
</tr>
<tr>
<td style="text-align: center;">BLEURT (Sellam et al., 2020)</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Regression</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Required</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">MT</td>
</tr>
<tr>
<td style="text-align: center;">PRISM (Thompson and Post, 2020)</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Embedding</td>
<td style="text-align: center;">Paraphrase</td>
<td style="text-align: center;">Optional</td>
<td style="text-align: center;">Optional</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">MT</td>
</tr>
<tr>
<td style="text-align: center;">UNIEVAL (Zhong et al., 2022)</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Boolean QA</td>
<td style="text-align: center;">Optional</td>
<td style="text-align: center;">Optional</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">MUL(2)</td>
</tr>
<tr>
<td style="text-align: center;">COMET (Rei et al., 2020)</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Regress, Rank</td>
<td style="text-align: center;">Optional</td>
<td style="text-align: center;">Optional</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">MT</td>
</tr>
<tr>
<td style="text-align: center;">BARTScore (Yuan et al., 2021)</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">BART</td>
<td style="text-align: center;">Generation</td>
<td style="text-align: center;">Optional</td>
<td style="text-align: center;">Optional</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">MUL(3)</td>
</tr>
<tr>
<td style="text-align: center;">FED (Mehri and Eskénazi, 2020a)</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">DialoGPT</td>
<td style="text-align: center;">Generation</td>
<td style="text-align: center;">Required</td>
<td style="text-align: center;">Optional</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Dialogue</td>
</tr>
<tr>
<td style="text-align: center;">HolisticEval (Pang et al., 2020)</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">GPT2</td>
<td style="text-align: center;">Generation</td>
<td style="text-align: center;">Optional</td>
<td style="text-align: center;">Optional</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Dialogue</td>
</tr>
<tr>
<td style="text-align: center;">GPTScore</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">GPT3/OPT</td>
<td style="text-align: center;">Any</td>
<td style="text-align: center;">Optional</td>
<td style="text-align: center;">Optional</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">MUL(5)</td>
</tr>
</tbody>
</table>
<p>Table 7: A comprehensive comparison of existing research on automated evaluation of generated texts. MUL(k) denotes multiple (k) applications explored. Custom denotes Custom Aspects.</p>
<table>
<thead>
<tr>
<th>Tasks</th>
<th>Dataset</th>
<th>Aspect</th>
</tr>
</thead>
<tbody>
<tr>
<td>Diag</td>
<td>FED-Diag</td>
<td>COH, DIV, FLE, UND,INQ <br> CON, INF, LIK, DEP, ERR</td>
</tr>
<tr>
<td></td>
<td>FED-Turn</td>
<td>INT, ENG, SPE, REL, <br> COR, SEM, UND, FLU</td>
</tr>
<tr>
<td>Summ</td>
<td>SummEval <br> Newsroom <br> REALSumm <br> Q-XSUM</td>
<td>COH, CON, FLU,REL <br> FLU, REL, INF, COH <br> COV <br> FAC</td>
</tr>
<tr>
<td>D2T</td>
<td>BAGEL <br> SFRES</td>
<td>FLU, REL, INF <br> FLU, REL, INF</td>
</tr>
<tr>
<td>MT</td>
<td>MQM-2020</td>
<td>FLU, COH, INF</td>
</tr>
</tbody>
</table>
<p>Table 8: An overview of tasks, datasets, and evaluation aspects. Summ. denote the text summarization task, $D 2 T$ denotes the Data-to-Text task, $M T$ denotes the machine translation. Tab. 1 summarized the definitions of the aspects explored in this work.
systems, including informativeness, relevance, fluency, and coherence. (4) QAGS_XSUM (Wang et al., 2020b) is another dataset focusing on the factuality aspect. It has 239 samples from XSUM and their summaries are generated by a fine-tuned BART model.</p>
<p>Data-to-Text aims to automatically generate a fluent and factual description for a given table. (1) BAGEL (Mairesse et al., 2010) contains 202 samples about restaurants in Cambridge. (2) SFRES (Wen et al., 2015) contains 581 samples about restaurants in San Francisco. These two datasets consider three evaluation aspects: informativeness, naturalness (relevance), and quality (fluency).</p>
<p>Machine Translation aims to translate a sentence from one language to another. We consider a sub-datasets of Multidimensional Quality Metrics (MQM) (Freitag et al., 2021), namely, MQM-2020 (Chinese-&gt;English). Due to limited annotations, here, we only consider three evaluation aspects: accuracy, fluency, and $M Q M$ with diverse scores.</p>
<h2>E Ablation Study</h2>
<h2>E. 1 Effectiveness of Demonstration</h2>
<p>The in-context learning helps a lot to achieve a good performance. However, how does the number of samples in the demonstration impact the performance? We conduct a case study on the five GPT3-based models explored in this work. The experimental results are shown in Fig. 5, and the specific performance values can be seen in Tab. 9.</p>
<h2>E. 2 Partial Order of Evaluation Aspect</h2>
<p>We have investigated the combination of different evaluation aspects to achieve further performance gains in § 6.2. Tab. 10 summarizes the aspect definition and Spearman correlation changes for INT, with the introduction of other aspects.</p>
<h2>F Prompt Design</h2>
<p>In this work, we have studied four popular text generation tasks: text summarization, machine translation, data-to-text, and dialogue response generation. The instructions for these tasks on different evaluation aspects are summarized in Tab. 11 and Tab. 12. Here, we convert the dialogue response generation task as a boolean question-answering task and in-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">K</th>
<th style="text-align: center;">ACC</th>
<th style="text-align: center;">FLU</th>
<th style="text-align: center;">MQM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT3-ada</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">24.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">26.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">25.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">24.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">24.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">24.7</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-babbage</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">29.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">30.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">30.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">31.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">31.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">32.6</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-curie</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">34.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">31.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">32.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">33.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">34.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">34.3</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-davinci001</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">32.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">33.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">35.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">37.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">38.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">39.1</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-davinci003</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">32.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">31.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">32.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">33.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">32.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">32.5</td>
</tr>
</tbody>
</table>
<p>Table 9: Spearman correlation of the GPT3-based models (e.g, text-ada-001 and text-davinci-001) with different demonstration sample numbers on the MQM-2020 dataset .K denotes the number of samples in the demonstration.
corporate the aspect definition into the question of the boolean question-answering task.</p>
<h2>G Experiment Results</h2>
<p>This section lists the full experimental results for the explored text generation tasks. The models considered here include the 9 baseline models: ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, MoverScore, PRISM, BARTSCORE, BARTSCORE+CNN, and BARTSCORE+CNN+Para, and 19 GPTScore models built based on the GPT3-based, GPT2based, OPT-based, and Flan-T5-based pre-trained models.</p>
<p>Tab. 13 lists the results of the text summarization datasets. Tab. 14 lists the results of the machine translation datasets. Tab. 15 shows the results of the data-to-text task on the BAGEL dataset. Tab. 16 shows the results of the data-to-text task on the</p>
<p>SFRES dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">X</th>
<th style="text-align: center;">Aspect</th>
<th style="text-align: center;">Aspect Definition</th>
<th style="text-align: center;">Spear</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Interesting (INT)</td>
<td style="text-align: center;">Is this response interesting to the convsersation?</td>
<td style="text-align: center;">36.9</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Engaging (ENG)</td>
<td style="text-align: center;">Is this an interesting response that is engaging?</td>
<td style="text-align: center;">40.7</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Specific (SPE)</td>
<td style="text-align: center;">Is this an interesting response that is specific and engaging?</td>
<td style="text-align: center;">48.6</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Correct (COR)</td>
<td style="text-align: center;">Is this an interesting response that is engaging, specific, and correct?</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Relevant (REL)</td>
<td style="text-align: center;">Is this an interesting response that is specific, engaging, relevant, and correct?</td>
<td style="text-align: center;">51.3</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Understandable (UND)</td>
<td style="text-align: center;">Is this an interesting response that is specific, engaging, relevant, correct, and understandable?</td>
<td style="text-align: center;">50.9</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Semantically appropriate (SEM)</td>
<td style="text-align: center;">Is this an interesting response that is specific, engaging, relevant, correct, understandable, and semantically appropriate?</td>
<td style="text-align: center;">51.4</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Fluent (FLU)</td>
<td style="text-align: center;">Is this an interesting response that is specific, engaging, relevant, correct, understandable, semantically appropriate, and fluent?</td>
<td style="text-align: center;">50.3</td>
</tr>
</tbody>
</table>
<p>Table 10: The aspect definition and Spearman correlation of INT. $X$ denotes the number of aspects combined with the INT. The scoring model is GPT3-c01.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Aspect</th>
<th style="text-align: center;">Function</th>
<th style="text-align: center;">Instruction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Text Summarization</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FAC</td>
<td style="text-align: center;">src-&gt;hypo</td>
<td style="text-align: center;">Generate a summary with consistent facts for the following text: ${\mathrm{src}} \mid \mathrm{n} \mid \mathrm{n} \mathrm{Tl} ; \mathrm{dr}{$ hypo $}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ref&lt;&gt;hypo</td>
<td style="text-align: center;">Rewrite the following text with consistent facts. ${$ ref/hypo $}$ In other words, ${$ hypo/ref $}$</td>
</tr>
<tr>
<td style="text-align: center;">COV</td>
<td style="text-align: center;">src-&gt;hypo</td>
<td style="text-align: center;">Generate a summary with as much semantic coverage as possible for the following text: ${\mathrm{src}} \mid \mathrm{n} \mid \mathrm{n} T \mathrm{~T} ; \mathrm{dr}{$ hypo $}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ref&lt;&gt;hypo</td>
<td style="text-align: center;">Rewrite the following text with the same semantics. ${$ ref/hypo $}$ In other words, ${$ hypo/ref $}$</td>
</tr>
<tr>
<td style="text-align: center;">CON</td>
<td style="text-align: center;">src-&gt;hypo</td>
<td style="text-align: center;">Generate factually consistent summary for the following text: ${\mathrm{src}} \mid \mathrm{n} \mid \mathrm{n} \mathrm{Tl} ; \mathrm{dr}{$ hypo $}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ref&lt;&gt;hypo</td>
<td style="text-align: center;">Rewrite the following text with consistent facts. ${$ ref/hypo $}$ In other words, ${$ hypo/ref $}$</td>
</tr>
<tr>
<td style="text-align: center;">INF</td>
<td style="text-align: center;">src-&gt;hypo</td>
<td style="text-align: center;">Generate an informative summary that captures the key points of the following text: ${\mathrm{src}} \mid \mathrm{n} \mid \mathrm{n} \mathrm{Tl} ; \mathrm{dr}{$ hypo $}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ref&lt;&gt;hypo</td>
<td style="text-align: center;">Rewrite the following text with its core information. ${$ ref/hypo $}$ In other words, ${$ hypo/ref $}$</td>
</tr>
<tr>
<td style="text-align: center;">COH</td>
<td style="text-align: center;">src-&gt;hypo</td>
<td style="text-align: center;">Generate a coherent summary for the following text: ${\mathrm{src}} \mid \mathrm{n} \mid \mathrm{n} \mathrm{Tl} ; \mathrm{dr}{$ hypo $}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ref&lt;&gt;hypo</td>
<td style="text-align: center;">Rewrite the following text into a coherent text. ${$ ref/hypo $}$ In other words, ${$ hypo/ref $}$</td>
</tr>
<tr>
<td style="text-align: center;">REL</td>
<td style="text-align: center;">src-&gt;hypo</td>
<td style="text-align: center;">Generate a relevant summary with consistent details for the following text: ${\mathrm{src}} \mid \mathrm{n} \mid \mathrm{n} \mathrm{Tl} ; \mathrm{dr}{$ hypo $}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ref&lt;&gt;hypo</td>
<td style="text-align: center;">Rewrite the following text with consistent details. ${$ ref/hypo $}$ In other words, ${$ hypo/ref $}$</td>
</tr>
<tr>
<td style="text-align: center;">FLU</td>
<td style="text-align: center;">src-&gt;hypo</td>
<td style="text-align: center;">Generate a fluent and grammatical summary for the following text: ${\mathrm{src}} \mid \mathrm{n} \mid \mathrm{n} \mathrm{Tl} ; \mathrm{dr}{$ hypo $}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ref&lt;&gt;hypo</td>
<td style="text-align: center;">Rewrite the following text into a fluent and grammatical text. ${$ ref/hypo $}$ In other words, ${$ hypo/ref $}$</td>
</tr>
<tr>
<td style="text-align: center;">Machine Translation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">ref&lt;&gt;hypo</td>
<td style="text-align: center;">Rewrite the following text with its core information and consistent facts:{ref/hypo} In other words, ${$ hypo/ref $}$</td>
</tr>
<tr>
<td style="text-align: center;">FLU</td>
<td style="text-align: center;">ref&lt;&gt;hypo</td>
<td style="text-align: center;">Rewrite the following text to make it more grammatical and well-written:{ref/hypo} In other words, ${$ hypo/ref $}$</td>
</tr>
<tr>
<td style="text-align: center;">MQM</td>
<td style="text-align: center;">ref&lt;&gt;hypo</td>
<td style="text-align: center;">Rewrite the following text into high-quality text with its core information:{ref/hypo} In other words, ${$ hypo/ref $}$</td>
</tr>
</tbody>
</table>
<h1>Data to Text</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">INF</th>
<th style="text-align: left;">ref&lt;&gt;hypo</th>
<th style="text-align: left;">Convert the following text to another expression that preserves key information:\n\n{ref/hypo} In <br> other words, ${$ hypo/ref $}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NAT</td>
<td style="text-align: left;">ref&lt;&gt;hypo</td>
<td style="text-align: left;">Convert the following text into another expression that is human-like and natural:\n\n{ref/hypo} In <br> other words, ${$ hypo/ref $}$</td>
</tr>
<tr>
<td style="text-align: left;">FLU</td>
<td style="text-align: left;">ref&lt;&gt;hypo</td>
<td style="text-align: left;">Convert the following text into another expression that preserves key information and is human-like <br> and natural:\n\n{ref/hypo} In other words, ${$ hypo/ref $}$</td>
</tr>
</tbody>
</table>
<p>Table 11: Instruction design on different aspects for text summarization, machine translation, and data-to-text tasks. $s r c$, hypo, and ref denote the source text, hypothesis text, and reference text, respectively. $a-&gt;b(a&lt;-b)$ denotes to evaluate the quality of $b(a)$ text based on the given $a(b)$ text.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Aspect</th>
<th style="text-align: center;">Instruction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">FED Turn-Level</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">INT</td>
<td style="text-align: center;">Answer the question based on the conversation between a human and AI. $\ln$ Question: Are the responses of AI interesting? (a) Yes. (b) No. $\ln$ Conversation: {History} $\ln$ Answer: Yes.</td>
</tr>
<tr>
<td style="text-align: center;">ENG</td>
<td style="text-align: center;">Answer the question based on the conversation between a human and AI. $\ln$ Question: Are the responses of AI engaging? (a) Yes. (b) No. $\ln$ Conversation: {History} $\ln$ Answer: Yes.</td>
</tr>
<tr>
<td style="text-align: center;">UND</td>
<td style="text-align: center;">Answer the question based on the conversation between a human and AI. $\ln$ Question: Are the responses of AI understandable? (a) Yes. (b) No. $\ln$ Conversation: {History} $\ln$ Answer: Yes.</td>
</tr>
<tr>
<td style="text-align: center;">REL</td>
<td style="text-align: center;">Answer the question based on the conversation between a human and AI. $\ln$ Question: Are the responses of AI relevant to the conversation? (a) Yes. (b) No.backslashnConversation: {History} $\ln$ Answer: Yes.</td>
</tr>
<tr>
<td style="text-align: center;">SPE</td>
<td style="text-align: center;">Answer the question based on the conversation between a human and AI. $\ln$ Question: Are the responses of AI generic or specific to the conversation? (a) Yes. (b) No. $\ln$ Conversation: {History} $\ln$ Answer: Yes.</td>
</tr>
<tr>
<td style="text-align: center;">COR</td>
<td style="text-align: center;">Answer the question based on the conversation between a human and AI. $\ln$ Question: Are the responses of AI correct to conversations? (a) Yes. (b) No. $\ln$ Conversation: {History} $\ln$ Answer: Yes.}</td>
</tr>
<tr>
<td style="text-align: center;">SEM</td>
<td style="text-align: center;">Answer the question based on the conversation between a human and AI. $\ln$ Question: Are the responses of AI semantically appropriate? (a) Yes. (b) No. $\ln$ Conversation: {History} $\ln$ Answer: Yes.</td>
</tr>
<tr>
<td style="text-align: center;">FLU</td>
<td style="text-align: center;">Answer the question based on the conversation between a human and AI. $\ln$ Question: Are the responses of AI fluently written? (a) Yes. (b) No. $\ln$ Conversation: {History} $\ln$ Answer: Yes.</td>
</tr>
</tbody>
</table>
<h1>FED Dialog-Level</h1>
<p>COH Answer the question based on the conversation between a human and AI. $\ln$ Question: Is the AI coherent and maintains a good conversation flow throughout the conversation? (a) Yes. (b) No. $\ln$ Conversation: {History} $\ln$ Answer: Yes.
DIV Answer the question based on the conversation between a human and AI. $\ln$ Question: Is there diversity in the AI responses? (a) Yes. (b) No. $\ln$ Conversation: {History} $\ln$ Answer: Yes.
FLE Answer the question based on the conversation between a human and AI. $\ln$ Question: Is the AI flexible and adaptable to human and their interests? (a) Yes. (b) No. $\ln$ Conversation: {History} $\ln$ Answer: Yes.
UND Answer the question based on the conversation between a human and AI. $\ln$ Question: Does the AI seem to understand the human? (a) Yes. (b) No. $\ln$ Conversation: {History} $\ln$ Answer: Yes.
INQ Answer the question based on the conversation between a human and AI. $\ln$ Question: Is the AI inquisitive throughout the conversation? (a) Yes. (b) No. $\ln$ Conversation: {History} $\ln$ Answer: Yes.
CON Answer the question based on the conversation between a human and AI. $\ln$ Question: Are the responses of AI consistent in the information it provides throughout the conversation? (a) Yes. (b) No. $\ln$ Conversation: {History} $\ln$ Answer: Yes.
INF nswer the question based on the conversation between a human and AI. $\ln$ Question: Are the responses of AI informative throughout the conversation? (a) Yes. (b) No. $\ln$ Conversation: {History} $\ln$ Answer: Yes.
LIK Answer the question based on the conversation between a human and AI. $\ln$ Question: Does the AI display a likeable personality? (a) Yes. (b) No. $\ln$ Conversation: {History} $\ln$ Answer: Yes.
DEP Answer the question based on the conversation between a human and AI. $\ln$ Question: Does the AI discuss topics in depth? (a) Yes. (b) No. $\ln$ Conversation: {History} $\ln$ Answer: Yes.
ERR Answer the question based on the conversation between a human and AI. $\ln$ Question: Is the AI able to recover from errors that it makes? (a) Yes. (b) No. $\ln$ Conversation: {History} $\ln$ Answer: Yes.</p>
<p>Table 12: Instruction design on various aspects for dialogue response generation task at the turn- and dialogue-level. History indicates the conversation history. We convert the evaluation of the response generation task as a questionanswering task, and the aspect definition is incorporated into the question of the question-answering task.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">NEWSROOM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">QXSUM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RSumm</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COH</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CON</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">FLU</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">REL</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">COV</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">COV</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-1</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-2</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BERTScore</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$-4.6$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">MoverScore</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PRISM</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BARTSCORE</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">+CNN</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">+CNN+Para</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">40.9</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GPT3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT3-a01</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">$71.9^{\dagger}$</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">$70.0^{\dagger}$</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">$67.0^{\dagger}$</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">$29.8^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-b01</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">$68.3^{\dagger}$</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">$35.2^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-c01</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">$70.9^{\dagger}$</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">$68.6^{\dagger}$</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">$22.1^{\dagger}$</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">$45.1^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-d01</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">$73.4^{\dagger}$</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">$70.0^{\dagger}$</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">$66.9^{\dagger}$</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">$72.1^{\dagger}$</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">33.9</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-d03</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">$68.9^{\dagger}$</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">$22.0^{\dagger}$</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">$38.0^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">$70.2^{\dagger}$</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">$67.9^{\dagger}$</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">$18.0^{\dagger}$</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">$36.4^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT2-M</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">$71.7^{\dagger}$</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">$68.0^{\dagger}$</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">$62.3^{\dagger}$</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">$18.7^{\dagger}$</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">$43.3^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2-L</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">$72.3^{\dagger}$</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">$68.3^{\dagger}$</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">$61.4^{\dagger}$</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">$67.8^{\dagger}$</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">$19.6^{\dagger}$</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">$41.3^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2-XL</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">$41.0^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J-6B</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">$22.0^{\dagger}$</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">$43.7^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">$71.5^{\dagger}$</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">$68.1^{\dagger}$</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">$62.5^{\dagger}$</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">$20.4^{\dagger}$</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">$41.1^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">OPT-350M</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">$71.5^{\dagger}$</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">$69.9^{\dagger}$</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">$68.1^{\dagger}$</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">$71.6^{\dagger}$</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">$42.3^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-1.3B</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">$73.6^{\dagger}$</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">$71.3^{\dagger}$</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">$67.8^{\dagger}$</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">39.7</td>
</tr>
<tr>
<td style="text-align: center;">OPT-6.7B</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">$41.9^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-13B</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">$69.6^{\dagger}$</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">$66.0^{\dagger}$</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">$71.5^{\dagger}$</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">$41.0^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-66B</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">$41.3^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">$72.3^{\dagger}$</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">$69.9^{\dagger}$</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">$67.0^{\dagger}$</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">$71.8^{\dagger}$</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">$41.2^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FT5-S</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">$69.2^{\dagger}$</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">$60.4^{\dagger}$</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">$65.5^{\dagger}$</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">$15.1^{\dagger}$</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">$35.7^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5-B</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">$59.9^{\dagger}$</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">$16.3^{\dagger}$</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">$38.6^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5-L</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">$28.8^{\dagger}$</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">$39.3^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5-XL</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">$25.6^{\dagger}$</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">$43.8^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5-XXL</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">$60.4^{\dagger}$</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">$67.8^{\dagger}$</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">$27.8^{\dagger}$</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">$41.1^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">$22.7^{\dagger}$</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">$39.7^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Overall Avg</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">$64.5^{\dagger}$</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">$20.2^{\dagger}$</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">$39.8^{\dagger}$</td>
</tr>
</tbody>
</table>
<p>Table 13: Spearman correlations on NEWSROOM and QXSUM datasets for text summarization task. VAL and IST denote the evaluator with vanilla and instruction, respectively. Values with $\dagger$ denote the evaluator with instruction significantly outperforms with vanilla. Values in bold are the best performance in a set of variants (e.g., GPT3 family).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">ACC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FLU</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MQM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
<td style="text-align: center;">IDM</td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
<td style="text-align: center;">IDM</td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
<td style="text-align: center;">IDM</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-1</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-2</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BERTScore</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">MoverScore</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PRISM</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BARTSCORE</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">+CNN</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">+CNN+Para</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GPT3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT3-a01</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">$27.9^{\dagger, \ddagger}$</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">$6.3^{\dagger}$</td>
<td style="text-align: center;">$11.6^{\dagger, \ddagger}$</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">$24.4^{\ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-b01</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">$29.8^{\dagger, \ddagger}$</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">$14.0^{\dagger, \ddagger}$</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">$31.2^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-c01</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">$30.3^{\dagger}$</td>
<td style="text-align: center;">$30.2^{\dagger}$</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">$17.9^{\dagger, \ddagger}$</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">$34.8^{\dagger}$</td>
<td style="text-align: center;">$34.5^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-d01</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">$31.2^{\dagger, \ddagger}$</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">$17.5^{\dagger, \ddagger}$</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">$32.5^{\dagger}$</td>
<td style="text-align: center;">$38.3^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-d03</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">$30.1^{\dagger}$</td>
<td style="text-align: center;">$29.5^{\dagger}$</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">$21.3^{\dagger, \ddagger}$</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">$34.8^{\dagger}$</td>
<td style="text-align: center;">$32.8^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">$29.7^{\dagger, \ddagger}$</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">$16.4^{\dagger, \ddagger}$</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">$31.2^{\dagger}$</td>
<td style="text-align: center;">$32.3^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT2-M</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">$29.6^{\dagger, \ddagger}$</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">$9.4^{\dagger}$</td>
<td style="text-align: center;">$15.1^{\dagger, \ddagger}$</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">$34.1^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2-L</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">$28.5^{\dagger}$</td>
<td style="text-align: center;">$32.2^{\dagger, \ddagger}$</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">$14.9^{\dagger, \ddagger}$</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">$33.9^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2-XL</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">$27.6^{\dagger}$</td>
<td style="text-align: center;">$29.7^{\dagger, \ddagger}$</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">$12.0^{\dagger}$</td>
<td style="text-align: center;">$17.4^{\dagger, \ddagger}$</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">$32.2^{\dagger}$</td>
<td style="text-align: center;">$35.8^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J-6B</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">$27.2^{\dagger}$</td>
<td style="text-align: center;">$29.5^{\dagger, \ddagger}$</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">$11.2^{\dagger}$</td>
<td style="text-align: center;">$15.9^{\dagger, \ddagger}$</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">$28.8^{\dagger}$</td>
<td style="text-align: center;">$30.3^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">$27.0^{\dagger}$</td>
<td style="text-align: center;">$30.3^{\dagger, \ddagger}$</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">$10.8^{\dagger}$</td>
<td style="text-align: center;">$15.8^{\dagger, \ddagger}$</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">$30.3^{\dagger}$</td>
<td style="text-align: center;">$33.5^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">OPT-350M</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">$28.6^{\dagger}$</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">$15.7^{\dagger, \ddagger}$</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">$32.5^{\dagger}$</td>
<td style="text-align: center;">31.8</td>
</tr>
<tr>
<td style="text-align: center;">OPT-1.3B</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">$28.0^{\ddagger}$</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">$13.3^{\dagger}$</td>
<td style="text-align: center;">$15.9^{\dagger, \ddagger}$</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">$33.6^{\dagger}$</td>
<td style="text-align: center;">$32.9^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-6.7B</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">$30.7^{\dagger}$</td>
<td style="text-align: center;">$30.6^{\dagger}$</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">$12.2^{\dagger}$</td>
<td style="text-align: center;">$15.0^{\dagger, \ddagger}$</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">$36.4^{\dagger}$</td>
<td style="text-align: center;">$36.9^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-13B</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">$29.5^{\dagger}$</td>
<td style="text-align: center;">$30.8^{\dagger, \ddagger}$</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">$11.7^{\dagger}$</td>
<td style="text-align: center;">$17.9^{\dagger, \ddagger}$</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">$35.5^{\dagger}$</td>
<td style="text-align: center;">$37.5^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-66B</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">$31.0^{\dagger}$</td>
<td style="text-align: center;">$33.4^{\dagger, \ddagger}$</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">$12.1^{\dagger}$</td>
<td style="text-align: center;">$16.8^{\dagger, \ddagger}$</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">$35.3^{\dagger}$</td>
<td style="text-align: center;">$36.4^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">$29.4^{\dagger}$</td>
<td style="text-align: center;">$30.3^{\dagger, \ddagger}$</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">$12.2^{\dagger}$</td>
<td style="text-align: center;">$16.3^{\dagger, \ddagger}$</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">$34.6^{\dagger}$</td>
<td style="text-align: center;">$35.1^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FT5-S</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">$28.7^{\dagger}$</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">$15.0^{\dagger, \ddagger}$</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">31.3</td>
</tr>
<tr>
<td style="text-align: center;">FT5-B</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">$27.4^{\dagger, \ddagger}$</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">$15.9^{\dagger, \ddagger}$</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">$30.0^{\ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5-L</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">$28.8^{\dagger, \ddagger}$</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">$13.0^{\dagger}$</td>
<td style="text-align: center;">$15.6^{\dagger, \ddagger}$</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">$31.6^{\dagger}$</td>
<td style="text-align: center;">$32.1^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5-XL</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">$28.1^{\ddagger}$</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">$10.2^{\dagger}$</td>
<td style="text-align: center;">$14.0^{\dagger, \ddagger}$</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">$33.5^{\dagger}$</td>
<td style="text-align: center;">$34.2^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5-XXL</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">$29.4^{\dagger}$</td>
<td style="text-align: center;">$30.5^{\dagger, \ddagger}$</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">$12.2^{\dagger}$</td>
<td style="text-align: center;">$16.2^{\dagger, \ddagger}$</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">$33.3^{\dagger}$</td>
<td style="text-align: center;">$33.8^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">$28.3^{\dagger, \ddagger}$</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">$11.0^{\dagger}$</td>
<td style="text-align: center;">$15.4^{\dagger, \ddagger}$</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">$32.3^{\dagger}$</td>
<td style="text-align: center;">$32.3^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Overall Avg</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">$27.8^{\dagger}$</td>
<td style="text-align: center;">$29.7^{\dagger, \ddagger}$</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">$11.1^{\dagger}$</td>
<td style="text-align: center;">$16.0^{\dagger, \ddagger}$</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">$32.1^{\dagger}$</td>
<td style="text-align: center;">$33.3^{\dagger, \ddagger}$</td>
</tr>
</tbody>
</table>
<p>Table 14: Spearman correlations on MQM-2020 dataset for machine translation task. VAL, IST, and IDM denote the evaluator with vanilla, instruction, and the combination of instruction and demonstration, respectively. Values with $\dagger$ denote the evaluator with instruction significantly outperforms with vanilla, and values with $\ddagger$ denote the evaluator with the combination of instruction and demonstration significantly outperforms with only instruction. Values in bold are the best performance in a set of variants (e.g., GPT3 family).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">INF</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NAT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FLU</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
<td style="text-align: center;">IST+DM</td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
<td style="text-align: center;">IST+DM</td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
<td style="text-align: center;">IST+DM</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-1</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-2</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BERTScore</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">MoverScore</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PRISM</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BARTSCORE</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">+CNN</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">+CNN+Para</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GPT3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT3-a01</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">$37.0^{\dagger}$</td>
<td style="text-align: center;">$42.5^{\dagger, \ddagger}$</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">$28.7^{\dagger}$</td>
<td style="text-align: center;">$41.7^{\dagger, \ddagger}$</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">$35.1^{\dagger}$</td>
<td style="text-align: center;">$40.2^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-b01</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">$44.5^{\dagger}$</td>
<td style="text-align: center;">$42.2^{\dagger}$</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">$29.8^{\dagger}$</td>
<td style="text-align: center;">$39.1^{\dagger, \ddagger}$</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">$33.8^{\dagger}$</td>
<td style="text-align: center;">$40.3^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-c01</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">$40.9^{\dagger}$</td>
<td style="text-align: center;">$47.5^{\dagger, \ddagger}$</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">$26.5^{\dagger}$</td>
<td style="text-align: center;">$39.9^{\dagger, \ddagger}$</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">$34.2^{\dagger}$</td>
<td style="text-align: center;">$44.2^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-d01</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">$43.6^{\dagger, \ddagger}$</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">$26.2^{\dagger}$</td>
<td style="text-align: center;">$36.6^{\dagger, \ddagger}$</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">$47.9^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-d03</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">$42.0^{\dagger, \ddagger}$</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">$21.4^{\dagger}$</td>
<td style="text-align: center;">$27.5^{\dagger, \ddagger}$</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">$44.4^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">$38.3^{\dagger}$</td>
<td style="text-align: center;">$43.6^{\dagger, \ddagger}$</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">$26.5^{\dagger}$</td>
<td style="text-align: center;">$36.9^{\dagger, \ddagger}$</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">$32.9^{\dagger}$</td>
<td style="text-align: center;">$43.4^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT2-M</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">$42.9^{\dagger}$</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">$33.2^{\dagger}$</td>
<td style="text-align: center;">$34.3^{\dagger, \ddagger}$</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">$39.6^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2-L</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">$42.2^{\dagger}$</td>
<td style="text-align: center;">$41.8^{\dagger}$</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">$33.5^{\dagger}$</td>
<td style="text-align: center;">$33.1^{\dagger}$</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">$40.0^{\dagger}$</td>
<td style="text-align: center;">$39.6^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2-XL</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">$42.0^{\dagger}$</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">$33.7^{\dagger}$</td>
<td style="text-align: center;">$34.8^{\dagger, \ddagger}$</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">$40.6^{\dagger}$</td>
<td style="text-align: center;">$44.2^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J-6B</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">$45.6^{\dagger}$</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">$31.9^{\dagger}$</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">$37.7^{\dagger}$</td>
<td style="text-align: center;">$42.0^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">$43.2^{\dagger}$</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">$33.0^{\dagger}$</td>
<td style="text-align: center;">$33.5^{\dagger, \ddagger}$</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">$39.3^{\dagger}$</td>
<td style="text-align: center;">$41.3^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">OPT-350M</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">$37.9^{\dagger, \ddagger}$</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">$39.9^{\ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-1.3B</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">$39.3^{\dagger}$</td>
<td style="text-align: center;">$38.2^{\dagger}$</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">$30.0^{\dagger}$</td>
<td style="text-align: center;">$32.9^{\dagger, \ddagger}$</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">$40.9^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-6.7B</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">$35.2^{\dagger, \ddagger}$</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">$43.6^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-13B</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">$38.9^{\dagger, \ddagger}$</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">$34.6^{\dagger, \ddagger}$</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">$41.2^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-66B</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">$43.2^{\dagger}$</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">$34.7^{\dagger, \ddagger}$</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">$37.6^{\dagger}$</td>
<td style="text-align: center;">$42.0^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">$33.7^{\dagger, \ddagger}$</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">$41.5^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FT5-S</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">26.6</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">$36.1^{\ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5-B</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">$43.6^{\dagger}$</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">$30.3^{\dagger}$</td>
<td style="text-align: center;">$27.3^{\dagger}$</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">$40.6^{\dagger}$</td>
<td style="text-align: center;">37.9</td>
</tr>
<tr>
<td style="text-align: center;">FT5-L</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">$42.8^{\dagger}$</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">$31.0^{\dagger}$</td>
<td style="text-align: center;">$32.6^{\dagger, \ddagger}$</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">$43.3^{\dagger}$</td>
<td style="text-align: center;">$44.5^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5-XL</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">$42.8^{\dagger}$</td>
<td style="text-align: center;">$43.3^{\dagger, \ddagger}$</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">$28.9^{\dagger}$</td>
<td style="text-align: center;">$27.8^{\dagger}$</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">$44.4^{\dagger}$</td>
<td style="text-align: center;">$41.9^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5-XXL</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">$28.8^{\dagger}$</td>
<td style="text-align: center;">$28.4^{\dagger}$</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">$42.5^{\dagger}$</td>
<td style="text-align: center;">$41.3^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">$29.7^{\dagger}$</td>
<td style="text-align: center;">$28.6^{\dagger}$</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">$41.1^{\dagger}$</td>
<td style="text-align: center;">$40.3^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Overall Avg</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">$40.6^{\dagger}$</td>
<td style="text-align: center;">$40.3^{\dagger}$</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">$29.8^{\dagger}$</td>
<td style="text-align: center;">$33.2^{\dagger, \ddagger}$</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">$37.6^{\dagger}$</td>
<td style="text-align: center;">$41.6^{\dagger, \ddagger}$</td>
</tr>
</tbody>
</table>
<p>Table 15: Spearman correlations on BAGEL dataset for data-to-text task. VAL, IST, and IDM denote the evaluator with vanilla, instruction, and the combination of instruction and demonstration, respectively. Values with $\dagger$ denote the evaluator with instruction significantly outperforms with vanilla, and values with $\ddagger$ denote the evaluator with the combination of instruction and demonstration significantly outperforms with only instruction. Values in bold are the best performance in a set of variants (e.g., GPT3 family).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">INF</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NAT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FLU</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
<td style="text-align: center;">IST+DM</td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
<td style="text-align: center;">IST+DM</td>
<td style="text-align: center;">VAL</td>
<td style="text-align: center;">IST</td>
<td style="text-align: center;">IST+DM</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-1</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-2</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BERTScore</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">MoverScore</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PRISM</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BARTSCORE</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">+CNN</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">+CNN+Para</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GPT3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT3-a01</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">$25.6^{\dagger}$</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">$34.0^{\dagger}$</td>
<td style="text-align: center;">$37.7^{\dagger, \ddagger}$</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">26.6</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-b01</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">$30.6^{\dagger}$</td>
<td style="text-align: center;">$26.1^{\dagger}$</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">$28.9^{\dagger}$</td>
<td style="text-align: center;">21.1</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-c01</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">$33.7^{\dagger, \ddagger}$</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">$28.5^{\dagger}$</td>
<td style="text-align: center;">$28.6^{\dagger}$</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">$27.6^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-d01</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">$33.9^{\dagger, \ddagger}$</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">$31.7^{\dagger}$</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">$39.7^{\dagger}$</td>
<td style="text-align: center;">27.1</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-d03</td>
<td style="text-align: center;">26.6</td>
<td style="text-align: center;">$29.6^{\dagger}$</td>
<td style="text-align: center;">$37.6^{\dagger, \ddagger}$</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">$27.0^{\dagger}$</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">28.2</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">$31.5^{\dagger, \ddagger}$</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">$30.4^{\dagger}$</td>
<td style="text-align: center;">$26.5^{\dagger}$</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">26.1</td>
</tr>
<tr>
<td style="text-align: center;">GPT2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT2-M</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">$32.7^{\dagger}$</td>
<td style="text-align: center;">$35.2^{\dagger, \ddagger}$</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">$34.8^{\dagger}$</td>
<td style="text-align: center;">$33.6^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2-L</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">$28.1^{\dagger}$</td>
<td style="text-align: center;">$20.2^{\dagger}$</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">$32.4^{\dagger}$</td>
<td style="text-align: center;">$37.8^{\dagger, \ddagger}$</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">$33.1^{\dagger}$</td>
<td style="text-align: center;">$35.9^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2-XL</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">$23.6^{\dagger}$</td>
<td style="text-align: center;">$23.8^{\dagger}$</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">$38.0^{\dagger, \ddagger}$</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">$29.8^{\dagger}$</td>
<td style="text-align: center;">$37.1^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J-6B</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">$25.6^{\dagger}$</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">$36.8^{\dagger, \ddagger}$</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">$34.5^{\dagger}$</td>
<td style="text-align: center;">$38.4^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">$25.1^{\dagger}$</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">$31.9^{\dagger}$</td>
<td style="text-align: center;">$37.0^{\dagger, \ddagger}$</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">$33.1^{\dagger}$</td>
<td style="text-align: center;">$36.2^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">OPT-350M</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">$28.7^{\dagger}$</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">$29.5^{\dagger}$</td>
<td style="text-align: center;">$35.0^{\dagger, \ddagger}$</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">$26.6^{\dagger}$</td>
<td style="text-align: center;">$27.3^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-1.3B</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">$28.3^{\dagger}$</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">$30.5^{\dagger}$</td>
<td style="text-align: center;">$38.7^{\dagger, \ddagger}$</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">$26.9^{\dagger}$</td>
<td style="text-align: center;">$29.8^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-6.7B</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">$31.0^{\dagger}$</td>
<td style="text-align: center;">$36.5^{\dagger, \ddagger}$</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">$25.8^{\dagger}$</td>
<td style="text-align: center;">$35.9^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-13B</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">$30.1^{\dagger}$</td>
<td style="text-align: center;">$38.0^{\dagger, \ddagger}$</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">$29.6^{\dagger}$</td>
<td style="text-align: center;">$34.9^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-66B</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">$24.7^{\dagger}$</td>
<td style="text-align: center;">$22.4^{\dagger}$</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">$29.1^{\dagger}$</td>
<td style="text-align: center;">$34.6^{\dagger, \ddagger}$</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">$25.3^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">$26.9^{\dagger}$</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">$30.0^{\dagger}$</td>
<td style="text-align: center;">$36.6^{\dagger, \ddagger}$</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">$25.6^{\dagger}$</td>
<td style="text-align: center;">$30.6^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FT5-S</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">15.9</td>
</tr>
<tr>
<td style="text-align: center;">FT5-B</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">$32.5^{\dagger}$</td>
<td style="text-align: center;">$33.4^{\dagger, \ddagger}$</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">$15.5^{\dagger}$</td>
<td style="text-align: center;">$16.8^{\dagger, \ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5-L</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">$37.1^{\dagger}$</td>
<td style="text-align: center;">$38.6^{\dagger, \ddagger}$</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">$21.1^{\ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5-XL</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">$37.4^{\ddagger}$</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">$22.5^{\ddagger}$</td>
</tr>
<tr>
<td style="text-align: center;">FT5-XXL</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">$34.7^{\dagger}$</td>
<td style="text-align: center;">$41.7^{\dagger, \ddagger}$</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">$22.2^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">$34.6^{\dagger}$</td>
<td style="text-align: center;">$36.8^{\dagger, \ddagger}$</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">$19.7^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Overall Avg</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">$34.2^{\dagger, \ddagger}$</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">$26.8^{\dagger}$</td>
<td style="text-align: center;">$28.2^{\dagger, \ddagger}$</td>
</tr>
</tbody>
</table>
<p>Table 16: Spearman correlations on SFRES dataset for data-to-text task. VAL, IST, and IDM denote the evaluator with vanilla, instruction, and the combination of instruction and demonstration, respectively. Values with $\dagger$ denote the evaluator with instruction significantly outperforms with vanilla, and values with $\ddagger$ denote the evaluator with the combination of instruction and demonstration significantly outperforms with only instruction. Values in bold are the best performance in a set of variants (e.g., GPT3 family).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://medium.com/pandorabots-blog/ mitsuku-wins-loebner-prize-2018-3e8d98c5f2a7&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
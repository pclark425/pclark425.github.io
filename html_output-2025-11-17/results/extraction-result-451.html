<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-451 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-451</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-451</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-268248803</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.01784v1.pdf" target="_blank">CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) such as ChatGPT are increasingly proficient in understanding and generating a mixture of code and text. Evaluation based on such $\textit{mixture}$ can lead to a more comprehensive understanding of the models' abilities in solving coding problems. However, in this context, current evaluation methods are either limited in task coverage or lack standardization. To address this issue, we propose using category theory as a framework for evaluation. Specifically, morphisms within a code category can represent code debugging and transformation, functors between two categories represent code translation, and functors between a code category and a natural language category represent code generation, explanation, and reproduction. We present an automatic evaluation framework called $\textbf{CatCode}$ ($\textbf{Cat}$egory $\textbf{Code}$) that can comprehensively assess the coding abilities of LLMs, including ChatGPT, Text-Davinci, and CodeGeeX.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e451.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e451.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>explain-reproduce_info_loss</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Information loss in explanation → reproduction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When LLMs are asked to explain code in natural language and then to reproduce code from that explanation, a substantial fraction of functional information is lost, causing reproduced programs to fail execution tests at much higher rates than direct code-to-code translation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CatCode standardized evaluation platform (explanation + reproduction pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-stage pipeline in CatCode where a model first maps a program (PL) to a natural-language explanation (explanation functor), then maps that explanation back to code in the same PL (reproduction functor), and the reproduced code is checked by execution-based tests.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>model-generated code explanation / problem description (natural-language explanation of code)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>function-level Java implementations (reproduced by model from explanation)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>information loss between natural-language explanation and code reproduction</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The generated natural-language explanation omits or alters details necessary to reconstruct functionally equivalent code (e.g., precise argument types, initialization values, control-flow nuances), so code reproduced from the explanation frequently differs in behavior from the original.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>explanation-to-code generation step (reproduction functor)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>execution-based evaluation: extract reproduced functions from model outputs and run test suites (Pass@1) to compare behavior to original code</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Pass@1 (execution-based) comparison: paper reports reproduced-code Pass@1 is ~30% lower on average than direct translation Pass@1 across datasets</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial — reproduced code Pass@1 is ~30% lower than translation Pass@1, indicating significant functional information loss; many reproduced solutions either fail tests ('Test Failure') or produce compilation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across all three datasets (HumanEval-X, MBXP, MathQA); consistently lower pass@1 for reproduction vs translation in experiments (average drop ≈30%).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Natural-language explanations produced by LLMs are lossy and may omit critical implementation details; models' mapping from NL back to code is imperfect and sensitive to missing details.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use richer prompts (few-shot exemplars) to force more complete explanations, include explicit specification templates (function name, args, return type, initialization), and standardize explanation format; improve test scripts and provide error messages to model for iterative debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not quantitatively evaluated in the paper — suggested as future work (e.g., few-shot prompts and templates may help), effectiveness unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>programming / code generation and evaluation (machine learning for code)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e451.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e451.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>func_equiv_vs_similarity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Misinterpreting functional equivalence as surface similarity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs often conflate surface-level similarity (syntactic or lexical) with true functional equivalence, causing failures to recognize semantically equivalent programs or to consider non-equivalent variants as equivalent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CatCode morphism identification / pairing test</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pairwise code-comparison pipeline where the model is given two code snippets and asked whether they are functionally equivalent (same input-output behavior) based on predefined morphisms.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>prompt asking for True/False functional-equivalence judgement and textual differences</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>filtered Java function pairs (original and morphism-applied variants)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>semantic misalignment: equating similarity with equivalence / failing to detect functional-preserving transformations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Models sometimes base equivalence judgements on surface cues (variable names, control structure similarity) rather than semantic behavior; they fail on both (a) recognizing self-morphisms (semantic-preserving edits) and (b) identifying global equivalence when different implementations compute same function.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>morphism identification / model judgement step</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>pairing test API that extracts model True/False answers and computes precision against ground-truth functional equivalence classes</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Precision(eq) and Precision(neq) computed from pairing-test labels; for global equivalence tasks models performed only slightly better than random (~50%) according to reported results</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Reduces reliability of automatic equivalence detection; local morphism detection mixed (models can detect some local-preserving edits) but global equivalence recognition is poor (near-random), undermining evaluations that rely on model judgements.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across datasets; certain morphism types (e.g., 'Unused Statements', 'Modify Condition', 'Boolean Exchange') are consistently hard to identify according to the paper's per-morphism analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Models lack robust program semantic understanding and over-rely on superficial features; training and prompting do not sufficiently capture semantics across diverse transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use execution-based equivalence checks where possible; augment data with diverse semantic-preserving transformations (predefined morphisms), use AST-based or symbolic analyses in the evaluation pipeline, and design prompts/coTs to encourage reasoning about semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not empirically evaluated in the paper; the authors provide a data-augmentation mechanism (predefined morphisms via JavaTransformer) but do not report mitigation performance gains.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>program analysis / ML for code</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e451.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e451.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>prompt_variability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt phrasing sensitivity causing inconsistent behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different natural-language prompts that convey similar intents produce different outputs from LLMs, introducing nondeterministic divergences between described tasks and implemented code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CatCode prompt selector + input constructor</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The platform uses prompt templates selected by a 'prompt selector' to instruct models for each task; variability in prompt wording is noted to produce differing model behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>task prompt / instruction templates</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>model-generated code outputs (responses depend on prompt wording)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous description / prompt sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Semantically similar prompts lead to different model outputs and therefore different code implementations or explanations, making the mapping from NL description to code non-robust and inconsistent across prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model input / instruction stage (prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>observational: authors note that varying prompts that express similar meanings produce different results; not exhaustively tested but reported as limitation</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>No systematic quantification in paper; authors highlight prompt sensitivity qualitatively and list it as limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Introduces variability and reduces comparability between runs and models; complicates standardization of evaluations because different prompts bias model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>General observation across tasks; exact prevalence not measured in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>LLM sensitivity to surface linguistic differences and lack of prompt-robust objective functions; instruction-following behavior depends on phrasing and exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Standardize prompts in the framework, use prompt selection heuristics, evaluate prompt robustness, and explore few-shot or chain-of-thought (CoT) prompting to stabilize outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not quantified in the paper; authors list prompt standardization and further study of prompt robustness as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>LLM evaluation / ML benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e451.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e451.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>translation_type_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Type and name alignment mismatches in code translation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatic translation from one programming language to another by LLMs introduces type mismatches, incorrect variable references, and API/semantic shifts that cause compilation errors or runtime failures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CatCode translation-functor evaluation (Java → Python / JavaScript)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Models are prompted (or given direct code for CodeGeeX) to translate Java functions into target languages; outputs are parsed into functions and executed against test suites.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>single-line instruction prompt for translation (e.g., 'translate Java to Python')</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>translated code implementations in Python/JavaScript</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different language type mapping / variable/name mismatches</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Translation errors commonly arise from mismatched types (e.g., Java collections → ambiguous Python types), incorrect handling of variable scopes/names leading to NameError/ReferenceError, and API mismatches causing compilation or runtime errors.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>code translation stage (functor between PL categories)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>execution-based evaluation of translated functions (Pass@1) and error-type analysis of failures (compilation vs test failures)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Pass@1 rates for translated code; failure-type statistics (counts of CannotFindSymbol, TypeError, NameError, ReferenceError, Test Failure) reported per dataset and model.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Causes translation failures dominated by compilation errors; translation accuracy varies by dataset and model (ChatGPT slightly better than Text-Davinci; MathQA translations nearly perfect due to simple structure).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common — most frequent failures in translation experiments were compilation/type-related errors according to the paper's Table 5 and failure analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguity in mapping language-specific types and APIs, lack of explicit type information in NL prompts, and model limitations in preserving interface/variable semantics across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Provide explicit type annotations or translation templates, post-process translations to align types, pass compiler/interpreter error messages back to model for iterative debugging, and improve prompt instructions to preserve signatures and variable semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not experimentally shown in the paper; suggested as future work (e.g., returning error messages to model for self-debugging).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>program translation / ML for code</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e451.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e451.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>evaluation_test_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Errors and mismatches in execution test scripts and evaluation infrastructure</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Discrepancies between model-generated or benchmark-provided test scripts and intended evaluation semantics (including incorrect or missing tests) lead to incorrect Pass@1 judgments and hide actual model failures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CatCode execution-based test API (MxEval integration)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluation uses an execution test API (MxEval) to compile and run functions; dataset-provided test harnesses or model-generated test scripts are used to assess correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>model-generated test descriptions and dataset-provided test cases / harnesses</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>evaluation test scripts / harnesses used to validate generated code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incorrect or incomplete test scripts / evaluation harness mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Some evaluation failures stem from incorrect test scripts (e.g., Mxeval-generated tests that do not match intended semantics) or missing test cases, causing false negatives/positives in Pass@1 measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation/test harness (execution testing)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>case analysis and inspection of failing runs; the paper cites examples where incorrect Mxeval test scripts led to misleading execution results.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Counting 'Test Failure' instances and analyzing failed test-case outputs; noting dataset splits where Mxeval is missing test cases (paper excludes missing cases and reports counts).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can produce misleading Pass@1 scores and mask or exaggerate model capabilities; authors removed certain problematic cases and noted that some translation failures were actually due to incorrect tests.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not fully quantified; specific problematic cases are identified (e.g., several Mxeval test cases missing or incorrect), but overall prevalence not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Automated generation/transpilation of tests can introduce errors; benchmark pipelines may not guarantee correct, comprehensive test harnesses across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Filter out or fix problematic test cases, improve test-harness generation quality, and manually inspect suspicious failures; standardize evaluation scripts in the CatCode platform and open-source the pipeline for community review.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially applied (the authors filtered/corrected some cases and excluded missing tests); systematic effectiveness not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>benchmarking / evaluation engineering</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e451.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e451.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>morphism_detection_edge_cases</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hard-to-detect morphisms (e.g., unused statements, condition modification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Certain small, semantic-affecting edits (or benign-looking edits) are difficult for LLMs to label correctly as equivalent or non-equivalent; these include insertion of unused statements, subtle condition changes, and boolean exchanges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CatCode local morphism generation + morphism identification evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Local morphisms (AST transformations) are applied to Java functions to create variants; models are evaluated on their ability to classify pairs as equivalent (self-morphism) or non-equivalent.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>pairing prompt describing two code snippets and asking True/False with differences</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Java functions transformed via predefined AST morphisms using JavaTransformer</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>subtle code transformation misclassification / ambiguous functional impact</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Morphisms such as Unused Statement (inserting dead code), Modify Condition (changing comparator boundaries), and Boolean Exchange (flipping boolean constants or logic) are frequently misclassified; some are function-preserving yet hard to detect, others subtly change semantics and are missed.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data augmentation / morphism application and model judgement</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>apply nine predefined AST morphisms (some self-morphisms, some not), then use pairing test to compute per-morphism precision;</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Average precision for 1-eq, 2-eq, and 1-neq local morphisms per dataset and per model; paper reports specific morphisms with the three worst average precision scores and discusses persistent difficulty across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Undermines trust in automated equivalence detection and the ability to benchmark semantic robustness; some morphism types consistently produce low precision and therefore bias evaluations of model semantic understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Persistent across datasets — 'Unused Statements' is particularly hard across datasets; other morphisms show dataset-specific difficulty (e.g., 'Modify Condition' on MathQA).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Challenges in reasoning about subtle semantic effects and lack of explicit execution-based checks in pairing tests; the model's surface-text reasoning may miss control-flow or boundary-condition changes.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Combine static/AST-aware checks and dynamic execution tests to validate equivalence, augment training/evaluation sets with diverse morphisms, and use pairwise execution where feasible to establish true functional equivalence.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not quantified; authors use JavaTransformer to generate morphisms for augmentation but do not report improvements from mitigation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>semantic robustness / program transformation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MBXP <em>(Rating: 2)</em></li>
                <li>MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation <em>(Rating: 2)</em></li>
                <li>Adversarial examples for models of code <em>(Rating: 2)</em></li>
                <li>On the generalizability of neural program models with respect to semantic-preserving program transformations <em>(Rating: 2)</em></li>
                <li>CodeBLEU: a method for automatic evaluation of code synthesis <em>(Rating: 1)</em></li>
                <li>Testing neural programs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-451",
    "paper_id": "paper-268248803",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "explain-reproduce_info_loss",
            "name_full": "Information loss in explanation → reproduction pipeline",
            "brief_description": "When LLMs are asked to explain code in natural language and then to reproduce code from that explanation, a substantial fraction of functional information is lost, causing reproduced programs to fail execution tests at much higher rates than direct code-to-code translation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CatCode standardized evaluation platform (explanation + reproduction pipeline)",
            "system_description": "Two-stage pipeline in CatCode where a model first maps a program (PL) to a natural-language explanation (explanation functor), then maps that explanation back to code in the same PL (reproduction functor), and the reproduced code is checked by execution-based tests.",
            "nl_description_type": "model-generated code explanation / problem description (natural-language explanation of code)",
            "code_implementation_type": "function-level Java implementations (reproduced by model from explanation)",
            "gap_type": "information loss between natural-language explanation and code reproduction",
            "gap_description": "The generated natural-language explanation omits or alters details necessary to reconstruct functionally equivalent code (e.g., precise argument types, initialization values, control-flow nuances), so code reproduced from the explanation frequently differs in behavior from the original.",
            "gap_location": "explanation-to-code generation step (reproduction functor)",
            "detection_method": "execution-based evaluation: extract reproduced functions from model outputs and run test suites (Pass@1) to compare behavior to original code",
            "measurement_method": "Pass@1 (execution-based) comparison: paper reports reproduced-code Pass@1 is ~30% lower on average than direct translation Pass@1 across datasets",
            "impact_on_results": "Substantial — reproduced code Pass@1 is ~30% lower than translation Pass@1, indicating significant functional information loss; many reproduced solutions either fail tests ('Test Failure') or produce compilation errors.",
            "frequency_or_prevalence": "Observed across all three datasets (HumanEval-X, MBXP, MathQA); consistently lower pass@1 for reproduction vs translation in experiments (average drop ≈30%).",
            "root_cause": "Natural-language explanations produced by LLMs are lossy and may omit critical implementation details; models' mapping from NL back to code is imperfect and sensitive to missing details.",
            "mitigation_approach": "Use richer prompts (few-shot exemplars) to force more complete explanations, include explicit specification templates (function name, args, return type, initialization), and standardize explanation format; improve test scripts and provide error messages to model for iterative debugging.",
            "mitigation_effectiveness": "Not quantitatively evaluated in the paper — suggested as future work (e.g., few-shot prompts and templates may help), effectiveness unknown.",
            "domain_or_field": "programming / code generation and evaluation (machine learning for code)",
            "reproducibility_impact": true,
            "uuid": "e451.0",
            "source_info": {
                "paper_title": "CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "func_equiv_vs_similarity",
            "name_full": "Misinterpreting functional equivalence as surface similarity",
            "brief_description": "LLMs often conflate surface-level similarity (syntactic or lexical) with true functional equivalence, causing failures to recognize semantically equivalent programs or to consider non-equivalent variants as equivalent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CatCode morphism identification / pairing test",
            "system_description": "Pairwise code-comparison pipeline where the model is given two code snippets and asked whether they are functionally equivalent (same input-output behavior) based on predefined morphisms.",
            "nl_description_type": "prompt asking for True/False functional-equivalence judgement and textual differences",
            "code_implementation_type": "filtered Java function pairs (original and morphism-applied variants)",
            "gap_type": "semantic misalignment: equating similarity with equivalence / failing to detect functional-preserving transformations",
            "gap_description": "Models sometimes base equivalence judgements on surface cues (variable names, control structure similarity) rather than semantic behavior; they fail on both (a) recognizing self-morphisms (semantic-preserving edits) and (b) identifying global equivalence when different implementations compute same function.",
            "gap_location": "morphism identification / model judgement step",
            "detection_method": "pairing test API that extracts model True/False answers and computes precision against ground-truth functional equivalence classes",
            "measurement_method": "Precision(eq) and Precision(neq) computed from pairing-test labels; for global equivalence tasks models performed only slightly better than random (~50%) according to reported results",
            "impact_on_results": "Reduces reliability of automatic equivalence detection; local morphism detection mixed (models can detect some local-preserving edits) but global equivalence recognition is poor (near-random), undermining evaluations that rely on model judgements.",
            "frequency_or_prevalence": "Observed across datasets; certain morphism types (e.g., 'Unused Statements', 'Modify Condition', 'Boolean Exchange') are consistently hard to identify according to the paper's per-morphism analysis.",
            "root_cause": "Models lack robust program semantic understanding and over-rely on superficial features; training and prompting do not sufficiently capture semantics across diverse transformations.",
            "mitigation_approach": "Use execution-based equivalence checks where possible; augment data with diverse semantic-preserving transformations (predefined morphisms), use AST-based or symbolic analyses in the evaluation pipeline, and design prompts/coTs to encourage reasoning about semantics.",
            "mitigation_effectiveness": "Not empirically evaluated in the paper; the authors provide a data-augmentation mechanism (predefined morphisms via JavaTransformer) but do not report mitigation performance gains.",
            "domain_or_field": "program analysis / ML for code",
            "reproducibility_impact": true,
            "uuid": "e451.1",
            "source_info": {
                "paper_title": "CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "prompt_variability",
            "name_full": "Prompt phrasing sensitivity causing inconsistent behavior",
            "brief_description": "Different natural-language prompts that convey similar intents produce different outputs from LLMs, introducing nondeterministic divergences between described tasks and implemented code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CatCode prompt selector + input constructor",
            "system_description": "The platform uses prompt templates selected by a 'prompt selector' to instruct models for each task; variability in prompt wording is noted to produce differing model behaviors.",
            "nl_description_type": "task prompt / instruction templates",
            "code_implementation_type": "model-generated code outputs (responses depend on prompt wording)",
            "gap_type": "ambiguous description / prompt sensitivity",
            "gap_description": "Semantically similar prompts lead to different model outputs and therefore different code implementations or explanations, making the mapping from NL description to code non-robust and inconsistent across prompt variants.",
            "gap_location": "model input / instruction stage (prompting)",
            "detection_method": "observational: authors note that varying prompts that express similar meanings produce different results; not exhaustively tested but reported as limitation",
            "measurement_method": "No systematic quantification in paper; authors highlight prompt sensitivity qualitatively and list it as limitation.",
            "impact_on_results": "Introduces variability and reduces comparability between runs and models; complicates standardization of evaluations because different prompts bias model performance.",
            "frequency_or_prevalence": "General observation across tasks; exact prevalence not measured in the experiments.",
            "root_cause": "LLM sensitivity to surface linguistic differences and lack of prompt-robust objective functions; instruction-following behavior depends on phrasing and exemplars.",
            "mitigation_approach": "Standardize prompts in the framework, use prompt selection heuristics, evaluate prompt robustness, and explore few-shot or chain-of-thought (CoT) prompting to stabilize outputs.",
            "mitigation_effectiveness": "Not quantified in the paper; authors list prompt standardization and further study of prompt robustness as future work.",
            "domain_or_field": "LLM evaluation / ML benchmarking",
            "reproducibility_impact": true,
            "uuid": "e451.2",
            "source_info": {
                "paper_title": "CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "translation_type_mismatch",
            "name_full": "Type and name alignment mismatches in code translation",
            "brief_description": "Automatic translation from one programming language to another by LLMs introduces type mismatches, incorrect variable references, and API/semantic shifts that cause compilation errors or runtime failures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CatCode translation-functor evaluation (Java → Python / JavaScript)",
            "system_description": "Models are prompted (or given direct code for CodeGeeX) to translate Java functions into target languages; outputs are parsed into functions and executed against test suites.",
            "nl_description_type": "single-line instruction prompt for translation (e.g., 'translate Java to Python')",
            "code_implementation_type": "translated code implementations in Python/JavaScript",
            "gap_type": "different language type mapping / variable/name mismatches",
            "gap_description": "Translation errors commonly arise from mismatched types (e.g., Java collections → ambiguous Python types), incorrect handling of variable scopes/names leading to NameError/ReferenceError, and API mismatches causing compilation or runtime errors.",
            "gap_location": "code translation stage (functor between PL categories)",
            "detection_method": "execution-based evaluation of translated functions (Pass@1) and error-type analysis of failures (compilation vs test failures)",
            "measurement_method": "Pass@1 rates for translated code; failure-type statistics (counts of CannotFindSymbol, TypeError, NameError, ReferenceError, Test Failure) reported per dataset and model.",
            "impact_on_results": "Causes translation failures dominated by compilation errors; translation accuracy varies by dataset and model (ChatGPT slightly better than Text-Davinci; MathQA translations nearly perfect due to simple structure).",
            "frequency_or_prevalence": "Common — most frequent failures in translation experiments were compilation/type-related errors according to the paper's Table 5 and failure analysis.",
            "root_cause": "Ambiguity in mapping language-specific types and APIs, lack of explicit type information in NL prompts, and model limitations in preserving interface/variable semantics across languages.",
            "mitigation_approach": "Provide explicit type annotations or translation templates, post-process translations to align types, pass compiler/interpreter error messages back to model for iterative debugging, and improve prompt instructions to preserve signatures and variable semantics.",
            "mitigation_effectiveness": "Not experimentally shown in the paper; suggested as future work (e.g., returning error messages to model for self-debugging).",
            "domain_or_field": "program translation / ML for code",
            "reproducibility_impact": true,
            "uuid": "e451.3",
            "source_info": {
                "paper_title": "CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "evaluation_test_mismatch",
            "name_full": "Errors and mismatches in execution test scripts and evaluation infrastructure",
            "brief_description": "Discrepancies between model-generated or benchmark-provided test scripts and intended evaluation semantics (including incorrect or missing tests) lead to incorrect Pass@1 judgments and hide actual model failures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CatCode execution-based test API (MxEval integration)",
            "system_description": "Evaluation uses an execution test API (MxEval) to compile and run functions; dataset-provided test harnesses or model-generated test scripts are used to assess correctness.",
            "nl_description_type": "model-generated test descriptions and dataset-provided test cases / harnesses",
            "code_implementation_type": "evaluation test scripts / harnesses used to validate generated code",
            "gap_type": "incorrect or incomplete test scripts / evaluation harness mismatch",
            "gap_description": "Some evaluation failures stem from incorrect test scripts (e.g., Mxeval-generated tests that do not match intended semantics) or missing test cases, causing false negatives/positives in Pass@1 measurements.",
            "gap_location": "evaluation/test harness (execution testing)",
            "detection_method": "case analysis and inspection of failing runs; the paper cites examples where incorrect Mxeval test scripts led to misleading execution results.",
            "measurement_method": "Counting 'Test Failure' instances and analyzing failed test-case outputs; noting dataset splits where Mxeval is missing test cases (paper excludes missing cases and reports counts).",
            "impact_on_results": "Can produce misleading Pass@1 scores and mask or exaggerate model capabilities; authors removed certain problematic cases and noted that some translation failures were actually due to incorrect tests.",
            "frequency_or_prevalence": "Not fully quantified; specific problematic cases are identified (e.g., several Mxeval test cases missing or incorrect), but overall prevalence not reported.",
            "root_cause": "Automated generation/transpilation of tests can introduce errors; benchmark pipelines may not guarantee correct, comprehensive test harnesses across languages.",
            "mitigation_approach": "Filter out or fix problematic test cases, improve test-harness generation quality, and manually inspect suspicious failures; standardize evaluation scripts in the CatCode platform and open-source the pipeline for community review.",
            "mitigation_effectiveness": "Partially applied (the authors filtered/corrected some cases and excluded missing tests); systematic effectiveness not quantified.",
            "domain_or_field": "benchmarking / evaluation engineering",
            "reproducibility_impact": true,
            "uuid": "e451.4",
            "source_info": {
                "paper_title": "CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "morphism_detection_edge_cases",
            "name_full": "Hard-to-detect morphisms (e.g., unused statements, condition modification)",
            "brief_description": "Certain small, semantic-affecting edits (or benign-looking edits) are difficult for LLMs to label correctly as equivalent or non-equivalent; these include insertion of unused statements, subtle condition changes, and boolean exchanges.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CatCode local morphism generation + morphism identification evaluation",
            "system_description": "Local morphisms (AST transformations) are applied to Java functions to create variants; models are evaluated on their ability to classify pairs as equivalent (self-morphism) or non-equivalent.",
            "nl_description_type": "pairing prompt describing two code snippets and asking True/False with differences",
            "code_implementation_type": "Java functions transformed via predefined AST morphisms using JavaTransformer",
            "gap_type": "subtle code transformation misclassification / ambiguous functional impact",
            "gap_description": "Morphisms such as Unused Statement (inserting dead code), Modify Condition (changing comparator boundaries), and Boolean Exchange (flipping boolean constants or logic) are frequently misclassified; some are function-preserving yet hard to detect, others subtly change semantics and are missed.",
            "gap_location": "data augmentation / morphism application and model judgement",
            "detection_method": "apply nine predefined AST morphisms (some self-morphisms, some not), then use pairing test to compute per-morphism precision;",
            "measurement_method": "Average precision for 1-eq, 2-eq, and 1-neq local morphisms per dataset and per model; paper reports specific morphisms with the three worst average precision scores and discusses persistent difficulty across datasets.",
            "impact_on_results": "Undermines trust in automated equivalence detection and the ability to benchmark semantic robustness; some morphism types consistently produce low precision and therefore bias evaluations of model semantic understanding.",
            "frequency_or_prevalence": "Persistent across datasets — 'Unused Statements' is particularly hard across datasets; other morphisms show dataset-specific difficulty (e.g., 'Modify Condition' on MathQA).",
            "root_cause": "Challenges in reasoning about subtle semantic effects and lack of explicit execution-based checks in pairing tests; the model's surface-text reasoning may miss control-flow or boundary-condition changes.",
            "mitigation_approach": "Combine static/AST-aware checks and dynamic execution tests to validate equivalence, augment training/evaluation sets with diverse morphisms, and use pairwise execution where feasible to establish true functional equivalence.",
            "mitigation_effectiveness": "Not quantified; authors use JavaTransformer to generate morphisms for augmentation but do not report improvements from mitigation strategies.",
            "domain_or_field": "semantic robustness / program transformation evaluation",
            "reproducibility_impact": true,
            "uuid": "e451.5",
            "source_info": {
                "paper_title": "CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MBXP",
            "rating": 2
        },
        {
            "paper_title": "MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation",
            "rating": 2,
            "sanitized_title": "multiple_a_scalable_and_polyglot_approach_to_benchmarking_neural_code_generation"
        },
        {
            "paper_title": "Adversarial examples for models of code",
            "rating": 2,
            "sanitized_title": "adversarial_examples_for_models_of_code"
        },
        {
            "paper_title": "On the generalizability of neural program models with respect to semantic-preserving program transformations",
            "rating": 2,
            "sanitized_title": "on_the_generalizability_of_neural_program_models_with_respect_to_semanticpreserving_program_transformations"
        },
        {
            "paper_title": "CodeBLEU: a method for automatic evaluation of code synthesis",
            "rating": 1,
            "sanitized_title": "codebleu_a_method_for_automatic_evaluation_of_code_synthesis"
        },
        {
            "paper_title": "Testing neural programs",
            "rating": 1,
            "sanitized_title": "testing_neural_programs"
        }
    ],
    "cost": 0.013077249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text
4 Mar 2024</p>
<p>Zhenru Lin 
Yiqun Yao yqyao@baai.ac.cn 
Yang Yuan yuanyang@tsinghua.edu.cn </p>
<p>IIIS
Tsinghua University</p>
<p>IIIS
Tsinghua University Shanghai Artificial Intelligence Laboratory Shanghai Qi Zhi Institute</p>
<p>CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text
4 Mar 202498C8BC8031D9F2E829461C91F26F9AE5arXiv:2403.01784v1[cs.AI]
Large language models (LLMs) such as ChatGPT are increasingly proficient in understanding and generating a mixture of code and text.Evaluation based on such mixture can lead to a more comprehensive understanding of the models' abilities in solving coding problems.However, in this context, current evaluation methods are either limited in task coverage or lack standardization.To address this issue, we propose using category theory as a framework for evaluation.Specifically, morphisms within a code category can represent code debugging and transformation, functors between two categories represent code translation, and functors between a code category and a natural language category represent code generation, explanation, and reproduction.We present an automatic evaluation framework called CatCode (Category Code) that can comprehensively assess the coding abilities of LLMs, including ChatGPT, Text-Davinci, and CodeGeeX.</p>
<p>Introduction</p>
<p>The success of large language models (LLMs) as programming assistants has been widely acknowledged, with their higher proficiency demonstrated in various coding tasks such as code generation [12,21], code explanation [14], and code translation [25], among others.For instance, Alpha-Code ranked in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces [12].The Codex-powered programming tool Copilot [7] serves as an effective tool for generating and editing code.Undoubtedly, LLMs exhibit remarkable capabilities when addressing different coding scenarios individually.However, what truly sets the recent LLMs apart is their ability, akin to ChatGPT-like models, to comprehend and align with human intents by processing a mixture of natural language and code.This unique attribute significantly lowers the entry barrier for users, leading to their widespread adoption and notable achievements.</p>
<p>To assess the coding ability of LLMs, numerous efforts have been made by researchers.However, current evaluation methods are either limited in task coverage or lack standardization.Match-based automatic frameworks such as CodeBLEU [19] rely primarily on similarity scores to evaluate the quality of code.However, such frameworks often fail to capture the nuances of code functionality and meaning.Execution-based evaluation methods, for example, MBXP [4] and MultiPL-E [6], can evaluate the function accuracy of code, but they primarily focus on code generation and translation  tasks.Task-based evaluation frameworks like CodeXGLUE [13] offer a comprehensive approach but lack standardization due to variations in datasets, task formulations, and APIs.Consequently, we still lack an evaluation framework to adapt to the context of a mixture of natural language and code, and there is a need to establish a comprehensive evaluation framework that not only supports diverse and novel task definitions, but also provides a standardized approach for evaluating the model under such a mixture context.</p>
<p>We aim to establish a comprehensive theoretical framework that can be open-sourced and applied to essential coding tasks, providing standardized automatic evaluation metrics.Designing an extensive and standardized evaluation framework becomes a challenging task in the absence of theoretical guidance.To address this need, we seek a theory that can effectively express the structural aspects of code, language, and their interrelationships.In this context, we find category theory, a branch of mathematics, to be particularly suitable for describing relationships between various elements.Furthermore, there are existing applications for employing category theory to describe code and language, making it an appropriate choice as a framework.</p>
<p>By utilizing category theory's mathematical abstractions, we can gain insights into the relationships among different programming languages (PLs) and natural languages (NLs).We consider PLs and NLs as categories, functionally equivalent programs as objects, and leverage functors and morphisms to capture the object relations within and across these categories.This gives a unified framework for describing functional equivalence, which not only works within a single PL, but also among different PLs, and even between PLs and NLs.By learning the morphisms within a programming language category, the model can grasp the similarities and differences between code snippets.Additionally, by acquiring knowledge of the functors between categories, the model can understand the relationship between different programming languages and natural languages.</p>
<p>Based on the categorical perspective above, we build a standardized evaluation framework (see Figure 1).It is standardized in terms of data definition, task formulation, and APIs.This framework can be extended to many code-related tasks as long as a categorical definition is given.In our experiments, we give some examples of common code-related tasks, and assess models such as ChatGPT and Text-Davinci for their capabilities in identifying functional equivalence in code, performing code translation, generating code explanations, and reconstructing code based on explanations.We have observed that these models still struggle to differentiate between the concepts of "functional equivalence" and "similarity" in code.While they demonstrated relatively satisfactory performance in code translation, maintaining functional equivalence between code and its corresponding explanations remains a persistent challenge.</p>
<p>Java</p>
<p>Reproduction functor explanation functor translation functor self-morphism morphism English python Our main contributions can be listed as follows:
A B C A' B' C' A'' B'' C'' Java A B C Java python A B C A'' B'' C'' Java A B C Java English A B C A' B' C' Java A B C
• We introduce CatCode, a novel evaluation perspective for code-related tasks based on category theory, which provides a comprehensive framework that encompasses a wide range of code-related task formulations.• We present a standardized automatic evaluation platform within the CatCode framework, that offers a quantitative assessment of the coding abilities of Language Models (LLMs) and can adapt to various datasets and models, which will be publicly available.• We evaluate competitive LLMs, including ChatGPT and Text-Davinci [15], providing insights into their strengths and limitations in understanding the mixture of NL and code.</p>
<p>Methods</p>
<p>Generally, it is difficult to achieve both comprehensiveness and standardization.In Section 2.1, we show that the categorical perspective, with its emphasis on generalization and abstraction, offers a valuable approach to achieving comprehensiveness in dealing with the mixture of code and natural language.In Section 2.2, we discuss the significance of standardization and outline strategies for achieving it.</p>
<p>Comprehensive Categorical Perspective</p>
<p>Category Theory has been applied to different fields, ranging from functional programming to logic, software design, and linguistics [20,2,9,5].Here we provide the basic definitions used in our work and explain their applications to describe code and natural language.-
Associativity: If f : a → b, g : b → c, and h : c → d then h • (g • f ) = (h • g) • f.
-Identity: For every object x, there exists a morphism id x : x → x called the identity morphism for x, such that for every morphism f : a → b, we have
id b • f = f = f • id a .
Application 2.1.(Category, Object, Morphism of Code and NL).For describing the mixture of code and natural languages, we first regard them as objects from different categories.We consider the code Figure 3: Standardized evaluation platform.The central pipeline offers a consistent approach for all evaluations.Behind the pipeline, we provide a variety of functions to automatically conduct the most important steps.With our platform released, the pipeline can easily accommodate novel datasets, tasks, and models by following the instructions outlined alongside the grey lines.</p>
<p>categories which contain all executable code in a certain language: C Java , C P ython , ... We define the natural language categories which contain all description/explanation/summarization of code etc.: C English , C Chinese , ... For simplicity, let's call them C 1 , C 2 , ... In a category, there are infinitely many different objects.We name these objects o 1 , o 2 , ... Each object contains infinitely many programs, with the same running outcome for each valid input.We use this definition because we focus on the functional equivalence of code function.For example, one may have slightly different implementations of quick sorts, but they represent exactly the same function. 2ased on the definition of objects, we define the morphism between two objects, as "necessary edits to convert a function to another".In particular, we define the self-morphism for each object, as "edits that do not change the functionality of the program".-For every object x in C, F (id x ) = id F (x) ;</p>
<p>-For all morphisms f : x → y and g :
y → z, F (g • f ) = F (g) • F (f ).
Application 2.2.(Functor of Mixture of Code and NL).We define the functor from C 1 to C 2 , to be the transform from one language to another language, but with the same functionality.When it applies to two code categories, it usually represents code translation, and we define it as a "translation functor".When the functor is between a code category and a natural language category, it may have many possible meanings, for instance, we can define "generation function" from NL to PL that generate code solution to a problem description, "explanation functor" from PL to NL that explains a piece of code, and "reproduction functor" from NL to PL that generates code snippets based on code descriptions.</p>
<p>Standardized Evaluation Platform</p>
<p>Figure 3 shows the streamlined process of our standardized evaluation platform.Standardization within the framework is achieved through a focus on three critical aspects: data definition, task formulation, and APIs.</p>
<p>Data definition</p>
<p>The original datasets may consist of a mixture of code and natural languages, so a clear data definition is crucial.The code can be in different languages (Java, Python, JavaScript, etc.), and on different levels (single-line, function-level, file-level).Natural language can be in different relationships with the code (problem descriptions, code comments, demands, etc.).</p>
<p>By establishing a standardized data definition, we ensure compatibility and facilitate the comparison and integration of different datasets.</p>
<p>For implementation, we use a "data formatter" to rearrange code based on the definition of objects and categories.Additionally, we provide "predefined morphisms" and implement them based on JavaTransformer [18] to automatically apply morphisms to code objects, which makes it easy for data augmentation.</p>
<p>Task formulation</p>
<p>We use a categorical perspective to formulate diverse code-related tasks using the unified language of objects, morphisms, and functors.</p>
<p>A good task formulation allows for a more generalized and flexible approach to defining more complex code problems, enabling the inclusion of a wide variety of code-related tasks and comparing their similarities and differences.</p>
<p>Based on the task formulation, we carefully decide what parts of the code and NL should be the model input, ensuring a cleaner setting.Then we use "prompt selector" to select a suitable prompt for task description, and use "input constructor" to combine the data and prompt as task input.</p>
<p>APIs</p>
<p>After the data is ready, we focus on the standardization of model APIs and test APIs.</p>
<p>Standardized APIs promote transparency, fairness, and efficiency in the evaluation.</p>
<p>By defining a clear and consistent set of APIs, we have integrated OpenAI models into the evaluation process and can test them using parallel requests.For postprocessing the model's output, we enable the extraction and filtering of plain text from the code-text mixture.For test APIs, we provide a "pairing test" API for evaluating the model's answer based on natural language, and an "execution test" API, which connects to Mxeval [4] for compiling and running the tests for a given code.</p>
<p>3 Experiments</p>
<p>Research Questions and Basic Settings</p>
<p>In this section, we exhibit some experimental examples of how to use our platform.Meanwhile, we explore the following three research questions (RQs) that correspond to those illustrated in Step 3 of Figure 1.</p>
<p>RQ1: Can the model exactly capture code functions and identify similarities/differences? RQ3: Can the model reproduce the code based on its explanation?
(
(Related tasks: code explanation, code summarization, code generation, code reproduction)</p>
<p>→ Categorical perspective: Can the model preserve the functional equivalence after applying an explanation functor and then a reproduction functor?</p>
<p>We conduct three experiments from categorical aspects accordingly.For a common setting, we use three multilingual datasets: HumanEval-X [24], MBXP [4], and MathQA [3,4] throughout the three experiments, and use Text-Davinci-003 (Text-Davinci for short), gpt3.5-turbo-0301(ChatGPT for
1-neq morphism ! 𝑎 ! 𝑏 ̅ 𝑐 ≁ 𝑎 ! 𝑎 ! " 𝑎 ! # 𝑎 $ 1-e q m o rp h is m 𝑑 = 2 𝑑 = 1 𝑑 = 1 ∃ global morphism 𝑑 ≫ 1 𝒞</p>
<p>Experiment 1: Morphism Identification Within a Code Category</p>
<p>Categorical Perspective Settings</p>
<p>Object Distance To test the model on both local and global scales, we define the "distance" between two codes.As illustrated in Figure 4.In a PL Category C, we use ∼ to express functional equivalence.ā = {x | x ∈ C, x ∼ a} means the set of all code that has the same functionality as a.In this category, ā and c are two different objects.Within ā, all code instances are functionally equivalent, e.g. a i and a j .We can apply predefined morphisms to the objects.For example, we can apply 1-step equivalent morphisms(1-eq morphism) on a i to get new program instances still in ā, or apply 1-step nonequivalent morphism(1-neq morphism) to get new instance in another equivalent class b.Distance d represents the minimal number of morphisms needed to transform one program to another given a predefined morphism set.For example, Applying two different self-morphism on a i returns a 1 i and a 2 i , then we need at least two steps to transform from a 1 i to a 2 i .Those are local-scale equivalence.If it is hard to transform a i to a j within just a few steps(e.g., 1 or 2 steps as illustrated here), we treat them as global-scale equivalence.</p>
<p>Implementation</p>
<p>Dataset For local morphism, we extract the Java functions within HumanEval-X, MBXP, and MathQA datasets.We perform the following 9 local morphisms based on AST transformations:</p>
<ol>
<li>Variable Renaming (VR): rename a variable, with a new name randomly chosen from all variable names in the dataset, or use "var N " for simplicity 2. Boolean Exchange (BE): propagate the exchange of "true" and "false" throughout the method and preserve the same logic 3. Loop Exchange (LE): exchange f or loops and while loops 4. Switch To If (SI): replace a switch statement with an equivalent if statement 5. Unused Statement(US): insert an unused string declaration to a randomly selected basic block in a method 6. Reorder Condition (RC): write the condition in reverse order (e.g., change i &lt; n to n &gt; i) 7. Permute Statement(PS): swap two independent statements (i.e.without data or control dependence) 8. Modify Condition(MC): change the binary operation in the condition (e.g., change i &lt; n to i ≤ n) 9. Remove Else(RE): delete the else branch of the if-else statement The first 7 morphisms are adopted from JavaTransform [18] that conducts functionally equivalent transformations 3 , which means they are self-morphisms.Other than self-morphisms, we add 2 morphisms: ModifyCondition and RemoveElse, that change the program's function.</li>
</ol>
<p>For global morphisms, since the three datasets above do not contain multiple solutions to the same problem using the same PL, we complement with code from the test split of Code Contest [12] dataset.</p>
<p>Models</p>
<p>We evaluate Text-Davinci and ChatGPT.We input (code, code) pairs from the datasets or generated by morphisms, and ask the model to answer whether they are functionally equivalent.</p>
<p>Evaluation We collect the model's responses and calculate the average precision for different morphisms.</p>
<p>Results</p>
<p>The results are demonstrated in Figure 4. Overall, Text-Davinci and ChatGPT make a tie.Locally, ChatGPT is better at identifying nonequivalent morphisms, and worse at equivalent ones.Globally, Text-Davinci is better at identifying global nonequivalent morphisms.Note that a random guess leads to a precision of 50%, so both models behave just slightly better than random guesses for identifying global equivalence.</p>
<p>Experiment 2: Translation Functor Between Different PL Categories</p>
<p>Categorical Perspective Settings</p>
<p>We investigate the model's ability to perform code translation by utilizing functors between two programming language categories.To isolate the influence of natural language, we exclude problem descriptions and code comments, forcing the models to focus solely on translating code from one programming language (PL) category to another.The input prompt may contain a request for translation using natural language (e.g., "translate the below Java code to Python code").</p>
<p>Implementation</p>
<p>Datasets We conduct evaluations on the HumanEval-X, MathQA, and MBXP datasets.The input category is Java, while the output categories are Python and JavaScript.</p>
<p>Models</p>
<p>We evaluate three models: Text-Davinci, ChatGPT, and CodeGeeX.For the first two models, we provide prompts that request the model to do the translation.However, for CodeGeeX, code translation is supported inherently, so we simply input the Java code without additional text.</p>
<p>Evaluation We extract the functions from the model's responses and assess their correctness using the Pass@1 rates of execution-based tests.</p>
<p>Results</p>
<p>The results are depicted in Figure 5.Among the three models, ChatGPT performs the best, exhibiting a slight advantage over Text-Davinci.With regards to the datasets, ChatGPT and Text-Davinci achieve nearly perfect translation accuracy on MathQA, whose data consists of functions with internal variable initialization, and without any input arguments, loops, or conditional statements.This indicates ChatGPT's and Text-Davinci's proficiency in reproducing exact numerical values and handling simple program structures.Code explanation and code generation tasks have been conducted separately in previous work.However, from a categorical perspective, we can make a combination.We define the explanation functor as the functor that maps an object from a PL category to an NL category, with a precise description of the code's function.We define the reproduction functor as the functor from an NL category to a PL category, that uses an explanation object in NL to reproduce the code of the same function in a PL category.If the reproduced code is functionally the same as the original code, we reckon the model preserves the functional equivalence after applying an explanation functor and then a generation functor.</p>
<p>Implementation</p>
<p>Datasets We conduct the evalution on three datasets: HumanEval-X, MathQA, and MBXP.For consistency, we select Java as the programming language (PL) category for all evaluations.</p>
<p>Models We evaluate Text-Davinci and ChatGPT, prompting each model twice.In the first prompt, we ask the models to describe the code using natural language, including the precise function name, arguments, and return type, and provide sufficient information to reproduce the code.In the second prompt, we provide the model's explanation and ask it to translate the description back into code.</p>
<p>Evaluation To evaluate the correctness of the reproduced code, we extract the functions from the model's second responses and assess them using the Pass@1 rates of execution-based tests.</p>
<p>Results</p>
<p>The results are depicted in Figure 5. On the same datasets and with the same two models, the average pass@1 rate is about 30% lower compared to that of the translation functor.This indicates the model has significant information loss in the explain-reproduce process.</p>
<p>Summary</p>
<p>Based on our experiments, we have found that both Text-Davinci and ChatGPT models are capable of identifying the impact of one or two local morphisms on code function.However, identifying global morphisms proves to be challenging for these models.In terms of code translation, both ChatGPT and Text-Davinci models outperform CodeGeex.They demonstrate accurate reproduction of basic syntax, variable names, and numerical values.Nevertheless, when dealing with more intricate program structures and aligning data types across different programming language categories, these models encounter obstacles that hinder more precise translation.It is important to note that maintaining functional equivalence between code and natural language presents a more formidable challenge compared to code translation, necessitating further research in this specific domain.</p>
<p>4 Related Work</p>
<p>Coding Abilities of Large Language Models</p>
<p>Large language models trained on code have demonstrated improvements in various coding tasks.</p>
<p>CodeBERT [8] was one of the initial models trained on code, while GraphCodeBERT [10] incorporated program Abstract Syntax Trees (AST) and data flow information to enhance code structure and context understanding.</p>
<p>The Encoder-Decoder architecture, used in models like CodeT5 [22] and PLBART [1], enables multi-task learning for code translation, understanding, and generation.PLBART [1], pretrained on Java and Python functions with natural language comments, excels in generating code from natural language descriptions.</p>
<p>More recently, the emergence of Codex [7] and ChatGPT4 has brought GPT-based models into the mainstream for content generation tasks, including coding.These models have shown impressive performance across various domains, but their proprietary nature, limited access to data, and lack of model checkpoints have made it challenging to thoroughly evaluate and understand their capabilities and limitations.Consequently, there is a need to develop a comprehensive framework to evaluate these black-box models.</p>
<p>Code Model Evaluation</p>
<p>There are four widely-used types of evaluation frameworks for code models: match-based, attackbased, task-based, and execution-based.</p>
<p>The match-based framework includes CodeBLEU [19] that adopts n-gram match as BLEU [16] and further injects Abstract Syntax Tree(AST) and data-flow similarity.However, match-based framework may consider different solutions with varying variable names, AST structures, and data flow as dissimilar, despite their functional equivalence.</p>
<p>The attack-based framework constructs adversarial program examples to assess the model's performance.Yefet et al. [23] generated adversarial code examples based on gradients, uncovering vulnerabilities to variable renaming and dead code injection.Similarly, Ramakrishnan et al. [11] and Rabin et al. [17] investigated the model's robustness and generalizability to semantic-preserving program transformations.However, those works primarily focused on white-box models, and the adversarial examples are not so commonly encountered in practice.Nevertheless, the idea of "functional equivalence" and "program transformations" in their framework is important for related research.</p>
<p>Task-based evaluations focus on the model's performance on specific coding tasks.CodeXGLUE [13] is a well-known task-based framework that employs diverse datasets for different coding tasks.It provides a very comprehensive evaluation framework.However, different tasks in it do not share enough common settings in data definition, task formulation, and test APIs.This hinders its use in a more scalable and easy-to-follow way.</p>
<p>The execution-based evaluation focuses on code generation and translation tasks, evaluating the correctness of the model-generated code based on test cases.MBXP [4] and MultiPL-E [6] are two execution-based benchmarks that support multiple programming languages.While our work also considers test cases from execution-based benchmarks, our focus extends beyond correctness evaluation to include code morphism and PL to NL functor aspects.</p>
<p>Limitations</p>
<p>While we strive to apply categorical perspectives to offer a comprehensive and standardized way of evaluation, we find that our current study has a number of limitations.First, more powerful concepts and properties in category theory haven't been fully utilized by our current study.Our experiments primarily focus on objects, morphisms, functors, and their combinations.However, we leave it for future work to further investigate the setting of composing many morphisms and many functors to further explore the commutative law and isomorphism properties of categories.Second, we use prompts to instruct the models to act as certain functors, but prompts that express similar meanings will lead to different results, we have not investigated which prompts are more suitable for the model or which models are more robust to different prompts.Moreover, due to limitations in computational resources and API accessibilities, we did not test more models.</p>
<p>Conclusion</p>
<p>Our contributions include introducing CatCode as a novel evaluation perspective based on category theory, which enables a comprehensive and mathematically abstract approach to evaluate LLMs that excel in understanding and generating a mixture of code and text.Based on categorical perspectives, we present a standardized automatic evaluation platform, which is adaptive to new datasets, tasks, and models.We evaluate competitive LLMs to provide valuable insights, and find out current models' deficiency in recognizing functionally equivalent code and preserving information of code function between the code and its explanation.We plan to open-source our platform, hoping to contribute to the comprehensive and standardized evaluation for LLMs by offering a categorical perspective to deal with the mixture of code and text.</p>
<p>We believe that CatCode represents an important step towards a more comprehensive and standardized evaluation of LLMs' abilities in solving coding problems.By combining the power of category theory with the growing capabilities of LLMs, we can unlock new possibilities for defining and evaluating more diverse code-related tasks.We hope that CatCode inspires further research and development in the field, leading to more sophisticated LLMs that can effectively assist developers in their coding tasks and contribute to advancements in artificial intelligence and software engineering as a whole.1: Baseline Models and their supported evaluation tasks.</p>
<p>Model Morphism Translation Functor Explanation Functor Reproduction Functor
Davinci ✓ ✓ ✓ ✓ ChatGPT ✓ ✓ ✓ ✓ CodeGeeX ✓ Table
• Due to the large size of the Code Contest dataset, only the test set is used.Considering that the maximum token length of Text-Davinci is 2048, we filter out solutions with a length greater than 500.This ensures that the code pair length remains ≤ 1000, allowing for additional tokens for prompt content and response.After filtering, 97 problems remain, and a problem may have multiple correct and incorrect solutions in different languages.We utilize the Java and Python splits of this dataset.</p>
<p>A.2 Model details</p>
<p>We utilize the following baseline models by making API calls to their official endpoints: Text-Davinci-003 (referred to as Davinci), ChatGPT, and CodeGeeX.The default hyperparameters of these models are used, with the exception of setting max_token to 500 when an early stop of the answer is observed.</p>
<p>Table 1 provides an overview of the tasks supported by each model.Since Text-Davinci-003 and ChatGPT support various types of text inputs and outputs, they are capable of performing all tasks.</p>
<p>On the other hand, the CodeGeeX API does not support natural language output, so we only employ it in translation functor experiments.</p>
<p>A.3 Evaluation Metrics</p>
<p>To assess the performance of the model, we conduct two types of tests: pairing test and executionbased test.</p>
<p>The pairing test is utilized for morphism identification.The task involves comparing two code snippets and requesting the model to explicitly answer "True" or "False", along with providing a comparison.For automatic evaluation, we extract the True" or "False" answer.The precision score is used to evaluate the model's ability to identify different equivalence classes.For equivalent objects, the precision is calculated as P recision(eq) = T P T P +F N , while for nonequivalent objects, the precision is calculated as P recision(neq) = T N T N +F P .The execution-based test is employed for both translation and reproduction experiments.We extract the functions from the model's responses and evaluate their correctness using the Pass@1 rates of execution-based tests.This test ensures that the translated/reproduced code produces the same expected results as the original code.</p>
<p>B Experimental Details B.1 Morphism Identification</p>
<p>Data statistics.Table 2 provides the statistics of the raw data, filtered data, and constructed pairs for the morphism identification experiment.The filtering strategy, described in Section A.1, involves removing comments and docstrings, resulting in filtered data consisting solely of Java code.The constructed pairs are generated through sampling.It is worth noting that certain morphisms can have multiple outputs for a given input.For instance, the Variable Renaming morphism can be applied to all variables in a function, resulting in multiple outputs equal to the number of variables.To balance the number of outputs across different morphism types, random sampling is employed when there are more than two applicable morphism types.For each code sample, two types of morphisms are first sampled, followed by sampling one output for each morphism type.3: Morphism types with the three worst average precision scores of each dataset.As an example for notations, BE-VR (66.67) stands for a 2-eq morphism of "Boolean Exchange" and "Variable Renaming" with a precision score of 66.67%.</p>
<p>Which morphism types are harder to be identified, and are they hard across datasets?We calculate the average precision scores for 1-eq, 2-eq and 1-neq local morphisms for ChatGPT answers, as shown in Table 3.</p>
<p>The following observations can be made:</p>
<p>• For one morphism: "Unused Statements", "Modify Condition" and "Boolean Exchange" are particularly difficult to identify.One possible reason for this difficulty is that these morphisms involve subtle changes or transformations in the code that may not be easily recognizable based on the surrounding context alone.It requires a more nuanced understanding of the code logic and structure to detect these morphisms</p>
<p>• For 2-eq morphisms: The observation that "Unused Statements" remains challenging when combined with other morphisms suggests that the presence of multiple morphisms in the code can further complicate the identification task.</p>
<p>• Dataset-specific difficulties: The results indicate that the difficulty of identifying "Unused Statements" persists across datasets, suggesting that this morphism type poses inherent challenges in code comprehension.On the other hand, the specific difficulty in identifying "Modify Condition" in the MathQA dataset may be attributed to the nature of the dataset itself, which focuses on math problem solvers.</p>
<p>B.2 Translation Functor</p>
<p>Data statistics.We use the filtered data in Table 2 as input, i.e. 159, 953, and 97 Java Snippets for HumanEval-X, MBXP, and MathQA, respectively.Each model generates a Python object and a JavaScript object for each Java object.During test phase, since the Mexeval dataset is missing 3 test cases in the Java split of HumanEval (TaskID: 32, 38, 50), we use 156, 953, and 97 Java functions to calculate the pass@1 rates for HumanEval-X, MBXP, and MathQA respectively.</p>
<p>Results statistics.The pass@1 score of different models on different datasets is shown in Table 4.</p>
<p>Overall, ChatGPT has a relatively higher translation ability.</p>
<p>Discussion of failure types.Table 5 represents the most frequent failure types of translation functor on three datasets.We observe that most errors are compilation errors rather than failures at corner test cases.</p>
<p>These errors are often caused by type mismatches between different programming languages.For instance, when translating a list in Java, there are multiple potential types in Python, including list, tuple, and List from the typing module.</p>
<p>The occurrence of "NameError" and "ReferenceError" indicates an unfaithful translation of variable names by the model or mismatched function input arguments across different languages for the same problem.This is a limitation inherent in the current evaluation setup.</p>
<p>C.1 Morphism Identification</p>
<p>Refer to Figure 6 and Figure 7.Those cases show the model can identify the literate differences between the code pairs but fails to clearly interpret their functional difference.</p>
<p>C.2 Translation Functor</p>
<p>According to the previous statistics, most translation errors happen in the compilation stage.We are also concerned about if there are no compile errors, when will the model make mistakes.Figure 8 shows such a case where some information about the numerical data type is missing when translation.</p>
<p>C.3 Explanation Functor and Reproduction Functor</p>
<p>Figure 9 shows a case where information loss and hallucination happen during explanation and reproduction.This is often the case when the model deals with the MathQA dataset.To improve the performance of the model, using few-shot prompts to illustrate how to explain the code may be useful.</p>
<p>Figure 1 :
1
Figure 1: The overall evaluation framework.We use category perspectives to reorganize and transform data, formulate different coding tasks, and conduct model evaluations.</p>
<p>Figure 2 :
2
Figure 2: Categorical framework for a mixture of code and NL.A, B and C represent different objects, A and A ′′ represent the equivalent object of A in other categories.</p>
<p>Figure 2 illustrates those concepts in a comprehensive way. Definition 2 . 1 .
21
(Category, Object, Morphism).A category C consists of a set of objects Ob(C) and a set of morphisms Hom C (a, b), from a to b for every a, b ∈ Ob(C).Given two morphisms f ∈ Hom C (a, b), g ∈ Hom C (b, c), we define their composition as g • f ∈ Hom C (a, c).The composition of morphisms is governed by two axioms:</p>
<p>Definition 2 . 2 .
22
(Functor).A functor F from a category C to a category D, written as F : C → D, maps an object x ∈ Ob(C) to F (x) ∈ Ob(D); as well as a morphism f : x → y in Hom(C) to F (f ) : F (x) → F (y) in D, such that the following two properties hold:</p>
<p>Related tasks: code detection, defect detection, code repair) → Categorical perspective: Can the model identify the self-morphisms and other morphisms within the same code category?RQ2: Can the model translate code between different programming languages?(Related tasks: code translation) → Categorical perspective: Can the model accurately perform code translation functor?</p>
<p>Figure 4 :
4
Figure 4: Morphism Identification Experiment."1", "2" and "global" stands for the distance of the code."Eq"and "neq" indicates whether the morphism is self-morphism.(Left) An illustration of morphisms and the definition of object distance.(Right) Comparison of Text-Davinci and ChatGPT for morphism identification.</p>
<p>Figure 5 :
5
Figure 5: Comparison of model performance.(Left) Model as a translation functor.(Right) Model as the combination of explanation functor and reproduction functor</p>
<p>Table 2 :
2
Dataset statistics for morphism identification experiment.
Discussion of different morphism types.</p>
<p>Table 6 :
6
Pass@1 score (%) statistics of the reproduced code, corresponding to the Figure5(Right).
ModelHumanEval MathQA MBXP Avg.TextDavinci33.075.555.254.6ChatGPT35.969.157.154.0ModelHumanEvalMathQAMBXPCannotFindSymbol (47)Test Failure (350)Test Failure (181)TextDavinciTypeError (38)UninitializedVariable (24)CannotFindSymbol (164)Test Failure (22)CannotFindSymbol (18)TypeError (27)CannotFindSymbol (46)Test Failure (401)CannotFindSymbol (137)ChatGPTTypeError (39)DuplicateDeclaration (60) Test Failure (126)Test Failure (17)SyntaxError (44)UnexpectedEOF (70)</p>
<p>Table 7 :
7
Most frequent failure types of the reproduced code.The data in the table cell is written as ErrorType(frequency). Specifically, the "Test Failure" type represents the code that complies successfully but fails to pass some of the test cases.Other types of errors in this table are primarily caused by compilation errors.</p>
<p>We can also extend the definition of the object to be the set of code with not only the same outcome, but also with the same time or space complexity, but due to the difficulties of automatically evaluating the complexity, this can be extended for future work.
Transformation samples can refer to: https://github.com/mdrafiqulrabin/tnpa-framework .
https://openai.com/blog/chatgpt/
https://huggingface.co/datasets/THUDM/humaneval-x
Code translation Docstring translationCode detection Defect detectionCode repairCode explanation Code summarizationOriginal Tasks Categorical PerspectiveGeneration functor Reproduction functorCode generation Code reproduction model as translation functorCode Objects We conduct the experiments within the PL category and define "the function with the same running outcome for every valid input" as the same object.Code MorphismThe general category perspectives only define morphisms between two code snippets, but morphisms do not have distance information.Considering fine-grained evaluation, we are curious about the model's ability to modify or debug the code, which corresponds to the local scale; and to write an equivalent new solution to a coding problem, which is of a global scale.AppendixThe appendix is divided into three main sections, each focusing on distinct aspects of the study.These sections are labeled as follows:A. Implementation Details: This section covers the implementation process, including the anonymous link to our code, dataset details, data filtering, model specifications, and evaluation metrics, providing information on how to reproduce based on our standard evaluation pipeline.B. Experimental Details: This part introduces the experimental settings and more experimental results in detail.We further discuss the model's performance on different morphism types in the identification experiment, and the common error types in the translation and reproduction experiments.C. Case Study: This section offers some intuitive examples of the experiments.Through analysis of those examples, we can find some limitations of current models, prompts, and test scripts.A Implementation DetailsWe have uploaded the code to the anonymous website: https://anonymous.4open.science/r/CatCode-6402/.A README.md file is provided, offering step-by-step instructions on how to use the evaluation platform and replicate the entire evaluation process in a standardized way.In this section, we will provide additional details regarding the reproduction process, including the dataset split, data filtering, models used, and evaluation metrics.A.1 Dataset DetailsThe following datasets were used in our evaluation:• HumanEval-X 5 : a human-crafted dataset with 164 problems, correct solutions in 5 different languages, and test cases.It is a benchmark for evaluating code generation and translation models.We use the Java split of HumanEval-X as input for applying morphism, code translation, and code explanation.Although Mxeval provide multilingual solutions, they are generated by models and do not make a distinction between correct and wrong solutions.In contrast, HumanEval-X provides correct solutions, making it a more suitable complement.• MBXP: a benchmark for evaluating code generation models.It covers over 10 programming languages and is generated using a scalable conversion framework that transpiles prompts and test cases from the original Python datasets into the corresponding data in the target language.It's a dataset similar to HumanEval-X but does not ensure the correctness of the transpiled code.• MathQA: a dataset for evaluating math problem solvers.All functions in this dataset have no input argument and return a number.The functions first initialize variables, then do some calculations, and do not use any loops or conditional statements.Compared to HumanEval-X and MBXP, the code in MathQA is simpler in structure and functionality.• Code Contest: a large-scale competitive multi-language programming dataset used for training AlphaCode[12].It consists of programming problems, test cases in the form of paired inputs and outputs, and both correct and incorrect solutions.Since the aforementioned datasets do not provide multiple solutions within the same programming language, Code Contest serves as a valuable complement by offering multiple solutions to the same problem, representing global equivalence and non-equivalence.Data filtering• Only the Java split of HumanEval-X, MBXP, and MathQA datasets are used.The local morphism is applied by JavaTransformer by AST edits at the function level.To use JavaTransformer, code must compile successfully and contain a single function.We filter out code in the Mxeval datasets that are generated by the model and cannot compile or contain multiple functions.Table4: Translation pass@1 score (%) statistics, corresponding to the Figure5(Left).The models translate Java code to two languages Python (Py) and JavaScript (JS), on three datasets.The average (Avg.) is the average score over the six scores.Java-&gt;Py Java-&gt;JS Java-&gt;Py Java-&gt;JS Java-&gt;Py Java-&gt;JS Unknown(10)ReferenceErr.(24Overall, the models have demonstrated the ability to perform correct translations in most cases between different programming languages.The errors mainly arise from type alignment issues between programming languages and local variable definitions.These errors are typically easy to debug for programmers if provided with relevant information.Future work could involve passing error messages to the model and allowing it to debug itself.B.3 Explanation and Reproduction FunctorData statistics.Same as Appendix B.2, we use 156, 953, and 97 Java functions to calculate the pass@1 rates for HumanEval-X, MBXP, and MathQA separately.Results statistics.Table6shows the pass@1 rates of different models.TextDavinci is better at MathQA, while ChatGPT is better at HumanEval and MBXP datasets.The pass@1 score is significantly lower compared to translation functor experiments, indicating that it's a more difficult task.Discussion of failure types.Table7shows the most frequent failure types of translation functor on the three datasets.Based on the results, it appears that both models encounter similar types of failures across the different evaluation tasks.The "Test Failure" type is particularly prominent, indicating that the reproduced code, although compiling successfully, fails to pass some of the test cases.Additionally, both models also encounter errors related to symbol identification, such as "CannotFindSymbol" and "TypeError".These errors are primarily compilation errors.It's worth further investigating the causes behind these failure types and exploring strategies to improve the models' performance, especially in handling test cases and resolving symbol-related errors.C Case StudyIn this Section, we analyze some classical cases from our three experiments.Prompt Text: Do the two codes below have the same output for each input?In your first sentence, answer "True" or "False".Then points out all the differences of the two codes.HumanEval/154LoopExchange-ReorderConditionChatGPT ResponseFalse.Differences:-In code A, the while loop is used to iterate through the <code>l+1</code>possible subpatterns of <code>a</code>that have length <code>l</code>.In code B, the for loop is used instead.-In code B, the condition for the for loop is <code>j &lt;= l</code>, which means it checks if <code>j</code>is less than or equal to <code>l</code>.This will result in the last possible subpattern being ignored because <code>j</code>will stop at <code>l</code>and not check anything after it.Therefore, the correct condition should be <code>j &lt; l</code>.-In code B, the condition for comparing the substrings is <code>if (a.substring(i, l + i).equals(pat.substring(j,j + l)))</code>.This checks if the substring of <code>a</code>starting from index <code>i</code>with a length of <code>l</code>equals the corresponding substring of <code>pat</code>.In code A, the condition is similar, except it uses <code>a.substring(i, i + l)</code>instead.Both conditions are correct, but they differ slightly in their expression.HumanEval/129 LoopExchange-ReorderConditionChatGPT ResponseFalse.The codes have differences in their inner loops and conditions inside the loops.Code A uses a while loop to iterate through the columns of each row, whereas Code B uses two for loops to iterate through both rows and columns of the grid.Code A also checks whether a cell contains a 1 before checking its neighbors, while Code B simply iterates through every cell regardless of its value.Finally, there is an error in Code B where it checks the neighbor to the bottom incorrectly: it should use <code>i + 1</code>instead of <code>1 + i</code>.Figure6: Two failed cases of ChatGPT in the morphism identification experiment.For "Hu-manEval/154", ChatGPT fails to interpret the loop execution condition correctly.For "Hu-manEval/129" ChatGPT reckon equivalent math expression as different.To improve the model's reasoning ability, a possible way is to utilize the method of Chain of Thoughts(CoT) to construct better prompts and give the model some time to think before making a decision.Prompt Text:Do the two codes below have the same output for each input?In your first sentence, answer "True" or "False".Then points out all the differences of the two codes.HumanEval/94BooleanExchange-VariableRenamingfor (int j = 2; j &lt; Math.sqrt(i)for (int j = 2; j &lt; Math.sqrt(i)ChatGPT ResponseFalse.Differences:-In code A, the <code>isPrime</code>boolean variable is assigned to <code>!(i != 1)</code>which is equivalent to <code>i == 1</code>.Whereas in code B, it is assigned to <code>i != 1</code>.-In code A, the condition for updating the <code>maxx</code>variable is <code>if (!(isPrime))</code>which means if the number is not prime.Whereas in code B, it is <code>if (isPrime)</code>which means if the number is prime.-...-Inside the inner loop of the <code>isPrime</code>check, code A sets <code>isPrime</code>to <code>true</code>if a factor of <code>i</code>is found and breaks out of the loop.This is incorrect since it sets <code>isPrime</code>to true even when a factor is found, whereas it should set it to false because <code>ih as factors other than 1 and itself.Code B correctly sets</code>isPrime<code>to</code>false`in this case.-...  Text-Davinci fails to consider data type conversions during translation.This issue becomes evident when examining the three test cases provided in Mxeval, as the execution encounters an error in the third case.However, if it were not for the incorrect test scripts generated by the model in Mxeval, the translated code should have been identified as incorrect in the first case.This particular case highlights the importance of enhancing the correctness of model-generated execution test scripts for a more accurate test of code.Prompt Text: (Explanation task) Please describe the following code written in ```using natural language.Your description should include the exact function name, its arguments, and the return type.Additionally, provide enough details such as variable initialization to allow someone to reproduce the code based on your explanation.Explanation (by Text-Davinci)This is a public static method called 'problem()' that does not take any arguments and returns a double value.The method initializes the variables n0 to n10 with double values and assigns them to the specified numbers.It then uses the Math.pow()method to use the variables n9 and n10 as the base for the power and the Math.min()method to use the variables n3 or n0 as the second parameter.It then assigns these two values, t0 and t1, as addends of a sum and returns the answer, a double value.Execution ResultsException --test case 0 did not pass.
Unified pretraining for program understanding and generation. Saikat Wasi Uddin Ahmad, Baishakhi Chakraborty, Kai-Wei Ray, Luke Chang, Dilek Zettlemoyer, Iz Hakkani-Tür, Steven Beltagy, Ryan Bethard, Tanmoy Cotterell, Yichao Chakraborty, Zhou, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. Kristina Toutanova, Anna Rumshisky, the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021Association for Computational LinguisticsJune 6-11, 2021. 2021</p>
<p>Category theory for programming. Benedikt Ahrens, Kobe Wullaert, CoRR, abs/2209.012592022</p>
<p>Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, Mathqa, arXiv:1905.13319Towards interpretable math word problem solving with operation-based formalisms. 2019arXiv preprint</p>
<p>Multi-lingual evaluation of code generation models. Ben Athiwaratkun, Krishna Sanjay, Zijian Gouda, Xiaopeng Wang, Yuchen Li, Ming Tian, Tan, Uddin Wasi, Shiqi Ahmad, Qing Wang, Mingyue Sun, Sujan Shang, Hantian Kumar Gonugondla, Varun Ding, Nathan Kumar, Arash Fulton, Siddhartha Farahani, Robert Jain, Haifeng Giaquinto, Qian, Krishna Murali, Ramesh Ramanathan, Baishakhi Nallapati, Parminder Ray, Sudipta Bhatia, Dan Sengupta, Bing Roth, Xiang, CoRR, abs/2210.148682022</p>
<p>Tai-Danae Bradley, arXiv:1809.05923What is applied category theory?. 2018arXiv preprint</p>
<p>Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, Arjun Guha, Michael Greenberg, Abhinav Jangda, MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation. IEEE Transactions on Software Engineering. 2023</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Felipe Petroski Such. Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech ZarembaJan Leike. 2021Evaluating large language models trained on code. CoRR, abs/2107.03374</p>
<p>Codebert: A pre-trained model for programming and natural languages. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, arXiv:2002.081552020arXiv preprint</p>
<p>Seven sketches in compositionality: An invitation to applied category theory. Brendan Fong, David I Spivak, arXiv:1803.053162018arXiv preprint</p>
<p>Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, arXiv:2009.08366Pre-training code representations with data flow. 2020arXiv preprint</p>
<p>Semantic robustness of models of source code. Jordan Henkel, Goutham Ramakrishnan, Zi Wang, Aws Albarghouthi, Somesh Jha, Thomas W Reps, IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2022. Honolulu, HI, USAIEEEMarch 15-18, 2022. 2022</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien De Masson D'autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022378</p>
<p>Codexglue: A machine learning benchmark dataset for code understanding and generation. Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, arXiv:2102.046642021arXiv preprint</p>
<p>Generating diverse code explanations using the GPT-3 large language model. Stephen Macneil, Andrew Tran, Dan Mogil, Seth Bernstein, Erin Ross, Ziheng Huang, ICER 2022: ACM Conference on International Computing Education Research, Lugano and Virtual Event Switzerland. Jan Vahrenhold, Kathi Fisler, Matthias Hauswirth, Diana Franklin, ACMAugust 7 -11, 202222022</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe, NeurIPS. 2022</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, PA, USAACLJuly 6-12, 2002. 2002</p>
<p>On the generalizability of neural program models with respect to semantic-preserving program transformations. Rafiqul Md, Islam Rabin, D Q Nghi, Ke Bui, Yijun Wang, Lingxiao Yu, Mohammad Jiang, Alipour Amin, Inf. Softw. Technol. 1351065522021</p>
<p>Testing neural programs. CoRR, abs. Rafiqul Md, Ke Islam Rabin, Mohammad Wang, Alipour Amin, 1908.10711, 2019</p>
<p>Codebleu: a method for automatic evaluation of code synthesis. Daya Shuo Ren, Shuai Guo, Long Lu, Shujie Zhou, Duyu Liu, Neel Tang, Ming Sundaresan, Ambrosio Zhou, Shuai Blanco, Ma, CoRR, abs/2009.102972020</p>
<p>Category Theory for the Sciences. David I Spivak, 2014MIT Press</p>
<p>Intellicode compose: code generation using transformer. Alexey Svyatkovskiy, Shengyu Shao Kun Deng, Neel Fu, Sundaresan, ESEC/FSE '20: 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. Prem Devanbu, Myra B Cohen, Thomas Zimmermann, USAACMNovember 8-13, 2020. 2020</p>
<p>Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. Yue Wang, Weishi Wang, Shafiq R Joty, Steven C H Hoi, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, Scott , Wen-Tau Yih, the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta CanaDominican RepublicAssociation for Computational Linguistics7-11 November, 2021. 2021</p>
<p>Adversarial examples for models of code. Noam Yefet, Uri Alon, Eran Yahav, 162:1-162:30Proc. ACM Program. Lang. 42020</p>
<p>Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, Jie Tang, 2023</p>
<p>Multilingual code snippets training for program translation. Ming Zhu, Karthik Suresh, Chandan K Reddy, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236Java-&gt;Py Java-&gt;JS Java-&gt;Py Java-&gt;JS Java-&gt;Py Java-&gt;JS</p>
<p>. HumanEval MathQA MBXP Model Avg. </p>            </div>
        </div>

    </div>
</body>
</html>
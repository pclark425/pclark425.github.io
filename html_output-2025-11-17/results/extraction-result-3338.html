<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3338 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3338</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3338</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-3950df97ea527009a32569cb7016bc3df1383dca</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3950df97ea527009a32569cb7016bc3df1383dca" target="_blank">QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a new model, QA-GNN, which addresses the problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) through two key innovations: relevance scoring and joint reasoning.</p>
                <p><strong>Paper Abstract:</strong> The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. Here we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph-based message passing. We evaluate QA-GNN on the CommonsenseQA and OpenBookQA datasets, and show its improvement over existing LM and LM+KG models, as well as its capability to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3338.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3338.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QA-GNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Question Answering with Graph Neural Networks (QA-GNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end LM+KG model that (1) scores relevance of KG nodes conditioned on the QA context using a pretrained LM and (2) performs joint reasoning by adding a QA-context node to the KG and running an attention-based GNN that updates LM and KG representations jointly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QA-GNN (RoBERTa + GNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines a pretrained masked language model encoder (RoBERTa) to encode the QA context and to compute per-KG-node relevance scores, with an attention-based multi-layer graph neural network operating on a 'working graph' that connects the QA context node to topic KG entities; message and attention computations are node-type, relation-type, and relevance-score aware.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>~360M (system size reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Language-conditioned KG node relevance scoring (retrieval weighting/pruning)', 'Joint graph reasoning (working graph with QA context node)', 'Attention-based GNN message passing (node-type, relation-aware, score-aware attention)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Relevance scoring: concatenate QA context text and entity text, compute likelihood via LM head to produce a relevance scalar per KG node used as a node feature and for pruning; Joint reasoning: add a QA-context node z connected to topic entities and run multi-layer GNN message passing so QA-context and KG nodes update each other; GNN: attention weights computed from queries/keys that include node embeddings, type embeddings, relation embeddings, and the embedded relevance score; messages incorporate relation and type embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse — integrates multiple distinct strategies (LM-based contextual relevance estimation + structured graph reasoning via multi-aspect attention GNN) rather than repeating the same style of KG-only reasoning; diversity is implemented in architecture (LM + relevance scoring + GNN) and validated by ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA; OpenBookQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>CommonsenseQA: 5-way multiple-choice QA requiring commonsense knowledge; OpenBookQA: 4-way multiple-choice QA requiring elementary science facts (both use ConceptNet subgraphs as KG).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>CommonsenseQA (in-house dev/test; RoBERTa-large backbone): QA-GNN IHdev 76.54 (±0.21), IHtest 73.41 (±0.92). Official test: 76.1. OpenBookQA (controlled experiments): RoBERTa-large backbone: QA-GNN 70.58 (±1.42); with AristoRoBERTa backbone: 82.77 (±1.56). Leaderboard OpenBookQA with AristoRoBERTa+QA-GNN: 82.8.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>QA-GNN vs baselines: On OpenBookQA QA-GNN is +5.7% over RoBERTa and +3.7% over prior best LM+KG (MHGRN) (reported in paper). On CommonsenseQA in-house splits QA-GNN improves over fine-tuned RoBERTa and over other LM+KG methods (e.g., MHGRN). Ablations show (1) adding edges connecting QA-context node to QA entity nodes is important (performance drops from 76.5% to 74.8% when removed), (2) relevance scoring gives a boost (75.56% → 76.54%), (3) including node-type, relation, and score in attention/message functions each improves performance, and (4) relevance scoring particularly helps large/noisy retrieved KGs (questions with >10 entities).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Joint updates between LM representation (QA context) and KG via a working graph plus LM-conditioned relevance scoring produce better structured reasoning (notably negation handling) and higher accuracy than (a) LM-only baselines and (b) prior LM+KG methods that keep LM and KG separate; relevance scoring reduces noise from large retrieved subgraphs and improves performance on examples with many entities.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Variants that connected the QA-context node to all KG nodes (instead of only topic entities) did not help and slightly degraded performance (-0.16%). Using contextual LM embeddings of nodes instead of just the scalar relevance score performed slightly worse (76.31% vs 76.54%). Removing the joint edges between z and KG nodes reduces gains on negation-handling (performance becomes close to MHGRN). The model still fails on some double-negation examples (noted as future work).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3338.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3338.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-large (fine-tuned LM baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-large (fine-tuned language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strong masked-language-model baseline used as the LM encoder and baseline QA model; used both standalone (fine-tuned on QA) and as the backbone LM in LM+KG systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RoBERTa: A robustly optimized bert pretraining approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A masked language model (BERT-style Transformer) pretraining recipe (Robustly optimized BERT); used as encoder f_enc producing [CLS] representations that are fine-tuned for QA tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Pure LM-based contextualized representation and classification (no external KG)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Fine-tune LM on (question, answer choice) pairs and predict answer via classification head using LM-encoded QA-context ([CLS] token) representation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single/similar style — uses only LM internal knowledge and standard fine-tuning (no explicit structured reasoning or KG augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA; OpenBookQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same as above; LM-only baseline for comparison with LM+KG methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>CommonsenseQA (IHdev/IHtest): 73.07 (±0.45) / 68.69 (±0.56). OpenBookQA (RoBERTa-large): 64.80 (±2.37). On negation subset (CommonsenseQA IHtest): overall 68.7, negation-containing questions 54.2.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Serves as ablation baseline: QA-GNN improves substantially over fine-tuned RoBERTa (e.g., +~5.7% on OpenBookQA and +~4.7% overall on CommonsenseQA IHtest in some comparisons). LM-only models perform poorly at structured reasoning such as negation handling.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuned LMs have broad coverage but underperform on structured reasoning (e.g., negation) compared to models that incorporate KG reasoning; LM+KG methods can improve accuracy, but improvements depend on how LM and KG are integrated.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>LM-only predictions often fail to adapt to small structural changes like negation or entity swaps; in case studies RoBERTa's predictions sometimes do not change when negation or topic entities are modified (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3338.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3338.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MHGRN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scalable Multi-Hop Graph Relation Network (MHGRN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior LM+KG method that models multi-hop relational reasoning in KGs and is a high-performing LM+KG baseline compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scalable multi-hop relational reasoning for knowledge-aware question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MHGRN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multi-hop relational GNN approach that explicitly models relations and multi-hop paths in the KG for knowledge-aware QA; used in prior work as the top LM+KG baseline and reproduced here with the same LM backbone for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['KG path/multi-hop relational reasoning via dedicated GNN architecture']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>MHGRN runs relational message passing to capture multi-hop relational paths in subgraphs retrieved from the KG and combines KG-derived features with LM representations (but does not jointly update the LM representation via a working-graph QA node).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar style — focuses on structured KG multi-hop path reasoning; keeps KG and LM modules relatively modular (separate updates) rather than joint unified updates.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA; OpenBookQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same datasets; MHGRN is used as a comparative LM+KG pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>CommonsenseQA (controlled experiments): MHGRN IHdev 74.45 (±0.10), IHtest 71.11 (±0.81). OpenBookQA (RoBERTa-large): MHGRN 66.85 (±1.19); with AristoRoBERTa: 80.6 (reported). On negation subset (CommonsenseQA IHtest): MHGRN 54.8 on negation questions (only +0.6% over RoBERTa).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>QA-GNN outperforms MHGRN in both datasets; ablations indicate that the joint updating of LM and KG (working graph) and relevance scoring in QA-GNN likely account for improvements vs MHGRN which treats LM and KG separately.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MHGRN provides improvements over simple LM baselines but lags behind QA-GNN; the separation of LM and KG representations may limit structured reasoning gains (e.g., negation handling).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>MHGRN gives limited improvement on negation-containing questions (+0.6% over RoBERTa) and struggles more with large/noisy retrieved KGs compared to QA-GNN with relevance scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3338.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3338.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KagNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KagNet: Knowledge-aware Graph Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior LM+KG method that models knowledge graph paths for commonsense reasoning and question answering; used as a baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Kagnet: Knowledge-aware graph networks for commonsense reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>KagNet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Builds graph-aware path encodings to combine KG paths with LM embeddings for QA, focusing on enumerating and modeling paths between topic entities in the retrieved subgraph.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Path-based KG reasoning (explicit path enumeration and encoding) combined with LM features']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Enumerate k-hop paths between topic entities in the KG and encode these paths to inform answer prediction in combination with LM features; KG and LM treated as separate modules.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar style — relies on KG path modeling; does not perform joint LM+KG updates or LM-conditioned node scoring as QA-GNN does.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Commonsense multiple-choice QA requiring leveraging commonsense KG paths.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>CommonsenseQA (controlled experiments): KagNet IHdev 73.47 (±0.22), IHtest 69.01 (±0.76). On negation subset it provided no improvement over RoBERTa (54.2 -> 54.2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>QA-GNN outperforms KagNet; KagNet's path-enumeration approach can introduce irrelevant nodes/paths when subgraphs are large, whereas QA-GNN's relevance scoring mitigates noisy nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Path enumeration alone (as in KagNet) yields limited gains on structured reasoning and is sensitive to noisy/large retrieved subgraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>KagNet gives little-to-no improvement on negation examples and can be hurt by large subgraphs due to many irrelevant nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3338.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3338.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RN (Relation Network)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relation Network (RN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A relation-aware neural module used as a KG-reasoning component in LM+KG pipelines, evaluated here as a baseline replacement for QA-GNN's GNN.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A simple neural network module for relational reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Relation Network (RN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A neural module designed to compute pairwise interactions (relations) between objects/entities and aggregate them for relational reasoning; here used on KG nodes with LM features.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Pairwise relational reasoning via pairwise interaction networks']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Compute pairwise (or small-group) relations between node representations and aggregate to form reasoning signals, without the explicit multi-layer relational GNN design of QA-GNN.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single/similar style — relation-focused, pairwise interactions; does not integrate LM-conditioned KG pruning or joint LM+KG updates.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA; OpenBookQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used as an alternative KG reasoning module plugged into the LM+KG framework for QA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>CommonsenseQA (controlled experiments): RN IHdev 74.57 (±0.91), IHtest 69.08 (±0.21). OpenBookQA (RoBERTa-large): RN 65.20 (±1.18); with AristoRoBERTa 75.35 (±1.39).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>RN provides moderate gains over LM-only in some settings but is outperformed by QA-GNN which uses richer attention/message mechanisms and LM-conditioned scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simple relation networks help but are less effective than QA-GNN's integrated joint reasoning and relevance scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>RN did not match QA-GNN performance; lacks mechanisms for LM-conditioned KG node scoring and joint updates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3338.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3338.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RGCN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relational Graph Convolutional Network (RGCN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph convolutional network variant that models multi-relational edges; evaluated as a KG reasoning baseline in LM+KG comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Modeling relational data with graph convolutional networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RGCN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GNN variant that uses relation-specific transformations (one per relation type) to propagate information across multi-relational graphs; used here as an off-the-shelf KG reasoning module.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Relation-specific graph convolutional message passing']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Apply relation-aware convolutional propagation where messages are transformed by relation-specific weights; combined with LM features but LM and KG representations are not jointly updated via a working-graph QA node in the baseline configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar style — KG-structure-centric reasoning with per-relation transforms; not integrated with LM-conditioned node scoring in the baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA; OpenBookQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used as baseline KG reasoning module plugged into the LM+KG framework.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>CommonsenseQA (controlled experiments): + RGCN IHdev 72.69 (±0.19), IHtest 68.41 (±0.66). OpenBookQA (RoBERTa-large): + RGCN 62.45 (±1.57); with AristoRoBERTa 74.60 (±2.53).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>RGCN as a baseline is outperformed by QA-GNN; QA-GNN avoids per-relation network duplication and adds LM-conditioned relevance scoring and joint updates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Basic relation-aware GNNs provide limited gains compared to more integrated LM+KG reasoning methods.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>RGCN sometimes underperforms the LM-only baseline in certain runs (per reported numbers), indicating integration choices matter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3338.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3338.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GconAttn</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-conditioned Attention (GconAttn)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior LM+KG method that uses graph-conditioned attention mechanisms for KG-aware reasoning; included as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving natural language inference using external knowledge in the science questions domain</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GconAttn</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applies attention mechanisms conditioned on KG structure to incorporate external knowledge into the LM pipeline for QA/NLI-style tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Graph-conditioned attention to integrate KG information with LM features']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Compute attention-weighted combinations of KG node signals conditioned on the QA context and LM features, but does not implement the QA-context-as-node joint GNN update nor LM-conditioned node relevance scoring as QA-GNN does.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar style — attention-based KG integration but less unified/joint than QA-GNN.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA; OpenBookQA (science NLI cited)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used as a baseline KG-attention integration approach for knowledge-aware QA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>CommonsenseQA (controlled experiments): + GconAttn IHdev 72.61 (±0.39), IHtest 68.59 (±0.96). OpenBookQA (RoBERTa-large): + GconAttn 64.75 (±1.48); with AristoRoBERTa 71.80 (±1.21).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>GconAttn yields smaller gains than QA-GNN; QA-GNN's joint graph and relevance scoring produce larger improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Graph-conditioned attention is helpful but benefits from being part of a joint LM+KG update and LM-conditioned node selection.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>GconAttn underperforms QA-GNN and in some setups underperforms simpler baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3338.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3338.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UnifiedQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UnifiedQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large unified question-answering system trained in a text-to-text manner and reported on leaderboards; mentioned as a high-performing but much larger alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unifiedqa: Crossing format boundaries with a single qa system</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UnifiedQA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A text-to-text QA model (variants up to 11B parameters) trained across multiple QA formats/datasets; mentioned for leaderboard comparison as outperforming the presented system but at much larger scale.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>up to 11B (11B variant cited in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Large-scale pretraining / multi-task QA finetuning (text-to-text transformer approach)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>UnifiedQA reframes diverse QA formats into a text-to-text problem and fine-tunes large T5-like models across QA datasets; reasoning emerges from large-scale pretraining/fine-tuning rather than explicit LM+KG joint structured reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single large LM style (scale and multi-task training rather than explicit KG-based diverse methods); not comparable architecture-wise to QA-GNN's LM+KG joint reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA; OpenBookQA (leaderboard comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Leaderboard competitor using large-scale text-to-text fine-tuned model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>CommonsenseQA official test: UnifiedQA reported 79.1 (noted in paper); OpenBookQA leaderboard: UnifiedQA 87.2 (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>UnifiedQA achieves higher leaderboard scores but is substantially larger (11B parameters) and trained on more data; authors note QA-GNN is competitive when excluding these much larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large-scale, text-only models can achieve very strong QA performance, but QA-GNN demonstrates that integrating explicit KG structure and LM-conditioned KG selection yields competitive performance with far fewer parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>UnifiedQA's higher performance attributed to much larger parameter counts and more training data, so direct architectural comparisons are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3338.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3338.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 (Text-to-Text Transfer Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large text-to-text Transformer model cited as strong leaderboard performer; mentioned for scale comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the limits of transfer learning with a unified text-to-text transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A text-to-text encoder-decoder Transformer family with variants up to 11B parameters; trained on large corpora and fine-tuned for downstream tasks, cited here as a top performer on OpenBookQA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B and larger variants mentioned (paper cites 3B variant performance)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Large-scale pretraining with text-to-text fine-tuning, implicit reasoning from scale']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Reasoning achieved implicitly via model capacity and large-scale multi-task pretraining, not via explicit KG integration.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single dominant style (large LM) rather than explicit heterogeneous reasoning modules.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OpenBookQA (leaderboard)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Leaderboard competitor using T5 variants; cited for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>OpenBookQA leaderboard: T5* reported 83.2 (paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>T5 outperforms QA-GNN on leaderboard, but uses much larger models and more data; QA-GNN focuses on explicit KG integration to achieve strong performance at smaller scale.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large text-only models can outperform smaller LM+KG hybrids, but explicit KG integration yields good performance with smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Scaling alone (T5, UnifiedQA) can beat QA-GNN, highlighting tradeoffs between model size/scale and structured KG integration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Kagnet: Knowledge-aware graph networks for commonsense reasoning <em>(Rating: 2)</em></li>
                <li>Scalable multi-hop relational reasoning for knowledge-aware question answering <em>(Rating: 2)</em></li>
                <li>A simple neural network module for relational reasoning <em>(Rating: 1)</em></li>
                <li>Modeling relational data with graph convolutional networks <em>(Rating: 1)</em></li>
                <li>RoBERTa: A robustly optimized bert pretraining approach <em>(Rating: 1)</em></li>
                <li>Unifiedqa: Crossing format boundaries with a single qa system <em>(Rating: 1)</em></li>
                <li>Exploring the limits of transfer learning with a unified text-to-text transformer <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3338",
    "paper_id": "paper-3950df97ea527009a32569cb7016bc3df1383dca",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "QA-GNN",
            "name_full": "Question Answering with Graph Neural Networks (QA-GNN)",
            "brief_description": "An end-to-end LM+KG model that (1) scores relevance of KG nodes conditioned on the QA context using a pretrained LM and (2) performs joint reasoning by adding a QA-context node to the KG and running an attention-based GNN that updates LM and KG representations jointly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "QA-GNN (RoBERTa + GNN)",
            "model_description": "Combines a pretrained masked language model encoder (RoBERTa) to encode the QA context and to compute per-KG-node relevance scores, with an attention-based multi-layer graph neural network operating on a 'working graph' that connects the QA context node to topic KG entities; message and attention computations are node-type, relation-type, and relevance-score aware.",
            "model_size": "~360M (system size reported in paper)",
            "reasoning_methods": [
                "Language-conditioned KG node relevance scoring (retrieval weighting/pruning)",
                "Joint graph reasoning (working graph with QA context node)",
                "Attention-based GNN message passing (node-type, relation-aware, score-aware attention)"
            ],
            "reasoning_methods_description": "Relevance scoring: concatenate QA context text and entity text, compute likelihood via LM head to produce a relevance scalar per KG node used as a node feature and for pruning; Joint reasoning: add a QA-context node z connected to topic entities and run multi-layer GNN message passing so QA-context and KG nodes update each other; GNN: attention weights computed from queries/keys that include node embeddings, type embeddings, relation embeddings, and the embedded relevance score; messages incorporate relation and type embeddings.",
            "diversity_of_methods": "Diverse — integrates multiple distinct strategies (LM-based contextual relevance estimation + structured graph reasoning via multi-aspect attention GNN) rather than repeating the same style of KG-only reasoning; diversity is implemented in architecture (LM + relevance scoring + GNN) and validated by ablations.",
            "reasoning_task_name": "CommonsenseQA; OpenBookQA",
            "reasoning_task_description": "CommonsenseQA: 5-way multiple-choice QA requiring commonsense knowledge; OpenBookQA: 4-way multiple-choice QA requiring elementary science facts (both use ConceptNet subgraphs as KG).",
            "performance_by_method": "CommonsenseQA (in-house dev/test; RoBERTa-large backbone): QA-GNN IHdev 76.54 (±0.21), IHtest 73.41 (±0.92). Official test: 76.1. OpenBookQA (controlled experiments): RoBERTa-large backbone: QA-GNN 70.58 (±1.42); with AristoRoBERTa backbone: 82.77 (±1.56). Leaderboard OpenBookQA with AristoRoBERTa+QA-GNN: 82.8.",
            "comparison_of_methods": "QA-GNN vs baselines: On OpenBookQA QA-GNN is +5.7% over RoBERTa and +3.7% over prior best LM+KG (MHGRN) (reported in paper). On CommonsenseQA in-house splits QA-GNN improves over fine-tuned RoBERTa and over other LM+KG methods (e.g., MHGRN). Ablations show (1) adding edges connecting QA-context node to QA entity nodes is important (performance drops from 76.5% to 74.8% when removed), (2) relevance scoring gives a boost (75.56% → 76.54%), (3) including node-type, relation, and score in attention/message functions each improves performance, and (4) relevance scoring particularly helps large/noisy retrieved KGs (questions with &gt;10 entities).",
            "key_findings": "Joint updates between LM representation (QA context) and KG via a working graph plus LM-conditioned relevance scoring produce better structured reasoning (notably negation handling) and higher accuracy than (a) LM-only baselines and (b) prior LM+KG methods that keep LM and KG separate; relevance scoring reduces noise from large retrieved subgraphs and improves performance on examples with many entities.",
            "counter_examples_or_negative_results": "Variants that connected the QA-context node to all KG nodes (instead of only topic entities) did not help and slightly degraded performance (-0.16%). Using contextual LM embeddings of nodes instead of just the scalar relevance score performed slightly worse (76.31% vs 76.54%). Removing the joint edges between z and KG nodes reduces gains on negation-handling (performance becomes close to MHGRN). The model still fails on some double-negation examples (noted as future work).",
            "uuid": "e3338.0",
            "source_info": {
                "paper_title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "RoBERTa-large (fine-tuned LM baseline)",
            "name_full": "RoBERTa-large (fine-tuned language model)",
            "brief_description": "A strong masked-language-model baseline used as the LM encoder and baseline QA model; used both standalone (fine-tuned on QA) and as the backbone LM in LM+KG systems.",
            "citation_title": "RoBERTa: A robustly optimized bert pretraining approach",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large",
            "model_description": "A masked language model (BERT-style Transformer) pretraining recipe (Robustly optimized BERT); used as encoder f_enc producing [CLS] representations that are fine-tuned for QA tasks in this paper.",
            "model_size": null,
            "reasoning_methods": [
                "Pure LM-based contextualized representation and classification (no external KG)"
            ],
            "reasoning_methods_description": "Fine-tune LM on (question, answer choice) pairs and predict answer via classification head using LM-encoded QA-context ([CLS] token) representation.",
            "diversity_of_methods": "Single/similar style — uses only LM internal knowledge and standard fine-tuning (no explicit structured reasoning or KG augmentation).",
            "reasoning_task_name": "CommonsenseQA; OpenBookQA",
            "reasoning_task_description": "Same as above; LM-only baseline for comparison with LM+KG methods.",
            "performance_by_method": "CommonsenseQA (IHdev/IHtest): 73.07 (±0.45) / 68.69 (±0.56). OpenBookQA (RoBERTa-large): 64.80 (±2.37). On negation subset (CommonsenseQA IHtest): overall 68.7, negation-containing questions 54.2.",
            "comparison_of_methods": "Serves as ablation baseline: QA-GNN improves substantially over fine-tuned RoBERTa (e.g., +~5.7% on OpenBookQA and +~4.7% overall on CommonsenseQA IHtest in some comparisons). LM-only models perform poorly at structured reasoning such as negation handling.",
            "key_findings": "Fine-tuned LMs have broad coverage but underperform on structured reasoning (e.g., negation) compared to models that incorporate KG reasoning; LM+KG methods can improve accuracy, but improvements depend on how LM and KG are integrated.",
            "counter_examples_or_negative_results": "LM-only predictions often fail to adapt to small structural changes like negation or entity swaps; in case studies RoBERTa's predictions sometimes do not change when negation or topic entities are modified (Table 8).",
            "uuid": "e3338.1",
            "source_info": {
                "paper_title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "MHGRN",
            "name_full": "Scalable Multi-Hop Graph Relation Network (MHGRN)",
            "brief_description": "A prior LM+KG method that models multi-hop relational reasoning in KGs and is a high-performing LM+KG baseline compared in this paper.",
            "citation_title": "Scalable multi-hop relational reasoning for knowledge-aware question answering",
            "mention_or_use": "use",
            "model_name": "MHGRN",
            "model_description": "A multi-hop relational GNN approach that explicitly models relations and multi-hop paths in the KG for knowledge-aware QA; used in prior work as the top LM+KG baseline and reproduced here with the same LM backbone for fair comparison.",
            "model_size": null,
            "reasoning_methods": [
                "KG path/multi-hop relational reasoning via dedicated GNN architecture"
            ],
            "reasoning_methods_description": "MHGRN runs relational message passing to capture multi-hop relational paths in subgraphs retrieved from the KG and combines KG-derived features with LM representations (but does not jointly update the LM representation via a working-graph QA node).",
            "diversity_of_methods": "Similar style — focuses on structured KG multi-hop path reasoning; keeps KG and LM modules relatively modular (separate updates) rather than joint unified updates.",
            "reasoning_task_name": "CommonsenseQA; OpenBookQA",
            "reasoning_task_description": "Same datasets; MHGRN is used as a comparative LM+KG pipeline.",
            "performance_by_method": "CommonsenseQA (controlled experiments): MHGRN IHdev 74.45 (±0.10), IHtest 71.11 (±0.81). OpenBookQA (RoBERTa-large): MHGRN 66.85 (±1.19); with AristoRoBERTa: 80.6 (reported). On negation subset (CommonsenseQA IHtest): MHGRN 54.8 on negation questions (only +0.6% over RoBERTa).",
            "comparison_of_methods": "QA-GNN outperforms MHGRN in both datasets; ablations indicate that the joint updating of LM and KG (working graph) and relevance scoring in QA-GNN likely account for improvements vs MHGRN which treats LM and KG separately.",
            "key_findings": "MHGRN provides improvements over simple LM baselines but lags behind QA-GNN; the separation of LM and KG representations may limit structured reasoning gains (e.g., negation handling).",
            "counter_examples_or_negative_results": "MHGRN gives limited improvement on negation-containing questions (+0.6% over RoBERTa) and struggles more with large/noisy retrieved KGs compared to QA-GNN with relevance scoring.",
            "uuid": "e3338.2",
            "source_info": {
                "paper_title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "KagNet",
            "name_full": "KagNet: Knowledge-aware Graph Networks",
            "brief_description": "A prior LM+KG method that models knowledge graph paths for commonsense reasoning and question answering; used as a baseline in this paper.",
            "citation_title": "Kagnet: Knowledge-aware graph networks for commonsense reasoning",
            "mention_or_use": "use",
            "model_name": "KagNet",
            "model_description": "Builds graph-aware path encodings to combine KG paths with LM embeddings for QA, focusing on enumerating and modeling paths between topic entities in the retrieved subgraph.",
            "model_size": null,
            "reasoning_methods": [
                "Path-based KG reasoning (explicit path enumeration and encoding) combined with LM features"
            ],
            "reasoning_methods_description": "Enumerate k-hop paths between topic entities in the KG and encode these paths to inform answer prediction in combination with LM features; KG and LM treated as separate modules.",
            "diversity_of_methods": "Similar style — relies on KG path modeling; does not perform joint LM+KG updates or LM-conditioned node scoring as QA-GNN does.",
            "reasoning_task_name": "CommonsenseQA",
            "reasoning_task_description": "Commonsense multiple-choice QA requiring leveraging commonsense KG paths.",
            "performance_by_method": "CommonsenseQA (controlled experiments): KagNet IHdev 73.47 (±0.22), IHtest 69.01 (±0.76). On negation subset it provided no improvement over RoBERTa (54.2 -&gt; 54.2).",
            "comparison_of_methods": "QA-GNN outperforms KagNet; KagNet's path-enumeration approach can introduce irrelevant nodes/paths when subgraphs are large, whereas QA-GNN's relevance scoring mitigates noisy nodes.",
            "key_findings": "Path enumeration alone (as in KagNet) yields limited gains on structured reasoning and is sensitive to noisy/large retrieved subgraphs.",
            "counter_examples_or_negative_results": "KagNet gives little-to-no improvement on negation examples and can be hurt by large subgraphs due to many irrelevant nodes.",
            "uuid": "e3338.3",
            "source_info": {
                "paper_title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "RN (Relation Network)",
            "name_full": "Relation Network (RN)",
            "brief_description": "A relation-aware neural module used as a KG-reasoning component in LM+KG pipelines, evaluated here as a baseline replacement for QA-GNN's GNN.",
            "citation_title": "A simple neural network module for relational reasoning",
            "mention_or_use": "use",
            "model_name": "Relation Network (RN)",
            "model_description": "A neural module designed to compute pairwise interactions (relations) between objects/entities and aggregate them for relational reasoning; here used on KG nodes with LM features.",
            "model_size": null,
            "reasoning_methods": [
                "Pairwise relational reasoning via pairwise interaction networks"
            ],
            "reasoning_methods_description": "Compute pairwise (or small-group) relations between node representations and aggregate to form reasoning signals, without the explicit multi-layer relational GNN design of QA-GNN.",
            "diversity_of_methods": "Single/similar style — relation-focused, pairwise interactions; does not integrate LM-conditioned KG pruning or joint LM+KG updates.",
            "reasoning_task_name": "CommonsenseQA; OpenBookQA",
            "reasoning_task_description": "Used as an alternative KG reasoning module plugged into the LM+KG framework for QA.",
            "performance_by_method": "CommonsenseQA (controlled experiments): RN IHdev 74.57 (±0.91), IHtest 69.08 (±0.21). OpenBookQA (RoBERTa-large): RN 65.20 (±1.18); with AristoRoBERTa 75.35 (±1.39).",
            "comparison_of_methods": "RN provides moderate gains over LM-only in some settings but is outperformed by QA-GNN which uses richer attention/message mechanisms and LM-conditioned scoring.",
            "key_findings": "Simple relation networks help but are less effective than QA-GNN's integrated joint reasoning and relevance scoring.",
            "counter_examples_or_negative_results": "RN did not match QA-GNN performance; lacks mechanisms for LM-conditioned KG node scoring and joint updates.",
            "uuid": "e3338.4",
            "source_info": {
                "paper_title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "RGCN",
            "name_full": "Relational Graph Convolutional Network (RGCN)",
            "brief_description": "A graph convolutional network variant that models multi-relational edges; evaluated as a KG reasoning baseline in LM+KG comparisons.",
            "citation_title": "Modeling relational data with graph convolutional networks",
            "mention_or_use": "use",
            "model_name": "RGCN",
            "model_description": "GNN variant that uses relation-specific transformations (one per relation type) to propagate information across multi-relational graphs; used here as an off-the-shelf KG reasoning module.",
            "model_size": null,
            "reasoning_methods": [
                "Relation-specific graph convolutional message passing"
            ],
            "reasoning_methods_description": "Apply relation-aware convolutional propagation where messages are transformed by relation-specific weights; combined with LM features but LM and KG representations are not jointly updated via a working-graph QA node in the baseline configuration.",
            "diversity_of_methods": "Similar style — KG-structure-centric reasoning with per-relation transforms; not integrated with LM-conditioned node scoring in the baseline.",
            "reasoning_task_name": "CommonsenseQA; OpenBookQA",
            "reasoning_task_description": "Used as baseline KG reasoning module plugged into the LM+KG framework.",
            "performance_by_method": "CommonsenseQA (controlled experiments): + RGCN IHdev 72.69 (±0.19), IHtest 68.41 (±0.66). OpenBookQA (RoBERTa-large): + RGCN 62.45 (±1.57); with AristoRoBERTa 74.60 (±2.53).",
            "comparison_of_methods": "RGCN as a baseline is outperformed by QA-GNN; QA-GNN avoids per-relation network duplication and adds LM-conditioned relevance scoring and joint updates.",
            "key_findings": "Basic relation-aware GNNs provide limited gains compared to more integrated LM+KG reasoning methods.",
            "counter_examples_or_negative_results": "RGCN sometimes underperforms the LM-only baseline in certain runs (per reported numbers), indicating integration choices matter.",
            "uuid": "e3338.5",
            "source_info": {
                "paper_title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "GconAttn",
            "name_full": "Graph-conditioned Attention (GconAttn)",
            "brief_description": "A prior LM+KG method that uses graph-conditioned attention mechanisms for KG-aware reasoning; included as a baseline.",
            "citation_title": "Improving natural language inference using external knowledge in the science questions domain",
            "mention_or_use": "use",
            "model_name": "GconAttn",
            "model_description": "Applies attention mechanisms conditioned on KG structure to incorporate external knowledge into the LM pipeline for QA/NLI-style tasks.",
            "model_size": null,
            "reasoning_methods": [
                "Graph-conditioned attention to integrate KG information with LM features"
            ],
            "reasoning_methods_description": "Compute attention-weighted combinations of KG node signals conditioned on the QA context and LM features, but does not implement the QA-context-as-node joint GNN update nor LM-conditioned node relevance scoring as QA-GNN does.",
            "diversity_of_methods": "Similar style — attention-based KG integration but less unified/joint than QA-GNN.",
            "reasoning_task_name": "CommonsenseQA; OpenBookQA (science NLI cited)",
            "reasoning_task_description": "Used as a baseline KG-attention integration approach for knowledge-aware QA.",
            "performance_by_method": "CommonsenseQA (controlled experiments): + GconAttn IHdev 72.61 (±0.39), IHtest 68.59 (±0.96). OpenBookQA (RoBERTa-large): + GconAttn 64.75 (±1.48); with AristoRoBERTa 71.80 (±1.21).",
            "comparison_of_methods": "GconAttn yields smaller gains than QA-GNN; QA-GNN's joint graph and relevance scoring produce larger improvements.",
            "key_findings": "Graph-conditioned attention is helpful but benefits from being part of a joint LM+KG update and LM-conditioned node selection.",
            "counter_examples_or_negative_results": "GconAttn underperforms QA-GNN and in some setups underperforms simpler baselines.",
            "uuid": "e3338.6",
            "source_info": {
                "paper_title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "UnifiedQA",
            "name_full": "UnifiedQA",
            "brief_description": "A large unified question-answering system trained in a text-to-text manner and reported on leaderboards; mentioned as a high-performing but much larger alternative.",
            "citation_title": "Unifiedqa: Crossing format boundaries with a single qa system",
            "mention_or_use": "mention",
            "model_name": "UnifiedQA",
            "model_description": "A text-to-text QA model (variants up to 11B parameters) trained across multiple QA formats/datasets; mentioned for leaderboard comparison as outperforming the presented system but at much larger scale.",
            "model_size": "up to 11B (11B variant cited in paper)",
            "reasoning_methods": [
                "Large-scale pretraining / multi-task QA finetuning (text-to-text transformer approach)"
            ],
            "reasoning_methods_description": "UnifiedQA reframes diverse QA formats into a text-to-text problem and fine-tunes large T5-like models across QA datasets; reasoning emerges from large-scale pretraining/fine-tuning rather than explicit LM+KG joint structured reasoning.",
            "diversity_of_methods": "Single large LM style (scale and multi-task training rather than explicit KG-based diverse methods); not comparable architecture-wise to QA-GNN's LM+KG joint reasoning.",
            "reasoning_task_name": "CommonsenseQA; OpenBookQA (leaderboard comparisons)",
            "reasoning_task_description": "Leaderboard competitor using large-scale text-to-text fine-tuned model.",
            "performance_by_method": "CommonsenseQA official test: UnifiedQA reported 79.1 (noted in paper); OpenBookQA leaderboard: UnifiedQA 87.2 (cited).",
            "comparison_of_methods": "UnifiedQA achieves higher leaderboard scores but is substantially larger (11B parameters) and trained on more data; authors note QA-GNN is competitive when excluding these much larger models.",
            "key_findings": "Large-scale, text-only models can achieve very strong QA performance, but QA-GNN demonstrates that integrating explicit KG structure and LM-conditioned KG selection yields competitive performance with far fewer parameters.",
            "counter_examples_or_negative_results": "UnifiedQA's higher performance attributed to much larger parameter counts and more training data, so direct architectural comparisons are limited.",
            "uuid": "e3338.7",
            "source_info": {
                "paper_title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "T5",
            "name_full": "T5 (Text-to-Text Transfer Transformer)",
            "brief_description": "A large text-to-text Transformer model cited as strong leaderboard performer; mentioned for scale comparison.",
            "citation_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "mention_or_use": "mention",
            "model_name": "T5",
            "model_description": "A text-to-text encoder-decoder Transformer family with variants up to 11B parameters; trained on large corpora and fine-tuned for downstream tasks, cited here as a top performer on OpenBookQA.",
            "model_size": "3B and larger variants mentioned (paper cites 3B variant performance)",
            "reasoning_methods": [
                "Large-scale pretraining with text-to-text fine-tuning, implicit reasoning from scale"
            ],
            "reasoning_methods_description": "Reasoning achieved implicitly via model capacity and large-scale multi-task pretraining, not via explicit KG integration.",
            "diversity_of_methods": "Single dominant style (large LM) rather than explicit heterogeneous reasoning modules.",
            "reasoning_task_name": "OpenBookQA (leaderboard)",
            "reasoning_task_description": "Leaderboard competitor using T5 variants; cited for comparison.",
            "performance_by_method": "OpenBookQA leaderboard: T5* reported 83.2 (paper).",
            "comparison_of_methods": "T5 outperforms QA-GNN on leaderboard, but uses much larger models and more data; QA-GNN focuses on explicit KG integration to achieve strong performance at smaller scale.",
            "key_findings": "Large text-only models can outperform smaller LM+KG hybrids, but explicit KG integration yields good performance with smaller models.",
            "counter_examples_or_negative_results": "Scaling alone (T5, UnifiedQA) can beat QA-GNN, highlighting tradeoffs between model size/scale and structured KG integration.",
            "uuid": "e3338.8",
            "source_info": {
                "paper_title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering",
                "publication_date_yy_mm": "2021-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Kagnet: Knowledge-aware graph networks for commonsense reasoning",
            "rating": 2
        },
        {
            "paper_title": "Scalable multi-hop relational reasoning for knowledge-aware question answering",
            "rating": 2
        },
        {
            "paper_title": "A simple neural network module for relational reasoning",
            "rating": 1
        },
        {
            "paper_title": "Modeling relational data with graph convolutional networks",
            "rating": 1
        },
        {
            "paper_title": "RoBERTa: A robustly optimized bert pretraining approach",
            "rating": 1
        },
        {
            "paper_title": "Unifiedqa: Crossing format boundaries with a single qa system",
            "rating": 1
        },
        {
            "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "rating": 1
        }
    ],
    "cost": 0.019244749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering</h1>
<p>Michihiro Yasunaga Hongyu Ren Antoine Bosselut<br>Percy Liang Jure Leskovec<br>Stanford University<br>{myasu,hyren, antoineb, pliang, jure}@cs.stanford.edu</p>
<h4>Abstract</h4>
<p>The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. Here we propose a new model, $Q A-G N N$, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph-based message passing. We evaluate QA-GNN on the CommonsenseQA and OpenBookQA datasets, and show its improvement over existing LM and LM+KG models, as well as its capability to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.</p>
<h2>1 Introduction</h2>
<p>Question answering systems must be able to access relevant knowledge and reason over it. Typically, knowledge can be implicitly encoded in large language models (LMs) pre-trained on unstructured text (Petroni et al., 2019; Bosselut et al., 2019), or explicitly represented in structured knowledge graphs (KGs), such as Freebase (Bollacker et al., 2008) and ConceptNet (Speer et al., 2017), where entities are represented as nodes and relations between them as edges. Recently, pre-trained LMs have demonstrated remarkable success in many question answering tasks (Liu et al., 2019; Raffel et al., 2020). However, while LMs have a broad coverage of knowledge, they do not empirically perform well on structured reasoning (e.g., handling negation) (Kassner and Schütze, 2020). On the other hand, KGs are more suited for structured reasoning (Ren et al., 2020; Ren and Leskovec, 2020) and enable explainable predictions e.g., by providing reasoning paths (Lin et al., 2019), but may lack coverage and
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Given the QA context (question and answer choice; purple box), we aim to derive the answer by performing joint reasoning over the language and the knowledge graph (green box).
be noisy (Bordes et al., 2013; Guu et al., 2015). How to reason effectively with both sources of knowledge remains an important open problem.</p>
<p>Combining LMs and KGs for reasoning (henceforth, LM+KG) presents two challenges: given a QA context (e.g., question and answer choices; Figure 1 purple box), methods need to (i) identify informative knowledge from a large KG (green box); and (ii) capture the nuance of the QA context and the structure of the KGs to perform joint reasoning over these two sources of information. Previous works (Bao et al., 2016; Sun et al., 2018; Lin et al., 2019) retrieve a subgraph from the KG by taking topic entities (KG entities mentioned in the given QA context) and their few-hop neighbors. However, this introduces many entity nodes that are semantically irrelevant to the QA context, especially when the number of topic entities or hops increases. Additionally, existing LM+KG methods for reasoning (Lin et al., 2019; Wang et al., 2019a; Feng et al., 2020; Lv et al., 2020) treat the QA context and KG as two separate modalities. They individually apply LMs to the QA context and graph</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of our approach. Given a QA context $(z)$, we connect it with the retrieved KG to form a joint graph (working graph; §3.1), compute the relevance of each KG node conditioned on $z$ ( $\S 3.2$; node shading indicates the relevance score), and perform reasoning on the working graph ( $\S 3.3$ ).
neural networks (GNNs) to the KG, and do not mutually update or unify their representations. This separation might limit their capability to perform structured reasoning, e.g., handling negation.</p>
<p>Here we propose $Q A-G N N$, an end-to-end LM+KG model for question answering that addresses the above two challenges. We first encode the QA context using an LM, and retrieve a KG subgraph following prior works (Feng et al., 2020). Our QA-GNN has two key insights: (i) Relevance scoring: Since the KG subgraph consists of all few-hop neighbors of the topic entities, some entity nodes are more relevant than others with respect to the given QA context. We hence propose KG node relevance scoring: we score each entity on the KG subgraph by concatenating the entity with the QA context and calculating the likelihood using a pretrained LM. This presents a general framework to weight information on the KG; (ii) Joint reasoning: We design a joint graph representation of the QA context and KG, where we explicitly view the QA context as an additional node ( $Q A$ context node) and connect it to the topic entities in the KG subgraph as shown in Figure 1. This joint graph, which we term the working graph, unifies the two modalities into one graph. We then augment the feature of each node with the relevance score, and design a new attention-based GNN module for reasoning. Our joint reasoning algorithm on the working graph simultaneously updates the representation of both the KG entities and the QA context node, bridging the gap between the two sources of information.</p>
<p>We evaluate QA-GNN on two question answering datasets that require reasoning with knowledge: CommonsenseQA (Talmor et al., 2019) and OpenBookQA (Mihaylov et al., 2018), using the ConceptNet KG (Speer et al., 2017). QA-GNN outperforms strong fine-tuned LM baselines as well as the existing best LM+KG model (with the same LM) by up to $5.7 \%$ and $3.7 \%$ respectively. In particular, QA-GNN exhibits improved performance on some forms of structured reasoning (e.g., correctly han-
dling negation and entity substitution in questions): it achieves $4.6 \%$ improvement over fine-tuned LMs on questions with negation, while existing LM+KG models are $+0.6 \%$ over fine-tuned LMs. We also show that one can extract reasoning processes from QA-GNN in the form of general KG subgraphs, not just paths (Lin et al., 2019), suggesting a general method for explaining model predictions.</p>
<h2>2 Problem Statement</h2>
<p>We aim to answer natural language questions using knowledge from a pre-trained LM and a structured KG. We use the term language model broadly to be any composition of two functions, $f_{\text {head }}\left(f_{\text {enc }}(\mathbf{x})\right)$, where $f_{\text {enc }}$, the encoder, maps a textual input $\mathbf{x}$ to a contextualized vector representation $\mathbf{h}^{\mathrm{LM}}$, and $f_{\text {head }}$ uses this representation to perform a desired task (which we discuss in $\S 3.2$ ). In this work, we specifically use masked language models (e.g., RoBERTa) as $f_{\text {enc }}$, and let $\mathbf{h}^{\mathrm{LM}}$ denote the output representation of a [CLS] token that is prepended to the input sequence $\mathbf{x}$, unless otherwise noted. We define the knowledge graph as a multi-relational graph $\mathcal{G}=$ $(\mathcal{V}, \mathcal{E})$. Here $\mathcal{V}$ is the set of entity nodes in the KG; $\mathcal{E} \subseteq \mathcal{V} \times \mathcal{R} \times \mathcal{V}$ is the set of edges that connect nodes in $\mathcal{V}$, where $\mathcal{R}$ represents a set of relation types.</p>
<p>Given a question $q$ and an answer choice $a \in \mathcal{C}$, we follow prior work (Lin et al., 2019) to link the entities mentioned in the question and answer choice to the given $\mathrm{KG} \mathcal{G}$. We denote $\mathcal{V}<em a="a">{q} \subseteq \mathcal{V}$ and $\mathcal{V}</em>} \subseteq \mathcal{V}$ as the set of KG entities mentioned in the question (question entities; blue entities in Figure1) and answer choice (answer choice entities; red entities in Figure 1), respectively, and use $\mathcal{V<em q="q">{q, a}:=\mathcal{V}</em>} \cup \mathcal{V<em _sub="{sub" _text="\text">{a}$ to denote all the entities that appear in either the question or answer choice, which we call topic entities. We then extract a subgraph from $\mathcal{G}$ for a question-choice pair, $\mathcal{G}</em>}}^{q, a}=\left(\mathcal{V<em _sub="{sub" _text="\text">{\text {sub }}^{q, a}, \mathcal{E}</em>$.}}^{q, a}\right),{ }^{1}$ which comprises all nodes on the $k$-hop paths between nodes in $\mathcal{V}_{q, a</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Relevance scoring of the retrieved KG: we use a pre-trained LM to calculate the relevance of each KG entity node conditioned on the QA context (§3.2).</p>
<h2>3 Approach: QA-GNN</h2>
<p>As shown in Figure 2, given a question and an answer choice $a$, we concatenate them to get the $Q A$ context $[q ; a]$. To reason over a given QA context using knowledge from both the LM and the KG, QA-GNN works as follows. First, we use the LM to obtain a representation for the QA context, and retrieve the subgraph $\mathcal{G}<em a="a" q_="q,">{\text {sub }}$ from the KG. Then we introduce a $Q A$ context node $z$ that represents the QA context, and connect $z$ to the topic entities $\mathcal{V}</em>}$ so that we have a joint graph over the two sources of knowledge, which we term the working graph, $\mathcal{G<em _mathrm_W="\mathrm{W">{\mathrm{W}}$ (§3.1). To adaptively capture the relationship between the QA context node and each of the other nodes in $\mathcal{G}</em>$ for multiple rounds (§3.3). Finally, we make the final prediction using the LM representation, QA context node representation and a pooled working graph representation (§3.4).}}$, we calculate a relevance score for each pair using the LM, and use this score as an additional feature for each node (§3.2). We then propose an attention-based GNN module that does message passing on the $\mathcal{G}_{\mathrm{W}</p>
<h3>3.1 Joint graph representation</h3>
<p>To design a joint reasoning space for the two sources of knowledge, we explicitly connect them in a common graph structure. We introduce a new QA context node $z$ which represents the QA context, and connect $z$ to each topic entity in $\mathcal{V}<em _sub="{sub" _text="\text">{q, a}$ on the KG subgraph $\mathcal{G}</em>$. These relation types capture the relationship between the QA context and the relevant entities in the KG, depending on whether the entity is found in the question portion or the answer portion of the QA context. Since this joint graph intuitively provides a reasoning space (working memory) over
the QA context and KG, we term it working graph $\mathcal{G}}}$ using two new relation types $r_{z, q}$ and $r_{z, a<em _mathrm_W="\mathrm{W">{\mathrm{W}}=\left(\mathcal{V}</em>}}, \mathcal{E<em _mathrm_W="\mathrm{W">{\mathrm{W}}\right)$, where $\mathcal{V}</em>}}=\mathcal{V<em _mathrm_W="\mathrm{W">{\text {sub }} \cup{z}$ and $\mathcal{E}</em>}}=$ $\mathcal{E<em q="q" z_="z,">{\text {sub }} \cup\left{\left(z, r</em>}, v\right) \mid v \in \mathcal{V<em a="a" z_="z,">{q}\right} \cup\left{\left(z, r</em>\right}$.}, v\right) \mid v \in \mathcal{V}_{a</p>
<p>Each node in the $\mathcal{G}<em q="q">{\mathrm{W}}$ is associated with one of the four types: $\mathcal{T}={\mathbf{Z}, \mathbf{Q}, \mathbf{A}, \mathbf{O}}$, each indicating the context node $z$, nodes in $\mathcal{V}</em>}$, nodes in $\mathcal{V<em _sub="{sub" _text="\text">{a}$, and other nodes, respectively (corresponding to the node color, purple, blue, red, gray in Figure 1 and 2). We denote the text of the context node $z$ (QA context) and KG node $v \in \mathcal{V}</em>(z)$ and text $(v)$.}}$ (entity name) as $\operatorname{text</p>
<p>We initialize the node embedding for $z$ using the LM representation of the QA context ( $\mathbf{z}^{\mathrm{LM}}=f_{\text {enc }}(\operatorname{text}(z))$ ), and each node on the $\mathcal{G}_{\text {sub }}$ using the entity embedding from Feng et al. (2020). In the subsequent sections, we will reason over the working graph in order to score a given (question, answer choice) pair.</p>
<h3>3.2 KG node relevance scoring</h3>
<p>Many nodes on the KG subgraph $\mathcal{G}<em _sub="{sub" _text="\text">{\text {sub }}$ (i.e., those heuristically retrieved from the KG) can be irrelevant under the current QA context. As an example shown in Figure 3, the retrieved KG subgraph $\mathcal{G}</em>}}$ with few-hop neighbors of the $\mathcal{V<em a="a" q_="q,">{q, a}$ may include nodes that are uninformative for the reasoning process, e.g., nodes "holiday" and "river bank" are off-topic; "human" and "place" are generic. These irrelevant nodes may result in overfitting or introduce unnecessary difficulty in reasoning, an issue especially when $\mathcal{V}</em>\right|&gt;400$ nodes on average if we consider 3-hop neighbors.}$ is large. For instance, we empirically find that using the ConceptNet KG (Speer et al., 2017), we will retrieve a KG with $\left|\mathcal{V}_{\text {sub }</p>
<p>In response, we propose node relevance scoring, where we use the pre-trained language model to score the relevance of each KG node $v \in \mathcal{V}_{\text {sub }}$</p>
<p>conditioned on the QA context. For each node $v$, we concatenate the entity $\operatorname{text}(v)$ with the QA context $\operatorname{text}(z)$ and compute the relevance score:</p>
<p>$$
\rho_{v}=f_{\text {head }}\left(f_{\text {enc }}([\operatorname{text}(z) ; \operatorname{text}(v)]))\right.
$$</p>
<p>where $f_{\text {head }} \circ f_{\text {enc }}$ denotes the probability of text $(v)$ computed by the LM. This relevance score $\rho_{v}$ captures the importance of each KG node relative to the given QA context, which is used for reasoning or pruning the working graph $\mathcal{G}_{\mathrm{W}}$.</p>
<h3>3.3 GNN architecture</h3>
<p>To perform reasoning on the working graph $\mathcal{G}<em t="t">{\mathrm{W}}$, our GNN module builds on the graph attention framework (GAT) (Veličković et al., 2018), which induces node representations via iterative message passing between neighbors on the graph. Specifically, in a $L$-layer QA-GNN, for each layer, we update the representation $\boldsymbol{h}</em>$ by}^{(\ell)} \in \mathbb{R}^{D}$ of each node $t \in \mathcal{V}_{\mathrm{W}</p>
<p>$$
\boldsymbol{h}<em n="n">{t}^{(\ell+1)}=f</em>}\left(\sum_{s \in \mathcal{N<em s="s" t="t">{t} \cup{t}} \alpha</em>} \boldsymbol{m<em t="t">{s t}\right)+\boldsymbol{h}</em>
$$}^{(\ell)</p>
<p>where $\mathcal{N}<em s="s" t="t">{t}$ represents the neighborhood of node $t$, $\boldsymbol{m}</em>} \in \mathbb{R}^{D}$ notes the message from each neighbor node $s$ to $t$, and $\alpha_{s t}$ is an attention weight that scales each message $\boldsymbol{m<em n="n">{s t}$ from $s$ to $t$. The sum of the messages is then passed through a 2-layer MLP, $f</em>}: \mathbb{R}^{D} \rightarrow \mathbb{R}^{D}$, with batch normalization (Ioffe and Szegedy, 2015). For each node $t \in \mathcal{V<em t="t">{\mathrm{W}}$, we set $\boldsymbol{h}</em>}^{(0)}$ using a linear transformation $f_{h}$ that maps its initial node embedding (described in $\S 3.1$ ) to $\mathbb{R}^{D}$. Crucially, as our GNN message passing operates on the working graph, it will jointly leverage and update the representation of the QA context and KG. We further propose an expressive message ( $\boldsymbol{m<em s="s" t="t">{s t}$ ) and attention $\left(\alpha</em>\right)$ computation below.</p>
<p>Node type \&amp; relation-aware message. As $\mathcal{G}<em t="t">{\mathrm{W}}$ is a multi-relational graph, the message passed from a source node to the target node should capture their relationship, i.e., relation type of the edge and source/target node types. To this end, we first obtain the type embedding $\boldsymbol{u}</em>$ from node $s$ to node $t$ by}$ of each node $t$, as well as the relation embedding $\boldsymbol{r}_{s t</p>
<p>$$
\boldsymbol{u}<em u="u">{t}=f</em>}\left(\mathrm{u<em s="s" t="t">{t}\right), \quad \boldsymbol{r}</em>}=f_{r}\left(\mathrm{e<em s="s">{s t}, \mathrm{u}</em>\right)
$$}, \mathrm{u}_{t</p>
<p>where $\mathrm{u}<em t="t">{s}, \mathrm{u}</em>} \in{0,1}^{|\mathcal{T}|}$ are one-hot vectors indicating the node types of $s$ and $t, \mathrm{e<em u="u">{s t} \in{0,1}^{|\mathcal{R}|}$ is a one-hot vector indicating the relation type of edge $(s, t), f</em>$ is a 2-layer MLP. We then compute the message from $s$ to $t$ as}: \mathbb{R}^{|\mathcal{T}|} \rightarrow \mathbb{R}^{D / 2}$ is a linear transformation, and $f_{r}: \mathbb{R}^{|\mathcal{R}|+2|\mathcal{T}|} \rightarrow \mathbb{R}^{D</p>
<p>$$
\boldsymbol{m}<em m="m">{s t}=f</em>}\left(\boldsymbol{h<em s="s">{s}^{(\ell)}, \boldsymbol{u}</em>\right)
$$}, \boldsymbol{r}_{s t</p>
<p>where $f_{m}: \mathbb{R}^{2.5 D} \rightarrow \mathbb{R}^{D}$ is a linear transformation.</p>
<p>Node type, relation, and score-aware attention. Attention captures the strength of association between two nodes, which is ideally informed by their node types, relations and node relevance scores.
We first embed the relevance score of each node $t$ by</p>
<p>$$
\boldsymbol{\rho}<em _rho="\rho">{t}=f</em>\right)
$$}\left(\rho_{t</p>
<p>where $f_{\rho}: \mathbb{R} \rightarrow \mathbb{R}^{D / 2}$ is an MLP. To compute the attention weight $\alpha_{s t}$ from node $s$ to node $t$, we obtain the query and key vectors $\boldsymbol{q}, \boldsymbol{k}$ by</p>
<p>$$
\begin{aligned}
&amp; \boldsymbol{q}<em q="q">{s}=f</em>}\left(\boldsymbol{h<em s="s">{s}^{(\ell)}, \boldsymbol{u}</em>}, \boldsymbol{\rho<em t="t">{s}\right) \
&amp; \boldsymbol{k}</em>}=f_{k}\left(\boldsymbol{h<em t="t">{t}^{(\ell)}, \boldsymbol{u}</em>}, \boldsymbol{\rho<em s="s" t="t">{t}, \boldsymbol{r}</em>\right)
\end{aligned}
$$</p>
<p>where $f_{q}: \mathbb{R}^{2 D} \rightarrow \mathbb{R}^{D}$ and $f_{k}: \mathbb{R}^{3 D} \rightarrow \mathbb{R}^{D}$ are linear transformations. The attention weight is then</p>
<p>$$
\alpha_{s t}=\frac{\exp \left(\gamma_{s t}\right)}{\sum_{t^{\prime} \in \mathcal{N}<em s="s" t_prime="t^{\prime">{s} \cup{s}} \exp \left(\gamma</em>}}\right)}, \quad \gamma_{s t}=\frac{\boldsymbol{q<em t="t">{s}^{\top} \boldsymbol{k}</em>
$$}}{\sqrt{D}</p>
<h3>3.4 Inference \&amp; Learning</h3>
<p>Given a question $q$ and an answer choice $a$, we use the information from both the QA context and the KG to calculate the probability of it being the answer $p(a \mid q) \propto \exp \left(\operatorname{MLP}\left(\boldsymbol{z}^{\mathrm{LM}}, \boldsymbol{z}^{\mathrm{GNN}}, \boldsymbol{g}\right)\right)$, where $\boldsymbol{z}^{\mathrm{GNN}}=\boldsymbol{h}<em v="v">{s}^{(L)}$ and $\boldsymbol{g}$ denotes the pooling of $\left{\boldsymbol{h}</em>\right}$. In the training data, each question has a set of answer choices with one correct choice. We optimize the model (both the LM and GNN components end-to-end) using the cross entropy loss.}^{(L)} \mid v \in \mathcal{V}_{\text {sub }</p>
<h3>3.5 Computation complexity</h3>
<p>We analyze the time and space complexity of our method and compare with prior works, KagNet (Lin et al., 2019) and MHGRN (Feng et al., 2020) in Table 1. As we handle edges of different relation types using different edge embeddings instead of designing an independent graph networks for each relation as in RGCN (Schlichtkrull et al., 2018) or MHGRN, the time complexity of our method is constant with respect to the number of relations and linear with respect to the number of nodes. We achieve the same space complexity as MHGRN (Feng et al., 2020).</p>
<h2>4 Experiments</h2>
<h3>4.1 Datasets</h3>
<p>We evaluate QA-GNN on two question answering datasets: CommonsenseQA (Talmor et al., 2019) and OpenBookQA (Mihaylov et al., 2018). CommonsenseQA is a 5-way multiple choice QA task that requires reasoning with commonsense knowledge, containing 12,102 questions. The test set of CommonsenseQA is not publicly available, and model predictions can only be evaluated once every two weeks via the official leaderboard. Hence,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Time</th>
<th style="text-align: center;">Space</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\mathcal{G}$ is a dense graph</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$L$-hop KagNet</td>
<td style="text-align: center;">$\mathcal{O}\left(</td>
<td style="text-align: center;">\mathcal{R}</td>
</tr>
<tr>
<td style="text-align: center;">$L$-hop MHGRN</td>
<td style="text-align: center;">$\mathcal{O}\left(</td>
<td style="text-align: center;">\mathcal{R}</td>
</tr>
<tr>
<td style="text-align: center;">$L$-layer QA-GNN</td>
<td style="text-align: center;">$\mathcal{O}\left(</td>
<td style="text-align: center;">\mathcal{V}</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{G}$ is a sparse graph with maximum node degree $\Delta \ll</td>
<td style="text-align: center;">\mathcal{V}</td>
<td style="text-align: center;">$</td>
</tr>
<tr>
<td style="text-align: center;">$L$-hop KagNet</td>
<td style="text-align: center;">$\mathcal{O}\left(</td>
<td style="text-align: center;">\mathcal{R}</td>
</tr>
<tr>
<td style="text-align: center;">$L$-hop MHGRN</td>
<td style="text-align: center;">$\mathcal{O}\left(</td>
<td style="text-align: center;">\mathcal{R}</td>
</tr>
<tr>
<td style="text-align: center;">$L$-layer QA-GNN</td>
<td style="text-align: center;">$\mathcal{O}(</td>
<td style="text-align: center;">\mathcal{V}</td>
</tr>
</tbody>
</table>
<p>Table 1: Computation complexity of different $L$-hop reasoning models on a dense/sparse graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ with the relation set $\mathcal{R}$.
we perform main experiments on the in-house (IH) data split used in Lin et al. (2019), and also report the score of our final system on the official test set. OpenBookQA is a 4-way multiple choice QA task that requires reasoning with elementary science knowledge, containing 5,957 questions. We use the official data split.</p>
<h3>4.2 Knowledge graphs</h3>
<p>We use ConceptNet (Speer et al., 2017), a generaldomain knowledge graph, as our structured knowledge source $\mathcal{G}$ for both of the above tasks. Given each QA context (question and answer choice), we retrieve the subgraph $\mathcal{G}<em _sub="{sub" _text="\text">{\text {sub }}$ from $\mathcal{G}$ following the pre-processing step described in Feng et al. (2020), with hop size $k=2$. Henceforth, in this section (§4) we use the term "KG" to refer to $\mathcal{G}</em>$.}</p>
<h3>4.3 Implementation \&amp; training details</h3>
<p>We set the dimension $(D=200)$ and number of layers $(L=5)$ of our GNN module, with dropout rate 0.2 applied to each layer (Srivastava et al., 2014). The parameters of the model are optimized by RAdam (Liu et al., 2020), with batch size 128, gradient clipping 1.0 (Pascanu et al., 2013), and learning rate $1 \mathrm{e}-5$ and $1 \mathrm{e}-3$ for the LM and GNN components respectively. Each model is trained using two GPUs (GTX Titan X), which takes $\sim 20$ hours on average. The above hyperparameters were tuned on the development set.</p>
<h3>4.4 Baselines</h3>
<p>Fine-tuned LM. To study the role of KGs, we compare with a vanilla fine-tuned LM, which does not use the KG. We use RoBERTa-large (Liu et al., 2019) for CommonsenseQA, and RoBERTa-large and AristoRoBERTa ${ }^{2}$ (Clark et al., 2019) for</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Methods</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">RoBERTa-large (w/o KG)</th>
<th style="text-align: left;">$73.07( \pm 0.45)$</th>
<th style="text-align: left;">$68.69( \pm 0.56)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">+ RGCN (Schlichtkrull et al., 2018)</td>
<td style="text-align: left;">$72.69( \pm 0.19)$</td>
<td style="text-align: left;">$68.41( \pm 0.66)$</td>
</tr>
<tr>
<td style="text-align: left;">+ GconAttn (Wang et al., 2019a)</td>
<td style="text-align: left;">$72.61( \pm 0.39)$</td>
<td style="text-align: left;">$68.59( \pm 0.96)$</td>
</tr>
<tr>
<td style="text-align: left;">+ KagNet (Lin et al., 2019)</td>
<td style="text-align: left;">$73.47( \pm 0.22)$</td>
<td style="text-align: left;">$69.01( \pm 0.76)$</td>
</tr>
<tr>
<td style="text-align: left;">+ RN (Santoro et al., 2017)</td>
<td style="text-align: left;">$74.57( \pm 0.91)$</td>
<td style="text-align: left;">$69.08( \pm 0.21)$</td>
</tr>
<tr>
<td style="text-align: left;">+ MHGRN (Feng et al., 2020)</td>
<td style="text-align: left;">$74.45( \pm 0.10)$</td>
<td style="text-align: left;">$71.11( \pm 0.81)$</td>
</tr>
<tr>
<td style="text-align: left;">+ QA-GNN (Ours)</td>
<td style="text-align: left;">$\mathbf{7 6 . 5 4}( \pm 0.21)$</td>
<td style="text-align: left;">$\mathbf{7 3 . 4 1}( \pm 0.92)$</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance comparison on Commonsense $\boldsymbol{Q A}$ in-house split (controlled experiments). As the official test is hidden, here we report the in-house Dev (IHdev) and Test (IHtest) accuracy, following the data split of Lin et al. (2019).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: left;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RoBERTa (Liu et al., 2019)</td>
<td style="text-align: left;">72.1</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa+FreeLB (Zhu et al., 2020) (ensemble)</td>
<td style="text-align: left;">73.1</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa+HyKAS (Ma et al., 2019)</td>
<td style="text-align: left;">73.2</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa+KE (ensemble)</td>
<td style="text-align: left;">73.3</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa+KEDGN (ensemble)</td>
<td style="text-align: left;">74.4</td>
</tr>
<tr>
<td style="text-align: left;">XLNet+GraphReason (Lv et al., 2020)</td>
<td style="text-align: left;">75.3</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa+MHGRN (Feng et al., 2020)</td>
<td style="text-align: left;">75.4</td>
</tr>
<tr>
<td style="text-align: left;">Albert+PG (Wang et al., 2020b)</td>
<td style="text-align: left;">75.6</td>
</tr>
<tr>
<td style="text-align: left;">Albert (Lan et al., 2020) (ensemble)</td>
<td style="text-align: left;">76.5</td>
</tr>
<tr>
<td style="text-align: left;">UnifiedQA ${ }^{3}$ (Khashabi et al., 2020)</td>
<td style="text-align: left;">$\mathbf{7 9 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa + QA-GNN (Ours)</td>
<td style="text-align: left;">76.1</td>
</tr>
</tbody>
</table>
<p>Table 3: Test accuracy on CommonsenseQA's official leaderboard. The top system, UnifiedQA (11B parameters) is 30x larger than our model.</p>
<h2>OpenBookQA.</h2>
<p>Existing LM+KG models. We compare with existing LM+KG methods, which share the same high-level framework as ours but use different modules to reason on the KG in place of QA-GNN ("yellow box" in Figure2): (1) Relation Network (RN) (Santoro et al., 2017), (2) RGCN (Schlichtkrull et al., 2018), (3) GconAttn (Wang et al., 2019a), (4) KagNet (Lin et al., 2019), and (5) MHGRN (Feng et al., 2020). (1),(2),(3) are relation-aware GNNs for KGs, and (4),(5) further model paths in KGs. MHGRN is the existing top performance model under this LM+KG framework. For fair comparison, we use the same LM in all the baselines and our model. The key differences between QA-GNN and these are that they do not perform relevance scoring or joint updates with the QA context (§3).</p>
<h3>4.5 Main results</h3>
<p>Table 2 and Table 4 show the results on CommonsenseQA and OpenBookQA, respectively. On both datasets, we observe consistent improvements over fine-tuned LMs and existing LM+KG models, e.g., on OpenBookQA, $+5.7 \%$ over RoBERTa, and $+3.7 \%$ over the prior best LM+KG system,
additional input to the QA context.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">RoBERTa-large</th>
<th style="text-align: center;">AristoRoBERTa</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Fine-tuned LMs (w/o KG)</td>
<td style="text-align: center;">$64.80( \pm 2.37)$</td>
<td style="text-align: center;">$78.40( \pm 1.64)$</td>
</tr>
<tr>
<td style="text-align: left;">+ RGCN</td>
<td style="text-align: center;">$62.45( \pm 1.57)$</td>
<td style="text-align: center;">$74.60( \pm 2.53)$</td>
</tr>
<tr>
<td style="text-align: left;">+ GconAtten</td>
<td style="text-align: center;">$64.75( \pm 1.48)$</td>
<td style="text-align: center;">$71.80( \pm 1.21)$</td>
</tr>
<tr>
<td style="text-align: left;">+ RN</td>
<td style="text-align: center;">$65.20( \pm 1.18)$</td>
<td style="text-align: center;">$75.35( \pm 1.39)$</td>
</tr>
<tr>
<td style="text-align: left;">+ MHGRN</td>
<td style="text-align: center;">$66.85( \pm 1.19)$</td>
<td style="text-align: center;">80.6</td>
</tr>
<tr>
<td style="text-align: left;">+ QA-GNN (Ours)</td>
<td style="text-align: center;">$\mathbf{7 0 . 5 8 ( \pm 1 . 4 2 )}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 7 7 ( \pm 1 . 5 6 )}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Test accuracy comparison on OpenBook $\boldsymbol{Q A}$ (controlled experiments). Methods with AristoRoBERTa use the textual evidence by Clark et al. (2019) as an additional input to the QA context.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: left;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Careful Selection (Banerjee et al., 2019)</td>
<td style="text-align: left;">72.0</td>
</tr>
<tr>
<td style="text-align: left;">AristoRoBERTa</td>
<td style="text-align: left;">77.8</td>
</tr>
<tr>
<td style="text-align: left;">KF + SIR (Banerjee and Baral, 2020)</td>
<td style="text-align: left;">80.0</td>
</tr>
<tr>
<td style="text-align: left;">AristoRoBERTa + PG (Wang et al., 2020b)</td>
<td style="text-align: left;">80.2</td>
</tr>
<tr>
<td style="text-align: left;">AristoRoBERTa + MHGRN (Feng et al., 2020)</td>
<td style="text-align: left;">80.6</td>
</tr>
<tr>
<td style="text-align: left;">Albert + KB</td>
<td style="text-align: left;">81.0</td>
</tr>
<tr>
<td style="text-align: left;">T5* (Raffel et al., 2020)</td>
<td style="text-align: left;">83.2</td>
</tr>
<tr>
<td style="text-align: left;">UnifiedQA* (Khashabi et al., 2020)</td>
<td style="text-align: left;">$\mathbf{8 7 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">AristoRoBERTa + QA-GNN (Ours)</td>
<td style="text-align: left;">82.8</td>
</tr>
</tbody>
</table>
<p>Table 5: Test accuracy on OpenBookQA leaderboard. All listed methods use the provided science facts as an additional input to the language context. The top 2 systems, UnifiedQA (11B params) and T5 (3B params) are 30x and 8x larger than our model.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Graph Connection (§3.1)</th>
<th style="text-align: center;">Dev Acc.</th>
<th style="text-align: center;">Relevance scoring (§3.2)</th>
<th style="text-align: center;">Dev Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">No edge between Z and KG nodes</td>
<td style="text-align: center;">74.81</td>
<td style="text-align: center;">Nothing</td>
<td style="text-align: center;">75.56</td>
</tr>
<tr>
<td style="text-align: center;">Connect Z to all KG nodes</td>
<td style="text-align: center;">76.38</td>
<td style="text-align: center;">w/ contextual embedding</td>
<td style="text-align: center;">76.31</td>
</tr>
<tr>
<td style="text-align: center;">Connect Z to QA entity nodes (final)</td>
<td style="text-align: center;">76.54</td>
<td style="text-align: center;">w/ relevance score (final)</td>
<td style="text-align: center;">76.54</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/ both</td>
<td style="text-align: center;">76.52</td>
</tr>
<tr>
<td style="text-align: center;">GNN Attention \&amp; Message (§3.3)</td>
<td style="text-align: center;">Dev Acc.</td>
<td style="text-align: center;">GNN Layers (§3.3)</td>
<td style="text-align: center;">Dev Acc.</td>
</tr>
<tr>
<td style="text-align: center;">Node type, relation, score-aware (final)</td>
<td style="text-align: center;">76.54</td>
<td style="text-align: center;">$L=4$</td>
<td style="text-align: center;">75.51</td>
</tr>
<tr>
<td style="text-align: center;">- type-aware</td>
<td style="text-align: center;">75.41</td>
<td style="text-align: center;">$L=4$</td>
<td style="text-align: center;">76.14</td>
</tr>
<tr>
<td style="text-align: center;">- relation-aware</td>
<td style="text-align: center;">75.61</td>
<td style="text-align: center;">$L=5$ (final)</td>
<td style="text-align: center;">76.94</td>
</tr>
<tr>
<td style="text-align: center;">- score-aware</td>
<td style="text-align: center;">75.56</td>
<td style="text-align: center;">$L=5$</td>
<td style="text-align: center;">76.21</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$L=7$</td>
<td style="text-align: center;">75.06</td>
</tr>
</tbody>
</table>
<p>Table 6: Ablation study of our model components, using the CommonsenseQA IHdev set.</p>
<p>MHGRN. The boost over MHGRN suggests that QA-GNN makes a better use of KGs to perform joint reasoning than existing LM+KG methods.</p>
<p>We also achieve competitive results to other systems on the official leaderboards (Table 3 and 5). Notably, the top two systems, T5 (Raffel et al., 2020) and UnifiedQA (Khashabi et al., 2020), are trained with more data and use 8 x to 30 x more parameters than our model (ours has $\sim 360 \mathrm{M}$ parameters). Excluding these and ensemble systems, our model is comparable in size and amount of data to other systems, and achieves the top performance on the two datasets.</p>
<h3>4.6 Analysis</h3>
<h3>4.6.1 Ablation studies</h3>
<p>Table 6 summarizes the ablation study conducted on each of our model components (§3.1, §3.2, §3.3), using the CommonsenseQA IHdev set.</p>
<p>Graph connection (top left table): The first key component of QA-GNN is the joint graph that connects the $z$ node (QA context) to QA entity nodes $V_{q, a}$ in the $\operatorname{KG}(\S 3.1)$. Without these edges, the QA context and KG cannot mutually update their representations, hurting the performance: 76.5\% $\rightarrow 74.8 \%$, which is close to the previous LM+KG system, MHGRN. If we connected $z$ to all the nodes in the KG (not just QA entities), the performance is comparable or drops slightly ( $-0.16 \%$ ).</p>
<p>KG node relevance scoring (top right table): We find the relevance scoring of KG nodes (§3.2) provides a boost: $75.56 \% \rightarrow 76.54 \%$. As a variant of the relevance scoring in Eq. 1, we also experimented with obtaining a contextual embedding $\boldsymbol{w}<em _sub="{sub" _text="\text">{v}$ for each node $v \in \mathcal{V}</em>}}$ and adding to the node features: $\boldsymbol{w<em _enc="{enc" _text="\text">{v}=f</em>(v)\right]\right)$. However, we find that it does not perform as well ( $76.31 \%$ ), and using both the relevance score and contextual embedding performs on par with using the score alone, suggesting that the score has a sufficient information in our tasks; hence, our final system simply uses the relevance score.}}\left(\left[\operatorname{text}(z) ; \operatorname{text</p>
<p>GNN architecture (bottom tables): We ablate the information of node type, relation, and relevance score from the attention and message computation in the GNN (§3.3). The results suggest that all these features improve the model performance. For the number of GNN layers, we find $L=5$ works the best on the dev set. Our intuition is that 5 layers allow various message passing or reasoning patterns between the QA context $(z)$ and KG, such as " $z \rightarrow 3$ hops on KG nodes $\rightarrow z$ ".</p>
<h3>4.6.2 Model interpretability</h3>
<p>We aim to interpret QA-GNN's reasoning process by analyzing the node-to-node attention weights induced by the GNN. Figure 4 shows two examples. In (a), we perform Best First Search (BFS) on the working graph to trace high attention weights from the QA context node ( $\mathbf{Z}$; purple) to Question entity nodes (blue) to Other (gray) or Answer choice entity nodes (orange), which reveals that the QA context $z$ attends to "elevator" and "basement" in the KG, "elevator" and "basement" both attend strongly to "building", and "building" attends to "office building", which is our final answer. In (b),</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Interpreting QA-GNN's reasoning process by analyzing the node-to-node attention weights induced by the GNN. Darker and thicker edges indicate higher attention weights.
we use BFS to trace attention weights from two directions: $\mathbf{Z} \rightarrow \mathbf{Q} \rightarrow \mathbf{O}$ and $\mathbf{Z} \rightarrow \mathbf{A} \rightarrow \mathbf{O}$, which reveals concepts ("sea" and "ocean") in the KG that are not necessarily mentioned in the QA context but bridge the reasoning between the question entity ("crab") and answer choice entity ("salt water"). While prior KG reasoning models (Lin et al., 2019; Feng et al., 2020) enumerate individual paths in the KG for model interpretation, QA-GNN is not specific to paths, and helps to find more general reasoning structures (e.g., a KG subgraph with multiple anchor nodes as in example (a)).</p>
<h3>4.6.3 Structured reasoning</h3>
<p>Structured reasoning, e.g., precise handling of negation or entity substitution (e.g., "hair" $\rightarrow$ "art" in Figure 5b) in question, is crucial for making robust predictions. Here we analyze QA-GNN's ability to perform structured reasoning and compare with baselines (fine-tuned LMs and existing LM+KG models).</p>
<p>Quantitative analysis. Table 7 compares model performance on questions containing negation words (e.g., no, not, nothing, unlikely), taken from the CommonsenseQA IHtest set. We find that previous LM+KG models (KagNet, MHGRN) provide limited improvements over RoBERTa on questions with negation ( $+0.6 \%$ ); whereas QA-GNN exhibits a bigger boost ( $+4.6 \%$ ), suggesting its strength</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">IHtest-Acc. <br> (Overall)</th>
<th style="text-align: center;">IHtest-Acc. <br> (Question w/ negation)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RoBERTa-large (w/o KG)</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">54.2</td>
</tr>
<tr>
<td style="text-align: left;">+ KagNet</td>
<td style="text-align: center;">$69.0(+0.3)$</td>
<td style="text-align: center;">$54.2(+0.0)$</td>
</tr>
<tr>
<td style="text-align: left;">+ MHGRN</td>
<td style="text-align: center;">$71.1(+2.4)$</td>
<td style="text-align: center;">$54.8(+0.6)$</td>
</tr>
<tr>
<td style="text-align: left;">+ QA-GNN (Ours)</td>
<td style="text-align: center;">$73.4(+4.7)$</td>
<td style="text-align: center;">$\mathbf{5 8 . 8 (+ 4 . 6 )}$</td>
</tr>
<tr>
<td style="text-align: left;">+ QA-GNN (no edge</td>
<td style="text-align: center;">$71.5(+2.8)$</td>
<td style="text-align: center;">$55.1(+0.9)$</td>
</tr>
<tr>
<td style="text-align: left;">between Z and KG)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 7: Performance on questions with negation in CommonsenseQA. () shows the difference with RoBERTa. Existing LM+KG methods (KagNet, MHGRN) provide limited improvements over RoBERTa $(+0.6 \%)$; QA-GNN exhibits a bigger boost ( $+4.6 \%$ ), suggesting its strength in structured reasoning.
in structured reasoning. We hypothesize that QAGNN's joint updates of the representations of the QA context and KG (during GNN message passing) allows the model to integrate semantic nuances expressed in language. To further study this hypothesis, we remove the connections between $z$ and KG nodes from our QA-GNN (Table 7 bottom): now the performance on negation becomes close to the prior work, MHGRN, suggesting that the joint message passing helps for performing structured reasoning.
Qualitative analysis. Figure 5 shows a case study to analyze our model's behavior for structured reasoning. The question on the left contains negation "not used for hair", and the correct answer is "B. art supply". We observe that in the 1st layer of QA-GNN, the attention from $z$ to question entities ("hair", "round brush") is diffuse. After multiples rounds of message passing on the working graph, $z$ attends strongly to "round brush" in the final layer of the GNN, but weakly to the negated entity "hair". The model correctly predicts the answer "B. art supply". Next, given the original question on the left, we (a) drop the negation or (b) modify the topic entity ("hair" $\rightarrow$ "art"). In (a), $z$ now attends strongly to "hair", which is not negated anymore. The model predicts the correct answer "A. hair brush". In (b), we observe that QA-GNN recognizes the same structure as the original question (with only the entity swapped): $z$ attends weakly to the negated entity ("art") like before, and the model correctly predicts "A. hair brush" over "B. art supply".</p>
<p>Table 8 shows additional examples, where we compare QA-GNN's predictions with the LM baseline (RoBERTa). We observe that RoBERTa tends to make the same prediction despite the modifications we make to the original questions (e.g., drop/insert negation, change an entity); on the other hand, QA-GNN adapts predictions to the modifications correctly (except for double negation</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Analysis of QA-GNN's behavior for structured reasoning. Given an original question (left), we modify its negation (middle) or topic entity (right): we find that QA-GNN adapts attention weights and final predictions accordingly, suggesting its capability to handle structured reasoning.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Example (Original taken from CommonsenseQA Dev)</th>
<th style="text-align: center;">RoBERTa Prediction</th>
<th style="text-align: center;">Our Prediction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">[Original] If it is not used for hair, a round brush is an example of what? <br> A. hair brush B. art supply</td>
<td style="text-align: center;">A. hair brush ( $\boldsymbol{\&amp;}$ )</td>
<td style="text-align: center;">B. art supply $(\checkmark)$</td>
</tr>
<tr>
<td style="text-align: center;">[Negation flip] If it is used for hair, a round brush is an example of what?</td>
<td style="text-align: center;">A. hair brush ( just no change?)</td>
<td style="text-align: center;">A. hair brush $(\checkmark)$</td>
</tr>
<tr>
<td style="text-align: center;">[Entity change] If it is not used for art, a round brush is an example of what?</td>
<td style="text-align: center;">A. hair brush ( just no change?)</td>
<td style="text-align: center;">A. hair brush $(\checkmark)$</td>
</tr>
<tr>
<td style="text-align: center;">[Original] If you have to read a book that is very dry you may become what? <br> A. interested B. bored</td>
<td style="text-align: center;">B. bored $(\checkmark)$</td>
<td style="text-align: center;">B. bored $(\checkmark)$</td>
</tr>
<tr>
<td style="text-align: center;">[Negation ver 1] If you have to read a book that is very dry you may not become what?</td>
<td style="text-align: center;">B. bored $(\&amp;)$</td>
<td style="text-align: center;">A. interested $(\checkmark)$</td>
</tr>
<tr>
<td style="text-align: center;">[Negation ver 2] If you have to read a book that is not dry you may become what?</td>
<td style="text-align: center;">B. bored $(\&amp;)$</td>
<td style="text-align: center;">A. interested $(\checkmark)$</td>
</tr>
<tr>
<td style="text-align: center;">[Double negation] If you have to read a book that is not dry you may not become what?</td>
<td style="text-align: center;">B. bored ( $\&amp;$ )</td>
<td style="text-align: center;">A. interested $(\&amp;)$</td>
</tr>
</tbody>
</table>
<p>Table 8: Case study of structured reasoning, comparing predictions by RoBERTa and our model (RoBERTa + QA-GNN). Our model correctly handles changes in negation and topic entities.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">IHtest-Acc. <br> (Question w/ <br> $\leq 10$ entities)</th>
<th style="text-align: center;">IHtest-Acc. <br> (Question w/ <br> $&gt;10$ entities)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RoBERTa-large (w/o KG)</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">70.0</td>
</tr>
<tr>
<td style="text-align: left;">+ MHGRN</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">70.1</td>
</tr>
<tr>
<td style="text-align: left;">+ QA-GNN (w/o node <br> relevance score)</td>
<td style="text-align: center;">$72.8(+1.3)$</td>
<td style="text-align: center;">$71.5(+1.4)$</td>
</tr>
<tr>
<td style="text-align: left;">+ QA-GNN (w/ node <br> relevance score; final system)</td>
<td style="text-align: center;">$73.4(+1.9)$</td>
<td style="text-align: center;">$\mathbf{7 3 . 5 (+ 3 . 4 )}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Performance on questions with fewer/more entities in CommonsenseQA. () shows the difference with MHGRN (LM+KG baseline). KG node relevance scoring ( $\$ 3.2$ ) boosts the performance on questions containing more entities (i.e. larger retrieved KG).
in the table bottom, which is a future work).</p>
<h3>4.6.4 Effect of KG node relevance scoring</h3>
<p>We find that KG node relevance scoring ( $\$ 3.2$ ) is helpful when the retrieved $\mathrm{KG}\left(\mathcal{G}<em _sub="{sub" _text="\text">{\text {sub }}\right)$ is large. Table 9 shows model performance on questions containing fewer $(\leq 10)$ or more $(&gt;10)$ entities in the CommonsenseQA IHtest set (on average, the former and latter result in 90 and 160 nodes in $\mathcal{G}</em>$, respectively). Existing LM+KG models such as MHGRN achieve limited performance on questions with more entities due to the size and noisiness of retrieved KGs: $70.1 \%$ accuracy vs $71.5 \%$ accuracy on questions with fewer entities. KG node relevance scoring mitigates this bottleneck, reducing the accuracy discrepancy: $73.5 \%$ and $73.4 \%$ accuracy on questions with more/fewer entities, respectively.}</p>
<h2>5 Related work and discussion</h2>
<p>Knowledge-aware methods for NLP. Various works have studied methods to augment NLP systems with knowledge. Existing works (Pan et al., 2019; Ye et al., 2019; Petroni et al., 2019; Bosselut et al., 2019) study pre-trained LMs' potential as latent knowledge bases. To provide more explicit and interpretable knowledge, several works integrate structured knowledge (KGs) into LMs (Mihaylov and Frank, 2018; Lin et al., 2019; Wang et al., 2019a; Yang et al., 2019; Wang et al., 2020b; Bosselut et al., 2021).</p>
<p>Question answering with LM+KG. In particular, a line of works propose LM+KG methods for question answering. Most closely related to ours are works by Lin et al. (2019); Feng et al. (2020); Lv et al. (2020). Our novelties are (1) the joint graph of QA context and KG, on which we mutually update the representations of the LM and KG; and (2) language-conditioned KG node relevance scoring. Other works on scoring or pruning KG nodes/paths rely on graph-based metrics such as PageRank, centrality, and off-the-shelf KG embeddings (Paul and Frank, 2019; Fadnis et al., 2019; Bauer et al., 2018; Lin et al., 2019), without reflecting the QA context.</p>
<p>Other QA tasks. Several works study other forms of question answering tasks, e.g., passagebased QA, where systems identify answers using given or retrieved documents (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018), and</p>
<p>KBQA, where systems perform semantic parsing of a given question and execute the parsed queries on knowledge bases (Berant et al., 2013; Yih et al., 2016; Yu et al., 2018). Different from these tasks, we approach question answering using knowledge available in LMs and KGs.</p>
<p>Knowledge representations. Several works study joint representations of external textual knowledge (e.g., Wikipedia articles) and structured knowledge (e.g., KGs) (Riedel et al., 2013; Toutanova et al., 2015; Xiong et al., 2019; Sun et al., 2019; Wang et al., 2019b). The primary distinction of our joint graph representation is that we construct a graph connecting each question and KG rather than textual and structural knowledge, approaching a complementary problem to the above works.</p>
<p>Graph neural networks (GNNs). GNNs have been shown to be effective for modeling graphbased data. Several works use GNNs to model the structure of text (Yasunaga et al., 2017; Zhang et al., 2018; Yasunaga and Liang, 2020) or KGs (Wang et al., 2020a). In contrast to these works, QA-GNN jointly models the language and KG. Graph Attention Networks (GATs) (Veličković et al., 2018) perform attention-based message passing to induce graph representations. We build on this framework, and further condition the GNN on the language input by introducing a QA context node ( $\S 3.1$ ), KG node relevance scoring ( $\S 3.2$ ), and joint update of the KG and language representations (§3.3).</p>
<h2>6 Conclusion</h2>
<p>We presented QA-GNN, an end-to-end question answering model that leverages LMs and KGs. Our key innovations include (i) Relevance scoring, where we compute the relevance of KG nodes conditioned on the given QA context, and (ii) Joint reasoning over the QA context and KGs, where we connect the two sources of information via the working graph, and jointly update their representations through GNN message passing. Through both quantitative and qualitative analyses, we showed QA-GNN's improvements over existing LM and LM+KG models on question answering tasks, as well as its capability to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.</p>
<h2>Acknowledgment</h2>
<p>We thank Rok Sosic, Weihua Hu, Jing Huang, Michele Catasta, members of the SNAP research group, P-Lambda group and Project MOWGLI
team, as well as our anonymous reviewers for valuable feedback.</p>
<p>We gratefully acknowledge the support of DARPA under Nos. N660011924033 (MCS); Funai Foundation Fellowship; ARO under Nos. W911NF-16-1-0342 (MURI), W911NF-16-1-0171 (DURIP); NSF under Nos. OAC-1835598 (CINES), OAC1934578 (HDR), CCF-1918940 (Expeditions), IIS-2030477 (RAPID); Stanford Data Science Initiative, Wu Tsai Neuro-sciences Institute, Chan Zuckerberg Biohub, Amazon, JP-Morgan Chase, Docomo, Hitachi, JD.com, KDDI, NVIDIA, Dell, Toshiba, and United Health Group. Hongyu Ren is supported by Masason Foundation Fellowship and the Apple PhD Fellowship. Jure Leskovec is a Chan Zuckerberg Biohub investigator.</p>
<h2>Reproducibility</h2>
<p>All code and data are available at
https://github.com/michiyasunaga/qagnn. Experiments are available at
https://worksheets.
codalab.org/worksheets/
0xf215deb05edf44a2ac353c711f52a25f.</p>
<h2>References</h2>
<p>Pratyay Banerjee and Chitta Baral. 2020. Knowledge fusion and semantic knowledge ranking for open domain question answering. arXiv preprint arXiv:2004.03101.</p>
<p>Pratyay Banerjee, Kuntal Kumar Pal, Arindam Mitra, and Chitta Baral. 2019. Careful selection of knowledge to solve open book question answering. In Association for Computational Linguistics (ACL).</p>
<p>Junwei Bao, Nan Duan, Zhao Yan, Ming Zhou, and Tiejun Zhao. 2016. Constraint-based question answering with knowledge graph. In International Conference on Computational Linguistics (COLING).</p>
<p>Lisa Bauer, Yicheng Wang, and Mohit Bansal. 2018. Commonsense for generative multi-hop question answering tasks. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In SIGMOD.</p>
<p>Antoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko.</p>
<ol>
<li>Translating embeddings for modeling multirelational data. In Advances in Neural Information Processing Systems (NeurIPS).</li>
</ol>
<p>Antoine Bosselut, Ronan Le Bras, and Yejin Choi. 2021. Dynamic neuro-symbolic knowledge graph construction for zero-shot commonsense question answering. In Proceedings of the AAAI Conference on Artificial Intelligence.</p>
<p>Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Çelikyilmaz, and Yejin Choi. 2019. Comet: Commonsense transformers for automatic knowledge graph construction. In Association for Computational Linguistics (ACL).</p>
<p>Peter Clark, Oren Etzioni, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, Niket Tandon, et al. 2019. From'f'to'a'on the ny regents science exams: An overview of the aristo project. arXiv preprint arXiv:1909.01958.</p>
<p>Kshitij Fadnis, Kartik Talamadupula, Pavan Kapanipathi, Haque Ishfaq, Salim Roukos, and Achille Fokoue. 2019. Heuristics for interpretable knowledge graph contextualization. arXiv preprint arXiv:1911.02085.</p>
<p>Yanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, and Xiang Ren. 2020. Scalable multi-hop relational reasoning for knowledge-aware question answering. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Kelvin Guu, John Miller, and Percy Liang. 2015. Traversing knowledge graphs in vector space. Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning (ICML).</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Association for Computational Linguistics (ACL).</p>
<p>Nora Kassner and Hinrich Schütze. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Association for Computational Linguistics (ACL).</p>
<p>Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. Unifiedqa: Crossing format boundaries with a single qa system. In Findings of EMNLP.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations (ICLR).</p>
<p>Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. 2019. Kagnet: Knowledge-aware graph networks for commonsense reasoning. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. 2020. On the variance of the adaptive learning rate and beyond. In International Conference on Learning Representations (ICLR).</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Shangwen Lv, Daya Guo, Jingjing Xu, Duyu Tang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, and Songlin Hu. 2020. Graph-based reasoning over heterogeneous external knowledge for commonsense question answering. In Proceedings of the AAAI Conference on Artificial Intelligence.</p>
<p>Kaixin Ma, Jonathan Francis, Quanyang Lu, Eric Nyberg, and Alessandro Oltramari. 2019. Towards generalizable neuro-symbolic systems for commonsense question answering. arXiv preprint arXiv:1910.14087.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Todor Mihaylov and Anette Frank. 2018. Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge. In Association for Computational Linguistics (ACL).</p>
<p>Xiaoman Pan, Kai Sun, Dian Yu, Jianshu Chen, Heng Ji, Claire Cardie, and Dong Yu. 2019. Improving question answering with external knowledge. arXiv preprint arXiv:1902.00993.</p>
<p>Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty of training recurrent neural networks. In International conference on machine learning (ICML), pages 1310-1318.</p>
<p>Debjit Paul and Anette Frank. 2019. Ranking and selecting multi-hop knowledge paths to better predict human needs. In North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2019. Language models as knowledge bases? In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text</p>
<p>transformer. Journal of Machine Learning Research (JMLR).</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Hongyu Ren, Weihua Hu, and Jure Leskovec. 2020. Query2box: Reasoning over knowledge graphs in vector space using box embeddings. In International Conference on Learning Representations (ICLR).</p>
<p>Hongyu Ren and Jure Leskovec. 2020. Beta embeddings for multi-hop logical reasoning in knowledge graphs. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M Marlin. 2013. Relation extraction with matrix factorization and universal schemas. In North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. 2017. A simple neural network module for relational reasoning. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolutional networks. In European Semantic Web Conference.</p>
<p>Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence.</p>
<p>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research (JMLR), 15(1):1929-1958.</p>
<p>Haitian Sun, Tania Bedrax-Weiss, and William W Cohen. 2019. Pullnet: Open domain question answering with iterative retrieval on knowledge bases and text. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William W Cohen. 2018. Open domain question answering using early fusion of knowledge bases and text. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and Michael Gamon. 2015. Representing text for joint embedding of text and knowledge bases. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph attention networks. In International Conference on Learning Representations (ICLR).</p>
<p>Hongwei Wang, Hongyu Ren, and Jure Leskovec. 2020a. Entity context and relational paths for knowledge graph completion. arXiv preprint arXiv:2002.06757.</p>
<p>Peifeng Wang, Nanyun Peng, Pedro Szekely, and Xiang Ren. 2020b. Connecting the dots: A knowledgeable path generator for commonsense question answering. arXiv preprint arXiv:2005.00691.</p>
<p>Xiaoyan Wang, Pavan Kapanipathi, Ryan Musa, Mo Yu, Kartik Talamadupula, Ibrahim Abdelaziz, Maria Chang, Achille Fokoue, Bassem Makni, Nicholas Mattei, et al. 2019a. Improving natural language inference using external knowledge in the science questions domain. In Proceedings of the AAAI Conference on Artificial Intelligence.</p>
<p>Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2019b. Kepler: A unified model for knowledge embedding and pretrained language representation. Transactions of the Association for Computational Linguistics (TACL).</p>
<p>Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and William Yang Wang. 2019. Improving question answering over incomplete kbs with knowledgeaware reader. In Association for Computational Linguistics (ACL).</p>
<p>An Yang, Quan Wang, Jing Liu, Kai Liu, Yajuan Lyu, Hua Wu, Qiaoqiao She, and Sujian Li. 2019. Enhancing pre-trained language representations with rich knowledge for machine reading comprehension. In Association for Computational Linguistics (ACL).</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Michihiro Yasunaga and Percy Liang. 2020. Graphbased, self-supervised program repair from diagnostic feedback. In International Conference on Machine Learning (ICML).</p>
<p>Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush Pareek, Krishnan Srinivasan, and Dragomir Radev. 2017. Graph-based neural multi-document summarization. In Conference on Computational Natural Language Learning (CoNLL).</p>
<p>Zhi-Xiu Ye, Qian Chen, Wen Wang, and Zhen-Hua Ling. 2019. Align, mask and select: A simple method for incorporating commonsense knowledge into language representation models. arXiv preprint arXiv:1908.06725.</p>
<p>Wen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei Chang, and Jina Suh. 2016. The value of semantic parse labeling for knowledge base question answering. In Association for Computational Linguistics (ACL).</p>
<p>Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. 2018. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Yuhao Zhang, Peng Qi, and Christopher D Manning. 2018. Graph convolution over pruned dependency trees improves relation extraction. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Thomas Goldstein, and Jingjing Liu. 2020. Freelb: Enhanced adversarial training for language understanding. In International Conference on Learning Representations (ICLR).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ OpenBookQA provides an extra corpus of scientific facts in a textual form. AristoRoBERTa uses the facts corresponding to each question, prepared by Clark et al. (2019), as an&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
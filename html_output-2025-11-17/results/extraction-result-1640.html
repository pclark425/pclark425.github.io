<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1640 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1640</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1640</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-272708823</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.12061v1.pdf" target="_blank">Generalized Robot Learning Framework</a></p>
                <p><strong>Paper Abstract:</strong> Imitation based robot learning has recently gained significant attention in the robotics field due to its theoretical potential for transferability and generalizability. However, it remains notoriously costly, both in terms of hardware and data collection, and deploying it in real-world environments demands meticulous setup of robots and precise experimental conditions. In this paper, we present a low-cost robot learning framework that is both easily reproducible and transferable to various robots and environments. We demonstrate that deployable imitation learning can be successfully applied even to industrial-grade robots, not just expensive collaborative robotic arms. Furthermore, our results show that multi-task robot learning is achievable with simple network architectures and fewer demonstrations than previously thought necessary. As the current evaluating method is almost subjective when it comes to real-world manipulation tasks, we propose Voting Positive Rate (VPR) - a novel evaluation strategy that provides a more objective assessment of performance. We conduct an extensive comparison of success rates across various self-designed tasks to validate our approach. To foster collaboration and support the robot learning community, we have open-sourced all relevant datasets and model checkpoints, available at huggingface.co/ZhiChengAI.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1640.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1640.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sim-to-Real (paper mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-real transfer for robot learning (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper discusses sim-to-real transfer as a commonly proposed solution for obtaining large-scale training data by training in simulation and adapting to the real world, but explicitly chooses not to use simulation in its experiments, arguing simulations are costly to set up for low-cost hardware and often fail to capture unforeseen real-world events.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generalized Robot Learning Framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Industrial-grade robotic arm (unspecified model used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A general-purpose industrial robotic arm used for real-world imitation learning experiments; integrated via a custom SDK and cable connection, instrumented with two RGB-D cameras (Intel RealSense D415) — one wrist-mounted and one global view — and teleoperated during data collection with an Oculus Quest 2 controller.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>No simulation environment was used in the authors' experiments; simulation is discussed conceptually in related work as providing graphics and physics modeling but was intentionally not employed due to practical constraints on low-cost hardware and the inevitability of real-world testing.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>not applicable (discussed conceptually); authors note literature includes high-fidelity physics and photorealistic rendering but do not instantiate such simulations here</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>N/A in this paper (authors refer to literature where physics and graphics fidelity are improved, but do not model them themselves)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>N/A in this paper — authors explicitly state simulations often omit unforeseen real-world events and require specialized setup/tuning, implying typical simplifications include unmodeled disturbances, imperfect contact dynamics, and unmodeled sensor/actuator idiosyncrasies</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical lab/bench setup using everyday, low-cost objects for 10 manipulation tasks (PickPlace, BlockPick, Basketball, RingToss, CupStack, ShapeDistinguish, WhichCube, PickSmall, PickBig, PickBigV2); two RealSense D415 cameras (wrist and global), consistent hardware across data collection and deployment, and demonstrations collected by human teleoperation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>None transferred from simulation; tasks were learned directly from real-world human demonstrations (imitation learning) for manipulation skills such as picking, placing, sorting, stacking, and size/color/shape discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Imitation learning / behavioral cloning-style offline learning using Denoising Diffusion Probabilistic Models (DDPMs) for trajectory prediction, with perception encoders (ResNet variants, FPN) and transformer/CNN denoisers.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Voting Positive Rate (VPR) — human-evaluation voting system where four evaluators independently judge each case; a case is successful only if all evaluators vote positively; success rates per task (VPR) reported as percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Authors cite general sim-to-real gap factors: unforeseen real-world events not captured by simulators, the need for specialized simulation setup and tuning, sensor and camera placement differences, calibration errors, contact dynamics mismodeling, and timing/actuator differences; paper notes camera-position sensitivity in typical imitation setups.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Because the authors did not use simulation, they identify alternative enabling conditions for robust real-world performance: consistent hardware between data collection and deployment, sufficient and diverse real demonstrations (dataset scaling), multi-angle camera data and fine-tuning on real data to improve generalization across views, careful checkpoint selection via real-world evaluation, and model architecture choices (FPN+Transformer) that improved real-task VPR.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>No quantitative simulation fidelity requirements given; authors state only that simulation requires meticulous design (physical dynamics modeling and success-metric definitions) and that simulations often fail to capture unforeseen events, motivating their real-data approach.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper argues that while sim-to-real is a common approach and simulation fidelity (graphics/physics) has improved in the literature, practical sim-to-real is difficult for low-cost setups because simulators can miss unforeseen real events and require specialized tuning. Consequently, the authors deliberately avoid simulation and demonstrate that end-to-end real-world imitation learning with sufficient diverse demonstrations, consistent hardware, multi-view real data, and appropriate model architectures can produce transferable manipulation skills for an industrial-grade arm; dataset scaling and real-data diversity had larger impacts on real performance than model scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalized Robot Learning Framework', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Imitation learning for generalizable self-driving policy with sim-to-real transfer <em>(Rating: 2)</em></li>
                <li>Towards closing the sim-to-real gap in collaborative multi-robot deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Reconciling reality through simulation: A real-to-sim-to-real approach for robust manipulation <em>(Rating: 2)</em></li>
                <li>Blind spot detection for safe sim-to-real transfer <em>(Rating: 2)</em></li>
                <li>Airsim: High-fidelity visual and physical simulation for autonomous vehicles <em>(Rating: 1)</em></li>
                <li>CARLA: An open urban driving simulator <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1640",
    "paper_id": "paper-272708823",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Sim-to-Real (paper mention)",
            "name_full": "Sim-to-real transfer for robot learning (mentioned in related work)",
            "brief_description": "The paper discusses sim-to-real transfer as a commonly proposed solution for obtaining large-scale training data by training in simulation and adapting to the real world, but explicitly chooses not to use simulation in its experiments, arguing simulations are costly to set up for low-cost hardware and often fail to capture unforeseen real-world events.",
            "citation_title": "Generalized Robot Learning Framework",
            "mention_or_use": "mention",
            "agent_system_name": "Industrial-grade robotic arm (unspecified model used in experiments)",
            "agent_system_description": "A general-purpose industrial robotic arm used for real-world imitation learning experiments; integrated via a custom SDK and cable connection, instrumented with two RGB-D cameras (Intel RealSense D415) — one wrist-mounted and one global view — and teleoperated during data collection with an Oculus Quest 2 controller.",
            "domain": "general robotics manipulation",
            "virtual_environment_name": null,
            "virtual_environment_description": "No simulation environment was used in the authors' experiments; simulation is discussed conceptually in related work as providing graphics and physics modeling but was intentionally not employed due to practical constraints on low-cost hardware and the inevitability of real-world testing.",
            "simulation_fidelity_level": "not applicable (discussed conceptually); authors note literature includes high-fidelity physics and photorealistic rendering but do not instantiate such simulations here",
            "fidelity_aspects_modeled": "N/A in this paper (authors refer to literature where physics and graphics fidelity are improved, but do not model them themselves)",
            "fidelity_aspects_simplified": "N/A in this paper — authors explicitly state simulations often omit unforeseen real-world events and require specialized setup/tuning, implying typical simplifications include unmodeled disturbances, imperfect contact dynamics, and unmodeled sensor/actuator idiosyncrasies",
            "real_environment_description": "Physical lab/bench setup using everyday, low-cost objects for 10 manipulation tasks (PickPlace, BlockPick, Basketball, RingToss, CupStack, ShapeDistinguish, WhichCube, PickSmall, PickBig, PickBigV2); two RealSense D415 cameras (wrist and global), consistent hardware across data collection and deployment, and demonstrations collected by human teleoperation.",
            "task_or_skill_transferred": "None transferred from simulation; tasks were learned directly from real-world human demonstrations (imitation learning) for manipulation skills such as picking, placing, sorting, stacking, and size/color/shape discrimination.",
            "training_method": "Imitation learning / behavioral cloning-style offline learning using Denoising Diffusion Probabilistic Models (DDPMs) for trajectory prediction, with perception encoders (ResNet variants, FPN) and transformer/CNN denoisers.",
            "transfer_success_metric": "Voting Positive Rate (VPR) — human-evaluation voting system where four evaluators independently judge each case; a case is successful only if all evaluators vote positively; success rates per task (VPR) reported as percentages.",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Authors cite general sim-to-real gap factors: unforeseen real-world events not captured by simulators, the need for specialized simulation setup and tuning, sensor and camera placement differences, calibration errors, contact dynamics mismodeling, and timing/actuator differences; paper notes camera-position sensitivity in typical imitation setups.",
            "transfer_enabling_conditions": "Because the authors did not use simulation, they identify alternative enabling conditions for robust real-world performance: consistent hardware between data collection and deployment, sufficient and diverse real demonstrations (dataset scaling), multi-angle camera data and fine-tuning on real data to improve generalization across views, careful checkpoint selection via real-world evaluation, and model architecture choices (FPN+Transformer) that improved real-task VPR.",
            "fidelity_requirements_identified": "No quantitative simulation fidelity requirements given; authors state only that simulation requires meticulous design (physical dynamics modeling and success-metric definitions) and that simulations often fail to capture unforeseen events, motivating their real-data approach.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "The paper argues that while sim-to-real is a common approach and simulation fidelity (graphics/physics) has improved in the literature, practical sim-to-real is difficult for low-cost setups because simulators can miss unforeseen real events and require specialized tuning. Consequently, the authors deliberately avoid simulation and demonstrate that end-to-end real-world imitation learning with sufficient diverse demonstrations, consistent hardware, multi-view real data, and appropriate model architectures can produce transferable manipulation skills for an industrial-grade arm; dataset scaling and real-data diversity had larger impacts on real performance than model scaling.",
            "uuid": "e1640.0",
            "source_info": {
                "paper_title": "Generalized Robot Learning Framework",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Imitation learning for generalizable self-driving policy with sim-to-real transfer",
            "rating": 2,
            "sanitized_title": "imitation_learning_for_generalizable_selfdriving_policy_with_simtoreal_transfer"
        },
        {
            "paper_title": "Towards closing the sim-to-real gap in collaborative multi-robot deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "towards_closing_the_simtoreal_gap_in_collaborative_multirobot_deep_reinforcement_learning"
        },
        {
            "paper_title": "Reconciling reality through simulation: A real-to-sim-to-real approach for robust manipulation",
            "rating": 2,
            "sanitized_title": "reconciling_reality_through_simulation_a_realtosimtoreal_approach_for_robust_manipulation"
        },
        {
            "paper_title": "Blind spot detection for safe sim-to-real transfer",
            "rating": 2,
            "sanitized_title": "blind_spot_detection_for_safe_simtoreal_transfer"
        },
        {
            "paper_title": "Airsim: High-fidelity visual and physical simulation for autonomous vehicles",
            "rating": 1,
            "sanitized_title": "airsim_highfidelity_visual_and_physical_simulation_for_autonomous_vehicles"
        },
        {
            "paper_title": "CARLA: An open urban driving simulator",
            "rating": 1,
            "sanitized_title": "carla_an_open_urban_driving_simulator"
        }
    ],
    "cost": 0.00965925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Generalized Robot Learning Framework
18 Sep 2024</p>
<p>Jiahuan Yan 
ZhiCheng AI</p>
<p>Zhouyang Hong 
ZhiCheng AI</p>
<p>Yu Zhao 
ZhiCheng AI</p>
<p>Yu Tian 
Harvard University</p>
<p>Yunxin Liu 
Tsinghua University</p>
<p>Travis Davies 
ZhiCheng AI</p>
<p>Luhui Hu 
ZhiCheng AI</p>
<p>Generalized Robot Learning Framework
18 Sep 202456DD5E8C247A542C081592729048929DarXiv:2409.12061v1[cs.RO]
Imitation based robot learning has recently gained significant attention in the robotics field due to its theoretical potential for transferability and generalizability.However, it remains notoriously costly, both in terms of hardware and data collection, and deploying it in real-world environments demands meticulous setup of robots and precise experimental conditions.In this paper, we present a low-cost robot learning framework that is both easily reproducible and transferable to various robots and environments.We demonstrate that deployable imitation learning can be successfully applied even to industrial-grade robots, not just expensive collaborative robotic arms.Furthermore, our results show that multi-task robot learning is achievable with simple network architectures and fewer demonstrations than previously thought necessary.As the current evaluating method is almost subjective when it comes to real-world manipulation tasks, we propose Voting Positive Rate (VPR)-a novel evaluation strategy that provides a more objective assessment of performance.We conduct an extensive comparison of success rates across various selfdesigned tasks to validate our approach.To foster collaboration and support the robot learning community, we have opensourced all relevant datasets and model checkpoints, available at https://huggingface.co/ZhiChengAI</p>
<p>I. INTRODUCTION</p>
<p>Robotic research [1]- [3] has increasingly focused on the potential of using imitation learning [4] for robotic manipulation [5], driven in part by the growing integration of generative AI [6] into industry.However, the high cost of current robot learning pipelines has remained a significant barrier, limiting access to practical development and scaling up.Hence, we propose a low-cost real robot imitation learning framework that facilitates efficient data collection, training, and inference on a industrial-grade robotic arm with common household control devices, making it possible for a broader range of researchers and practitioners to engage in robotics innovation.</p>
<p>To rigorously evaluate the effectiveness of the proposed framework, we designed 10 distinct robotic tasks, each characterized by specific features tailored to real-world conditions.A comprehensive analysis of these tasks is provided, encompassing both the rationale behind their design and their empirical performance in deployment.Section III-D delves into the requirements and methodologies involved in the design of these tasks, while Section IV systematically examines how task-specific characteristics impact performance outcomes in real-world testing scenarios.</p>
<p>In addition to the versatility of using a general-purpose robotic arm that can meet the demands of various industrial scenarios, our framework also demonstrates model generalization.Specifically, we successfully enable a single checkpoint to perform multiple tasks by combining datasets Fig. 1: Overview of the framework: A real-world robot learning setup can be constructed using everyday household items, a robotic arm, a controller and two cameras.and applying minor adjustments to the training strategy, which are discussed in Section IV-C and Section IV-E.To support the broader research community, we also release the entire dataset generated through our framework.This dataset, encompassing diverse tasks and environmental conditions, serves as an additional resource for future research in robot learning, promoting reproducibility and enabling further advancements in the field.</p>
<p>Our main contributions can be summarized as:</p>
<p>• We introduce a novel low-cost imitation learning framework that is accessible even to individual researchers.• We collected over 4,000 episodes across 10 distinct real-world robotic tasks, which are publicly released alongside our findings on the correlation between task difficulty and performance.• We demonstrate model generalization by successfully enabling task adaptation through minimal dataset integration and slight modifications to the training process.</p>
<p>II. RELATED WORK</p>
<p>A. Imitation Learning</p>
<p>Imitation Learning (IL) [7], [8] is a prominent approach in robotics and autonomous systems, enabling agents to acquire complex behaviors by mimicking expert demonstrations [9]- [11].Among the various IL techniques, behavioral cloning (BC) has been widely used, framing the task as a supervised learning problem where actions are directly mapped from perceptions [12], [13].A well-known example is the ACT policy [14], which exemplifies explicit policy learning.However, explicit policies often struggle with multimodal behavior [15], as they tend to perform poorly in scenarios with diverse demonstrated actions.To address these limitations, recent work has explored implicit policies, such as utilizing energy-based models to map perceptions to actions [15], [16].While these models offer improved performance in handling multimodal behavior, they present challenges in training stability, particularly due to the requirement of generating negative samples for the Info-NCE loss [15].</p>
<p>Diffusion-based policies [1], [17] addresses the instability of implicit policies by modeling the gradient field of an implicit action score, eliminating the need for negative sampling and improving training stability.Similar to Diffusion Policy, we adopt Denoising Diffusion Probabilistic Models (DDPMs) [18] as a paradigm for action prediciton.It is well-satisfied with our demand of switching different model structures based on our exhaustive experiment.</p>
<p>B. End-to-End Robot Learning</p>
<p>A significant challenge in robotic learning is the availability of diverse and abundant training data.Sim-to-real transfer has been proposed as a solution, where robot models are initially trained in simulation and then adapted to realworld tasks using domain adaptation techniques [19]- [21].Advances in simulation software, particularly in realistic graphics and physics [22]- [24], have improved the fidelity of simulated data, making it more comparable to real-world data.However, sim-to-real approaches often fail to capture unforeseen real-world events [25], and require specialized skills for setting up and tuning simulations.</p>
<p>Alternatively, end-to-end robot learning offers more accessible, data-driven approach to robot training with imitation learning, as it eliminates many of the complexities of simto-real techniques.Projects such as Dobb-E [26], ALOHA [14], and UMI [27] have demonstrated successful learning from human demonstrations for end-to-end robot learning.However, these methods are either constrained by profes-sional setups (ALOHA, UMI) or do not involve industrialgrade robotic arms (Dobb-E).In this paper, we propose an end-to-end robot learning framework designed for beginners, capable of operating in less-than-ideal conditions, making robot learning more accessible to non-experts.</p>
<p>III. FRAMEWORK SETUP</p>
<p>A. Hardware Preparation</p>
<p>Our hardware devices for data collection and model deployment facilities are listed as follows:</p>
<p>• Robotic Arm: The robotic arm used in our experiments is an industrial-grade model, with a custom-built software development kit (SDK) and a cable connection for communication.While we refrain from disclosing the specific robot model, our framework is designed to be agnostic to the hardware platform.Thus, if our framework is deployable on this robot, it can be adapted for use on almost any robotic system.• Cameras: Two Intel RealSense D415 RGB-D cameras were utilized for frame acquisition.One camera was mounted on the end-effector of the robotic arm to provide a close-up perspective, while the second was positioned to offer a global view of the workspace.The camera type is not a strict requirement, any RGB camera can be used in place of the RealSense D415, depending on individual circumstances.• Controller: An Oculus Quest 2 right-hand controller was employed for data collection.Although the controller facilitated the robot's movement, the headset was also required to ensure a stable spatial coordinate system.The controller's "B" button was programmed to stop the robot's motion, enabling the operator to reposition the controller without interfering with the arm's operations.</p>
<p>This hardware configuration is not rigidly fixed and can be adapted by replacing components based on individual requirements.However, based on our perceptions, it is critical that the hardware used for both data collection and model deployment remain consistent to maintain alignment across the system.</p>
<p>B. Data Collection</p>
<p>Before arranging human manipulators to collect data for real-task, we took precautions to ensure the controller operated within an obstacle-free area to avoid introducing sudden errors in the control data.To synchronize the operator's movements with the robotic arm's movements in real space, we adjusted the orientation of the headset.This adjustment aligned the coordinate systems, providing an intuitive interaction experience where the controller's movements corresponded directly with the robotic arm's direction.</p>
<p>We employ a widely adopted strategy for robot data collection, where the trajectory of the robot is recorded alongside timestamps and corresponding video footage.The trajectory data includes the absolute position and orientation of the robot's end effector (x, y, z, ox, oy, oz), along with additional information indicating the gripper's state.In our setup, the gripper's state is represented using Pulse Width Modulation (PWM), which reflects the motor-driven force applied to the gripper.</p>
<p>During data collection, two operators are involved: one arranges the objects based on specific scenarios and their own intuition, while the other uses a controller to remotely operate the robot arm to complete the task.These operators are excluded from the human evaluation process, as discussed in Section III-E.The collected data is tagged with the operators' names to distinguish between identical tasks performed by different collectors, a distinction that serves an important purpose, which will be elaborated on in Section IV-D.</p>
<p>In our setup, the number of episodes for each task is not fixed; it is primarily determined by the complexity of the task, including factors such as logical intricacy, the number of objects involved, and the required number of generalization scenarios.As a general guideline, we use 10 episodes per generalization scenario, typically requiring around 100 demonstrations for a specific task.Under optimal conditions, this process takes approximately 0.5 to 1 hour of manual effort.The detailed number is listed in Table I.</p>
<p>C. Robot Policy System</p>
<p>Following the design of Diffusion Policy [1], we decouple the policy for robot control system into two components: the perception module, which processes information from the physical world and generates embeddings, and the action prediction module, which takes these embeddings as input and outputs corresponding trajectories.</p>
<p>1) Perception Module: Our perception data consists of visual inputs from two cameras: one mounted on the robotic arm's wrist for a first-person view, and the other providing a third-person perspective.Additionally, low-dimensional state information such as the position and orientation of the end effector and the gripper's PWM signal is included.This data represents the robot's perception of the physical world and is processed by a deep learning network to be transformed into embeddings.We experimented with several network architectures, including ResNet18, ResNet34, and ResNet34 configured as a Feature Pyramid Network (FPN) [28], [29].Of these, the FPN ResNet34 yielded the best performance, significantly enhancing visual feature representation by leveraging multiple resolutions.</p>
<p>2) Action Prediction Module: This model is designed to map the encoded perception data into a real trajectory that can effectively control the robot arm and gripper to complete tasks.In our current setup, we use the Denoising Diffusion Probabilistic Model (DDPM) [18] as the backbone, iteratively denoising initially randomized trajectories to make them practical for robot manipulation.We explored two main network architectures within the DDPM framework: Convolutional Neural Networks (CNN) and Transformers.</p>
<p>These two modules function together as a policy control system within our framework, but the choice of network architecture for each module is flexible and not strictly constrained to any specific design.</p>
<p>D. Task Design</p>
<p>Well-defined tasks are crucial for evaluating model performance and ensuring consistent, quantifiable results.In this study, we present 10 real-world tasks, as shown in Figure 3, using low-cost, easily accessible objects to facilitate easy replication.Each task targets specific model capabilities, such as color recognition in "BlockPick" and size differentiation in "PickSmall," allowing us to evaluate the model's ability to handle diverse features of scenarios in industrial demands.</p>
<p>The tasks are designed to balance simplicity in setup with complexity in execution, providing a broad range of challenges to test the model's generalization abilities.This approach ensures that the tasks are accessible to the wider research community while offering meaningful insights into the model's performance across different dimensions of perception and decision-making.</p>
<p>E. Voting Positive Rate</p>
<p>Evaluating model performance in real-world environments during training is challenging, as continuous real-time evaluation is often impractical.A common approach is to test models in simulation environments, which can help approximate real-world conditions.However, simulations require meticulous design, including the modeling of physical dynamics and defining success metrics.One key difficulty is determining when a model should be considered to have failed in simulation, a decision that is much easier for humans to make in real-world tests.We chose to forgo simulated evaluation not only because it is difficult to implement with low-cost hardware (designing a custom simulation environment for a personal robot arm may not be feasible), but also because real-world testing is inevitable when transitioning into industry for practical applications.As such, simulated evaluation was deemed unnecessary for the scope of our work.</p>
<p>When it comes to evaluating the model in the real environment, we propose the Voting Positive Rate as a way to mitigate the inherent subjectivity of human judgment.Similar to prior work [2], human evaluators assess whether the model successfully completes tasks, but we introduce a voting system to improve reliability.Four evaluators independently judge each task, which is broken down into several cases.Each case is evaluated five times, with initial conditions set by a person who was not involved in the data collection process.A test is deemed successful only if all evaluators vote positively; otherwise, it is classified as a failure.This voting system, referred to as the Voting Positive Rate, helps reduce bias and inconsistency in human evaluations.</p>
<p>F. Checkpoint Selection</p>
<p>Checkpoint selection in imitation learning remains a challenging problem, as optimizing the loss function does not necessarily correlate with achieving the highest success rate in real-world scenarios.As noted in [2], the loss curve during training often lacks a clear correlation with actual task performance, which is aligned with the phenomena we also observed in our experiments.Given the impracticality of evaluating model performance after every epoch in real-world tasks, our approach involves training the model overnight, saving checkpoints every 50 epochs, and subsequently evaluating each checkpoint to select the one with the highest success rate.</p>
<p>G. Model Deployment</p>
<p>The deployment of the trained model is currently executed on our PC, with the generated actions transferred to the robot via a cable.Our alignment strategy involves having the model predict actions along with their corresponding desired timestamps.The robot then moves based on both the action and the timestamp.If a timestamp is deemed invalid (e.g., if it exceeds the allotted time), the action is discarded accordingly.</p>
<p>IV. EXPERIMENTS AND RESULTS</p>
<p>A. Task Analysis</p>
<p>Following the standardized checkpoint selection method discussed in Section III-F, we present our analysis of the relationship between task success rates and task-specific characteristics.To minimize the impact of model architecture and training strategy, we utilized a consistent architecture and selected the optimal checkpoint based solely on performance, regardless of training duration.</p>
<p>Table I summarizes the manual feature extractions for each task, along with corresponding dataset sizes.While it is difficult to establish a direct linear relationship between task features and success rates, our findings suggest the following key perceptions:</p>
<p>1) Number of Demonstrations: The number of demonstrations significantly influences the final success rate.This is evident in tasks such as 'PickSmall,' 'PickBig,' and 'PickBigV2,' which serve as controlled experiments where the dataset size is the primary variable.The success rate increases as the dataset size grows, but plateaus at a certain threshold.2) Task Complexity: Task complexity is directly correlated with the Voting Positive Rate (VPR).As expected, tasks that require more complex decisionmaking-such as those involving multiple sequential steps-tend to be more challenging.To quantify this, we introduce a manually defined marker, Logic Step, which represents the number of logical deductions a human would need to make to control the robot arm.This concept is inspired by the use of Chain-of-Thought prompting in Large Language Models [30], [31].. 3) Feature Distinguishability: Based on practical experience, tasks that prominently feature color differentiation appear to benefit more from the ResNet-based perception encoder.This suggests that the current architecture is particularly effective for tasks where visual color features are critical.</p>
<p>B. Model Architecture Ablation Study</p>
<p>As aforementioned III-C, we decoupled the whole controlling system into perception module and action prediction module, and each of them can be instanced by certain network architectures.Hence, we conducted an ablation study to evaluate the performance of various model architectures by modifying both of them.As shown in Table I, the study compares the performance of ResNet18 with a CNN based module, and ResNet18, ResNet34, and FPN-based ResNet34 with transformer-based across different tasks.</p>
<p>For simpler tasks such as PickPlace and Basketball, the ResNet18+UNet architecture achieved relatively good VPR, with a notable 92% rate.This indicates that CNN-based noise prediction networks are effective when the task complexity is relatively low, likely due to their ability to capture local spatial patterns efficiently.</p>
<p>However, transformer-based architectures perform generally better compared to CNN-based.When task complexity increased, the outnum becomes larger.Transformers particularly excelled in the most complex tasks, such as PickSmall, where ResNet34+Transformer outperformed all other architectures with a 66.7% success rate.This confirms that transformers are well-suited for handling tasks requiring more sophisticated temporal and spatial reasoning, as their self-attention mechanism allows them to capture long-range dependencies more effectively than CNNs.</p>
<p>Among Transformer group, FPN+Transformer is considered as the optimal choice, as it generally perform better.For instance, in the CupStack task, the FPN+Transformer architecture outperformed others, achieving an 80% success rate, compared to 70% for ResNet34+Transformer and 57.5% for ResNet18+Transformer. Similarly, in the PickBig task, FPN+Transformer achieved 76.6%, significantly outperforming the ResNet18+Transformer (50%) and ResNet34+Transformer (26.7%).When simply enlarge the perception model (e.g., from ResNet18 to ResNet34), the improvement of performance is not significant, ResNet34 even perfor worse on some simple tasks (e.g., 'PickPlace', 'BlockPick' and 'Basketball').</p>
<p>The ablation study underscores that while CNN-based architectures may perform well in simpler environments, transformer-based architectures offer a clear advantage in complex, dynamic tasks.These results suggest that, for tasks requiring intricate action sequences and environmental interactions, transformer-based models should be prioritized for their robustness and performance.</p>
<p>C. Dataset Scaling Matters More Than Training Scaling</p>
<p>We explicitly explored the effects of scaling both the dataset size and the model architecture by introducing additional demonstrations and increasing the model's hidden dimensions (i.e., adding more learnable parameters) alongside extended training epochs.Interestingly, our results indicate that simply increasing the number of model parameters and training time does not lead to improved performance.In fact, this approach often results in a significant drop in performance, rendering the model's success rate unacceptably low.</p>
<p>Conversely, increasing the number of demonstrations consistently improves model performance, as evidenced by the results from the 'PickSmall', 'PickBig', and 'PickBigV2' tasks.These three tasks were deliberately designed to be similar, with the primary difference being the number of demonstrations.The results are compelling: by increasing the number of demonstrations from 60 to 120, the VPR improves by 9.9%.However, this improvement tends to plateau; when increasing the demonstrations from 120 to 240, the VPR only improves by 1.7%.In addition to expanding the dataset, we also experimented with using a larger transformer model, but the results were far from expected.Although we aimed to replicate the scaling laws' success observed in other domains by simultaneously increasing both dataset size and model complexity, our findings indicate that this strategy requires further exploration to be successfully applied.</p>
<p>D. Data Quality</p>
<p>During our experiments, we identified a recurring data quality issue, which we present here.When two data collectors independently gathered data for the same tasks, we observed significant variations in model performance, even when the demonstration lengths and training epochs were kept constant.These findings were consistent across most of the tasks we designed, particularly for tasks with longer demonstrations.While it is possible that this performance fluctuation could be attributed to the checkpoint selection problem mentioned earlier, we are inclined to believe it is more closely related to the proficiency level of the data collectors [1], [2].However, the definition of "proficiency" remains unclear to us.Therefore, we are simply reporting our perceptions here and leave this question open for further investigation by our peers.</p>
<p>E. Model Generalization</p>
<p>Multi-task Generalization: Similar to the early stages of deep learning, prior work in imitation learning has predominantly focused on training models for single, specific tasks.In contrast, we have developed a multi-task learning approach by combining two datasets and training on a preexisting checkpoint that was already fine-tuned on one of the tasks.The efficacy of this approach was demonstrated by fine-tuning the 'BlockPick' task using a model checkpoint previously trained on the 'Basketball' task for 650 epochs.Remarkably, the 'BlockPick' task was successfully completed after only 50 additional epochs of fine-tuning,  underscoring the efficiency and effectiveness of our multitask learning strategy.Environmental Scene Generalization: In standard imitation learning setups, demonstration data is typically collected with fixed camera positions, commonly involving one global view and another camera mounted on the end effector.This setup requires identical camera positions and view angles during inference to replicate the training conditions.Consequently, models trained under these constraints often exhibit limited adaptability and generalization to even minor variations in camera placement in real-world scenarios.By incorporating merely two distinct camera views, with one camera fixed and the other placed at two different positions (each providing 40 demonstrations), we fine-tuned the model for an additional 100 epochs.Notably, in the 'BlockPick' task, this fine-tuning with multi-angle camera data enabled the model to maintain high performance despite variations in camera positioning, including zero-shot [32] positions and angles that were entirely unseen during the training phase.This result indicates that the model achieved robust generalization across a range of environmental visual conditions.</p>
<p>V. FUTURE WORK</p>
<p>Imitation learning should be adaptable across various robotic platforms and industrial tasks, without relying on overly complex or impractical setups.In this study, we utilized a Diffusion Policy-inspired architecture with modifications to the perception module and action prediction module.Future direction could explore the utilization of human language instruction [3], [33], [34] to strengthen the logic deduction capacity.</p>
<p>Though our framework can work smoothly from data collection to model deployment, which provides a pipeline for efficient replicating based on data driven paradigm.One of the main obstacle is the required data size, and our demand is to decrease the data size needed while remaining the model performace.One of the convincing method is to leverage the capability of model pretrained on large, opensource robotic trajectory datasets, such as X-Embodiment [35], which allows policies to adapt to novel tasks and environments with minimal fine-tuning.</p>
<p>In summary, our future work will focus on progressively reducing the data dependency of our framework by leveraging advanced transfer learning models and methods.We believe this approach will contribute significantly to the robot learning community.</p>
<p>VI. CONCLUSION</p>
<p>In this work, we present a cost-effective and generalized framework for deploying robotic systems in industry-relevant tasks, significantly reducing hardware expenses compared to conventional research setups.Our methodology minimizes the time required for data collection and model training.This efficiency makes the framework highly accessible to a wide range of users, from academic researchers to industry practitioners.</p>
<p>We also provided detailed guidelines for task design, success rate evaluation, and optimal checkpoint selection.Our experiments demonstrated the feasibility of training multitask models on real-world tasks, and we observed that even minor adjustments to the model architecture can result in substantial performance improvements across different tasks.These findings contribute valuable insights into optimizing model architectures for diverse, complex environments.</p>
<p>In summary, we introduced a low-cost imitation learning framework supported by a dataset of 10 real-world tasks, designed to accelerate progress in embodied intelligence.By fostering research and open-source collaboration, we aim to enable the development of emergent capabilities in robotics, similar to those observed in large-scale language models, thus driving future advancements in autonomous systems.</p>
<p>Fig. 2 :
2
Fig.2: End-to-End Framework: The pipeline illustrates the end-to-end process for a cost-efficient imitation learning implementation, from hardware setup and task design to data collection, modeling and training, evaluation (Voting Positive Rate), and model deployment.This framework is designed to be structurally simple and economically feasible for deployment.</p>
<p>Fig. 3 :
3
Fig. 3: Offline training tasks.For each of the ten training tasks, we collected data for offline training.(a) PickPlace: The robot arm grabs a bottle and places it into a paper plate receptacle.(b) BlockPick: The robot arm picks the red cube to the red plate, then the green cube to the left plate.(c) Basketball: The robot arm picks up a tennis ball and places it into a toy basketball hoop.(d) RingToss: The robot places the red ring onto a white peg, followed by the yellow ring.(e) CupStack: The robot arm stacks the purple cup onto the blue one, then the unpainted cup on top.(f) ShapeDistinguish: The robot selects the correct foam shape based on the drawn shape and transfers it.(g) WhichCube: The robot arm places three pink cubes onto corresponding identity cards.(h) PickSmall: Out of three candidates, the robot arm picks the smallest cubes from two and places them on a plate.(i) PickBig: Out of three candidates, the robot arm picks the largest cubes from two and places them on a plate.(j) PickBigV2: Out of four candidates, the robot arm picks the largest cubes from two and places them on a plate.</p>
<p>TABLE I :
I
Overview of task characteristics: Object Num refers to the number of objects the robot interacts with; Recep Num indicates the number of possible changes to the receptacle; Cases specifies the number of initial conditions; Color, Size, and Shape indicate whether the task involves classifying these features; Logic Step represents the number of logical deductions required; Avg Length is the average duration of each demonstration video; Demo Num denotes the total number of demonstrations for each task.Model structure abbreviations: Res18/UNet refers to ResNet18 with a UNet architecture, Res18/TF(Transformer) refers to ResNet18 with a Transformerbased architecture, Res34/TF refers to ResNet34 with a Transformer-based architecture, and FPN/TF refers to a Feature Pyramid Network (FPN) with a Transformer-based architecture.Each model checkpoint was selected based on exhaustive evaluation mentioned on Section III-F.</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, Shuran Song, 2024</p>
<p>What matters in learning from offline human demonstrations for robot manipulation. Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, Roberto Martín-Martín, 2021</p>
<p>Octo: An open-source generalist robot policy. Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, Sergey Levine, 2024</p>
<p>Robot learning from demonstration. Stefan Christopher G Atkeson, Schaal, ICML. 199797</p>
<p>Robot manipulators: mathematics, programming, and control: the computer control of robot manipulators. Paul Richard, 1981Richard Paul</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. Tianhao Zhang, Zoe Mccarthy, Owen Jow, Dennis Lee, Xi Chen, Ken Goldberg, Pieter Abbeel, 2018 IEEE International Conference on Robotics and Automation (ICRA). 2018</p>
<p>Ajay Mandlekar, Danfei Xu, Roberto Martín-Martín, Silvio Savarese, Li Fei-Fei, arXiv:2003.06085Learning to generalize across long-horizon tasks from human demonstrations. 2020arXiv preprint</p>
<p>Embodied intelligence via learning and evolution. Agrim Gupta, Silvio Savarese, Surya Ganguli, Li Fei-Fei, Nature communications. 12157212021</p>
<p>Robot peels banana with goal-conditioned dual-action deep imitation learning. Heecheol Kim, Yoshiyuki Ohmura, Yasuo Kuniyoshi, arXiv:2203.097492022arXiv preprint</p>
<p>Towards learning hierarchical skills for multi-phase manipulation tasks. Oliver Kroemer, Christian Daniel, Gerhard Neumann, Herke Van Hoof, Jan Peters, 2015 IEEE international conference on robotics and automation (ICRA). IEEE2015</p>
<p>Efficient training of artificial neural networks for autonomous navigation. Dean A Pomerleau, Neural Computation. 311991</p>
<p>A reduction of imitation learning and structured prediction to no-regret online learning. Stephane Ross, Geoffrey J Gordon, J Andrew Bagnell, 2011</p>
<p>Learning fine-grained bimanual manipulation with low-cost hardware. Tony Z Zhao, Vikash Kumar, Sergey Levine, Chelsea Finn, arXiv:2304.137052023arXiv preprint</p>
<p>. Pete Florence, Corey Lynch, Andy Zeng, Oscar Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, Jonathan Tompson, 2021Implicit behavioral cloning</p>
<p>Strictly batch imitation learning by energy-based distribution matching. Daniel Jarrett, Ioana Bica, Mihaela Van Der Schaar, 2021</p>
<p>Planning with diffusion for flexible behavior synthesis. Michael Janner, Yilun Du, Joshua B Tenenbaum, Sergey Levine, 2022</p>
<p>Denoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in neural information processing systems. 202033</p>
<p>Imitation learning for generalizable self-driving policy with sim-to-real transfer. Zoltán Lőrincz, Márton Szemenyei, Róbert Moni, 2022</p>
<p>Towards closing the sim-to-real gap in collaborative multi-robot deep reinforcement learning. Wenshuai Zhao, Jorge Pena Queralta, Li Qingqing, Tomi Westerlund, 2020 5th International conference on robotics and automation engineering (ICRAE). IEEE2020</p>
<p>Reconciling reality through simulation: A real-to-sim-to-real approach for robust manipulation. Marcel Torne, Anthony Simeonov, Zechu Li ; Chan, Tao Chen, Abhishek Gupta, Pulkit Agrawal, April. 2024</p>
<p>Airsim: High-fidelity visual and physical simulation for autonomous vehicles. Shital Shah, Debadeepta Dey, Chris Lovett, Ashish Kapoor, Field and Service Robotics. 2017</p>
<p>CARLA: An open urban driving simulator. Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun, Proceedings of the 1st Annual Conference on Robot Learning. the 1st Annual Conference on Robot Learning2017</p>
<p>Robot Operating System (ROS): The Complete Reference. Fadri Furrer, Michael Burri, Markus Achtelik, Roland Siegwart, chapter RotorS-A Modular Gazebo MAV Simulator Framework. ChamSpringer International Publishing20161</p>
<p>Blind spot detection for safe sim-to-real transfer. Ramya Ramakrishnan, Ece Kamar, Debadeepta Dey, Eric Horvitz, Julie Shah, Journal of Artificial Intelligence Research. 672020</p>
<p>Nur Muhammad, Mahi Shafiullah, Anant Rai, Haritheja Etukuru, Yiqian Liu, Ishan Misra, Soumith Chintala, Lerrel Pinto, arXiv:2311.16098On bringing robots home. 2023arXiv preprint</p>
<p>Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots. Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau, Benjamin Burchfiel, Siyuan Feng, Russ Tedrake, Shuran Song, 2024</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Feature pyramid networks for object detection. Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, ( Shixiang, Machel Shane) Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Language models are few-shot learners. Tom B Brown, arXiv:2005.141652020arXiv preprint</p>
<p>Language-driven representation learning for robotics. Siddharth Karamcheti, Suraj Nair, Annie S Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, Percy Liang, arXiv:2302.127662023arXiv preprint</p>
<p>Scaling cross-embodied learning: One policy for manipulation, navigation, locomotion and aviation. Ria Doshi, Homer Walke, Oier Mees, Sudeep Dasari, Sergey Levine, arXiv:2408.118122024arXiv preprint</p>
<p>. Abby O' Neill, Embodiment CollaborationAbdul Rehman, Embodiment CollaborationAbhinav Gupta, Embodiment CollaborationAbhiram Maddukuri, Embodiment CollaborationAbhishek Gupta, Embodiment CollaborationAbhishek Padalkar, Embodiment CollaborationAbraham Lee, Embodiment CollaborationAcorn Pooley, Embodiment CollaborationAgrim Gupta, Embodiment CollaborationAjay Mandlekar, Embodiment CollaborationAjinkya Jain, Embodiment CollaborationAlbert Tung, Embodiment CollaborationAlex Bewley, Embodiment CollaborationAlex Herzog, Embodiment CollaborationAlex Irpan, Embodiment CollaborationAlexander Khazatsky, Embodiment CollaborationAnant Rai, Embodiment CollaborationAnchit Gupta, Embodiment CollaborationAndrew Wang, Embodiment CollaborationAndrey Kolobov, Embodiment CollaborationAnikait Singh, Embodiment CollaborationAnimesh Garg, Embodiment CollaborationAniruddha Kembhavi, Embodiment CollaborationAnnie Xie, Embodiment CollaborationAnthony Brohan, Embodiment CollaborationAntonin Raffin, Embodiment CollaborationArchit Sharma, Embodiment CollaborationArefeh Yavary, Embodiment CollaborationArhan Jain, Embodiment CollaborationAshwin Balakrishna, Embodiment CollaborationAyzaan Wahid, Embodiment CollaborationBen Burgess-Limerick, Embodiment CollaborationBeomjoon Kim, Embodiment CollaborationBernhard Schölkopf, Embodiment CollaborationBlake Wulfe, Embodiment CollaborationBrian Ichter, Embodiment CollaborationCewu Lu, Embodiment CollaborationCharles Xu, Embodiment CollaborationCharlotte Le, Embodiment CollaborationChelsea Finn, Embodiment CollaborationChen Wang, Embodiment CollaborationChenfeng Xu, Embodiment CollaborationCheng Chi, Embodiment CollaborationChenguang Huang, Embodiment CollaborationChristine Chan, Embodiment CollaborationChristopher Agia, Embodiment CollaborationChuer Pan, Embodiment CollaborationChuyuan Fu, Embodiment CollaborationColine Devin, Embodiment CollaborationDanfei Xu, Embodiment CollaborationDaniel Morton, Embodiment CollaborationDanny Driess, Embodiment CollaborationDaphne Chen, Embodiment CollaborationDeepak Pathak, Embodiment CollaborationDhruv Shah, Embodiment CollaborationDieter Büchler, Embodiment CollaborationDinesh Jayaraman, Embodiment CollaborationDmitry Kalashnikov, Embodiment CollaborationDorsa Sadigh, Embodiment CollaborationEdward Johns, Embodiment CollaborationEthan Foster, Embodiment CollaborationFangchen Liu, Embodiment CollaborationFederico Ceola, Embodiment CollaborationFei Xia, Embodiment CollaborationFeiyu Zhao, Embodiment CollaborationFelipe Vieira Frujeri, Embodiment CollaborationFreek Stulp, Embodiment CollaborationGaoyue Zhou, Embodiment CollaborationGaurav S Sukhatme, Embodiment CollaborationGautam Salhotra, Embodiment CollaborationGe Yan, Embodiment CollaborationGilbert Feng, Embodiment CollaborationGiulio Schiavi, Embodiment CollaborationGlen Berseth, Embodiment CollaborationGregory Kahn, Embodiment CollaborationGuangwen Yang, Embodiment CollaborationGuanzhi Wang, Embodiment CollaborationHao Su, Embodiment CollaborationHao-Shu Fang, Embodiment CollaborationHaochen Shi, Embodiment CollaborationHenghui Bao, Embodiment CollaborationHeni Ben Amor, Embodiment CollaborationHenrik I Christensen, Embodiment CollaborationHiroki Furuta, Embodiment CollaborationHomanga Bharadhwaj, Embodiment CollaborationHomer Walke, Embodiment CollaborationHongjie Fang, Embodiment CollaborationHuy Ha, Embodiment CollaborationIgor Mordatch, Embodiment CollaborationIlija Radosavovic, Embodiment CollaborationIsabel Leal, Embodiment CollaborationJacky Liang, Embodiment CollaborationJad Abou-Chakra, Embodiment CollaborationJaehyung Kim, Embodiment CollaborationJaimyn Drake, Embodiment CollaborationJan Peters, Embodiment CollaborationJan Schneider, Embodiment CollaborationJasmine Hsu, Embodiment CollaborationJay Vakil, Embodiment CollaborationJeannette Bohg, Embodiment CollaborationJeffrey Bingham, Embodiment CollaborationJeffrey Wu, Embodiment CollaborationJensen Gao, Embodiment CollaborationJiaheng Hu, Embodiment CollaborationJiajun Wu, Embodiment CollaborationJialin Wu, Embodiment CollaborationJiankai Sun, Embodiment CollaborationJianlan Luo, Embodiment CollaborationJiayuan Gu, Embodiment CollaborationJie Tan, Embodiment CollaborationJihoon Oh, Embodiment CollaborationJimmy Wu, Embodiment CollaborationJingpei Lu, Embodiment CollaborationJingyun Yang, Embodiment CollaborationJitendra Malik, Embodiment CollaborationJoão Silvério, Embodiment CollaborationJoey Hejna, Embodiment CollaborationJonathan Booher, Embodiment CollaborationJonathan Tompson, Embodiment CollaborationJonathan Yang, Embodiment CollaborationJordi Salvador, Embodiment CollaborationJoseph J Lim, Embodiment CollaborationJunhyek Han, Embodiment CollaborationKaiyuan Wang, Embodiment CollaborationKanishka Rao, Embodiment CollaborationKarl Pertsch, Embodiment CollaborationKarol Hausman, Embodiment CollaborationKeegan Go, Embodiment CollaborationKeerthana Gopalakrishnan, Embodiment CollaborationKen Goldberg, Embodiment CollaborationKendra Byrne, Embodiment CollaborationKenneth Oslund, Embodiment CollaborationKrishnanKevin Black, Kevin Lin, Kevin Zhang; Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana</p>
<p>. Kuan Srinivasan, Fang, Pratap Kunal, Kuo-Hao Singh, Kyle Zeng, Kyle Hatch, Laurent Hsu, Lawrence Itti, Lerrel Yunliang Chen, Li Pinto, Liam Fei-Fei, Tan, " Linxi, Lionel Jim" Fan, Lisa Ott, Luca Lee, Magnum Weihs, Marion Chen, Marius Lepert, Masayoshi Memmel, Masha Tomizuka, Mateo Guaman Itkina, Max Castro, Maximilian Spero, Michael Du, Michael C Ahn, Mingtong Yip, Mingyu Zhang, Minho Ding, Mohan Heo, Mohit Kumar Srirama, Sharma, Jin Moo, Naoaki Kim, Nicklas Kanazawa, Nicolas Hansen, Heess, J Nikhil, Niko Joshi, Ning Suenderhauf, Norman Liu, Nur Di Palo, Muhammad Mahi, Oier Shafiullah, Oliver Mees, Osbert Kroemer, Bastani, Patrick "tree" Pannag R Sanketi, Patrick Miller, Paul Yin, Peng Wohlhart, Peter Xu, Peter David Fagan, Pierre Mitrano, Pieter Sermanet, Priya Abbeel, Qiuyu Sundaresan, Quan Chen, Rafael Vuong, Ran Rafailov, Ria Tian, Roberto Doshi, Rohan Mart'in-Mart'in, Rosario Baijal, Rose Scalise, Roy Hendrix, Runjia Lin, Ruohan Qian, Russell Zhang, Rutav Mendonca, Ryan Shah, Ryan Hoque, Samuel Julian, Sean Bustamante, Sergey Kirmani, Shan Levine, Sherry Lin, Shikhar Moore, Shivin Bahl, Shubham Dass, Shubham Sonawani, Shuran Tulsiani, Sichun Song, Siddhant Xu, Siddharth Haldar, Simeon Karamcheti, Simon Adebola, Soroush Guist, Stefan Nasiriany, Stefan Schaal, Stephen Welker, Subramanian Tian, Sudeep Ramamoorthy, Suneel Dasari, Sungjae Belkhale, Suraj Park, Suvir Nair, Takayuki Mirchandani, Tanmay Osa, Tatsuya Gupta, Tatsuya Harada, Ted Matsushima, Thomas Xiao, Tianhe Kollar, Tianli Yu, Todor Ding, Tony Z Davchev, Travis Zhao, Trevor Armstrong, Trinity Darrell, Vidhi Chung, Vikash Jain, Vincent Kumar, Wei Vanhoucke, Wenxuan Zhan, Wolfram Zhou, Xi Burgard, Xiangyu Chen, Xiaolong Chen, Xinghao Wang, Xinyang Zhu, Xiyuan Geng, Xu Liu, Xuanlin Liangwei, Yansong Li, Yao Pang, Yecheng Lu, Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, 2024Yueh-Hua WuLee, Yuchen Cui, Yue Cao; Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan MaZhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, and Zipeng Lin. Open x-embodiment: Robotic learning datasets and rt-x models</p>            </div>
        </div>

    </div>
</body>
</html>
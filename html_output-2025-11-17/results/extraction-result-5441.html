<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5441 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5441</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5441</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-ed0ed87161a2beab9e1bed3e783d7487a5f1062a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ed0ed87161a2beab9e1bed3e783d7487a5f1062a" target="_blank">LM vs LM: Detecting Factual Errors via Cross Examination</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> Inspired by truth-seeking mechanisms in law, a factuality evaluation framework for LMs that is based on cross-examination is proposed that outperforms existing methods and baselines, often by a large gap.</p>
                <p><strong>Paper Abstract:</strong> A prominent weakness of modern language models (LMs) is their tendency to generate factually incorrect text, which hinders their usability. A natural question is whether such factual errors can be detected automatically. Inspired by truth-seeking mechanisms in law, we propose a factuality evaluation framework for LMs that is based on cross-examination. Our key idea is that an incorrect claim is likely to result in inconsistency with other claims that the model generates. To discover such inconsistencies, we facilitate a multi-turn interaction between the LM that generated the claim and another LM (acting as an examiner) which introduces questions to discover inconsistencies. We empirically evaluate our method on factual claims made by multiple recent LMs on four benchmarks, finding that it outperforms existing methods and baselines, often by a large gap. Our results demonstrate the potential of using interacting LMs for capturing factual errors.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5441.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5441.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMVLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LM vs LM cross-examination (LMVLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot, multi-turn cross-examination protocol where an "EXAMINER" LM asks targeted questions to an "EXAMINEE" LM about a claim, uses iterative follow-up questions to surface inconsistencies, and outputs a binary factuality decision (correct/incorrect).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned OpenAI conversational model (gpt-3.5-turbo) used both as EXAMINER and EXAMINEE in experiments; exact parameter count not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>LMVLM cross-examination (iterative questioning / generate-then-reflect)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Three-stage prompted interaction: (1) Setup — examiner is prompted to generate short targeted questions about the EXAMINEE's claim; (2) Follow-up — EXAMINER receives EXAMINEE's answers and may iteratively ask follow-up questions conditioned on prior answers (repeat until no follow-ups or max turns); (3) Factuality decision — EXAMINER outputs 'correct' or 'incorrect'. EXAMINER and EXAMINEE can be the same LM with different prompts. Zero-shot prompting, max of 5 turns in practice; follow-up questions are sampled in majority-ensemble runs.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Factual question answering (LAMA, TriviaQA, Natural Questions, PopQA; plus a Falsehoods synthetic set)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Closed-book open-ended QA and fact completion: convert model answer into a claim and evaluate claim truth via cross-examination against dataset gold answers (and a synthetic set of intentionally wrong claims).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>LMVLM (ChatGPT as both EXAMINER and EXAMINEE) F1 by dataset: LAMA 76.7, TriviaQA 76.8, NQ 77.7, PopQA 80.2 (Precision and Recall values are reported per table). LMVLM (Majority ensemble, 3 runs) F1: LAMA 80.8, TriviaQA 82.6, NQ 79.1, PopQA 85.4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Ablation removing the follow-up-questions stage (i.e., no iterative follow-ups) yields F1: LAMA 75.1, TriviaQA 75.5, NQ 73.4, PopQA 77.4. (Reported as '- Follow-up' in Table 4.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative ablation: including follow-up iterations increases F1 over the ablated (no-follow-up) variant across datasets (e.g., NQ F1 increases 73.4 -> 77.7, +4.3; PopQA +2.8 F1). LMVLM outperforms baseline uncertainty/self-checking methods (AYS, IDK, Confidence-Based etc.) on precision, recall and F1; e.g., for ChatGPT on PopQA LMVLM F1 80.2 vs best baseline ≤65.2 in the rejection-oriented evaluation (Table 4). Qualitative analyses show examiners use paraphrase, implication validation, decomposition and requests for attribution to reveal inconsistencies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Method is more costly due to multiple LM queries; depends on examiner's ability to follow instructions and reason over long context (works better with larger models); examiner logical flaws can produce inaccurate decisions; some examinees supply mutually consistent but incorrect internal facts making contradictions rare (examiner fails to detect), and ~9-15% of examinations are inconclusive (examiner doesn't produce a binary decision).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LM vs LM: Detecting Factual Errors via Cross Examination', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5441.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5441.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMVLM-GPT3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LMVLM with GPT-3 (text-davinci-003) as EXAMINER/EXAMINEE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same LMvLM cross-examination protocol applied with GPT-3 (text-davinci-003) serving as both examiner and examinee in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI instruction-tuned large language model (text-davinci-003); exact parameterization not stated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>LMVLM cross-examination (iterative questioning / generate-then-reflect)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Identical three-stage protocol implemented via GPT-3 prompts: examiner generates questions, examinee answers, examiner issues follow-ups conditioned on answers, and concludes. Maximum of five turns; iterative follow-ups are an essential part of the reflection-like process.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Factual question answering (LAMA, TriviaQA, Natural Questions, PopQA; plus Falsehoods)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Closed-book open-ended QA and fact completion; claims are examined vs dataset gold answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>LMVLM (GPT-3 as both EXAMINER and EXAMINEE) F1 by dataset (rejection-oriented): LAMA 74.1, TriviaQA 72.1, NQ 68.5, PopQA 72.2. LMVLM (Majority) F1: LAMA 79.3, TriviaQA 77.2, NQ 78.0, PopQA 76.4 (see Table 5). On the Falsehoods dataset LMVLM achieved 75%-99% accuracy across datasets (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Ablation removing follow-up questions yields F1: LAMA 73.7, TriviaQA 71.1, NQ 68.3, PopQA 68.3 (Table 5 '- Follow-up'). Notable F1 drops include PopQA: 72.2 -> 68.3 (-3.9) and TriviaQA: 72.1 -> 71.1 (-1.0).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Ablation shows follow-up iterations improve recall substantially and F1 moderately across datasets (e.g., PopQA F1 +3.9; some datasets show smaller gains). LMVLM outperforms baseline uncertainty detectors (e.g., Confidence-Based, AYS, IDK) on F1 in the rejection-oriented evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same practical limitations as above: cost, dependence on model reasoning and instruction-following; examiner may not always reach conclusive decisions (about 9.1% inconclusive in GPT-3 setting). Some failure cases occur when EXAMINEE provides internally consistent but wrong answers, hindering detection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LM vs LM: Detecting Factual Errors via Cross Examination', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5441.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5441.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMvLM(ChatGPT->LLaMA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LMVLM with ChatGPT as EXAMINER and LLaMA-7B as EXAMINEE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cross-examination where a stronger examiner (ChatGPT) probes a smaller open model (LLaMA-7B) examinee to detect factual errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (examiner) / LLaMA-7B (examinee)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT: instruction-tuned conversational model (gpt-3.5-turbo). LLaMA-7B: an open foundation model with ~7B parameters (as named in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>LMVLM cross-examination (multi-turn examiner probing examinee)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Examiner (ChatGPT) issues questions, receives answers from LLaMA-7B, issues follow-ups, and decides on claim truth. Same three-stage protocol; highlights use of a stronger examiner to probe a weaker examinee's internal consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Factual question answering (LAMA, TriviaQA, Natural Questions, PopQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Closed-book QA and fact completion; claims generated by LLaMA-7B are judged by ChatGPT via the LMvLM protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>LMvLM (ChatGPT examiner / LLaMA-7B examinee) F1 by dataset (rejection-oriented): LAMA 73.0, TriviaQA 75.2, NQ 71.7, PopQA 78.8. LMvLM (Majority) F1: LAMA 78.1, TriviaQA 77.2, NQ 78.7, PopQA 82.5 (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Ablation without follow-ups yields F1: LAMA 72.0, TriviaQA 74.6, NQ 70.7, PopQA 77.1 (Table 6 '- Follow-up'). Differences are smaller but consistent (e.g., LAMA +1.0 F1, NQ +1.0 F1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Ablation indicates follow-up iterations increase F1 modestly across datasets; qualitative analysis shows ChatGPT examiner uses logical decomposition and implication validation more often (possibly a factor in stronger detection performance versus GPT-3). LMvLM (Majority) achieves higher F1 than baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Smaller examinee models (e.g., LLaMA-7B) may lack the instruction-following or long-context reasoning needed for detailed follow-up answers; examiner logical flaws or examinee consistency of wrong facts still impede detection. About 10.3% of examiner decisions were inconclusive in this configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LM vs LM: Detecting Factual Errors via Cross Examination', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5441.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5441.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced prior work proposing a model that uses self-reflection and dynamic memory to evaluate and revise its actions; mentioned in related work as an example of instructing an LM when to 'reflect' during planned actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion (self-reflection controller)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Mentioned as a prior multi-step planning approach where a model uses another component to decide when to reflect on its recent actions; paper reference only, no experiments or numbers reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Mentioned as related work; no experimental details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in this paper beyond being noted in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LM vs LM: Detecting Factual Errors via Cross Examination', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5441.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5441.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency (decoding aggregation over multiple reasoning chains)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding technique that aggregates multiple sampled chains of thought to improve reasoning robustness; referenced as prior work on iterative/coherent reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-consistency (aggregation over sampled CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Mentioned as prior work that aggregates multiple generated reasoning paths (chains of thought) to improve final answer quality; cited in related work but not used experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Cited as evidence that multi-sample reasoning + aggregation can boost performance in reasoning tasks; no direct experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LM vs LM: Detecting Factual Errors via Cross Examination', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5441.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5441.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain of Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate reasoning steps from LMs before producing a final answer; referenced as related work that enables multi-step self-questioning and improved reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Referenced as prior work where LMs are prompted to produce intermediate reasoning steps (self-ask/follow-up) before concluding; the paper positions LMVLM as related but distinct because LMVLM uses a separate examiner LM to probe consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Referenced studies show CoT can improve multi-step and mathematical reasoning; no direct comparison numbers in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not elaborated in this paper beyond general related-work discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LM vs LM: Detecting Factual Errors via Cross Examination', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5441.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5441.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Maieutic prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maieutic prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method that recursively generates supporting facts and reasons to construct logically consistent explanations; cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Maieutic prompting: Logically consistent reasoning with recursive explanations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Maieutic prompting (recursive explanation)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Mentioned in related work as an approach that recursively generates facts and reasons to support answers; LMVLM is compared conceptually as another way to elicit internal model beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Only referenced; no further details provided within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LM vs LM: Detecting Factual Errors via Cross Examination', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5441.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5441.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inner Monologue</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inner monologue (planning with language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A planning approach that uses internal 'monologue' or intermediate planning steps to improve problem-solving; included in related work as an example of internal iterative reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Inner monologue: Embodied reasoning through planning with language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Inner monologue / planning</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Mentioned as prior work that uses planning and internal reasoning (monologue) to improve performance on planning/problem-solving tasks; cited for conceptual relation to LMVLM's multi-step probing.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Cited in related work; no experimental crossover presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LM vs LM: Detecting Factual Errors via Cross Examination', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Maieutic prompting: Logically consistent reasoning with recursive explanations <em>(Rating: 1)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5441",
    "paper_id": "paper-ed0ed87161a2beab9e1bed3e783d7487a5f1062a",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "LMVLM",
            "name_full": "LM vs LM cross-examination (LMVLM)",
            "brief_description": "A zero-shot, multi-turn cross-examination protocol where an \"EXAMINER\" LM asks targeted questions to an \"EXAMINEE\" LM about a claim, uses iterative follow-up questions to surface inconsistencies, and outputs a binary factuality decision (correct/incorrect).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "Instruction-tuned OpenAI conversational model (gpt-3.5-turbo) used both as EXAMINER and EXAMINEE in experiments; exact parameter count not specified in the paper.",
            "reflection_method_name": "LMVLM cross-examination (iterative questioning / generate-then-reflect)",
            "reflection_method_description": "Three-stage prompted interaction: (1) Setup — examiner is prompted to generate short targeted questions about the EXAMINEE's claim; (2) Follow-up — EXAMINER receives EXAMINEE's answers and may iteratively ask follow-up questions conditioned on prior answers (repeat until no follow-ups or max turns); (3) Factuality decision — EXAMINER outputs 'correct' or 'incorrect'. EXAMINER and EXAMINEE can be the same LM with different prompts. Zero-shot prompting, max of 5 turns in practice; follow-up questions are sampled in majority-ensemble runs.",
            "num_iterations": 5,
            "task_name": "Factual question answering (LAMA, TriviaQA, Natural Questions, PopQA; plus a Falsehoods synthetic set)",
            "task_description": "Closed-book open-ended QA and fact completion: convert model answer into a claim and evaluate claim truth via cross-examination against dataset gold answers (and a synthetic set of intentionally wrong claims).",
            "performance_with_reflection": "LMVLM (ChatGPT as both EXAMINER and EXAMINEE) F1 by dataset: LAMA 76.7, TriviaQA 76.8, NQ 77.7, PopQA 80.2 (Precision and Recall values are reported per table). LMVLM (Majority ensemble, 3 runs) F1: LAMA 80.8, TriviaQA 82.6, NQ 79.1, PopQA 85.4.",
            "performance_without_reflection": "Ablation removing the follow-up-questions stage (i.e., no iterative follow-ups) yields F1: LAMA 75.1, TriviaQA 75.5, NQ 73.4, PopQA 77.4. (Reported as '- Follow-up' in Table 4.)",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative ablation: including follow-up iterations increases F1 over the ablated (no-follow-up) variant across datasets (e.g., NQ F1 increases 73.4 -&gt; 77.7, +4.3; PopQA +2.8 F1). LMVLM outperforms baseline uncertainty/self-checking methods (AYS, IDK, Confidence-Based etc.) on precision, recall and F1; e.g., for ChatGPT on PopQA LMVLM F1 80.2 vs best baseline ≤65.2 in the rejection-oriented evaluation (Table 4). Qualitative analyses show examiners use paraphrase, implication validation, decomposition and requests for attribution to reveal inconsistencies.",
            "limitations_or_failure_cases": "Method is more costly due to multiple LM queries; depends on examiner's ability to follow instructions and reason over long context (works better with larger models); examiner logical flaws can produce inaccurate decisions; some examinees supply mutually consistent but incorrect internal facts making contradictions rare (examiner fails to detect), and ~9-15% of examinations are inconclusive (examiner doesn't produce a binary decision).",
            "uuid": "e5441.0",
            "source_info": {
                "paper_title": "LM vs LM: Detecting Factual Errors via Cross Examination",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LMVLM-GPT3",
            "name_full": "LMVLM with GPT-3 (text-davinci-003) as EXAMINER/EXAMINEE",
            "brief_description": "Same LMvLM cross-examination protocol applied with GPT-3 (text-davinci-003) serving as both examiner and examinee in experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-003)",
            "model_description": "OpenAI instruction-tuned large language model (text-davinci-003); exact parameterization not stated in the paper.",
            "reflection_method_name": "LMVLM cross-examination (iterative questioning / generate-then-reflect)",
            "reflection_method_description": "Identical three-stage protocol implemented via GPT-3 prompts: examiner generates questions, examinee answers, examiner issues follow-ups conditioned on answers, and concludes. Maximum of five turns; iterative follow-ups are an essential part of the reflection-like process.",
            "num_iterations": 5,
            "task_name": "Factual question answering (LAMA, TriviaQA, Natural Questions, PopQA; plus Falsehoods)",
            "task_description": "Closed-book open-ended QA and fact completion; claims are examined vs dataset gold answers.",
            "performance_with_reflection": "LMVLM (GPT-3 as both EXAMINER and EXAMINEE) F1 by dataset (rejection-oriented): LAMA 74.1, TriviaQA 72.1, NQ 68.5, PopQA 72.2. LMVLM (Majority) F1: LAMA 79.3, TriviaQA 77.2, NQ 78.0, PopQA 76.4 (see Table 5). On the Falsehoods dataset LMVLM achieved 75%-99% accuracy across datasets (Table 7).",
            "performance_without_reflection": "Ablation removing follow-up questions yields F1: LAMA 73.7, TriviaQA 71.1, NQ 68.3, PopQA 68.3 (Table 5 '- Follow-up'). Notable F1 drops include PopQA: 72.2 -&gt; 68.3 (-3.9) and TriviaQA: 72.1 -&gt; 71.1 (-1.0).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Ablation shows follow-up iterations improve recall substantially and F1 moderately across datasets (e.g., PopQA F1 +3.9; some datasets show smaller gains). LMVLM outperforms baseline uncertainty detectors (e.g., Confidence-Based, AYS, IDK) on F1 in the rejection-oriented evaluation.",
            "limitations_or_failure_cases": "Same practical limitations as above: cost, dependence on model reasoning and instruction-following; examiner may not always reach conclusive decisions (about 9.1% inconclusive in GPT-3 setting). Some failure cases occur when EXAMINEE provides internally consistent but wrong answers, hindering detection.",
            "uuid": "e5441.1",
            "source_info": {
                "paper_title": "LM vs LM: Detecting Factual Errors via Cross Examination",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LMvLM(ChatGPT-&gt;LLaMA)",
            "name_full": "LMVLM with ChatGPT as EXAMINER and LLaMA-7B as EXAMINEE",
            "brief_description": "Cross-examination where a stronger examiner (ChatGPT) probes a smaller open model (LLaMA-7B) examinee to detect factual errors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (examiner) / LLaMA-7B (examinee)",
            "model_description": "ChatGPT: instruction-tuned conversational model (gpt-3.5-turbo). LLaMA-7B: an open foundation model with ~7B parameters (as named in the paper).",
            "reflection_method_name": "LMVLM cross-examination (multi-turn examiner probing examinee)",
            "reflection_method_description": "Examiner (ChatGPT) issues questions, receives answers from LLaMA-7B, issues follow-ups, and decides on claim truth. Same three-stage protocol; highlights use of a stronger examiner to probe a weaker examinee's internal consistency.",
            "num_iterations": 5,
            "task_name": "Factual question answering (LAMA, TriviaQA, Natural Questions, PopQA)",
            "task_description": "Closed-book QA and fact completion; claims generated by LLaMA-7B are judged by ChatGPT via the LMvLM protocol.",
            "performance_with_reflection": "LMvLM (ChatGPT examiner / LLaMA-7B examinee) F1 by dataset (rejection-oriented): LAMA 73.0, TriviaQA 75.2, NQ 71.7, PopQA 78.8. LMvLM (Majority) F1: LAMA 78.1, TriviaQA 77.2, NQ 78.7, PopQA 82.5 (Table 6).",
            "performance_without_reflection": "Ablation without follow-ups yields F1: LAMA 72.0, TriviaQA 74.6, NQ 70.7, PopQA 77.1 (Table 6 '- Follow-up'). Differences are smaller but consistent (e.g., LAMA +1.0 F1, NQ +1.0 F1).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Ablation indicates follow-up iterations increase F1 modestly across datasets; qualitative analysis shows ChatGPT examiner uses logical decomposition and implication validation more often (possibly a factor in stronger detection performance versus GPT-3). LMvLM (Majority) achieves higher F1 than baselines.",
            "limitations_or_failure_cases": "Smaller examinee models (e.g., LLaMA-7B) may lack the instruction-following or long-context reasoning needed for detailed follow-up answers; examiner logical flaws or examinee consistency of wrong facts still impede detection. About 10.3% of examiner decisions were inconclusive in this configuration.",
            "uuid": "e5441.2",
            "source_info": {
                "paper_title": "LM vs LM: Detecting Factual Errors via Cross Examination",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "brief_description": "A referenced prior work proposing a model that uses self-reflection and dynamic memory to evaluate and revise its actions; mentioned in related work as an example of instructing an LM when to 'reflect' during planned actions.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Reflexion (self-reflection controller)",
            "reflection_method_description": "Mentioned as a prior multi-step planning approach where a model uses another component to decide when to reflect on its recent actions; paper reference only, no experiments or numbers reported here.",
            "num_iterations": null,
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": null,
            "evidence_of_improvement": "Mentioned as related work; no experimental details provided in this paper.",
            "limitations_or_failure_cases": "Not discussed in this paper beyond being noted in related work.",
            "uuid": "e5441.3",
            "source_info": {
                "paper_title": "LM vs LM: Detecting Factual Errors via Cross Examination",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Self-consistency",
            "name_full": "Self-consistency (decoding aggregation over multiple reasoning chains)",
            "brief_description": "A decoding technique that aggregates multiple sampled chains of thought to improve reasoning robustness; referenced as prior work on iterative/coherent reasoning.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Self-consistency (aggregation over sampled CoT)",
            "reflection_method_description": "Mentioned as prior work that aggregates multiple generated reasoning paths (chains of thought) to improve final answer quality; cited in related work but not used experimentally in this paper.",
            "num_iterations": null,
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": null,
            "evidence_of_improvement": "Cited as evidence that multi-sample reasoning + aggregation can boost performance in reasoning tasks; no direct experiments in this paper.",
            "limitations_or_failure_cases": "Not discussed here.",
            "uuid": "e5441.4",
            "source_info": {
                "paper_title": "LM vs LM: Detecting Factual Errors via Cross Examination",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain of Thought prompting",
            "brief_description": "A prompting technique that elicits intermediate reasoning steps from LMs before producing a final answer; referenced as related work that enables multi-step self-questioning and improved reasoning.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Chain-of-Thought (CoT)",
            "reflection_method_description": "Referenced as prior work where LMs are prompted to produce intermediate reasoning steps (self-ask/follow-up) before concluding; the paper positions LMVLM as related but distinct because LMVLM uses a separate examiner LM to probe consistency.",
            "num_iterations": null,
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": null,
            "evidence_of_improvement": "Referenced studies show CoT can improve multi-step and mathematical reasoning; no direct comparison numbers in this paper.",
            "limitations_or_failure_cases": "Not elaborated in this paper beyond general related-work discussion.",
            "uuid": "e5441.5",
            "source_info": {
                "paper_title": "LM vs LM: Detecting Factual Errors via Cross Examination",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Maieutic prompting",
            "name_full": "Maieutic prompting",
            "brief_description": "A prior method that recursively generates supporting facts and reasons to construct logically consistent explanations; cited as related work.",
            "citation_title": "Maieutic prompting: Logically consistent reasoning with recursive explanations",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Maieutic prompting (recursive explanation)",
            "reflection_method_description": "Mentioned in related work as an approach that recursively generates facts and reasons to support answers; LMVLM is compared conceptually as another way to elicit internal model beliefs.",
            "num_iterations": null,
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": null,
            "evidence_of_improvement": "Only referenced; no further details provided within this paper.",
            "limitations_or_failure_cases": "Not discussed here.",
            "uuid": "e5441.6",
            "source_info": {
                "paper_title": "LM vs LM: Detecting Factual Errors via Cross Examination",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Inner Monologue",
            "name_full": "Inner monologue (planning with language models)",
            "brief_description": "A planning approach that uses internal 'monologue' or intermediate planning steps to improve problem-solving; included in related work as an example of internal iterative reasoning.",
            "citation_title": "Inner monologue: Embodied reasoning through planning with language models",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Inner monologue / planning",
            "reflection_method_description": "Mentioned as prior work that uses planning and internal reasoning (monologue) to improve performance on planning/problem-solving tasks; cited for conceptual relation to LMVLM's multi-step probing.",
            "num_iterations": null,
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": null,
            "evidence_of_improvement": "Cited in related work; no experimental crossover presented in this paper.",
            "limitations_or_failure_cases": "Not discussed in this paper.",
            "uuid": "e5441.7",
            "source_info": {
                "paper_title": "LM vs LM: Detecting Factual Errors via Cross Examination",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Maieutic prompting: Logically consistent reasoning with recursive explanations",
            "rating": 1
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 1
        }
    ],
    "cost": 0.0180625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LM vs LM: Detecting Factual Errors via Cross Examination</h1>
<p>Roi Cohen ${ }^{1}$ May Hamri ${ }^{1}$ Mor Geva ${ }^{2}$ Amir Globerson ${ }^{1,3}$<br>${ }^{1}$ Blavatnik School of Computer Science, Tel Aviv University<br>${ }^{2}$ Google DeepMind ${ }^{3}$ Google Research<br>{roi1, mayhamri}@mail.tau.ac.il, pipek@google.com, gamir@tauex.tau.ac.il</p>
<h4>Abstract</h4>
<p>A prominent weakness of modern language models (LMs) is their tendency to generate factually incorrect text, which hinders their usability. A natural question is whether such factual errors can be detected automatically. Inspired by truth-seeking mechanisms in law, we propose a factuality evaluation framework for LMs that is based on cross-examination. Our key idea is that an incorrect claim is likely to result in inconsistency with other claims that the model generates. To discover such inconsistencies, we facilitate a multi-turn interaction between the LM that generated the claim and another LM (acting as an examiner) which introduces questions to discover inconsistencies. We empirically evaluate our method on factual claims made by multiple recent LMs on four benchmarks, finding that it outperforms existing methods and baselines, often by a large gap. Our results demonstrate the potential of using interacting LMs to capture factual errors.</p>
<h2>1 Introduction</h2>
<p>Modern language models (LMs) often generate inconsistent (Elazar et al., 2021), non-attributable (Rashkin et al., 2021; Bohnet et al., 2022; Liu et al., 2023a), or factually incorrect text (Tam et al., 2022; Devaraj et al., 2022; Maynez et al., 2020), thus negatively impacting the reliability of these models (Amodei et al., 2016; Hendrycks et al., 2021). This has prompted the community to develop methods that calibrate the confidence of model predictions to better align with their quality (Brundage et al., 2020). For example, prior methods have used probabilistic approaches (Jiang et al., 2020; Zablotskaia et al., 2023) clustering (Kuhn et al., 2023), finetuning (Kadavath et al., 2022; Lin et al., 2022) and in-context learning (Alivanistos et al., 2022; Cohen et al., 2023).</p>
<p>In this work, we take a different approach to this problem, motivated by truth-seeking mechanisms in law. Specifically, we consider the setting
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of our LMVLM approach. The first line shows the statement made by the EXAMINEE LLM. Then an interaction between the EXAMINER and EXAMINEE takes place, and the EXAMINER arrives at a conclusion whether the original statement was correct or not (here it concludes that it was a false statement).
where a witness is cross-examined in order to check whether their statement is factually correct or not. In such a setting, the examiner asks questions that aim to lead towards contradictory statements by the witness, while a contradiction implies that the witness lied at least in some of the statements, hence the well known quote "Were you lying then or are you lying now?" (Wilder et al., 1957).</p>
<p>To employ this mechanism to LM factual calibration, we propose the following setting, illustrated in Figure 1. Our goal is to check whether a statement made by an LM ("The Greek god of marriage is Hera") is factually correct. We refer to the model</p>
<p>that generated this statement as the EXAMINEE. To check whether this fact is correct, we use another LM, called EXAMINER, to conduct a crossexamination of EXAMINEE. Concretely, we craft designated prompts to facilitate a multi-turn interaction between the two LMs, where EXAMINER issues questions (e.g., "Is Hera associated with marriage in any way?") to EXAMINEE to check the veracity of the original statement. The examination is concluded by a decision from EXAMINER as to whether the original claim was correct or not.</p>
<p>Our problem setting is related to that of calibration <em>Guo et al. (2017)</em>, where the goal is to predict the probability at which a model will err. However, unlike previous approaches to this problem, we use text generated by LMs. Our approach is motivated by the intuition that calibration is actually an elaborate reasoning process where one checks the level of support that a fact has based on other statements the model believes. We argue that such complex reasoning is naturally performed via the strong conversational skills of modern LMs.</p>
<p>We use our method to detect errors in LM generation in the context of factual question-answering. Our experiments with several recent LMs – CHAT-GPT, GPT-3 <em>Brown et al. (2020); Ouyang et al. (2022)</em>, and LLAMA <em>Touvron et al. (2023)</em> – show that cross-examination effectively detects factually incorrect claims generated by LMs. Specifically, across multiple datasets and examination settings, it detects over 70% of the incorrect claims while maintaining a high precision of &gt;80%, outperforming strong baselines by a large gap.</p>
<p>Further analysis shows that examiner LMs introduce multiple questions throughout the examination, and employ various strategies to reveal inconsistencies, including question paraphrasing, validation of implicated arguments, claim decomposition, and requests for evidence.</p>
<p>To conclude, our contributions are (a) framing the task of factuality testing as an interaction between two LMs, (b) proposing a concrete implementation of this interaction via the use of one LM with different prompts in a zero-shot setting, and (c) demonstrating improved factuality detection accuracy across several benchmarks.</p>
<h2>2 LM Cross-Examination</h2>
<p>Our goal is to employ an "examiner" LM (EXAMINER) to evaluate claims generated by another LM (EXAMINEE). To this end, we leverage the recent success of prompting <em>Liu et al. (2023b)</em>, to facilitate a cross-examination setting between the two LMs. In such a setting, EXAMINER should introduce questions with the objective of revealing inconsistencies with respect to an initial claim made by EXAMINEE. Such inconsistencies can be considered as a signal for uncertainty of EXAMINEE in its original claim, and thus, can be used to assess whether its original statement was correct.</p>
<p>Given an EXAMINER LM and a claim <em>C</em> generated by an EXAMINEE, our method establishes a multi-turn interaction between the LMs, where at each turn the other LM is prompted with a designated prompt that incorporates the outputs from previous turns. This interaction continues until the examiner has no further questions and can provide its final decision. To establish a meaningful interaction that reveals possible inconsistencies, we define three stages for the examination, each guided by a specific prompt. As part of each prompt for EXAMINEE or EXAMINER, we provide the outputs generated in the previous rounds for context. We next describe the examination stages in detail, with the overall process illustrated in Figure 2.</p>
<h3>Stage 1: Setup</h3>
<p>The examination begins by "assigning" the EXAMINER its role. Namely, describing the task setting, providing it with the EXAMINEE's claim, and asking it to generate questions for the EXAMINEE.</p>
<p>Next, we feed the questions generated by EXAMINER, one at a time, to EXAMINEE, concatenated to the following instructions: Please answer the following questions regarding your claim. The response from EXAMINEE yields a set of answers to the questions from EXAMINER.</p>
<h3>Stage 2: Follow-up Questions</h3>
<p>We next feed EXAMINER with the answers generated by EXAMINEE to its initial questions, and ask EXAMINER whether it has any follow-up questions. Notably, outputs from EXAMINER at this stage are conditioned on the previous output from EXAMINEE. If the answer from EXAMINER is "Yes", we then further prompt it to obtain more questions. This phase is</p>
<p><sup>1</sup>In practice, EXAMINER and EXAMINEE can be the same LM (e.g., GPT-3) that is prompted in two different ways to define its different roles.</p>
<p><sup>2</sup>We observe that this effectively steers EXAMINER to ask natural questions directly related to the given claim <em>C</em> (§5).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The three-stage process of cross-examination between the EXAMINER and EXAMINEe, where the factuality of a claim $C$ generated by EXAMINEe is estimated by EXAMINER.
conducted iteratively, until either EXAMINER declares it has no follow-up questions, or the number of turns has reached a threshold. ${ }^{3}$</p>
<p>Stage 3: Factuality Decision Once no further questions are obtained from EXAMINER, we prompt it to conclude whether the claim $C$ is true or false. Specifically, we request it to reply with either "correct" or "incorrect" as its final conclusion. In cases where the examiner does not output either of "correct" or "incorrect", we consider its final decision to be a rejection of the claim. Typically though, we observe that the examiner follows the instructions and indeed generates a definitive conclusion (see statistics in §5).</p>
<h2>3 Related Work</h2>
<p>Attribution and Fact Checking Our goal is closely related to works on attribution and fact verification. Namely, checking if a LM-generated text is faithful to some source text (Bohnet et al., 2022; Honovich et al., 2022). This problem has been addressed via several approaches, including question generation (Wang et al., 2020; Honovich et al., 2021; Scialom et al., 2021), NLI (Thorne et al., 2018; Welleck et al., 2019; Maynez et al., 2020;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Dziri et al., 2022; Gao et al., 2022; Kamoi et al., 2023), data augmentation (Atanasova et al., 2022; Wright et al., 2022; Gekhman et al., 2023), and planning schemes that allow the model to self-edit its own generation (Schick et al., 2022). Unlike these works, we are not assuming any reference text or an external knowledge base. Instead, we directly check if the LM's claim is likely to be correct, by probing the model for inconsistencies. Our approach also uses multi-turn dialogue as a key component.</p>
<p>Model Calibration A key challenge with prediction models is to provide a probability of the answer being incorrect, a problem known as model calibration (Guo et al., 2017). The problem of factual-error detection can be viewed as a variation of calibration, where instead of a continuous probability, we provide a binary prediction for whether the model is correct or not. This is also related to the setting of selective prediction, where a model can choose to abstain from answering a query (Varshney et al., 2022; Kamath et al., 2020). Common approaches to calibration are to perform various transformations on model logits (Desai and Durrett, 2020; Jiang et al., 2021), and measuring uncertainty (e.g., see Kuhn et al., 2023). More recent works have studied the use of LMs for providing calibration, by training them on statements known to be factually correct or incorrect. This "supervised" approach has been explored via finetuning (Kadavath et al., 2022; Lin et al., 2022) and in-context learning (Cohen et al., 2023; Alivanistos et al., 2022).</p>
<p>Our work focuses on zero-shot factual error detection that involves just two categories: predicting whether a model's claim is correct or incorrect. We propose a novel approach to this problem, using multi-turn LLM interaction. While we focus on a binary setting, one could envision an extension of our approach to continuous outputs (for example, to output a probabilistic estimation for the correctness of the claim).</p>
<p>Multi-Agent LMs Using multiple LMs in an interactive manner is a relatively new idea with many potential applications. It has been shown that LMs can utilize additional LMs or tools to better solve downstream tasks (Schick et al., 2023). Additionally, Park et al. (2022) showed that in a social setting, LMs demonstrate certain social skills that emerge from this interaction, and Shinn</p>
<p>et al. (2023) proposes that a LM can use a different model to instruct it when to "reflect" on its recent action, while performing a planned sequence of actions aimed at solving a given query. Intuitively, this model detects signs of hallucination or inefficient planning within the LM's trajectory.</p>
<p>Consistency Across Generations LMs have been shown to generate inconsistent outputs given different prompt paraphrases (Elazar et al., 2021; Newman et al., 2021). Prior work showed that prompts can be automatically optimized to produce factually correct claims more robustly (Lester et al., 2021; Zhong et al., 2021; Qin and Eisner, 2021). Hao et al. (2022) utilized multiple generated paraphrases to gauge consistency (Hao et al., 2022), and other works (Elazar et al., 2021; Zhou et al., 2022) further proposed training objectives to improve model consistency. Another approach to handling multiple outputs is via variants of decoding strategies (Wang et al., 2022), or model ensembles (Sun et al., 2022). In our work, we build on these, assuming inconsistencies are more likely to occur with incorrect claims, and let an examiner model search for these by introducing questions to the examinee.</p>
<p>Chain of Thought Reasoning Recent work has shown that LMs can be prompted to elaborate on their reasoning process, to self-ask themselves follow-up questions, before reaching a final conclusion, and that this could be exploited to improve mathematical, multi-hop and common-sense reasoning skills (Wei et al., 2022; Press et al., 2022; Yoran et al., 2023), along with planning and problem-solving abilities (Huang et al., 2022; Long, 2023). Another interesting approach to complex reasoning in LMs is recent work on Maieutic prompting (Jung et al., 2022), that answers a question by recursively generating a set of facts and reasoning over those.</p>
<p>Our approach may be viewed as constructing an elaborate chain-of-thought explanation for the examinee's claim. However, we do not train this explanation via in-context or fine-tuning, and rather rely on different prompts for its generation.</p>
<h2>4 Experiments</h2>
<p>In this section, we conduct experiments on multiple datasets and models to evaluate our approach, focusing on the task of factual question-answering.</p>
<h3>4.1 Experimental Setup</h3>
<p>Factual Question Answering One key use-case of LMs is answering questions seeking factual knowledge. For example, "How old was Barack Obama when he was first elected?". In such cases, it is crucial for the model to answer the question correctly, or to indicate that it does not know the answer. We thus evaluate our approach on several Question Answering and Fact Completion datasets. These are typically provided as a set of $(Q, A)$ pairs of a question $Q$ and its ground-truth answer $A$. Having gold answers allows us to evaluate if a predicted answer is factually correct or not, which can be used to evaluate our LMVLM approach.</p>
<p>To apply cross-examination in this setting, we first convert the answer predicted by the model into a EXAMINEE claim that can be provided as input to the examination procedure. Formally, given a question $Q$, if $Q$ is phrased as a fill-in-the-blank question (e.g. "Bailey Peninsula is located in $\qquad$ "), then we feed it to the EXAMINEE model to obtain a prediction that completes the sentence and forms a claim. In cases where $Q$ is phrased as a question (e.g. "Where is Bailey Peninsula located?"), we prompt the model to provide an answer in a claim format with: "Please answer the following question: $<Q>$ Please phrase your answer as a claim." This process results in a claim $C$ that states the model's "belief" about the answer to $Q$. We then evaluate the truthfulness of $C$ through cross-examination, and compare the examiner's decision of whether $C$ is correct or not to the ground-truth correctness.</p>
<p>Factuality Evaluation Labels To evaluate our method, it is necessary to have "gold decisions" to compare the examiner's decisions against. Such labels can be obtained from the ground-truth answers in the data, namely, the decision for a claim $C$ is correct if it matches an evaluation of $C$ against the gold answer $A$. To evaluate if the claim $C$ obtained for a question $Q$ is correct with respect to the ground-truth answer $A$, we first check if $A$ or any of its aliases (if provided as part of the dataset, e.g., "FC Tottenham" and "Tottenham Hotspur") appears as a sub-string in $C$ (Schick et al., 2023; Meng et al., 2022). Next, to avoid incorrect labels resulting from this automatic evaluation (Bulian et al., 2022), we manually review all the claims marked as incorrect in the first step, and fix any labeling mistakes. We also filter out any ambiguous or unclear claims generated by EXAMINEE.</p>
<table>
<thead>
<tr>
<th>Stage</th>
<th>GPT3 Prompt(s)</th>
<th>ChatGPT Prompt(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>(1) Setup</td>
<td>Imagine trying to prove that a claim that someone claims is true, is wrong. You have the opportunity to ask any question in order to prove that the claim is wrong. The claim is: $<C>$. What would you ask in order to validate that?</td>
<td>Your goal is to try to verify the correctness of the following claim: $<C>$, based on the background information you will gather. To gather this, You will provide short questions whose purpose will be to verify the correctness of the claim, and I will reply to you with the answers to these. Hopefully, with the help of the background questions and their answers, you will be able to reach a conclusion as to whether the claim is correct or possibly incorrect. Please keep asking questions as long as you're yet to be sure regarding the true veracity of the claim. Please start with the first questions.</td>
</tr>
<tr>
<td>(2) Follow-Up Questions</td>
<td>(i) Do you have any follow-up questions? Please answer with Yes or No. (ii) What are the follow-up questions?</td>
<td></td>
</tr>
<tr>
<td>(3) Factuality Decision</td>
<td>Based on the interviewee's answers to your questions, what is your conclusion regarding the correctness of the claim? Do you think it is correct or incorrect?</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 1: Prompts provided to EXAMINER in each stage of the examination, with respect to a claim $C$ by EXAMINEE.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">EXAMINEE</th>
<th style="text-align: right;">LAMA</th>
<th style="text-align: right;">TriviaQA</th>
<th style="text-align: right;">NQ</th>
<th style="text-align: right;">PopQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLAMA-7B</td>
<td style="text-align: right;">53.9</td>
<td style="text-align: right;">48.4</td>
<td style="text-align: right;">33.8</td>
<td style="text-align: right;">24.9</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: right;">79.8</td>
<td style="text-align: right;">74.2</td>
<td style="text-align: right;">50.1</td>
<td style="text-align: right;">43.9</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: right;">80.9</td>
<td style="text-align: right;">77.2</td>
<td style="text-align: right;">53.3</td>
<td style="text-align: right;">45.6</td>
</tr>
</tbody>
</table>
<p>Table 2: Portion of factually correct claims by every EXAMINEE LM on each dataset.</p>
<p>Examiner Evaluation We evaluate how well the examiner detects claims that are factually incorrect, using the following metrics: ${ }^{4}$</p>
<ul>
<li>Precision: the portion of incorrect claims, out of the claims rejected by the examiner.</li>
<li>Recall: the portion of incorrect claims rejected by the examiner, out of all the incorrect claims.</li>
<li>F1: the harmonic mean of precision and recall.</li>
</ul>
<p>For completeness, we additionally report (in §C) the complementary Precision, Recall, and F1 scores with respect to detection of correct claims.</p>
<p>Data We consider the following datasets: LAMA (Petroni et al., 2019), TriviaQA (Joshi et al., 2017), Natural Questions (NQ) (Kwiatkowski et al., 2019) and PopQA (Mallen et al., 2022). These datasets cover a wide range of queries, from real user queries (NQ), to trivia questions (TriviaQA), and subject-relation-object facts phrased as queries (LAMA, PopQA). We consider the closed-book open-ended setting, where we do not provide any context or answer choices to the model. We evaluate our approach on 1,000 random examples from the test set (or from the development set if a test</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>False claim Correct claim
"Wanlockhead is the highest "Wanlockhead is the high- "Wanlockhead is the high- village in France because it is est village in Scotland, a located in the French Alps." country in Europe."
"Louis Oosthuizen the 2010 "Louis Oosthuizen, the
Open Golf Champion is Amer- 2010 Open Golf Cham- "Louis Oosthuizen, the
ican, because he was born in pion, is South African." the United States."
"The screenwriter for "Smile" "The screenwriter for
was definitely Steven Spiel- "Smile" was Jerry Bel-
berg." son."
"Fontenay is located in the "Fontenay is located in beautiful country of Antarctica." France."</p>
<p>Table 3: Example false claims generated by ChatGPT for PopQA and by GPT-3 for TriviaQA.
set is not available). ${ }^{5}$
In addition, we created a dataset of false claims to further test our approach. This "Falsehoods" dataset contains only wrong claims, created separately for each model (GPT-3 and ChatGPT) and for each of the four QA datasets. Concretely, given a model and a question $Q$, we prompt the model to generate a false answer (see $\S B$ for details). We verify that these are indeed incorrect claims by checking that the gold answer (and any of its aliases, if they exist) does not occur in the generated text. This yields a subset of examples that are realistic, namely, the answer matches the target type (e.g., "a city") but is incorrect (see examples in Table 3). The examiner's decision for these examples should always be to reject.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>LAMA</th>
<th></th>
<th></th>
<th>TriviaQA</th>
<th></th>
<th></th>
<th>NQ</th>
<th></th>
<th></th>
<th>PopQA</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
</tr>
<tr>
<td>AYS</td>
<td>82.3</td>
<td>25.2</td>
<td>38.6</td>
<td>79.9</td>
<td>17.9</td>
<td>29.2</td>
<td>85.2</td>
<td>29.1</td>
<td>43.3</td>
<td>78.4</td>
<td>35.7</td>
<td>63.9</td>
</tr>
<tr>
<td>IDK</td>
<td>49.1</td>
<td>52.4</td>
<td>50.7</td>
<td>48.7</td>
<td>66.5</td>
<td>56.2</td>
<td>62.5</td>
<td>60.7</td>
<td>61.6</td>
<td>70.0</td>
<td>61.1</td>
<td>65.2</td>
</tr>
<tr>
<td>LMVLM</td>
<td>85.1</td>
<td>70.7</td>
<td>76.7</td>
<td>82.8</td>
<td>71.6</td>
<td>76.8</td>
<td>74.5</td>
<td>74.9</td>
<td>77.7</td>
<td>83.6</td>
<td>77.1</td>
<td>80.2</td>
</tr>
<tr>
<td>LMVLM (Majority)</td>
<td>$\mathbf{8 6 . 6}$</td>
<td>$\mathbf{7 5 . 8}$</td>
<td>$\mathbf{8 0 . 8}$</td>
<td>$\mathbf{8 4 . 5}$</td>
<td>$\mathbf{8 0 . 8}$</td>
<td>$\mathbf{8 2 . 6}$</td>
<td>$\mathbf{8 2 . 3}$</td>
<td>$\mathbf{7 6 . 1}$</td>
<td>$\mathbf{7 9 . 1}$</td>
<td>$\mathbf{8 7 . 0}$</td>
<td>$\mathbf{8 4 . 0}$</td>
<td>$\mathbf{8 5 . 4}$</td>
</tr>
<tr>
<td>- Follow-up</td>
<td>83.8</td>
<td>68.1</td>
<td>75.1</td>
<td>82.3</td>
<td>69.7</td>
<td>75.5</td>
<td>74.8</td>
<td>72.1</td>
<td>73.4</td>
<td>82.0</td>
<td>73.3</td>
<td>77.4</td>
</tr>
</tbody>
</table>
<p>Table 4: Precision (P), Recall (R), and F1 scores for LMVLM with ChatGPT as EXAMINER and EXAMINEE, compared to baselines. The last row shows an ablation of our method without the follow-up questions stage.</p>
<table>
<thead>
<tr>
<th></th>
<th>LAMA</th>
<th></th>
<th></th>
<th>TriviaQA</th>
<th></th>
<th></th>
<th>NQ</th>
<th></th>
<th></th>
<th>PopQA</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
</tr>
<tr>
<td>AYS</td>
<td>74.8</td>
<td>17.9</td>
<td>28.9</td>
<td>80.3</td>
<td>19.8</td>
<td>31.8</td>
<td>74.9</td>
<td>20.7</td>
<td>32.3</td>
<td>74.6</td>
<td>22.7</td>
<td>34.8</td>
</tr>
<tr>
<td>IDK</td>
<td>43.0</td>
<td>42.1</td>
<td>42.5</td>
<td>47.9</td>
<td>45.7</td>
<td>46.7</td>
<td>60.9</td>
<td>45.3</td>
<td>52.0</td>
<td>52.1</td>
<td>37.6</td>
<td>43.7</td>
</tr>
<tr>
<td>Confidence-Based</td>
<td>38.6</td>
<td>$\mathbf{8 5 . 8}$</td>
<td>53.2</td>
<td>39.6</td>
<td>$\mathbf{8 4 . 4}$</td>
<td>53.9</td>
<td>56.2</td>
<td>72.7</td>
<td>63.4</td>
<td>60.8</td>
<td>69.7</td>
<td>64.9</td>
</tr>
<tr>
<td>IC-IDK</td>
<td>71.5</td>
<td>46.3</td>
<td>56.2</td>
<td>70.6</td>
<td>49.7</td>
<td>60.1</td>
<td>70.0</td>
<td>57.6</td>
<td>63.2</td>
<td>76.9</td>
<td>37.7</td>
<td>50.6</td>
</tr>
<tr>
<td>LMVLM</td>
<td>78.8</td>
<td>69.9</td>
<td>74.1</td>
<td>81.6</td>
<td>64.6</td>
<td>72.1</td>
<td>70.5</td>
<td>66.6</td>
<td>68.5</td>
<td>75.5</td>
<td>69.1</td>
<td>72.2</td>
</tr>
<tr>
<td>LMVLM (Majority)</td>
<td>$\mathbf{8 0 . 7}$</td>
<td>77.9</td>
<td>$\mathbf{7 9 . 3}$</td>
<td>$\mathbf{8 3 . 1}$</td>
<td>72.1</td>
<td>$\mathbf{7 7 . 2}$</td>
<td>$\mathbf{7 9 . 3}$</td>
<td>$\mathbf{7 6 . 8}$</td>
<td>$\mathbf{7 8 . 0}$</td>
<td>$\mathbf{8 2 . 2}$</td>
<td>$\mathbf{7 1 . 4}$</td>
<td>$\mathbf{7 6 . 4}$</td>
</tr>
<tr>
<td>- Follow-up</td>
<td>76.4</td>
<td>71.1</td>
<td>73.7</td>
<td>78.7</td>
<td>64.8</td>
<td>71.1</td>
<td>66.6</td>
<td>70.1</td>
<td>68.3</td>
<td>70.9</td>
<td>65.8</td>
<td>68.3</td>
</tr>
</tbody>
</table>
<p>Table 5: Precision (P), Recall (R), and F1 scores for LMVLM with GPT-3 as EXAMINER and EXAMINEE, compared to baselines. The last row shows an ablation of our method without the follow-up questions stage.</p>
<p>Models We use ChatGPT (gpt-3.5-turbo), GPT-3 (text-davinci-003) (Brown et al., 2020; Ouyang et al., 2022), and LLAMA-7B (Touvron et al., 2023), in three EXAMINER vs. EXAMINEE cross-examination settings: GPT-3 vs. GPT-3, ChatGPT vs. ChatGPT, and ChatGPT vs. LLAMA. Notably, using the same LM as EXAMINER and EXAMINEE (except for their prompts, which are different), provides a cleaner setting where both LMs share the same knowledge. The prompts used for each LM at every stage of the examination are shown in Table 1.</p>
<p>Baselines For each setting, we compare LMVLM with recent methods for uncertainty detection and variants of our approach:</p>
<ul>
<li>Confidence-Based: The prediction head of LMs outputs a probability for the predicted token. It is a common practice to use this probability as a measure of confidence in the prediction (Yoshikawa and Okazaki, 2023). In our case, the LM generates a multi-token claim, and we calculate the confidence for the claim as the product of probabilities for all predicted tokens of the answer only. In order to output a binary decision (i.e., is the claim correct or not), we optimize a threshold over the train dataset to maximize F1. Note that our examination approach does not require tuning any threshold.</li>
<li>Are you sure? (AYS): Recent work (Kadavath et al., 2022; Cohen et al., 2023) has shown that LMs can be trained to estimate their certainty in generated facts. Here, we use a zero-shot version of this approach where we directly "ask" the model whether it is sure. Specifically, we add the following prompt right after the claim generation: "Are you sure regarding the correctness of your claim? Please answer with Yes or No". Then we take the output as the prediction whether the claim is correct or not.</li>
<li>I don’t know (IDK): Recently, Ganguli et al. (2023) showed that LMs might have the capability to self-correct themselves, when instructed to do so. Here we instruct the model to output "I don't know" if it is uncertain, by concatenating the following sentence to the original query: "If you are not sure you know the answer, answer with 'I don't know' only.". If the model answers 'I don’t know’ we label the corresponding claim as false, and otherwise true.</li>
<li>In-context IDK (IC-IDK): We teach the model to output that it doesn't know the answer, via incontext demonstrations. We follow Cohen et al. (2023) and test each of the queries within an incontext setting. For each query, we first provide the model with $K$ demonstrations, as $D$ of them labeled as "Don't know" examples, while the rest $K-D$ are provided with their gold answer from the dataset. The "Don't know" examples are randomly selected out of a set of examples the model</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>LAMA</th>
<th></th>
<th></th>
<th>TriviaQA</th>
<th></th>
<th></th>
<th>NQ</th>
<th></th>
<th></th>
<th>PopQA</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
</tr>
<tr>
<td>AYS</td>
<td>61.4</td>
<td>38.0</td>
<td>46.9</td>
<td>60.0</td>
<td>35.7</td>
<td>44.8</td>
<td>71.1</td>
<td>15.0</td>
<td>24.8</td>
<td>74.8</td>
<td>14.2</td>
<td>23.9</td>
</tr>
<tr>
<td>IC-IDK</td>
<td>56.6</td>
<td>49.0</td>
<td>52.5</td>
<td>58.9</td>
<td>52.5</td>
<td>55.5</td>
<td>66.2</td>
<td>53.4</td>
<td>59.1</td>
<td>66.8</td>
<td>50.1</td>
<td>57.3</td>
</tr>
<tr>
<td>IDK</td>
<td>61.6</td>
<td>44.8</td>
<td>51.9</td>
<td>62.0</td>
<td>32.9</td>
<td>43.0</td>
<td>64.4</td>
<td>12.1</td>
<td>20.4</td>
<td>66.7</td>
<td>16.8</td>
<td>26.8</td>
</tr>
<tr>
<td>Confidence-Based</td>
<td>54.9</td>
<td>$\mathbf{7 6 . 7}$</td>
<td>64.0</td>
<td>56.9</td>
<td>$\mathbf{8 5 . 8}$</td>
<td>68.4</td>
<td>64.4</td>
<td>63.5</td>
<td>63.9</td>
<td>64.6</td>
<td>53.6</td>
<td>58.6</td>
</tr>
<tr>
<td>LMvLM</td>
<td>81.1</td>
<td>66.4</td>
<td>73.0</td>
<td>80.1</td>
<td>70.8</td>
<td>75.2</td>
<td>79.3</td>
<td>65.5</td>
<td>71.7</td>
<td>84.9</td>
<td>73.6</td>
<td>78.8</td>
</tr>
<tr>
<td>LMvLM (Majority)</td>
<td>$\mathbf{8 2 . 9}$</td>
<td>73.9</td>
<td>$\mathbf{7 8 . 1}$</td>
<td>$\mathbf{8 0 . 3}$</td>
<td>76.8</td>
<td>$\mathbf{7 8 . 5}$</td>
<td>$\mathbf{8 3 . 7}$</td>
<td>$\mathbf{7 4 . 2}$</td>
<td>$\mathbf{7 8 . 7}$</td>
<td>$\mathbf{8 8 . 3}$</td>
<td>$\mathbf{7 7 . 4}$</td>
<td>$\mathbf{8 2 . 5}$</td>
</tr>
<tr>
<td>- Follow-up</td>
<td>79.7</td>
<td>65.7</td>
<td>72.0</td>
<td>80.0</td>
<td>69.8</td>
<td>74.6</td>
<td>79.4</td>
<td>63.7</td>
<td>70.7</td>
<td>83.3</td>
<td>71.8</td>
<td>77.1</td>
</tr>
</tbody>
</table>
<p>Table 6: Precision (P), Recall (R), and F1 scores for LMvLM with ChatGPT as EXAMINER and LLAMA as EXAMINEE, compared to baselines. The last row is an ablation of our method without the follow-up questions stage.</p>
<table>
<thead>
<tr>
<th></th>
<th>LAMA</th>
<th>TriviaQA</th>
<th>NQ</th>
<th>PopQA</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3</td>
<td>65.7</td>
<td>98.4</td>
<td>89.9</td>
<td>83.1</td>
</tr>
<tr>
<td>GPT-3 (Majority)</td>
<td>75.8</td>
<td>98.5</td>
<td>92.0</td>
<td>88.0</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>83.6</td>
<td>97.9</td>
<td>90.4</td>
<td>88.8</td>
</tr>
<tr>
<td>ChatGPT (Majority)</td>
<td>87.1</td>
<td>98.6</td>
<td>94.2</td>
<td>93.9</td>
</tr>
</tbody>
</table>
<p>Table 7: Accuracy of GPT-3 and ChatGPT as EXAMINER on false claims generated for each dataset.
failed on, while evaluating it on an held-out set of examples from the dataset in a zero-shot setting. Intuitively, these examples' answers are likely to be unknown to the model, hence we labeled them with "Don't know". The model predictions are either a target text or "Don't know". Based on the output, we generate a factuality label as in the IDK baseline above. Notably, this baseline requires labeled data for the in-context demonstrations, which is not necessary for our approach.</p>
<ul>
<li>LMvLM: A single execution of our method, where we accept or reject the claim according to the examiner's final decision.</li>
<li>LMvLM (Majority): For a given claim, we apply our method three times (with the same EXAMINER and EXAMINEE), using sampling generation for follow-up questions generation. We reject the claim in case at least two of the examinations concluded it is false.
Since output probabilities are not provided as part of the ChatGPT's API, we cannot provide results for the Confidence-Based baselines in this case. Moreover, we observe that ChatGPT often fails to understand the task of IC-IDK.</li>
</ul>
<h3>4.2 Results</h3>
<p>Tables 4, 5, 6 show the results for the settings ChatGPT vs. ChatGPT, GPT-3 vs. GPT-3, and LLAMA vs. ChatGPT, respectively. Across</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">ChatGPT / <br> ChatGPT</th>
<th style="text-align: center;">GPT-3 / <br> GPT-3</th>
<th style="text-align: center;">ChatGPT / <br> LLAMA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># of questions</td>
<td style="text-align: center;">$7.0 \pm 2.8$</td>
<td style="text-align: center;">$6.4 \pm 4.3$</td>
<td style="text-align: center;">$6.8 \pm 4.4$</td>
</tr>
<tr>
<td style="text-align: left;"># of follow-up <br> questions per <br> iteration</td>
<td style="text-align: center;">$1.3 \pm 1.0$</td>
<td style="text-align: center;">$1.3 \pm 0.6$</td>
<td style="text-align: center;">$1.1 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: left;"># of follow-up iter- <br> ations</td>
<td style="text-align: center;">$1.9 \pm 1.2$</td>
<td style="text-align: center;">$1.3 \pm 0.7$</td>
<td style="text-align: center;">$1.6 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: left;"># of questions per <br> iteration</td>
<td style="text-align: center;">$3.1 \pm 2.1$</td>
<td style="text-align: center;">$2.7 \pm 1.6$</td>
<td style="text-align: center;">$2.9 \pm 1.9$</td>
</tr>
<tr>
<td style="text-align: left;">\% of inconclusive <br> examiner decisions</td>
<td style="text-align: center;">$14.8 \%$</td>
<td style="text-align: center;">$9.1 \%$</td>
<td style="text-align: center;">$10.3 \%$</td>
</tr>
</tbody>
</table>
<p>Table 8: Cross-examination statistics for each setting (EXAMINER/EXAMINEE), averaged across datasets.
all settings, our method outperforms the baselines, often by a large gap. For example, it obtains 85.4 F1 compared to $\leq 65.2$ by baselines for CHATGPT on PopQA (Table 4), and 77.2 F1 compared to $\leq 60.1$ for GPT-3 on TriviaQA (Table 5). Notably, the most substantial gains are in terms of recall, showing the superiority of our method in detecting factually incorrect claims (when compared to the baselines which achieve reasonable precision too). Interestingly, we observe that ChatGPT generally outperforms GPT-3.</p>
<p>Last, Table 7 shows the accuracy of our method and baselines on our Falsehood dataset. For both ChatGPT and GPT-3, LMvLM successfully rejects the vast majority of the false claims, obtaining $87 \%-98 \%$ accuracy with ChatGPT and $75 \%-99 \%$ with GPT-3 across all datasets.</p>
<h3>4.3 Ablations</h3>
<p>We perform an ablation, where we remove the follow-up iterations in the examination process to gauge their benefit. Results are reported for GPT-3 in Table 5 (last row), showing a large decrease in performance (e.g. $78 \rightarrow 68.3$ in F1 for NQ and $77.2 \rightarrow 71.1$ for TriviaQA). Notably, recall scores</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Pattern</th>
<th style="text-align: center;">Example statements/questions generated by EXAMINER during examination</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Rephrasing the claim</td>
<td style="text-align: center;">Claim: "The first Fast and Furious film was released in 2001." In which year was the first Fast and Furious film released?</td>
</tr>
<tr>
<td style="text-align: center;">Rephrasing Questions</td>
<td style="text-align: center;">Claim: "The screenwriter who is credited with writing the screenplay for Winner is Wendy Riss" 1. What is the name of the screenwriter who is credited with writing the screenplay for Winner? 2. Who is credited with writing the screenplay for Winner?</td>
</tr>
<tr>
<td style="text-align: center;">Validation of Implications</td>
<td style="text-align: center;">Claim: "The director of The Town was Ben Affleck." Is Ben Affleck known for directing any movies?</td>
</tr>
<tr>
<td style="text-align: center;">Logical decomposition</td>
<td style="text-align: center;">Claim: "The second oldest of the Pevensie children in C S Lewis's The Lion, the Witch and the Wardrobe is Edmund." <br> 1. What is the birth order of the Pevensie children in C S Lewis's The Lion, the Witch and the Wardrobe? 2. What are their ages? 3. Who appears second in this list?</td>
</tr>
<tr>
<td style="text-align: center;">Request for attribution</td>
<td style="text-align: center;">Claim: "The screenwriter of Cover Up is Bill Blake" <br> Is there any evidence or documentation that supports the claim that Bill Blake was the screenwriter for Cover Up?</td>
</tr>
<tr>
<td style="text-align: center;">Wrong intermediate answers</td>
<td style="text-align: center;">Claim: "There are eight vertices (corners) on an octahedron." <br> EXAMINER: How many vertices does an octahedron have? EXAMINEE: An octahedron has eight vertices, each of which is the point where three edges meet.</td>
</tr>
</tbody>
</table>
<p>Table 9: Examples for frequent patterns of ChatGPT and GPT-3 observed through manual analysis of crossexaminations.
are decreased by $6 \%-10 \%$. Overall, this shows the importance of the follow-up questions issued by the examiner to assess the examinee's claim.</p>
<h2>5 Analysis of Cross-Examinations</h2>
<p>We analyze cross-examinations by GPT-3 and ChatGPT to better understand the success and failure cases of our method. We find that examiner LMs typically ask multiple questions in the examination, and perhaps surprisingly, apply different strategies to reveal inconsistencies.</p>
<p>Examination Statistics Table 8 provides statistics on the cross-examinations performed by CHATGPT and GPT-3. Both models introduce multiple queries (6-7 on average) during an examination, with typically 1-2 steps of follow-up questions, which are important for the examiner's decision (§4.3). We also observe a non-negligible number of claims ( $9 \%-15 \%$ ) where the examiner LM does not arrive at a concrete final decision (i.e., it does not generate "correct" or "incorrect" as the final decision, we reject the claim in those cases). In our qualitative analysis, we identify reasons that could explain these cases.</p>
<p>Qualitative Analysis We manually analyze a sample of 96 examinations - 48 by each LM, with 6 correct and 6 incorrect examinations for each model and each dataset. We observe the following trends (examples are in Table 9):</p>
<ol>
<li>Rephrasing the claim: In about $60 \%$ of the examinations, both LMs introduce questions
which are paraphrases of the original question. This supports the assumption that the EXAMINER seeks inconsistencies by generating variants of the original claim.</li>
<li>Rephrasing Questions: In about half of the examinations, both LMs introduce questions that are similar to previously asked questions or have a different phrasing. This is a desirable behavior as it can reveal inconsistencies if the examinee provides a different answer for the same question.</li>
<li>Validation of logical implications: The EXAMINER asks EXAMINEE regarding implied arguments that must be true whenever the original claim is correct. This can be observed in $70 \%$ of the correct detections of GPT-3, and $87.5 \%$ out of the correct detections of CHATGPT.</li>
<li>Logical questions: The EXAMINER decomposes the claim into multiple sub-questions which together compose a trajectory to validating it. Such decompositions appear in about $75 \%$ of the cases for CHATGPT but only $10 \%$ in GPT-3. We observe these in $33 \%$ of the correct detections of GPT-3, and $70 \%$ for CHATGPT.</li>
<li>Request for attribution: The EXAMINER ask the EXAMINEE about the existence of external evidence to support the claim. This happens in about $30 \%$ of the cases for both LMs.</li>
<li>Wrong intermediate answers: The EXAMINEE responds with factually incorrect answers to one or more of the questions originated by</li>
</ol>
<p>the EXAMINER. We observe this occurs mostly in cases where the original claim is false (it happens in only in about $14 \%$ of the cases where the EXAMINEE is correct). In both models, this can be observed in about half of the cases where the claim is false and has also been detected by the EXAMINER. Furthermore, it occurs in about $80 \%$ of the cases where the EXAMINER has accepted a false claim, and in $45 \%$ where the EXAMINER has rejected a correct claim.</p>
<p>We note that in most cases where LMVLM fails, EXAMINEE has provided incorrect information to EXAMINER. This might indicate that in those cases EXAMINEE has encoded a large set of factually wrong facts that are mutually consistent, thus making it hard for the EXAMINER to detect inconsistencies. Finally, the fact that CHATGPT more commonly validates the claim through logical questions might be a key factor in its superiority over GPT-3 in our setting.</p>
<h2>6 Conclusion</h2>
<p>We introduce LMVLM, a method for zero-shot detection of factuality errors, inspired by the crossexamination practice employed in a court of law. Our method uses prompting to facilitate a multiturn interaction between an examiner LM and an examinee LM, to reveal inconsistencies that imply factually incorrect claims. We evaluate LMVLM in the context of factual question answering, showing it substantially improves detection of factual errors made by LMs.</p>
<p>Our method builds on a fundamental connection between self-consistency (i.e., consistency of an LM with itself) and factual consistency (i.e., consistency between factual claims generated by an LM and ground-truth facts). We consider the LM itself as the source of information, and we test whether a claim it has generated is faithful and consistent with several other beliefs it has.</p>
<p>Our work can be extended in several ways. First, LMVLM provides interpretable information about related beliefs of the model, which could be analyzed to understand what makes the model commit certain mistakes. Second, one may incorporate several LM instances into the factuality detection process, rather the having only a single EXAMINER. Finally, one can train the EXAMINER to generate questions more effectively.</p>
<h2>Limitations</h2>
<p>We note three limitations of our method LMVLM. First, unlike other methods, it requires multiple queries of the examinee and examiner LMs, which could be costly when using external APIs such as those used in this work. This could be a key consideration when scaling this approach to large numbers of claims.</p>
<p>Second, for our method to succeed, both LMs (EXAMINEE and EXAMINER), but mostly EXAMINER, should be able to follow instructions and have the ability to reason over information in a relatively long context. This skill is currently mostly demonstrated by larger models ( $&gt;10 \mathrm{~B}$ parameters) and thus our method may not perform as well for smaller models.</p>
<p>Last, any logical flaws in the examiner's operation are likely to affect the overall examination, potentially leading to inaccurate decisions. However, our experiments show that, even if such flaws occur, our method is still useful on average as it substantially improves factuality detection. Nonetheless, developing safety mechanisms that detect and mitigate logical flaws is an important research direction, that we leave for future work.</p>
<h2>Acknowledgements</h2>
<p>We thank Roee Aharoni and Avi Caciularu for valuable feedback and constructive suggestions. This work is supported in part by the Israeli Science Foundation.</p>
<h2>References</h2>
<p>Dimitrios Alivanistos, Selene Báez Santamaría, Michael Cochez, Jan-Christoph Kalo, Emile van Krieken, and Thiviyan Thanapalasingam. 2022. Prompting as probing: Using language models for knowledge base construction. arXiv preprint arXiv:2208.11057.</p>
<p>Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. 2016. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565.</p>
<p>Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. 2022. Fact checking with insufficient evidence. Transactions of the Association for Computational Linguistics, 10:746-763.</p>
<p>Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui,</p>
<p>et al. 2022. Attributed question answering: Evaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Miles Brundage, Shahar Avin, Jasmine Wang, Haydn Belfield, Gretchen Krueger, Gillian Hadfield, Heidy Khlaaf, Jingying Yang, Helen Toner, Ruth Fong, et al. 2020. Toward trustworthy ai development: mechanisms for supporting verifiable claims. arXiv preprint arXiv:2004.07213.</p>
<p>Jannis Bulian, Christian Buck, Wojciech Gajewski, Benjamin Börschinger, and Tal Schuster. 2022. Tomayto, tomahto. beyond token-level answer equivalence for question answering evaluation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 291-305, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Roi Cohen, Mor Geva, Jonathan Berant, and Amir Globerson. 2023. Crawling the internal knowledgebase of language models. In Findings of the Association for Computational Linguistics: EACL 2023, pages 1856-1869, Dubrovnik, Croatia. Association for Computational Linguistics.</p>
<p>Shrey Desai and Greg Durrett. 2020. Calibration of pre-trained transformers. arXiv preprint arXiv:2003.07892.</p>
<p>Ashwin Devaraj, William Sheffield, Byron Wallace, and Junyi Jessy Li. 2022. Evaluating factuality in text simplification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 73317345, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. 2022. Evaluating attribution in dialogue systems: The BEGIN benchmark. Transactions of the Association for Computational Linguistics, 10:1066-1083.</p>
<p>Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, and Yoav Goldberg. 2021. Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics, 9:1012-1031.</p>
<p>Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamilé Lukoliūtė, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, et al. 2023. The capacity for moral selfcorrection in large language models. arXiv preprint arXiv:2302.07459.</p>
<p>Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Y Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. 2022. Rarr: Researching and revising what language models say, using language models. arXiv preprint arXiv:2210.08726.</p>
<p>Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen Elkind, and Idan Szpektor. 2023. Trueteacher: Learning factual consistency evaluation with large language models. arXiv preprint arXiv:2305.11171.</p>
<p>Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1321-1330. PMLR.</p>
<p>Shibo Hao, Bowen Tan, Kaiwen Tang, Hengzhe Zhang, Eric P Xing, and Zhiting Hu. 2022. Bertnet: Harvesting knowledge graphs from pretrained language models. arXiv preprint arXiv:2206.14268.</p>
<p>Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. 2021. Unsolved problems in ml safety. arXiv preprint arXiv:2109.13916.</p>
<p>Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. 2022. True: Re-evaluating factual consistency evaluation. arXiv preprint arXiv:2204.04991.</p>
<p>Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. $q^{2}$ : Evaluating factual consistency in knowledgegrounded dialogues via question generation and question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7856-7870, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. 2022. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608.</p>
<p>Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2021. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9:962-977.</p>
<p>Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551.</p>
<p>Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. 2022. Maieutic prompting: Logically consistent reasoning with recursive explanations. In Conference on Empirical Methods in Natural Language Processing.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221.</p>
<p>Amita Kamath, Robin Jia, and Percy Liang. 2020. Selective question answering under domain shift. arXiv preprint arXiv:2006.09462.</p>
<p>Ryo Kamoi, Tanya Goyal, and Greg Durrett. 2023. Shortcomings of question answering based factuality frameworks for error localization. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 132-146, Dubrovnik, Croatia. Association for Computational Linguistics.</p>
<p>Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334.</p>
<p>Nelson F Liu, Tianyi Zhang, and Percy Liang. 2023a. Evaluating verifiability in generative search engines. arXiv preprint arXiv:2304.09848.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023b. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35.</p>
<p>Jieyi Long. 2023. Large language model guided tree-of-thought. arXiv preprint arXiv:2305.08291.</p>
<p>Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. 2022. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, Online. Association for Computational Linguistics.</p>
<p>Kevin Meng, David Bau, Alex J Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. In Advances in Neural Information Processing Systems.</p>
<p>Benjamin Newman, Prafulla Kumar Choubey, and Nazneen Rajani. 2021. P-adapters: Robustly extracting factual information from language models with diverse prompts. arXiv preprint arXiv:2110.07280.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Joon Sung Park, Lindsay Popowski, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2022. Social simulacra: Creating populated prototypes for social computing systems. In The 35th Annual ACM Symposium on User Interface Software and Technology, UIST 2022, Bend, OR, USA, 29 October 2022 - 2 November 2022, pages 74:1-74:18. ACM.</p>
<p>Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2019. Language models as knowledge bases? arXiv preprint arXiv:1909.01066.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350.</p>
<p>Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying LMs with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5203-5212, Online. Association for Computational Linguistics.</p>
<p>Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2021. Measuring attribution in natural language generation models. arXiv preprint arXiv:2112.12870.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022. Peer: A collaborative language model. arXiv preprint arXiv:2208.11663.</p>
<p>Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick Gallinari. 2021. QuestEval: Summarization asks for fact-based evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6594-6604, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an autonomous agent with dynamic memory and self-reflection. CoRR, abs/2303.11366.</p>
<p>Meiqi Sun, Wilson Yan, Pieter Abbeel, and Igor Mordatch. 2022. Quantifying uncertainty in foundation models via ensembles. In NeurIPS 2022 Workshop on Robustness in Sequence Modeling.</p>
<p>Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, and Colin Raffel. 2022. Evaluating the factual consistency of large language models through summarization. arXiv preprint arXiv:2211.08412.</p>
<p>James Thorne, Andreas Vlachos, Oana Cocarascu, Christos Christodoulopoulos, and Arpit Mittal. 2018. The fact extraction and VERification (FEVER) shared task. In Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 1-9, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Neeraj Varshney, Swaroop Mishra, and Chitta Baral. 2022. Investigating selective prediction approaches across several tasks in IID, OOD, and adversarial settings. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1995-2002, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008-5020, Online. Association for Computational Linguistics.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Sean Welleck, Jason Weston, Arthur Szlam, and Kyunghyun Cho. 2019. Dialogue natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3731-3741, Florence, Italy. Association for Computational Linguistics.</p>
<p>Billy Wilder, Agatha Christie, Harry Kurnitz, Alexandre Trauner, Tyrone Power, Marlène Dietrich, and Charles Laughton. 1957. Witness for the Prosecution. United Artists.</p>
<p>Dustin Wright, David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Isabelle Augenstein, and Lucy Lu Wang. 2022. Generating scientific claims for zeroshot scientific fact checking. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2448-2460, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. 2023. Answering questions by meta-reasoning over multiple chains of thought. arXiv preprint arXiv:2304.13007.</p>
<p>Hiyori Yoshikawa and Naoaki Okazaki. 2023. Selective-LAMA: Selective prediction for confidence-aware evaluation of language models. In Findings of the Association for Computational Linguistics: EACL 2023, pages 2017-2028, Dubrovnik, Croatia. Association for Computational Linguistics.</p>
<p>Polina Zablotskaia, Du Phan, Joshua Maynez, Shashi Narayan, Jie Ren, and Jeremiah Liu. 2023. On uncertainty calibration and selective generation in probabilistic neural summarization: A benchmark study. arXiv preprint arXiv:2304.08653.</p>
<p>Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021. Factual probing is [mask]: Learning vs. learning to recall. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5017-5033.</p>
<p>Chunting Zhou, Junxian He, Xuezhe Ma, Taylor BergKirkpatrick, and Graham Neubig. 2022. Prompt consistency for zero-shot task generalization. arXiv preprint arXiv:2205.00049.</p>
<p>A Additional Evaluation</p>
<p>We follow the same experimental setting as in §4, but evaluate performance with respect to acceptance of claims rather than rejection. In addition, we introduce an ensemble AYS + LMVLM; for a given claim, we first run the AYS method, and if the claim is rejected by this method we then apply LMVLM (Majority) to obtain a final decision.</p>
<p>Tables 10 and 11 shows the evaluation results for the settings of ChatGPT vs. ChatGPT and GPT-3 vs. GPT-3, respectively.</p>
<p>In terms of precision, our method outperforms the other baselines, often by a large gap (e.g., 81.6 compared to $\leq 60$ by baselines for ChatGPT on PopQA, and 68.7 compared to $\leq 52.4$ for GPT-3 on PopQA). Moreover, this is while maintaining a good recall performance with respect to the baselines, except for AYS that obtains the best recall scores. Importantly, surpass the baselines across all settings, overall obtaining the best F1 scores of $83.3-95.8$ for ChatGPT and $74-94.9$ for GPT-3 for factuality verification. Moreover, unlike other methods, LMVLM performs consistently well across datasets, including PopQA and NQ, which consider more challenging queries by real users or about rare entities that LMs struggle with (Mallen et al., 2022).</p>
<h1>B Falsehoods Data</h1>
<p>To generate a wrong claim, given a query $Q$ for one of the QA datasets we use, we prompt our models the following way: in case $Q$ is in a question format, we use "Please answer the following question with a wrong answer: <Q>" and further request the LM to "Please also phrase your answer as an argument.". in case $Q$ is in a sentence-completion format, we use "Please complete the following sentence with a wrong answer: <Q>" and further concatenate $Q$ with the model answer. Table 12 introduces a few examples of these, generated by GPT-3.</p>
<h2>C Example Cross-Examinations</h2>
<p>Full cross-examination examples are provided in Tables 13, 14, 15, 16, 17, 18, 19, 20.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">LAMA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TriviaQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PopQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">IDK</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">63.9</td>
</tr>
<tr>
<td style="text-align: center;">AYS</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">66.6</td>
</tr>
<tr>
<td style="text-align: center;">LMVLM</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">97.1</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">78.3</td>
</tr>
<tr>
<td style="text-align: center;">LMVLM (Majority)</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">97.2</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">83.3</td>
</tr>
<tr>
<td style="text-align: center;">AYS + LMVLM (Ensemble)</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">87.7</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">98.1</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">63.2</td>
</tr>
</tbody>
</table>
<p>Table 10: Precision (P), Recall (R), and F1 scores of ChatGPT as Examiner and ExAmineE.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">LAMA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TriviaQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PopQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">Confidence-Based</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">47.0</td>
</tr>
<tr>
<td style="text-align: center;">IDK</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">47.4</td>
</tr>
<tr>
<td style="text-align: center;">IC-IDK</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">64.5</td>
</tr>
<tr>
<td style="text-align: center;">AYS</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">90.1</td>
<td style="text-align: center;">62.4</td>
</tr>
<tr>
<td style="text-align: center;">LMVLM</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">67.7</td>
</tr>
<tr>
<td style="text-align: center;">LMVLM (Majority)</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">74.0</td>
</tr>
<tr>
<td style="text-align: center;">AYS + LMVLM (Ensemble)</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">61.8</td>
</tr>
</tbody>
</table>
<p>Table 11: Precision (P), Recall (R), and F1 scores of GPT-3 as ExAmineR and ExAmineE.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">False claim</th>
<th style="text-align: left;">True claim</th>
<th style="text-align: left;">Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">"Windows is an operating system developed by <br> Apple."</td>
<td style="text-align: left;">"Windows is an operating system developed by LAMA <br> Microsoft."</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">"The Hispaniolan lizard cuckoo (Coccyzus lon- <br> girostris) is a species of cuckoo in the Cuculidae <br> family.It is found in the Dominican Republic and <br> Honduras."</td>
<td style="text-align: left;">"The Hispaniolan lizard cuckoo (Coccyzus lon- LAMA <br> girostris) is a species of cuckoo in the Cuculidae <br> family.It is found in the Dominican Republic and <br> Haiti."</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">"The first modern electric battery was demon- <br> strated by Thomas Edison, an American inventor."</td>
<td style="text-align: left;">"The first modern electric battery was demon- TriviaQA <br> strated by Alessandro Volta."</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">"I believe that the actor who played Rockford's <br> father, "Rocky," in the TV series, "The Rockford <br> Files," was Tom Selleck."</td>
<td style="text-align: left;">"The actor who played Rockford's father, "Rocky," TriviaQA <br> in the TV series, "The Rockford Files," was Noah <br> Beery Jr."</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">"The Taurus Mountains are located in the United <br> States, specifically in the state of California."</td>
<td style="text-align: left;">"The Taurus Mountains are located in the south- NQ <br> ern Turkey"</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">"I heard that Taylor Swift is doing the 2018 Super <br> Bowl Half Time Show."</td>
<td style="text-align: left;">"Justin Timberlake was the featured performer in NQ <br> the 2018 Super Bowl Half Time Show."</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">"Red Velvet is a type of cake"</td>
<td style="text-align: left;">"Red Velvet is a genre of music."</td>
<td style="text-align: left;">PopQA</td>
</tr>
<tr>
<td style="text-align: left;">"The author of The Jacket is J.K. Rowling, be- <br> cause she is the author of the Harry Potter series."</td>
<td style="text-align: left;">"The author of The Jacker is Andrew Clements." PopQA</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 12: Example false claims generated by GPT-3 for our four QA datasets.</p>
<p>EXAMINEE: The song 'I'm in the Mood for Dancing' was written by Irish songwriter and musician, Ben Findon.</p>
<h1>EXAMINER:</h1>
<ol>
<li>Who is the artist that recorded the song "I'm in the Mood for Dancing"?</li>
<li>When was the song "I'm in the Mood for Dancing" first released?</li>
<li>Who are the listed songwriters for "I'm in the Mood for Dancing"?</li>
<li>What is the nationality of songwriter Ben Findon?</li>
<li>Are there any credible sources or references that credit Ben Findon as the writer of "I'm in the Mood for Dancing"?</li>
</ol>
<p>EXAMINEE: As an AI language model, I can answer the following questions based on available data:</p>
<ol>
<li>The song "I'm in the Mood for Dancing" was originally recorded by the Irish family band The Nolan Sisters, later known as The Nolans.</li>
<li>The song "I'm in the Mood for Dancing" was first released in 1979, the year in which it became a major international hit.</li>
<li>The listed songwriters for "I'm in the Mood for Dancing" are Ben Findon, Mike Myers, and Robert Puzey.</li>
<li>Ben Findon is a British songwriter.</li>
<li>
<p>Yes, there are credible sources that credit Ben Findon as the co-writer of "I'm in the Mood for Dancing." The song is registered with several music licensing agencies, such as ASCAP and BMI, which list Ben Findon as one of the song's writers. Additionally, the credits for the song are also listed in the liner notes of The Nolans' debut album of the same name.
EXAMINER: Can you provide any other sources or references that credit Ben Findon as the co-writer of "I'm in the Mood for Dancing"?
EXAMINEE: As an AI language model, I can provide additional sources or references that credit Ben Findon as the co-writer of "I'm in the Mood for Dancing."</p>
</li>
<li>
<p>AllMusic, a music database and online resource, lists Ben Findon as one of the songwriters for "I'm in the Mood for Dancing."</p>
</li>
<li>BMI, a performing rights organization, lists Ben Findon as a co-writer of "I'm in the Mood for Dancing" in their repertoire of musical works.</li>
<li>The official website of The Nolans also credits Ben Findon as one of the songwriters for "I'm in the Mood for Dancing."</li>
<li>The website of Discogs, a database and online marketplace for music, lists Ben Findon as a credited songwriter for the song.</li>
<li>The liner notes of The Nolans' album "I'm in the Mood..." credit Ben Findon, Mike Myers, and Robert Puzey as the song's writers.
These sources all provide additional evidence that Ben Findon is a co-writer of "I'm in the Mood for Dancing."
EXAMINER: Based on the available information, the claim that "The song 'I'm in the Mood for Dancing' was written by Irish songwriter and musician, Ben Findon" is incorrect. It appears that Ben Findon is only one of the co-writers of the song, alongside Mike Myers and Robert Puzey. The song was originally recorded by the Irish family band The Nolan Sisters, later known as The Nolans.</li>
</ol>
<p>Table 13: Example cross-examination with ChatGPT as Examiner and ExAmineE, for the claim "The song 'I'm in the Mood for Dancing' was written by Irish songwriter and musician, Ben Findon." (which is correct). For brevity, prompts are not shown.</p>
<p>EXAMINEE: The producer of Precious was Lee Daniels.</p>
<h1>EXAMINER :</h1>
<ol>
<li>Can you provide a list of the producers involved in the production of Precious?</li>
<li>Who was the director of Precious?</li>
<li>Did Lee Daniels win any awards for producing Precious?</li>
<li>What other movies has Lee Daniels produced?</li>
<li>Are there any interviews or articles where Lee Daniels specifically states that he was the producer of Precious?</li>
</ol>
<h2>EXAMINEE :</h2>
<ol>
<li>Sure, here is a list of producers involved in the production of "Precious": Lee Daniels, Gary Magness, Sarah Siegel-Magness, Oprah Winfrey, Tom Heller, and Tyler Perry.</li>
<li>The director of "Precious" is Lee Daniels.</li>
<li>Yes, Lee Daniels won several awards for producing "Precious," including the Academy Award for Best Picture in 2010.</li>
<li>Lee Daniels has produced several other films throughout his career, including "The Paperboy," "Monster's Ball," "The Woodsman," and "The Butler."</li>
<li>Yes, there are several interviews and articles in which Lee Daniels specifically states that he was one of the producers of "Precious." In fact, he is often credited as one of the driving forces behind the film's success and is known for his efforts to bring the story to the screen.
EXAMINER: Based on the provided answers, the claim that the producer of "Precious" was Lee Daniels is correct.</li>
</ol>
<p>Table 14: Example cross-examination with ChatGPT as EXAMINER and EXAMINEE, for the claim "The producer of Precious was Lee Daniels." (which is correct). For brevity, prompts are not shown.</p>
<p>EXAMINEE: There are eight vertices (corners) on an octahedron</p>
<h2>EXAMINER :</h2>
<ol>
<li>What is an octahedron?</li>
<li>How many faces does an octahedron have?</li>
<li>How many edges does an octahedron have?</li>
<li>How many vertices does an octahedron have?</li>
<li>Are all vertices of an octahedron located at the same distance from the center?</li>
</ol>
<h2>EXAMINEE :</h2>
<ol>
<li>An octahedron is a three-dimensional geometric shape that is made up of eight equal equilateral triangles. It is a polyhedron with eight faces.</li>
<li>An octahedron has eight faces, each of which is an equilateral triangle.</li>
<li>An octahedron has 12 edges that connect its eight vertices.</li>
<li>An octahedron has eight vertices, each of which is the point where three edges meet.</li>
<li>No, not all vertices of an octahedron are located at the same distance from the center. There are two types of vertices in an octahedron: those at the corners of the shape, which are equidistant from the center, and those located at the center of each face, which are further away from the center than the corner vertices.
EXAMINER: Based on the answers, the claim that "there are eight vertices on an octahedron" is correct.</li>
</ol>
<p>Table 15: Example cross-examination with ChatGPT as Examiner and ExAmineE, for the claim "There are eight vertices (corners) on an octahedron." (which is incorrect). For brevity, prompts are not shown.</p>
<p>EXAMINEE : Augustus was the first Roman Emperor to sport a beard.</p>
<h1>EXAMINER :</h1>
<ol>
<li>Who was Augustus?</li>
<li>What is the definition of a Roman Emperor?</li>
<li>Was it common for Roman Emperors to wear beards?</li>
<li>Who were the previous Roman Emperors before Augustus?</li>
<li>Are there any historical records or paintings that depict Augustus with a beard?</li>
</ol>
<h2>EXAMINEE :</h2>
<ol>
<li>Augustus was the first Roman Emperor who ruled from 27 BCE to 14 CE. He was the successor of Julius Caesar and the founder of the Roman Empire.</li>
<li>A Roman Emperor was a political leader who held supreme authority in the Roman Empire, including military, judicial, and legislative powers. The position was established after Julius Caesar's assassination, and Augustus was the first to hold the title officially.</li>
<li>No, it wasn't common for Roman Emperors to wear beards. In fact, most Roman Emperors were clean-shaven, and wearing a beard was perceived as a symbol of being barbaric or uncivilized.</li>
<li>The previous rulers before Augustus were known as Roman Republic's leaders, such as Julius Caesar, Pompey the Great, Marcus Licinius Crassus, and others.</li>
<li>There are no known historical records or paintings that depict Augustus with a beard. Most of the depictions show him with a clean-shaven face or a small mustache. Additionally, Augustus himself was known for promoting a clean-shaven look, and he even encouraged his advisors and officials to do the same.
EXAMINER: Based on the information that has been gathered, the claim that Augustus was the first Roman Emperor to sport a beard is incorrect. In fact, it was not common for Roman Emperors to wear beards, and Augustus was known for promoting a clean-shaven look. Additionally, there are no known historical records or paintings that depict Augustus with a beard.</li>
</ol>
<p>Table 16: Example cross-examination with ChatGPT as Examiner and ExAmineE, for the claim "Augustus was the first Roman Emperor to sport a beard." (which is incorrect). For brevity, prompts are not shown.</p>
<p>EXAMINEE : The capital of Eritrea is Asmara.</p>
<p>EXAMINER : What is the official record of the capital of Eritrea according to the government?</p>
<p>EXAMINEE : The official record of the capital of Eritrea according to the government is Asmara.</p>
<p>EXAMINER : Does the government of Eritrea recognize any other cities as official capitals?</p>
<p>EXAMINEE : No, the government of Eritrea only recognizes Asmara as its official capital.</p>
<p>EXAMINER : Based on the answers to my questions, I conclude that the claim that the capital of Eritrea is Asmara is correct.</p>
<p>Table 17: Example cross-examination with GPT-3 as EXAMINER and EXAMINEE, for the claim "The capital of Eritrea is Asmara" (which is correct). For brevity, prompts are not shown.</p>
<p>EXAMINEE : Croatian composer Igor Stravinsky was born in Zagreb.</p>
<p>EXAMINER : Where was Igor Stravinsky born?</p>
<p>EXAMINEE: Igor Stravinsky was born in Oranienbaum, Russia (now Lomonosov, Russia) on June 17, 1882.</p>
<p>EXAMINER : What was the nationality of Igor Stravinsky's parents?</p>
<p>EXAMINEE : Igor Stravinsky's parents were both Russian.</p>
<p>EXAMINER : Based on the information provided, the claim that Croatian composer Igor Stravinsky was born in Zagreb is incorrect.</p>
<p>Table 18: Example cross-examination with GPT-3 as EXAMINER and EXAMINEE, for the claim "Croatian composer Igor Stravinsky was born in Zagreb" (which is incorrect). For brevity, prompts are not shown.</p>
<p>EXAMINEE : Cristiano Ronaldo joined Manchester United in 2003.</p>
<p>EXAMINER : Which club did Cristiano Ronaldo join in 2003?</p>
<p>EXAMINEE : Cristiano Ronaldo joined Sporting CP in 2003.</p>
<p>EXAMINER : do you have any proof that Cristiano Ronaldo joined Sporting CP in 2003?</p>
<p>EXAMINEE: Yes, there is proof that Cristiano Ronaldo joined Sporting CP in 2003. Sporting CP has released official documents confirming the transfer, and there are also numerous news articles and interviews from the time that confirm the move.
EXAMINER : Based on the evidence provided, it is clear that the claim that Cristiano Ronaldo joined Manchester United in 2003 is false. He actually joined Sporting CP in 2003.</p>
<p>Table 19: Example cross-examination with GPT-3 as EXAMINER and EXAMINEE, for the claim "Cristiano Ronaldo joined Manchester United in 2003" (which is correct). For brevity, prompts are not shown.</p>
<p>EXAMINEE : The United States Capitol Building is topped by the Statue of Freedom.</p>
<p>EXAMINER : What is the actual figure that is located on top of the United States Capitol Building?</p>
<p>EXAMINEE: The actual figure located on top of the United States Capitol Building is a bronze statue of the Roman goddess Freedom, which stands 19 feet 6 inches tall.</p>
<p>EXAMINER: Is the figure known as the Statue of Freedom?</p>
<p>EXAMINEE: No, the figure is not known as the Statue of Freedom. It is known as the Statue of Liberty.</p>
<p>EXAMINER : Based on the answers to the questions, the claim that the United States Capitol Building is topped by the Statue of Freedom is incorrect. The actual figure located on top of the United States Capitol Building is a bronze statue of the Roman goddess Freedom, which is not known as the Statue of Freedom.</p>
<p>Table 20: Example cross-examination with GPT-3 as EXAMINER and EXAMINEE, for the claim "The United States Capitol Building is topped by the Statue of Freedom" (which is correct). For brevity, prompts are not shown.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ We say that the examiner "rejects" a claim if the examiner concludes that the claim is incorrect.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{5}$ We use only a subset of examples due to the high cost of executing large LMs like GPT-3, which we include in our experiments.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
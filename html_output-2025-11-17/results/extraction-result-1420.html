<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1420 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1420</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1420</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-b9408b0c3445ad17f33e7a64073c1d2e6674e69a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b9408b0c3445ad17f33e7a64073c1d2e6674e69a" target="_blank">Pathdreamer: A World Model for Indoor Navigation</a></p>
                <p><strong>Paper Venue:</strong> ALVR</p>
                <p><strong>Paper TL;DR:</strong> It is shown that planning ahead with Pathdreamer brings about half the benefit of looking ahead at actual observations from unobserved parts of the environment, which will help unlock model-based approaches to challenging embodied navigation tasks such as navigating to specified objects and VLN.</p>
                <p><strong>Paper Abstract:</strong> People navigating in unfamiliar buildings take advantage of myriad visual, spatial and semantic cues to efficiently achieve their navigation goals. Towards equipping computational agents with similar capabilities, we introduce Pathdreamer, a visual world model for agents navigating in novel indoor environments. Given one or more previous visual observations, Pathdreamer generates plausible high-resolution 360◦ visual observations (RGB, semantic segmentation and depth) for viewpoints that have not been visited, in buildings not seen during training. In regions of high uncertainty (e.g. predicting around corners, imagining the contents of an unseen room), Pathdreamer can predict diverse scenes, allowing an agent to sample multiple realistic outcomes for a given trajectory. We demonstrate that Pathdreamer encodes useful and accessible visual, spatial and semantic knowledge about human environments by using it in the downstream task of Vision-and-Language Navigation (VLN). Specifically, we show that planning ahead with Pathdreamer brings about half the benefit of looking ahead at actual observations from unobserved parts of the environment. We hope that Pathdreamer will help unlock model-based approaches to challenging embodied navigation tasks such as navigating to specified objects and VLN.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1420.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1420.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pathdreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pathdreamer: A World Model for Indoor Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stochastic, hierarchical visual world model that generates high-resolution 360° RGB, semantic segmentation and depth panoramas for future viewpoints in previously unseen indoor environments, using a two-stage Structure→Image pipeline with a learned stochastic latent and 3D point-cloud reprojection for context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pathdreamer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage neural generative world model: (1) Structure Generator — a stochastic encoder-decoder (ResNet-50 based RedNet-like) that back-projects prior semantics/depth into a 3D semantic point cloud, reprojects to produce sparse guidance images, and then generates semantic segmentation and depth images conditioned on a learned conditional prior over a spatial latent noise tensor z_t (KL loss enforces prior→posterior). (2) Image Generator — a Multi-SPADE (spatially-adaptive normalization) GAN that conditions on predicted semantics+depth and sparse reprojected RGB guidance to synthesize realistic 1024×512 RGB panoramas; training uses GAN hinge loss, perceptual (VGG) loss and feature-matching.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural visual world model (stochastic hierarchical pixel-generating world model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Indoor embodied navigation / view synthesis / vision-and-language navigation (VLN)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Semantic: mean Intersection-over-Union (mIoU); Depth: MAE (L1) (used in Structure loss); RGB: Fréchet Inception Distance (FID); Downstream task: VLN metrics (Navigation Error NE, Success Rate SR, SPL, nDTW, sDTW).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Semantic mIoU (Val-Unseen, 1 prior observation): ~78.3% (Teacher Forcing), recurrent ~77.5%; multi-step (1-6) degrades to ~50.8%. RGB FID reported per ablation (Pathdreamer with generated semantics achieves substantially better FID than nearest-neighbor; using ground-truth semantics further improves FID). Downstream VLN: using Pathdreamer predictions yields SR=39.9% (1-step), 46.5% (2-step), 50.4% (3-step) on Val-Unseen — roughly half the gain provided by ground-truth lookahead (GT SR=44.6%, 54.3%, 59.3% respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderately interpretable: outputs are explicit, human-interpretable modalities (semantic segmentation maps, depth and RGB) so model internal outputs can be inspected; the structure-stage latent z_t can be sampled/interpolated to visualize alternative plausible ‘‘room reveals’’. However, internal feature maps and learned CNNs remain largely black-box (no symbolic structure).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of predicted semantic/depth maps and RGB outputs; sampling/interpolation of latent noise tensor z_t to inspect stochastic diversity; qualitative examples and ablation comparisons (e.g., with/without RGB guidance, teacher-forcing vs recurrent) to analyze behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training specifics reported but no hardware FLOP counts: Structure Generator trained for 50 epochs (batch 64) at 512×256; Image Generator trained 500 epochs (batch 128) at 1024×512; optimizers and learning rates given. Inference requires back-projection/re-projection of 3D point cloud and GAN-based high-resolution synthesis, so inference is substantially heavier than nearest-neighbor or simple copying baselines; exact latency, parameter counts or GPU-days are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to simple baselines (Nearest Neighbor, Repeated pano) Pathdreamer is much higher compute but yields much better semantic fidelity and downstream utility on unseen environments; compared to SPADE-only variants, Pathdreamer is more robust on Val-Unseen because it preserves RGB guidance and explicit semantic+depth structure. No quantitative FLOP/throughput comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>On Vision-and-Language Navigation (R2R Val-Unseen) when used to provide look-ahead observations for instruction-trajectory ranking: Success Rate improves from baseline repeated/blank pano ~35–41% to Pathdreamer 39.9% (1-step), 46.5% (2-step), 50.4% (3-step). Ground-truth lookahead achieves 44.6%, 54.3%, 59.3%. Pathdreamer therefore recovers roughly half the benefit of real lookahead.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Pathdreamer’s higher-fidelity semantic and RGB predictions meaningfully improve downstream VLN planning compared to naive baselines, demonstrating that pixel-level predictive world models can encode task-relevant spatial/semantic priors; however imperfect fidelity and accumulated errors limit its ability to match real lookahead — higher fidelity (e.g., ground-truth semantics) would further improve utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Design trades: hierarchical semantic+depth first stage yields improved long-horizon stability versus direct RGB prediction but increases model complexity; stochastic latent enables diverse plausible predictions but introduces sampling variance; maintaining RGB guidance improves generalization to unseen environments at additional modelling cost; recurrent training reduces train-test mismatch for seen data but did not generalize improvement to unseen environments (tradeoff between overfitting and robustness).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: two-stage hierarchy (semantic+depth structure then RGB synthesis); back-projected 3D point cloud context for long-term consistency; conditional learned prior over spatial latent z_t (KL loss) to model stochasticity and enable diverse room reveals; Multi-SPADE blocks with dual conditioning (semantics+reprojected RGB) and partial convolutions to handle sparse guidance; separate training of stages to leverage larger RGB target set for Image Generator.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared with non-generative baselines (Nearest Neighbor / Repeated pano) Pathdreamer produces much better semantics/RGB for unseen areas at higher computational cost. Compared with single-stage SPADE, Pathdreamer generalizes better to unseen environments by preserving RGB guidance and explicit structure. Compared with compact latent world models (e.g., Ha et al. 'World Models', Dreamer), Pathdreamer generates explicit pixels and semantics, which are more directly interpretable and useful for pixel-grounded tasks but are typically more computationally intensive and may be less sample-efficient; compared with scene-specific methods like NeRF, Pathdreamer is designed to generalize across unseen buildings from very few views (1+), whereas NeRF requires many views and per-scene optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends a stochastic, hierarchical model that explicitly predicts semantic+depth structure first and then synthesizes RGB, includes 3D point-cloud re-projection for context and maintains RGB guidance to generalize to unseen environments; suggests recurrent training can help longer rollouts in seen data but careful regularization is needed for generalization. They also note improving the Structure Generator (closer to GT semantics) would further improve RGB fidelity and downstream utility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pathdreamer: A World Model for Indoor Navigation', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1420.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1420.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World Models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of learned environment models that compress high-dimensional observations into compact latent state representations which can be used for planning or policy learning (originally introduced by Ha & Schmidhuber).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>World models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent-space generative models that learn an encoder/decoder and a dynamics model in a compact latent space; used to imagine future latent states for planning or policy learning (original concept introduced in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Control and reinforcement learning (e.g., simulated control tasks, Atari-like domains in follow-up work)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Typically measured as next-state prediction error in latent or pixel space, and downstream task performance (policy rewards); in-cited-paper metrics are not enumerated in this Pathdreamer paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported in this paper; referenced as conceptual prior. Specific numerical fidelity values are not given here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent representations are compact and can be inspected, but internals are neural and not fully interpretable; provides higher-level, lower-dimensional summaries compared to pixel generators.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not detailed here — generally latent visualization and rollouts are used in original literature.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified in this paper; typically more efficient than pixel-space generative models at planning time because predictions operate in low-dimensional latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Mentioned as alternative to pixel-generating world models: latent models are more compact and often more computationally efficient for planning (cited conceptually), though may be less directly interpretable to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not quantified in this paper; cited as evidence that model-based approaches can improve sample efficiency and planning in RL.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Paper contrasts pixel-generating world models (like Pathdreamer) with latent world models: latent models are efficient and useful for planning and RL but do not directly produce human-interpretable pixels; Pathdreamer targets pixel outputs for broad downstream use.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Latent models trade interpretability (no direct pixels) for computational efficiency and compactness; pixel-generating models are more general and human-interpretable but computationally heavier and harder to scale.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Pathdreamer explicitly chose pixel outputs (semantics+RGB+depth) rather than compact latent states to increase generality and human interpretability for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Identified as an alternative family to Pathdreamer: latent models (World Models, Dreamer) vs pixel-generating models — each has different trade-offs in fidelity, interpretability and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>The paper does not prescribe an overall optimal configuration; it argues for pixel outputs when human-interpretable observations are valuable for downstream tasks, while acknowledging latent models' efficiency benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pathdreamer: A World Model for Indoor Navigation', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1420.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1420.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dream to Control (Dreamer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dream to Control: Learning Behaviors by Latent Imagination</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent world-model approach that learns compact latent dynamics and uses 'imagination' in latent space to learn policies (referred to in Pathdreamer as representative of latent model-based methods).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer (latent imagination)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns an encoder to map observations to latents, a dynamics model in latent space, and a policy/value model trained by imagining trajectories in latent space; optimizes behavior without generating pixel-level predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Reinforcement learning control domains (e.g., continuous control, Atari follow-up domains in other works)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Typically task reward and latent prediction accuracy; not specifically reported in Pathdreamer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not specified in this paper; referenced as literature demonstrating model-based RL benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent states are compact and not directly human-interpretable; introspection via latent-space rollouts is possible but limited.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not described in this paper; original works use visualization of imagined rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified here; latent imagination is generally more efficient for multi-step planning than pixel-space rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Mentioned conceptually as more computationally efficient than pixel-space generative models for planning; no quantitative comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not quantified here; cited as evidence for learning behaviors via latent imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Paper positions Dreamer-like methods as complementary: efficient for learning policies but do not supply pixel-level observations useful for tasks that require explicit visual predictions or inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Latent imagination trades off human-interpretability of predictions for efficiency in planning and policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Pathdreamer deliberately focuses on pixel outputs to make the world model directly useful for vision-grounded downstream tasks (e.g., VLN).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Identified as an alternative class (latent models) that offers efficiency and compactness versus Pathdreamer's explicit pixel+semantic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified; authors note differing objectives — pixel outputs beneficial when tasks need visual predictions, while latent models are preferable for policy learning efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pathdreamer: A World Model for Indoor Navigation', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1420.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1420.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mastering Atari with Discrete World Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mastering Atari with Discrete World Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent approach using discrete latent world models to learn and plan in Atari environments, referenced as an example of model-based RL successes with latent models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari with discrete world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Discrete latent world models for Atari</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>World models that compress observations into discrete latent codes (e.g., VQ-VAE style or discrete dynamics) used to model environment dynamics and plan or train agents within the learned discrete latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent (discrete) world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games / game-playing benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Typically policy score / game score and prediction accuracy in latent space; not reported in Pathdreamer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported in this paper; cited to illustrate latent-model successes in simple/highly constrained visual domains.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Discrete codes provide some structure that can be analyzed but are not directly photorealistic or human-interpretable as images.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not described here; original work inspects discrete codes and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified in Pathdreamer; discrete latent models are generally more efficient to rollout than pixel-space generators.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Cited as effective for Atari-scale domains where pixel-generating models are less necessary; not directly compared numerically in Pathdreamer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not quantified here; cited as examples where model-based approaches attained strong RL performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Used to highlight that pixel-generating world models have traditionally been limited to simple environments and that latent/discrete approaches have been more prevalent in RL success stories.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Illustrates a domain trade: discrete latent models excel in constrained simulated domains, while pixel-space models are needed for explicit visual predictions in complex real-world scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Paper contrasts Pathdreamer’s pixel outputs with latent/discrete world models' compactness and sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Presented as alternative world-model family that favors efficiency and control performance over human-interpretable pixel prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified; authors argue for pixel outputs for embodied visual tasks and latent/discrete models for efficient RL in simpler domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pathdreamer: A World Model for Indoor Navigation', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1420.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1420.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Value Prediction Network</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Value Prediction Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based architecture that predicts future rewards and values through an internal predictive model to support planning, referenced as an example of non-pixel world models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Value prediction network</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Value Prediction Network (VPN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns a predictive model of future abstract/metric quantities (e.g., value, reward) rather than raw pixels, enabling planning by prediction of task-specific measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>task-specific predictive world model (latent/metric)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Reinforcement learning / planning (simulated tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Measured in terms of predicted value/reward accuracy and downstream policy performance; not specified numerically in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not provided in Pathdreamer; cited to show alternative world-model outputs (measurements/rewards) rather than pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Interpretable to the extent predictions are in task-relevant scalar quantities (e.g., reward/value), but internal model remains neural and not fully transparent.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not described here — typically evaluation of predicted values and downstream policy behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified here; generally more efficient than pixel-generating models since outputs are low-dimensional.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Described conceptually as more compact/task-specific than pixel-space models and thus computationally cheaper for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not quantified in this paper; cited as an approach focusing on task-specific predictions rather than generative pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Used to highlight that world models need not produce pixels — predicting task-specific measurements can suffice and be more efficient for certain downstream objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Predicting task-specific quantities increases efficiency and direct utility for that task but reduces generality and human-interpretable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Pathdreamer contrasts its pixel+semantic outputs (general, human-interpretable) against task-specific predictive models like VPN.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>VPN-like models are more efficient for reward-driven tasks but are less general-purpose than pixel/semantic world models.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed; Pathdreamer frames VPN as a complementary design for task-specific efficiency when pixel outputs are unnecessary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pathdreamer: A World Model for Indoor Navigation', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1420.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1420.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeRF (Neural Radiance Fields)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NeRF: Representing scenes as neural radiance fields for view synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-fidelity neural scene representation that synthesizes novel views by learning a continuous radiance field per scene — cited as a strong view-synthesis baseline with substantial limitations for generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nerf: Representing scenes as neural radiance fields for view synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural Radiance Fields (NeRF)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Scene-specific implicit volumetric representation trained to map spatial coordinates and viewing directions to color and density, enabling photorealistic novel view synthesis via volume rendering, but requiring many input views and per-scene optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>scene-specific implicit neural representation</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Novel view synthesis / 3D scene representation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Perceptual / pixel-level view synthesis quality (PSNR/SSIM/LPIPS typically) — discussed qualitatively in paper as high-quality but requiring many views.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not numerically compared in this paper; authors note NeRF produces very high-quality renderings but typically needs 20–62 images per scene and does not generalize well to unseen environments.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Provides high-fidelity continuous scene function but is not explicitly interpretable in semantic terms; representations are per-scene and implicit.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not applicable here; referenced as alternative with different generalization/conditioning properties.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High: per-scene optimization and volume rendering are computationally expensive; requires many input views and heavy per-scene training (cited qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less suitable than Pathdreamer for few-shot generalization to novel buildings — NeRF needs many views and per-scene training, whereas Pathdreamer aims to generalize from as little as one observation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>High-fidelity view synthesis for single scenes but limited generalization; not applied to VLN in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>NeRF is excellent for high-quality re-rendering of a known scene but impractical for agent planning in novel environments due to requirements for many images and per-scene optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>NeRF trades per-scene fidelity for lack of generalization and high data/compute needs; Pathdreamer trades some photorealism for cross-scene generalization and task-utility.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Paper chose a generalizable, multi-scene-trained pixel-generating model rather than per-scene implicit models like NeRF to support few-shot generalization for embodied agents.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>NeRF provides superior per-scene fidelity but is unsuitable for Pathdreamer's objective of generalizing across many unseen buildings from a small number of views.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pathdreamer: A World Model for Indoor Navigation', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>World models <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Mastering atari with discrete world models <em>(Rating: 2)</em></li>
                <li>Value prediction network <em>(Rating: 1)</em></li>
                <li>Nerf: Representing scenes as neural radiance fields for view synthesis <em>(Rating: 2)</em></li>
                <li>World-consistent video-to-video synthesis <em>(Rating: 1)</em></li>
                <li>Semantic image synthesis with spatially-adaptive normalization <em>(Rating: 1)</em></li>
                <li>Stochastic video generation with a learned prior <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1420",
    "paper_id": "paper-b9408b0c3445ad17f33e7a64073c1d2e6674e69a",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "Pathdreamer",
            "name_full": "Pathdreamer: A World Model for Indoor Navigation",
            "brief_description": "A stochastic, hierarchical visual world model that generates high-resolution 360° RGB, semantic segmentation and depth panoramas for future viewpoints in previously unseen indoor environments, using a two-stage Structure→Image pipeline with a learned stochastic latent and 3D point-cloud reprojection for context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pathdreamer",
            "model_description": "Two-stage neural generative world model: (1) Structure Generator — a stochastic encoder-decoder (ResNet-50 based RedNet-like) that back-projects prior semantics/depth into a 3D semantic point cloud, reprojects to produce sparse guidance images, and then generates semantic segmentation and depth images conditioned on a learned conditional prior over a spatial latent noise tensor z_t (KL loss enforces prior→posterior). (2) Image Generator — a Multi-SPADE (spatially-adaptive normalization) GAN that conditions on predicted semantics+depth and sparse reprojected RGB guidance to synthesize realistic 1024×512 RGB panoramas; training uses GAN hinge loss, perceptual (VGG) loss and feature-matching.",
            "model_type": "neural visual world model (stochastic hierarchical pixel-generating world model)",
            "task_domain": "Indoor embodied navigation / view synthesis / vision-and-language navigation (VLN)",
            "fidelity_metric": "Semantic: mean Intersection-over-Union (mIoU); Depth: MAE (L1) (used in Structure loss); RGB: Fréchet Inception Distance (FID); Downstream task: VLN metrics (Navigation Error NE, Success Rate SR, SPL, nDTW, sDTW).",
            "fidelity_performance": "Semantic mIoU (Val-Unseen, 1 prior observation): ~78.3% (Teacher Forcing), recurrent ~77.5%; multi-step (1-6) degrades to ~50.8%. RGB FID reported per ablation (Pathdreamer with generated semantics achieves substantially better FID than nearest-neighbor; using ground-truth semantics further improves FID). Downstream VLN: using Pathdreamer predictions yields SR=39.9% (1-step), 46.5% (2-step), 50.4% (3-step) on Val-Unseen — roughly half the gain provided by ground-truth lookahead (GT SR=44.6%, 54.3%, 59.3% respectively).",
            "interpretability_assessment": "Moderately interpretable: outputs are explicit, human-interpretable modalities (semantic segmentation maps, depth and RGB) so model internal outputs can be inspected; the structure-stage latent z_t can be sampled/interpolated to visualize alternative plausible ‘‘room reveals’’. However, internal feature maps and learned CNNs remain largely black-box (no symbolic structure).",
            "interpretability_method": "Visualization of predicted semantic/depth maps and RGB outputs; sampling/interpolation of latent noise tensor z_t to inspect stochastic diversity; qualitative examples and ablation comparisons (e.g., with/without RGB guidance, teacher-forcing vs recurrent) to analyze behaviour.",
            "computational_cost": "Training specifics reported but no hardware FLOP counts: Structure Generator trained for 50 epochs (batch 64) at 512×256; Image Generator trained 500 epochs (batch 128) at 1024×512; optimizers and learning rates given. Inference requires back-projection/re-projection of 3D point cloud and GAN-based high-resolution synthesis, so inference is substantially heavier than nearest-neighbor or simple copying baselines; exact latency, parameter counts or GPU-days are not reported.",
            "efficiency_comparison": "Compared to simple baselines (Nearest Neighbor, Repeated pano) Pathdreamer is much higher compute but yields much better semantic fidelity and downstream utility on unseen environments; compared to SPADE-only variants, Pathdreamer is more robust on Val-Unseen because it preserves RGB guidance and explicit semantic+depth structure. No quantitative FLOP/throughput comparison provided.",
            "task_performance": "On Vision-and-Language Navigation (R2R Val-Unseen) when used to provide look-ahead observations for instruction-trajectory ranking: Success Rate improves from baseline repeated/blank pano ~35–41% to Pathdreamer 39.9% (1-step), 46.5% (2-step), 50.4% (3-step). Ground-truth lookahead achieves 44.6%, 54.3%, 59.3%. Pathdreamer therefore recovers roughly half the benefit of real lookahead.",
            "task_utility_analysis": "Pathdreamer’s higher-fidelity semantic and RGB predictions meaningfully improve downstream VLN planning compared to naive baselines, demonstrating that pixel-level predictive world models can encode task-relevant spatial/semantic priors; however imperfect fidelity and accumulated errors limit its ability to match real lookahead — higher fidelity (e.g., ground-truth semantics) would further improve utility.",
            "tradeoffs_observed": "Design trades: hierarchical semantic+depth first stage yields improved long-horizon stability versus direct RGB prediction but increases model complexity; stochastic latent enables diverse plausible predictions but introduces sampling variance; maintaining RGB guidance improves generalization to unseen environments at additional modelling cost; recurrent training reduces train-test mismatch for seen data but did not generalize improvement to unseen environments (tradeoff between overfitting and robustness).",
            "design_choices": "Key choices: two-stage hierarchy (semantic+depth structure then RGB synthesis); back-projected 3D point cloud context for long-term consistency; conditional learned prior over spatial latent z_t (KL loss) to model stochasticity and enable diverse room reveals; Multi-SPADE blocks with dual conditioning (semantics+reprojected RGB) and partial convolutions to handle sparse guidance; separate training of stages to leverage larger RGB target set for Image Generator.",
            "comparison_to_alternatives": "Compared with non-generative baselines (Nearest Neighbor / Repeated pano) Pathdreamer produces much better semantics/RGB for unseen areas at higher computational cost. Compared with single-stage SPADE, Pathdreamer generalizes better to unseen environments by preserving RGB guidance and explicit structure. Compared with compact latent world models (e.g., Ha et al. 'World Models', Dreamer), Pathdreamer generates explicit pixels and semantics, which are more directly interpretable and useful for pixel-grounded tasks but are typically more computationally intensive and may be less sample-efficient; compared with scene-specific methods like NeRF, Pathdreamer is designed to generalize across unseen buildings from very few views (1+), whereas NeRF requires many views and per-scene optimization.",
            "optimal_configuration": "Paper recommends a stochastic, hierarchical model that explicitly predicts semantic+depth structure first and then synthesizes RGB, includes 3D point-cloud re-projection for context and maintains RGB guidance to generalize to unseen environments; suggests recurrent training can help longer rollouts in seen data but careful regularization is needed for generalization. They also note improving the Structure Generator (closer to GT semantics) would further improve RGB fidelity and downstream utility.",
            "uuid": "e1420.0",
            "source_info": {
                "paper_title": "Pathdreamer: A World Model for Indoor Navigation",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "World Models (Ha & Schmidhuber)",
            "name_full": "World Models",
            "brief_description": "A class of learned environment models that compress high-dimensional observations into compact latent state representations which can be used for planning or policy learning (originally introduced by Ha & Schmidhuber).",
            "citation_title": "World models",
            "mention_or_use": "mention",
            "model_name": "World Models (Ha & Schmidhuber)",
            "model_description": "Latent-space generative models that learn an encoder/decoder and a dynamics model in a compact latent space; used to imagine future latent states for planning or policy learning (original concept introduced in cited work).",
            "model_type": "latent world model",
            "task_domain": "Control and reinforcement learning (e.g., simulated control tasks, Atari-like domains in follow-up work)",
            "fidelity_metric": "Typically measured as next-state prediction error in latent or pixel space, and downstream task performance (policy rewards); in-cited-paper metrics are not enumerated in this Pathdreamer paper.",
            "fidelity_performance": "Not reported in this paper; referenced as conceptual prior. Specific numerical fidelity values are not given here.",
            "interpretability_assessment": "Latent representations are compact and can be inspected, but internals are neural and not fully interpretable; provides higher-level, lower-dimensional summaries compared to pixel generators.",
            "interpretability_method": "Not detailed here — generally latent visualization and rollouts are used in original literature.",
            "computational_cost": "Not specified in this paper; typically more efficient than pixel-space generative models at planning time because predictions operate in low-dimensional latent space.",
            "efficiency_comparison": "Mentioned as alternative to pixel-generating world models: latent models are more compact and often more computationally efficient for planning (cited conceptually), though may be less directly interpretable to humans.",
            "task_performance": "Not quantified in this paper; cited as evidence that model-based approaches can improve sample efficiency and planning in RL.",
            "task_utility_analysis": "Paper contrasts pixel-generating world models (like Pathdreamer) with latent world models: latent models are efficient and useful for planning and RL but do not directly produce human-interpretable pixels; Pathdreamer targets pixel outputs for broad downstream use.",
            "tradeoffs_observed": "Latent models trade interpretability (no direct pixels) for computational efficiency and compactness; pixel-generating models are more general and human-interpretable but computationally heavier and harder to scale.",
            "design_choices": "Pathdreamer explicitly chose pixel outputs (semantics+RGB+depth) rather than compact latent states to increase generality and human interpretability for downstream tasks.",
            "comparison_to_alternatives": "Identified as an alternative family to Pathdreamer: latent models (World Models, Dreamer) vs pixel-generating models — each has different trade-offs in fidelity, interpretability and efficiency.",
            "optimal_configuration": "The paper does not prescribe an overall optimal configuration; it argues for pixel outputs when human-interpretable observations are valuable for downstream tasks, while acknowledging latent models' efficiency benefits.",
            "uuid": "e1420.1",
            "source_info": {
                "paper_title": "Pathdreamer: A World Model for Indoor Navigation",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Dream to Control (Dreamer)",
            "name_full": "Dream to Control: Learning Behaviors by Latent Imagination",
            "brief_description": "A latent world-model approach that learns compact latent dynamics and uses 'imagination' in latent space to learn policies (referred to in Pathdreamer as representative of latent model-based methods).",
            "citation_title": "Dream to control: Learning behaviors by latent imagination",
            "mention_or_use": "mention",
            "model_name": "Dreamer (latent imagination)",
            "model_description": "Learns an encoder to map observations to latents, a dynamics model in latent space, and a policy/value model trained by imagining trajectories in latent space; optimizes behavior without generating pixel-level predictions.",
            "model_type": "latent world model",
            "task_domain": "Reinforcement learning control domains (e.g., continuous control, Atari follow-up domains in other works)",
            "fidelity_metric": "Typically task reward and latent prediction accuracy; not specifically reported in Pathdreamer.",
            "fidelity_performance": "Not specified in this paper; referenced as literature demonstrating model-based RL benefits.",
            "interpretability_assessment": "Latent states are compact and not directly human-interpretable; introspection via latent-space rollouts is possible but limited.",
            "interpretability_method": "Not described in this paper; original works use visualization of imagined rollouts.",
            "computational_cost": "Not specified here; latent imagination is generally more efficient for multi-step planning than pixel-space rollouts.",
            "efficiency_comparison": "Mentioned conceptually as more computationally efficient than pixel-space generative models for planning; no quantitative comparison provided.",
            "task_performance": "Not quantified here; cited as evidence for learning behaviors via latent imagination.",
            "task_utility_analysis": "Paper positions Dreamer-like methods as complementary: efficient for learning policies but do not supply pixel-level observations useful for tasks that require explicit visual predictions or inspection.",
            "tradeoffs_observed": "Latent imagination trades off human-interpretability of predictions for efficiency in planning and policy learning.",
            "design_choices": "Pathdreamer deliberately focuses on pixel outputs to make the world model directly useful for vision-grounded downstream tasks (e.g., VLN).",
            "comparison_to_alternatives": "Identified as an alternative class (latent models) that offers efficiency and compactness versus Pathdreamer's explicit pixel+semantic outputs.",
            "optimal_configuration": "Not specified; authors note differing objectives — pixel outputs beneficial when tasks need visual predictions, while latent models are preferable for policy learning efficiency.",
            "uuid": "e1420.2",
            "source_info": {
                "paper_title": "Pathdreamer: A World Model for Indoor Navigation",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Mastering Atari with Discrete World Models",
            "name_full": "Mastering Atari with Discrete World Models",
            "brief_description": "A recent approach using discrete latent world models to learn and plan in Atari environments, referenced as an example of model-based RL successes with latent models.",
            "citation_title": "Mastering atari with discrete world models",
            "mention_or_use": "mention",
            "model_name": "Discrete latent world models for Atari",
            "model_description": "World models that compress observations into discrete latent codes (e.g., VQ-VAE style or discrete dynamics) used to model environment dynamics and plan or train agents within the learned discrete latent space.",
            "model_type": "latent (discrete) world model",
            "task_domain": "Atari games / game-playing benchmarks",
            "fidelity_metric": "Typically policy score / game score and prediction accuracy in latent space; not reported in Pathdreamer.",
            "fidelity_performance": "Not reported in this paper; cited to illustrate latent-model successes in simple/highly constrained visual domains.",
            "interpretability_assessment": "Discrete codes provide some structure that can be analyzed but are not directly photorealistic or human-interpretable as images.",
            "interpretability_method": "Not described here; original work inspects discrete codes and performance.",
            "computational_cost": "Not specified in Pathdreamer; discrete latent models are generally more efficient to rollout than pixel-space generators.",
            "efficiency_comparison": "Cited as effective for Atari-scale domains where pixel-generating models are less necessary; not directly compared numerically in Pathdreamer.",
            "task_performance": "Not quantified here; cited as examples where model-based approaches attained strong RL performance.",
            "task_utility_analysis": "Used to highlight that pixel-generating world models have traditionally been limited to simple environments and that latent/discrete approaches have been more prevalent in RL success stories.",
            "tradeoffs_observed": "Illustrates a domain trade: discrete latent models excel in constrained simulated domains, while pixel-space models are needed for explicit visual predictions in complex real-world scenes.",
            "design_choices": "Paper contrasts Pathdreamer’s pixel outputs with latent/discrete world models' compactness and sample efficiency.",
            "comparison_to_alternatives": "Presented as alternative world-model family that favors efficiency and control performance over human-interpretable pixel prediction.",
            "optimal_configuration": "Not specified; authors argue for pixel outputs for embodied visual tasks and latent/discrete models for efficient RL in simpler domains.",
            "uuid": "e1420.3",
            "source_info": {
                "paper_title": "Pathdreamer: A World Model for Indoor Navigation",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Value Prediction Network",
            "name_full": "Value Prediction Network",
            "brief_description": "A model-based architecture that predicts future rewards and values through an internal predictive model to support planning, referenced as an example of non-pixel world models.",
            "citation_title": "Value prediction network",
            "mention_or_use": "mention",
            "model_name": "Value Prediction Network (VPN)",
            "model_description": "Learns a predictive model of future abstract/metric quantities (e.g., value, reward) rather than raw pixels, enabling planning by prediction of task-specific measurements.",
            "model_type": "task-specific predictive world model (latent/metric)",
            "task_domain": "Reinforcement learning / planning (simulated tasks)",
            "fidelity_metric": "Measured in terms of predicted value/reward accuracy and downstream policy performance; not specified numerically in this paper.",
            "fidelity_performance": "Not provided in Pathdreamer; cited to show alternative world-model outputs (measurements/rewards) rather than pixels.",
            "interpretability_assessment": "Interpretable to the extent predictions are in task-relevant scalar quantities (e.g., reward/value), but internal model remains neural and not fully transparent.",
            "interpretability_method": "Not described here — typically evaluation of predicted values and downstream policy behaviour.",
            "computational_cost": "Not specified here; generally more efficient than pixel-generating models since outputs are low-dimensional.",
            "efficiency_comparison": "Described conceptually as more compact/task-specific than pixel-space models and thus computationally cheaper for planning.",
            "task_performance": "Not quantified in this paper; cited as an approach focusing on task-specific predictions rather than generative pixels.",
            "task_utility_analysis": "Used to highlight that world models need not produce pixels — predicting task-specific measurements can suffice and be more efficient for certain downstream objectives.",
            "tradeoffs_observed": "Predicting task-specific quantities increases efficiency and direct utility for that task but reduces generality and human-interpretable outputs.",
            "design_choices": "Pathdreamer contrasts its pixel+semantic outputs (general, human-interpretable) against task-specific predictive models like VPN.",
            "comparison_to_alternatives": "VPN-like models are more efficient for reward-driven tasks but are less general-purpose than pixel/semantic world models.",
            "optimal_configuration": "Not discussed; Pathdreamer frames VPN as a complementary design for task-specific efficiency when pixel outputs are unnecessary.",
            "uuid": "e1420.4",
            "source_info": {
                "paper_title": "Pathdreamer: A World Model for Indoor Navigation",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "NeRF (Neural Radiance Fields)",
            "name_full": "NeRF: Representing scenes as neural radiance fields for view synthesis",
            "brief_description": "A high-fidelity neural scene representation that synthesizes novel views by learning a continuous radiance field per scene — cited as a strong view-synthesis baseline with substantial limitations for generalization.",
            "citation_title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "mention_or_use": "mention",
            "model_name": "Neural Radiance Fields (NeRF)",
            "model_description": "Scene-specific implicit volumetric representation trained to map spatial coordinates and viewing directions to color and density, enabling photorealistic novel view synthesis via volume rendering, but requiring many input views and per-scene optimization.",
            "model_type": "scene-specific implicit neural representation",
            "task_domain": "Novel view synthesis / 3D scene representation",
            "fidelity_metric": "Perceptual / pixel-level view synthesis quality (PSNR/SSIM/LPIPS typically) — discussed qualitatively in paper as high-quality but requiring many views.",
            "fidelity_performance": "Not numerically compared in this paper; authors note NeRF produces very high-quality renderings but typically needs 20–62 images per scene and does not generalize well to unseen environments.",
            "interpretability_assessment": "Provides high-fidelity continuous scene function but is not explicitly interpretable in semantic terms; representations are per-scene and implicit.",
            "interpretability_method": "Not applicable here; referenced as alternative with different generalization/conditioning properties.",
            "computational_cost": "High: per-scene optimization and volume rendering are computationally expensive; requires many input views and heavy per-scene training (cited qualitatively).",
            "efficiency_comparison": "Less suitable than Pathdreamer for few-shot generalization to novel buildings — NeRF needs many views and per-scene training, whereas Pathdreamer aims to generalize from as little as one observation.",
            "task_performance": "High-fidelity view synthesis for single scenes but limited generalization; not applied to VLN in this paper.",
            "task_utility_analysis": "NeRF is excellent for high-quality re-rendering of a known scene but impractical for agent planning in novel environments due to requirements for many images and per-scene optimization.",
            "tradeoffs_observed": "NeRF trades per-scene fidelity for lack of generalization and high data/compute needs; Pathdreamer trades some photorealism for cross-scene generalization and task-utility.",
            "design_choices": "Paper chose a generalizable, multi-scene-trained pixel-generating model rather than per-scene implicit models like NeRF to support few-shot generalization for embodied agents.",
            "comparison_to_alternatives": "NeRF provides superior per-scene fidelity but is unsuitable for Pathdreamer's objective of generalizing across many unseen buildings from a small number of views.",
            "uuid": "e1420.5",
            "source_info": {
                "paper_title": "Pathdreamer: A World Model for Indoor Navigation",
                "publication_date_yy_mm": "2021-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "World models",
            "rating": 2
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2
        },
        {
            "paper_title": "Mastering atari with discrete world models",
            "rating": 2
        },
        {
            "paper_title": "Value prediction network",
            "rating": 1
        },
        {
            "paper_title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "rating": 2
        },
        {
            "paper_title": "World-consistent video-to-video synthesis",
            "rating": 1
        },
        {
            "paper_title": "Semantic image synthesis with spatially-adaptive normalization",
            "rating": 1
        },
        {
            "paper_title": "Stochastic video generation with a learned prior",
            "rating": 1
        }
    ],
    "cost": 0.01852225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Pathdreamer: A World Model for Indoor Navigation</h1>
<p>Jing Yu Koh^{1} Honglak Lee^{2} Yinfei Yang^{1} Jason Baldridge^{1} Peter Anderson^{1}
^{1}Google Research ^{2}University of Michigan</p>
<h2>Abstract</h2>
<p><em>People navigating in unfamiliar buildings take advantage of myriad visual, spatial and semantic cues to efficiently achieve their navigation goals. Towards equipping computational agents with similar capabilities, we introduce Pathdreamer, a visual world model for agents navigating in novel indoor environments. Given one or more previous visual observations, Pathdreamer generates plausible high-resolution 360° visual observations (RGB, semantic segmentation and depth) for viewpoints that have not been visited, in buildings not seen during training. In regions of high uncertainty (e.g. predicting around corners, imagining the contents of an unseen room), Pathdreamer can predict diverse scenes, allowing an agent to sample multiple realistic outcomes for a given trajectory. We demonstrate that Pathdreamer encodes useful and accessible visual, spatial and semantic knowledge about human environments by using it in the downstream task of Vision-and-Language Navigation (VLN). Specifically, we show that planning ahead with Pathdreamer brings about half the benefit of looking ahead at actual observations from unobserved parts of the environment. We hope that Pathdreamer will help unlock model-based approaches to challenging embodied navigation tasks such as navigating to specified objects and VLN.</em></p>
<h2>1. Introduction</h2>
<p>World models [23], or models of environments [72], are an appealing way to represent an agent's knowledge about its surroundings. An agent with a world model can predict its future by 'imagining' the consequences of a series of proposed actions. This capability can be used for sampling-based planning [16, 57], learning policies directly from the model (i.e., learning in a dream) [17, 23, 64, 25], and for counterfactual reasoning [6]. Model-based approaches such as these also typically improve the sample efficiency of deep reinforcement learning [72, 62]. However, world models that generate high-dimensional visual observations (i.e., images) have typically been restricted to relatively simple environments, such as Atari games [62] and tabletops [16].</p>
<p>Our goal is to develop a generic visual world model for agents navigating in indoor environments. Specifically,</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p><strong>Figure 1:</strong> Generating photorealistic 360° visual observations from an imagined 6.3m trajectory in a previously unseen building. Observations also include depth and segmentations (not shown here).</p>
<p>given one or more previous observations and a proposed navigation action sequence, we aim to generate plausible high-resolution visual observations for viewpoints that have not been visited, and do so in buildings not seen during training. Beyond applications in video editing and content creation, solving this problem would unlock model-based methods for many embodied AI tasks, including navigating to objects [5], instruction-guided navigation [3, 66, 40] and dialog-guided navigation [74, 26]. For example, an agent asked to find a certain type of object in a novel building, e.g. '<em>find a chair</em>', could perform mental simulations using the world model to identify navigation trajectories that are most likely to include chair observations – without moving.</p>
<p>Building such a model is challenging. It requires synthesizing completions of partially visible objects, using as few as one previous observation. This is akin to novel view synthesis from a single image [19, 80], but with potentially unbounded viewpoint changes. There is also the related but considerably more extreme challenge of predicting around corners. For example, as shown in Fig. 1, any future navigation trajectory passing the entrance of an unseen room requires the model to plausibly imagine the entire contents</p>
<p>of that room (we dub this the room reveal problem). This requires generalizing from the visual, spatial and semantic structure of previously explored environments-which in our case are photo-realistic 3D captures of real indoor spaces in the Matterport3D dataset [7]. A third problem is temporal consistency: predictions of unseen building regions should ideally be stochastic (capturing the full distribution of possible outcomes), but revisited regions should be rendered in a consistent manner to previous observations.</p>
<p>Towards this goal, we introduce Pathdreamer. Given one or more visual observations (consisting of RGB, depth and semantic segmentation for panoramas) of an indoor scene, Pathdreamer synthesizes high-resolution visual observations (RGB, depth and semantic segmentations) along a specified trajectory through future viewpoints, using a hierarchical two-stage approach. Pathdreamer's first stage, Structure Generator, generates depth and semantic segmentations. Inspired by video prediction [11], these outputs are conditioned on a latent noise tensor capturing the stochastic information about the next observation (such as the layout of an unseen room) that cannot be predicted deterministically. The second stage's Image Generator renders the depth and semantic segmentations as realistic RGB images using modified Multi-SPADE blocks [63, 51]. To maintain long-term consistency in the generated observations, both stages use back-projected 3D point cloud representations which are re-projected into image space for context [51].</p>
<p>Pathdreamer can generate plausible views for unseen indoor scenes under large viewpoint changes (see Figure 1), while also addressing the room reveal problem - in this case correctly hypothesizing that the unseen room revealed at position 2 will most likely resemble a kitchen. Empirically, using the Matterport3D dataset [7] and $360^{\circ}$ observations, we evaluate both stages of our model against prior work and reasonable baselines and ablations. We find that the hierarchical structure of the model is essential for predicting over large viewpoint changes, that maintaining both RGB and semantic context is required, and that prediction quality degrades gradually when we evaluate with trajectory rollouts of up to 13 m (with viewpoints 2.25 m apart on average).</p>
<p>Encouraged by these results, we investigate whether Pathdreamer's RGB predictions can improve performance on a downstream task: Vision-and-Language Navigation (VLN), using the R2R dataset [3]. VLN requires agents to interpret and execute natural language navigation instructions in a photorealistic 3D environment. A robust finding from previous research is that performance improves dramatically when agents can look ahead at unobserved parts of the environment while following an instruction [50]. We show that replacing look-ahead observations with Pathdreamer predictions maintains around half of the gains, a finding we expect to have significant implications for VLN research. In summary, our main contributions include:</p>
<ul>
<li>Proposing the study of visual world models for generic indoor environments and defining evaluation protocols and baselines for future work.</li>
<li>Pathdreamer, a stochastic hierarchical visual world model combining multiple, independent threads of previous work on video prediction [11], semantic image synthesis [63] and video-to-video synthesis [51].</li>
<li>Extensive experiments characterizing the performance of Pathdreamer and demonstrating improved results on the downstream VLN task [3].</li>
</ul>
<h2>2. Related Work</h2>
<p>Video Prediction Our work is closely related to the task of video prediction, which aims to predict the future frames of a video sequence. While some video prediction methods predict RGB video frames directly [76, 1, 41, 44], many others use hierarchical models to first predict an intermediate representation (such as semantic segmentation) [47, 35, 77, 82, 42], which improves the fidelity of long-term predictions [42]. Several approaches have also incorporated 3D point cloud representations, using projective camera geometry to explicitly infer aspects of the next frame $[75,51,43]$. Inspired by this work, we adopt and combine both the hierarchical two-stage approach and 3D point cloud representations. Further, since our interest is in action-conditional world models, we provide a trajectory of future viewpoints to the model rather than assuming a constant frame rate and modeling camera motion implicitly, which is more typical in video generation [44, 42].</p>
<p>Action-Conditional Video Prediction Conditional video prediction to improve agent reasoning and planning has been explored in several tasks. This includes video prediction for Atari games conditioned on control inputs [60, $10,62,25]$ and 3D game environments like Doom [23]. In robotics, action-conditional video prediction has been investigated for object pushing in tabletop settings to improve generalization to novel objects [15, 16, 14]. This work has been restricted to simple environments and low-resolution images, such as $64 \times 64$ images of objects in a wooden box. To the best of our knowledge, we are the first to investigate action-conditional video prediction in building-scale environments with high-resolution $(1024 \times 512)$ images.</p>
<p>World Models and Navigation Priors World models [23] are an appealing way to summarize and distill knowledge about complex, high-dimensional environments. However, world models can differ in their outputs. While Pathdreamer predicts visual observations, there is also a vast literature on world models that predict compact latent representations of future states [38, 24, 25] or other taskspecific measurements [13] or rewards [61]. This includes recent work attempting to learn statistical regularities and other priors for indoor navigation-for example, by min-</p>
<p>ing spatial co-occurrences from real estate video tours [8], learning to predict top-down belief maps over room characteristics [58], or learning to reconstruct house floor plans using audio and visual cues from a short video sequence [65]. In contrast to these approaches, we focus on explicitly predicting visual observations (i.e., pixels) which are generic, human-interpretable, and apply to a wide variety of downstream tasks and applications. Further, recent work identifies a close correlation between image prediction accuracy and downstream task performance in model-based RL [4].</p>
<p>Embodied Navigation Agents High-quality 3D environment datasets such as Matterport3D [7], StreetLearn [56, 53], Gibson [81] and Replica [71] have triggered intense interest in developing embodied agents that act in realistic human environments [2]. Tasks of interest include ObjectNav [5] (navigating to an instance of a particular kind of object), and Vision-and-Language Navigation (VLN) [3], in which agents must navigate according to natural language instructions. Variations of VLN include indoor navigation [3, 33, 66, 40], street-level navigation [9, 53], vision-and-dialog navigation [59, 74, 26], VLN in continuous environments [39], and more. Notwithstanding considerable exploration of pretraining strategies [46, 27, 50, 87], data augmentation approaches [20, 21, 73], agent architectures and loss functions [86, 48, 49], existing work in this space considers only model-free approaches. Our aim is to unlock model-based approaches to these tasks, using a visual world model to encode prior commonsense knowledge about human environments and thereby relieve the burden on the agent to learn these regularities. Underscoring the potential of this direction, we note that using the ground-truth environment for planning with beam search typically improves VLN success rates on the R2R dataset by 17-19% [20, 73].</p>
<p>Novel View Synthesis We position our work with respect to novel view synthesis [19, 37, 29, 18, 70, 85, 54]. Methods for representing 3D scenes include point cloud representations [80], layered depth images [12], and mesh representations [68]. Recently, neural radiance fields (NeRF) [55, 52, 83] achieved impressive results by capturing volume density and color implicitly with a neural network. NeRF models synthesize very high quality 3D scenes, but a significant drawback for our purposes is that they require a large number of input views to render a single scene (e.g., 20–62 images per scene in [55]). More importantly, these models are typically trained to represent a single scene, and do not yet generalize well to unseen environments. In contrast, our problem demands generalization to unseen environments, using as little as one previous observation.</p>
<h2>3. Pathdreamer</h2>
<p>Pathdreamer is a world model that generates high-resolution visual observations from a trajectory of future viewpoints in buildings it has never observed. The input to Pathdreamer is a sequence of previous observations consisting of RGB images $I_{1:t-1}$, semantic segmentation images $s_{1:t-1}$, and depth images $d_{1:t-1}$ (where the depth and segmentations could be ground-truth or estimates from a model). We assume that a corresponding sequence of camera poses $T_{1:t-1}$ is available from an odometry system, and that the camera intrinsics are known or estimated. Our goal is to generate realistic RGB, semantic segmentation and depth images for a trajectory of future poses $T_{t},T_{t+1}, \ldots, T_{T}$, which may be provided up front or iteratively by some agent interacting with the returned observations. Note that we generate depth and segmentation because these modalities are useful in many downstream tasks. We assume that the future trajectory may traverse unseen areas of the environment, requiring the model to not only in-fill minor object dis-occlusions, but also to imagine entire room reveals (Figure 1).</p>
<p>Figure 2 shows our proposed hierarchical two-stage model for addressing this challenge. It uses a latent noise tensor $z_{t}$ to capture the stochastic information about the next observation (e.g. the layout of an unseen room) that cannot be predicted deterministically. Given a sampled noise tensor $z_{t}$, the first stage (Structure Generator) generates a new depth image $\hat{d}<em t="t">{t}$ and segmentation image $\hat{s}</em>}$ to provide a plausible high-level semantic representation of the scene, using as context the previous semantic and depth images $s_{1: t-1}, d_{1: t-1}$. In the second stage (Image Generator), the predicted semantic and depth images $\hat{s<em t="t">{t}, \hat{d}</em>}$ are rendered into a realistic RGB image $\hat{I<em 1:="1:" t-1="t-1">{t}$ using previous RGB images $I</em>$.}$ as context. In each stage, context is provided by accumulating previous observations as a 3D point cloud which is re-projected into 2D using $T_{t</p>
<h3>3.1. Structure Generator: Segmentation &amp; Depth</h3>
<p>Pathdreamer's first stage is the Structure Generator, a stochastic encoder-decoder network for generating diverse, plausible segmentation and depth images. Like [51], to provide the previous observation context, we first back-project the previous segmentations $s_{1: t-1}$ into a unified 3D semantic point cloud using the depth images $d_{1: t-1}$ and camera poses $T_{1: t-1}$. We then re-project this point cloud back into pixel space using $T_{t}$ to create sparse segmentation and depth guidance images $s_{t}^{\prime}, d_{t}^{\prime}$ which reflect the current pose.</p>
<p>The input to the encoder is a one-hot encoding of the semantic guidance image $s_{t}^{\prime} \in \mathbb{R}^{W \times H \times C}$, concatenated with the depth guidance image $d_{t}^{\prime} \in \mathbb{R}^{W \times H \times 1}$. The architecture of the encoder-decoder model is based on RedNet [34] – a ResNet-50 [28] architecture designed for indoor RGB-D semantic segmentation. RedNet uses transposed convolutions for upsampling in the decoder and skip connections between the encoder and decoder to preserve spatial information. Since the input contains a segmentation image, and segmentation classes differ across datasets, the encoder-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Pathdreamer model architecture at step t. Given a history of visual observations (RGB, depth and semantics) and a trajectory of future viewpoints, the Structure Generator conditions on a sampled noise tensor before generating semantic and depth outputs to provide a high-level structural representation of the scene. Realistic RGB images are synthesized by the Image Generator in the second stage.</p>
<p>decoder is not pretrained. We introduce the latent spatial noise tensor $z_t \in \mathbb{R}^{H' \times W' \times 32}$ into the model by concatenating it with the feature map between the encoder and the decoder. The final output of the encoder-decoder model is a segmentation image $\hat{s}_t$ and a depth image $d_t$, with segmentation predictions generated by a C-way softmax and depth outputs normalized in the range (0, 1) and generated via a sigmoid function. At each step during inference, the segmentation prediction $\hat{s}_t$ is back-projected and added to the point cloud to assist prediction in future timesteps.</p>
<p>To generate the noise tensor $z_t$, we take inspiration from SVG [11] and learn a conditional prior noise distribution $p_{\psi}(z_t | s_t', d_t)$. Intuitively, there are many possible scenes that may be generated for an unseen building region. We would like $z_t$ to carry the stochastic information about the next observation that the deterministic encoder cannot capture, and we would like for the decoder to make good use of that information. During training, we encourage the first outcome by using a KL-divergence loss to force the prior distribution $p_{\psi}(z_t | s_t', d_t)$ to be close to the posterior distribution $\phi(z_t | s_t', d_t)$ which is conditioned on the ground-truth segmentation and depth images. We encourage the second outcome by providing the decoder with sampled $z_t$ values from the posterior distribution $q_{\phi}$ (conditioned on the ground-truth outputs) during training. During inference, the latent noise $z_t$ is sampled from the prior distribution $p_{\psi}$ and the posterior distribution $q_{\phi}$ is not used. Both distributions are modeled using 3-layer CNNs that take their input from the encoder and output two channels representing $\mu$ and $\sigma$ to parameterize a multivariate Gaussian distribution $\mathcal{N}(\mu, \sigma)$. As shown in Figure 3, the noise is useful in encoding diverse, plausible representations of unseen regions.</p>
<p>Overall, the Structure Generator is trained to minimize a joint loss consisting of a cross-entropy loss $L_{\text{ce}}$ for semantic predictions, a mean absolute error term for depth predictions, and the KL-divergence term for the noise tensor:</p>
<p>$$
\mathcal{L}<em _text_ce="\text{ce">{\text{Structure}} = \lambda</em>}} L_{\text{ce}}(s_t, \hat{s<em _text_d="\text{d">t)
+ \lambda</em>}} \left| d_t - \hat{d<em _text_KL="\text{KL">t \right|_1
+ \lambda</em>(z_t | s_t', d_t))
\tag{1}
$$}} D_{\text{KL}} (q_{\phi}(z_t | s_t', d_t), p_{\psi</p>
<p>where $\lambda_{\text{ce}}$, $\lambda_{\text{d}}$, and $\lambda_{\text{KL}}$ are weights determined by a grid search. We set these to 1, 100, and 0.5 respectively.</p>
<h3>3.2. Image Generator: RGB</h3>
<p>The Image Generator is an image-to-image translation GAN [22, 78] that converts the semantic and depth predictions $\hat{s}_t$, $d_t$ from the first stage into a realistic RGB image $I_t$. Our model architecture is based on SPADE blocks [63] that use spatially-adaptive normalization layers to insert context into multiple layers of the network. As with our Structure Generator, we maintain an accumulating 3D point cloud containing all previous image observations. This provides a sparse RGB guidance image $I_t'$ when re-projected. Similar to Multi-SPADE [51], we insert two SPADE normalization layers into each residual block: one conditioned on the concatenated semantic and depth inputs $[\hat{s}_t, d_t]$, and one conditioned on the RGB guidance image $I_t'$. The sparsity of the RGB guidance image is handled by applying partial convolutions [45]. In total Image Generator consists of 7 Multi-SPADE blocks, preceded by a single convolution block.</p>
<p>Following SPADE [63], the model is trained with the GAN hinge loss, feature matching loss [78], and perceptual loss [36] from a pretrained VGG-19 [69] model. During training, the generator is provided with the ground-truth segmentation image $s_t$ and ground-truth depth image $d_t$. Our discriminator architecture is based on PatchGAN [32], and takes as input the concatenation of the ground-truth image $I_t$ or generated image $I_t'$, the ground-truth depth image $d_t$ and the ground-truth semantic image $s_t$. The losses for</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: When predicting around corners, the Structure Generator can sample diverse and semantically plausible scene layouts which are closely reflected in the RGB output of the Image Generator, shown here for two guidance image inputs (left columns; unseen areas are indicated by solid black regions). Each example shows three alternative <em>room reveals</em> and the groundtruth. In the bottom example, the model considers various completions for a bedroom but fails to anticipate the groundtruth's matching lamp on the opposite side of the bed.</p>
<p>The generator <em>G</em> and the discriminator <em>D</em> are:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _text_GAN="\text{GAN">{G} &amp;= -\lambda</em>}} \mathbb{E<em _text_VGG="\text{VGG">{x_t} [D(G(x_t))] \
&amp;+ \lambda</em>(G(x_t)) \right|}} \sum_{i=1}^{n} \frac{1}{n} \left| \phi^{(i)}(I_t) - \phi^{(i)<em _text_FM="\text{FM">1 \
&amp;+ \lambda</em>(G(x_t)) \right|}} \sum_{i} \frac{1}{n} \left| D^{(i)}(I_t) - D^{(i)<em D="D">1 \
\mathcal{L}</em>} &amp;= -\mathbb{E<em x_t="x_t">{x_t} [\min(0, -1 + D(I_t))] \
&amp;- \mathbb{E}</em> [\min(0, -1 - D(G(x_t)))]
\end{aligned}
$$</p>
<p>where <em>x<sub>t</sub></em> = (<em>s<sub>t</sub></em>, <em>d<sub>t</sub></em>, <em>I<sub>t</sub><sup></em></sup><em>) denotes the complete set of inputs to the generator, </em>φ̃<em> denotes the output of the </em>i<em><sup>th</sup> layer of the pretrained VGG-19 model, </em>D<em> denotes the output of the discriminator's </em>i<em>-th layer (conditioning inputs </em>s<sub>t</sub><em>, </em>d<sub>t</sub>* to the discriminator have been dropped to save space). Like the Structure Generator, the Image Generator is not pretrained.</p>
<h3>3.3. Training and Inference</h3>
<p><strong>Dataset</strong> For training and evaluation we use Matterport3D [7], a dataset of 10.8k RGB-D images from 90 building-scale indoor environments. For each environment, Matterport3D also includes a textured 3D mesh which is annotated with 40 semantic classes of objects and building components. To align with downstream VLN tasks, in all experiments the RGB, depth and semantic images are 360° panoramas in equirectangular format.</p>
<p><strong>Trajectories</strong> To train Pathdreamer, we sampled 400k trajectories from the Matterport3D training environments. To define feasible trajectories, we used the navigation graphs from the Room-to-Room (R2R) dataset [3], in which nodes correspond to panoramic image locations, and edges define navigable state transitions. For each trajectory 5–8 panoramas were sampled, choosing the starting node and the edge transitions uniformly at random. On average the viewpoints in these trajectories are 2m apart. Training with relatively large viewpoint changes is desirable, since the model learns to synthesize observations with large viewpoint changes in a single step (without the need to incur the computational cost of generating intervening frames). However, this does not preclude Pathdreamer from generating smooth video outputs at high frame rates<sup>1</sup>.</p>
<p><strong>Training</strong> The first and second stages of the model are trained separately. For the Image Generator, we use the Matterport3D RGB panoramas as training targets at 1024×512 resolution. We use the Habitat simulator [67] to render ground-truth depth and semantic training inputs and stitch these into equirectangular panoramas. We perform data augmentation by randomly cropping and horizontally rolling the RGB panoramas, which we found essential due to the limited number of panoramas available.</p>
<p>To train the Structure Generator, we again used Habitat to render depth and semantic images. Since this stage does not require aligned RGB images for training, in this case we performed data augmentation by perturbing the viewpoint coordinates with a random Gaussian noise vector drawn from <em>N</em>(0, 0.2m) independently along each 3D axis. The Structure Generator was trained with equirectangular panoramas at 512×256 resolution.</p>
<p><strong>Inference</strong> To avoid heading discontinuities during inference, we use circular padding on the image x-axis for both the Structure Generator and the Image Generator.</p>
<p><sup>1</sup>See https://youtu.be/StklIENGqs0 for our video generation results.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Example full prediction sequence beginning with one observation (depth, semantics, RGB) as context and generating observations for 3 new viewpoints traversing a corridor. At 2.3m the model completes a room reveal, imagining a kitchen-like space. After 8.6m the model's predictions degrade. More examples are provided in the supplementary.</p>
<p>The 512×256 resolution semantic and depth outputs of the Structure Generator are upsampled to 1024×512 using nearest neighbor interpolation before they are passed to the Image Generator. In quantitative experiments, we set the Structure Generator noise tensor <em>z<sub>t</sub></em> to the mean of the prior.</p>
<h2>4. Experiments</h2>
<p>For evaluation we use the paths from the Val-Seen and Val-Unseen splits of the R2R dataset [3]. Val-Seen contains 340 trajectories from environments in the Matterport3D training split. Val-Unseen contains 783 trajectories in Matterport3D environments not seen in training. Since R2R trajectories contain 5-7 panoramas and at least 1 previous observation is given as context, we report evaluations over 1–6 steps, representing predictions over trajectory rollouts of around 2–13m (panoramas are 2.25m apart on average). See Figure 4 for an example rollout over 8.6m. We characterize the performance of Pathdreamer in comparison to baselines, ablations and in the context of the downstream task of Vision-and-Language Navigation (VLN).</p>
<h3>4.1. Pathdreamer Results</h3>
<p><strong>Semantic Generation</strong> A key feature of our approach is the ability to generate semantic segmentation and depth outputs, in addition to RGB. We evaluate the generated semantic segmentation images using mean Intersection-Over-Union (mIOU) and report results for:</p>
<ul>
<li><strong>Nearest Neighbor</strong>: A baseline without any learned components, using nearest-neighbor interpolation to fill holes in the projected semantic guidance image <em>s′<sub>t</sub></em>.</li>
<li><strong>Ours (Teacher Forcing)</strong>: Structure Generator trained using the ground truth semantic and depth images as the previous observation at every time step.</li>
<li><strong>Ours (Recurrent)</strong>: Structure Generator trained while feeding back its own semantic and depth predictions as previous observations for the next step prediction. This reduces train-test mismatch and may allow the model to compensate for errors when doing longer roll-outs.</li>
</ul>
<p>We also tried training the hierarchical convolutional LSTM from [42], but found that it frequently collapsed to a single class prediction. We attribute this to the large viewpoint changes and heavy occlusion in the training sequences; we believe this can be more effectively modeled with point cloud geometry than with a geometry-unaware LSTM.</p>
<p>As illustrated in Table 1, Pathdreamer performs far better than the Nearest Neighbor baseline regardless of the number of steps in the rollout or the number of previous observations used as context. As expected, performance in seen environments is higher than unseen. Perhaps surprisingly, in Figure 5a we show that Recurrent training improves results during longer rollouts in the training environments (Val-Seen), but this does not improve results on Val-Unseen, perhaps indicating that the error compensation learned by the Image Generator does not easily generalize.</p>
<p>In addition to accurate predictions, we also want generated results to be <em>diverse</em>. Figure 3 shows that our model can generate diverse semantic scenes by interpolating the noise tensor <em>z<sub>t</sub></em>, and that the RGB outputs closely reflect the generated semantic image. This allows us to generate multiple plausible alternatives for the same navigation trajectory.</p>
<p><strong>RGB Generation</strong> To evaluate the quality of RGB panoramas generated by the Image Generator, we compute the Fréchet Inception Distance (FID) [30] between generated and real images for each step in the paths. We report results using the semantic images generated by the Structure Generator as inputs (i.e., our full model). To quantify the potential for uplift with better Structure Generators, we also report results using ground truth semantic segmentations as input. We compare to two ablated versions of our model:</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
(a) Semantic segmentation mean-IOU ( $\uparrow$ ). [TF]: Teacher Forcing. [Rec]: Recurrent.
<img alt="img-5.jpeg" src="img-5.jpeg" />
(b) RGB generation FID ( $\downarrow$ ). [GT]: Ground truth semantic inputs. [SG]: Structure Generator predictions.</p>
<p>Figure 5: Pathdreamer semantic segmentation mean-IOU (above) and RGB generation FID (below). Results are shown for Val-Seen (left) and Val-Unseen (right). Confidence intervals indicate the range of outcomes with 1,2 or 3 previous observations as context.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Val-Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Val-Unseen</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Context</td>
<td style="text-align: center;">1 Step</td>
<td style="text-align: center;">$1-6$ Steps</td>
<td style="text-align: center;">1 Step</td>
<td style="text-align: center;">$1-6$ Steps</td>
</tr>
<tr>
<td style="text-align: left;">Nearest Neighbor</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">30.6</td>
</tr>
<tr>
<td style="text-align: left;">Ours (Teacher Forcing)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\mathbf{8 4 . 9}$</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">$\mathbf{7 8 . 3}$</td>
<td style="text-align: center;">50.8</td>
</tr>
<tr>
<td style="text-align: left;">Ours (Recurrent)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">$\mathbf{6 5 . 9}$</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">$\mathbf{5 0 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Nearest Neighbor</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">33.8</td>
</tr>
<tr>
<td style="text-align: left;">Ours (Teacher Forcing)</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$\mathbf{8 5 . 4}$</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">$\mathbf{7 7 . 4}$</td>
<td style="text-align: center;">55.5</td>
</tr>
<tr>
<td style="text-align: left;">Ours (Recurrent)</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">$\mathbf{7 0 . 2}$</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">$\mathbf{5 5 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Nearest Neighbor</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">37.7</td>
</tr>
<tr>
<td style="text-align: left;">Ours (Teacher Forcing)</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\mathbf{8 5 . 1}$</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">$\mathbf{7 7 . 3}$</td>
<td style="text-align: center;">60.4</td>
</tr>
<tr>
<td style="text-align: left;">Ours (Recurrent)</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">$\mathbf{7 2 . 7}$</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">$\mathbf{6 0 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Mean-IOU ( $\uparrow$ ) for generated semantic segmentations with varying context and prediction steps.</p>
<ul>
<li>No Semantics: The semantic and depth inputs $s_{t}, d_{t}$ are removed from the Multi-SPADE blocks.</li>
<li>SPADE: An ablation of the RGB inputs to the model, comprising the previous RGB image $I_{t-1}$ and the reprojected RGB guidance image $I_{t}^{\prime}$. The semantic image $s_{t}$ replaces $I_{t-1}$ as input to the model and the $I_{t}^{\prime}$ input layers are removed from the Multi-SPADE blocks, making this effectively the SPADE model [63].
As shown in Table 2, SPADE performs the best in ValSeen, indicating that the model has the capacity to mem-</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Inputs</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Val-Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Val-Unseen</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Context</td>
<td style="text-align: center;">Obs</td>
<td style="text-align: center;">Sem</td>
<td style="text-align: center;">RGB</td>
<td style="text-align: center;">1 Step</td>
<td style="text-align: center;">1-6 Steps</td>
<td style="text-align: center;">1 Step 1-6 Steps</td>
</tr>
<tr>
<td style="text-align: center;">No Semantics</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">35.2 90.8</td>
</tr>
<tr>
<td style="text-align: center;">SPADE</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">GT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">47.3 50.3</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">GT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">32.4 39.9</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">SG</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">34.8 70.4</td>
</tr>
<tr>
<td style="text-align: center;">No Semantics</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">39.5 78.6</td>
</tr>
<tr>
<td style="text-align: center;">SPADE</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">GT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">52.3 51.2</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">GT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">35.3 39.9</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">SG</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">38.2 61.0</td>
</tr>
<tr>
<td style="text-align: center;">No Semantics</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">41.9 67.5</td>
</tr>
<tr>
<td style="text-align: center;">SPADE</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">GT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">52.8 50.7</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">GT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">35.5 39.2</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">SG</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">38.5 52.9</td>
</tr>
</tbody>
</table>
<p>Table 2: FID scores ( $\downarrow$ ) for generated RGB images with varying context and prediction steps, using either ground truth semantics (GT) or Structure Generator predictions (SG) as input.
orize the training environments. In this case, RGB inputs are not necessary. However, our model performs noticeably better in Val-Unseen, highlighting the importance of maintaining RGB context in unseen environments (which is our focus). Performance degrades significantly in the No Semantics setting in both Val-Seen and Val-Unseen. We observed that without semantic inputs, the model is unable to generate meaningful images over longer horizons, which validates our two-stage hierarchical approach. These results are reflected in the FID scores, as well as qualitatively (Figure 6); Image Generator's outputs are significantly crisper, especially over longer horizons. Due to the benefit of guidance images, the Image Generator's textures are also generally better matched with the unseen environment, while SPADE tends to wash out textures, usually creating images of a standard style. Figure 5b plots performance for every setting step-by-step. FID of the Image Generator improves substantially when using ground truth semantics, particularly for longer rollouts, highlighting the potential to benefit from improvements to the Structure Generator.</p>
<h3>4.2. VLN Results</h3>
<p>Finally, we evaluate whether predictions from Pathdreamer can improve performance on a downstream visual navigation task. We focus on Vision-and-Language Navigation (VLN) using the R2R dataset [3]. Because reaching the navigation goal requires successfully grounding natural language instructions to visual observations, this provides a challenging task-based assessment of prediction quality.</p>
<p>In our inference setting, at each step while moving through the environment we use a baseline VLN agent based on [79] to generate a large number of possible future trajectories using beam search. We then rank these alternative trajectories using an instruction-trajectory compatibility model [84] to assess which trajectory best matches the</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: Visual comparison of ablated Image Generator outputs on Val-Unseen with <em>ground truth</em> segmentation and depth. Both RGB and semantic context are important for best performance.</p>
<p>instruction. The agent then executes the first action from the top-ranked trajectory before repeating the process. We consider three different planning horizons, with future trajectories containing 1, 2 or 3 forward steps.</p>
<p>The instruction-trajectory compatibility model is a dual-encoder that separately encodes textual instructions and trajectories (encoded using visual observations and path geometry) into a shared latent space. To improve performance on incomplete paths, we introduce truncated paths into the original contrastive training scheme proposed in [84]. The compatibility model is trained using only ground truth observations. However, during inference, RGB observations for future steps are drawn from three different sources:</p>
<ul>
<li><strong>Ground truth</strong>: RGB observations from the actual environment, i.e., look-ahead observations.</li>
<li><strong>Pathdreamer</strong>: RGB predictions from our model.</li>
<li><strong>Repeated pano</strong>: A simple baseline in which the most recent RGB observation is repeated in future steps.</li>
<li><strong>Blank pano</strong>: A simple baseline in which blank images are provided as future observations.</li>
</ul>
<p>Note that in all cases the geometry of the future trajectories is determined by the ground truth R2R navigation graphs. In Table 3, we report Val-Unseen results for this experiment using standard metrics for VLN: navigation error (NE), success rate (SR), shortest path length (SPL), normalized Dynamic Time Warping (nDTW) [31], and success weighted by normalized Dynamic Time Warping (sDTW) [31].</p>
<p>Consistent with prior work [20, 73], we find that looking ahead using ground truth visual observations provides a robust performance boost, e.g., success rate increases from 44.6% with 1 planning step (top panel) to 59.3% with 3 planning steps (bottom panel). At the other extreme, the Repeated pano baseline is weak, with a success rate of just 35.7% with 1 planning step (top row). The Blank pano baseline is similar, with a success rate of 35.9%. This is not surprising: these baselines deny the compatibility model any useful visual representation of the next action, which is crucial to performance [20, 73]. However, increasing the planning horizon does improve performance even for the Repeated/Blank pano baselines, since the compatibility model is able to compare the geometry of alternative future trajectories. Finally, we observe that using Pathdreamer's visual observations closes about half the gap between the Repeated pano baseline and the ground truth observations, e.g., 50.4% success with Pathdreamer vs. 40.6% and 59.3% respectively for the others. We conclude that using Pathdreamer as a visual world model can improve performance on downstream tasks, although existing agents still rely on using a navigation graph to define the feasible action space at each step. Pathdreamer is complementary to current SOTA model-based approaches, and a combination would likely lead to further boosts in VLN performance, which is worth investigating in future work.</p>
<table>
<thead>
<tr>
<th>Observations</th>
<th>Plan Steps</th>
<th>NE ↓</th>
<th>SR ↑</th>
<th>SPL ↑</th>
<th>nDTW ↑</th>
<th>sDTW ↑</th>
</tr>
</thead>
<tbody>
<tr>
<td>Repeated pano</td>
<td>1</td>
<td>6.75</td>
<td>35.7</td>
<td>33.8</td>
<td>52.0</td>
<td>31.2</td>
</tr>
<tr>
<td>Blank pano</td>
<td>1</td>
<td>7.29</td>
<td>35.9</td>
<td>33.7</td>
<td>50.9</td>
<td>31.5</td>
</tr>
<tr>
<td>Pathdreamer</td>
<td>1</td>
<td>6.55</td>
<td>39.9</td>
<td>38.3</td>
<td>54.6</td>
<td>35.2</td>
</tr>
<tr>
<td>Ground truth</td>
<td>1</td>
<td>5.80</td>
<td>44.6</td>
<td>42.7</td>
<td>58.9</td>
<td>39.4</td>
</tr>
<tr>
<td>Repeated pano</td>
<td>2</td>
<td>6.76</td>
<td>36.8</td>
<td>34.0</td>
<td>51.8</td>
<td>31.7</td>
</tr>
<tr>
<td>Blank pano</td>
<td>2</td>
<td>6.65</td>
<td>40.0</td>
<td>37.2</td>
<td>53.3</td>
<td>34.8</td>
</tr>
<tr>
<td>Pathdreamer</td>
<td>2</td>
<td>5.8</td>
<td>46.5</td>
<td>43.9</td>
<td>59.1</td>
<td>41.2</td>
</tr>
<tr>
<td>Ground truth</td>
<td>2</td>
<td>4.95</td>
<td>54.3</td>
<td>51.3</td>
<td>64.9</td>
<td>48.3</td>
</tr>
<tr>
<td>Repeated pano</td>
<td>3</td>
<td>6.25</td>
<td>40.6</td>
<td>37.7</td>
<td>55.6</td>
<td>35.2</td>
</tr>
<tr>
<td>Blank pano</td>
<td>3</td>
<td>6.48</td>
<td>41.9</td>
<td>38.8</td>
<td>54.2</td>
<td>36.1</td>
</tr>
<tr>
<td>Pathdreamer</td>
<td>3</td>
<td>5.32</td>
<td>50.4</td>
<td>47.3</td>
<td>61.8</td>
<td>44.4</td>
</tr>
<tr>
<td>Ground truth</td>
<td>3</td>
<td>4.44</td>
<td>59.3</td>
<td>55.8</td>
<td>67.9</td>
<td>52.7</td>
</tr>
</tbody>
</table>
<p>Table 3: VLN Val-Unseen results using an instruction-trajectory compatibility model to rank alternative future trajectories with planning horizons of 1, 2 or 3 steps.</p>
<p>1% with 1 planning step (top row). The Blank pano baseline is similar, with a success rate of 35.9%. This is not surprising: these baselines deny the compatibility model any useful visual representation of the next action, which is crucial to performance [20, 73]. However, increasing the planning horizon does improve performance even for the Repeated/Blank pano baselines, since the compatibility model is able to compare the geometry of alternative future trajectories. Finally, we observe that using Pathdreamer's visual observations closes about half the gap between the Repeated pano baseline and the ground truth observations, e.g., 50.4% success with Pathdreamer vs. 40.6% and 59.3% respectively for the others. We conclude that using Pathdreamer as a visual world model can improve performance on downstream tasks, although existing agents still rely on using a navigation graph to define the feasible action space at each step. Pathdreamer is complementary to current SOTA model-based approaches, and a combination would likely lead to further boosts in VLN performance, which is worth investigating in future work.</p>
<h3>5. Conclusion</h3>
<p>Pathdreamer is a stochastic hierarchical visual world model that can synthesize realistic and diverse 360° panoramic images for unseen trajectories in real buildings. As a visual world model, Pathdreamer also shows strong promise in improving performance on downstream tasks, which we show with VLN. Most notably, we show that Pathdreamer captures around half the benefit of looking ahead at actual observations from the environment. The efficacy of Pathdreamer in the VLN task may be attributed to its ability to model fundamental constraints in the real world, and thus relieve agents from having to learn the geometry and visual and semantic structure of buildings. Applying Pathdreamer to other embodied navigation tasks such as Object-Nav [5], VLN-CE [39] and street-level navigation [9, 53] are natural directions for future work.</p>
<h2>References</h2>
<p>[1] Sandra Aigner and Marco Körner. Futuregan: Anticipating the future frames of video sequences using spatiotemporal 3d convolutions in progressively growing gans. arXiv preprint arXiv:1810.01325, 2018. 2
[2] Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, and Amir R. Zamir. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757, 2018. 3
[3] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In CVPR, pages 3674-3683, 2018. 1, 2, 3, 5, 6,7
[4] Mohammad Babaeizadeh, Mohammad Taghi Saffar, Danijar Hafner, Harini Kannan, Chelsea Finn, Sergey Levine, and Dumitru Erhan. Models, pixels, and rewards: Evaluating design trade-offs in visual model-based reinforcement learning. arXiv preprint arXiv:2012.04603, 2020. 3
[5] Dhruv Batra, Aaron Gokaslan, Aniruddha Kembhavi, Oleksandr Maksymets, Roozbeh Mottaghi, Manolis Savva, Alexander Toshev, and Erik Wijmans. Objectnav revisited: On evaluation of embodied agents navigating to objects. arXiv preprint arXiv:2006.13171, 2020. 1, 3, 8
[6] Lars Buesing, Theophane Weber, Yori Zwols, Sebastien Racaniere, Arthur Guez, Jean-Baptiste Lespiau, and Nicolas Heess. Woulda, coulda, shoulda: Counterfactually-guided policy search. In $I C L R, 2019.1$
[7] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from RGBD data in indoor environments. International Conference on 3D Vision (3DV), 2017. 2, 3, 5
[8] Matthew Chang, Arjun Gupta, and Saurabh Gupta. Semantic visual navigation by watching youtube videos. NeurIPS, 2020. 3
[9] Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In CVPR, 2019. 3, 8
[10] Silvia Chiappa, Sébastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent environment simulators. $I C L R$, 2017. 2
[11] Emily Denton and Rob Fergus. Stochastic video generation with a learned prior. 2018. 2, 4
[12] Helisa Dhamo, Keisuke Tateno, Iro Laina, Nassir Navab, and Federico Tombari. Peeking behind objects: Layered depth prediction from a single image. Pattern Recognition Letters, 125:333-340, 2019. 3
[13] Alexey Dosovitskiy and Vladlen Koltun. Learning to act by predicting the future. In $I C L R, 2017.2$
[14] Frederik Ebert, Chelsea Finn, Alex X Lee, and Sergey Levine. Self-supervised visual planning with temporal skip connections. Conference on Robot Learning (CoRL), 2017. 2
[15] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. In NeurIPS, pages 64-72, 2016. 2
[16] Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In ICRA, pages 2786-2793. IEEE, 2017. 1, 2
[17] Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter Abbeel. Deep spatial autoencoders for visuomotor learning. In ICRA, pages 512-519. IEEE, 2016. 1
[18] John Flynn, Michael Broxton, Paul Debevec, Matthew DuVall, Graham Fyffe, Ryan Overbeck, Noah Snavely, and Richard Tucker. Deepview: View synthesis with learned gradient descent. In CVPR, pages 2367-2376, 2019. 3
[19] John Flynn, Ivan Neulander, James Philbin, and Noah Snavely. Deepstereo: Learning to predict new views from the world's imagery. In CVPR, 2016. 1, 3
[20] Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor BergKirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-follower models for vision-and-language navigation. NeurIPS, 2018. 3, 8
[21] Tsu-Jui Fu, Xin Eric Wang, Matthew F Peterson, Scott T Grafton, Miguel P Eckstein, and William Yang Wang. Counterfactual vision-and-language navigation via adversarial path sampler. In ECCV, pages 71-86, 2020. 3
[22] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. NeurIPS, 2014. 4
[23] David Ha and Jürgen Schmidhuber. World models. NeurIPS, 2018. 1, 2
[24] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. $I C L R, 2019.2$
[25] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020. 1, 2
[26] Meera Hahn, Jacob Krantz, Dhruv Batra, Devi Parikh, James M. Rehg, Stefan Lee, and Peter Anderson. Where are you? localization from embodied dialog. 2020. 1, 3
[27] Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao. Towards learning a generic agent for vision-and-language navigation via pre-training. In CVPR, June 2020. 3
[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770-778, 2016. 3
[29] Phlipp Henzler, Volker Rasche, Timo Ropinski, and Tobias Ritschel. Single-image tomography: 3d volumes from 2d cranial x-rays. In Computer Graphics Forum, volume 37, pages 377-388. Wiley Online Library, 2018. 3
[30] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, 2017. 6</p>
<p>[31] Gabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, and Jason Baldridge. General evaluation for instruction conditioned navigation using dynamic time warping. NeurIPS Workshop on Visually Grounded Interaction and Language (ViGIL), 2019. 8
[32] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. CVPR, 2017. 4
[33] Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie, and Jason Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. 2019. 3
[34] Jindong Jiang, Lunan Zheng, Fei Luo, and Zhijun Zhang. Rednet: Residual encoder-decoder network for indoor rgbd semantic segmentation. arXiv preprint arXiv:1806.01054, 2018. 3
[35] Xiaojie Jin, Huaxin Xiao, Xiaohui Shen, Jimei Yang, Zhe Lin, Yunpeng Chen, Zequn Jie, Jiashi Feng, and Shuicheng Yan. Predicting scene parsing and motion dynamics in the future. In NeurIPS, pages 6915-6924, 2017. 2
[36] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, pages 694-711, 2016. 4
[37] Abhishek Kar, Christian Häne, and Jitendra Malik. Learning a multi-view stereo machine. In NeurIPS, pages 365-376, 2017. 3
[38] Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick Van der Smagt. Deep variational bayes filters: Unsupervised learning of state space models from raw data. $I C L R$, 2017. 2
[39] Jacob Krantz, Erik Wijmans, Arjun Majundar, Dhruv Batra, and Stefan Lee. Beyond the nav-graph: Vision and language navigation in continuous environments. In ECCV, 2020. 3, 8
[40] Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge. Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. EMNLP, 2020. 1, 3
[41] Yong-Hoon Kwon and Min-Gyu Park. Predicting future frames using retrospective cycle gan. In CVPR, pages 18111820, 2019. 2
[42] Wonkwang Lee, Whie Jung, Han Zhang, Ting Chen, Jing Yu Koh, Thomas Huang, Hyungsuk Yoon, Honglak Lee, and Seunghoon Hong. Revisiting hierarchical approach for persistent long-term video prediction. In $I C L R, 2021.2,6$
[43] Zuoyue Li, Zhaopeng Cui, and Martin R Oswald. Streetview panoramic video synthesis from a single satellite image. arXiv preprint arXiv:2012.06628, 2020. 2
[44] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from a single image. arXiv preprint arXiv:2012.09855, 2020. 2
[45] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for irregular holes using partial convolutions. In ECCV, pages $85-100,2018.4$
[46] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In NeurIPS, pages 13-23, 2019. 3
[47] Pauline Luc, Natalia Neverova, Camille Couprie, Jakob Verbeek, and Yann LeCun. Predicting deeper into the future of semantic segmentation. In ICCV, pages 648-657, 2017. 2
[48] Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, and Caiming Xiong. Selfmonitoring navigation agent via auxiliary progress estimation. $I C L R, 2019.3$
[49] Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming Xiong, and Zsolt Kira. The regretful agent: Heuristic-aided navigation through progress estimation. In CVPR, pages 6732-6740, 2019. 3
[50] Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, and Dhruv Batra. Improving vision-and-language navigation with image-text pairs from the web. ECCV, 2020. 2, 3
[51] Arun Mallya, Ting-Chun Wang, Karan Sapra, and MingYu Liu. World-consistent video-to-video synthesis. ECCV, 2020. 2, 3, 4
[52] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. In CVPR, 2021. 3
[53] Harsh Mehta, Yoav Artzi, Jason Baldridge, Eugene Ie, and Piotr Mirowski. Retouchdown: Adding touchdown to streetlearn as a shareable resource for language grounding tasks in street view. EMNLP Workshop on Spatial Language Understanding (SpLU), 2020. 3, 8
[54] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (TOG), 38(4):1-14, 2019. 3
[55] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. ECCV, 2020. 3
[56] Piotr Mirowski, Andras Banki-Horvath, Keith Anderson, Denis Teplyashin, Karl Moritz Hermann, Mateusz Malinowski, Matthew Koichi Grimes, Karen Simonyan, Koray Kavukcuoglu, Andrew Zisserman, et al. The streetlearn environment and dataset. arXiv preprint arXiv:1903.01292, 2019. 3
[57] Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. In ICRA, pages 7559-7566. IEEE, 2018. 1
[58] Medhini Narasimhan, Erik Wijmans, Xinlei Chen, Trevor Darrell, Dhruv Batra, Devi Parikh, and Amanpreet Singh. Seeing the un-scene: Learning amodal semantic maps for room navigation. In ECCV, pages 513-529. Springer, 2020. 3
[59] Khanh Nguyen and Hal Daumé III. Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. EMNLP, 2019. 3</p>
<p>[60] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. In NeurIPS, pages 28632871, 2015. 2
[61] Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, NeurIPS, 2017. 2
[62] Blazej Osinski, Chelsea Finn, Dumitru Erhan, George Tucker, Henryk Michalewski, Konrad Czechowski, Lukasz Mieczyslaw Kaiser, Mohammad Babaeizadeh, Piotr Kozakowski, Piotr Milos, Roy H Campbell, Afroz Mohiuddin, Ryan Sepassi, and Sergey Levine. Model-based reinforcement learning for atari. In $I C L R, 2020.1,2$
[63] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In CVPR, pages 2337-2346, 2019. 2, 4, 7
[64] Aj Piergiovanni, Alan Wu, and Michael S Ryoo. Learning real-world robot policies by dreaming. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 7680-7687. IEEE, 2019. 1
[65] Senthil Purushwalkam, Sebastian Vicenc Amengual Gari, Vamsi Krishna Ithapu, Carl Schissler, Philip Robinson, Abhinav Gupta, and Kristen Grauman. Audio-visual floorplan reconstruction. arXiv preprint arXiv:2012.15470, 2020. 3
[66] Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, and Anton van den Hengel. Reverie: Remote embodied visual referring expression in real indoor environments. In CVPR, 2020. 1, 3
[67] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A Platform for Embodied AI Research. In ICCV, 2019. 5
[68] Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin Huang. 3d photography using context-aware layered depth inpainting. In CVPR, June 2020. 3
[69] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. $I C L R$, 2015. 4
[70] Pratul P Srinivasan, Richard Tucker, Jonathan T Barron, Ravi Ramamoorthi, Ren Ng, and Noah Snavely. Pushing the boundaries of view extrapolation with multiplane images. In CVPR, pages 175-184, 2019. 3
[71] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele, Steven Lovegrove, and Richard Newcombe. The Replica dataset: A digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2019. 3
[72] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. A Bradford Book, Cambridge, MA, USA, 2018. 1
[73] Hao Tan, Licheng Yu, and Mohit Bansal. Learning to navigate unseen environments: Back translation with environmental dropout. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2610-2621, 2019. 3, 8
[74] Jesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer. Vision-and-dialog navigation. In Conference on Robot Learning (CoRL), pages 394-406. PMLR, 2020. 1,3
[75] Suhani Vora, Reza Mahjourian, Soeren Pirk, and Anelia Angelova. Future semantic segmentation leveraging 3d information. ECCV 3D Reconstruction meets Semantics Workshop, 2018. 2
[76] Jacob Walker, Abhinav Gupta, and Martial Hebert. Patch to the future: Unsupervised visual prediction. In CVPR, pages 3302-3309, 2014. 2
[77] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-tovideo synthesis. NeurIPS, 2018. 2
[78] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional gans. In CVPR, pages 8798-8807, 2018. 4
[79] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and selfsupervised imitation learning for vision-language navigation. In CVPR, pages 6629-6638, 2019. 7
[80] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. Synsin: End-to-end view synthesis from a single image. In CVPR, pages 7467-7477, 2020. 1, 3
[81] Fei Xia, Amir R. Zamir, Zhi-Yang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: real-world perception for embodied agents. In CVPR. IEEE, 2018. 3
[82] Jingwei Xu, Bingbing Ni, Zefan Li, Shuo Cheng, and Xiaokang Yang. Structure preserving video prediction. In CVPR, pages 1460-1469, 2018. 2
[83] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. arXiv preprint arXiv:2012.02190, 2020. 3
[84] Ming Zhao, Peter Anderson, Vihan Jain, Su Wang, Alex Ku, Jason Baldridge, and Eugene Ie. On the evaluation of vision-and-language navigation instructions. In Conference of the European Chapter of the Association for Computational Linguistics (EACL), 2021. 7, 8
[85] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. SIGGRAPH, 2018. 3
[86] Fengda Zhu, Yi Zhu, Xiaojun Chang, and Xiaodan Liang. Vision-language navigation with self-supervised auxiliary reasoning tasks. In CVPR, pages 10012-10022, 2020. 3
[87] Wanrong Zhu, Xin Wang, Tsu-Jui Fu, An Yan, Pradyumna Narayana, Kazoo Sone, Sugato Basu, and William Yang Wang. Multimodal text style transfer for outdoor vision-andlanguage navigation. EACL, 2021. 3</p>
<h2>A. Qualitative Results</h2>
<h2>A.1. Val-Unseen Results</h2>
<p>We present additional qualitative results generated by the Pathdreamer model, on the Val-Unseen split of the R2R dataset. These environments are not seen by the model during training, and act as a test of the generalization ability of Pathdreamer. Figure 7 presents cherry-picked examples of Pathdreamer generated sequences and Figure 8 presents randomly chosen examples.</p>
<h2>A.2. Val-Seen Results</h2>
<p>The Val-Seen split contains novel navigation trajectories for environments seen in the training split. As described in the main paper, the Structure Generator and Image Generator models perform significantly better on Val-Seen, since they can memorize the training environments with less generalization required. We observe that generation results maintain high fidelity even at large distances from the location of the input observation. Figure 9 presents cherrypicked examples and Figure 10 presents random examples.</p>
<h2>A.3. Noise Interpolation</h2>
<p>As described in the main paper, the Structure Generator is able to generate alternative, diverse, room reveals for a given scene. In areas without guidance pixels (i.e., in areas of the environment not seen from a previous view), interpolating over different noise vectors produces diverse and plausible outcomes. Figure 11 presents cherry-picked examples of Pathdreamer sequences when conditioned on different noise vectors. Figure 12 contains additional examples of randomly selected sequences conditioned on random noise vectors.</p>
<h2>B. Generated Videos</h2>
<p>In addition to image generation, Pathdreamer is capable of generating continuous video sequences, simply by sequencing generated images with small viewpoint changes. We provide videos displaying Pathdreamer generated results for several unseen environments. We also compare Pathdreamer's rendering quality using multiple observations to the output of the Habitat simulator using meshbased rendering, illustrating a favorable comparison in quality. We refer readers to the YouTube link ${ }^{2}$ for the video results.</p>
<h2>C. Implementation Details</h2>
<p>Structure Generator We trained this model with a batch size of 64 , over 50 epochs. For the first 30 epochs the model is trained with teacher forcing, i.e., the previous observation</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>that is used as input at each step is the ground-truth previous observation. During the teacher forcing stage, the number of ground truth context frames used for a path of length $L$ is decayed uniformly from $L-1$ to 1 . After 30 epochs, we switch to the recurrent setting in which the model's previous prediction is used as the previous observation at each step during the rollout (similar to our setup at inference time). We use the Adam optimizer with parameters $\beta_{1}=0.9$ and $\beta_{2}=0.999$. We apply a learning rate which starts at $1 e^{-4}$ and warms up to $2 e^{-4}$ uniformly over 10 epochs.</p>
<p>Image Generator This model was trained with a batch size of 128 over 500 epochs. We use the Adam optimizer with parameters $\beta_{1}=0.5$ and $\beta_{2}=0.999$, and a learning rate of $2 e^{-4}$ for both the generator and the discriminator. During training, the discriminator is trained for 2 steps for each generator training step. Following standard practice, at inference time we applied an exponential moving average (EMA) to the generator weights with 0.999 decay. For the choice of loss weights, we set $\lambda_{\mathrm{GAN}}=1, \lambda_{\mathrm{VOG}}=0.07$, and $\lambda_{\mathrm{FM}}=0$. We found experimentally that excluding the feature matching loss speeds up training throughput and did not have a significant effect on the results.</p>
<p>Evaluation Details We compute the FID score ${ }^{3}$ using 10,000 random samples for each prediction sequence step. As the R2R validation sets contain 783 and 340 sequences for Val-Unseen and Val-Seen respectively, we perform data augmentation with random horizontal roll and flips to acquire 10,000 samples.</p>
<p>We run evaluation every 2000 training steps, and select the best checkpoint on the Val-Seen and Val-Unseen set for reporting results for the respective split. We note that training time is generally significantly longer on Val-Seen as compared to Val-Unseen. Due to the benefit of overfitting on the training set for Val-Seen, performance continues to improve on even as generalization performance on ValUnseen deteriorates.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" />
(a) At 5.5 m , Pathdreamer completes a room reveal - imagining a living room space. Despite differences in details (e.g. door on the left rather than a window, and placement of a couch closer to the right), the result is quite close to the ground truth.
<img alt="img-8.jpeg" src="img-8.jpeg" />
(b) In this example, the navigation sequence exits a bedroom. Pathdreamer is able to maintain consistency and realism for unseen corners of the room ( 1.9 m and 5.1 m ). At 8.4 m , Pathdreamer also generates a plausible hallway seen after exiting the room.
<img alt="img-9.jpeg" src="img-9.jpeg" />
(c) Despite the initial provided observation being quite distant, Pathdreamer is able to synthesize realistic segmentation and RGB details of the table, paintings, and doorway at 3.5 m after moving closer. A plausible hallway scene is also synthesized at 10.3 m and 12.5 m .</p>
<p>Figure 7: Selected examples of predicted sequences from the Val-Unseen split using one ground truth observation as context.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" />
(a) In this sequence, feature predictions in the segmentation space are generally preserved up to 4.7 m . The predictions tend to diverge from the groundtruth after, due to the failure of the model to predict the existence of a stairwell at 4.7 m
<img alt="img-11.jpeg" src="img-11.jpeg" />
(b) Model predictions in the segmentation space are generally accurate up to 4.8 m . However, the corresponding RGB outputs are not as realistic, likely due to difficulties in generating intricate objects such as stair railings.
<img alt="img-12.jpeg" src="img-12.jpeg" />
(c) In this example, the model fails to predict the existence of the room entrance in the center of the panorama at 2.1 m , which leads to the outputs eventually diverging from the ground truth. Despite this, predicted results remain generally plausible.</p>
<p>Figure 8: Randomly selected prediction sequences from the Val-Unseen split using one ground truth observation as context.</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" />
(a) Pathdreamer is able to almost perfectly recreate the navigation sequence, despite only being provided with a single ground truth observation. Several details are missed in the RGB outputs, such as the sliding glass doors at 5.6 m . This is likely due to the poor quality depth returns from glass doors in the training data.
<img alt="img-14.jpeg" src="img-14.jpeg" />
(b) From the initial observation, Pathdreamer is able to recreate the scene from multiple perspectives: (1) the middle of the stairs at 1.5 m , (2) the bottom of the stairs at 4.4 m , and (3) from the corner of the room at 6.5 m . The ability to accurately representing geometry is one of its strengths as a world model.
<img alt="img-15.jpeg" src="img-15.jpeg" />
(c) In this example, the outdoor scenery is accurately recreated due to the information from both the segmentation predictions and the projected RGB values. Some textural details are lost in the couch and table at 6.3 m , which suggest the potential to improve results by training a stronger Image Generator.</p>
<p>Figure 9: Selected examples of predicted sequences from the Val-Seen split using one ground truth observation as context.</p>
<p><img alt="img-16.jpeg" src="img-16.jpeg" />
(a) Pathdreamer is able to perform view synthesis for potentially unbounded viewpoint changes. Provided with an observation of the kitchen counter from one angle, realistic views can be synthesized from other perspectives (highlighted at 5.1 m and 7.9 m ).
<img alt="img-17.jpeg" src="img-17.jpeg" />
(b) While Pathdreamer is able to maintain consistency of the generated semantic segmentations in this example, the complexity of the structures (e.g. arched ceiling beams) make RGB generation difficult, and the results are blurred in certain regions.
<img alt="img-18.jpeg" src="img-18.jpeg" />
(c) Pathdreamer is able to handle complex scene geometry, as shown in this example with multiple floors. While the overall scene looks fairly realistic, the Image Generator has some trouble with generating realistic looking stairs (e.g. at 5.5 m ).</p>
<p>Figure 10: Random prediction sequences from the Val-Seen split using one ground truth observation as context.</p>
<p><img alt="img-19.jpeg" src="img-19.jpeg" />
(a) Several plausible scene layouts are predicted for the unknown region of the image on the right (indicated in black in the guidance inputs). Notably, Pathdreamer is able to synthesize a complete set of a bed, cushion, and chair in the second example (noise vector $z_{2}$ ).
<img alt="img-20.jpeg" src="img-20.jpeg" />
(b) Pathdreamer predicts several plausible scenes: (1) a room with a painting on the left wall, (2) a room without a painting, but with a table and chair on the right, and (3) a room without a table or chair, but with stairs leading down.
<img alt="img-21.jpeg" src="img-21.jpeg" />
(c) Several varying but plausible scenes are considered by Pathdreamer: (1) a window with curtains open at the side, (2) a window covered by curtains, and (3) a large floor to ceiling window on the right of the room.</p>
<p>Figure 11: Diverse and semantically plausible predictions can be sampled for scenes containing previously unseen regions. Shown here are several selected examples, each consisting of three alternative room reveals and the corresponding ground truth (from Val-Unseen).</p>
<p><img alt="img-22.jpeg" src="img-22.jpeg" />
(a) A wall is correctly predicted to exist on the right of the image. Varying layouts and decorations for the wall are proposed, which all create valid segmentation and RGB generation results.
<img alt="img-23.jpeg" src="img-23.jpeg" />
(b) In this example, the missing region of the image is correctly predicted to be a hallway. Several plausible variations of the hallway are synthesized, each containing objects that are likely to be present.
<img alt="img-24.jpeg" src="img-24.jpeg" />
(c) This example contains stairs leading to a hallway, which is correctly predicted by the model. Several potential layouts are synthesized, although none of them fully synthesize the entire door frame present in the ground truth example.</p>
<p>Figure 12: Random examples of generation results when sampled with different noise vectors.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://youtu.be/StklIENGqs0&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ We used https://github.com/mseitzer/pytorch-fid for computing results.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-337 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-337</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-337</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-252280732</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2209.07000v1.pdf" target="_blank">V I P HY : Probing “Visible” Physical Commonsense Knowledge</a></p>
                <p><strong>Paper Abstract:</strong> ,</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e337.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e337.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CapBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Caption-pretrained BERT (CapBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-base language model further pretrained on image captions (COCO, CC3M, Visual Genome) to concentrate on visual-language distributional statistics; used as a diagnostic unimodal baseline to evaluate how much visible physical commonsense can be encoded in text-only weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CapBERT (caption-pretrained BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BERT-base architecture pretrained further on ~4M image captions from COCO, CC3M and Visual Genome to adapt its parameterized knowledge toward visual-linguistic distributions; no visual input at inference unless provided by prompt text.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>VIPHY probing (color / size / spatial)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Textual probing tasks derived from images that test visually-accessible commonsense (color labels, pairwise size relations, and allocentric relative-elevation spatial relations) by mapping aggregated image-derived labels to natural-language prompts and evaluating model predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-relational + spatial (probing of commonsense about objects and their spatial relations)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + spatial</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on image captions (language-only), fine-tuning on VIPHY probe splits when applicable</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting and finetuning linear probes on frozen encoder representations; multi-label prompting for color and spatial, binary label classification for size</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit in model weights as distributional language statistics (natural-language descriptions of visual scenes); no explicit symbolic spatial map is constructed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Relaxed Accuracy (R-Acc), True Confidence (Conf), macro F1 (for multi-label tasks); accuracy for size relations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Size task (finetuned linear probe): standard=83.69% acc, subtype=79.14% acc, transitive=91.82% acc (Table 6). Color task (finetuned): R-Acc ≈ 62.34% (Table 4). Spatial task: outperforms many VLMs on spatial probes but remains below human performance (human spatial R-Acc reported ~88.24%).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Strong at encoding and generalizing object-relational size knowledge, including transitive size inference (high transitive accuracy); robust to contextual subtype noise relative to VLMs; retains relational/statistical priors about typical object sizes learned from captions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Weaker on purely perceptual attributes like color compared to image-conditioned VLMs (lower color R-Acc); does not leverage direct sensory cues so struggles where visual salience or multi-modal perceptual statistics are critical; spatial predictions still below human levels.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to LM average (BERT/RoBERTa/DeBERTa/UnifiedQA) and VLM average (VisualBERT/ViLT/FLAVA/CLIP): CapBERT size performance (83.69%) > VLM average (79.15%) > LM average (69.37%); human size = 90.12% (Table 6). On color, CLIP (VLM) scored higher (~79.96% R-Acc) than CapBERT (~62.34% R-Acc); human (CoDa) = 97.45%.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Finetuning only a linear classifier on frozen encoders was used; few-shot experiments showed CapBERT improves with small amounts of training data (prompt adaptation), and adding contextual subtypes to size labels reduced performance across models, with LMs (including CapBERT) being more robust than VLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A language model pretrained only on image captions (no direct sensory input at probe time) can encode and recall object-relational and spatial size knowledge better than several vision-language models, especially for relational and transitive size inferences, indicating that distributional caption text contains strong object-relational and spatial priors that can be consolidated in unimodal weights.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'V I P HY : Probing “Visible” Physical Commonsense Knowledge', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e337.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e337.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLMs (VisualBERT/ViLT/FLAVA/CLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vision–Language Models (examples: VisualBERT, ViLT, FLAVA, CLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multimodal models pretrained on paired image-text datasets that combine visual encoders and language encoders to ground language in images; evaluated both with and without image-region inputs to probe visible physical commonsense.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VisualBERT / ViLT / FLAVA / CLIP (grouped)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Varied architectures: VisualBERT uses joint multimodal transformer inputs; ViLT uses a light vision-language transformer without region supervision; FLAVA is a foundational multimodal alignment model; CLIP is contrastively pretrained to align image and text embeddings. All are pretrained on large paired image-text corpora enabling visual grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>VIPHY probing (color / size / spatial)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same VIPHY probes — models are evaluated in zero-shot and finetuned settings, sometimes given cropped image regions (e.g., for color predictions) or only textual prompts (to test consolidation/generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-relational + spatial (probing of commonsense derived from images)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational + perceptual (color) — different strengths across attributes</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on paired image-text datasets (image-caption corpora), plus task-specific fine-tuning when applied</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting (text-only prompts or image+text), finetuning linear probes on frozen multimodal encoders, region-cropped queries for attribute extraction (color)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit multimodal embeddings combining visual features, positional encodings (bounding boxes / patch positions), and language token embeddings; no explicit symbolic spatial map — spatial relations inferred by depth/position encodings and learned cross-modal correspondences</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Relaxed Accuracy (R-Acc), True Confidence (Conf), macro F1, accuracy for size relations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Size task (finetuned VLM average): standard=79.15% acc, subtype=64.35% acc, transitive=79.53% acc (Table 6). Color task: best VLM (CLIP) R-Acc ≈ 79.96% while other VLMs vary (VisualBERT lower). Overall VLMs outperform basic LMs on color but underperform or are comparable on size/spatial compared to caption-pretrained LM (CapBERT).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Good at perceptual attribute prediction (color) when provided visual crop inputs (CLIP and other VLMs outperform unimodal LMs on color); VLMs can ground typical visual attributes from paired image-text statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Struggle to consolidate and recall relational size and allocentric spatial knowledge compared to the caption-pretrained LM — positional encodings / visual inputs as used by these VLMs are insufficient for robust storage of generalized size/spatial commonsense; performance drops substantially when contextual subtypes are introduced for size; poorer transitive generalization than CapBERT.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to CapBERT and unimodal LM averages: VLMs lead on color (e.g., CLIP ~79.96% R-Acc vs CapBERT ~62.34%), but on size/spatial CapBERT > VLM average; humans substantially outperform all models (e.g., human size = 90.12%).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Freezing encoder weights and training only a linear probe was the primary evaluation; few-shot finetuning shows VLMs improve, indicating reliance on prompt/task adaptation; incorporating contextual subtypes harms VLM performance more than LM performance, suggesting weaker abstract contextual consolidation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Although VLMs have direct sensory pretraining and perform well on perceptual attributes (color) when given image inputs, they under-consolidate relational spatial and size knowledge compared to a caption-pretrained LM; positional/visual encodings used by VLMs appear insufficient for robust, generalizable spatial/object-relational memory absent continued task-specific supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'V I P HY : Probing “Visible” Physical Commonsense Knowledge', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e337.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e337.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIPHY</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VIPHY: Visible Physical Commonsense Dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale, automatically constructed dataset and probing suite that extracts 'visible' physical commonsense (color, size, spatial relations) from images using subtype selection, monocular depth estimation, and aggregation to produce textual probes for models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model — a dataset and pipeline: parses dense captions to get object names/subtype candidates, selects subtypes using a vision-language selection module (UniCL), computes depth maps with DPT, extracts color via OFA prompts on cropped regions, clusters objects by area×depth for perceived size, and derives relative-elevation spatial relations by partitioning by depth and using centroid/lowest-point rules.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>VIPHY visible-commonsense probing tasks (color, size, spatial)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Probing tasks designed to measure the consolidated visual commonsense knowledge in language and vision-language models: (1) color (multi-label over 11 basic colors), (2) size (pairwise smaller/larger relations aggregated across images and clusters), (3) spatial (allocentric relative-elevation relations {above, below, similar level} conditioned on scene type).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-relational + spatial probing tasks (used to evaluate models' internal knowledge without direct image access in many probes)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational + perceptual</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>extracted from Visual Genome and ADE20K images, augmented with ConceptNet hyponyms and captions; depth from DPT, color predictions from OFA, subtype selection from UniCL</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>dataset used to elicit knowledge via textual prompts (zero-shot/fine-tune) to LMs and VLMs; also used image-region queries for some VLM color probes</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>aggregated image-derived labels and relations represented as textual labels and probe sets (multi-label color sets, pairwise size relations, discrete spatial relations), not as explicit geometric maps inside models</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Relaxed Accuracy (R-Acc), True Confidence (Conf), macro F1, accuracy for size relations; human benchmarks collected for upper bounds</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Provides the evaluation benchmark rather than a single model result; human upper bounds reported (color ~97.45 R-Acc, size ~90.12 acc, spatial ~88.24 R-Acc). Models exhibit large gaps below human on all three dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Dataset enabled finding that caption-pretrained LMs consolidate relational size/spatial priors well and that VLMs are strong at perceptual color when given image crops.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Limitations include reliance on monocular depth (DPT) and assumed camera orientation (orthogonal to ground), potential noise from subtype-selection and caption-derived candidates, and dataset biases from source images (geographic and reporting biases).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used as common benchmark to compare LMs (BERT, RoBERTa, DeBERTa, UnifiedQA), caption-pretrained CapBERT, and VLMs (VisualBERT/ViLT/FLAVA/CLIP). Revealed CapBERT > VLMs on size/spatial, VLMs > LMs on color in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Analyses included: effect of adding contextual subtypes (degrades performance especially for VLMs), transitive evaluation set for size (shows CapBERT strong transitive generalization), and few-shot experiments (showing prompt adaptation benefits for VLMs and CapBERT).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A targeted image-derived probing resource shows that textual pretraining on captions can encode durable object-relational and allocentric spatial priors that transfer to textual probing without images, while VLMs — despite visual access during pretraining — may not consolidate relational/spatial knowledge as effectively; positional encodings and visual inputs alone are insufficient for robust, generalizable spatial/object-relational memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'V I P HY : Probing “Visible” Physical Commonsense Knowledge', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>Experience grounds language <em>(Rating: 2)</em></li>
                <li>Things not written in text: Exploring spatial commonsense from visual signals <em>(Rating: 2)</em></li>
                <li>The world of an octopus: How reporting bias influences a language model's perception of color <em>(Rating: 1)</em></li>
                <li>Language models as zero-shot planners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-337",
    "paper_id": "paper-252280732",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "CapBERT",
            "name_full": "Caption-pretrained BERT (CapBERT)",
            "brief_description": "A BERT-base language model further pretrained on image captions (COCO, CC3M, Visual Genome) to concentrate on visual-language distributional statistics; used as a diagnostic unimodal baseline to evaluate how much visible physical commonsense can be encoded in text-only weights.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CapBERT (caption-pretrained BERT)",
            "model_size": null,
            "model_description": "BERT-base architecture pretrained further on ~4M image captions from COCO, CC3M and Visual Genome to adapt its parameterized knowledge toward visual-linguistic distributions; no visual input at inference unless provided by prompt text.",
            "task_name": "VIPHY probing (color / size / spatial)",
            "task_description": "Textual probing tasks derived from images that test visually-accessible commonsense (color labels, pairwise size relations, and allocentric relative-elevation spatial relations) by mapping aggregated image-derived labels to natural-language prompts and evaluating model predictions.",
            "task_type": "object-relational + spatial (probing of commonsense about objects and their spatial relations)",
            "knowledge_type": "object-relational + spatial",
            "knowledge_source": "pre-training on image captions (language-only), fine-tuning on VIPHY probe splits when applicable",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot prompting and finetuning linear probes on frozen encoder representations; multi-label prompting for color and spatial, binary label classification for size",
            "knowledge_representation": "implicit in model weights as distributional language statistics (natural-language descriptions of visual scenes); no explicit symbolic spatial map is constructed",
            "performance_metric": "Relaxed Accuracy (R-Acc), True Confidence (Conf), macro F1 (for multi-label tasks); accuracy for size relations",
            "performance_result": "Size task (finetuned linear probe): standard=83.69% acc, subtype=79.14% acc, transitive=91.82% acc (Table 6). Color task (finetuned): R-Acc ≈ 62.34% (Table 4). Spatial task: outperforms many VLMs on spatial probes but remains below human performance (human spatial R-Acc reported ~88.24%).",
            "success_patterns": "Strong at encoding and generalizing object-relational size knowledge, including transitive size inference (high transitive accuracy); robust to contextual subtype noise relative to VLMs; retains relational/statistical priors about typical object sizes learned from captions.",
            "failure_patterns": "Weaker on purely perceptual attributes like color compared to image-conditioned VLMs (lower color R-Acc); does not leverage direct sensory cues so struggles where visual salience or multi-modal perceptual statistics are critical; spatial predictions still below human levels.",
            "baseline_comparison": "Compared to LM average (BERT/RoBERTa/DeBERTa/UnifiedQA) and VLM average (VisualBERT/ViLT/FLAVA/CLIP): CapBERT size performance (83.69%) &gt; VLM average (79.15%) &gt; LM average (69.37%); human size = 90.12% (Table 6). On color, CLIP (VLM) scored higher (~79.96% R-Acc) than CapBERT (~62.34% R-Acc); human (CoDa) = 97.45%.",
            "ablation_results": "Finetuning only a linear classifier on frozen encoders was used; few-shot experiments showed CapBERT improves with small amounts of training data (prompt adaptation), and adding contextual subtypes to size labels reduced performance across models, with LMs (including CapBERT) being more robust than VLMs.",
            "key_findings": "A language model pretrained only on image captions (no direct sensory input at probe time) can encode and recall object-relational and spatial size knowledge better than several vision-language models, especially for relational and transitive size inferences, indicating that distributional caption text contains strong object-relational and spatial priors that can be consolidated in unimodal weights.",
            "uuid": "e337.0",
            "source_info": {
                "paper_title": "V I P HY : Probing “Visible” Physical Commonsense Knowledge",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "VLMs (VisualBERT/ViLT/FLAVA/CLIP)",
            "name_full": "Vision–Language Models (examples: VisualBERT, ViLT, FLAVA, CLIP)",
            "brief_description": "Multimodal models pretrained on paired image-text datasets that combine visual encoders and language encoders to ground language in images; evaluated both with and without image-region inputs to probe visible physical commonsense.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "VisualBERT / ViLT / FLAVA / CLIP (grouped)",
            "model_size": null,
            "model_description": "Varied architectures: VisualBERT uses joint multimodal transformer inputs; ViLT uses a light vision-language transformer without region supervision; FLAVA is a foundational multimodal alignment model; CLIP is contrastively pretrained to align image and text embeddings. All are pretrained on large paired image-text corpora enabling visual grounding.",
            "task_name": "VIPHY probing (color / size / spatial)",
            "task_description": "Same VIPHY probes — models are evaluated in zero-shot and finetuned settings, sometimes given cropped image regions (e.g., for color predictions) or only textual prompts (to test consolidation/generalization).",
            "task_type": "object-relational + spatial (probing of commonsense derived from images)",
            "knowledge_type": "spatial + object-relational + perceptual (color) — different strengths across attributes",
            "knowledge_source": "pre-training on paired image-text datasets (image-caption corpora), plus task-specific fine-tuning when applied",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot prompting (text-only prompts or image+text), finetuning linear probes on frozen multimodal encoders, region-cropped queries for attribute extraction (color)",
            "knowledge_representation": "implicit multimodal embeddings combining visual features, positional encodings (bounding boxes / patch positions), and language token embeddings; no explicit symbolic spatial map — spatial relations inferred by depth/position encodings and learned cross-modal correspondences",
            "performance_metric": "Relaxed Accuracy (R-Acc), True Confidence (Conf), macro F1, accuracy for size relations",
            "performance_result": "Size task (finetuned VLM average): standard=79.15% acc, subtype=64.35% acc, transitive=79.53% acc (Table 6). Color task: best VLM (CLIP) R-Acc ≈ 79.96% while other VLMs vary (VisualBERT lower). Overall VLMs outperform basic LMs on color but underperform or are comparable on size/spatial compared to caption-pretrained LM (CapBERT).",
            "success_patterns": "Good at perceptual attribute prediction (color) when provided visual crop inputs (CLIP and other VLMs outperform unimodal LMs on color); VLMs can ground typical visual attributes from paired image-text statistics.",
            "failure_patterns": "Struggle to consolidate and recall relational size and allocentric spatial knowledge compared to the caption-pretrained LM — positional encodings / visual inputs as used by these VLMs are insufficient for robust storage of generalized size/spatial commonsense; performance drops substantially when contextual subtypes are introduced for size; poorer transitive generalization than CapBERT.",
            "baseline_comparison": "Compared to CapBERT and unimodal LM averages: VLMs lead on color (e.g., CLIP ~79.96% R-Acc vs CapBERT ~62.34%), but on size/spatial CapBERT &gt; VLM average; humans substantially outperform all models (e.g., human size = 90.12%).",
            "ablation_results": "Freezing encoder weights and training only a linear probe was the primary evaluation; few-shot finetuning shows VLMs improve, indicating reliance on prompt/task adaptation; incorporating contextual subtypes harms VLM performance more than LM performance, suggesting weaker abstract contextual consolidation.",
            "key_findings": "Although VLMs have direct sensory pretraining and perform well on perceptual attributes (color) when given image inputs, they under-consolidate relational spatial and size knowledge compared to a caption-pretrained LM; positional/visual encodings used by VLMs appear insufficient for robust, generalizable spatial/object-relational memory absent continued task-specific supervision.",
            "uuid": "e337.1",
            "source_info": {
                "paper_title": "V I P HY : Probing “Visible” Physical Commonsense Knowledge",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "VIPHY",
            "name_full": "VIPHY: Visible Physical Commonsense Dataset",
            "brief_description": "A large-scale, automatically constructed dataset and probing suite that extracts 'visible' physical commonsense (color, size, spatial relations) from images using subtype selection, monocular depth estimation, and aggregation to produce textual probes for models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "model_description": "Not a model — a dataset and pipeline: parses dense captions to get object names/subtype candidates, selects subtypes using a vision-language selection module (UniCL), computes depth maps with DPT, extracts color via OFA prompts on cropped regions, clusters objects by area×depth for perceived size, and derives relative-elevation spatial relations by partitioning by depth and using centroid/lowest-point rules.",
            "task_name": "VIPHY visible-commonsense probing tasks (color, size, spatial)",
            "task_description": "Probing tasks designed to measure the consolidated visual commonsense knowledge in language and vision-language models: (1) color (multi-label over 11 basic colors), (2) size (pairwise smaller/larger relations aggregated across images and clusters), (3) spatial (allocentric relative-elevation relations {above, below, similar level} conditioned on scene type).",
            "task_type": "object-relational + spatial probing tasks (used to evaluate models' internal knowledge without direct image access in many probes)",
            "knowledge_type": "spatial + object-relational + perceptual",
            "knowledge_source": "extracted from Visual Genome and ADE20K images, augmented with ConceptNet hyponyms and captions; depth from DPT, color predictions from OFA, subtype selection from UniCL",
            "has_direct_sensory_input": null,
            "elicitation_method": "dataset used to elicit knowledge via textual prompts (zero-shot/fine-tune) to LMs and VLMs; also used image-region queries for some VLM color probes",
            "knowledge_representation": "aggregated image-derived labels and relations represented as textual labels and probe sets (multi-label color sets, pairwise size relations, discrete spatial relations), not as explicit geometric maps inside models",
            "performance_metric": "Relaxed Accuracy (R-Acc), True Confidence (Conf), macro F1, accuracy for size relations; human benchmarks collected for upper bounds",
            "performance_result": "Provides the evaluation benchmark rather than a single model result; human upper bounds reported (color ~97.45 R-Acc, size ~90.12 acc, spatial ~88.24 R-Acc). Models exhibit large gaps below human on all three dimensions.",
            "success_patterns": "Dataset enabled finding that caption-pretrained LMs consolidate relational size/spatial priors well and that VLMs are strong at perceptual color when given image crops.",
            "failure_patterns": "Limitations include reliance on monocular depth (DPT) and assumed camera orientation (orthogonal to ground), potential noise from subtype-selection and caption-derived candidates, and dataset biases from source images (geographic and reporting biases).",
            "baseline_comparison": "Used as common benchmark to compare LMs (BERT, RoBERTa, DeBERTa, UnifiedQA), caption-pretrained CapBERT, and VLMs (VisualBERT/ViLT/FLAVA/CLIP). Revealed CapBERT &gt; VLMs on size/spatial, VLMs &gt; LMs on color in many cases.",
            "ablation_results": "Analyses included: effect of adding contextual subtypes (degrades performance especially for VLMs), transitive evaluation set for size (shows CapBERT strong transitive generalization), and few-shot experiments (showing prompt adaptation benefits for VLMs and CapBERT).",
            "key_findings": "A targeted image-derived probing resource shows that textual pretraining on captions can encode durable object-relational and allocentric spatial priors that transfer to textual probing without images, while VLMs — despite visual access during pretraining — may not consolidate relational/spatial knowledge as effectively; positional encodings and visual inputs alone are insufficient for robust, generalizable spatial/object-relational memory.",
            "uuid": "e337.2",
            "source_info": {
                "paper_title": "V I P HY : Probing “Visible” Physical Commonsense Knowledge",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2,
            "sanitized_title": "learning_transferable_visual_models_from_natural_language_supervision"
        },
        {
            "paper_title": "Experience grounds language",
            "rating": 2,
            "sanitized_title": "experience_grounds_language"
        },
        {
            "paper_title": "Things not written in text: Exploring spatial commonsense from visual signals",
            "rating": 2,
            "sanitized_title": "things_not_written_in_text_exploring_spatial_commonsense_from_visual_signals"
        },
        {
            "paper_title": "The world of an octopus: How reporting bias influences a language model's perception of color",
            "rating": 1,
            "sanitized_title": "the_world_of_an_octopus_how_reporting_bias_influences_a_language_models_perception_of_color"
        },
        {
            "paper_title": "Language models as zero-shot planners",
            "rating": 1,
            "sanitized_title": "language_models_as_zeroshot_planners"
        }
    ],
    "cost": 0.01455575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>VIPHY: Probing "Visible" Physical Commonsense Knowledge</p>
<p>Shikhar Singh ssingh43@usc.edu 
University of Southern California</p>
<p>Ehsan Qasemi 
University of Southern California</p>
<p>Muhao Chen 
University of Southern California</p>
<p>VIPHY: Probing "Visible" Physical Commonsense Knowledge
631E3164BF709DE5FF973A7455E15174Intra-cluster relations Cluster by Size: Inter-cluster relations ImageDepth[Object / Subtype] Query VLM: cropped image "ball" → "football" Source Data: Image[Object: Name-Box] KB: Subtype Candidates
Vision-language models (VLMs) have shown remarkable performance on visual reasoning tasks (e.g.attributes, location).While such tasks measure the requisite knowledge to ground and reason over a given visual instance, they do not, however, measure the ability of VLMs to retain and generalize such knowledge.In this work, we evaluate VLMs' ability to acquire "visible" physical knowledge -the information that is easily accessible from images of static scenes, particularly along the dimensions of object color, size, and space.We build an automatic pipeline to derive a comprehensive knowledge resource for calibrating and probing these models.Our results indicate a severe gap between model and human performance across all three dimensions.Furthermore, we demonstrate that an LM tuned on the captions significantly outperforms VLMs on both size and spatial tasks -highlighting that despite sufficient access to ground language with visual modality, they struggle to retain such knowledge.The dataset and code are available at https: //github.com/luka-group/ViPhy.</p>
<p>Introduction</p>
<p>The ability to reason and acquire knowledge from experience, while being intuitive for humans, has been a long-standing challenge for AI agents (Mc-Carthy et al., 1960).Examples such as the color of grass, or the relative position of monitor and table, are formally regarded as commonsense knowledge (Chi, 2005).The retention of such knowledge in humans is achievable due to the presence of long-term memory, broadly classified into episodic and semantic memory (Tulving, 1972;Camina and Güell, 2017).While the former stores the information pertaining to personal events, the latter is geared towards general, decontextualized knowledge. 1Prior studies (Greenberg and Verfaellie, 2010) have acknowledged the interdependency between them, particularly the consolidation of semantic knowledge from episodic memories -aids humans to acquire commonsense from experience.</p>
<p>Pretrained language models (Devlin et al., 2019;Raffel et al., 2020) have demonstrated the capacity to reason (Wang et al., 2019) and retain knowledge (Petroni et al., 2019;Da et al., 2021).Likewise, vision-language models (Lu et al., 2019;Radford et al., 2021) driven by the availability of largescale paired image-text datasets have shown strong performance on visual reasoning tasks (Antol et al., 2015;Chen et al., 2015).While such tasks emphasize the model's ability to draw inferences from a specific visual instance -primarily to ground entities and reason about their attributes and relations, they do not, however, explicitly measure the consolidation of such knowledge. 2In this work, we evaluate the model's ability to generalize aspects of grounding and reasoning tasks, regarded as commonsense knowledge.</p>
<p>Prior works have been largely directed towards probing language models pertaining to object properties such as weight, size, speed, and affor-dance (Forbes and Choi, 2017;Forbes et al., 2019).Drawing upon the notion of world scopes (Bisk et al., 2020a), we find that such datasets, albeit comprehensive across aspects of physical knowledge, are ideally suited for embodied agents capable of interacting with the physical environment.This motivates us to develop resources that better align with the world scope of existing AI systems, primarily vision-language models.</p>
<p>In this work, we introduce VIPHY, a visible physical commonsense dataset designed to probe aspects of physical knowledge that are easily accessible in images of static scenes.Therefore, it can be argued that models pre-trained on such data have sufficient access to the "visible world".We build a large-scale dataset along three dimensions of objects: (1) color, (2) size, and (3) space.In contrast to prior works (Paik et al., 2021;Zhang et al., 2022), we bypass crowdsourced annotations in favor of an automated pipeline to derive a resource spanning 14k objects (30×) from raw images.This is achieved by extracting object subtypes -informed by the visual context in images (e.g.kitchen sink).We leverage image data, along with existing visionlanguage and depth perception models to develop VIPHY.</p>
<p>Beyond scale, we introduce a resource for probing spatial knowledge of common environments.Although one can reason along several types of spatial relations for a visual instance (e.g. a cat behind a laptop; Liu et al. (2022a)), we find that mapping them to commonsense knowledge is nontrivial. 3We define spatial relations by selecting "ground" as the observer and specifying the relative elevation of objects under an allocentric reference frame (Klatzky, 1998).</p>
<p>We probe state-of-the-art models on VIPHY, and find a significant gap across all three dimensions, compared to human performance.Previous works (Paik et al., 2021;Liu et al., 2022b) have corroborated the improvements from language grounding towards acquiring visual knowledgeour results, however, show a more nuanced picture.While VLMs fare much better than LMs on recalling colors, the caption pretrained baseline (Zhang et al., 2022) significantly outperforms VLMs on both size and spatial inference tasks.This highlights that despite access to visual modality, existing VLMs struggle to effectively consolidate such 3 Due to challenges in specifying the reference frame of the observer, the canonical pose of the objects, and the situational nature of a scene.knowledge.</p>
<p>The contributions of this work can be summarized as follows: (1) We build a comprehensive dataset, covering multiple aspects of visually accessible knowledge ( §3), which is developed through an automated pipeline to derive high-quality resources from images at scale ( §2). ( 2) We conduct extensive benchmarking across several state-of-theart language and vision-language models and find significant gaps in human performance ( §4). ( 3) We demonstrate a baseline tuned on the caption that significantly outperforms its vision-language counterparts -highlighting that despite access to images, VLMs struggle to consolidate such knowledge.</p>
<p>Pipeline</p>
<p>We provide a conceptual overview of our pipeline for developing VIPHY, as illustrated in Fig. 3.During the preprocessing stage, we build an internal database comprising object names and corresponding subtype candidates.Given image and object regions as input4 , we substitute object names with their subtypes ( §2.1), and compute the corresponding depth map.The processed data is used to extract color, size and spatial knowledge ( §2.2).</p>
<p>Object Subtype</p>
<p>While object recognition datasets consider a wide range of objects, such tasks do not necessitate finegrained categories (Zou et al., 2019).However, object subtypes inform attributes such as color, and help contextualize objects in absence of visual signals (e.g. office chair).Although subtypes are generally accessible from knowledge bases (KB), their coverage is often limited. 5We extend this definition to include objects defined by visual context -indicating event, location, state, part, etc. (Appendix Tab.9).For subtype collection, we parse captions to build a set of object names.We then employ suffix-based lexical matching to derive subtypes for each object, and merge with hyponyms from knowledge base.The resulting data represents a mapping between the object name and its candidate subtypes.</p>
<p>As our goal is to derive object attributes and relations directly from images, we design a subtype selection module to annotate the source image regions with the best subtype.This is required since human annotators often abstract the object name to avoid redundancy when presented with visual context (example in Appendix Fig. 12) -congruent with the maxim of quantity (Grice, 1975).Likewise, existing object detectors are not suited for open-vocabulary and fine-grained classification (Minderer et al., 2022).</p>
<p>The module is designed to query from subtype 5 We report ~60% object name overlap between Concept-Net KB (Speer et al., 2017) and our collection -derived from dense captions in Visual Genome (Krishna et al., 2017).candidates using visual features.It employs a twostage approach to filter candidates using image context, and select the best subtype with region-level features, as illustrated in Fig. 2. The visual and textual inputs are embedded using a dual stream vision-language model.Formally, given the visual feature of the image I, textual features of the object o and subtype candidates C o , we extract the appropriate subtype as follows:
C = {c|c ∈ C o , sim(c, I) &gt; sim(o, I)} ∪ o
Here, sim(.) is the cosine similarity.Intuitively, since the object name is independent of visual context, it serves as an anchor for excluding subtypes that do not align with the contextual cues.In the next stage, we incorporate visual features of the object region o v , to query from filtered candidate set C, and compute the best subtype s o :
s o = arg max c∈C sim(o v , c)
The preprocessed dataset comprises object-subtype mapping for every bounding box region in the image.</p>
<p>Knowledge Extraction</p>
<p>Given the image and depth map, along with objectsubtype region annotations, we independently extract color, size and spatial knowledge.</p>
<p>Color Prior works (Paik et al., 2021) have relied on human annotations to acquire the color distribution of objects instead of inferring color from pixel values due to challenges such as lighting, shadow, segmentation, etc.However, we argue that large-scale availability of images can mitigate potential noise associated with automated extraction.Given the ubiquity of color attribute in visual reasoning tasks (Antol et al., 2015;Hudson and Manning, 2019), we find that VLMs pretrained on such datasets are reliable for inferring color from images.As object localization is decoupled from attribute recognition in the pipeline, the input to the VLM is simply the cropped image region, queried with a predefined textual prompt (detailed in §3.1).</p>
<p>Size To derive size relations, we consider cooccurring objects in a scene.As objects in an image are expected to appear at varying depths, we approximate perceived size by including scene depth.Given an image, depth map and object-region annotations as inputs, the objects are clustered by sizedefined as the bounding box area scaled by mean depth of the region.The sorted partitions are then used to derive inter-cluster relations.The object pair relations are aggregated across images.The number of clusters are fixed for all instances.</p>
<p>Spatial We define spatial knowledge as the relative elevation between objects, for a given scene type.To infer these relations directly from image, however, is challenging as perspective projection of 3D world distorts the relative elevation due to variation in depth.We discount this distortion by partitioning the image by depth, and compute intracluster object relations, i.e. we discard the depth coordinate of objects that belong to the same cluster, and simply compare the relative elevation.The inter-cluster relations are derived transitively via overlapping partitions -defined by objects with dual membership, as illustrated in Fig. 4. The spatial relations are aggregated across all images for a given scene type.We detail the specifics of mapping object annotations to spatial relations in Appendix A.3.</p>
<p>Dataset</p>
<p>This section details the specific data sources and models used to develop VIPHY ( §3.1).We also report the dataset statistics and task format for each dimension ( §3.2).Additional parameters related to dataset construction are provided in Appendix A.1.</p>
<p>Construction</p>
<p>Sources We leverage two datasets: (1) Visual Genome (Krishna et al., 2017), and (2) ADE20K (Zhou et al., 2017).The dense captions in Visual Genome provide a broad coverage of object classes, making it a suitable resource for collecting subtype candidates.For extracting hyponyms from knowledge base, we acquire "is-a" relations from ConceptNet (Speer et al., 2017), and augment the subtype candidate set.We extract spatial relations from ADE20K, as it provides images categorized by scene type -primarily indoor environments with high object density: {bedroom, bathroom, kitchen, living room, office}.</p>
<p>Models To collect subtype candidates (as detailed in §2.1), we perform part-of-speech tagging to extract object names (noun) from caption data, using LSTM-CRF (Akbik et al., 2018).Our subtype selection module is instantiated with UniCL (Yang et al., 2022) -designed for discriminative representations and broad semantic coverage of entities.To compute depth map from monocular image, we use DPT (Ranftl et al., 2021).To infer object color from image region, we query OFA (Wang et al., 2022), using the prompt template: "what color is the <object>?",and map zeroshot predictions to basic color set (Berlin and Kay, 1991) as detailed in Appendix A.4.</p>
<p>Task Summary</p>
<p>Table 1 summarizes statistics for VIPHY, comprising the number of objects, classes and instances for each dimension.For multi-label tasks, we report the label cardinality, i.e. number of labels for a sample.We also indicate the number of objects with subtypes and subtype cardinality6 .Note that while we extract size relations and color attributes from same source images ( §3.1), we ignore contextual subtypes for size relations, as they serve a limited role towards informing object size (e.g.rain coat).However, as we only consider co-occurring objects, we implicitly incorporate context for objects in comparison, i.e. help disambiguate word sense.We collect 7.1k smaller and 14.5k larger relations, and balance labels by including their complements.</p>
<p>The label distributions of color dataset is provided in Appendix Fig. 10.</p>
<p>Formulation The objective of VIPHY tasks is to measure the ability to generalize physical knowledge pertaining to objects.To probe models with textual prompts, we map the raw distribution of labels (acquired by our pipeline) to typical values, as detailed in Appendix A.2.In the color task, objects can have multiple labels from the set of 11 basic colors as defined in Berlin and Kay (1991): {red, orange, yellow, brown, green, blue, purple, pink, white, gray, black}.Likewise, we consider multiple labels for spatial relations from {below, above, similar level}, conditioned on a scene type as mention in §3.1.Lastly, size relations are mapped to a single label from {smaller, larger}.</p>
<p>Experiments</p>
<p>We evaluate several state-of-the-art models under zero-shot and finetune settings ( §4.1), and conduct further analysis of model performance ( §4.2).The datasets are partitioned into 20% train, 10% dev,</p>
<p>• LMs: BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), DeBERTa (He et al., 2020) and UnifiedQA (Khashabi et al., 2020).</p>
<p>• VLMs: VisualBERT (Li et al., 2019), ViLT (Kim et al., 2021), CLIP (Radford et al., 2021) and FLAVA (Singh et al., 2022).</p>
<p>CapBERT In addition to the aforementioned baselines, we explore to what degree does an LM pretrained only on image captions, encodes visual knowledge.Such a model effectively serves as a diagnostic baseline -ablating the grounding mechanism in VLMs.We build CapBERT by pretraining BERT base7 on captions (4M) from COCO (Chen et al., 2015), CC3M (Sharma et al., 2018) and VG (Krishna et al., 2017) datasets.While a contemporaneous work by Zhang et al. (2022) developed a similar baseline, we find limited comparisons on color and no evaluation on their size dataset.Through extensive benchmarking, we are able to derive novel insights with CapBERT ( §4.1).</p>
<p>Finetuning To evaluate under finetune setting, we train a linear classifier on top of the model's output, while rest of the weights are frozen.We use Softmax Cross Entropy loss for single and multilabel setups, following Mahajan et al. (2018).All probes are finetuned for 50 epochs, with batch size of 8, using Adam optimizer (Kingma and Ba, 2015) and a learning rate of 10 −4 .</p>
<p>Prompts For probing LMs and VLMs, we provide manually designed textual prompt as input to the model.The prompt templates for probing across color, size and spatial tasks, under zero-shot (ZS) and finetune (FT) settings are given in Table 2.Besides models trained on the masked language objective8 , the question-answering baseline (Uni-fiedQA) follows a common template9 for both ZS and FT settings.</p>
<p>Metrics We introduce the following metrics for measuring multi-label task performance:  • Relaxed Accuracy (R-Acc) -The prediction (P i ) is accurate if the most probable label (l i ) belongs to the set of ground-truth labels (T i ).
RA = i∈D [l i ∩ T i ] ∧ [l i = arg max l P i (l)] |D| • True Confidence (Conf) -
The sum of predicted probabilities for labels in the ground-truth set.
C = i∈D l∈T i P i (l) |D|
Here, D denotes samples in the evaluation set.In addition to the aforementioned metrics, we also report the macro-averaged F1-score (F1).</p>
<p>Human Performance To provide an upper bound on VIPHY tasks, we use CoDa (Paik et al., 2021) for color -computed over 432 overlapping objects. 10For size and spatial tasks, we evaluate 100 relations with three external annotators11 (crowdsourced) and report the average scores.</p>
<p>Results</p>
<p>Zero-Shot We report zero-shot performance using R-Acc metric, across all tasks in Table 3.For spatial task, we only consider two labels from {above, below}, due to the limitation of single word masking in selected baselines.We observe significant variations in model performance across tasks, with VLMs (VisualBERT) performing worse than their LM counterparts -underscoring the challenges of manual prompting (Jiang et al., 2020).The best scoring baseline (UnifiedQA) falls at least 30% points below human scores.Finetune When compared to zero-shot results, we report improved calibration under finetuned probing, as evident from results on color (Tab.4), size (Tab.6) and spatial tasks (Tab.5).We find that VLMs score higher than LMs -specifically their "caption-only" counterpart (CapBERT) on the color task.These results hint at the role of color attribute in grounding entities.However, CapBERT outperforms VLMs on both size and spatial tasks, indicating that despite access to visual representations, VLMs do not retain such relational knowledge as effectively.In particular, it highlights that position encoded visual inputs in VLMs remain insufficient towards consolidating spatial knowledge.Lastly, CapBERT outperforming other LMs is likely due to the domain similarity between the pretraining source and the evaluation tasks12 .</p>
<p>Analysis</p>
<p>Color: Cardinality We further analyze model performance with respect to label cardinality (i.e.number of ground-truth colors for an object), by grouping objects accordingly.As shown in Fig. 5, we report results for three baselines, their average, along with human scores.While the performance is expected to increase with the cardinality13 , we notice an inconsistency between model and human scores.In particular, while difference in overall confidence scores (as inferred from Table 4) for human and the model average is ~18%, the relative differences between the two -ranges from ~12% (x = 6) to ~40% (x = 1), where x-axis denotes the label cardinality.While color influences object    perception in humans (Gegenfurtner and Rieger, 2000), these results show that VLMs do not ascribe a similar degree of saliency to color, especially for uni-color objects (i.e.cardinality of one).</p>
<p>Color: Category-wise To conduct error analysis on color task, we define categories by grouping ob- jects that belong to a common entity.We manually select category names from the object ontology provided in Open Images dataset (Kuznetsova et al., 2020).We assign each object to a category by computing the most similar category.We use Sentence-BERT (Reimers and Gurevych, 2019) and thus artificially convert each category to a sentence by concatenating the category name with few objectsserving as semantic cues (Appendix Tab. 8).We report category-wise performance (R-Acc) for BERT and CLIP, as provided in Fig. 6.We observe significant differences in performance between the two baselines, across categories.For animal and nature categories, BERT performs poorly with respect to the mean score.This difference widens for food, hinting at the effects of reporting bias as their typical colors are less likely to be written in text.</p>
<p>The strong performance of CLIP for this category, however, indicates that such under reporting can be mitigated by visual modality.In contrast, for categories such as electronics and vehicle, we observe that LMs perform well -likely because color often plays a role in describing objects such as gadgets and cars.</p>
<p>Size: Transitivity As the size dataset14 is composed of frequently co-occurring object pairs, we intend to evaluate model's ability to infer relative size for objects linked transitively across scenes.We build a new evaluation set comprising transitive relations from the standard size dataset, ensuring no overlapping instances between the two.The results (Tab.6) indicate that LMs (on average) improve by significant margin, compared to VLMs.While the improvements on the evaluation set can .be partially attributed to objects on the relatively extreme ends of size clusters being paired up, they are able to generalize on transitive relations.</p>
<p>Size: Subtypes While qualifiers denoting visual context tend to inform the typicality of color attribute, their effect on size is likely inconsequential.Therefore, models should retain their performance with reference to the standard set.We test this hypothesis by creating an evaluation set comprising contextual subtypes for objects in the standard test set.While the addition of subtypes leads to performance drop across all models (Tab.6), we observe that LMs are more robust in comparison to VLMs.</p>
<p>Related Works</p>
<p>Physical Commonsense Recent years have witnessed a renewed interest in commonsense via natural language benchmarks (Talmor et al., 2019;Singh et al., 2021).Specific works have evaluated the language models on their ability to reason about physical commonsense (Bisk et al., 2020b;Qasemi et al., 2022), and identified reporting bias as a potential bottleneck (Forbes et al., 2019;Paik et al., 2021).In this work, we direct our focus towards vision-language models pretrained on large paired image-text datasets, and evaluate them on visually accessible commonsense knowledge.While prior works have probed knowledge pertaining to color (Paik et al., 2021;Mullenbach et al., 2019) and size (Talmor et al., 2020) Vision-Language Resources While image classification (Deng et al., 2009) can be construed as one of the earliest attempts at bridging vision and language, recent years have witnessed a plethora of resources.Visual reasoning tasks have been directed towards object attributes (Antol et al., 2015), activities (Chen et al., 2015), as well as social (Zellers et al., 2019) and temporal commonsense (Fu et al., 2022).Recently, VLMs (Lu et al., 2019;Li et al., 2020;Radford et al., 2021) have demonstrated strong performance on such tasks.These works evaluate the requisite knowledge to reason about a specific instance, VIPHY in contrast probes the knowledge retained in the absence of visual context, i.e. generalized from instances.</p>
<p>Knowledge in LMs Recent advancements in language models (Devlin et al., 2019;Raffel et al., 2020), pretrained on large corpora, has led to significant improvements across several reasoning tasks (Wang et al., 2019).Prior works have also highlighted the capacity of these models to acquire several types of knowledge such as factual (Petroni et al., 2019;Roberts et al., 2020), instructional (Huang et al., 2022) and commonsense (Da et al., 2021).In this work, we study to what degree do their vision-language analogs (VLMs) -driven by the availability of massive paired image-text datasets, retain information that is easily accessible in images.</p>
<p>Conclusion</p>
<p>We present VIPHY, a large scale resource for probing "visible" physical knowledge -information easily accessible from images of static scenes, across dimensions of color, size and space.We design an automated pipeline to extract and consolidate such knowledge facts from images, and introduce a new resource for evaluating spatial knowledge of common environments.Our benchmarking evaluation highlights a huge gap between model and human performance across all three tasks.Furthermore, while prior works have reported VLMs to be more effective, our caption pretrained baseline (CapBERT) significantly outperforms VLMs on the ability to recall size and spatial knowledge.These results underscore that despite access to visual modality, existing VLMs struggle to retain visual knowledge as effectively.</p>
<p>Acknowledgement</p>
<p>We wish to acknowledge Bowen Zhang for his thoughtful comments on our paper.</p>
<p>Ethical Implications</p>
<p>We build VIPHY from existing images from crowdverified visual datasets which have been identified to lack geographical diversity, often limited to scenes from Europe and North America (Shankar et al., 2017).Furthermore, such datasets are subjected to several kinds of biases at different stages of collection and annotation such as selection bias, framing bias and observer bias (Fabbrizzi et al., 2022).Therefore, its likely that such biases will be reflected in our dataset as well.As we also report benchmarking results on VIPHY, the model performance may not be reflected as accurately on knowledge pertaining to different geographical and cultural backgrounds, as studied by Yin et al. (2021).Lastly, our proposed resource is limited to English, and thus excludes any considerations for multilingual models (Yin et al., 2022).</p>
<p>Limitations</p>
<p>We extract spatial knowledge directly from images, we assume that the camera's image plane is somewhat orthogonal to the ground (transverse plane), and do not account for the edge cases where the image plane will be parallel, i.e. top view.For collecting subtype candidates, we parse Visual Genome (Krishna et al., 2017) captions to acquire object names.To assign object subtypes, we rely on pretrained vision-language model (UniCL; Yang et al., 2022) which can be a likely source of noise in the pipeline.Furthermore, quantifying their performance on subtype selection task is beyond the scope of our work, due to unavailability of groundtruth annotations.In contrast to previous works, since we automatically estimate object size from the image -the estimations are limited by the quality of bounding boxes and depth predictions from models.</p>
<p>Figure 1 :
1
Figure 1: We propose VIPHY for probing the ability to generalize visually accessible knowledge -particularly along the dimensions of color, size, and space.</p>
<p>Figure 2 :
2
Figure 2: Subtype Selection Module: Given object o in image I, assigns subtype s o from candidate set C o .</p>
<p>Figure 3 :
3
Figure 3: Pipeline Overview: The preprocessing step computes the image depth map, and re-annotates objects by selecting the best subtype from the set of candidates (KB).The color, size and spatial knowledge are then derived independently.</p>
<p>Figure 4 :
4
Figure 4: Illustrates transitive spatial relation, computed across partitions (ordered by depth).The y-axis denotes elevation, while the z-axis indicates depth.</p>
<p>Figure 5 :
5
Figure 5: Effect of label cardinality (x-axis) on color prediction, as measured by R-Acc and Conf.The Avg curves (black) indicate average model performance.</p>
<p>Figure 6 :
6
Figure 6: Category-wise performance on color (R-Acc).The dashed lines indicate average scores.</p>
<p>Figure 9 :Figure 11 :
911
Figure 9: Few-shot performance on size.</p>
<p>Figure 12 :
12
Figure 12: A sample image and corresponding captions from Visual Genome dataset.Illustrates how humans omit the subtype kitchen sink, when annotating images.</p>
<p>Table 1 :
1
Dataset statistics for VIPHY.The cardinality is reported with mean and standard deviation.
Subtype objects objects with subtype subtype cardinality14k 1.8k 7.73 (12.91)Color objects (instances) classes label cardinality14k 11 2.42 (1.37)Sizeobjects instances relations1.6k 43k 2Spatial objects instances relations label cardinality scenes300 6.5k 3 1.28 (0.45) 5</p>
<p>Table 2 :
2
Prompt templates across tasks and evaluation settings.Here, O, R and S are placeholders for object, relation and scene type respectively.</p>
<p>Table 3 :
3
Zero-shot results (R-Acc) across all tasks.
ModelColorSizeSpatialBERTlarge RoBERTalarge UnifiedQAlarge 51.00 48.39 0.59 VisualBERT 9.0644.61 47.01 51.76 24.9120.96 17.52 63.04 9.57Human97.4590.1288.24</p>
<p>Table 4 :
4
Color results.
ModelR-AccConfF1CapBERT BERTbase RoBERTalarge UnifiedQAlarge 62.34 70.45 66.87 55.95 DeBERTaxxl 72.7458.55 55.59 49.28 -59.5940.91 30.94 20.93 -36.33VisualBERT ViLT FLAVA CLIP66.22 64.83 76.33 79.9650.99 53.92 62.84 65.5024.46 30.27 38.74 49.54Human CoDa97.4578.6568.82ModelR-AccConfF1CapBERT BERTbase RoBERTalarge UnifiedQAlarge 62.04 69.93 67.25 54.88 DeBERTaxxl 62.3062.09 59.91 58.40 -61.2760.78 61.34 58.88 -60.54VisualBERT ViLT FLAVA CLIP63.08 65.78 63.71 65.1058.40 60.28 61.06 63.5658.88 59.80 60.56 62.26Human88.24-81.22</p>
<p>Table 5 :
5
Spatial results.</p>
<p>Table 6 :
6
Size results reported across different evaluation sets, measured by accuracy (random baseline: 50%).
ModelStandardSubtypeTransitiveCapBERT83.6979.1491.82BERTbase RoBERTalarge UnifiedQAlarge 62.20 78.35 65.23 DeBERTaxxl 74.7372.28 57.12 60.66 66.8877.29 69.31 90.78 69.79LMaverage69.3764.2374.54VisualBERT ViLT FLAVA CLIP76.99 78.54 82.67 75.4364.00 57.32 69.54 66.5677.69 86.18 81.78 72.48VLMaverage79.1564.3579.53Human90.12--</p>
<p>Table 8 :
8
Categories and objects used as semantic cues.We map them to sentence with "<Category>: <Objects>" template.</p>
<p>Context Examples Event wedding cake, bathing soap Location kitchen sink, street lamp State bare tree, sliced apple Part piano key, bike wheel</p>
<p>Table 9 :
9
Context-based subtype examples</p>
<p>For instance, the memory of one's birthday cake is episodic, whereas knowing that most birthdays include a cake is part of semantic memory.
Counting bike wheels for different instances is an example of reasoning while generalizing that bikes have two wheels is consolidating across instances.
Available from either manual annotations or object detection/segmentation model (e.g.He et al. (2017)).
Note: The subtype cardinality is only reported for objects with subtype.
70% test set.Baselines We consider the following language (LM) and vision-language (VLM) models:
Note that our VLM baselines follow the base model size. To ensure fair comparisons, we build the base version.
CLIP being the exception, cannot be evaluated under ZS setting. Under FT, it uses EOS instead of CLS token.
The prompt includes all classes as choices.
The label distributions of VIPHY and CoDa are provided in Appendix -Fig.10and Fig.
  11  The inter-annotator agreement measure (Fleiss' Kappa) for size and spatial are 0.85 and 0.78, respectively.
For instance, a prepositional phrase can convey both abstract (on schedule) and physical (on table) relations, with captions predominantly containing the latter.
R-Acc &amp; Conf are 1, when cardinality is 11.
For reference, the #instances for size evaluation sets are as follows -standard: 30k, subtype: 23k, transitive: 20k.
Appendix A Implementation DetailsA.1 Pipeline ParametersTo cluster objects for computing relative size, we use Jenks Natural Breaks(Jenks, 1967), with #clusters = 5 following the manual groupings of object sizes inLiu et al. (2022b).We also experimented with #clusters = 3, but qualitatively observed less optimal clusters.In spatial module, we create 3 partitions of uniform size and overlap.A.2 Typical LabelsWe derive typical labels from the raw label probabilities (C) by filtering classes as per a predefined threshold p min , that can be interpreted as either noise or rare occurrence.Formally, we apply the filter as follows:Here, p min is defined as:The resulting distribution is re-normalized and the filtering step is applied recursively.A.3 Defining Spatial RelationsOur objective is to map the raw coordinates in an image for two objects to discrete relations.We define a simple set of rules to convey above and similar level, from object annotations.Given bounding box or polygon mask, we first compute its centroid along with the lowest point.We then compare the ycoordinates of objects as illustrated in Fig.7.If an object's lowest point is above the other's centroid, we map it to above, else similar level.A.4 Prediction to Basic ColorWe use in our pipeline, and map generated text to the basic color set as shown in Table7.If the model predicts multiple colors, we assign each of them to the object instance.A.5 Few-Shot PerformanceWe report few-shot results for color (Fig.8) and size (Fig.9) task, by building variants of the train set with respect to the percentage of samples.The improvement in VLMs and CapBERT likely indicates prompt adaptation due to their relatively weaker linguistic ability -compared to LMs.
Contextual string embeddings for sequence labeling. Alan Akbik, Duncan Blythe, Roland Vollgraf, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsSanta Fe, New Mexico, USAAssociation for Computational Linguistics2018</p>
<p>VQA: visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, Devi Parikh, 10.1109/ICCV.2015.2792015 IEEE International Conference on Computer Vision, ICCV 2015. Santiago, ChileIEEE Computer Society2015. December 7-13, 2015</p>
<p>Basic color terms: Their universality and evolution. Brent Berlin, Paul Kay, 1991Univ of California Press</p>
<p>Experience grounds language. Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, Joseph Turian, 10.18653/v1/2020.emnlp-main.703Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020a</p>
<p>PIQA: reasoning about physical commonsense in natural language. Yonatan Bisk, Rowan Zellers, Ronan Lebras, Jianfeng Gao, Yejin Choi, The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020b. February 7-12, 20202020The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</p>
<p>The neuroanatomical, neurophysiological and psychological basis of memory: Current models and their origins. Eduardo Camina, Francisco Güell, Frontiers in pharmacology. 84382017</p>
<p>Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, Lawrence Zitnick, arXiv:1504.00325Microsoft coco captions: Data collection and evaluation server. 2015arXiv preprint</p>
<p>Commonsense conceptions of emergent processes: Why some misconceptions are robust. Chi Michelene, The journal of the learning sciences. 1422005</p>
<p>Analyzing commonsense emergence in few-shot knowledge models. Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, Antoine Bosselut, 3rd Conference on Automated Knowledge Base Construction. 2021</p>
<p>Imagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Fei-Fei Li, 10.1109/CVPR.2009.52068482009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Miami, Florida, USAIEEE Computer Society2009. 2009. June 2009</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>A survey on bias in visual datasets. Computer Vision and Image Understanding. Simone Fabbrizzi, Symeon Papadopoulos, Eirini Ntoutsi, Ioannis Kompatsiaris, 2022103552</p>
<p>Verb physics: Relative physical knowledge of actions and objects. Maxwell Forbes, Yejin Choi, 10.18653/v1/P17-1025Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Do neural language representations learn physical commonsense. Maxwell Forbes, Ari Holtzman, Yejin Choi, 2019In CogSci</p>
<p>There's a time and place for reasoning beyond the image. Xingyu Fu, Ben Zhou, Ishaan Chandratreya, Carl Vondrick, Dan Roth, 10.18653/v1/2022.acl-long.81Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Sensory and cognitive contributions of color to the recognition of natural scenes. R Karl, Jochem Gegenfurtner, Rieger, Current Biology. 10132000</p>
<p>Interdependence of episodic and semantic memory: Evidence from neuropsychology. L Daniel, Mieke Greenberg, Verfaellie, Journal of the International Neuropsychological society. 1652010</p>
<p>Logic and conversation. Herbert P Grice, Speech acts. Brill1975</p>
<p>Mask R-CNN. Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross B Girshick, 10.1109/ICCV.2017.322arXiv:2006.03654IEEE International Conference on Computer Vision, ICCV 2017. Jianfeng Liu, Weizhu Gao, Chen, Venice, ItalyIEEE Computer Society2017. October 22-29, 2017. 2020arXiv preprintDeberta: Decoding-enhanced bert with disentangled attention</p>
<p>Language models as zero-shot planners. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, arXiv:2201.07207Extracting actionable knowledge for embodied agents. 2022arXiv preprint</p>
<p>GQA: A new dataset for real-world visual reasoning and compositional question answering. Drew A Hudson, Christopher D Manning, 10.1109/CVPR.2019.00686IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USA2019. June 16-20, 2019Computer Vision Foundation / IEEE</p>
<p>The data model concept in statistical mapping. International yearbook of cartography. Jenks George, 19677</p>
<p>How can we know what language models know? Transactions of the. Zhengbao Jiang, Frank F Xu, 10.1162/tacl_a_00324Jun Araki, and Graham Neubig. 2020Association for Computational Linguistics8</p>
<p>UNIFIEDQA: Crossing format boundaries with a single QA system. Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi, 10.18653/v1/2020.findings-emnlp.171Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Vilt: Vision-and-language transformer without convolution or region supervision. Wonjae Kim, Bokyung Son, Ildoo Kim, International Conference on Machine Learning. PMLR2021</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, 3rd International Conference on Learning Representations, ICLR 2015. San Diego, CA, USA2015. May 7-9, 2015Conference Track Proceedings</p>
<p>Allocentric and egocentric spatial representations: Definitions, distinctions, and interconnections. Roberta L Klatzky, Spatial cognition. Springer1998</p>
<p>Visual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, International journal of computer vision. 12312017</p>
<p>The open images dataset v4. Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, 2020128International Journal of Computer Vision</p>
<p>Liunian Harold, Li , Mark Yatskar, Cho-Jui Da Yin, Kai-Wei Hsieh, Chang, arXiv:1908.03557Visualbert: A simple and performant baseline for vision and language. 2019arXiv preprint</p>
<p>Oscar: Objectsemantics aligned pre-training for vision-language tasks. Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, European Conference on Computer Vision. Springer2020</p>
<p>Fangyu Liu, Guy Emerson, Nigel Collier, arXiv:2205.00363Visual spatial reasoning. 2022aarXiv preprint</p>
<p>Things not written in text: Exploring spatial commonsense from visual signals. Xiao Liu, Da Yin, Yansong Feng, Dongyan Zhao, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics2022b1</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. NeurIPS; Vancouver, BC, Canada2019. 2019. 2019. December 8-14, 2019</p>
<p>Exploring the limits of weakly supervised pretraining. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens Van Der Maaten, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>Programs with common sense. RLE and MIT computation center. John Mccarthy, 1960Cambridge, MA, USA</p>
<p>Simple open-vocabulary object detection with vision transformers. Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, arXiv:2205.062302022arXiv preprint</p>
<p>Do nuclear submarines have nuclear captains? a challenge dataset for commonsense reasoning over adjectives and objects. James Mullenbach, Jonathan Gordon, Nanyun Peng, Jonathan , 10.18653/v1/D19-1625Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsMay. 2019</p>
<p>The world of an octopus: How reporting bias influences a language model's perception of color. Cory Paik, Stéphane Aroca-Ouellette, Alessandro Roncone, Katharina Kann, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, 10.18653/v1/D19-1250Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Paco: Preconditions attributed to commonsense knowledge. Ehsan Qasemi, Filip Ilievski, Muhao Chen, Pedro Szekely, EMNLP-Findings. 2022</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International Conference on Machine Learning. PMLR2021</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 211402020</p>
<p>Vision transformers for dense prediction. René Ranftl, Alexey Bochkovskiy, Vladlen Koltun, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. Nils Reimers, Iryna Gurevych, 10.18653/v1/D19-1410Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>How much knowledge can you pack into the parameters of a language model?. Adam Roberts, Colin Raffel, Noam Shazeer, 10.18653/v1/2020.emnlp-main.437Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Shreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson, Sculley, arXiv:1711.08536No classification without representation: Assessing geodiversity issues in open data sets for the developing world. 2017arXiv preprint</p>
<p>Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, 10.18653/v1/P18-1238Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>Flava: A foundational language and vision alignment model. Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, Douwe Kiela, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>COM2SENSE: A commonsense reasoning benchmark with complementary sentences. Shikhar Singh, Nuan Wen, Yu Hou, Pegah Alipoormolabashi, Te-Lin Wu, Xuezhe Ma, Nanyun Peng, 10.18653/v1/2021.findings-acl.78Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. the Thirty-First AAAI Conference on Artificial IntelligenceSan Francisco, California, USAAAAI Press2017. February 4-9, 2017</p>
<p>oLMpics-on what language model pre-training captures. Alon Talmor, Yanai Elazar, Yoav Goldberg, Jonathan Berant, 10.1162/tacl_a_00342Transactions of the Association for Computational Linguistics. 82020</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Episodic and semantic memory., Organization of memory., pages xiii, 423-xiii. Endel Tulving, 1972Academic Press423Oxford, England</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. NeurIPS; Vancouver, BC, Canada2019. 2019. 2019. December 8-14, 2019</p>
<p>Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang, International Conference on Machine Learning. PMLR2022</p>
<p>Unified contrastive learning in image-text-label space. Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, Jianfeng Gao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Geomlama: Geo-diverse commonsense probing on multilingual pre-trained language models. Hritik Da Yin, Masoud Bansal, Liunian Monajatipoor, Kai-Wei Harold Li, Chang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Broaden the vision: Geodiverse visual commonsense reasoning. Liunian Da Yin, Ziniu Harold Li, Nanyun Hu, Kai-Wei Peng, Chang, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>From recognition to cognition: Visual commonsense reasoning. Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi, 10.1109/CVPR.2019.00688IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USAComputer Vision Foundation / IEEE2019. June 16-20, 2019</p>
<p>Visual commonsense in pretrained unimodal and multimodal models. Chenyu Zhang, Benjamin Van Durme, Zhuowan Li, Elias Stengel-Eskin, 10.18653/v1/2022.naacl-main.390Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Scene parsing through ADE20K dataset. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, Antonio Torralba, 10.1109/CVPR.2017.5442017 IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HI, USAIEEE Computer Society2017. 2017. July 21-26, 2017</p>
<p>Object detection in 20 years: A survey. Zhengxia Zou, Zhenwei Shi, Yuhong Guo, Jieping Ye, arXiv:1905.050552019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
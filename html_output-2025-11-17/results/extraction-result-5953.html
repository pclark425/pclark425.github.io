<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5953 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5953</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5953</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-cd9113cd9677883c865cde747cddc0542e43fc4c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cd9113cd9677883c865cde747cddc0542e43fc4c" target="_blank">Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color</a></p>
                <p><strong>Paper Venue:</strong> Conference on Computational Natural Language Learning</p>
                <p><strong>Paper TL;DR:</strong> A thorough case study on color finds that warmer colors are, on average, better aligned to the perceptual color space than cooler ones, suggesting an intriguing connection to findings from recent work on efficient communication in color naming.</p>
                <p><strong>Paper Abstract:</strong> Pretrained language models have been shown to encode relational information, such as the relations between entities or concepts in knowledge-bases — (Paris, Capital, France). However, simple relations of this type can often be recovered heuristically and the extent to which models implicitly reflect topological structure that is grounded in world, such as perceptual structure, is unknown. To explore this question, we conduct a thorough case study on color. Namely, we employ a dataset of monolexemic color terms and color chips represented in CIELAB, a color space with a perceptually meaningful distance metric. Using two methods of evaluating the structural alignment of colors in this space with text-derived color term representations, we find significant correspondence. Analyzing the differences in alignment across the color spectrum, we find that warmer colors are, on average, better aligned to the perceptual color space than cooler ones, suggesting an intriguing connection to findings from recent work on efficient communication in color naming. Further analysis suggests that differences in alignment are, in part, mediated by collocationality and differences in syntactic usage, posing questions as to the relationship between color perception and usage and context.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5953.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5953.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>This paper (Color topology via LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This study evaluates whether text-trained language models (BERT, RoBERTa, ELECTRA) encode topology of human perceptual color space (CIELAB) by extracting color-term representations and measuring alignment via Representation Similarity Analysis and linear mapping probes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>BERT, RoBERTa, ELECTRA (large variants)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Transformer-based masked (BERT, RoBERTa) and discriminatively trained (ELECTRA) pretrained language models; experiments use layerwise contextual embeddings and several extraction configurations (non-contextual NC, random-context RC, controlled-context CC).</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Determine whether text-only LM-derived color-term representations encode the topological structure of human perceptual color space (CIELAB).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Cognitive science / computational linguistics (color perception and lexical semantics).</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Extract word-type and contextual embeddings for color terms (NC, RC, CC); compute centroids per color term and per Munsell chip; evaluate alignment with CIELAB using Representation Similarity Analysis (Kendall's τ) and regularised linear mapping probes (LASSO) with control random mappings; also use PMI and fastText baselines; analyze corpus statistics and surprisal.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Not framed as laws, but the distilled generalizable principle is that distributional text signals encode aspects of perceptual topology: language-derived embeddings partially recover perceptual color-space relations (e.g., clusterings and relative similarities).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Kendall's τ between representational similarity matrices (RSA); proportion of explained variance and probe selectivity for linear mapping; per-color-term RSA rows; rank-based retrieval of predicted CIELAB codes; statistical tests and mixed-effects models for corpus predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LM-derived representations show significant, though partial, alignment to CIELAB topology (RSA correlations significant for several layers/configs). Linear probes can predict CIELAB coordinates with high selectivity relative to random controls. Larger models align better; warm colors align more than cool colors; low-dimensional subspaces capture much of the variance. Simple PMI baselines and fastText show weaker but non-zero alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human involvement limited to use of an existing human-collected Color Lexicon (Munsell chart naming data) and human-derived CIELAB centroids; analyses and significance testing performed by authors (no human-in-the-loop law distillation).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Color Lexicon of American English (Munsell chart free-naming data); language models pretrained on large web corpora (Common Crawl etc.); PMI statistics computed on Common Crawl; controlled sentence templates generated for context.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Small number of frequently used color terms considered (18) for RSA analyses; language is influenced by non-perceptual usages (metaphor, fixed collocations) which reduce alignment; not full isomorphism between text and perceptual structure; reporting-bias and contextual/syntactic usage mediate representations; vision-and-language preliminary experiments showed no major improvement over text-only models.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Warm colors (yellow, orange, red) show stronger alignment to perceptual space than cool colors (blue, green); PMI-based collocationality negatively predicts alignment (e.g., 'Red Army' reduces perceptual signal); surprisal (ambiguity in labeling a chip) correlates with worse regression ranking of predicted CIELAB codes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5953.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5953.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Petroni et al. 2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models as knowledge bases?</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work that mines pretrained language models for factual and commonsense knowledge by converting knowledge-base triplets into cloze-style queries to query the models' fill-in-the-blank predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models as knowledge bases?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>pretrained masked language models (as referenced; specific model instances not specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Pretrained transformer masked language models treated as implicit knowledge stores queried via cloze templates.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Retrieve factual and commonsense triples from pretrained LMs by converting KB facts into cloze prompts and reading model completions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Commonsense / factual knowledge extraction from pretrained LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Formulate knowledge-base triples as cloze statements and query pretrained LMs; interpret top completions as retrieved facts.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Not explicitly laws; the approach extracts factual triples or common-sense statements (e.g., entity-attribute relationships) rather than domain laws.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Success measured by whether models produce the correct completions for cloze queries (accuracy/precision of retrieved facts) — described in the cited work (as summarized here).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as evidence that LMs can encode factual and relational information detectable via cloze querying; used in this paper as part of related work motivating that text-only data can encode world knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Automated querying and comparison to KB facts; original work typically involves dataset construction and evaluation (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Pretrained LM's original text corpora; knowledge-base triplet datasets used to form cloze queries (not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Does not imply extraction of generalizable scientific 'laws'; constraints include template sensitivity, reporting bias in text, and that extracted items are often factual triples rather than general principles.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Mentioned as an example of mining LMs for factual and relational knowledge via cloze statements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5953.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5953.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Davison et al. 2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Commonsense knowledge mining from pretrained models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work that mines commonsense knowledge from pretrained language models, cited here as evidence that text-only training can embed relational/world knowledge discoverable via probing or queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Commonsense knowledge mining from pretrained models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>pretrained language models (unspecified in this paper's citation summary)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>General pretrained LMs used as sources for commonsense knowledge extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Extract commonsense assertions/statements from pretrained LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Commonsense knowledge acquisition from LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Query pretrained models to surface commonsense relations (as summarized by this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Commonsense assertions or relational facts rather than formal laws.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Typically measured by correctness/acceptability of extracted assertions (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as supporting evidence that pretrained LMs capture some world knowledge extractable by appropriate methods; no detailed results presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Not detailed in this paper's mention; such studies commonly use automated probes with dataset-based evaluation and sometimes human validation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Pretrained corpora and commonsense datasets used in the cited work (not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Extraction tends to produce facts/assertions rather than abstract laws; quality depends on reporting bias and model training data coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Cited alongside Petroni et al. as an example of mining knowledge from LMs via query-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5953.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5953.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Forbes et al. 2019a</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Do neural language representations learn physical commonsense?</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited study investigating whether LM representations encode object properties (e.g., 'oranges are round') and affordances (e.g., 'oranges can be eaten').</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Do neural language representations learn physical commonsense?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>neural language model representations (specific models not detailed in this paper's citation summary)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Contextualized language model embeddings probed for property and affordance knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Assess whether LMs encode physical commonsense properties and affordances of objects.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Physical commonsense / object properties.</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Probe LM representations to retrieve object properties and affordances; compare to expected properties.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Object-property associations (e.g., typical attributes) rather than formal scientific laws.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Effectiveness at retrieving object properties/affordances (accuracy/precision in the cited work as described here).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited here to indicate LMs can capture some object attributes and affordances but that prior work finds encyclopedic/functional properties are better captured than perceptual ones.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Not specified in this paper's summary; original work likely uses dataset-based evaluation and human judgments for some analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>LM pretraining corpora and datasets listing object properties (not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LMs better model encyclopedic and functional properties than perceptual properties; perceptual grounding is challenging without multimodal supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Example property: 'oranges are round' used to illustrate the type of property probed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5953.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5953.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shwartz & Choi 2020</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Do neural language models overcome reporting bias?</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work extracting sentences where color terms modify nouns in Wikipedia and testing LM masked-prediction to evaluate whether LMs capture concept-color relations despite reporting bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Do neural language models overcome reporting bias?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>masked language models (specific models not enumerated in this paper's summary)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Masked-language-prediction models evaluated on masked color-term prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Test whether LMs can predict color modifiers given noun contexts, thereby overcoming reporting bias in text.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Lexical color-concept associations / reporting bias analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Extract sentences from Wikipedia where color term modifies a noun, mask the color term, and measure LM's prediction accuracy for the masked token.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Not a law; evaluates capacity to associate concepts with prototypical colors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Prediction accuracy on masked-token recovery; qualitative analysis of over-generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited finding: LMs can model concept-color relationships to some extent but are prone to over-generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Automated masked-prediction evaluation against corpora; human analysis reported in the cited work for interpretation (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Wikipedia sentences containing color-term modifiers (as used in the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Over-generalization and reporting bias reduce fidelity to perceptual grounding; masks method limited by data sparsity and usage patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5953.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5953.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ilharco et al. 2020</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probing text models for common ground with visual representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited study training a probe to map LM text-caption representations to paired visual representations of image patches to evaluate how much visual information text representations contain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Probing text models for common ground with visual representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>text-only pretrained language models (specific models not enumerated in this paper's summary)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Contextualized textual encoders whose embeddings are linearly mapped to visual patch representations for retrieval tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Measure how useful LM text representations are for retrieving semantically-aligned image patches (i.e., common ground between text and vision).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Vision-language alignment / multimodal retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Train a probe mapping textual caption embeddings to image patch visual embeddings and evaluate retrieval effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>No formal laws; extracts cross-modal correspondences (text-to-image alignment) that indicate how semantic information is encoded in text models.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Effectiveness at retrieving semantically-aligned image patches (retrieval accuracy/precision), compared to human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited result: many recent LMs' text representations are effective at retrieving semantically-aligned image patches but still far below human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Automated probe training and retrieval evaluation; human baseline comparisons in the cited work (as reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Image-caption paired datasets used by the cited work (not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Text-only representations underperform humans and lack full perceptual grounding; probe-based evaluation needs careful controls to avoid memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5953.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5953.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Weir et al. 2020</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probing neural language models for human tacit assumptions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work showing LMs can capture stereotypical tacit assumptions and retrieve concepts given their associated properties, with better performance on encyclopedic/functional properties than on perceptual ones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Probing neural language models for human tacit assumptions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>pretrained language models (models unspecified in the citation summary)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Contextualized LMs probed for their implicit knowledge about concepts and properties.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Investigate to what extent LMs encode tacit, stereotypical assumptions about concepts (property-to-concept retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Commonsense and conceptual property encoding in LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Probe LMs to retrieve concepts given properties and measure recovery performance.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Stereotypical property-concept associations (not formal scientific laws).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Success at retrieving target concepts from property cues (accuracy/precision), as summarized in the related work here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited finding: LMs adept at retrieving concepts from associated properties; better at encyclopedic/functional properties than perceptual features.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Not detailed here; typical methodology involves dataset-based automated probes and sometimes human validation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Datasets of property-concept associations used by the cited work (not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Perceptual properties are less well captured; biases and reporting patterns in text influence results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models as knowledge bases? <em>(Rating: 2)</em></li>
                <li>Commonsense knowledge mining from pretrained models <em>(Rating: 2)</em></li>
                <li>Do neural language representations learn physical commonsense? <em>(Rating: 2)</em></li>
                <li>Probing text models for common ground with visual representations <em>(Rating: 2)</em></li>
                <li>Do neural language models overcome reporting bias? <em>(Rating: 2)</em></li>
                <li>Probing neural language models for human tacit assumptions <em>(Rating: 2)</em></li>
                <li>Color naming across languages reflects color use <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5953",
    "paper_id": "paper-cd9113cd9677883c865cde747cddc0542e43fc4c",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [
        {
            "name_short": "This paper (Color topology via LMs)",
            "name_full": "Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color",
            "brief_description": "This study evaluates whether text-trained language models (BERT, RoBERTa, ELECTRA) encode topology of human perceptual color space (CIELAB) by extracting color-term representations and measuring alignment via Representation Similarity Analysis and linear mapping probes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "BERT, RoBERTa, ELECTRA (large variants)",
            "llm_model_description": "Transformer-based masked (BERT, RoBERTa) and discriminatively trained (ELECTRA) pretrained language models; experiments use layerwise contextual embeddings and several extraction configurations (non-contextual NC, random-context RC, controlled-context CC).",
            "task_goal": "Determine whether text-only LM-derived color-term representations encode the topological structure of human perceptual color space (CIELAB).",
            "domain": "Cognitive science / computational linguistics (color perception and lexical semantics).",
            "methodology": "Extract word-type and contextual embeddings for color terms (NC, RC, CC); compute centroids per color term and per Munsell chip; evaluate alignment with CIELAB using Representation Similarity Analysis (Kendall's τ) and regularised linear mapping probes (LASSO) with control random mappings; also use PMI and fastText baselines; analyze corpus statistics and surprisal.",
            "type_of_qualitative_law": "Not framed as laws, but the distilled generalizable principle is that distributional text signals encode aspects of perceptual topology: language-derived embeddings partially recover perceptual color-space relations (e.g., clusterings and relative similarities).",
            "evaluation_metrics": "Kendall's τ between representational similarity matrices (RSA); proportion of explained variance and probe selectivity for linear mapping; per-color-term RSA rows; rank-based retrieval of predicted CIELAB codes; statistical tests and mixed-effects models for corpus predictors.",
            "results_summary": "LM-derived representations show significant, though partial, alignment to CIELAB topology (RSA correlations significant for several layers/configs). Linear probes can predict CIELAB coordinates with high selectivity relative to random controls. Larger models align better; warm colors align more than cool colors; low-dimensional subspaces capture much of the variance. Simple PMI baselines and fastText show weaker but non-zero alignment.",
            "human_involvement": "Human involvement limited to use of an existing human-collected Color Lexicon (Munsell chart naming data) and human-derived CIELAB centroids; analyses and significance testing performed by authors (no human-in-the-loop law distillation).",
            "dataset_or_corpus": "Color Lexicon of American English (Munsell chart free-naming data); language models pretrained on large web corpora (Common Crawl etc.); PMI statistics computed on Common Crawl; controlled sentence templates generated for context.",
            "limitations_or_challenges": "Small number of frequently used color terms considered (18) for RSA analyses; language is influenced by non-perceptual usages (metaphor, fixed collocations) which reduce alignment; not full isomorphism between text and perceptual structure; reporting-bias and contextual/syntactic usage mediate representations; vision-and-language preliminary experiments showed no major improvement over text-only models.",
            "notable_examples": "Warm colors (yellow, orange, red) show stronger alignment to perceptual space than cool colors (blue, green); PMI-based collocationality negatively predicts alignment (e.g., 'Red Army' reduces perceptual signal); surprisal (ambiguity in labeling a chip) correlates with worse regression ranking of predicted CIELAB codes.",
            "uuid": "e5953.0",
            "source_info": {
                "paper_title": "Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Petroni et al. 2019",
            "name_full": "Language models as knowledge bases?",
            "brief_description": "Cited work that mines pretrained language models for factual and commonsense knowledge by converting knowledge-base triplets into cloze-style queries to query the models' fill-in-the-blank predictions.",
            "citation_title": "Language models as knowledge bases?",
            "mention_or_use": "mention",
            "llm_model_name": "pretrained masked language models (as referenced; specific model instances not specified in this paper)",
            "llm_model_description": "Pretrained transformer masked language models treated as implicit knowledge stores queried via cloze templates.",
            "task_goal": "Retrieve factual and commonsense triples from pretrained LMs by converting KB facts into cloze prompts and reading model completions.",
            "domain": "Commonsense / factual knowledge extraction from pretrained LMs.",
            "methodology": "Formulate knowledge-base triples as cloze statements and query pretrained LMs; interpret top completions as retrieved facts.",
            "type_of_qualitative_law": "Not explicitly laws; the approach extracts factual triples or common-sense statements (e.g., entity-attribute relationships) rather than domain laws.",
            "evaluation_metrics": "Success measured by whether models produce the correct completions for cloze queries (accuracy/precision of retrieved facts) — described in the cited work (as summarized here).",
            "results_summary": "Cited as evidence that LMs can encode factual and relational information detectable via cloze querying; used in this paper as part of related work motivating that text-only data can encode world knowledge.",
            "human_involvement": "Automated querying and comparison to KB facts; original work typically involves dataset construction and evaluation (not detailed here).",
            "dataset_or_corpus": "Pretrained LM's original text corpora; knowledge-base triplet datasets used to form cloze queries (not specified here).",
            "limitations_or_challenges": "Does not imply extraction of generalizable scientific 'laws'; constraints include template sensitivity, reporting bias in text, and that extracted items are often factual triples rather than general principles.",
            "notable_examples": "Mentioned as an example of mining LMs for factual and relational knowledge via cloze statements.",
            "uuid": "e5953.1",
            "source_info": {
                "paper_title": "Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Davison et al. 2019",
            "name_full": "Commonsense knowledge mining from pretrained models",
            "brief_description": "Cited work that mines commonsense knowledge from pretrained language models, cited here as evidence that text-only training can embed relational/world knowledge discoverable via probing or queries.",
            "citation_title": "Commonsense knowledge mining from pretrained models",
            "mention_or_use": "mention",
            "llm_model_name": "pretrained language models (unspecified in this paper's citation summary)",
            "llm_model_description": "General pretrained LMs used as sources for commonsense knowledge extraction.",
            "task_goal": "Extract commonsense assertions/statements from pretrained LMs.",
            "domain": "Commonsense knowledge acquisition from LMs.",
            "methodology": "Query pretrained models to surface commonsense relations (as summarized by this paper).",
            "type_of_qualitative_law": "Commonsense assertions or relational facts rather than formal laws.",
            "evaluation_metrics": "Typically measured by correctness/acceptability of extracted assertions (not detailed here).",
            "results_summary": "Cited as supporting evidence that pretrained LMs capture some world knowledge extractable by appropriate methods; no detailed results presented here.",
            "human_involvement": "Not detailed in this paper's mention; such studies commonly use automated probes with dataset-based evaluation and sometimes human validation.",
            "dataset_or_corpus": "Pretrained corpora and commonsense datasets used in the cited work (not specified here).",
            "limitations_or_challenges": "Extraction tends to produce facts/assertions rather than abstract laws; quality depends on reporting bias and model training data coverage.",
            "notable_examples": "Cited alongside Petroni et al. as an example of mining knowledge from LMs via query-based approaches.",
            "uuid": "e5953.2",
            "source_info": {
                "paper_title": "Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Forbes et al. 2019a",
            "name_full": "Do neural language representations learn physical commonsense?",
            "brief_description": "Cited study investigating whether LM representations encode object properties (e.g., 'oranges are round') and affordances (e.g., 'oranges can be eaten').",
            "citation_title": "Do neural language representations learn physical commonsense?",
            "mention_or_use": "mention",
            "llm_model_name": "neural language model representations (specific models not detailed in this paper's citation summary)",
            "llm_model_description": "Contextualized language model embeddings probed for property and affordance knowledge.",
            "task_goal": "Assess whether LMs encode physical commonsense properties and affordances of objects.",
            "domain": "Physical commonsense / object properties.",
            "methodology": "Probe LM representations to retrieve object properties and affordances; compare to expected properties.",
            "type_of_qualitative_law": "Object-property associations (e.g., typical attributes) rather than formal scientific laws.",
            "evaluation_metrics": "Effectiveness at retrieving object properties/affordances (accuracy/precision in the cited work as described here).",
            "results_summary": "Cited here to indicate LMs can capture some object attributes and affordances but that prior work finds encyclopedic/functional properties are better captured than perceptual ones.",
            "human_involvement": "Not specified in this paper's summary; original work likely uses dataset-based evaluation and human judgments for some analyses.",
            "dataset_or_corpus": "LM pretraining corpora and datasets listing object properties (not specified here).",
            "limitations_or_challenges": "LMs better model encyclopedic and functional properties than perceptual properties; perceptual grounding is challenging without multimodal supervision.",
            "notable_examples": "Example property: 'oranges are round' used to illustrate the type of property probed.",
            "uuid": "e5953.3",
            "source_info": {
                "paper_title": "Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Shwartz & Choi 2020",
            "name_full": "Do neural language models overcome reporting bias?",
            "brief_description": "Cited work extracting sentences where color terms modify nouns in Wikipedia and testing LM masked-prediction to evaluate whether LMs capture concept-color relations despite reporting bias.",
            "citation_title": "Do neural language models overcome reporting bias?",
            "mention_or_use": "mention",
            "llm_model_name": "masked language models (specific models not enumerated in this paper's summary)",
            "llm_model_description": "Masked-language-prediction models evaluated on masked color-term prediction tasks.",
            "task_goal": "Test whether LMs can predict color modifiers given noun contexts, thereby overcoming reporting bias in text.",
            "domain": "Lexical color-concept associations / reporting bias analysis.",
            "methodology": "Extract sentences from Wikipedia where color term modifies a noun, mask the color term, and measure LM's prediction accuracy for the masked token.",
            "type_of_qualitative_law": "Not a law; evaluates capacity to associate concepts with prototypical colors.",
            "evaluation_metrics": "Prediction accuracy on masked-token recovery; qualitative analysis of over-generalization.",
            "results_summary": "Cited finding: LMs can model concept-color relationships to some extent but are prone to over-generalization.",
            "human_involvement": "Automated masked-prediction evaluation against corpora; human analysis reported in the cited work for interpretation (not detailed here).",
            "dataset_or_corpus": "Wikipedia sentences containing color-term modifiers (as used in the cited work).",
            "limitations_or_challenges": "Over-generalization and reporting bias reduce fidelity to perceptual grounding; masks method limited by data sparsity and usage patterns.",
            "uuid": "e5953.4",
            "source_info": {
                "paper_title": "Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Ilharco et al. 2020",
            "name_full": "Probing text models for common ground with visual representations",
            "brief_description": "Cited study training a probe to map LM text-caption representations to paired visual representations of image patches to evaluate how much visual information text representations contain.",
            "citation_title": "Probing text models for common ground with visual representations",
            "mention_or_use": "mention",
            "llm_model_name": "text-only pretrained language models (specific models not enumerated in this paper's summary)",
            "llm_model_description": "Contextualized textual encoders whose embeddings are linearly mapped to visual patch representations for retrieval tasks.",
            "task_goal": "Measure how useful LM text representations are for retrieving semantically-aligned image patches (i.e., common ground between text and vision).",
            "domain": "Vision-language alignment / multimodal retrieval.",
            "methodology": "Train a probe mapping textual caption embeddings to image patch visual embeddings and evaluate retrieval effectiveness.",
            "type_of_qualitative_law": "No formal laws; extracts cross-modal correspondences (text-to-image alignment) that indicate how semantic information is encoded in text models.",
            "evaluation_metrics": "Effectiveness at retrieving semantically-aligned image patches (retrieval accuracy/precision), compared to human performance.",
            "results_summary": "Cited result: many recent LMs' text representations are effective at retrieving semantically-aligned image patches but still far below human performance.",
            "human_involvement": "Automated probe training and retrieval evaluation; human baseline comparisons in the cited work (as reported here).",
            "dataset_or_corpus": "Image-caption paired datasets used by the cited work (not specified here).",
            "limitations_or_challenges": "Text-only representations underperform humans and lack full perceptual grounding; probe-based evaluation needs careful controls to avoid memorization.",
            "uuid": "e5953.5",
            "source_info": {
                "paper_title": "Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Weir et al. 2020",
            "name_full": "Probing neural language models for human tacit assumptions",
            "brief_description": "Cited work showing LMs can capture stereotypical tacit assumptions and retrieve concepts given their associated properties, with better performance on encyclopedic/functional properties than on perceptual ones.",
            "citation_title": "Probing neural language models for human tacit assumptions",
            "mention_or_use": "mention",
            "llm_model_name": "pretrained language models (models unspecified in the citation summary)",
            "llm_model_description": "Contextualized LMs probed for their implicit knowledge about concepts and properties.",
            "task_goal": "Investigate to what extent LMs encode tacit, stereotypical assumptions about concepts (property-to-concept retrieval).",
            "domain": "Commonsense and conceptual property encoding in LMs.",
            "methodology": "Probe LMs to retrieve concepts given properties and measure recovery performance.",
            "type_of_qualitative_law": "Stereotypical property-concept associations (not formal scientific laws).",
            "evaluation_metrics": "Success at retrieving target concepts from property cues (accuracy/precision), as summarized in the related work here.",
            "results_summary": "Cited finding: LMs adept at retrieving concepts from associated properties; better at encyclopedic/functional properties than perceptual features.",
            "human_involvement": "Not detailed here; typical methodology involves dataset-based automated probes and sometimes human validation.",
            "dataset_or_corpus": "Datasets of property-concept associations used by the cited work (not specified in this paper).",
            "limitations_or_challenges": "Perceptual properties are less well captured; biases and reporting patterns in text influence results.",
            "uuid": "e5953.6",
            "source_info": {
                "paper_title": "Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 2
        },
        {
            "paper_title": "Commonsense knowledge mining from pretrained models",
            "rating": 2
        },
        {
            "paper_title": "Do neural language representations learn physical commonsense?",
            "rating": 2
        },
        {
            "paper_title": "Probing text models for common ground with visual representations",
            "rating": 2
        },
        {
            "paper_title": "Do neural language models overcome reporting bias?",
            "rating": 2
        },
        {
            "paper_title": "Probing neural language models for human tacit assumptions",
            "rating": 2
        },
        {
            "paper_title": "Color naming across languages reflects color use",
            "rating": 1
        }
    ],
    "cost": 0.015529,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color</h1>
<p>Mostafa Abdou ${ }^{+}$<br>University of Copenhagen<br>Artur Kulmizev<br>Uppsala University<br>Daniel Hershcovich<br>University of Copenhagen<br>Stella Frank<br>University of Trento<br>Ellie Pavlick<br>Brown University<br>Anders Søgaard<br>University of Copenhagen</p>
<h4>Abstract</h4>
<p>Pretrained language models have been shown to encode relational information, such as the relations between entities or concepts in knowledge-bases - (Paris, Capital, France). However, simple relations of this type can often be recovered heuristically and the extent to which models implicitly reflect topological structure that is grounded in world, such as perceptual structure, is unknown. To explore this question, we conduct a thorough case study on color. Namely, we employ a dataset of monolexemic color terms and color chips represented in CIELAB, a color space with a perceptually meaningful distance metric. Using two methods of evaluating the structural alignment of colors in this space with textderived color term representations, we find significant correspondence. Analyzing the differences in alignment across the color spectrum, we find that warmer colors are, on average, better aligned to the perceptual color space than cooler ones, suggesting an intriguing connection to findings from recent work on efficient communication in color naming. Further analysis suggests that differences in alignment are, in part, mediated by collocationality and differences in syntactic usage, posing questions as to the relationship between color perception and usage and context.</p>
<h2>1 Introduction</h2>
<p>Without grounding or interaction with the world, language models (LMs) learn representations that encode various aspects of formal linguistic structure (e.g., morphosyntax (Tenney et al., 2019)) and semantic information (e.g., lexical similarity (Reif et al., 2019a)). Beyond this, it has been suggested that text-only training data is enough for LMs to also acquire factual and relational information about the world (Davison et al., 2019; Petroni et al., 2019). This includes, for instance, some</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Right: Color orientation in 3d CIELAB space. Left: linear mapping from BERT (CC, see §2) color term embeddings to the CIELAB space.
features of concrete and abstract concepts, such as objects' attributes and affordances (Forbes et al., 2019b; Weir et al., 2020). Furthermore, the representational geometry of LMs has been found to naturally reflect human lexical similarity and relatedness judgements, as well as analogy relationships (Chronis and Erk, 2020). However, the extent to which these models reflect the structures that exist in humans' perceptual world-such as the topology of visual perception (Chen, 1982), the structure of the color spectrum (Ennis and Zaidi, 2019; Provenzi, 2020), or of odour spaces (Rossiter, 1996; Chastrette, 1997)—is not well-understood.</p>
<p>If LMs are indeed able to capture such topologies-in some domains, at least-it would mean that these structures are a) somehow reflected in language and, thereby, encoded in the textual training data on which models are trained, and b) learnable using models' current training objectives and architectural inductive biases. To the extent they are not, the question becomes whether the information is not there in the data, or whether model and training objective limitations are to blame. Certainly, this latter point relates to an ongoing debate regarding what exactly language models can be expected to learn from ungrounded form alone (Bender and Koller, 2020; Bisk et al., 2020; Merrill et al., 2021). While there have been many inter-</p>
<p>esting theoretical debates around this topic, few studies have tried to address this question empirically.</p>
<p>In this paper, we conduct a case study on color. Indeed, color perception in humans and its relation to speakers' use of color terms has long been the subject of studies in cognitive science (Kay and McDaniel, 1978; Berlin and Kay, 1991; Regier et al., 2007; Kay et al., 2009). To this end, spaces have been defined in which Euclidean distances between related colors are correlated with reported perceptual differences. ${ }^{1}$ In addition, the semantics of color terms have long been understood to hold particular linguistic significance, as they are theorised to be subject to universal constraints that arise directly from the neurophysiological mechanisms and properties underlying visual perception and cognition (Kay and McDaniel, 1978; Berlin and Kay, 1991; Kay et al., 1991). ${ }^{2}$ Due to these factors, color offers a useful test-bed for investigating whether or not structural information about the topology of the perceptual world might be encoded in linguistic representations.</p>
<p>To explore this in detail, we employ a dataset of English color terms and their corresponding color chips ${ }^{3}$, the latter of which are represented in CIELAB - a perceptually uniform color space. In addition to the color chip CIELAB coordinates, we extract linguistic representations for the corresponding color terms. With these two representations in mind (see Figure 1 for a demonstrative plot from our experiments), we employ two methods of measuring structural correspondence, with which we evaluate the alignment between the two spaces. Figure 2 shows an illustration of the experimental setup. We find that the structures of various language model representations show alignment with the structure of the CIELAB space, demonstrating that some approximation of perceptual color space topology can indeed be learned from text alone.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>We also show that part of this distributional signal is learnable by simple models - e.g. models based on pointwise mutual information (PMI) statistics - although large-scale language model pretraining (e.g., BERT) encodes the topology markedly better.</p>
<p>Analysis shows that larger language models align better than smaller ones and that much of the variance in CIELAB space can be explained by low-dimensional subspaces of LM-induced color term representations. To better understand the results, we also analyse the differences in alignment across the color spectrum, observing that warm colors are generally better aligned than cool ones. Further investigation reveals a connection to findings reported in work on communication efficiency in color naming, which posits that warmer colors are communicated more efficiently. Finally, we investigate various corpus statistics which could influence alignment, finding that a measure of color term collocationality based on PMI statistics corresponds to lower alignment, while the entropy of a color term's dependency relation distribution (i.e. terms occurring as adjectival modifiers, nominal subjects, etc.) and how often it occurs as an adjectival modifier correspond to a stronger one.</p>
<h2>2 Methodology</h2>
<p>Color data We employ the Color Lexicon of American English, which provides extensive data on color naming. The lexicon consists of 51 monolexemic color name judgements for each of the 330 Munsell Chart color chips ${ }^{4}$ (Lindsey and Brown, 2014). The color terms are solicited through a free-naming task, resulting in 122 terms.</p>
<p>Perceptual color space Following previous work (Regier et al., 2007; Zaslavsky et al., 2018; Chaabouni et al., 2021), we map colors to their corresponding points in the 3D CIELAB space, where the first dimension $L$ expresses lightness, the second $A$ expresses position between red and green, and the third $B$ expresses the position between blue and yellow. Distances between colors in the space correspond to their perceptual difference.</p>
<p>Language models Our analysis is conducted on three widely used language models (LMs): BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), both of which employ a masked language modelling objective, and ELECTRA (Clark et al.,</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Our experimental setup. In the center is a Munsell color chart. Each chip in the chart is represented in the CIELAB space (right) and has 51 color term annotations. Color term embeddings are extracted through various methods. In the Representation Similarity Analysis experiments, a corresponding color chip centroid is computed in the CIELAB space. In the Linear Mapping experiments, a color term embedding centroid is computed per chip.
2020), which is trained instead with a discriminative token replacement detection objective. ${ }^{5}$</p>
<p>Baselines In addition to the aforementioned language models, we consider two different baselines:</p>
<ul>
<li>PMI statistics, which are computed ${ }^{6}$ for the color terms in common crawl, using window sizes of $1(\mathrm{pmi}-1), 2(\mathrm{pmi}-2)$, and $3(\mathrm{pmi}-3)$. The result is a vocabulary length vector quantifying the likelihood of co-occurrence of the color term with every other vocabulary item in within that window.</li>
<li>Word-type FastText embeddings trained on Common Crawl (Bojanowski et al., 2017).</li>
</ul>
<p>Representation Extraction We follow Bommasani et al. (2020) and Vulić et al. (2020) in defining configurations for the extraction of word-type representations from LM hidden states. In the first configuration (NC), a color term is encoded without context, with the appropriate delimiter tokens attached (e.g. [CLS] red [SEP] for BERT). In the second, $S$ sentential contexts that include the color term are encoded and the hidden states representing these contexts are mean pooled. These $S$ contexts are either randomly sampled from common crawl (RC), or deterministically generated to allow for control over contextual variation (CC). If a color term is split by an LM's tokenizer into more than one token, subword token encodings are averaged over. For each color term and configuration,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>an embedding vector of hidden state dimension $d_{L M}$ is extracted per layer, per model.</p>
<p>Controlled context To control for the effect of variation in the sentence contexts used to construct color term representations, we employ a templative approach to generate a set of identical contexts for all color terms. When generating controlled contexts, we create three frames in which the terms can appear:</p>
<ul>
<li>Copula: the <obj> is <col></li>
<li>Possession: i have a <col> <obj></li>
<li>Spatial: the <col> <obj> is there</li>
</ul>
<p>We use these frames in order to limit the contextual variation across colors ( $&lt;$ col $&gt;$ ) and to isolate their representations amidst as little semantic interference as possible, all while retaining a naturalistic quality to the input. We also aggregate over numerous object nouns (<obj>), which the color terms are used to describe. We select objects from the McRae et al. (2005) data which are labelled in the latter as plausibly occurring in many colors and which are stratified across 13 category sets, e.g. fan $\in$ APPLIANCES, skirt $\in$ CLOTHING, etc. Collapsing over categories, we generate sentences combinatorially across frames, objects and color terms, resulting in $3 \times 122 \times 18=6588$ sentences, 366 per term.</p>
<h2>3 Evaluation</h2>
<p>We employ two complimentary evaluation methods to gauge the correspondence of the color term text-derived representations to the perceptual color</p>
<p>space. The first, Representation Similarity Analysis (RSA), is non-parametric and uses pairwise comparisons of stimuli to provide a measure of the global topological alignment between two spaces. The second employs a learned linear mapping, evaluating the extent to which two spaces can be aligned via transformation (rotation, scaling, etc.).</p>
<p>RSA (Kriegeskorte et al., 2008) is a method of relating different representational modalities, which was first employed in neuroscientific studies. RSA abstracts away from activity patterns themselves (e.g. neuron values in representational vectors) and instead computes representational (dis)-similarity matrices (RSMs), which characterize the information carried by a given representation method through global (dis)-similarity structure. Kendall's rank correlation coefficient $(\tau)$ is computed between RSMs derived from the two spaces, providing a summary statistic indicative of the overall representational alignment between them. RSA is non-parametric and therefore circumvents many of the various methodological weaknesses associated with the probing paradigm (Belinkov, 2021).</p>
<p>For each color term, we compute a centroid in the CIELAB space following the approach described in Lindsey and Brown (2014). Each centroid is defined as the average CIELAB coordinate of the samples (i.e. color chips) that were named with the corresponding term (across the 51 subjects). This results in $N$ parallel points in the color term embedding and perceptual color spaces, where $N$ is the number of color terms considered. For our analysis, we exclude color terms used less frequently than a cutoff $f=100$ in the color lexicon, leaving us with the 18 most commonly used color terms. ${ }^{7}$ We then separately construct an $N \times N$ RSM for each of the LM spaces and for CIELAB . Each cell in the RSM corresponds to the similarity between the activity patterns associated with pairs of experimental conditions $n_{i}, n_{j} \in N$.</p>
<p>For the color term embedding space, we employ Pearson's correlation coefficient $(r)$ as a similarity measure between each pair of embeddings $n_{i}, n_{j} \in N$. For the CIELAB space, we elect to use the following method, per Regier et al.'s (2007) suggestion: $\operatorname{sim}\left(n_{i}, n_{j}\right)=\exp \left(-c \times\left[\operatorname{dist}\left(n_{i}, n_{j}\right)\right]^{2}\right)$, where $c$ is a scaling factor (set to 0.001 in all ex-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>periments reported here) and $\operatorname{dist}\left(n_{i}, n_{j}\right)$ is the CIELAB distance $\left(\Delta \mathrm{E}<em i="i">{-} \mathrm{CMC}^{*}\right)^{8}$ between chips $n</em>$. This similarity measure is derived from the psychological literature on categorization and is meant to model the assumption that beyond a certain distance colors appear entirely different, so that increasing the distance has no further effect on dissimilarity. Finally, we report the mean Kendall's $\tau$ between the color term embedding and color space RSMs. We also report $\tau$ per color term (i.e. per row in the RSM), which corresponds to how well-aligned each individual color term is.}$ and $n_{j</p>
<p>Linear mapping We train regularised linear regression models to map from color term embedding space $X \in \mathbb{R}^{n \times d_{L M}}$ to CIELAB space $Y \in \mathbb{R}^{n \times 3}$, minimising $\mathcal{L}(W ; \alpha)=|X W-Y|<em 1="1">{2}^{2}+\alpha|W|</em> ;|W|}$, where $W \in \mathbb{R}^{3 \times d_{L M}}$ is a linear map and $\alpha$ is the lasso regularization hyper parameter. We vary $\alpha$ across a wide range of settings to examine the effect of probe complexity, which we measure using the nuclear norm of the linear projection matrix $W \in \mathbb{R}^{\phi \times \iota<em i="1">{*}=\sum</em>(W)$ is the $i$ th singular value of $W$ (Pimentel et al., 2020). The fitness of the regressors, evaluated using $n$-fold cross-validation $(n=6)$ indicates the alignability of the two spaces, given a linear transformation. Centroids corresponding to each Munsell color chip are computed in the color term embedding space via the weighted mean of the embeddings of the 51 terms used to label it. As in the RSA experiments, terms occurring less frequently than the cutoff $(f=100)$ are excluded. For evaluation, we compute the average (across splits and datapoints) proportion of explained variance as well as the ranking of a predicted color term embedding according to the Pearson distance $(1-r)$ to gold.}^{\min \left(\phi, \iota\right)} \sigma_{i}(W)$, where $\sigma_{i</p>
<p>Control task As proposed by Hewitt and Liang (2019), we construct a random control task for the linear mapping experiments, wherein we randomly swap each color chip's CIELAB code for another. This is meant to break the mapping between the color chips and their corresponding terms. Control task results are reported as the mean of 10 different random re-mappings. We report probe selectivity, which is defined as the difference between proportion of explained variance in the standard experimental condition and in the control task (He-</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">NC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">RSA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">lin. map</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RSA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">lin. map</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RSA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">lin. map</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">max</td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">max</td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">max</td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">max</td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">max</td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">max</td>
<td style="text-align: center;">mean</td>
</tr>
<tr>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">$0.16^{*}$</td>
<td style="text-align: center;">$0.01_{\pm 0.09}$</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">$0.73_{\pm 0.01}$</td>
<td style="text-align: center;">$0.26^{\dagger}$</td>
<td style="text-align: center;">$0.20_{\pm 0.03}$</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">$0.73_{\pm 0.08}$</td>
<td style="text-align: center;">$0.24^{\dagger}$</td>
<td style="text-align: center;">$0.19_{\pm 0.03}$</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">$0.75_{\pm 0.05}$</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">$0.33^{\S}$</td>
<td style="text-align: center;">$0.02_{\pm 0.11}$</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">$0.73_{\pm 0.01}$</td>
<td style="text-align: center;">$0.20^{*}$</td>
<td style="text-align: center;">$0.14_{\pm 0.04}$</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">$0.73_{\pm 0.01}$</td>
<td style="text-align: center;">$0.19^{*}$</td>
<td style="text-align: center;">$0.14_{\pm 0.04}$</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">$0.76_{\pm 0.09}$</td>
</tr>
<tr>
<td style="text-align: center;">ELECTRA</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">$0.01_{\pm 0.08}$</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">$0.64_{\pm 0.13}$</td>
<td style="text-align: center;">$0.25^{\dagger}$</td>
<td style="text-align: center;">$0.19_{\pm 0.05}$</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">$0.73_{\pm 0.01}$</td>
<td style="text-align: center;">$0.23^{\dagger}$</td>
<td style="text-align: center;">$0.16_{\pm 0.04}$</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">$0.76_{\pm 0.01}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Results for the RSA experiments show max and mean (across layers) Kendall's $\tau$; correlations that are significantly non-zero are marked with ${ }^{*}, \dagger$ and $\S$ for $p&lt;0.05,&lt;0.01$ and $&lt;0.001$ respectively. Results for the linear mapping experiments show max and mean selectivity.
witt and Liang, 2019). We run similar control for the RSA experiments, where the CIELAB space centroids are randomly shuffled.</p>
<h2>4 Results</h2>
<p>Table 1 shows the max, mean, and standard deviation (across layers) of alignment scores for each of the LMs, per alignment method and setting. For RSA, we observe significant correlations across all configurations: most LM layers show a topological alignment with color space. Notably, this is also true for the static embeddings and for one of the PMI baselines (Table 2). Although some variance is observed, ${ }^{9}$ the presence of significant correlations is telling, given the small sample size (18). Furthermore, randomly permuting the color space centroids leads to RSA correlations that are non-significant for all setups ( $p&gt;0.05$ ), which lends further credence to models' alignment with CIELAB structure.</p>
<p>Figure 3 shows the breakdown of correlations per color term for the three LMs under CC, as well as for fastText. We find that this ranking of color terms is largely stable across models and layer. Full RSMs for all models and CIELAB are in appendix C. The RSMs show evidence of the higher correlations for colors like violet, orange, and purple, being driven by general clusterings of similarity/dissimilarity. For instance, for both the CIELAB and CC BERT RSMs, violet's top nearest neighbors include purple, lavender, pink, and orange, and its furthest neighbors include aqua, olive, black, and gray. Correlations do not, however, appear to be driven by consistently aligned partial orderings within the clusters. In addition, we compute RSA correlations between the different</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Model | RSA | lin. map |
| :--: | :--: | :--: |
| pmi-1 | 0.14 | 0.72 |
| pmi-2 | 0.11 | 0.70 |
| pmi-3 | $0.17^{<em>}$ | 0.71 |
| fastText | $0.23^{</em>}$ | 0.72 |</p>
<p>Table 2: Baseline results. RSA results show Kendall's $\tau$; results with * are significantly non-zero $(p&lt;0.05)$. Linear mapping results show selectivity.
models. Results show that NC embeddings have low alignment to all others (details in appendix B).</p>
<p>For the linear mapping experiments, we observe the highest selectivity scores for CC (Table 1, right) compared to NC and RC (Table 1, left, middle) and baselines (Table 2). This validates our intuition that controlling for variation in sentence context would reveal increased alignment to color space.</p>
<p>Furthermore, we observe that, over the full range of probe complexities for the experimental condition and the control task (described as in $\S 3$ ), all models demonstrate high selectivity (see G for full results). It is, therefore, safe to attribute the fitness of the probes to information encoded in the color term representations, rather than to memorization. In terms of individual colors, Figure 4a depicts the ranking of predicted CIELAB codes per Munsell color chip for BERT (CC). We find that these results are largely stable across models and layers (see appendix F for full set of results and for reference chart). Also, we observe that clusterings of chips with certain modal color terms (green, blue) show worse rankings than the rest.</p>
<h2>5 Analysis and Discussion</h2>
<p>Having demonstrated the existence of models' alignment to CIELAB across various configurations, we now present an analysis and discussion of these results.</p>
<p>Dimensionality of color subspace Previous work has shown that linguistic information such as</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: RSA results (Kendal's $\tau$ ) broken down by color term for each of the LMs under the CC configuration and for the fastText baseline.
part-of-speech category, dependency relation type, and word sense, is expressed in low-dimensional subspaces of language model representations (Reif et al., 2019b; Durrani et al., 2020; Hernandez and Andreas, 2021). We investigate the dimensionality of the subspace required to predict the CIELAB chip codes from the term embeddings, following the methodology of Durrani et al. (2020). Averaging over the three predicted CIELAB dimensions, we rank the linear mapping coefficients (from the experiments described in §2), sorting the weights by their absolute values in descending order. Results (appendix H) show that across models and layers, $\sim 0.4$ of the variance in the CIELAB chip codes can be explained by assigning $95 \%$ of the weights to $\sim 10$ dimensions. $30-40$ dimensions are sufficient to explain $\sim 0.7$ of the variance, nearly the proportion of variance explained by the full representations (Table 1).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">RSA max</th>
<th style="text-align: center;">RSA mean</th>
<th style="text-align: center;">lin. map.. max</th>
<th style="text-align: center;">lin. map. mean</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT-mini</td>
<td style="text-align: center;">0.077</td>
<td style="text-align: center;">$0.043 \pm 0.340$</td>
<td style="text-align: center;">0.729</td>
<td style="text-align: center;">$0.582 \pm 0.291$</td>
</tr>
<tr>
<td style="text-align: left;">BERT-small</td>
<td style="text-align: center;">0.106</td>
<td style="text-align: center;">$0.070 \pm 0.191$</td>
<td style="text-align: center;">0.734</td>
<td style="text-align: center;">$0.598 \pm 0.294$</td>
</tr>
<tr>
<td style="text-align: left;">BERT-medium</td>
<td style="text-align: center;">0.097</td>
<td style="text-align: center;">$0.057 \pm 0.035$</td>
<td style="text-align: center;">0.739</td>
<td style="text-align: center;">$0.654 \pm 0.221$</td>
</tr>
<tr>
<td style="text-align: left;">BERT-base</td>
<td style="text-align: center;">0.162</td>
<td style="text-align: center;">$0.092 \pm 0.058$</td>
<td style="text-align: center;">0.740</td>
<td style="text-align: center;">$0.677 \pm 0.182$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results for the four smaller BERT models. RSA results (left) show max and mean (across layers) Kendall's correlation coefficient ( $\tau$ ). Correlations that are significantly non-zero are indicated with: * : $p&lt;0.05$. Results for the Linear Mapping experiments (right) show max and mean selectivity. Standard deviation across layers is included with the mean results.</p>
<p>Effect of model size We also evaluate the effect of model size on alignment by testing four smaller BERT (CC) models ${ }^{10}$ using the same setup described above. The results (table 3) show that alignment as measured by both RSA and linear mapping progressively increases with model size,</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>meaning that that with growing complexity, model representational geometry of color terms moves towards isomorphism to CIELAB.</p>
<p>Color temperature In Figures 3 \&amp; 4a we observe that on average, warmer colors (yellow, orange, red, etc.) show a closer alignment than cooler ones (blue, green, etc.). In recent work, Gibson et al. (2017) reported that the former are on average communicated more efficiently (see next paragraph) than the latter, across languages. This is attributed to warmer colors being more prevalent as colors of behaviorally relevant items in the environment - salient objects - compared to cooler ones, which occur more often as background colors. To verify this observation, we partition the space of chips into two (see appendix D for details) and compute the average explained variance across warm and cool colors. The results (see appendix D for plots) show that, term embeddings of warm colors are better aligned to CIELAB than those of cool ones, across models and configurations. This is consistent with the bias described in Gibson et al. (2017), which we conjecture might be filtering through into the distributional statistics of (color terms in) textual corpora, influencing the representations learned by various methods which leverage these statistics.</p>
<p>Connection to listener surprisal Gibson et al. (2017)'s findings are based on the application of an information theoretic analysis to color naming, framing it as a communication game where a speaker has a particular color chip $c$ in mind and uses a word $w$ to indicate it then a listener has to correctly guess $c$, given $w$. Communication efficiency is measured through surprisal, $S$, which in this setting corresponds to the average number of guesses an optimal listener takes to arrive at the correct color chip. We calculate $S(c)$ for each</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />
(a) Each circle on the chart represents the ranking of the predicted color chip when ranked according to Pearson distance from gold (larger circle $\cong$ higher/better ranking).
<img alt="img-4.jpeg" src="img-4.jpeg" />
(b) Each circle on the chart represents a color chip's suprisal score (larger circle $\cong$ higher score).</p>
<p>Figure 4: (a) shows linear mapping results for BERT, under the CC configuration, broken down by Munsell color chip; (b) shows suprisal per chip. Circle colors reflect the modal color term assigned to the chips.
chip in the color lexicon. Surprisal is defined as $S(c)=\sum_{w} P(w \mid c) \cdot \log \left(\frac{1}{P(c \mid w)}\right)$, where $P(w \mid c)$ is the probability that a color c gets labeled as $w$ and $P(c \mid w)$ is computed using Bayes Theorem. Here, $P(w)$ represents how often a particular word gets used across the color space (and participants), and $P(c)$ is a uniform prior. Figure 4 b shows surprisal per chip. High surprisal chips correspond to a lower color naming consensus among speakers, meaning that a more variable range of terms is used for these (color) contexts. We hypothesize that this could be reflected in the representations of color terms corresponding to high surprisal chips. To test this, we compute Spearman's correlation $(\rho)$ between a chip's regression score (predicted color chip code ranking) and its surprisal. We find significant Spearman's rank correlation between lower
ranking and higher surprisal for all LMs under all configurations $(0.12 \leq \rho \leq 0.17, p&lt;0.05)$.</p>
<p>What factors predict color space alignment? Given that LMs are trained exclusively on text corpora, we hypothesize that alignment between their embeddings and CIELAB is influenced by corpus usage statistics. To determine which factors could predict alignment score, we extract color term log frequency, part-of-speech tag (POS), dependency relation (DREL), and dependency tree head (HEAD) statistics for all color terms from a dependency-parsed (Straka et al., 2016) common crawl corpus. In addition to this, we compute, per color term, the entropy of its normalised PMI distribution (pmi-col, see §2) as a measure of collocation. ${ }^{11}$ We then fit a Linear Mixed Effects Model (Gałecki and Burzykowski, 2013) to the features listed above, with RSA score (Table 1) as the response variable, and model type as a random effect.</p>
<p>We follow a multi-level step-wise model building sequence, where a baseline model is first fit with color term log frequency as a single fixed effect. A model which includes pmi-col as an additional fixed effect is then fit, and these two terms are included as control predictors in all later models. Following this, we compute POS, DREL, and HEAD lemma distribution entropies per color term (pos-ent, deprel-ent, head-ent). Higher entropies indicate that the term is employed in more diverse contexts with respect to those categories. Following entropy computation, we separately fit models including each three entropy statistic features. Finally, we calculate the proportion of: POS tags that are adjectives, adj-prop; DRELs that are adjectival modifiers, amod-prop; and those that are copulas, cop-prop. The first two evaluate the effect of a color term occurring more or less often as an adjectival modifier, while the latter tests the hypothesis that assertions such as The banana is yellow could provide indirect grounding (Merrill et al., 2021), thereby leading to higher alignment. Including the entropy term which led to the best fit (deprel-ent) in the previous level, models are fit including terms for each of the proportion statistics. Model comparison is carried out by computing the log likelihood ratio between models that differ in a single term. See appendix J for model details.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Results show that:</p>
<ul>
<li>pmi-col significantly improves fit above log frequency and has a negative coefficient, meaning that terms that occur in more fixed collocations are less aligned to the perceptual space. Intuitively, this makes sense as the color terms in many collocations such as e.g. Red Army or Black Death are employed in contexts which are largely metaphorical rather than attributive or descriptive.</li>
<li>deprel-ent and head-ent (but not pos-ent) lead to a significantly improved fit compared to the control predictors; we observe positive coefficients for both, indicating RSA score is higher for terms that occur in more varied syntactic dependency relations and modify a more diverse set of syntactic heads. This suggests that occurring in a more diverse set of contexts might be beneficial for robust representation learning, in correspondence with the idea of sample diversity in the active learning literature (Brinker, 2003; Yang et al., 2015). pos-ent's lack of significance, on the other hand, indicates that the degree of specification offered by the POS tagset might be too coarse to meaningfully differentiate between color terms, e.g. nouns can occur in a variety of DRELs such as subjects, objects, oblique modifiers (per the Universal Dependecies (Nivre et al., 2020)).</li>
<li>out of the proportion statistics, only the amod-prop term improves fit; it has a positive coefficient, thus color terms occurring more frequently as adjectival modifiers show higher scores. adj-prop is not significant, providing further evidence for the POS tagset's level of granularity being too coarse. Finally, as cop-prop is not significant, it appears that occurring more frequently in assertion-like copula constructions does not confer an advantage in terms of alignment to perceptual structure.</li>
</ul>
<p>Vision-and-Language models In a preliminary set of experiments, we evaluated multi-modal Vision-and-Language models (VisualBERT (Li et al., 2019) and VideoBERT (Sun et al., 2019)), finding no major differences in results from the text-only models presented in this study.</p>
<h2>6 Related Work</h2>
<p>Distributional word representations have long been theorized to capture various types of information about the world (Schütze, 1992). Early work in this regard employed semantic similarity and relatedness datasets to measure alignment to human judgements (Agirre et al., 2009; Bruni et al., 2012; Hill et al., 2015). Rubinstein et al. (2015), however, question whether the distributional hypothesis is equally applicable to all types of semantic information, finding that taxonomic properties (such as animacy) are better modelled than attributive ones (color, size, etc.). To a similar end, Lucy and Gauthier (2017) analyze how well distributional representations encode various aspects of grounded meaning. They investigate whether language models would "be worse off for not having physically bumped into walls before they hold discussions on wall-collisions?", finding that perceptual features are poorly modelled compared to encyclopedic and taxonomic ones.</p>
<p>More recently, several studies have asked related questions in the context of language models. For example, Davison et al. (2019) and Petroni et al. (2019) mine LMs for factual and commonsense knowledge by converting knowledge base triplets into cloze statements that are used to query the models. In a similar vein, Forbes et al. (2019a) investigate LM representations' encoding of object properties (e.g., oranges are round), and affordances (e.g. oranges can be eaten), as well as the interplay between the two. Weir et al. (2020) demonstrate that LMs can capture stereotypic tacit assumptions about generic concepts, showing that they are adept at retrieving concepts given their associated properties (e.g., bear given $A$ $\qquad$ has fur, is big, and has claws.). Similar to other work, they find that LMs better model encyclopedic and functional properties than they do perceptual ones.</p>
<p>In an investigation of whether or not LMs are able to overcome reporting bias, Shwartz and Choi (2020) extract all sentences in Wikipedia where one of 11 color terms modifies a noun and test how well predicted the color term is when it is masked. They find that LMs are able to model this relationship between concepts and associated colors to a certain extent, but are prone to over-generalization. Finally, Ilharco et al. (2020) train a probe to map LM representations of textual captions to paired visual representations of image patches, in order to evaluate how useful the former are for discerning</p>
<p>between different visual representations. They find that many recent LMs yield representations that are effective at retrieving semantically-aligned image patches, but still far under-perform humans.</p>
<h2>7 Outlook</h2>
<p>It is commonly held that the learning of phenomena which rely on sensory perception is only possible through direct experience. Indeed, the view that people born blind could not be expected to acquire coherent knowledge about colors has been prevalent since at least the empiricist philosophers (Locke, 1847; Hume, 1938) and still holds currency (Jackson, 1982). Nevertheless, recent research highlighting the contribution of language and of semantic associations between concepts towards learning has demonstrated that the congenitally blind do in fact show a striking understanding of both color similarity (Saysani et al., 2018) and object colors (Kim et al., 2020).</p>
<p>This paper investigated whether representations of color terms that are derived from text only express a degree of isomorphism to the structure of humans' perceptual color space. ${ }^{12}$ Results from our experiments evidenced that such a topological correspondence exists. Notably, color term representations based on simple co-occurance statistics already demonstrated correspondence; those extracted from language models aligned more closely. We observed that warm colors, on average, show more alignment than cooler ones, linking to recent findings on communication efficiency in color naming (Gibson et al., 2017).</p>
<p>Further analysis based on surprisal - an information theoretic measure, used to evaluate how efficiently a color is communicated between a speaker and a listener - revealed a correlation between lower topological alignment and higher color chip surprisal, suggesting that the kind of contexts a color occurs in play a role in determining alignment. Exploring this, we tested a set of color term corpus-derived statistics for how well they predict alignment, finding that a measure of a color term's collocationality corresponds to lower alignment, while the entropy of its dependency relation distribution and it occurring more frequently as and adjectival modifier correspond to closer alignment.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Our results and analyses present empirical evidence of topological alignment between text-based color term representations and perceptual color spaces. With respect to the debate started by Bender and Koller (2020), we hope that this work offers a modest step towards furthering our understanding of the kinds of "meaning" we expect language models to acquire, with and without grounded or embodied learning approaches, and that it will provide motivation for further work in this direction.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank Vinit Ravishankar and Mitja Nikolaus for their feedback and comments. Mostafa Abdou and Anders Søgaard are supported by a Google Focused Research Award and a Facebook Research Award.</p>
<h2>References</h2>
<p>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches.</p>
<p>Yonatan Belinkov. 2021. Probing classifiers: Promises, shortcomings, and alternatives. arXiv preprint arXiv:2102.12452.</p>
<p>Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On meaning, form, and understanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185-5198, Online. Association for Computational Linguistics.</p>
<p>Brent Berlin and Paul Kay. 1991. Basic color terms: Their universality and evolution. Univ of California Press.</p>
<p>Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, et al. 2020. Experience grounds language. arXiv preprint arXiv:2004.10151.</p>
<p>Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135-146.</p>
<p>Rishi Bommasani, Kelly Davis, and Claire Cardie. 2020. Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 47584781, Online. Association for Computational Linguistics.</p>
<p>Klaus Brinker. 2003. Incorporating diversity in active learning with support vector machines. In Proceedings of the 20th international conference on machine learning (ICML-03), pages 59-66.</p>
<p>Elia Bruni, Gemma Boleda, Marco Baroni, and NamKhanh Tran. 2012. Distributional semantics in technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 136-145.</p>
<p>Rahma Chaabouni, Eugene Kharitonov, Emmanuel Dupoux, and Marco Baroni. 2021. Communicating artificial neural networks develop efficient color-naming systems. Proceedings of the National Academy of Sciences, 118(12).</p>
<p>M Chastrette. 1997. Trends in structure-odor relationship. SAR and QSAR in Environmental Research, $6(3-4): 215-254$.</p>
<p>Lin Chen. 1982. Topological structure in visual perception. Science, 218(4573):699-700.</p>
<p>Gabriella Chronis and Katrin Erk. 2020. When is a bishop not like a rook? when it's like a rabbi! multiprototype BERT embeddings for estimating semantic relationships. In Proceedings of the 24th Conference on Computational Natural Language Learning, pages 227-244, Online. Association for Computational Linguistics.</p>
<p>Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2020. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555.</p>
<p>Joe Davison, Joshua Feldman, and Alexander Rush. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 1173-1178, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Nadir Durrani, Hassan Sajjad, Fahim Dalvi, and Yonatan Belinkov. 2020. Analyzing individual neurons in pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4865-4880, Online. Association for Computational Linguistics.</p>
<p>Robert J Ennis and Qasim Zaidi. 2019. Geometrical structure of perceptual color space: mental representations and adaptation invariance. Journal of vision, 19(12):1-1.</p>
<p>Maxwell Forbes, Ari Holtzman, and Yejin Choi. 2019a. Do neural language representations learn physical commonsense? arXiv preprint arXiv:1908.02899.</p>
<p>Maxwell Forbes, Christine Kaeser-Chen, Piyush Sharma, and Serge Belongie. 2019b. Neural naturalist: Generating fine-grained image comparisons. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 708717, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Andrzej Gałecki and Tomasz Burzykowski. 2013. Linear mixed-effects model. In Linear Mixed-Effects Models Using R, pages 245-273. Springer.</p>
<p>Edward Gibson, Richard Futrell, Julian Jara-Ettinger, Kyle Mahowald, Leon Bergen, Sivalogeswaran Ratnasingam, Mitchell Gibson, Steven T Piantadosi, and Bevil R Conway. 2017. Color naming across languages reflects color use. Proceedings of the National Academy of Sciences, 114(40):10785-10790.</p>
<p>Evan Hernandez and Jacob Andreas. 2021. The lowdimensional linear geometry of contextualized word representations. arXiv preprint arXiv:2105.07109.</p>
<p>John Hewitt and Percy Liang. 2019. Designing and interpreting probes with control tasks. arXiv preprint arXiv:1909.03368.</p>
<p>Felix Hill, Roi Reichart, and Anna Korhonen. 2015. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. Computational Linguistics, 41(4):665-695.</p>
<p>David Hume. 1938. An Abstract of a Treatise of Human Nature, 1740. CUP Archive.</p>
<p>Gabriel Ilharco, Rowan Zellers, Ali Farhadi, and Hannaneh Hajishirzi. 2020. Probing text models for common ground with visual representations. arXiv preprint arXiv:2005.00619.</p>
<p>Frank Jackson. 1982. Epiphenomenal qualia. The Philosophical Quarterly (1950-), 32(127):127-136.</p>
<p>Paul Kay, Brent Berlin, Luisa Maffi, William R Merrifield, and Richard Cook. 2009. The world color survey. CSLI Publications Stanford, CA.</p>
<p>Paul Kay, Brent Berlin, and William Merrifield. 1991. Biocultural implications of systems of color naming. Journal of Linguistic Anthropology, 1(1):12-25.</p>
<p>Paul Kay and Chad K McDaniel. 1978. The linguistic significance of the meanings of basic color terms. Language, pages 610-646.</p>
<p>Judy Sein Kim, Brianna Aheimer, Verónica Montané Manrara, and Marina Bedny. 2020. Shared understanding of color among congenitally blind and sighted adults.</p>
<p>Nikolaus Kriegeskorte, Marieke Mur, and Peter A Bandettini. 2008. Representational similarity analysisconnecting the branches of systems neuroscience. Frontiers in systems neuroscience, 2:4.</p>
<p>Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557.</p>
<p>Delwin T Lindsey and Angela M Brown. 2014. The color lexicon of american english. Journal of vision, 14(2):17-17.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>John Locke. 1847. An essay concerning human understanding. Kay \&amp; Troutman.</p>
<p>Li Lucy and Jon Gauthier. 2017. Are distributional representations ready for the real world? evaluating word vectors for grounded perceptual meaning. arXiv preprint arXiv:1705.11168.</p>
<p>Ken McRae, George S Cree, Mark S Seidenberg, and Chris McNorgan. 2005. Semantic feature production norms for a large set of living and nonliving things. Behavior research methods, 37(4):547-559.</p>
<p>William Merrill, Yoav Goldberg, Roy Schwartz, and Noah A Smith. 2021. Provable limitations of acquiring meaning from ungrounded form: What will future language models understand? arXiv preprint arXiv:2104.10809.</p>
<p>Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Hajič, Christopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. 2020. Universal Dependencies v2: An evergrowing multilingual treebank collection. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 4034-4043, Marseille, France. European Language Resources Association.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 2463-2473, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Tiago Pimentel, Naomi Saphra, Adina Williams, and Ryan Cotterell. 2020. Pareto probing: Trading off accuracy for complexity. arXiv preprint arXiv:2010.02180.</p>
<p>Edoardo Provenzi. 2020. Geometry of color perception. part 1: structures and metrics of a homogeneous color space. The Journal of Mathematical Neuroscience, 10(1):1-19.</p>
<p>Terry Regier, Paul Kay, and Naveen Khetarpal. 2007. Color naming reflects optimal partitions of color space. Proceedings of the National Academy of Sciences, 104(4):1436-1441.</p>
<p>Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Andy Coenen, Adam Pearce, and Been Kim. 2019a. Visualizing and measuring the geometry of bert. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.</p>
<p>Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Andy Coenen, Adam Pearce, and Been Kim. 2019b. Visualizing and measuring the geometry of bert. Advances in Neural Information Processing Systems, 32:8594-8603.</p>
<p>Karen J Rossiter. 1996. Structure- odor relationships. Chemical reviews, 96(8):3201-3240.</p>
<p>Dana Rubinstein, Effi Levi, Roy Schwartz, and Ari Rappoport. 2015. How well do distributional models capture different types of semantic knowledge? In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 726-730.</p>
<p>Armin Saysani, Michael C Corballis, and Paul M Corballis. 2018. Colour envisioned: Concepts of colour in the blind and sighted. Visual Cognition, 26(5):382-392.</p>
<p>Hinrich Schütze. 1992. Dimensions of meaning. In SC, pages 787-796.</p>
<p>Vered Shwartz and Yejin Choi. 2020. Do neural language models overcome reporting bias? In Proceedings of the 28th International Conference on Computational Linguistics, pages 6863-6870, Barcelona, Spain (Online). International Committee on Computational Linguistics.</p>
<p>Milan Straka, Jan Hajic, and Jana Straková. 2016. Udpipe: trainable pipeline for processing conll-u files performing tokenization, morphological analysis, pos tagging and parsing. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 42904297.</p>
<p>Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. 2019. Videobert: A joint model for video and language representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7464-7473.</p>
<p>Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 45934601, Florence, Italy. Association for Computational Linguistics.</p>
<p>Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Well-read students learn better: On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962.</p>
<p>Ivan Vulić, Edoardo Maria Ponti, Robert Litschko, Goran Glavaš, and Anna Korhonen. 2020. Probing pretrained language models for lexical semantics. arXiv preprint arXiv:2010.05731.</p>
<p>Nathaniel Weir, Adam Poliak, and Benjamin Van Durme. 2020. Probing neural language models for human tacit assumptions.</p>
<p>Yi Yang, Zhigang Ma, Feiping Nie, Xiaojun Chang, and Alexander G Hauptmann. 2015. Multi-class active learning by uncertainty sampling with diversity maximization. International Journal of Computer Vision, 113(2):113-127.</p>
<p>Noga Zaslavsky, Charles Kemp, Terry Regier, and Naftali Tishby. 2018. Efficient compression in color naming and its evolution. Proceedings of the National Academy of Sciences, 115(31):7937-7942.</p>
<h2>A List of included color terms</h2>
<p>Red, green, maroon, brown, black, blue, purple, orange, pink, yellow, peach, white, gray, olive, turquoise, violet, lavender, and aqua.</p>
<h2>B RSA between models</h2>
<p>Figure 5 shows a the result of representation similarity analysis between the representations derived from all models (and configurations) as well as CIELAB, showing Kendall's correlation coefficient between flattened RSMs.</p>
<h2>C Representation Similarity Matrices</h2>
<p>Figures 6 to 9 show the representation similarity matrices employed for the RSA analyses, for the layer with the highest RSA score from each of the controlled-context (CC) models.</p>
<h2>D Warm vs. Cool colors</h2>
<p>Figures 10 and 11 show Linear Mapping and RSA results broken down by color temperature. The color space is split according to temperature measured according to the Hue dimension in the Hue-Value-Saturation space ${ }^{13}$.</p>
<h2>E Corpus statistics</h2>
<p>Figures 12 and 13 show log frequency and entropy of distributions over part-of-speech categories, dependency relations, and lemmas of dependency tree heads of color terms in common crawl.</p>
<h2>F Linear mapping results by munsell color chip</h2>
<p>Figure 14 shows linear mapping results broken down by Munsell chip for all models and configurations.</p>
<h2>G Linear mapping control task and probe complexity</h2>
<p>Figure 15 shows the full results over a range of probe complexities for the standard experimental condition as well the random control task.</p>
<h2>H Dimensionality of color subspace</h2>
<p>Figure 16 shows the proportion of explained variance with respect to the number of dimensions which are assigned $95 \%$ of the linear regression coefficient weights.</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>I Effect of model size</h2>
<p>Our model size experiments are run using four BERT models of different sizes: BERT-mini (4 layers, hidden size: 256), BERT-small (4 layers, hidden size: 512), BERT-medium (8 layers, hidden size: 512), and BERT-base (12 layers, hidden size: 768). Further model specification and training details for the first three can be found in Turc et al. (2019) and for last in Devlin et al. (2019).</p>
<h2>J Linear Mixed Effects Model</h2>
<p>To fit Linear Mixed Effects Models, we use the LME4 package. With model type (BERTCC, RoBERTa-NC, etc.) as a random effect, we follow a step-wise model construction sequence which proceeds along four levels of nesting: (i) in the first level color logfrequency is the only fixed effect, (ii) in the second pmi-colloc is added to that, (iii) in the third, each of pos-ent, deprel-ent, head-ent is added separately to the a model with log frequency and pmi-colloc, (iv) the term that leads to the best fit from the previous level deprel-ent is included, then each of the proportion terms adj-prop, amod-prop, cop-prop is added. The reported regression coefficients are extracted from the minimal model containing each term.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Result of representation similarity analysis between all models (and configurations), showing Kendall's correlation coefficient between flattened RSMs. Results are shown for layers which are maximally correlated with CIELAB, per model. -rc indicates random-context, -cc indicates controlled-context, and -nc indicates non-context.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: CIELAB RSM</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: BERT(CC) RSM</p>
<p>RoBERTa (large), controlled context, emb.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 8: RoBERTa(CC) RSM</p>
<p>Electra (large), controlled context, emb.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 9: ELECTRA(CC) RSM</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 10: Linear mapping results (proportion of explained variance) broken down by color chip temperature for each of the baselines and the LMs.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 11: RSA results (Kendall's $\tau$ ) broken down by color temperature for each for each of the baselines and the LMs.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 12: Log frequency of color terms in common crawl.</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 13: Entropy of distributions over part-of-speech categories, dependency relations, and lemmas of dependency tree heads of color terms in common crawl.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{13}$ https://psychology.wikia.org/wiki/ HSV_color_space&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{8}$ We use the colormath Python package, setting illuminant to C, and assuming 2 degree standard observer.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
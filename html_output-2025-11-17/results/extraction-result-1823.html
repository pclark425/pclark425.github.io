<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1823 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1823</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1823</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-af518750f36b6304265b39f5848d99b5b3238b36</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/af518750f36b6304265b39f5848d99b5b3238b36" target="_blank">Grasp Stability Prediction with Sim-to-Real Transfer from Tactile Sensing</a></p>
                <p><strong>Paper Venue:</strong> IEEE/RJS International Conference on Intelligent RObots and Systems</p>
                <p><strong>Paper TL;DR:</strong> This work integrates simulation of robot dynamics and vision-based tactile sensors by modeling the physics of contact, and uses simulated contact forces at the robot's end-effector to inform the generation of realistic tactile outputs.</p>
                <p><strong>Paper Abstract:</strong> Robot simulation has been an essential tool for data-driven manipulation tasks. However, most existing simulation frameworks lack either efficient and accurate models of physical interactions with tactile sensors or realistic tactile simulation. This makes the sim-to-real transfer for tactile-based manipulation tasks still challenging. In this work, we integrate simulation of robot dynamics and vision-based tactile sensors by modeling the physics of contact. This contact model uses simulated contact forces at the robot's end-effector to inform the generation of realistic tactile outputs. To eliminate the sim-to-real transfer gap, we calibrate our physics simulator of robot dynamics, contact model, and tactile optical simulator with real-world data, and then we demonstrate the effectiveness of our system on a zero-shot sim-to-real grasp stability prediction task where we achieve an average accuracy of 90.7% on various objects. Experiments reveal the potential of applying our simulation framework to more complicated manipulation tasks. We open-source our simulation framework at https://github.com/CMURoboTouch/Taxim/tree/taxim-robot.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1823.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1823.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Integrated PyBullet+Contact+Taxim sim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Integrated physics + contact-dynamics + Taxim tactile simulation (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulation pipeline that couples PyBullet rigid-body physics, a calibrated simplified contact model mapping normal/shear forces to GelSight indentation and shear, PyRender contact-map rendering, and Taxim optical tactile rendering; used to train grasp-stability predictors in simulation and transfer them zero-shot to a real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>UR5e + Weiss WSG-50 gripper with GelSight sensor</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A 6-DOF UR5e manipulator equipped with a Weiss WSG-50 parallel-jaw gripper on which a GelSight high-resolution vision-based tactile sensor is mounted; used to execute grasp trials and record tactile images during grasping and lifting.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (tactile-based grasping)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet + PyRender + Taxim (custom contact model)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>PyBullet rigid-body physics simulates robot dynamics, collisions and contact forces; a custom contact model maps simulated contact forces/poses to indentation volumes and shear displacements; PyRender generates a contact map (virtual depth camera) and Taxim renders RGB GelSight-like tactile images using a calibrated lookup table.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-fidelity tactile optical rendering combined with calibrated rigid-body physics and an approximate (linear) contact-deformation model; not full soft-body FEM.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>contact dynamics (normal and shear forces from PyBullet), friction, object mass and center-of-mass, indentation volume <-> depth mapping (iterative reconstruction), collision margin calibration, and tactile optics (Taxim lookup-table rendering).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>No full soft-body finite-element deformation of the sensor; contact deformation approximated via linear mappings (V = k_n F_n, D = k_s F_s) and iterative reconstruction of depth maps; approximated contact-volume→depth solution via binary search; relied on calibrated lookup-table rather than physics-based light transport for optics.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical UR5e robot with WSG-50 gripper and a real GelSight sensor; objects from YCB and Google Scanned Objects datasets with measured masses and COMs; grasp trials executed with varying grasp locations/heights/forces and lift outcomes labeled (success/failure).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Binary grasp stability prediction (successful lift vs failure) from tactile images (single or temporal sequences) trained entirely in simulation and tested zero-shot on real tactile images.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised learning: ResNet-18 feature extractor (pretrained) + MLP classifier for single images; ResNet-18 features + LSTM + MLP for sequential images.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Classification accuracy (%) on held-out real-world grasp trials (per-object and averaged over objects).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Sim2Sim (Single): average ~96.8% accuracy; Sim2Sim (Sequential): 100% (reported per-table averages).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Sim2Real (Single): average ~86.4% accuracy; Sim2Real (Sequential): average 90.7% accuracy (reported averages across 12 objects).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Mismatch in friction coefficients, incorrect object mass/center-of-mass, mesh/model coarseness (simulation mesh vs real object), inaccuracies in contact-deformation modeling (soft-body effects), PyBullet collision margin artifacts, and sensor-model mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Calibrating physics parameters (friction, object mass/COM), integrating a contact model that maps simulated forces to indentation/shear, using Taxim optical simulator calibrated with the real GelSight, and tuning the simulator friction using a small set (~20) of real grasps per object to match outcome distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Accurate contact dynamics (mapping forces to deformation) and calibrated tactile optical rendering are critical; incorrect friction or COM values significantly reduce Sim2Real accuracy; dataset sizes of ~100–200 simulated grasps per object suffice for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>The simulator's friction coefficient was tuned per object using ≈20 real-world grasp examples (held out from test set) by searching friction in [0,1] with step 0.05 to minimize mismatched grasp labels between sim and real; this is simulator calibration rather than model fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Comparing to a baseline tactile simulator (TACTO) that uses a simpler contact model, the paper's integrated simulator achieved much higher Sim2Real accuracy: Sim2Real (Sequential) 90.7% vs TACTO Sim2Real average ~42.6%; ablations show Sim2Real accuracy drops sharply when friction or COM are inaccurate (example: for potted meat can, friction 0.45 produced Sim2Real 0.958 vs friction 0.8 gave 0.541).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Integrating calibrated contact-dynamics modeling with a calibrated optical tactile simulator (Taxim) enables high-quality zero-shot sim-to-real transfer for tactile-based grasp stability prediction (≈90.7% sequential accuracy). Physics parameter mismatch (friction, COM) and coarse meshes are major causes of transfer failure; modest simulated dataset sizes (100–200 trials) suffice; purely geometric/optical tactile simulation (without accurate contact dynamics) is insufficient for reliable sim-to-real grasp prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grasp Stability Prediction with Sim-to-Real Transfer from Tactile Sensing', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1823.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1823.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TACTO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tacto: A fast, flexible, and open-source simulator for high-resolution visionbased tactile sensors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source tactile simulator (uses pyrender) that renders tactile images from contact shapes and is commonly combined with physics simulators (e.g., PyBullet) but, in the evaluated form, uses a simpler contact model than the integrated pipeline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tacto: A fast, flexible, and open-source simulator for high-resolution visionbased tactile sensors</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>UR5e + WSG-50 + GelSight (baseline evaluation setup)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Same physical robot/gripper/sensor setup as used by the authors; TACTO was used to generate simulated tactile images for training baseline predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (tactile-based grasping)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>TACTO (pyrender) combined with a physics engine (e.g., PyBullet)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>TACTO renders tactile images by placing virtual geometry and using pyrender to produce images from contact shapes; typically used alongside a physics engine to provide contact shapes/poses.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-resolution optical tactile rendering (geometric), but with a simplified/incorrect contact-deformation model in the evaluated configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Contact shape rendering and optical appearance of tactile readings.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Contact-deformation physics approximated (assumed normal force linear to indentation depth), insufficient handling of collision margin nuances, and no calibrated mapping from simulated forces to true sensor deformation—leading to inaccurate tactile signals in some contact scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Same real grasping testbed (UR5e + GelSight + YCB/Google objects) used to evaluate Sim2Real performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Grasp stability prediction from tactile images trained in simulation and tested on real tactile images (baseline comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised learning (same architectures as main pipeline: ResNet-18 features + MLP/LSTM classifiers).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Classification accuracy on real grasp trials.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Sim2Sim (TACTO baseline) average reported ≈84.1% (per-table average across objects for single-image sim-to-sim).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Sim2Real (TACTO baseline) average reported ≈42.6% (per-table average across objects for single-image sim-to-real).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Inaccurate contact model assumptions (linear force→indentation), collision margin handling issues, and lack of calibrated mapping from contact forces to sensor deformation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper identifies that a correct contact-deformation model and careful collision margin handling are required; TACTO's simpler assumptions limited Sim2Real performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>TACTO's Sim2Real performance (~42.6%) was much lower than the integrated calibrated pipeline (≈90.7% sequential), indicating that optical rendering alone with a simplified contact model is insufficient for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>TACTO can provide high-quality optical tactile images for some perception tasks, but when used with a simplistic contact model its Sim2Real performance for grasp-stability prediction is poor; accurate contact dynamics modeling and collision-margin handling are necessary to reach high transfer performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grasp Stability Prediction with Sim-to-Real Transfer from Tactile Sensing', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1823.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1823.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Taxim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Taxim: An example-based simulation model for gelsight tactile sensors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An example-based optical tactile simulator that uses a calibrated lookup table to map contact shapes (contact maps) to realistic GelSight-like RGB tactile images; integrated by the authors to render tactile readings from simulated contact maps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Taxim: An example-based simulation model for gelsight tactile sensors</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Taxim (tactile rendering module) within the simulation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A tactile-image renderer that takes contact map geometry as input and produces high-fidelity GelSight-like images using a lookup-table calibrated to a real sensor.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics tactile sensing / perception</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Taxim (example-based tactile image simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Optical tactile simulator that models GelSight sensor appearance by mapping 3D contact geometry to RGB images via a calibrated lookup table derived from real sensor data.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-fidelity optical tactile rendering (example-based, calibrated), but not a physics-based light transport model.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Tactile optics and image appearance (marker motion, texture), mapping from contact geometry to RGB tactile images.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Does not model contact-deformation physics itself (expects contact map input); uses example/look-up rather than full light transport or volumetric rendering; depends on accuracy of input contact map.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Calibrated against a real GelSight sensor used in the robotic testbed; lookup table derived from real sensor data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Contributed to sim-to-real transfer of tactile-image-based grasp stability prediction by producing realistic tactile images from simulated contacts.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Used to generate supervised training images in simulation (ResNet+MLP/LSTM models trained on Taxim outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Contributed to final Sim2Real accuracy (see integrated pipeline metrics); no separate standalone Sim2Real metric reported for Taxim only.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Accuracy depends on fidelity of contact-map input and calibration quality; mismatches in contact geometry (mesh coarseness) or in contact-deformation mapping will reduce realism.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Lookup-table calibration with the real sensor; combining Taxim with a contact model that provides realistic contact maps and calibrated physics parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Optical tactile rendering should be calibrated to the real sensor; optical realism alone is necessary but not sufficient—must be paired with accurate contact-dynamics modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Taxim's lookup table is calibrated with the real GelSight sensor used in experiments (calibration procedure and dataset referenced but not exhaustively quantified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High-quality, calibrated optical tactile simulation (Taxim) is an essential component of successful sim-to-real for tactile perception tasks, but it must be combined with an accurate contact-dynamics model and calibrated physics parameters to achieve reliable zero-shot transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grasp Stability Prediction with Sim-to-Real Transfer from Tactile Sensing', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tacto: A fast, flexible, and open-source simulator for high-resolution visionbased tactile sensors <em>(Rating: 2)</em></li>
                <li>Taxim: An example-based simulation model for gelsight tactile sensors <em>(Rating: 2)</em></li>
                <li>Tactile sim-to-real policy transfer via real-to-sim image translation <em>(Rating: 2)</em></li>
                <li>Learning deep policies for robot bin picking by simulating robust grasping sequences <em>(Rating: 2)</em></li>
                <li>Sim-to-real for robotic tactile sensing via physics-based simulation and learned latent projections <em>(Rating: 2)</em></li>
                <li>Mat: Multi-fingered adaptive tactile grasping via deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Stable grasping under pose uncertainty using tactile feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1823",
    "paper_id": "paper-af518750f36b6304265b39f5848d99b5b3238b36",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Integrated PyBullet+Contact+Taxim sim",
            "name_full": "Integrated physics + contact-dynamics + Taxim tactile simulation (this paper)",
            "brief_description": "A simulation pipeline that couples PyBullet rigid-body physics, a calibrated simplified contact model mapping normal/shear forces to GelSight indentation and shear, PyRender contact-map rendering, and Taxim optical tactile rendering; used to train grasp-stability predictors in simulation and transfer them zero-shot to a real robot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "UR5e + Weiss WSG-50 gripper with GelSight sensor",
            "agent_system_description": "A 6-DOF UR5e manipulator equipped with a Weiss WSG-50 parallel-jaw gripper on which a GelSight high-resolution vision-based tactile sensor is mounted; used to execute grasp trials and record tactile images during grasping and lifting.",
            "domain": "general robotics manipulation (tactile-based grasping)",
            "virtual_environment_name": "PyBullet + PyRender + Taxim (custom contact model)",
            "virtual_environment_description": "PyBullet rigid-body physics simulates robot dynamics, collisions and contact forces; a custom contact model maps simulated contact forces/poses to indentation volumes and shear displacements; PyRender generates a contact map (virtual depth camera) and Taxim renders RGB GelSight-like tactile images using a calibrated lookup table.",
            "simulation_fidelity_level": "high-fidelity tactile optical rendering combined with calibrated rigid-body physics and an approximate (linear) contact-deformation model; not full soft-body FEM.",
            "fidelity_aspects_modeled": "contact dynamics (normal and shear forces from PyBullet), friction, object mass and center-of-mass, indentation volume &lt;-&gt; depth mapping (iterative reconstruction), collision margin calibration, and tactile optics (Taxim lookup-table rendering).",
            "fidelity_aspects_simplified": "No full soft-body finite-element deformation of the sensor; contact deformation approximated via linear mappings (V = k_n F_n, D = k_s F_s) and iterative reconstruction of depth maps; approximated contact-volume→depth solution via binary search; relied on calibrated lookup-table rather than physics-based light transport for optics.",
            "real_environment_description": "Physical UR5e robot with WSG-50 gripper and a real GelSight sensor; objects from YCB and Google Scanned Objects datasets with measured masses and COMs; grasp trials executed with varying grasp locations/heights/forces and lift outcomes labeled (success/failure).",
            "task_or_skill_transferred": "Binary grasp stability prediction (successful lift vs failure) from tactile images (single or temporal sequences) trained entirely in simulation and tested zero-shot on real tactile images.",
            "training_method": "Supervised learning: ResNet-18 feature extractor (pretrained) + MLP classifier for single images; ResNet-18 features + LSTM + MLP for sequential images.",
            "transfer_success_metric": "Classification accuracy (%) on held-out real-world grasp trials (per-object and averaged over objects).",
            "transfer_performance_sim": "Sim2Sim (Single): average ~96.8% accuracy; Sim2Sim (Sequential): 100% (reported per-table averages).",
            "transfer_performance_real": "Sim2Real (Single): average ~86.4% accuracy; Sim2Real (Sequential): average 90.7% accuracy (reported averages across 12 objects).",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Mismatch in friction coefficients, incorrect object mass/center-of-mass, mesh/model coarseness (simulation mesh vs real object), inaccuracies in contact-deformation modeling (soft-body effects), PyBullet collision margin artifacts, and sensor-model mismatches.",
            "transfer_enabling_conditions": "Calibrating physics parameters (friction, object mass/COM), integrating a contact model that maps simulated forces to indentation/shear, using Taxim optical simulator calibrated with the real GelSight, and tuning the simulator friction using a small set (~20) of real grasps per object to match outcome distributions.",
            "fidelity_requirements_identified": "Accurate contact dynamics (mapping forces to deformation) and calibrated tactile optical rendering are critical; incorrect friction or COM values significantly reduce Sim2Real accuracy; dataset sizes of ~100–200 simulated grasps per object suffice for this task.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "The simulator's friction coefficient was tuned per object using ≈20 real-world grasp examples (held out from test set) by searching friction in [0,1] with step 0.05 to minimize mismatched grasp labels between sim and real; this is simulator calibration rather than model fine-tuning.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Comparing to a baseline tactile simulator (TACTO) that uses a simpler contact model, the paper's integrated simulator achieved much higher Sim2Real accuracy: Sim2Real (Sequential) 90.7% vs TACTO Sim2Real average ~42.6%; ablations show Sim2Real accuracy drops sharply when friction or COM are inaccurate (example: for potted meat can, friction 0.45 produced Sim2Real 0.958 vs friction 0.8 gave 0.541).",
            "key_findings": "Integrating calibrated contact-dynamics modeling with a calibrated optical tactile simulator (Taxim) enables high-quality zero-shot sim-to-real transfer for tactile-based grasp stability prediction (≈90.7% sequential accuracy). Physics parameter mismatch (friction, COM) and coarse meshes are major causes of transfer failure; modest simulated dataset sizes (100–200 trials) suffice; purely geometric/optical tactile simulation (without accurate contact dynamics) is insufficient for reliable sim-to-real grasp prediction.",
            "uuid": "e1823.0",
            "source_info": {
                "paper_title": "Grasp Stability Prediction with Sim-to-Real Transfer from Tactile Sensing",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "TACTO",
            "name_full": "Tacto: A fast, flexible, and open-source simulator for high-resolution visionbased tactile sensors",
            "brief_description": "An open-source tactile simulator (uses pyrender) that renders tactile images from contact shapes and is commonly combined with physics simulators (e.g., PyBullet) but, in the evaluated form, uses a simpler contact model than the integrated pipeline in this paper.",
            "citation_title": "Tacto: A fast, flexible, and open-source simulator for high-resolution visionbased tactile sensors",
            "mention_or_use": "use",
            "agent_system_name": "UR5e + WSG-50 + GelSight (baseline evaluation setup)",
            "agent_system_description": "Same physical robot/gripper/sensor setup as used by the authors; TACTO was used to generate simulated tactile images for training baseline predictors.",
            "domain": "general robotics manipulation (tactile-based grasping)",
            "virtual_environment_name": "TACTO (pyrender) combined with a physics engine (e.g., PyBullet)",
            "virtual_environment_description": "TACTO renders tactile images by placing virtual geometry and using pyrender to produce images from contact shapes; typically used alongside a physics engine to provide contact shapes/poses.",
            "simulation_fidelity_level": "high-resolution optical tactile rendering (geometric), but with a simplified/incorrect contact-deformation model in the evaluated configuration.",
            "fidelity_aspects_modeled": "Contact shape rendering and optical appearance of tactile readings.",
            "fidelity_aspects_simplified": "Contact-deformation physics approximated (assumed normal force linear to indentation depth), insufficient handling of collision margin nuances, and no calibrated mapping from simulated forces to true sensor deformation—leading to inaccurate tactile signals in some contact scenarios.",
            "real_environment_description": "Same real grasping testbed (UR5e + GelSight + YCB/Google objects) used to evaluate Sim2Real performance.",
            "task_or_skill_transferred": "Grasp stability prediction from tactile images trained in simulation and tested on real tactile images (baseline comparison).",
            "training_method": "Supervised learning (same architectures as main pipeline: ResNet-18 features + MLP/LSTM classifiers).",
            "transfer_success_metric": "Classification accuracy on real grasp trials.",
            "transfer_performance_sim": "Sim2Sim (TACTO baseline) average reported ≈84.1% (per-table average across objects for single-image sim-to-sim).",
            "transfer_performance_real": "Sim2Real (TACTO baseline) average reported ≈42.6% (per-table average across objects for single-image sim-to-real).",
            "transfer_success": false,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Inaccurate contact model assumptions (linear force→indentation), collision margin handling issues, and lack of calibrated mapping from contact forces to sensor deformation.",
            "transfer_enabling_conditions": null,
            "fidelity_requirements_identified": "Paper identifies that a correct contact-deformation model and careful collision margin handling are required; TACTO's simpler assumptions limited Sim2Real performance.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "TACTO's Sim2Real performance (~42.6%) was much lower than the integrated calibrated pipeline (≈90.7% sequential), indicating that optical rendering alone with a simplified contact model is insufficient for this task.",
            "key_findings": "TACTO can provide high-quality optical tactile images for some perception tasks, but when used with a simplistic contact model its Sim2Real performance for grasp-stability prediction is poor; accurate contact dynamics modeling and collision-margin handling are necessary to reach high transfer performance.",
            "uuid": "e1823.1",
            "source_info": {
                "paper_title": "Grasp Stability Prediction with Sim-to-Real Transfer from Tactile Sensing",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Taxim",
            "name_full": "Taxim: An example-based simulation model for gelsight tactile sensors",
            "brief_description": "An example-based optical tactile simulator that uses a calibrated lookup table to map contact shapes (contact maps) to realistic GelSight-like RGB tactile images; integrated by the authors to render tactile readings from simulated contact maps.",
            "citation_title": "Taxim: An example-based simulation model for gelsight tactile sensors",
            "mention_or_use": "use",
            "agent_system_name": "Taxim (tactile rendering module) within the simulation pipeline",
            "agent_system_description": "A tactile-image renderer that takes contact map geometry as input and produces high-fidelity GelSight-like images using a lookup-table calibrated to a real sensor.",
            "domain": "robotics tactile sensing / perception",
            "virtual_environment_name": "Taxim (example-based tactile image simulator)",
            "virtual_environment_description": "Optical tactile simulator that models GelSight sensor appearance by mapping 3D contact geometry to RGB images via a calibrated lookup table derived from real sensor data.",
            "simulation_fidelity_level": "high-fidelity optical tactile rendering (example-based, calibrated), but not a physics-based light transport model.",
            "fidelity_aspects_modeled": "Tactile optics and image appearance (marker motion, texture), mapping from contact geometry to RGB tactile images.",
            "fidelity_aspects_simplified": "Does not model contact-deformation physics itself (expects contact map input); uses example/look-up rather than full light transport or volumetric rendering; depends on accuracy of input contact map.",
            "real_environment_description": "Calibrated against a real GelSight sensor used in the robotic testbed; lookup table derived from real sensor data.",
            "task_or_skill_transferred": "Contributed to sim-to-real transfer of tactile-image-based grasp stability prediction by producing realistic tactile images from simulated contacts.",
            "training_method": "Used to generate supervised training images in simulation (ResNet+MLP/LSTM models trained on Taxim outputs).",
            "transfer_success_metric": "Contributed to final Sim2Real accuracy (see integrated pipeline metrics); no separate standalone Sim2Real metric reported for Taxim only.",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Accuracy depends on fidelity of contact-map input and calibration quality; mismatches in contact geometry (mesh coarseness) or in contact-deformation mapping will reduce realism.",
            "transfer_enabling_conditions": "Lookup-table calibration with the real sensor; combining Taxim with a contact model that provides realistic contact maps and calibrated physics parameters.",
            "fidelity_requirements_identified": "Optical tactile rendering should be calibrated to the real sensor; optical realism alone is necessary but not sufficient—must be paired with accurate contact-dynamics modeling.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Taxim's lookup table is calibrated with the real GelSight sensor used in experiments (calibration procedure and dataset referenced but not exhaustively quantified in this paper).",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "High-quality, calibrated optical tactile simulation (Taxim) is an essential component of successful sim-to-real for tactile perception tasks, but it must be combined with an accurate contact-dynamics model and calibrated physics parameters to achieve reliable zero-shot transfer.",
            "uuid": "e1823.2",
            "source_info": {
                "paper_title": "Grasp Stability Prediction with Sim-to-Real Transfer from Tactile Sensing",
                "publication_date_yy_mm": "2022-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tacto: A fast, flexible, and open-source simulator for high-resolution visionbased tactile sensors",
            "rating": 2
        },
        {
            "paper_title": "Taxim: An example-based simulation model for gelsight tactile sensors",
            "rating": 2
        },
        {
            "paper_title": "Tactile sim-to-real policy transfer via real-to-sim image translation",
            "rating": 2
        },
        {
            "paper_title": "Learning deep policies for robot bin picking by simulating robust grasping sequences",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-real for robotic tactile sensing via physics-based simulation and learned latent projections",
            "rating": 2
        },
        {
            "paper_title": "Mat: Multi-fingered adaptive tactile grasping via deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Stable grasping under pose uncertainty using tactile feedback",
            "rating": 1
        }
    ],
    "cost": 0.014942500000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Grasp Stability Prediction with Sim-to-Real Transfer from Tactile Sensing</h1>
<p>Zilin Si^{1}, Zirui Zhu^{2}, Arpit Agarwal^{1}, Stuart Anderson^{3} and Wenzhen Yuan^{1}
[^{1}Zilin Si, Arpit Agarwal and Wenzhen Yuan are with the Robotics Institute, Carnegie Mellon University (zsi, arpital, wenzheny)@andrew.cmu.edu
^{2}Zirui Zhu is with the Department of Electrical Engineering, Tsinghua University zhuzr17@mails.tsinghua.edu.cn
^{3}Stuart Anderson is with Meta Reality Labs Research stuarta@fb.com]</p>
<h6>Abstract</h6>
<p>Robot simulation has been an essential tool for data-driven manipulation tasks. However, most existing simulation frameworks lack either efficient and accurate models of physical interactions with tactile sensors or realistic tactile simulation. This makes the sim-to-real transfer for tactile-based manipulation tasks still challenging. In this work, we integrate simulation of robot dynamics and vision-based tactile sensors by modeling the physics of contact. This contact model uses simulated contact forces at the robot’s end-effector to inform the generation of realistic tactile outputs. To eliminate the sim-to-real transfer gap, we calibrate our physics simulator of robot dynamics, contact model, and tactile optical simulator with real-world data, and then we demonstrate the effectiveness of our system on a zero-shot sim-to-real grasp stability prediction task where we achieve an average accuracy of 90.7% on various objects. Experiments reveal the potential of applying our simulation framework to more complicated manipulation tasks. We open-source our simulation framework at https://github.com/CMURoboTouch/Taxim/tree/taxim-robot.</p>
<h2>I. INTRODUCTION</h2>
<p>With the advent of deep learning, data-driven methods for solving various tasks like grasping [1], cloth manipulation [2] and object properties estimation [3] have become ubiquitous in the robotic community. Data-driven techniques typically require large amounts of data generated from interaction with an environment, which incurs significant time and labor costs. In contrast to collecting data with real robots, simulation is an easy and effective tool for generating largescale datasets.</p>
<p>Dense vision-based tactile sensing provides rich contact information including contact shapes, textures, forces that can be used in various tasks such as shape reconstruction [4], pose estimation [5], grasping [6], slip detection [7] etc. When solving these problems with data-driven approaches, simulating accurate contact dynamics and dense vision-based tactile readings can provide efficient data collection. There exists multiple vision-based tactile simulation [8], [9], [10], [11] which takes contact shapes as inputs and simulate the tactile images, and [12], [13] integrate the tactile simulation into robot dynamics simulation engines. However, these simulators are limited for dexterous manipulation tasks which also requires accurate contact dynamics simulation to handle contact forces, friction and the converting from contact physics to contact shapes of tactile sensors. [14],</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: The pipeline of Sim2Real grasping stability prediction: we train an grasping stability prediction model using tactile images collected in our simulation and then test its performance on real-world tactile images.</p>
<p>[15] model the contact dynamics for low-resolution tactile sensors but they are rather simpler than modeling for the vision-based tactile sensors and cannot be applied for most perception tasks. In order to benefit both perception and manipulation tasks, we proposal to integrate a realistic dense vision-based tactile sensor simulation with existing robot simulation engines by bridging with a contact dynamics simulation model.</p>
<p>The eventual goal of using simulation is to assist solving problems in real world, therefore sim-to-real transfer tasks make a good indicator to evaluate simulation frameworks. Previous perception work with tactile sensing such as shape mapping [16], contact pose estimation [12] and edge/surface following [13] only used simulation of tactile readings based on geometry and succeeded in sim-to-real transfer. However, sim-to-real grasping requires not only realistic geometric tactile simulation but also accurate contact dynamics simulation, where the latter one was not achieved in the previous simulation frameworks. To this end, we present the first integrated simulation framework with modeling the contact dynamics and combining it with a dense vision-based tactile simulator. By calibrating physics parameters of contact dynamics and tactile optical simulation with real-wolrd data, we show our simulation framework can successfully be applied to sim-to-real grasping stability prediction task with high accuracy.</p>
<p>In this work, we model the contact dynamics to convert the contact forces to the contact deformation and then integrate the Taxim [11], a realistic vision-based tactile simulator, to improve the tactile simulation system’s fidelity. We apply our simulation framework on a sim-to-real grasp stability</p>
<p>prediction task. We learn a model given tactile images to predict the grasp outcomes completely in simulation and then perform zero-shot sim-to-real transfer to test on tactile images from real grasp experiments. To the best of our knowledge, this is the first successful attempt at zero-shot sim-to-real transfer for grasp stability prediction with dense vision-based tactile sensing. This shows the potential of our simulation framework on more complex manipulation tasks.</p>
<h2>II. RELATED WORK</h2>
<h2>A. Grasping with tactile sensing</h2>
<p>Tactile sensing has been widely applied to grasping tasks given their rich contact information. Schill et al. [17] learnt a classifer from 6 tactile sensors on a robot hand to continuously estimate the grasp stability during the grasp until reach the stable grasp. Bekiroglu et al. [18] learnt a latent variable probabilistic model from vision, tactile and action parameters. They used the conditional of this model to estimate the grasp stability. Calandra et al. [19] showed that visuo-tactile deep neural network model can improve the ability to predict the grasp. And furthermore, the author proposed an action-conditional model to learn the regrasping policies from visual and tactile sensing in [1]. Note in all the above cases, a data collection stage in the real world was performed to build a learning model by grasping various objects. This process is time consuming and requires a human in the loop to reset the moving objects during grasping.</p>
<p>Alternatively, to overcome the limitation in the data size, an exploration-based grasping method with visual and tactile data was developed and tested in the simulation [20]. Bekiroglu et al. [21] learnt a probabilistic model of grasp stability, given tactile signal, joint configuration of the hand, object shape class and approach vector of the hand. They trained and tested the model on both simulated and real data but without sim-to-real transfer. Hogan et al. [6] proposed a grasp quality metric based on tactile images and used the simulated regrasp candidates along with their tactile images to search for grasp adjustment. To predict the grasp stability given tactile images, our solution is to train completely in simulation, given a high fidelity simulation system with calibrated physical parameters with real-world examples and test directly on a real robot.</p>
<h2>B. Simulation of Tactile Sensors</h2>
<p>There are a lot of physics engines available to robotics practitioners [22] which allow full robot simulation with multiple sensing modalities. However, tactile simulation is limited in those simulators as deformable soft surfaces in tactile sensors are hard to simulate accurately and efficiently. Due to above, tactile simulation is still a challenging research area. Existing work [23], [24], [25], [26] build the mechanics models to simulate the soft body deformation for tactile sensors. For vision-based tactile sensors like GelSight [27], optical simulation [9], [10], [8], [11] is also essential as it is used to measure shapes of objects in contact. Several sim-to-real robot perception work [5], [16], [28] has revealed the accuracy and sufficiency of tactile optical simulation, and we leverage the existing tactile sensing simulation in robot simulator to make better use of them.</p>
<p>To integrate the tactile simulation with physics simulation for manipulation tasks, Moisio et al. [14] simulated a low-resolution tactile sensor considering soft contacts and full friction description and applied it on grasping tasks. Kappassov et al. [15] presented a tactile simulation framework for tactile arrays in Gazebo simulation environment considering the effect of contact forces and showed tactile servoing applications. Wang et al. [12] presented TACTO, a simulator using pyrender to simulate optical tactile sensors and combined it to a physics simulator PyBullet. Compared to those simulation frameworks, we combine the simulation of contact physics including forces, frictions and slips with the vision-based tactile simulation of contact shapes, which allows the potential to simulate the more complicated grasping scenarios with slip and efficient sim-to-real transfer.</p>
<h2>C. Sim-to-Real Learning</h2>
<p>The control policies learned in simulation can be applied to real robots by framing the problem as transfer learning between the data distribution of simulation and real worlds. Dang et al. [29] presented a learning approach to estimate the grasp stability and make hand adjustments based on low-resolution tactile sensing data from robot hand. They realized sim-to-real transfer but their testing objects are rather limited in number and have simpler shapes. In [13], the authors proposed an image-conditioned generator network that translates between real and simulated images. They used this network to transfer policies trained in simulation for various tactile manipulation tasks. In [30], the authors set up two multi-fingered hands with tactile sensors in PyBullet, used it to learn a reinforcement learning policy for grasping and transferred the learnt policy to real world environments. Mahler et al. [31] considered the problem of bin picking by simulating quasi-static physics in PyBullet with a paralleljaw gripper. They posed the sim-to-real task as a transferring GQ-CNN features between simulation and real world data from Dex-Net 2.0 dataset [32]. We train a grasp stability prediction model based on dense vision-based tactile images in our simulation framework and show that the model can be successfully transferred to the real data without any explicitly transfer step.</p>
<h2>III. SIMULATION FRAMEWORK</h2>
<p>In this section, we present our integrated simulation framework with tactile sensing. The framework includes three parts as shown in Fig. 2: physics simulation in Section III-A, contact simulation in Section III-B and tactile simulation in Section III-C. We use PyBullet to simulate the physics, and transfer the contact forces and poses to the contact deformation of a GelSight tactile sensor [27], which has a soft surface to interact with the object and an embedded camera to convert the contact geometries to RGB images.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Our proposed simulation framework includes physics simulation, contact simulation and tactile simulation. Physics simulator handles the robot dynamics which provide the contact forces and poses. Contact models map them to indentation depths and shear displacements of the contact, and generate the contact map to feed into the tactile simulator. Tactile simulation renders the RGB tactile images.</p>
<p>Then the tactile simulator renders tactile images according to the contact deformation.</p>
<h3>A. Physics Simulation</h3>
<p>Physics simulation reproduces the real world dynamic interaction between the robots and objects, and its accuracy directly impacts sim-to-real transfer. We use PyBullet as our physics simulation engine. For the grasping task, we load the robot arm, gripper and a GelSight mounted on the gripper with proper geometries and links as shown in Fig. 1.</p>
<p>Grasping requires accurate contact simulation, therefore physics parameters setting such as friction, objects' mass and their center of mass becomes essential. In order to match the simulation with the real grasping scenario, we measure the weights and center of mass of objects in reality as the reference. We then estimate the friction of the contact surface by calibrating with the real world data. Specifically, we search for the best friction coefficient by adjusting friction values in simulation to minimize the grasping label mismatching between real and simulated data under the same grasping configurations. See Section V-B for more details.</p>
<h3>B. Contact Simulation</h3>
<p>Contact simulation refers to simulating the deformation of GelSight's soft surface under applied contact forces during the interaction with the object. PyBullet simulates the robot dynamics, but it lacks accurate contact model since the soft body deformation of tactile sensors is not able to be simulated with only rigid body collision. Instead of applying computationally costly soft body simulation, we use an simplified model that maps contact forces to the deformation of the tactile sensor.</p>
<p>When the tactile sensor touches the object, PyBullet calculates the contact force and relative poses between the object and the sensor. To simulate the object's indentation into the tactile sensor's surface under the normal loading and sliding on the contact surface under the shear loading, we map the contact force to the indentation depth and the shear motion of the contact. Enlighten by [27] and from experimental measurements on our real GelSight sensor plotted in Fig. 3, we approximate the linear mapping between</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Experiment results showed that the contact force is almost linear to the deformation of the GelSight in both normal and shear directions, regardless of the contact geometry. (a) Experiment results of the contact volume vs. normal force for balls with different diameters. (b) Experiment results of the sum of markers' shear motion vs. overall shear force.</p>
<p>the normal force and the indentation volume, and between the shear force and the shear motion of the contact. We characterize their coefficient $k_n$ and $k_s$ as:</p>
<p>$$
\begin{aligned}
V &amp;= k_n F_n \
D &amp;= k_s F_s
\end{aligned}
\tag{1}
$$</p>
<p>where $V$ is the indentation volume, $F_n$ is the normal force; $D$ is the shear displacement, $F_s$ is the shear force.</p>
<p>Recovering the indentation depth map from the volume $V$ does not have a close-form solution, therefore we use the binary searching to find a best estimated depth map. Given an initial depth map, we integrate the indentation depth $d(A)$ within the contact area $A$ to the volume as:</p>
<p>$$
V_{est} = \int d(A) dA \tag{2}
$$</p>
<p>We iteratively adjust the $d(A)$ to minimize the error between the estimated volume $V_{est}$ and the target volume $V$ to find the best solution.</p>
<p>To simulate the contact shape, we utilize the PyRender in a similar way as TACTO [12] and place an virtual depth camera behind the sensor surface to capture the 3D contact shape, defined as the contact map. Given the indentation depth, shear displacement and the relative poses between the object and the tactile sensor, we move the object along the normal direction of contact with indentation depth and the tangential direction with the shear displacement in PyRender and then render the contact map.</p>
<p>Note that PyBullet reserves a collision margin between two colliding objects to ensure numerical stability where two objects considered as collided are still separated by a distance from each other. This margin even changes when the object has non-convex geometric surfaces. Therefore, we adapt this margin in PyRender to get precise contact indentation depth.</p>
<h3>C. Tactile Simulation</h3>
<p>After getting the contact map from the contact model, we render the GelSight tactile images with an state-of-the-art simulation model, Taxim [11]. Taxim uses a lookup table to map the contact shapes to the tactile images. The lookup table is calibrated with a real sensor used in the experiment.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Grasp pipeline for both simulation and real experiments. We initialize the robot on top of the object, move the gripper down to a preset height, close the gripper with a preset grasping force to grasp the object, and then lift it. We record the tactile readings from a GelSight sensor after grasping.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 6: Grasp stability prediction networks. (a) We input single tactile images to a feature extractor (CNN) and a classifier (MLP) to predict the grasp results. (b) We input a sequence of tactile images to feature extractors (CNN), a LSTM module and then a classifier (MLP) to predict the grasp results.</p>
<h1>IV. SIM-TO-REAL GRASPING PREDICTION</h1>
<p>We formalize the grasping stability prediction as a supervise learning problem where we use the tactile images during grasping to predict the grasp outcomes. We use simulated tactile images and labels from our simulation framework to train the learning model and test it on real-world data with a zero-shot sim-to-real transfer.</p>
<h2>A. Grasp Stability Prediction Model</h2>
<p>We try to classify the grasp stability into binary labels: success when the object can be stably lifted and failure otherwise. We predict grasp outcomes based on a single or sequential tactile images from a GelSight sensor. We take the tactile image at the moment of the object is grasped as the single input. To get the sequential input, we record tactile images for three seconds after the robot closes the gripper.</p>
<p>We build a neural network model to extract latent features of tactile images and then use a classifier to predict the grasp stability base on these features. For the model with single tactile inputs, we use a pre-trained ResNet-18 [33] as our feature extractor and a multi-layer perceptual (MLP) as classifier as shown in Fig. 6 (a). For the sequential inputs, we feed the sequence of image features to a long short-term memory (LSTM) module and then forward the last hidden state's output to the classifier as shown in Fig. 6 (b).</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 5: Demonstration of a grasping configuration: it is defined as the grasp location, height and force.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7: We classify grasp as successful grasp (a) and failed grasp (b), (c) including translational and rotational slip.</p>
<h2>B. Grasping Pipeline</h2>
<p>Our grasping pipeline includes initialization, approaching, grasping, lifting and labeling as shown in Fig 4. We initialize the robot and the gripper on the top of the object. Given the specified grasping configuration, the robot rotates to the target orientation, moves straightly down to the grasping height, and adjust the grasping location on the plane parallel to table. During the grasping, the gripper closes with a certain speed and force. Then till the gripper closes entirely, we record tactile images from the GelSight and use them as our grasping stability prediction model inputs. The robot then lifts the object for 18 cm. During the lifting, if the object remains stable in the gripper, we label the grasp as 'success'; otherwise, if the object falls or slips in the gripper, we label the grasp as 'failure'. There are two different kinds of failure: translational slip or rotational slip coming from lacking grasping forces or wrong grasping locations respectively as shown in Fig. 7. In experiments, we set certain thresholds, 15 cm translational movement and 0.1 radians rotational movement of objects to detect grasp failure.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8: Examples of tactile readings under different grasping scenarios. Different grasping locations as marked on the object and grasping forces F lead to different grasping outcomes. We show the sequence of the tactile readings during grasping, where geometries of contact can be used to predict the grasping outcomes.</p>
<h3><em>C. Grasping Configuration Generation</em></h3>
<p>We collect grasp data with various grasping configurations regarding the grasp location and force. We describe the grasping configuration as a vector (F, X, Y, Z), where F is the grasping force, X, Y is the grasping location on the horizontal plane, and Z is the grasping height as shown in Fig. 5. We define the range of each dimension of grasping, discretize the grasping space and then conduct the grasping.</p>
<h3>V. EXPERIMENTS</h3>
<p>We conduct both simulated and real grasp experiments and evaluate our simulation framework's ability for sim-to-real transfer based on collected data. Our data collection includes variance in three parameters of grasping: the target object, the grasping location, and the grasping force. We show the grasp stability prediction model's performance on both single and sequential tactile images. We provide details of our experiments in the following sections.</p>
<h4>A. Data Collection</h4>
<p>We use a UR5e robot with a Weiss WSG-50 gripper, and mount a GelSight [34] sensor on one side of the gripper. We use the objects from YCB [35] and GoogleScan [36] datasets for grasping where the mesh models are available. We conduct the same grasping process in both simulation and real as shown in Fig. 4.</p>
<p>We selected twelve objects for both simulation and real-world data collection as shown in Table I. To demonstrate our model's ability to make predictions over a broad set of objects, we chose objects that provide a range of shapes, surface frictions, masses, and center of mass locations.</p>
<p><strong>Testing dataset from real world data</strong> For each object, we grasp it at three to five different heights, ranging from the top of the object to the minimum reachable height; we vary the grasping location on the XY plane with three to six values depending on the size of the object; and we linearize the grasping force with six different values ranging from 5N to 10N. We also add mass (water or clay) ranging from 100g to 500g to light objects such as empty bottles to get more grasping failures. These parameters are chosen so that the collected grasp data is approximately balanced with 363 successful and 389 failed grasps in total.</p>
<p><strong>Training dataset from simulated data</strong> We use the same set of objects in simulation. For each object, we collected 100 to 150 grasp trials for training and 50 grasp trials for testing following the same configurations mentioned in testing dataset. For each grasp, we record the rendered tactile images and automatically label its corresponding grasp outcomes based on the pose changing of the object.</p>
<p>Some data examples are shown in Fig. 8 where we mark the grasp locations, grasp forces, grasp labels and the corresponding tactile readings.</p>
<h4>B. Optimization of Friction</h4>
<p>To eliminate the gap of sim-to-real transfer, we use a few real-world grasping examples (around 20) apart from the testing dataset for each object as reference, and tune the friction coefficient in simulation with an optimization process. As shown in Fig. 9, for each object (here we use</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 9: We optimize the friction coefficient of object surface by matching the grasping labels between simulated and real data under the same configuration of grasping heights and forces as shown in (a) and (b). We evaluate possible friction coefficients for several objects and choose the friction values that minimize mislabeling as shown in (c).</p>
<table>
<thead>
<tr>
<th>Object</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>average</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sim2Sim (TACTO [12])</td>
<td>0.860</td>
<td>0.900</td>
<td>0.960</td>
<td>0.860</td>
<td>0.720</td>
<td>0.940</td>
<td>1.000</td>
<td>0.850</td>
<td>0.920</td>
<td>0.960</td>
<td>0.550</td>
</tr>
<tr>
<td>Sim2Sim (Single)</td>
<td>1.000</td>
<td>0.958</td>
<td>0.875</td>
<td>1.000</td>
<td>0.937</td>
<td>1.000</td>
<td>0.875</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
</tr>
<tr>
<td>Sim2Sim (Sequential)</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
</tr>
<tr>
<td>Sim2Real (TACTO [12])</td>
<td>0.542</td>
<td>0.667</td>
<td>0.094</td>
<td>0.688</td>
<td>0.208</td>
<td>0.604</td>
<td>0.333</td>
<td>0.484</td>
<td>0.625</td>
<td>0.375</td>
<td>0.344</td>
</tr>
<tr>
<td>Sim2Real (Single)</td>
<td>0.958</td>
<td>0.895</td>
<td>0.875</td>
<td>0.812</td>
<td>0.791</td>
<td>0.833</td>
<td>0.750</td>
<td>0.953</td>
<td>0.958</td>
<td>1.000</td>
<td>0.843</td>
</tr>
<tr>
<td>Sim2Real (Sequential)</td>
<td>1.000</td>
<td>0.916</td>
<td>0.968</td>
<td>0.821</td>
<td>0.833</td>
<td>0.812</td>
<td>1.000</td>
<td>0.937</td>
<td>0.958</td>
<td>1.000</td>
<td>0.875</td>
</tr>
</tbody>
</table>
<p>TABLE I: The result of grasp stability prediction. We test the prediction accuracy for both sim-to-sim and sim-to-real transfers with a single tactile image and sequences of tactile images. We compare the performance with TACTO [12].
the mustard bottle as example), we generate the distribution of the grasping outcomes based on the grasping heights and forces for both simulated and real data. Adjusting the friction coefficient in simulation will lead to different distributions, and we search for the best friction coefficient based on how well the generated outcome distribution matches the realworld one. We discretize the friction coefficient search space in the range [0,1] with a step size of 0.05, then we plot the number of mislabeling in Fig. 9 (c). For each object, there is a point with the fewest mislabels and we denote the corresponding friction coefficient as the best one to use.</p>
<h2>C. Learning Model and Training Settings</h2>
<p>We use Res-Net 18 [33] pre-trained on ImageNet as the tactile feature extractor. For single tactile inputs, we use a two-layer MLP 512-256-2 with ReLu activation and a 0.2 dropout after the first linear layer as classifier. For sequential tactile inputs, we use a three-layer LSTM module with 128 hidden layer size, then a two-layer MLP 128-64-2 with ReLu activation and a 0.2 dropout after the first linear layer as the classifier. For both learning models, we use the Adam optimizer with $1 \times 10^{-4}$ learning rate, $5 \times 10^{-5}$ weight decay and a ReduceLROnPlateau scheduler. We train the model with 8 batch size and 30 epochs. We set the learning rate of the feature extractor as 0.8 of the learning rate of the rest modules to prevent overfitting. We train individual prediction model for each object.</p>
<h2>D. Grasp Stability Prediction with Single Tactile Image</h2>
<p>We first test our sim-to-real grasp stability prediction model with single tactile images. The results are reported in Table I indicated as Sim2Sim (Single) and Sim2Real (Single). We denote the case of training on simulated data and testing on simulated data as "Sim2Sim" and the case of training on simulated data and testing on real data as "Sim2Real". From the table, we show our Sim2Sim prediction accuracy is $100 \%$ excluding objects (Rubik's cube, cup, spatula, strawberry) which have ambiguous shapes. For those shapes, it is hard to locate the grasp based on single tactile images since they look similar or even the same.</p>
<p>We also show our Sim2Real prediction accuracy are all above $80 \%$ except objects spatula and strawberry. The mesh models of these objects are too coarse compared to real objects, which leads to significant differences between
simulated and real tactile images. On average, our Sim2Real gap is less than $10 \%$ comparing to our Sim2Sim results.</p>
<p>We conduct the same grasping experiments using TACTO [12] as a baseline and post results in the Table I. The results show that our method outperforms TACTO. A major failure of TACTO is caused by inaccurate contact model: it models the contact that the normal force is linear to the indentation depth, which does not match the physics of the sensor in the real world. In addition, TACTO does not properly calculate the collision margin so that it causes occasional failure in generating tactile signals upon contact.</p>
<h2>E. Grasp Stability Prediction with Sequential Tactile Image</h2>
<p>Single tactile images are less informative when objects have ambiguous geometries such as the flat surface of a Rubik's cube. Therefore we also test our simulation performance on models with sequential tactile images as inputs. Sim2Sim and Sim2Real accuracy results are shown in Table I indicated as Sim2Sim (Sequential) and Sim2Real (Sequential). Comparing to the single tactile results, we notice that the Sim2Real performance on objects Rubik's cube, cup, spatula, and strawberry is improved. And the average performance also improves to $90.7 \%$. This is mostly because the sequential data shows either the stable or the changing contacts from tactile images after lifting which can indicate the object's motion. And it serves as better features to predict the grasp outcomes.</p>
<h2>F. Effects of Dataset Size, Friction and Center of Mass</h2>
<p>We do ablation study on different setting of dataset size, friction on potted meat can object, and center of mass on scissor object in simulation. As shown in Table V-F, the performance of Sim2Sim and Sim2Real both increase along with the increasing dataset size but reach the plateau after 200. This suggests that choosing dataset size between 100 to 200 is sufficient for this task. Comparing to the best choice 0.45 for friction coefficient and -0.03 for center of mass, even though the Sim2Sim accuracy stays high, the Sim2Real accuracy drops quickly with inaccurate parameters. This indicates that the physics settings affect the results significantly and it is necessary to set the proper physics parameters for effective sim-to-real transfer.</p>
<table>
<thead>
<tr>
<th>Dataset size</th>
<th>50</th>
<th>100</th>
<th>200</th>
<th>500</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sim2Sim Acc</td>
<td>0.875</td>
<td>0.937</td>
<td>1.000</td>
<td>1.000</td>
</tr>
<tr>
<td>Sim2Real Acc</td>
<td>0.791</td>
<td>0.833</td>
<td>0.958</td>
<td>0.958</td>
</tr>
<tr>
<td>Friction coefficient</td>
<td>0.2</td>
<td>$\mathbf{0 . 4 5}$</td>
<td>0.8</td>
<td></td>
</tr>
<tr>
<td>Sim2Sim Acc</td>
<td>1.000</td>
<td>1.000</td>
<td>0.937</td>
<td></td>
</tr>
<tr>
<td>Sim2Real Acc</td>
<td>0.666</td>
<td>0.958</td>
<td>0.541</td>
<td></td>
</tr>
<tr>
<td>Center of Mass</td>
<td>-0.03</td>
<td>0.00</td>
<td>0.03</td>
<td></td>
</tr>
<tr>
<td>Sim2Sim Acc</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
<td></td>
</tr>
<tr>
<td>Sim2Real Acc</td>
<td>0.958</td>
<td>0.708</td>
<td>0.625</td>
<td></td>
</tr>
</tbody>
</table>
<p>TABLE II: Ablation study of dataset size, friction coefficient on potted meat can, and center of mass on scissor. 0.45 and -0.03 are the best parameters for friction and center of mass we used in our experiments.</p>
<h2>VI. CONCLUSIONS</h2>
<p>In this work, we present a robot simulation framework with tactile sensing, where we integrate a contact model mapping the contact forces to the deformation of the soft surface of the GelSight tactile sensor. We test the performance of our framework on grasp stability prediction task where we directly transfer the model trained on simulated data to real-world data. We show that our model can achieve $75 \%-100 \%$ accuracy in sim-to-real transfer on objects with various shapes and physical properties while hasn't been exposed to a single real tactile image.</p>
<p>In the future, we would like to extend this work to sim-to-real policy transfer of grasp-regrasp planning based on our grasp prediction. In addition, we would like to combine tactile sensing with other modalities such as vision in simulation and explore its sim-to-real transfer. We would also like to apply our simulation framework on various manipulation tasks such as picking-and-placing, pushing, and sliding tasks.</p>
<h2>ACKNOWLEDGMENT</h2>
<p>This work was funded by Meta AI research. The authors would like to thank Hung-Jui Huang, Yifan You, Dr. Roberto Calandra for the help and discussion on this work.</p>
<h2>REFERENCES</h2>
<p>[1] R. Calandra, A. Owens, D. Jayaraman, J. Lin, W. Yuan, J. Malik, E. H. Adelson, and S. Levine, "More than a feeling: Learning to grasp and regrasp using vision and touch," IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 3300-3307, 2018.
[2] T. Weng, S. M. Bajracharya, Y. Wang, K. Agrawal, and D. Held, "Fabricffownet: Bimanual cloth manipulation with a flow-based policy," in Conference on Robot Learning. PMLR, 2022, pp. 192202.
[3] W. Yuan, C. Zhu, A. Owens, M. A. Srinivasan, and E. H. Adelson, "Shape-independent hardness estimation using deep learning and a gelsight tactile sensor," in 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2017, pp. 951-958.
[4] S. Wang, J. Wu, X. Sun, W. Yuan, W. T. Freeman, J. B. Tenenbaum, and E. H. Adelson, "3d shape perception from monocular vision, touch, and shape priors," in 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018, pp. 16061613.
[5] M. Bauza, E. Valls, B. Lim, T. Sechopoulos, and A. Rodriguez, "Tactile object pose estimation from the first touch with geometric contact rendering," arXiv preprint arXiv:2012.05205, 2020.
[6] F. R. Hogan, M. Bauza, O. Canal, E. Donlon, and A. Rodriguez, "Tactile regrasp: Grasp adjustments via simulated tactile transformations," in 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018, pp. 2963-2970.
[7] W. Yuan, R. Li, M. A. Srinivasan, and E. H. Adelson, "Measurement of shear and slip with a gelsight tactile sensor," in 2015 IEEE International Conference on Robotics and Automation (ICRA), 2015, pp. 304-311.
[8] A. Agarwal, T. Man, and W. Yuan, "Simulation of vision-based tactile sensors using physics based rendering," in 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021, pp. $1-7$.
[9] D. F. Gomes, P. Paoletti, and S. Luo, "Generation of gelsight tactile images for sim2real learning," IEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 4177-4184, 2021.
[10] F. R. Hogan, M. Jenkin, S. Rezaei-Shoshtari, Y. Girdhar, D. Meger, and G. Dudek, "Seeing through your skin: Recognizing objects with a novel visuotactile sensor," in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2021, pp. 1218-1227.
[11] Z. Si and W. Yuan, "Taxim: An example-based simulation model for gelsight tactile sensors," IEEE Robotics and Automation Letters, 2022.
[12] S. Wang, M. M. Lambeta, P.-W. Chou, and R. Calandra, "Tacto: A fast, flexible, and open-source simulator for high-resolution visionbased tactile sensors," IEEE Robotics and Automation Letters, 2022.
[13] A. Church, J. Lloyd, N. F. Lepora, et al., "Tactile sim-to-real policy transfer via real-to-sim image translation," in Conference on Robot Learning. PMLR, 2022, pp. 1645-1654.
[14] S. Moisio, B. León, P. Korkealaakso, and A. Morales, "Simulation of tactile sensors using soft contacts for robot grasping applications," in 2012 IEEE International Conference on Robotics and Automation. IEEE, 2012, pp. 5037-5043.
[15] Z. Kappassov, J.-A. Corrales-Ramon, and V. Perdereau, "Simulation of tactile sensing arrays for physical interaction tasks," in 2020 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM), 2020, pp. 196-201.
[16] S. Suresh, Z. Si, J. G. Mangelson, W. Yuan, and M. Kaess, "Efficient shape mapping through dense touch and vision," arXiv preprint arXiv:2109.09884, 2021.
[17] J. Schill, J. Laaksonen, M. Przybylski, V. Kyrki, T. Asfour, and R. Dillmann, "Learning continuous grasp stability for a humanoid robot hand based on tactile sensing," in 2012 4th IEEE RAS \&amp; EMBS International Conference on Biomedical Robotics and Biomechatronics (BioRob). IEEE, 2012, pp. 1901-1906.
[18] Y. Bekiroglu, A. Damianou, R. Detry, J. A. Stork, D. Kragic, and C. H. Ek, "Probabilistic consolidation of grasp experience," in 2016 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2016, pp. 193-200.
[19] R. Calandra, A. Owens, M. Upadhyaya, W. Yuan, J. Lin, E. H. Adelson, and S. Levine, "The feeling of success: Does touch sensing help predict grasp outcomes?" in Conference on Robot Learning. PMLR, 2017, pp. 314-323.
[20] C. de Farias, N. Marturi, R. Stolkin, and Y. Bekiroglu, "Simultaneous tactile exploration and grasp refinement for unknown objects," IEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 3349-3356, 2021.
[21] Y. Bekiroglu, J. Laaksonen, J. A. Jorgensen, V. Kyrki, and D. Kragic, "Assessing grasp stability based on learning and haptic data," IEEE Transactions on Robotics, vol. 27, no. 3, pp. 616-629, 2011.
[22] T. Erez, Y. Tassa, and E. Todorov, "Simulation tools for modelbased robotics: Comparison of bullet, havok, mujoco, ode and physx," in 2015 IEEE international conference on robotics and automation (ICRA). IEEE, 2015, pp. 4397-4404.
[23] Z. Pezzementi, E. Jantho, L. Estrade, and G. D. Hager, "Characterization and simulation of tactile sensors," in 2010 IEEE Haptics Symposium. IEEE, 2010, pp. 199-205.
[24] Y. S. Narang, K. Van Wyk, A. Mousavian, and D. Fox, "Interpreting and predicting tactile signals via a physics-based and data-driven framework," arXiv preprint arXiv:2006.03777, 2020.
[25] Y. Narang, B. SandaraJingam, M. Macklin, A. Mousavian, and D. Fox, "Sim-to-real for robotic tactile sensing via physics-based simulation and learned latent projections," arXiv preprint arXiv:2103.16747, 2021.
[26] C. Sferrazza, A. Wahlsten, C. Trueeb, and R. D’Andrea, "Ground truth force distribution for learning-based tactile sensing: A finite element approach," IEEE Access, vol. 7, pp. 173 438-173 449, 2019.
[27] W. Yuan, S. Dong, and E. H. Adelson, "Gelsight: High-resolution robot tactile sensors for estimating geometry and force," Sensors, vol. 17, no. 12, p. 2762, 2017.
[28] R. Gao, Z. Si, Y.-Y. Chang, S. Clarke, J. Bohg, L. Fei-Fei, W. Yuan, and J. Wu, "Objectfolder 2.0: A multisensory object dataset for</p>
<p>sim2real transfer," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 10 598-10 608.
[29] H. Dang and P. K. Allen, "Stable grasping under pose uncertainty using tactile feedback," Autonomous Robots, vol. 36, no. 4, pp. 309330, 2014.
[30] B. Wu, I. Akinola, J. Varley, and P. Allen, "Mat: Multi-fingered adaptive tactile grasping via deep reinforcement learning," arXiv preprint arXiv:1909.04787, 2019.
[31] J. Mahler and K. Goldberg, "Learning deep policies for robot bin picking by simulating robust grasping sequences," in Conference on robot learning. PMLR, 2017, pp. 515-524.
[32] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg, "Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics," arXiv preprint arXiv:1703.09312, 2017.
[33] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," 2015.
[34] S. Dong, W. Yuan, and E. H. Adelson, "Improved gelsight tactile sensor for measuring geometry and slip," in 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2017, pp. 137-144.
[35] B. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel, and A. M. Dollar, "Benchmarking in manipulation research: Using the yale-cmu-berkeley object and model set," IEEE Robotics \&amp; Automation Magazine, vol. 22, no. 3, p. 36-52, Sep 2015. [Online]. Available: http://dx.doi.org/10.1109/MRA.2015.2448951
[36] "Google Scanned Objects," https://app.ignitionrobotics.org/ GoogleResearch/fuel/collections/Google\%20Scanned\%20Objects.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-292 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-292</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-292</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-ef5f7cd21b5d34797636239a7b9c8ba6af440aab</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ef5f7cd21b5d34797636239a7b9c8ba6af440aab" target="_blank">Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A new inference framework is proposed, called Recursion of Thought (RoT), which introduces several special tokens that the models can output to trigger context-related operations, and dramatically improves LMs' inference capability to solve problems, whose solution consists of hundreds of thousands of tokens.</p>
                <p><strong>Paper Abstract:</strong> Generating intermediate steps, or Chain of Thought (CoT), is an effective way to significantly improve language models' (LM) multi-step reasoning capability. However, the CoT lengths can grow rapidly with the problem complexity, easily exceeding the maximum context size. Instead of increasing the context limit, which has already been heavily investigated, we explore an orthogonal direction: making LMs divide a problem into multiple contexts. We propose a new inference framework, called Recursion of Thought (RoT), which introduces several special tokens that the models can output to trigger context-related operations. Extensive experiments with multiple architectures including GPT-3 show that RoT dramatically improves LMs' inference capability to solve problems, whose solution consists of hundreds of thousands of tokens.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e292.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e292.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recursion of Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-agnostic inference framework that lets language models generate special tokens (GO, STOP, THINK, TAIL) to create and manage multiple short contexts and recursively solve subproblems, enabling divide-and-conquer reasoning beyond the single-context length limit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (and other algorithmic tasks via recursive decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Demonstrated on very large problems: up to 48-digit addition/subtraction and 16-digit multiplication/division with GPT-3; up to 64-digit addition/subtraction and 32-digit multiplication/division with tiny Transformer (experiments vary by setup).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Supervised fine-tuning of sequence models with ground-truth recursive intermediate steps and special tokens; models trained to output THINK instead of sub-answer first token; iterative recursive inference replaces THINK with returned sub-answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Enables near-perfect accuracy (qualitative) across tested arithmetic tasks and scales where Chain-of-Thought cannot be applied due to context limits; RoT-trained models solved problems requiring up to hundreds of thousands of reasoning tokens in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Encourages models to implement explicit divide-and-conquer, elementary-school style algorithms: decomposition into digit-level subproblems (last-digit add/subtract, carry/borrow handling, digit-splitting multiplication, long-division via compare-and-subtract), using THINK/TAIL recursion to spawn contexts and replace THINK with returned answers; training uses supervised intermediate steps so the model learns when to recurse and how to combine subresults.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>RoT decouples total reasoning length from single-context limit: performance remains high as problem complexity (number of subproblems / total tokens) grows, constrained primarily by per-subproblem accuracy rather than context length. No emergent length generalization was achieved (training on shorter lengths did not generalize to longer lengths).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Requires extremely high per-subproblem accuracy (e.g., >99.9%) because many subproblems compose into one final problem; does not by itself confer length generalization (models trained on N-digit tasks do not necessarily generalize to 2N-digit tasks); currently depends on supervised ground-truth intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against direct-answer baseline (WT) and standard Chain-of-Thought (CoT) trained baseline; RoT outperforms WT and matches CoT accuracy where CoT is applicable, while RoT scales beyond CoT's context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>By teaching models to spawn and manage recursive short contexts via special tokens, language models can learn algorithmic, elementary-school procedures and solve extremely large arithmetic problems (100K+ token reasoning traces) with near-perfect accuracy, overcoming single-context length limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e292.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e292.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (RoT experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 fine-tuned with Recursion of Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 was fine-tuned with the RoT training procedure on synthetic arithmetic and algorithmic tasks and evaluated on very large-digit arithmetic; RoT enabled GPT-3 to solve problems far beyond the 2048-token single-context limit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (also algorithmic tasks such as LCS, LPS, 0-1 knapsack, MCM)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Reported experiments include up to 48-digit addition/subtraction and 16-digit multiplication/division (and other sizes); problems that lead to CoT solutions exceeding 2048 tokens were targeted.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuning via OpenAI API for 10K steps, batch size 256, supervised RoT training with ground-truth recursive intermediate steps and special tokens (GO/THINK/STOP/TAIL).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Qualitatively 'near-perfect' accuracy across every reported GPT-3 RoT experiment; WT failed on these complexities and CoT was not applicable because of the 2048 context limit. (Exact numeric accuracies not enumerated in main text.)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>GPT-3, when supervised with RoT, learned to emit THINK/TAIL to create recursive subcontexts and to perform digit-wise elementary algorithms (carries, borrows, splits, long division trials) rather than relying on black-box memorization; recursion token mechanism lets the model offload intermediate computations into new contexts returned as answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>With RoT, GPT-3 maintained high accuracy as operand digit lengths and total reasoning tokens increased well beyond single-context limits; CoT would be limited by context length (2048) while RoT scales to much larger overall reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Prompting-only methods could not reach required per-subproblem accuracy; RoT still fails to generalize to larger lengths than seen in training (no length generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to Without Thought (WT) and Chain-of-Thought (CoT) baselines; WT failed; CoT matched accuracy when context-length permitted but could not be used for largest problems due to context limit.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Fine-tuning GPT-3 with supervised RoT yields near-perfect arithmetic performance on tasks (tens of digits and beyond) that produce extremely long stepwise solutions, demonstrating recursion/context-splitting enables scaling beyond single-context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e292.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e292.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tiny Transformer (RoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Small Transformer (536K params) trained with Recursion of Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A randomly-initialized, small Transformer (536K parameters) trained from scratch with RoT and supervised recursive traces, able to learn complex arithmetic up to very large digit lengths in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Tiny Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>536K</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Transformer (decoder/sequence model, context limit 2048 in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction up to 64 digits; multiplication/division up to 32 digits in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Addition/subtraction tested to 64 digits; multiplication/division tested to 32 digits; problems include large multi-digit integers that exceed typical numeric datatypes (e.g., 32-digit > 64-bit).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Trained from random initialization with supervised RoT dataset (per-problem recursive contexts sampled); batches built from unique contexts; evaluation on large test sets (30K problems, repeated runs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>RoT-trained tiny Transformer achieved near-perfect accuracy across tested arithmetic tasks and sizes (qualitative); CoT also achieves high accuracy on small problems but hits context-size limits; WT accuracy drops quickly as difficulty increases. Exact numeric accuracies presented as averages and stds in figures (not tabulated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Even without pretraining, the small Transformer learned algorithmic, recursive procedures (digit-splitting, carry/borrow, tail recursion) when trained with RoT supervision; the model uses the same THINK/GO/STOP/TAIL protocol to orchestrate subproblem solving and answer composition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Despite small parameter counts, RoT enables scaling to extremely complex problems by distributing reasoning across many short contexts; limitations are primarily compute and dataset generation cost for evaluation, not model capacity for the demonstrated sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Training-only on a fixed length does not enable length generalization; overall system sensitive to per-subproblem error rates given the large number of composed subproblems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with WT and CoT baselines trained under same data; WT degrades rapidly with difficulty, CoT performs well where context fits but cannot handle largest tasks; RoT matches or exceeds where CoT applicable and extends beyond it.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>RoT supervision allows even tiny, unpretrained Transformers to learn and execute multi-step arithmetic algorithms at large scales (dozens of digits) by dividing problems into recursive contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e292.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e292.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tiny LSTM (RoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Small LSTM (272K params) trained with Recursion of Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small LSTM sequence model (272K parameters) trained from scratch with RoT supervision, evaluated on arithmetic tasks to test the model-agnostic nature of RoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Tiny LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>272K</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>LSTM sequence model (context limit 512 in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (same set as other experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Context limit smaller (512); experiments vary but tested arithmetic tasks at sizable digit lengths consistent with this context constraint (difficulties lower than tiny Transformer due to context limit).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Trained from random initialization with supervised RoT contexts and target sequences; comparison to Transformer and GPT-3 under same RoT framework.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>RoT improved reasoning capability for the LSTM, enabling it to solve more complex problems than WT, although specific numeric accuracies are not tabulated in the main text. Patterns are consistent: WT worst, CoT good when applicable, RoT scales best.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>RoT's recursive context mechanism is architecture-agnostic: LSTM learned to emit THINK and handle subcontext-based recursion to perform arithmetic decomposition; LSTM required shorter context windows but still benefited from dividing reasoning across contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Limited by smaller context length (512) and model capacity, but RoT still enabled scaling up problem complexity relative to WT for the same architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Same general limitations: no length generalization beyond training lengths; heavy reliance on extremely accurate subproblem solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to WT and CoT baselines; similar qualitative patterns as other architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>RoT works across architectures: even small LSTMs can learn recursive, algorithmic arithmetic procedures when trained with supervised multi-context traces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e292.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e292.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain of Thought (unrolled, non-recursive baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline where the model is trained to generate all intermediate steps sequentially in a single context (standard chain-of-thought fine-tuning), using the same ground-truth intermediate steps as the RoT decomposition but without recursion/multiple contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (same task suite when applicable)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Applicable to problems whose full chain-of-thought fits within the single-context limit (e.g., smaller-digit problems); cannot handle problems where CoT context exceeds model context size (e.g., very large-digit problems targeted by RoT).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Supervised training to generate entire chain-of-thought in one context (unrolled recursive traces).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Achieves near-perfect accuracy on problems where the CoT context fits under the context-length limit; however, CoT cannot be applied to the largest problems due to context-size constraints (e.g., 2048-token limit for GPT-3), so no results for those scales.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Solves by serially emitting intermediate steps within one long context; does not address context-length constraints and therefore cannot exploit arbitrarily long solution traces.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Good accuracy for small-to-moderate sized problems but performance halts at maximum context size; unlike RoT, CoT cannot scale the total reasoning length beyond single-context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Fails to be applicable when required CoT length exceeds model context size; massive single-context generation is infeasible for problems requiring hundreds of thousands of tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Directly compared with RoT and WT in experiments: CoT matches accuracy where applicable but is limited by context length; RoT extends CoT's accuracy to far larger overall reasoning workloads.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Chain-of-Thought fine-tuning works for problems within the context window but cannot scale to problems with extremely long reasoning traces; RoT provides a mechanism to overcome that limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e292.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e292.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WT (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Without Thought (direct-answer baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline where the model is trained to output the answer directly from the question without generating intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Tested across the same ranges as other methods; accuracy degrades quickly as operand digit counts increase.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Supervised fine-tuning to map questions directly to final answers (no intermediate step generation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Performs poorly on larger/difficult arithmetic tasks even after fine-tuning; accuracy drops quickly as problem difficulty (number of digits) increases â€” far worse than CoT or RoT.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Likely relies on memorization or shallow pattern learning; lacks compositional intermediate computations and so fails on multi-step digitwise algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Degrades rapidly with increased operand digit count / problem complexity; cannot scale to large-digit arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Breaks down on multi-digit arithmetic requiring carries/borrows/long multiplication/division; insufficient for problems that require many intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as the direct-answer baseline in comparisons with CoT and RoT; RoT and CoT dramatically outperform WT for multi-step arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Direct-answer fine-tuning (WT) is insufficient for large multi-digit arithmetic; generating structured intermediate computation (via CoT or RoT) is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Decomposed prompting: A modular approach for solving complex tasks <em>(Rating: 1)</em></li>
                <li>Theoretical limitations of selfattention in neural sequence models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-292",
    "paper_id": "paper-ef5f7cd21b5d34797636239a7b9c8ba6af440aab",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "RoT",
            "name_full": "Recursion of Thought",
            "brief_description": "A model-agnostic inference framework that lets language models generate special tokens (GO, STOP, THINK, TAIL) to create and manage multiple short contexts and recursively solve subproblems, enabling divide-and-conquer reasoning beyond the single-context length limit.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (and other algorithmic tasks via recursive decomposition)",
            "number_range_or_complexity": "Demonstrated on very large problems: up to 48-digit addition/subtraction and 16-digit multiplication/division with GPT-3; up to 64-digit addition/subtraction and 32-digit multiplication/division with tiny Transformer (experiments vary by setup).",
            "method_or_intervention": "Supervised fine-tuning of sequence models with ground-truth recursive intermediate steps and special tokens; models trained to output THINK instead of sub-answer first token; iterative recursive inference replaces THINK with returned sub-answer.",
            "performance_result": "Enables near-perfect accuracy (qualitative) across tested arithmetic tasks and scales where Chain-of-Thought cannot be applied due to context limits; RoT-trained models solved problems requiring up to hundreds of thousands of reasoning tokens in experiments.",
            "mechanistic_insight": "Encourages models to implement explicit divide-and-conquer, elementary-school style algorithms: decomposition into digit-level subproblems (last-digit add/subtract, carry/borrow handling, digit-splitting multiplication, long-division via compare-and-subtract), using THINK/TAIL recursion to spawn contexts and replace THINK with returned answers; training uses supervised intermediate steps so the model learns when to recurse and how to combine subresults.",
            "performance_scaling": "RoT decouples total reasoning length from single-context limit: performance remains high as problem complexity (number of subproblems / total tokens) grows, constrained primarily by per-subproblem accuracy rather than context length. No emergent length generalization was achieved (training on shorter lengths did not generalize to longer lengths).",
            "failure_modes": "Requires extremely high per-subproblem accuracy (e.g., &gt;99.9%) because many subproblems compose into one final problem; does not by itself confer length generalization (models trained on N-digit tasks do not necessarily generalize to 2N-digit tasks); currently depends on supervised ground-truth intermediate steps.",
            "comparison_baseline": "Compared against direct-answer baseline (WT) and standard Chain-of-Thought (CoT) trained baseline; RoT outperforms WT and matches CoT accuracy where CoT is applicable, while RoT scales beyond CoT's context limits.",
            "key_finding": "By teaching models to spawn and manage recursive short contexts via special tokens, language models can learn algorithmic, elementary-school procedures and solve extremely large arithmetic problems (100K+ token reasoning traces) with near-perfect accuracy, overcoming single-context length limits.",
            "uuid": "e292.0",
            "source_info": {
                "paper_title": "Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "GPT-3 (RoT experiments)",
            "name_full": "GPT-3 fine-tuned with Recursion of Thought",
            "brief_description": "GPT-3 was fine-tuned with the RoT training procedure on synthetic arithmetic and algorithmic tasks and evaluated on very large-digit arithmetic; RoT enabled GPT-3 to solve problems far beyond the 2048-token single-context limit.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (also algorithmic tasks such as LCS, LPS, 0-1 knapsack, MCM)",
            "number_range_or_complexity": "Reported experiments include up to 48-digit addition/subtraction and 16-digit multiplication/division (and other sizes); problems that lead to CoT solutions exceeding 2048 tokens were targeted.",
            "method_or_intervention": "Fine-tuning via OpenAI API for 10K steps, batch size 256, supervised RoT training with ground-truth recursive intermediate steps and special tokens (GO/THINK/STOP/TAIL).",
            "performance_result": "Qualitatively 'near-perfect' accuracy across every reported GPT-3 RoT experiment; WT failed on these complexities and CoT was not applicable because of the 2048 context limit. (Exact numeric accuracies not enumerated in main text.)",
            "mechanistic_insight": "GPT-3, when supervised with RoT, learned to emit THINK/TAIL to create recursive subcontexts and to perform digit-wise elementary algorithms (carries, borrows, splits, long division trials) rather than relying on black-box memorization; recursion token mechanism lets the model offload intermediate computations into new contexts returned as answers.",
            "performance_scaling": "With RoT, GPT-3 maintained high accuracy as operand digit lengths and total reasoning tokens increased well beyond single-context limits; CoT would be limited by context length (2048) while RoT scales to much larger overall reasoning traces.",
            "failure_modes": "Prompting-only methods could not reach required per-subproblem accuracy; RoT still fails to generalize to larger lengths than seen in training (no length generalization).",
            "comparison_baseline": "Compared to Without Thought (WT) and Chain-of-Thought (CoT) baselines; WT failed; CoT matched accuracy when context-length permitted but could not be used for largest problems due to context limit.",
            "key_finding": "Fine-tuning GPT-3 with supervised RoT yields near-perfect arithmetic performance on tasks (tens of digits and beyond) that produce extremely long stepwise solutions, demonstrating recursion/context-splitting enables scaling beyond single-context limits.",
            "uuid": "e292.1",
            "source_info": {
                "paper_title": "Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Tiny Transformer (RoT)",
            "name_full": "Small Transformer (536K params) trained with Recursion of Thought",
            "brief_description": "A randomly-initialized, small Transformer (536K parameters) trained from scratch with RoT and supervised recursive traces, able to learn complex arithmetic up to very large digit lengths in experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Tiny Transformer",
            "model_size": "536K",
            "model_architecture": "Transformer (decoder/sequence model, context limit 2048 in experiments)",
            "arithmetic_operation_type": "addition, subtraction up to 64 digits; multiplication/division up to 32 digits in experiments",
            "number_range_or_complexity": "Addition/subtraction tested to 64 digits; multiplication/division tested to 32 digits; problems include large multi-digit integers that exceed typical numeric datatypes (e.g., 32-digit &gt; 64-bit).",
            "method_or_intervention": "Trained from random initialization with supervised RoT dataset (per-problem recursive contexts sampled); batches built from unique contexts; evaluation on large test sets (30K problems, repeated runs).",
            "performance_result": "RoT-trained tiny Transformer achieved near-perfect accuracy across tested arithmetic tasks and sizes (qualitative); CoT also achieves high accuracy on small problems but hits context-size limits; WT accuracy drops quickly as difficulty increases. Exact numeric accuracies presented as averages and stds in figures (not tabulated in text).",
            "mechanistic_insight": "Even without pretraining, the small Transformer learned algorithmic, recursive procedures (digit-splitting, carry/borrow, tail recursion) when trained with RoT supervision; the model uses the same THINK/GO/STOP/TAIL protocol to orchestrate subproblem solving and answer composition.",
            "performance_scaling": "Despite small parameter counts, RoT enables scaling to extremely complex problems by distributing reasoning across many short contexts; limitations are primarily compute and dataset generation cost for evaluation, not model capacity for the demonstrated sizes.",
            "failure_modes": "Training-only on a fixed length does not enable length generalization; overall system sensitive to per-subproblem error rates given the large number of composed subproblems.",
            "comparison_baseline": "Compared with WT and CoT baselines trained under same data; WT degrades rapidly with difficulty, CoT performs well where context fits but cannot handle largest tasks; RoT matches or exceeds where CoT applicable and extends beyond it.",
            "key_finding": "RoT supervision allows even tiny, unpretrained Transformers to learn and execute multi-step arithmetic algorithms at large scales (dozens of digits) by dividing problems into recursive contexts.",
            "uuid": "e292.2",
            "source_info": {
                "paper_title": "Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Tiny LSTM (RoT)",
            "name_full": "Small LSTM (272K params) trained with Recursion of Thought",
            "brief_description": "A small LSTM sequence model (272K parameters) trained from scratch with RoT supervision, evaluated on arithmetic tasks to test the model-agnostic nature of RoT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Tiny LSTM",
            "model_size": "272K",
            "model_architecture": "LSTM sequence model (context limit 512 in experiments)",
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (same set as other experiments)",
            "number_range_or_complexity": "Context limit smaller (512); experiments vary but tested arithmetic tasks at sizable digit lengths consistent with this context constraint (difficulties lower than tiny Transformer due to context limit).",
            "method_or_intervention": "Trained from random initialization with supervised RoT contexts and target sequences; comparison to Transformer and GPT-3 under same RoT framework.",
            "performance_result": "RoT improved reasoning capability for the LSTM, enabling it to solve more complex problems than WT, although specific numeric accuracies are not tabulated in the main text. Patterns are consistent: WT worst, CoT good when applicable, RoT scales best.",
            "mechanistic_insight": "RoT's recursive context mechanism is architecture-agnostic: LSTM learned to emit THINK and handle subcontext-based recursion to perform arithmetic decomposition; LSTM required shorter context windows but still benefited from dividing reasoning across contexts.",
            "performance_scaling": "Limited by smaller context length (512) and model capacity, but RoT still enabled scaling up problem complexity relative to WT for the same architecture.",
            "failure_modes": "Same general limitations: no length generalization beyond training lengths; heavy reliance on extremely accurate subproblem solutions.",
            "comparison_baseline": "Compared to WT and CoT baselines; similar qualitative patterns as other architectures.",
            "key_finding": "RoT works across architectures: even small LSTMs can learn recursive, algorithmic arithmetic procedures when trained with supervised multi-context traces.",
            "uuid": "e292.3",
            "source_info": {
                "paper_title": "Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "CoT (baseline)",
            "name_full": "Chain of Thought (unrolled, non-recursive baseline)",
            "brief_description": "A baseline where the model is trained to generate all intermediate steps sequentially in a single context (standard chain-of-thought fine-tuning), using the same ground-truth intermediate steps as the RoT decomposition but without recursion/multiple contexts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (same task suite when applicable)",
            "number_range_or_complexity": "Applicable to problems whose full chain-of-thought fits within the single-context limit (e.g., smaller-digit problems); cannot handle problems where CoT context exceeds model context size (e.g., very large-digit problems targeted by RoT).",
            "method_or_intervention": "Supervised training to generate entire chain-of-thought in one context (unrolled recursive traces).",
            "performance_result": "Achieves near-perfect accuracy on problems where the CoT context fits under the context-length limit; however, CoT cannot be applied to the largest problems due to context-size constraints (e.g., 2048-token limit for GPT-3), so no results for those scales.",
            "mechanistic_insight": "Solves by serially emitting intermediate steps within one long context; does not address context-length constraints and therefore cannot exploit arbitrarily long solution traces.",
            "performance_scaling": "Good accuracy for small-to-moderate sized problems but performance halts at maximum context size; unlike RoT, CoT cannot scale the total reasoning length beyond single-context limits.",
            "failure_modes": "Fails to be applicable when required CoT length exceeds model context size; massive single-context generation is infeasible for problems requiring hundreds of thousands of tokens.",
            "comparison_baseline": "Directly compared with RoT and WT in experiments: CoT matches accuracy where applicable but is limited by context length; RoT extends CoT's accuracy to far larger overall reasoning workloads.",
            "key_finding": "Chain-of-Thought fine-tuning works for problems within the context window but cannot scale to problems with extremely long reasoning traces; RoT provides a mechanism to overcome that limitation.",
            "uuid": "e292.4",
            "source_info": {
                "paper_title": "Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "WT (baseline)",
            "name_full": "Without Thought (direct-answer baseline)",
            "brief_description": "A baseline where the model is trained to output the answer directly from the question without generating intermediate steps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division",
            "number_range_or_complexity": "Tested across the same ranges as other methods; accuracy degrades quickly as operand digit counts increase.",
            "method_or_intervention": "Supervised fine-tuning to map questions directly to final answers (no intermediate step generation).",
            "performance_result": "Performs poorly on larger/difficult arithmetic tasks even after fine-tuning; accuracy drops quickly as problem difficulty (number of digits) increases â€” far worse than CoT or RoT.",
            "mechanistic_insight": "Likely relies on memorization or shallow pattern learning; lacks compositional intermediate computations and so fails on multi-step digitwise algorithms.",
            "performance_scaling": "Degrades rapidly with increased operand digit count / problem complexity; cannot scale to large-digit arithmetic.",
            "failure_modes": "Breaks down on multi-digit arithmetic requiring carries/borrows/long multiplication/division; insufficient for problems that require many intermediate steps.",
            "comparison_baseline": "Used as the direct-answer baseline in comparisons with CoT and RoT; RoT and CoT dramatically outperform WT for multi-step arithmetic.",
            "key_finding": "Direct-answer fine-tuning (WT) is insufficient for large multi-digit arithmetic; generating structured intermediate computation (via CoT or RoT) is critical.",
            "uuid": "e292.5",
            "source_info": {
                "paper_title": "Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 1
        },
        {
            "paper_title": "Decomposed prompting: A modular approach for solving complex tasks",
            "rating": 1
        },
        {
            "paper_title": "Theoretical limitations of selfattention in neural sequence models",
            "rating": 1
        }
    ],
    "cost": 0.01436,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models</h1>
<p>Soochan Lee<br>Seoul National University<br>soochan.lee@vision.snu.ac.kr</p>
<p>Gunhee Kim<br>Seoul National University<br>SNU-LG AI Research Center<br>gunhee@snu.ac.kr</p>
<h4>Abstract</h4>
<p>Generating intermediate steps, or Chain of Thought (CoT), is an effective way to significantly improve language models' (LM) multistep reasoning capability. However, the CoT lengths can grow rapidly with the problem complexity, easily exceeding the maximum context size. Instead of increasing the context limit, which has already been heavily investigated, we explore an orthogonal direction: making LMs divide a problem into multiple contexts. We propose a new inference framework, called Recursion of Thought (RoT), which introduces several special tokens that the models can output to trigger context-related operations. Extensive experiments with multiple architectures including GPT-3 show that RoT dramatically improves LMs' inference capability to solve problems, whose solution consists of hundreds of thousands of tokens.</p>
<h2>1 Introduction</h2>
<p>Recently, LMs have become a prominent direction to solve reasoning. Given a question sequence, the models are tasked to predict the following answer sequence. One recent line of research for reasoning with LMs is chain of thought (CoT) generation (Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022). In CoT generation, complex reasoning problems are solved by generating intermediate reasoning steps, or chain of thought, before producing the final answer. This allows the problem's complexity to be spread across multiple token generations, making each generation more straightforward given the previous tokens.</p>
<p>Although CoT dramatically increases reasoning accuracy, there is a critical issue that limits its utility: the effective context size of sequence models cannot grow unbounded. Context refers to the set of input tokens that a model is conditioned on when generating output. Practically, all sequence models have a limit on the maximum context length due to various reasons. For instance, Transformers
(Vaswani et al., 2017) suffer from a quadratic computational cost on the context length, and RNNs (Hochreiter and Schmidhuber, 1997) struggle with long-term dependency modeling. Therefore, even the state-of-the-art LMs limit the maximum context length to a few thousand tokens. However, complex real-world problems may take even millions of tokens of reasoning steps to reach the answer.</p>
<p>While there has been extensive research on Transformers with longer contexts (Tay et al., 2020b), we explore an orthogonal direction: divide and conquer. Our new model-agnostic inference framework Recursion of Thought (RoT) lets an LM recursively create multiple contexts by producing special tokens. Therefore, even if a problem's solution exceeds the maximum context size, the model can divide it into multiple short contexts. We show the potential of RoT with our new synthetic benchmark consisting of eight arithmetic and algorithmic tasks. One can easily adjust the difficulty of the tasks to produce problems with extremely long ( $100 \mathrm{~K}+$ tokens) reasoning steps. Without any task-specific component, such as a calculator, the models with RoT can easily learn to solve extremely complex problems whose solutions consist of hundreds of thousands of tokens. To the best of our knowledge, no previous work comes close to handling this scale of reasoning procedures. Since RoT is an early exploration in this direction, it needs several improvements to be applied to more practical scenarios. Nonetheless, the impressive experimental results suggest that the multi-context paradigm of RoT might play an important role in future LMs. We also provide our PyTorch (Paszke et al., 2019) implementation that can fully reproduce the experiments. ${ }^{1}$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>2 Related Work</p>
<p>Scratchpad <em>Nye et al. (2021)</em> is one of the earliest approaches demonstrating that fine-tuning language models to produce CoT can largely improve reasoning accuracy. In the paper, the authors also mention the confined context size as a major hurdle to scaling their method. More recently, it has been found that sufficiently large pre-trained language models can be induced to produce CoT, by simply tuning the prompt <em>Wei et al. (2022); Kojima et al. (2022)</em>. Several concurrent works extend CoT prompting to decompose complex problems into smaller problems <em>Dua et al. (2022); Zhou et al. (2022); Khot et al. (2022)</em>. Although these works also share the principle of divide and conquer like RoT, they mostly focus on improving the reasoning accuracy of relatively small problems whose solutions usually can fit in a single context. On the other hand, we focus on solving problems that the solutions are orders of magnitude longer than the context size. More detailed description of related work can be found in Appendix A.</p>
<h2>3 Recursion of Thought (RoT)</h2>
<h3>3.1 Inference</h3>
<p>We start with how an RoT-trained LM performs at test time. RoT is a model-agnostic framework, whose only requirement is that the model can infer $p(x_{i+1}|X_{1: i})$, the probability of the next token $x_{i+1}$ given a sequence $X_{1: i}=[x_{1} ; \ldots ; x_{i}]$. For recursive context control, we introduce the following special tokens: GO, STOP, and THINK. GO and STOP respectively mark the start and end of a problem sequence. They can be nested inside another GO-STOP pair to indicate a subproblem. THINK initiates a recursion procedure. RoT teaches a model how to use these tokens so that it can perform divide-and-conquer problem-solving. We formulate each inference context of a QA problem, denoted $X$, as the following concatenation:</p>
<p>$X=[Q ; Q^{\mathrm{sub}, 1} ; A^{\mathrm{sub}, 1} ; \ldots ; Q^{\mathrm{sub}, N} ; A^{\mathrm{sub}, N} ; A]$
where $Q$ and $A$ are the main question and answer sequence, and $Q^{\mathrm{sub},<em>}$ and $A^{\mathrm{sub},</em>}$ are those of the top-level subproblems. Although a subproblem can have smaller, lower-level subproblems recursively, only the top-level subproblems remain in an RoT context. During inference, a model is given $Q$ and tasked to generate the rest. Questions ( $Q$ and
$Q^{\mathrm{sub},<em>}$ ) start with a GO token, and answers ( $A$ and $A^{\mathrm{sub},</em>}$ ) end with a STOP token. In the base cases, contexts do not have $\left(Q^{\mathrm{sub},<em>}, A^{\mathrm{sub},</em>}\right)$ pairs.</p>
<p>Figure 1 presents an example of solving $408+$ 351 for better understanding. The pseudocode and more detailed illustrations can be found in Appendix B and E. RoT starts by initializing the context $X$ with the original question $Q$ (i.e., GO 40 $8+351 \times$ in Figure 1). Then, similar to CoT, the model solves multiple subproblems (generating $Q^{\mathrm{sub},<em>}$ and $A^{\mathrm{sub},</em>}$ ) before producing the final answer. However, there is a key difference: instead of producing a sub-answer directly, the model outputs the THINK token. This special token triggers a recursive process that separates the sub-question in a new context. If the new context is a base case (i.e., $X^{2}, X^{4}$, and $X^{5}$ ), the answer is produced directly. Otherwise, the model recursively solves more subproblems. If enough subproblems are solved, the model generates the final answer ending with a STOP. Once an answer is returned to the previous context, it replaces the THINK token, and the generation continues.</p>
<p>For tail recursion, where the last subquestionâ€™s answer becomes the final answer, we additionally introduce the TAIL token. If TAIL is used in the place of a GO token in the last subquestion $Q^{\mathrm{sub}, N}$, its answer $A^{\mathrm{sub}, N}$ is treated as the final answer $A$. Tail recursion is crucial since it enables an indefinitely long chain of recursion without overflowing the call stack.</p>
<p>The generality of RoT. Recursion, the core of RoT, is an incredibly general concept that serves as a fundamental building block for functional programming languages. Any non-recursive procedure can be converted to a recursive form via continuation-passing style. Therefore, there is no theoretical limit on the range of problems that RoT can handle.</p>
<h3>3.2 Training</h3>
<p>Currently, we train RoT in a supervised manner, using ground truth (GT) intermediate steps that include when to output the special tokens. The GTs are constructed following the standard procedures developed for humans. For example, the procedures for arithmetic problems are borrowed from elementary school math. More details can be found in Appendix H. We leave training RoT with less supervision as a future work.</p>
<p>Each training example is constructed as a pair</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of the Recursion of Thought inference solving 408+351. Each table represents an inference context <em>X</em><em> in order of creation. For each context, the model is given </em>Q<em> and tasked to generate the rest. The model outputs the THINK token when it needs to generate </em>A<em><sup></em></sup>, the answer of a subproblem. The THINK token triggers a recursive process and is later replaced by the answer returned from the process.</p>
<p>of a ground truth context sequence <em>X</em> and the corresponding target sequence <em>Y</em>. An example and the pseudocode for creating a target sequence are presented in Figure 3 and Algorithm 2 in Appendix D. Overall, <em>Y</em> is a copy of <em>X</em> except for the parts corresponding to <em>Q</em> and <em>A</em><sup><em></sup>. Since the question </em>Q<em> is always given in a context, </em>Q<em> is replaced by special PAD tokens, which are excluded from the loss function. Each subproblem's answer </em>A<em><sup></em>sub<em></sup> is replaced by a THINK token followed by several PADs that fill in the rest to make sure |</em>X<em>| = |</em>Y<em>|. This way, the model is trained to output THINK instead of the first token of </em>A<em><sup></em>sub<em></sup>. Since the whole </em>A<em><sup></em>sub<em></sup> will be returned from the recursive process and replace the THINK during inference, we do not need a training signal for the rest of </em>A<em><sup></em>sub*</sup>.</p>
<p>Given a pair âŸ¨<em>X</em>, <em>Y</em>âŸ©, the training objective is defined as follows:</p>
<p>$$
\mathcal{L} = -\sum_{i} I[y_{i+1} \ne \text{PAD}] \log p(y_{i+1}|X_{1:i}) \tag{2}
$$</p>
<p>where <em>I</em> is the indicator function that excludes PADs from training. Its form is almost identical to the standard LM objective: $\mathcal{L}<em i="i">{\text{LM}} = -\sum</em>)$, which is to predict the next token given previous tokens. Therefore, any sequence model is trained in the standard way, i.e., end-to-end via stochastic gradient descent.} \log p(x_{i+1}|X_{1:i</p>
<h2>4 Experiments</h2>
<h3>4.1 Baselines</h3>
<p>We compare RoT with two baselines. The first one is to output an answer directly from a question, which we call <em>Without Thought</em> (WT). The other one is to generate all the intermediate steps before the answer without recursion (Nye et al., 2021), which we refer to as <em>Chain of Thought</em> (CoT; not to be confused with the CoT <em>prompting</em> (Wei et al., 2022)). We construct the ground truths for CoTs by unraveling the same recursive process which we design for RoT, into a single context sequence (see Appendix F for examples). Therefore, the number of tokens to generate while solving a problem is the same for both CoT and RoT (if we do not count the THINK tokens). However, the sizes of the individual contexts of CoT are far longer than those of RoT due to the recursively nested subproblems, limiting the range of solvable problems. Refer to Appendix M for a more detailed analysis of the context sizes. For a fair comparison, we <em>train</em> these baselines and do not use any prompting technique. When evaluating, we consider a problem to be correctly solved only if all the intermediate steps and the answer are correct.</p>
<h3>4.2 The Reasoning Problems</h3>
<p>To evaluate the reasoning capabilities, we test four basic arithmetic tasks and four algorithmic tasks: addition, subtraction, multiplication, division, longest common subsequence, longest palindromic subsequence, 0-1 knapsack, and matrix chain multiplication. The details can be found in Appendix G. We choose these tasks because we can easily increase the problem difficulty while being able to get ground truth solutions. Therefore, we can test problems whose solution contains hundreds of thousands of tokens. All problems are formulated in pure sequence modeling, without any external programs (e.g., calculator) involved.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparison of the thought types. In each graph, the x-axis is the problem difficulty, while the y-axis is the reasoning accuracy. Each point represents an independent experiment. The green vertical lines indicate the maximum problem difficulty that CoT can handle without exceeding the maximum context size.</p>
<h3>4.3 Experiments with GPT-3</h3>
<p>Using the OpenAI API, we fine-tune GPT-3 for each reasoning task in Â§4.2 for 10K steps with a batch size of 256. The results are presented in Figure 2a, and the technical details are described in Appendix I. Each point in the graphs represents one experiment at a certain problem difficulty. We report the accuracy on a test set of 1K unique problems randomly sampled as explained in Appendix G. To the best of our knowledge, the problems at this scale (e.g., 48-digit addition/subtraction and 16-digit multiplication/division) have never been solved by any LM without the help of external programs. For reference, Minerva <em>Lewkowycz et al. (2022)</em> achieves around 80% accuracy on 10-digit addition and 20% on 18-digit addition.</p>
<p><strong>Results.</strong> Even WT fine-tuning cannot make GPT-3 deal with such a level of complexity, while CoT is not applicable due to the context limit of 2048. The green dotted lines mark the maximum difficulty that can be handled by CoT under the context limit. On the other hand, RoT enables the GPT-3 to achieve near-perfect scores in every experiment. As presented in Appendix M, solving each problem requires up to tens of thousands of tokens. Without any architectural change, RoT makes GPT-3 handle these extremely complex problems.</p>
<h3>4.4 Experiments with Tiny Language Models</h3>
<p>Recent research on reasoning has been mostly focused on extremely large pre-trained LMs. In this section, we show an interesting result that RoT can make even tiny models, without any pre-training, perform convoluted reasoning procedures. We test the two basic sequence model architectures: a Transformer <em>Vaswani et al. (2017)</em> with 536K parameters and an LSTM <em>Hochreiter and Schmidhuber (1997)</em> with 272K parameters. These models are more than a million times smaller than the recent 540B-parameter PaLM <em>Chowdhery et al. (2022)</em>. The context limit is set to 2048 for the Transformer and 512 for the LSTM.</p>
<p>By virtue of their small sizes, we conduct far more extensive experiments than GPT-3, which are presented in Figure 2b and Figure 2c. For each experiment, we train a randomly initialized model and evaluate it on a test set of 30K unique problems. We repeat each experiment eight times and report the average and standard deviation of the accuracies. With the tiny Transformer, we experiment to the extent that even humans would find daunting. For example, we test addition/subtraction up to 64 digits and multiplication/division up to 32 digits. Note that a 32-digit number cannot even fit into the 64-bit integer datatype.</p>
<p>Throughout the experiments, we observe consistent patterns: (i) WT's accuracy drops most quickly as the problem difficulty increases, (ii) CoT achieves near-perfect accuracy, but it can only be applied to simple problems due to the context limit, (iii) RoT achieves near-perfect accuracy and can be scaled up to extremely complex problems. Despite</p>
<p>the small sizes, RoT makes the Transformers master all types of extremely complex problems. We do not test more difficult problems mainly because the evaluation becomes too costly, not because RoT is incapable of learning them.</p>
<h2>5 Conclusion</h2>
<p>We explored the novel idea of making LMs produce special tokens to create multiple contexts. Following the principle of divide and conquer, LMs with RoT can solve extremely complex problems that have never been handled by any LM. We believe the core idea of utilizing multiple contexts has a great potential and can play an essential role in future language models.</p>
<h2>Limitations</h2>
<p>Although RoT remarkably improves LMsâ€™ reasoning capability, we currently rely on supervised training to teach RoT. To apply RoT to a wider range of tasks, it would be crucial to reduce the expensive supervision. Parallel to our work, Khot et al. (2022) use prompting techniques to induce LMs to decompose problems. However, prompting has other drawbacks. First, lengthy prompts should be added for each inference, causing additional computational overhead. And more critically, it is hard to guarantee high accuracy. To achieve reasonable accuracy in the tasks in our experiments, each subproblem should be solved at extremely high accuracy (e.g., $&gt;99.9 \%$ ) since each problem may contain hundreds or thousands of subproblems. We have tested several prompting techniques with GPT-3, but could not get satisfactory accuracy. Therefore, we conclude that solely relying on prompting cannot be a solution to this problem. As one possible approach, we may combine RoT with the RL-based methodologies that are developed for reducing supervision of Neural ProgrammerInterpreters (Li et al., 2017; Fox et al., 2018; Pierrot et al., 2019).</p>
<p>Another limitation of this work is that the experiments are performed on somewhat synthetic tasks. Since our goal is to enable LMs to solve reasoning problems whose intermediate steps are orders of magnitude longer than the context limit, we need a dataset with such complex problems. However, no currently available dataset meets this requirement. For example, the Long-Range Arena benchmark (Tay et al., 2020a) has at most 16K-token sequences and focuses on problems with long inputs and short outputs. On the other hand, we tackle problems that require generating 100K+ tokens to solve. Gathering natural language data at this scale is extremely challenging and costly. Therefore, we currently resort to arithmetic and algorithmic problems since it is easy to scale them up and generate ground-truth solutions. In the future, we hope to see new datasets and benchmarks that cover natural language reasoning at this scale.</p>
<p>Interestingly, RoT cannot facilitate length generalization, e.g., training on 8-digit multiplication with RoT cannot make a model generalize to 16digit multiplication. We believe this problem is rooted in a more fundamental limitation of the Transformer architecture (Hahn, 2020), orthogonal to RoT. Fortunately, since RoT is a model-agnostic framework, we would be able to apply RoT to more advanced architectures to come in the future, which might be capable of length generalization.</p>
<h2>Ethics Statement</h2>
<p>Since the problem types in our experiments are pure arithmetic or algorithmic tasks, we do not find any ethical concerns directly related to our work. If RoT is applied to more general problems, the training data should meet ethical standards to ensure the non-toxic behavior of the model.</p>
<h2>Acknowledgements</h2>
<p>We thank Jaekyeom Kim, Hyunwoo Kim, and Dongjoo Kim for their thoughtful discussions. This work is partly supported by LG AI Research, the Institute of Information \&amp; Communications Technology Planning \&amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-01082, SW StarLab; No.2022-0-00156, Fundamental research on continual meta-learning for quality enhancement of casual videos and their 3D metaverse transformation), and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No.2023R1A2C2005573).</p>
<h2>References</h2>
<p>Jonathon Cai, Richard Shin, and Dawn Song. 2017. Making neural programming architectures generalize via recursion. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul</p>
<p>Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek B Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier GarcÃ­a, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling language modeling with pathways. ArXiv, abs/2204.02311.</p>
<p>Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. 2022. Successive prompting for decomposing complex questions. ArXiv, abs/2212.04092.</p>
<p>Roy Fox, Richard Shin, Sanjay Krishnan, Ken Goldberg, Dawn Song, and Ion Stoica. 2018. Parametrized hierarchical procedures for neural programming. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.</p>
<p>Michael Hahn. 2020. Theoretical limitations of selfattention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156171 .</p>
<p>Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9:17351780 .</p>
<p>Tushar Khot, H. Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022. Decomposed prompting: A modular approach for solving complex tasks. ArXiv, abs/2210.02406.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. ArXiv, abs/2205.11916.</p>
<p>Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra.
2022. Solving quantitative reasoning problems with language models. ArXiv, abs/2206.14858.</p>
<p>Chengtao Li, Daniel Tarlow, Alexander L. Gaunt, Marc Brockschmidt, and Nate Kushman. 2017. Neural program lattices. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show your work: Scratchpads for intermediate computation with language models. ArXiv, abs/2112.00114.</p>
<p>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems 32, pages 8024-8035. Curran Associates, Inc.</p>
<p>Thomas Pierrot, Guillaume Ligner, Scott E. Reed, Olivier Sigaud, Nicolas Perrin, Alexandre Laterre, David Kas, Karim Beguir, and Nando de Freitas. 2019. Learning compositional neural programs with recursive tree search and planning. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 14646-14656.</p>
<p>Scott E. Reed and Nando de Freitas. 2016. Neural programmer-interpreters. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.</p>
<p>Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020a. Long range arena: A benchmark for efficient transformers. ArXiv, abs/2011.04006.</p>
<p>Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020b. Efficient transformers: A survey. ACM Computing Surveys, 55:1 - 28.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903.</p>
<p>Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Huai hsin Chi. 2022. Least-to-most prompting enables complex reasoning in large language models. ArXiv, abs/2205.10625.</p>
<h1>A Extended Related Work</h1>
<p>Chain of Thought. Scratchpad (Nye et al., 2021) fine-tunes LMs to generate CoT before the final answer. It demonstrates its effectiveness in 8-digit addition, polynomial evaluation, and Python program execution. Instead of fine-tuning, it is found that we can elicit large pre-trained LMs to produce CoT with appropriate prompting. For example, CoT prompting (Wei et al., 2022) adds several QA exemplars with CoT before the main question, encouraging the model to generate final answers in a similar manner. Compared to the few-shot CoT prompting of Wei et al. (2022), Kojima et al. (2022)'s zero-shot CoT prompting is even simpler; after a question, they start the answer with "Let's think step by step," and then let the model finish the rest. Minerva (Lewkowycz et al., 2022) utilizes these prompting techniques with a specially curated scientific pre-training dataset to achieve remarkable results on various reasoning benchmarks.</p>
<p>Prompting Language Models to Divide and Conquer Reasoning Problems. Based on CoT prompting (Wei et al., 2022), several concurrent works demonstrate that decomposing problems into smaller subproblems can effectively improve reasoning accuracy. Successive prompting (Dua et al., 2022) induces a model to alternate between generating a question and answering the question until the final answer is produced. Similarly, least-to-most prompting (Zhou et al., 2022) makes a model start from the easiest subproblem and progressively solve more complex ones on top of the previous results. Decomposed prompting (Khot et al., 2022) is a modular approach that the subproblems are solved by different modules depending on the problem type. It also supports recursive decomposition. These works are all closely related to our work. Our work is unique in that we deal with far more complex problems that consist of thousands of subproblems. In this case, the individual subproblems should be solved with almost perfect accuracy, or the overall accuracy drops significantly. We empirically find that such a level of accuracy is hard to achieve by simply prompting a pre-trained LM.</p>
<p>Neural Programmer-Interpreter (NPI). Unlike language models, NPI (Reed and de Freitas, 2016) interacts with its environment through a series of program execution. It consists of an LSTM core, an encoder for each domain, and a memory of program embeddings. At every time step, the LSTM core takes a program embedding, arguments, and an observation of its environment to produce the next program embedding and corresponding arguments. Cai et al. (2017) combine NPI with recursion and show that recursion plays a critical role in generalization. Since NPI requires full execution traces for training, there are multiple works to relax this requirement using reinforcement learning (Li et al., 2017; Fox et al., 2018; Pierrot et al., 2019).</p>
<h1>B RoT Inference Algorithm</h1>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Recursion</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">Thought</span><span class="w"> </span><span class="n">Inference</span>
<span class="nl">Require</span><span class="p">:</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="k">sequence</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">M</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="n">trained</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">Recursion</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">Thought</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">question</span><span class="w"> </span><span class="k">sequence</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">Q</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">function</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">ROT</span><span class="err">}</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">Q</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">X</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">Q</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">i_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">ans</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="o">|</span><span class="n">X</span><span class="o">|+</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">false</span>
<span class="w">        </span><span class="k">while</span><span class="w"> </span><span class="k">True</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">Generate</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="n">token</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">X</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="o">[</span><span class="n">X ; x</span><span class="o">]</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">STOP</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">X_</span><span class="err">{</span><span class="n">i_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">ans</span><span class="w"> </span><span class="err">}}:</span><span class="o">|</span><span class="n">X</span><span class="o">|</span><span class="err">}\</span><span class="p">)</span>
<span class="w">            </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">60</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="n">i_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="k">go</span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="o">|</span><span class="n">X</span><span class="o">|</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">Mark</span><span class="w"> </span><span class="k">last</span><span class="w"> </span><span class="mi">60</span>
<span class="w">            </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">TAIL</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="n">i_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="k">go</span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="o">|</span><span class="n">X</span><span class="o">|</span><span class="err">\</span><span class="p">)</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">true</span>
<span class="w">            </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">THINK</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="n">Q</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">sub</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">X_</span><span class="err">{</span><span class="n">i_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="k">go</span><span class="err">}}:</span><span class="o">|</span><span class="n">X</span><span class="o">|-</span><span class="mi">1</span><span class="err">}\</span><span class="p">)</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="n">A</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">sub</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">ROT</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">Q</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">sub</span><span class="w"> </span><span class="err">}}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">t</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">                    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">A</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">sub</span><span class="w"> </span><span class="err">}}\</span><span class="p">)</span>
<span class="w">                </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="n">X</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="nf">left</span><span class="o">[</span><span class="n">X_{1:|X|-1} ; A^{\text {sub }}\right</span><span class="o">]</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>$\triangleright$ Replace THINK with $A^{\text {sub }}$</p>
<p>$$
i_{\text {ans }} \leftarrow|X|+1
$$</p>
<p>end if
end while
end function</p>
<h2>C Training Batch Distribution</h2>
<p>We use the same problem distribution for both training and evaluation since out-of-distribution generalization is not within the scope of this paper. That is, when teaching 6-digit multiplication to the model, both training and test sets are all examples of 6-digit multiplication. The problem distributions are elaborated in Appendix G. Another important detail regarding the training of RoT is that each training example in a batch is a context, not a whole problem. Since RoT generates multiple contexts per problem, often a large portion of contexts can be a duplicate (mostly the base cases). Therefore, to build a training batch for RoT, we first sample a top-level problem and find the set of unique RoT contexts from the problem. Out of the unique contexts, we randomly sample one context as a training example. We find this simple technique works well, and we do not need a more sophisticated method, such as the adaptive curriculum learning in Reed and de Freitas (2016).</p>
<h1>D Target Sequence</h1>
<p>Algorithm 2 Creating the target sequence
Require: Context $X=\left[Q ; Q^{\mathrm{sub}, 1} ; A^{\mathrm{sub}, 1} ; \ldots ; Q^{\mathrm{sub}, N} ; A^{\mathrm{sub}, N} ; A\right]$
$1: Y \leftarrow \underbrace{\mathrm{PAD} \ldots \mathrm{PAD}}<em _A_mathrm_sub="|A^{\mathrm{sub">{|Q|}$
2: for $n$ in $1 \ldots N$ do
3: $\quad Y \leftarrow\left[Y ; Q^{\mathrm{sub}, n}\right]$
4: $\quad Y \leftarrow\left[Y ; \operatorname{THINK} \underbrace{\mathrm{PAD} \ldots \mathrm{PAD}}</em>\right]$
5: end for
6: $Y \leftarrow[Y ; A]$
7: return $Y$}, n}|-1</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\text {sub,1 }}$</th>
<th style="text-align: center;">$A^{\text {sub,1 }}$</th>
<th style="text-align: center;">$Q^{\text {sub,2 }}$</th>
<th style="text-align: center;">$A^{\text {sub,2 }}$</th>
<th style="text-align: center;">$A$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{1}$</td>
<td style="text-align: center;">40843510008435075</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">755575555555757575757575757575757575757575757575757575757575757575757575757575757575757575757575757575757575</td>
</tr>
</tbody>
</table>
<p>Figure 3: The target sequence $Y^{1}$ for $X^{1}$ in Figure 1.</p>
<h2>E A Step-by-Step Illustration of RoT Inference</h2>
<p>In this section, we provide a step-by-step illustration of the example in Figure 1. Here we assume an ideal model fully trained for RoT.</p>
<h2>Step 1</h2>
<p>The context is initialized with the question $Q$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{1}$</td>
</tr>
</tbody>
</table>
<p>Step 2
The model generates the first subquestion $8+1$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\mathrm{sub}, 1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{1}$</td>
<td style="text-align: center;">408435100841</td>
</tr>
</tbody>
</table>
<p>Step 3
Instead of immediately producing the answer, the model outputs the THINK token.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\mathrm{sub}, 1}$</th>
<th style="text-align: center;">$A^{\mathrm{sub}, 1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{1}$</td>
<td style="text-align: center;">4084351000841</td>
<td style="text-align: center;">THINK</td>
</tr>
</tbody>
</table>
<h1>Step 4</h1>
<p>The THINK token triggers the creation of a new context. The new context is initialized with the subproblem starting from the last $\underline{\underline{0}}$ of $X^{1}$, i.e., $8+1$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\text {sub,1 }}$</th>
<th style="text-align: center;">$A^{\text {sub,1 }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{1}$</td>
<td style="text-align: center;">408 + 351 = 408 + 1 = THINK</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$Q$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$X^{2}$</td>
<td style="text-align: center;">8 + 1 =</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h2>Step 5</h2>
<p>Since the subproblem is a base case, the model outputs the answer 9 immediately.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\text {sub,1 }}$</th>
<th style="text-align: center;">$A^{\text {sub,1 }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{1}$</td>
<td style="text-align: center;">408 + 351 = 408 + 1 = THINK</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$Q$</td>
<td style="text-align: center;">$A$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$X^{2}$</td>
<td style="text-align: center;">8 + 1 = 9 STOP</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h2>Step 6</h2>
<p>The answer is returned and replaces the THINK token.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\text {sub,1 }}$</th>
<th style="text-align: center;">$A^{\text {sub,1 }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{1}$</td>
<td style="text-align: center;">408 + 351 = 408 + 1 = 9 STOP</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h2>Step 7</h2>
<p>The model generates the next subproblem, which is to add the remaining digits. Then, it produces THINK to find its answer.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\text {sub,1 }}$</th>
<th style="text-align: center;">$A^{\text {sub,1 }}$</th>
<th style="text-align: center;">$Q^{\text {sub,2 }}$</th>
<th style="text-align: center;">$A^{\text {sub,2 }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{1}$</td>
<td style="text-align: center;">408 + 351 = 408 + 1 = 9 STOP</td>
<td style="text-align: center;">40 + 35 = THINK</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Step 8
The THINK token creates a new context $X^{3}$ for solving $40+35$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\text {sub,1 }}$</th>
<th style="text-align: center;">$A^{\text {sub,1 }}$</th>
<th style="text-align: center;">$Q^{\text {sub,2 }}$</th>
<th style="text-align: center;">$A^{\text {sub,2 }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{1}$</td>
<td style="text-align: center;">408 + 351 = 408 + 1 = 9 STOP</td>
<td style="text-align: center;">40 + 35 =</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Step 9
Since $40+35$ is not a base case, the model recursively produces more subproblems. In this case, the first subproblem is to add the last digits, i.e., 0 and 5 . Then it outputs the THINK token to solve the subproblem.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\text {sub,1 }}$</th>
<th style="text-align: center;">$A^{\text {sub,1 }}$</th>
<th style="text-align: center;">$Q^{\text {sub,2 }}$</th>
<th style="text-align: center;">$A^{\text {sub,2 }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{1}$</td>
<td style="text-align: center;">408 + 351 = 408 + 1 = 9 STOP</td>
<td style="text-align: center;">40 + 35 =</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Step 10
The new context $X^{4}$ is created to solve $0+5$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\text {sub,1 }}$</th>
<th style="text-align: center;">$A^{\text {sub,1 }}$</th>
<th style="text-align: center;">$Q^{\text {sub,2 }}$</th>
<th style="text-align: center;">$A^{\text {sub,2 }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{1}$</td>
<td style="text-align: center;">408 + 351 = 408 + 1 = 9 STOP</td>
<td style="text-align: center;">40 + 35 =</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\text {sub,1 }}$</th>
<th style="text-align: center;">$A^{\text {sub,1 }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{3}$</td>
<td style="text-align: center;">40 + 35 = 40 + 5 = THINK</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$A$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{4}$</td>
<td style="text-align: center;">0 + 5 = 5 STOP</td>
</tr>
</tbody>
</table>
<p>Step 11
The answer is returned to $X^{3}$ and replaces the TH1NR token.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\text {sub,1 }}$</th>
<th style="text-align: center;">$A^{\text {sub,1 }}$</th>
<th style="text-align: center;">$Q^{\text {sub,2 }}$</th>
<th style="text-align: center;">$A^{\text {sub,2 }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{1}$</td>
<td style="text-align: center;">408 + 351 = 408 + 1 = 9 STOP</td>
<td style="text-align: center;">40 + 35 =</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Step 12
The model generates the next subproblem.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\text {sub,1 }}$</th>
<th style="text-align: center;">$A^{\text {sub,1 }}$</th>
<th style="text-align: center;">$Q^{\text {sub,2 }}$</th>
<th style="text-align: center;">$A^{\text {sub,2 }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{1}$</td>
<td style="text-align: center;">408 + 351 = 408 + 1 = 9 STOP</td>
<td style="text-align: center;">40 + 35 =</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Step 13
$X^{5}$ is created to solve the subproblem $4+3$. Since this is a base case, the model produces the answer directly.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\text {sub,1 }}$</th>
<th style="text-align: center;">$A^{\text {sub,1 }}$</th>
<th style="text-align: center;">$Q^{\text {sub,2 }}$</th>
<th style="text-align: center;">$A^{\text {sub,2 }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{1}$</td>
<td style="text-align: center;">408 + 351 = 408 + 1 = 9 STOP</td>
<td style="text-align: center;">40 + 35 =</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Step 14
$X^{5}$ is $4 \quad 3$ = 7 STOP</p>
<p>Step 14
The answer from $X^{5}$ replaces the THINK token in $X^{3}$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\text {sub,1 }}$</th>
<th style="text-align: center;">$A^{\text {sub,1 }}$</th>
<th style="text-align: center;">$Q^{\text {sub,2 }}$</th>
<th style="text-align: center;">$A^{\text {sub,2 }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{1}$</td>
<td style="text-align: center;">408 + 351 508 + 1 9 STOP</td>
<td style="text-align: center;">40 + 35</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">THINK</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\text {sub,1 }}$</th>
<th style="text-align: center;">$A^{\text {sub,1 }}$</th>
<th style="text-align: center;">$Q^{\text {sub,2 }}$</th>
<th style="text-align: center;">$A^{\text {sub,2 }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{3}$</td>
<td style="text-align: center;">40 + 35</td>
<td style="text-align: center;">40 + 5</td>
<td style="text-align: center;">5 STOP</td>
<td style="text-align: center;">4 + 3 7 STOP</td>
</tr>
</tbody>
</table>
<p>Step 15
Since all subproblems are solved in $X^{3}$, the answer 75 is generated and returned to $X^{1}$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\text {sub,1 }}$</th>
<th style="text-align: center;">$A^{\text {sub,1 }}$</th>
<th style="text-align: center;">$Q^{\text {sub,2 }}$</th>
<th style="text-align: center;">$A^{\text {sub,2 }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{1}$</td>
<td style="text-align: center;">408 + 351 508 + 1 9 STOP</td>
<td style="text-align: center;">40 + 35</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">THINK</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\text {sub,1 }}$</th>
<th style="text-align: center;">$A^{\text {sub,1 }}$</th>
<th style="text-align: center;">$Q^{\text {sub,2 }}$</th>
<th style="text-align: center;">$A^{\text {sub,2 }}$</th>
<th style="text-align: center;">$A$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{3}$</td>
<td style="text-align: center;">40 + 35</td>
<td style="text-align: center;">40 + 5</td>
<td style="text-align: center;">5 STOP</td>
<td style="text-align: center;">4 + 3 7 STOP</td>
<td style="text-align: center;">7 5 STOP</td>
</tr>
</tbody>
</table>
<p>Step 16
The answer of $X^{3}$ replaces the THINK token in $X^{1}$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\text {sub,1 }}$</th>
<th style="text-align: center;">$A^{\text {sub,1 }}$</th>
<th style="text-align: center;">$Q^{\text {sub,2 }}$</th>
<th style="text-align: center;">$A^{\text {sub,2 }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{1}$</td>
<td style="text-align: center;">408 + 351 508 + 1 9 STOP</td>
<td style="text-align: center;">40 + 35</td>
<td style="text-align: center;">7 5 STOP</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Step 17
Since the subproblems in $X^{1}$ are all solved, the model produces the final answer.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$Q$</th>
<th style="text-align: center;">$Q^{\text {sub,1 }}$</th>
<th style="text-align: center;">$A^{\text {sub,1 }}$</th>
<th style="text-align: center;">$Q^{\text {sub,2 }}$</th>
<th style="text-align: center;">$A^{\text {sub,2 }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X^{1}$</td>
<td style="text-align: center;">408 + 351 508 + 1 9 STOP</td>
<td style="text-align: center;">40 + 35</td>
<td style="text-align: center;">7 5 STOP</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h1>F Examples of CoT Training Data</h1>
<p>If we solve the example of 408+351 in Figure 1 with RoT, the following five contexts are produced.</p>
<ul>
<li>$X^{1}: 60408+351=608+1=9$ STOP $6040+35=75$ STOP 759 STOP</li>
<li>$X^{2}: 608+1=9$ STOP</li>
<li>
<p>$X^{3}: 6040+35=600+5=5$ STOP $604+3=7$ STOP 75 STOP</p>
</li>
<li>
<p>$X^{4}: G 0 \quad 0+5=5$ STOP</p>
</li>
<li>$X^{5}: G 04+3=7$ STOP</li>
</ul>
<p>The CoT context of the same problem is:</p>
<ul>
<li>$X^{\text {CoT }}: G 04 \quad 0 \quad 8+351=G 08+1=9$ STOP $G 04 \quad 0+35=G 00+5$ STOP $G 04+$ 3 STOP 75 STOP 759 STOP</li>
</ul>
<p>In a slightly more complicated example of $34 \times 5$, the RoT contexts are as follows:</p>
<ul>
<li>$X^{1}: G 034 \times 5=G 04 \times 5=20$ STOP $G 03 \times 5=15$ STOP TAIL $150+20=$ THINK</li>
<li>$X^{2}: G 04 \times 5=20$ STOP</li>
<li>$X^{3}: G 03 \times 5=15$ STOP</li>
<li>$X^{4}: G 0150+20=G 00+0=0$ STOP $G 015+2=17$ STOP 170 STOP</li>
<li>$X^{5}: G 0 \quad 0+0=0$ STOP</li>
<li>$X^{6}: G 015+2=G 05+2=7$ STOP 17 STOP</li>
<li>$X^{7}: G 05+2=7$ STOP</li>
</ul>
<p>The corresponding CoT context is:</p>
<ul>
<li>$X^{\text {CoT }}: G 034 \times 5=G 04 \times 5=20$ STOP $G 03 \times 5=15$ STOP TAIL $150+20=G 0$ $0+0=0$ STOP $G 015+2=G 05+2=7$ STOP 17 STOP 170 STOP</li>
</ul>
<p>Notice that the CoT context consists of all the corresponding RoT contexts as its subsequences. The number of tokens to generate is identical to that of RoT if we do not count the THINK tokens. Even in these simple examples, however, the context size of CoT is far longer than that of RoT. For much more complex problems, such as 8-digit multiplication or 0-1 Knapsack, the CoT context size can be orders of magnitude larger than RoT. See Appendix M for more details on the distribution of context sizes.</p>
<h1>G Problem Specifications</h1>
<h2>G. 1 The Arithmetic Problems</h2>
<p>For arithmetic tasks, we test addition, subtraction, multiplication, and division on non-negative integers. For subtraction, we add a constraint that the first operand is not less than the second one, to enforce non-negative answers. For division, we let the output include both a quotient and a remainder, separated by a special token R, e.g., G0 $7+3=2$ R 1 STOP.</p>
<p>As briefly mentioned in $\S 4.2$, naively sampling the operands from a uniform distribution makes the operands extremely biased towards large numbers. For example, the probability of sampling a 2-digit number from the 6-digit space is less than $0.01 \%$. Thus, we define a variation of the log-uniform distribution (often called the reciprocal distribution) to sample the operands. As a result, we obtain roughly the same proportion of operands for each number of digits.</p>
<p>The probability density of a log-uniform distribution is proportional to the reciprocal of the value. By definition, zero is not in the support of a log-uniform distribution, and samples are overly concentrated to the first few values in the sampling range. Therefore, we slightly extend the log-uniform distribution by introducing an offset parameter $\delta$. To sample an integer in range $[\alpha, \beta)$ with offset $\delta$, we first uniformly sample a real number $r$ in range $[\log (\alpha+\delta), \log (\beta+\delta)]$. Then, $r$ is transformed to $[\exp (r)-\delta]$. We denote the extended log-uniform distribution $U_{\log }(\alpha, \beta, \delta)$. As $\delta$ gets larger, the samples are more dispersed to larger numbers. In the experiments, we set $\delta=3$.</p>
<p>Additionally, we introduce several other sampling details for division problems. Assume that we independently sample two numbers $a$ and $b$ for the dividend and the divisor. In about half of the cases, the</p>
<table>
<thead>
<tr>
<th>Addition</th>
<th>Subtraction</th>
<th>Multiplication</th>
<th>Division</th>
</tr>
</thead>
<tbody>
<tr>
<td>$1330+121163$</td>
<td>$376776-35241$</td>
<td>$9466 \times 176175$</td>
<td>$620261 \div 155034$</td>
</tr>
<tr>
<td>$114780+4356$</td>
<td>$10638-100$</td>
<td>$179 \times 516$</td>
<td>$111730 \div 1176$</td>
</tr>
<tr>
<td>$638+2$</td>
<td>$109033-52649$</td>
<td>$5509 \times 133$</td>
<td>$28268 \div 1$</td>
</tr>
<tr>
<td>$35+77$</td>
<td>$85137-3098$</td>
<td>$6783 \times 2$</td>
<td>$588137 \div 25571$</td>
</tr>
<tr>
<td>$114261+354$</td>
<td>$22355-2824$</td>
<td>$6 \times 80285$</td>
<td>$180330 \div 739$</td>
</tr>
<tr>
<td>$3+13792$</td>
<td>$7-1$</td>
<td>$37275 \times 19258$</td>
<td>$879975 \div 97772$</td>
</tr>
<tr>
<td>$10151+7$</td>
<td>$652781-78853$</td>
<td>$168484 \times 154$</td>
<td>$111461 \div 905026$</td>
</tr>
<tr>
<td>$22+1399$</td>
<td>$64914-3114$</td>
<td>$3331 \times 40$</td>
<td>$42338 \div 14003$</td>
</tr>
<tr>
<td>$363356+450475$</td>
<td>$13041-1422$</td>
<td>$349 \times 158$</td>
<td>$108 \div 384103$</td>
</tr>
<tr>
<td>$73+11$</td>
<td>$28293-4540$</td>
<td>$17988 \times 262130$</td>
<td>$60002 \div 7479$</td>
</tr>
<tr>
<td>$179895+4128$</td>
<td>$11553-3576$</td>
<td>$8140 \times 1670$</td>
<td>$131467 \div 131290$</td>
</tr>
<tr>
<td>$3+10$</td>
<td>$656291-2795$</td>
<td>$51 \times 5$</td>
<td>$890679 \div 62$</td>
</tr>
<tr>
<td>$1+141972$</td>
<td>$93-42$</td>
<td>$16497 \times 158$</td>
<td>$228 \div 131108$</td>
</tr>
<tr>
<td>$57612+18403$</td>
<td>$55972-1782$</td>
<td>$74 \times 10$</td>
<td>$892 \div 124$</td>
</tr>
<tr>
<td>$9+1621$</td>
<td>$84587-51$</td>
<td>$216 \times 13414$</td>
<td>$15 \div 964156$</td>
</tr>
<tr>
<td>$3370+381$</td>
<td>$273269-5867$</td>
<td>$621 \times 2$</td>
<td>$369044 \div 28364$</td>
</tr>
<tr>
<td>$678+8854$</td>
<td>$274405-14$</td>
<td>$2 \times 5951$</td>
<td>$457 \div 46$</td>
</tr>
<tr>
<td>$422+10348$</td>
<td>$51926-9$</td>
<td>$189486 \times 13080$</td>
<td>$14687 \div 730$</td>
</tr>
<tr>
<td>$118+582$</td>
<td>$4272-229$</td>
<td>$552792 \times 763$</td>
<td>$200361 \div 1049$</td>
</tr>
<tr>
<td>$1343+408534$</td>
<td>$223267-377$</td>
<td>$77 \times 3$</td>
<td>$19715 \div 965179$</td>
</tr>
<tr>
<td>$24+9251$</td>
<td>$14857-1994$</td>
<td>$179090 \times 469029$</td>
<td>$98 \div 7$</td>
</tr>
<tr>
<td>$315+652424$</td>
<td>$914771-836$</td>
<td>$1037 \times 258$</td>
<td>$406 \div 9$</td>
</tr>
<tr>
<td>$355+4434$</td>
<td>$3035-2963$</td>
<td>$8 \times 769974$</td>
<td>$47345 \div 122$</td>
</tr>
<tr>
<td>$22+834928$</td>
<td>$30-12$</td>
<td>$47765 \times 7254$</td>
<td>$391613 \div 1631$</td>
</tr>
<tr>
<td>$3028+357$</td>
<td>$149-4$</td>
<td>$5608 \times 18164$</td>
<td>$892642 \div 3898$</td>
</tr>
<tr>
<td>$777+1355$</td>
<td>$89057-6$</td>
<td>$21437 \times 12$</td>
<td>$241554 \div 1901$</td>
</tr>
<tr>
<td>$154874+81059$</td>
<td>$296410-9$</td>
<td>$15007 \times 15$</td>
<td>$116475 \div 12908$</td>
</tr>
<tr>
<td>$64936+216852$</td>
<td>$45-3$</td>
<td>$539860 \times 427$</td>
<td>$488317 \div 197443$</td>
</tr>
<tr>
<td>$3+340939$</td>
<td>$78906-3$</td>
<td>$3583 \times 9754$</td>
<td>$7519 \div 325$</td>
</tr>
<tr>
<td>$3+984775$</td>
<td>$56560-29960$</td>
<td>$13 \times 66$</td>
<td>$3560 \div 847611$</td>
</tr>
<tr>
<td>$50581+1183$</td>
<td>$98-6$</td>
<td>$266394 \times 185$</td>
<td>$9711 \div 1385$</td>
</tr>
<tr>
<td>$415+943$</td>
<td>$16551-920$</td>
<td>$3988 \times 12$</td>
<td>$44540 \div 103$</td>
</tr>
<tr>
<td>$110+49$</td>
<td>$25606-194$</td>
<td>$5514 \times 57$</td>
<td>$19721 \div 58$</td>
</tr>
<tr>
<td>$15+17058$</td>
<td>$45-37$</td>
<td>$5 \times 1712$</td>
<td>$59544 \div 24$</td>
</tr>
<tr>
<td>$36278+100$</td>
<td>$129443-70196$</td>
<td>$17 \times 430178$</td>
<td>$333057 \div 333057$</td>
</tr>
<tr>
<td>$6+23516$</td>
<td>$221-54$</td>
<td>$227 \times 127$</td>
<td>$25719 \div 5142$</td>
</tr>
<tr>
<td>$1462+848$</td>
<td>$11010-818$</td>
<td>$20888 \times 54$</td>
<td>$7544 \div 46$</td>
</tr>
<tr>
<td>$1002+2773$</td>
<td>$47759-67$</td>
<td>$96 \times 232801$</td>
<td>$45 \div 410$</td>
</tr>
<tr>
<td>$135+178346$</td>
<td>$10-8$</td>
<td>$175 \times 1050$</td>
<td>$195659 \div 2047$</td>
</tr>
<tr>
<td>$22672+162038$</td>
<td>$1439-153$</td>
<td>$146 \times 166$</td>
<td>$412572 \div 16$</td>
</tr>
</tbody>
</table>
<p>Table 1: 40 randomly selected samples of each type of 6-digit arithmetic problems.
dividend $a$ would be less than the divisor $b$, so the quotients will be zero for those cases. To ensure a diverse range of quotients, we sample the divisor $b$ from $U_{\log }\left(1,10^{N}, 3\right)$, the quotient $c$ from $U_{\log }\left(0,10^{N} / b, 3\right)$, and the remainder $r$ from $U_{\log }(0, b, 3)$. The dividend is calculated from these values: $a=b \times c+r$. This way, we can sample division problems with a diverse range of quotients and remainders.</p>
<p>Table 1 presents 40 problem samples for each 6-digit problem type. Several properties of our sampling scheme can be observed in the table. First, each number ranges over diverse numbers of digits. Second, the division problems are mostly non-trivial, i.e., the quotients are not concentrated at zero.</p>
<h1>G. 2 The Algorithmic Problems</h1>
<h2>G.2.1 Longest Common Subsequence (LCS)</h2>
<p>The question of an LCS problem is two number sequences joined by the LCS token, and the answer is the corresponding LCS and its length separated by $\frac{r}{b}$. Here is an example of a length-4 LCS problem:</p>
<ul>
<li>$Q: G 01234$ LCS $2468 \times$</li>
<li>$A: 24 \frac{1}{4} 2$ STOP</li>
</ul>
<p>For a length- $N$ LCS problem, we sample two sequences of length $N$. Each character of the sequences is randomly sampled from $0-9$ with equal probability.</p>
<h1>G.2.2 Longest Palindromic Subsequence (LPS)</h1>
<p>The question of a length- $N$ LPS problem starts with the LPS, followed by a sequence of length $N$. Similar to LCS, the answer contains the corresponding LPS and its length separated by $\mp$. The following is an example of a length-8 LPS problem:</p>
<ul>
<li>Q: GO LPS 41253261</li>
<li>$A: 12321 \mp 5$ STOP</li>
</ul>
<p>The sequence of an LPS problem is sampled in the same way as done for the LCS problem.</p>
<h2>G.2.3 0-1 Knapsack</h2>
<p>Each item in a 0-1 Knapsack problem is represented by its value and weight. For instance, 12 \&amp; 34 represents an item with a value of 12 and a weight of 34 . The question part of a $0-1$ Knapsack problem is a sequence consisting of the KNAPSACK token, a list of items separated by $\mp$, the token @, and the capacity of the knapsack. The answer part starts with a list of items to include, then $\$$ and finally the total value. The following is an example of a 3-item knapsack problem.</p>
<ul>
<li>Q: GO KNAPSACK 5 \&amp; 12,25 \&amp; 15,19 \&amp; 18 @ 40</li>
<li>$A: 25 \&amp; 15,19 \&amp; 18 \$ 44$ STOP</li>
</ul>
<p>In this example, given a knapsack of capacity 40, the last two are selected with a total value of 44.
For a fixed number of items, we uniformly sample each item's value and weight from the integers of range $[1,99]$.</p>
<h2>G.2.4 Matrix Chain Multiplication (MCM)</h2>
<p>The cost of multiplying many matrices is very sensitive to the order of multiplication. Matrix chain multiplication is the task of finding the best order with the minimum cost. Here, the cost is defined to be the total number of element multiplications. In the example of three matrices $A, B$, and $C$, whose shapes are $4 \times 2,2 \times 8$, and $8 \times 3$ respectively, the cost of computing $(A B) C$ is $4 \times 2 \times 8+4 \times 8 \times 3=160$, while another order $A(B C)$ costs only $2 \times 8 \times 3+4 \times 2 \times 3=72$. In the question of an MCM problem, the sizes of the matrices are enumerated, and the answer contains the order and the total cost separated by $\mp$. The example above is represented as the following sequences.</p>
<ul>
<li>Q: GO MCM $4 \times 2,2 \times 8,8 \times 3$</li>
<li>$A: 4 \times 2,(2 \times 8,8 \times 3) \mp 72$ STOP</li>
</ul>
<p>Given a fixed number of matrices, we sample the sizes of matrices from the range [1, 99].</p>
<h2>G.2.5 Sorting</h2>
<p>Although not included in the main text, we test the problem of sorting multi-digit numbers. The results are presented in Appendix N. The problem difficulty is defined by the maximum number of terms. For a sorting problem of at most $N$ terms, we first uniformly sample the number of terms from $[2, N]$. Then we sample each term from $U_{\log }(0,1000,5)$. The following is an example of the sorting problem.</p>
<ul>
<li>Q: GO SORT 139,160,434,796,41</li>
<li>$A: 41,139,160,434,796$ STOP</li>
</ul>
<h1>H Details of the Recursive Reasoning Procedures</h1>
<p>In this section, we elaborate on the procedures to recursively solve the arithmetic problems. Specifically, we present the algorithms to produce the subproblems of a problem. Therefore, for a set of randomly sampled questions, we can generate ground truth contexts using these algorithms. For better understanding, we present the key parts of our Python code, the thought methods. For each problem, we create a child class the Problem class and implement thought static method. The method takes a set of arguments for a problem and returns the list of direct subproblems. Each subproblem is represented by a problem class, problem arguments, and recursion type (whether it is a tail recursion or not). We use named tuple T to group this information:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">namedtuple</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Thought&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;prob_cls&#39;</span><span class="p">,</span> <span class="s1">&#39;args&#39;</span><span class="p">,</span> <span class="s1">&#39;type&#39;</span><span class="p">],</span> <span class="n">defaults</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;&#39;</span><span class="p">])</span>
</code></pre></div>

<p>For instance, T(Mul, $(3,4))$ represents a regular subproblem of $3 \times 4$, and T(Add, (12, 340), 'tail') represents a subproblem of $12+340$ which should be performed as a tail recursion. Once the thought method returns a list of Ts, we can recursively find more subproblems for each subproblem.</p>
<h2>H. 1 Addition</h2>
<p>The core idea of our recursive procedure for addition is to first add the last digits and then add the rest. If the sum of the last digits is greater than or equal to 10 , we insert another subproblem for adding the carry right after adding the last digits.</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="k">Add</span><span class="p">(</span><span class="n">Problem</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="nv">@staticmethod</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">thought</span><span class="p">(</span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">list</span><span class="o">[</span><span class="n">T</span><span class="o">]</span><span class="err">:</span>
<span class="w">        </span><span class="nf">left</span><span class="p">,</span><span class="w"> </span><span class="nf">right</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">args</span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">Base</span><span class="w"> </span><span class="n">cases</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nf">left</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="nf">right</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">10</span><span class="err">:</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="err">[]</span>
<span class="w">        </span><span class="mi">1</span><span class="n">_last</span><span class="p">,</span><span class="w"> </span><span class="n">r_last</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">left</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="nf">right</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">10</span>
<span class="w">        </span><span class="n">thoughts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">T(Add, (1_last, r_last))</span><span class="o">]</span>
<span class="w">        </span><span class="mi">1</span><span class="n">_rest</span><span class="p">,</span><span class="w"> </span><span class="n">r_rest</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">left</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="nf">right</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="mi">10</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="mi">1</span><span class="n">_last</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">r_last</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">10</span><span class="err">:</span>
<span class="w">            </span><span class="n">thoughts</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">T</span><span class="p">(</span><span class="k">Add</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="n">_rest</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)))</span>
<span class="w">            </span><span class="mi">1</span><span class="n">_rest</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="mi">1</span><span class="n">_rest</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">r_rest</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="err">:</span>
<span class="w">            </span><span class="n">thoughts</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">T</span><span class="p">(</span><span class="k">Add</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="n">_rest</span><span class="p">,</span><span class="w"> </span><span class="n">r_rest</span><span class="p">)))</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">thoughts</span>
</code></pre></div>

<p>Figure 1 in the main draft is an example with no carry, and the following is another example of 27+65 with a carry.</p>
<ul>
<li>$X^{1}: 00317+65=007+5=12$ STOP 0031+1=32 STOP 0032+6=38 STOP 382 STOP</li>
<li>$X^{2}: 007+5=12$ STOP</li>
<li>$X^{3}: 0031+1=001+1=2$ STOP 32 STOP</li>
<li>$X^{4}: 001+1=2$ STOP</li>
<li>$X^{5}: 0032+6=002+6=8$ STOP 38 STOP</li>
<li>$X^{6}: 002+6=8$ STOP</li>
</ul>
<h1>H. 2 Subtraction</h1>
<p>Similar to addition, we first subtract the last digits and solve the rest recursively. When subtracting the last digits $x$ and $y$, we always borrow 10 for $x$ to prevent a negative result. The borrowing of 10 is easy for a sequence model: just put 1 before $x$. Therefore, the base cases of subtraction are when $a \leq 19$ and $b \leq 9$. If the subtraction result of the last digits is smaller than 10 , i.e., the borrow is actually needed, we subtract 1 from the rest of the first operand $m$.</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="n">Sub</span><span class="p">(</span><span class="n">Problem</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="nv">@staticmethod</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">thought</span><span class="p">(</span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">list</span><span class="o">[</span><span class="n">T</span><span class="o">]</span><span class="err">:</span>
<span class="w">        </span><span class="nf">left</span><span class="p">,</span><span class="w"> </span><span class="nf">right</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">args</span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">Base</span><span class="w"> </span><span class="n">cases</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nf">left</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="mi">19</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="nf">right</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="mi">9</span><span class="err">:</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="err">[]</span>
<span class="w">        </span><span class="mi">1</span><span class="n">_last</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">left</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">10</span>
<span class="w">        </span><span class="n">r_last</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">right</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">10</span>
<span class="w">        </span><span class="n">thoughts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">T(Sub, (1_last, r_last))</span><span class="o">]</span>
<span class="w">        </span><span class="mi">1</span><span class="n">_rest</span><span class="p">,</span><span class="w"> </span><span class="n">r_rest</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">left</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="nf">right</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="mi">10</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="mi">1</span><span class="n">_last</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">r_last</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">10</span><span class="err">:</span>
<span class="w">            </span><span class="n">thoughts</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">T</span><span class="p">(</span><span class="n">Sub</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="n">_rest</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)))</span>
<span class="w">            </span><span class="mi">1</span><span class="n">_rest</span><span class="w"> </span><span class="o">-=</span><span class="w"> </span><span class="mi">1</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">r_rest</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="err">:</span>
<span class="w">            </span><span class="n">thoughts</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">T</span><span class="p">(</span><span class="n">Sub</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="n">_rest</span><span class="p">,</span><span class="w"> </span><span class="n">r_rest</span><span class="p">)))</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">thoughts</span>
</code></pre></div>

<p>Here is an example of 432-216:</p>
<ul>
<li>$X^{1}: 00432=216=0012=6=6$ STOP $0043=1=42$ STOP $0042=21=$ 21 STOP 216 STOP</li>
<li>$X^{2}: 0012=6=6$ STOP</li>
<li>$X^{3}: 0043=1=0013=1=12$ STOP 42 STOP</li>
<li>$X^{4}: 0013=1=12$ STOP</li>
<li>$X^{5}: 0042=21=0012=1=11$ STOP $004=2=2$ STOP 21 STOP</li>
<li>$X^{6}: 0012=1=11$ STOP</li>
<li>$X^{7}: 004=2=2$ STOP</li>
</ul>
<p>Notice that the final answer and the questions of each subproblem can be easily constructed from the previous sequence.</p>
<h2>H. 3 Multiplication</h2>
<p>The base cases of multiplication are (i) when either operand is 0 or 1 , or (ii) when both operands are less than 10. If one of the operands is 0 , then the answer is zero; when one of them is 1 , then the answer is just a copy of the other operand. For the cases where both operands are less than 10, we just let the model memorize them, which is similar to an elementary school math curriculum.</p>
<p>There are two types of non-base cases. For the simpler case, where the second operand is less than 10 , we first split the first operand into the last digit and the rest. We then multiply each of them with the second operand and combine the results. Otherwise, we split the second operand into the last digit and the rest. The first operand is multiplied to each of them, and the results are summed.</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="n">Mul</span><span class="p">(</span><span class="n">Problem</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="nv">@staticmethod</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">thought</span><span class="p">(</span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">list</span><span class="o">[</span><span class="n">T</span><span class="o">]</span><span class="err">:</span>
<span class="w">        </span><span class="nf">left</span><span class="p">,</span><span class="w"> </span><span class="nf">right</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">args</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>    6 # Base cases
    if left &lt;= 1 or right &lt;= 1:
        return []
    if left &lt;= 9 and right &lt;= 9:
        return []
    thoughts = []
    if right &lt; 10:
        thoughts.append(T(Mul, (left % 10, right)))
        thoughts.append(T(Mul, (left // 10, right)))
        a1 = (left % 10) <span class="gs">* right</span>
<span class="gs">        a2 = (left // 10) *</span> right
        thoughts.append(T(Add, (a2 <span class="gs">* 10, a1), &#39;tail&#39;))</span>
<span class="gs">    else:</span>
<span class="gs">        a1 = left *</span> (right % 10)
        thoughts.append(T(Mul, (left, right % 10)))
        a2 = left <span class="gs">* (right // 10)</span>
<span class="gs">        thoughts.append(T(Mul, (left, right // 10)))</span>
<span class="gs">        thoughts.append(T(Add, (a2 *</span> 10, a1), &#39;tail&#39;))
    return thoughts
</code></pre></div>

<p>Here are some example contexts of multiplication:</p>
<ul>
<li>$X^{1}: 0043 * 21=0043 * 1=43$ STOP $0043 * 2=86$ STOP TAIL $860+43$ $=$ THINK</li>
<li>$X^{2}: 0043 * 1=43$ STOP</li>
<li>$X^{3}: 0043 * 2=003 * 2=6$ STOP $004 * 2=8$ STOP TAIL $80+6=$ THINK</li>
<li>$X^{4}: 003 * 2=6$ STOP</li>
<li>$X^{5}: 004 * 2=8$ STOP</li>
<li>$X^{6}: 0080+6=000+6=6$ STOP 86 STOP</li>
<li>$X^{7}: 000+6=6$ STOP</li>
<li>$X^{8}: 00860+43=000+3=3$ STOP $0086+4=90$ STOP 903 STOP</li>
<li>$X^{9}: 000+3=3$ STOP</li>
<li>$X^{10}: 0086+4=006+4=10$ STOP $008+1=9$ STOP 90 STOP</li>
<li>$X^{11}: 006+4=10$ STOP</li>
<li>$X^{12}: 008+1=9$ STOP</li>
</ul>
<p>Notice that we use tail recursion in $X^{1}$ and $X^{3}$.</p>
<h1>H. 4 Comparison</h1>
<p>Comparison is used as a subroutine during division. The procedure for comparison consists of three steps:</p>
<ol>
<li>Compare the numbers of digits.</li>
<li>If the numbers of digits are the same, compare the most significant digits.</li>
<li>If the most significant digits are identical, compare the remaining digits recursively.</li>
</ol>
<p>We find that the sequence models can perform the first step without an explicit subproblem. Therefore, we only add intermediate steps for the second and third steps.</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="n">Compare</span><span class="p">(</span><span class="n">Problem</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="nv">@staticmethod</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">thought</span><span class="p">(</span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">list</span><span class="o">[</span><span class="n">T</span><span class="o">]</span><span class="err">:</span>
<span class="w">        </span><span class="nf">left</span><span class="p">,</span><span class="w"> </span><span class="nf">right</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">args</span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">Base</span><span class="w"> </span><span class="n">cases</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nf">left</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="nf">right</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">10</span><span class="err">:</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="err">[]</span>
<span class="w">        </span><span class="n">thoughts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span>
<span class="w">        </span><span class="n">digit_l</span><span class="p">,</span><span class="w"> </span><span class="n">digit_r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="nf">left</span><span class="p">)),</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="nf">right</span><span class="p">))</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">digit_l</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nl">digit_r</span><span class="p">:</span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">Compare</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">first</span><span class="w"> </span><span class="n">digits</span>
<span class="w">            </span><span class="n">l_first</span><span class="p">,</span><span class="w"> </span><span class="n">r_first</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">int</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="nf">left</span><span class="p">)</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="p">),</span><span class="w"> </span><span class="nc">int</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="nf">right</span><span class="p">)</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="p">)</span>
<span class="w">            </span><span class="n">thoughts</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">T</span><span class="p">(</span><span class="n">Compare</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="n">l_first</span><span class="p">,</span><span class="w"> </span><span class="n">r_first</span><span class="p">)))</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">l_first</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nl">r_first</span><span class="p">:</span>
<span class="w">                </span><span class="err">#</span><span class="w"> </span><span class="n">Compare</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">rest</span>
<span class="w">                </span><span class="n">l_rest</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">int</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="nf">left</span><span class="p">)</span><span class="o">[</span><span class="n">1:</span><span class="o">]</span><span class="p">)</span>
<span class="w">                </span><span class="n">r_rest</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">int</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="nf">right</span><span class="p">)</span><span class="o">[</span><span class="n">1:</span><span class="o">]</span><span class="p">)</span>
<span class="w">                </span><span class="n">thoughts</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">T</span><span class="p">(</span><span class="n">Compare</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="n">l_rest</span><span class="p">,</span><span class="w"> </span><span class="n">r_rest</span><span class="p">)))</span>
</code></pre></div>

<p>The following is an example of comparing 153 and 159.</p>
<ul>
<li>$X^{1}$ : 50153 VS 159 = 501 VS $1=$ EQ STOP 5053 VS 59 = LT STOP LT STOP</li>
<li>$X^{2}$ : 501 VS $1=$ EQ STOP</li>
<li>$X^{3}$ : 5053 VS 59 = 505 VS 5 = EQ STOP 503 VS 9 = LT STOP LT STOP</li>
<li>$X^{4}$ : 505 VS 5 = EQ STOP</li>
<li>$X^{5}$ : 503 VS 9 = LT STOP</li>
</ul>
<h1>H. 5 Division</h1>
<p>Solving division is the most challenging among the four basic arithmetic operations since the procedure is basically trial and error, searching for the correct quotient. Nonetheless, the following process is a recursive version of the elementary school division.</p>
<p>The base case is when the dividend is less than or equal to the divisor. If the dividend is smaller than the divisor, the quotient is 0 , and the remainder is the dividend. If the dividend is equal to the divisor, then the quotient is 1 , and the remainder is 0 . Both cases can be handled relatively easily by neural sequence models. To determine whether it is one of these cases, we always perform the comparison as the first subproblem.</p>
<p>If it is not a base case, we check whether the dividend is smaller than 10 times the divisor. If the dividend is smaller, we subtract the divisor from the dividend and recursively divide the result with the divisor. The final answer is attained by simply adding 1 to the quotient of the smaller division.</p>
<p>To explain the other case, where the dividend is greater than 10 times the divisor, let us call the dividend $a$ and the divisor $b$. First, we split the $a$ into the last digit $x$ and the remaining digits $m$. Then, we divide $m$ with the divisor $b$, i.e., we are solving the one-digit-smaller subproblem first. Since we define the division operation to return both a quotient and a remainder, the quotient $q_{1}=m / b$ and the remainder $r_{1}=m$ $\bmod b$ from the subproblem are added to the context. Next, we concatenate the remainder and $x$, which is numerically computing $r \times 10+x$, and divide it again with $b$. Let the quotient and the remainder of this operation $q_{2}$ and $r_{2}$. Then, the quotient of the final answer is $q_{1} \times 10+q_{2}$, while the remainder is simply $r_{2}$.</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="n">Div</span><span class="p">(</span><span class="n">Problem</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="nv">@staticmethod</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">thought</span><span class="p">(</span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">list</span><span class="o">[</span><span class="n">T</span><span class="o">]</span><span class="err">:</span>
<span class="w">    </span><span class="nf">left</span><span class="p">,</span><span class="w"> </span><span class="nf">right</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">args</span>
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/soochan-lee/RoT&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
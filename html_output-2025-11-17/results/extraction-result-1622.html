<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1622 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1622</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1622</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-37c8f0f4915b68f94669d7eeb51b4785b35c70de</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/37c8f0f4915b68f94669d7eeb51b4785b35c70de" target="_blank">Dyna-bAbI: unlocking bAbI’s potential with dynamic synthetic benchmarking</a></p>
                <p><strong>Paper Venue:</strong> STARSEM</p>
                <p><strong>Paper TL;DR:</strong> Dyna-bAbI is developed, a dynamic framework providing fine-grained control over task generation in bAbI, underscoring the importance of highly controllable task generators for creating robust NLU systems through a virtuous cycle of model and data development.</p>
                <p><strong>Paper Abstract:</strong> While neural language models often perform surprisingly well on natural language understanding (NLU) tasks, their strengths and limitations remain poorly understood. Controlled synthetic tasks are thus an increasingly important resource for diagnosing model behavior. In this work we focus on story understanding, a core competency for NLU systems. However, the main synthetic resource for story understanding, the bAbI benchmark, lacks such a systematic mechanism for controllable task generation. We develop Dyna-bAbI, a dynamic framework providing fine-grained control over task generation in bAbI. We demonstrate our ideas by constructing three new tasks requiring compositional generalization, an important evaluation setting absent from the original benchmark. We tested both special-purpose models developed for bAbI as well as state-of-the-art pre-trained methods, and found that while both approaches solve the original tasks (99{% accuracy), neither approach succeeded in the compositional generalization setting, indicating the limitations of the original training data.We explored ways to augment the original data, and found that though diversifying training data was far more useful than simply increasing dataset size, it was still insufficient for driving robust compositional generalization (with 70{% accuracy for complex compositions). Our results underscore the importance of highly controllable task generators for creating robust NLU systems through a virtuous cycle of model and data development.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1622.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1622.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dyna-bAbI compositional-holdout</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dyna-bAbI compositional generalization (held-out composition) split</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum-like dataset design introduced in this paper where training splits are composed of M sub-tasks each exposing only subsets of concepts, and the test split contains novel combinations (the union) of those concepts, i.e., compositional hold-outs that force generalization to unseen concept combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>T5-base (representative pre-trained agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A pre-trained text-to-text transformer (T5-base, 220M parameters) fine-tuned for extractive QA on bAbI-style stories; experiments also include other agents (RoBERTa, EntNet, STM, BiDAF) for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td>220M</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Dyna-bAbI / bAbI micro-world</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A synthetic story-understanding benchmark (bAbI) with a programmatic micro-world simulator that generates short narratives (events like MOVE, GRAB, DROP, GIVE) and questions; Dyna-bAbI is a Python-based generator providing fine-grained control over events, linguistic mappings (co-reference, negation, indefinites, conjunction), question types, and support composition.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures (story-level entity and object tracking; event comprehension)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Tracking agents/objects across locations, reasoning about possession (GRAB/DROP/GIVE), yes-no/location/count/list questions about story entities.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Tasks are compositional in that test instances require novel combinations of primitive concepts (events, linguistic constructs, question types); composition is characterized by support composition f_c (which events/linguistic constructs support the answer) and number of supporting facts |f| (n), with complexity increasing as n increases or when support compositions are unseen at training.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>compositional hold-out curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Training splits are constructed from M subtasks each exposing disjoint subsets of the final test concepts (C_train'_i), so models see all atomic concepts during training but never the full combinations present in the test split C_test = union_i C_train'_i. This isolates compositional generalization: models must compose known primitives into novel configurations at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>compositional complexity / held-out combinations (not explicit temporal ordering)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>From single-support questions (n=1) up to multi-support compositional questions (n >= 4); experiments analyze n = 1, 2, 3, >=4</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Pre-trained models (e.g., T5) obtain near-perfect performance on IID/original bAbI (>99%) but on compositional hold-out (mix splits) performance drops: pre-trained models show ~20–50% absolute drop vs IID, non-pretrained models drop ~50–80%. For T5 specifically (examples reported on mix(T7)): n=1: 86.8% (T5); n=2 (w/o GIVE): 71.8%; n=2 (with GIVE): 5.05%; n=3 (w/o GIVE): 44.3%; n=3 (with GIVE): 15.2%. Overall, complex compositions achieve <70% accuracy for SOTA models as reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Training on the original concat (IID) bAbI data yields near-perfect IID evaluation (>99%) but poor OOD compositional generalization: large accuracy drops on mix splits (pre-trained models drop 20–50%, non-pretrained 50–80%). Exact per-model numbers vary (see Table 8 and Table 9 for breakdowns).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Compared curriculum-like setups: concat (original IID), inject (question-injection augmentation), and diverse (support-uniform rejection-sampled training). Diverse training (support-uniform) produced substantially better compositional generalization and higher data-efficiency than inject (which simply increased dataset size massively). Inject (very large training sets; e.g., inject(T12) size 368,831) was less effective than diverse (smaller size e.g., diverse(T12) 24,772) at improving OOD performance.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>The compositional-holdout design explicitly probes transfer/generalization: models often fail to generalize to novel combinations even when atomic concepts were seen during training. Pre-trained models generalize better than non-pretrained but still fail on many complex compositions (especially involving GIVE events or double disjunctions). Inoculation experiments (fine-tuning on small OOD samples) show rapid transfer: adding ~500 OOD samples per question type raised performance to >90% on challenge sets, indicating patterns are learnable but not acquired from original training sets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Exposing models to all atomic concepts at training is insufficient for robust compositional generalization; holding out combinations reveals substantial failures. Pre-training improves OOD robustness (smaller drops) but does not solve complex composition failures. Designing training splits to increase diversity of supporting-fact compositions (the 'diverse' split) is more beneficial and data-efficient than naively increasing dataset size (inject). Some composition types (e.g., GIVE, double disjunctions, yes-no phrasing variants) remain especially challenging; small targeted fine-tuning (inoculation) can quickly remedy many failures (>=90% with ~500 OOD samples per question type).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dyna-bAbI: unlocking bAbI’s potential with dynamic synthetic benchmarking', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1622.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1622.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>inject augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Question-injection augmentation (inject splits)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An augmentation strategy where for each original question in the training data, all additional possible questions of specified types are added (e.g., adding where-P and where-O questions), massively increasing the number of training questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>T5-base (representative); comparisons also include RoBERTa, BiDAF, EntNet, STM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>T5-base fine-tuned for 12 epochs; other agents include pre-trained encoders (RoBERTa) and specialized bAbI architectures (EntNet, STM) used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td>220M (T5-base); other models vary and include non-pretrained small models</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Dyna-bAbI / bAbI</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Synthetic story micro-world; inject splits reuse original passages and add additional question types to each passage (e.g., additional where- questions).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense story-procedures (entity/object tracking, possession reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Adding extra location questions (where-P, where-O) for each passage beyond the original question set.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Not primarily compositional by design; inject retains the same support-composition patterns as original but increases question coverage per passage.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Same range as original tasks (various n), but larger counts of questions per passage; inject(T12) training size reported as 368,831 (very large).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Inject augmentation improved some OOD performance relative to concat but was substantially less effective than the diverse (support-uniform) strategy; precise gains vary by model and task. The paper reports that merely injecting more questions is less helpful than increasing diversity and is less data-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Baseline concat training (no inject) yields near-perfect IID performance (>99%) but poor OOD generalization; inject gives some gains but not as large as diverse.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Compared to diverse: inject (very large datasets) produced inferior compositional generalization relative to diverse splits which used rejection sampling to balance supporting-fact compositions; diverse was more data-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Inject fails to reliably enable compositional generalization to held-out combinations despite large dataset size; models still struggle on novel support compositions, especially complex ones (n>=3) and those involving GIVE.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adding more questions per passage (inject) is less effective than increasing the diversity of supporting-fact compositions (diverse). Data quantity alone (even very large injection) does not substitute for targeted diversity in training examples when aiming for compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dyna-bAbI: unlocking bAbI’s potential with dynamic synthetic benchmarking', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1622.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1622.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>diverse (support-uniform)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diverse training via rejection sampling (support-composition uniformization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum / data-sampling strategy that uses rejection sampling to produce training data with a roughly uniform distribution over number of supporting facts per question and varied support compositions, thereby exposing models to a wider variety of compositional patterns while holding out certain combinations to test OOD generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>T5-base (representative); other models evaluated too</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>T5-base (220M) fine-tuned on diverse splits (e.g., diverse(T12) size ~24,772); other models (RoBERTa, BiDAF, EntNet, STM) also evaluated on same splits.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td>220M</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Dyna-bAbI / bAbI</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Synthetic story micro-world where diverse splits were created by rejection sampling to make the number of supporting facts per question roughly uniform and to increase diversity of support compositions; complex linguistic constructs were intentionally only seen with MOVE events at training (to hold out some combinations).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures (story-level entity and object state tracking and event reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Questions that require 1, 2, 3, or >=4 supporting facts drawn from mixed event types (MOVE, GIVE, POSS) and linguistic constructs (COREF, CONJ, NEGATE, INDEF).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Training emphasizes a wide variety of support compositions (which events and constructs form supporting facts); composition complexity operationalized by number of supporting facts (n) and by novel combinations withheld at training.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>support-composition uniform curriculum (diverse)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Rejection sampling used to generate training samples so that the distribution over number of supporting facts per question is roughly uniform (avoiding predominance of trivial 1–2 fact examples). Certain combinations (e.g., complex linguistic constructs with non-MOVE events) are held out at training to create compositional test challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>compositional complexity / support-fact variety (not temporal ordering)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Exposes tasks across support sizes n=1..>=4; diverse(T7) training size 17,000; diverse(T12) training size 24,772 (examples in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Diverse training substantially improved compositional generalization and was more data-efficient than inject; T5 trained on diverse(T12) shows near-perfect performance for simple compositions (n <= 2) but degraded performance for n >= 3. Example breakdowns (mix(T7) reported): n=1: T5 86.8%; n=2 (w/o GIVE) 71.8%; n=2 (with GIVE) 5.05%; n=3 (w/o GIVE) 44.3%; n=3 (with GIVE) 15.2%. Overall, complex compositions remain under ~70% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Without the diverse sampling (e.g., concat baseline), models achieve high IID accuracy but exhibit larger OOD failures. Diverse outperforms concat and inject in OOD compositional generalization despite smaller training set sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Direct comparison: diverse >> inject in terms of OOD compositional generalization and data efficiency; concat (original) performs worst on mix (compositional) splits. Exact numeric gains vary by task and model but trends are consistent across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Diverse training improves transfer to novel support compositions but does not fully solve compositionality; generalization degrades with increasing n and for specific phenomena (GIVE events, double disjunctions, question-format paraphrases).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Balancing training data by support-composition (diverse) yields better compositional generalization and is more data-efficient than simply increasing dataset size (inject). However, even diverse training leaves significant failure modes for complex compositions (n>=3) and particular event types (GIVE), indicating that curriculum/data design helps but is insufficient alone to achieve robust systematic generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dyna-bAbI: unlocking bAbI’s potential with dynamic synthetic benchmarking', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1622.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1622.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>inoculation finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inoculation by fine-tuning on small OOD subsets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rapid-adaptation strategy (inoculation) where a model trained on original tasks is fine-tuned on a small number of OOD (compositional challenge) examples per question type to measure learnability of withheld patterns; adapted from the methodology of Liu et al. (2019) and applied here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>T5-base (representative)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Pre-trained T5-base fine-tuned first on original bAbI/conct data and then further fine-tuned (inoculated) on small numbers of OOD compositional samples per question type.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td>220M</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Dyna-bAbI / bAbI</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Synthetic story micro-world; inoculation samples are drawn disjointly from the challenge test data (mix splits) but contain the withheld compositional patterns to which the model must adapt.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense story procedures and compositional QA patterns</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Small sets of compositional QA examples that exemplify held-out concept combinations (e.g., questions involving support compositions with GIVE, COREF and multiple supporting facts).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Not a curriculum across increasing difficulty per se; inoculation is targeted fine-tuning on the specific withheld compound patterns to test if the model can rapidly learn them.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>targeted OOD fine-tuning (inoculation)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>After base training on original tasks, the model is fine-tuned on small amounts of OOD challenge data (increasing 'dose' of inoculation) per question type; performance is evaluated as a function of the number of inoculation examples.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>small targeted exposure to withheld compositional patterns (not ordered progression)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Inoculation doses tested up to a few hundred examples per question type (paper reports ~500 samples per question type as effective).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Performance quickly improves with small inoculation doses: the paper reports that with only ~500 additional inoculation samples per question type, performance reaches >90% accuracy on both mix(T7) and mix(T12) challenge sets (Figure 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Without inoculation (i.e., only original training), models perform poorly on held-out compositional tests (large OOD drops as reported elsewhere).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>The inoculation experiment demonstrates that small targeted fine-tuning is highly effective at recovering performance on compositional challenges compared to training only on original data; it is complementary to diverse sampling but addresses residual failures quickly.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Inoculation shows that withheld compositional patterns are learnable with small amounts of targeted data: models can generalize to the compositional challenge set rapidly after limited OOD exposure, indicating that failures are largely due to insufficient training-pattern coverage rather than fundamental incapacity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Small amounts of targeted OOD fine-tuning (inoculation) can rapidly remediate compositional generalization failures (>=90% with ~500 samples per question type), implying that curriculum/data coverage is critical; however, reliance on inoculation suggests that standard training distributions do not naturally induce robust composition learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dyna-bAbI: unlocking bAbI’s potential with dynamic synthetic benchmarking', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A benchmark for systematic generalization in grounded language understanding <em>(Rating: 2)</em></li>
                <li>gSCAN: Generalization in grounded SCAN (Ruis et al., 2020) <em>(Rating: 2)</em></li>
                <li>Measuring compositional generalization: A comprehensive method on realistic data <em>(Rating: 2)</em></li>
                <li>Can small and synthetic benchmarks drive modeling innovation? a retrospective study of question answering modeling approaches <em>(Rating: 2)</em></li>
                <li>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks <em>(Rating: 1)</em></li>
                <li>CLUTRR: A diagnostic benchmark for inductive reasoning from text <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1622",
    "paper_id": "paper-37c8f0f4915b68f94669d7eeb51b4785b35c70de",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "Dyna-bAbI compositional-holdout",
            "name_full": "Dyna-bAbI compositional generalization (held-out composition) split",
            "brief_description": "A curriculum-like dataset design introduced in this paper where training splits are composed of M sub-tasks each exposing only subsets of concepts, and the test split contains novel combinations (the union) of those concepts, i.e., compositional hold-outs that force generalization to unseen concept combinations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "T5-base (representative pre-trained agent)",
            "agent_description": "A pre-trained text-to-text transformer (T5-base, 220M parameters) fine-tuned for extractive QA on bAbI-style stories; experiments also include other agents (RoBERTa, EntNet, STM, BiDAF) for comparison.",
            "agent_size": "220M",
            "environment_name": "Dyna-bAbI / bAbI micro-world",
            "environment_description": "A synthetic story-understanding benchmark (bAbI) with a programmatic micro-world simulator that generates short narratives (events like MOVE, GRAB, DROP, GIVE) and questions; Dyna-bAbI is a Python-based generator providing fine-grained control over events, linguistic mappings (co-reference, negation, indefinites, conjunction), question types, and support composition.",
            "procedure_type": "commonsense procedures (story-level entity and object tracking; event comprehension)",
            "procedure_examples": "Tracking agents/objects across locations, reasoning about possession (GRAB/DROP/GIVE), yes-no/location/count/list questions about story entities.",
            "compositional_structure": "Tasks are compositional in that test instances require novel combinations of primitive concepts (events, linguistic constructs, question types); composition is characterized by support composition f_c (which events/linguistic constructs support the answer) and number of supporting facts |f| (n), with complexity increasing as n increases or when support compositions are unseen at training.",
            "uses_curriculum": true,
            "curriculum_name": "compositional hold-out curriculum",
            "curriculum_description": "Training splits are constructed from M subtasks each exposing disjoint subsets of the final test concepts (C_train'_i), so models see all atomic concepts during training but never the full combinations present in the test split C_test = union_i C_train'_i. This isolates compositional generalization: models must compose known primitives into novel configurations at test time.",
            "curriculum_ordering_principle": "compositional complexity / held-out combinations (not explicit temporal ordering)",
            "task_complexity_range": "From single-support questions (n=1) up to multi-support compositional questions (n &gt;= 4); experiments analyze n = 1, 2, 3, &gt;=4",
            "performance_with_curriculum": "Pre-trained models (e.g., T5) obtain near-perfect performance on IID/original bAbI (&gt;99%) but on compositional hold-out (mix splits) performance drops: pre-trained models show ~20–50% absolute drop vs IID, non-pretrained models drop ~50–80%. For T5 specifically (examples reported on mix(T7)): n=1: 86.8% (T5); n=2 (w/o GIVE): 71.8%; n=2 (with GIVE): 5.05%; n=3 (w/o GIVE): 44.3%; n=3 (with GIVE): 15.2%. Overall, complex compositions achieve &lt;70% accuracy for SOTA models as reported in the paper.",
            "performance_without_curriculum": "Training on the original concat (IID) bAbI data yields near-perfect IID evaluation (&gt;99%) but poor OOD compositional generalization: large accuracy drops on mix splits (pre-trained models drop 20–50%, non-pretrained 50–80%). Exact per-model numbers vary (see Table 8 and Table 9 for breakdowns).",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Compared curriculum-like setups: concat (original IID), inject (question-injection augmentation), and diverse (support-uniform rejection-sampled training). Diverse training (support-uniform) produced substantially better compositional generalization and higher data-efficiency than inject (which simply increased dataset size massively). Inject (very large training sets; e.g., inject(T12) size 368,831) was less effective than diverse (smaller size e.g., diverse(T12) 24,772) at improving OOD performance.",
            "transfer_generalization": "The compositional-holdout design explicitly probes transfer/generalization: models often fail to generalize to novel combinations even when atomic concepts were seen during training. Pre-trained models generalize better than non-pretrained but still fail on many complex compositions (especially involving GIVE events or double disjunctions). Inoculation experiments (fine-tuning on small OOD samples) show rapid transfer: adding ~500 OOD samples per question type raised performance to &gt;90% on challenge sets, indicating patterns are learnable but not acquired from original training sets.",
            "key_findings": "Exposing models to all atomic concepts at training is insufficient for robust compositional generalization; holding out combinations reveals substantial failures. Pre-training improves OOD robustness (smaller drops) but does not solve complex composition failures. Designing training splits to increase diversity of supporting-fact compositions (the 'diverse' split) is more beneficial and data-efficient than naively increasing dataset size (inject). Some composition types (e.g., GIVE, double disjunctions, yes-no phrasing variants) remain especially challenging; small targeted fine-tuning (inoculation) can quickly remedy many failures (&gt;=90% with ~500 OOD samples per question type).",
            "uuid": "e1622.0",
            "source_info": {
                "paper_title": "Dyna-bAbI: unlocking bAbI’s potential with dynamic synthetic benchmarking",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "inject augmentation",
            "name_full": "Question-injection augmentation (inject splits)",
            "brief_description": "An augmentation strategy where for each original question in the training data, all additional possible questions of specified types are added (e.g., adding where-P and where-O questions), massively increasing the number of training questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "T5-base (representative); comparisons also include RoBERTa, BiDAF, EntNet, STM",
            "agent_description": "T5-base fine-tuned for 12 epochs; other agents include pre-trained encoders (RoBERTa) and specialized bAbI architectures (EntNet, STM) used for comparison.",
            "agent_size": "220M (T5-base); other models vary and include non-pretrained small models",
            "environment_name": "Dyna-bAbI / bAbI",
            "environment_description": "Synthetic story micro-world; inject splits reuse original passages and add additional question types to each passage (e.g., additional where- questions).",
            "procedure_type": "commonsense story-procedures (entity/object tracking, possession reasoning)",
            "procedure_examples": "Adding extra location questions (where-P, where-O) for each passage beyond the original question set.",
            "compositional_structure": "Not primarily compositional by design; inject retains the same support-composition patterns as original but increases question coverage per passage.",
            "uses_curriculum": false,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": null,
            "task_complexity_range": "Same range as original tasks (various n), but larger counts of questions per passage; inject(T12) training size reported as 368,831 (very large).",
            "performance_with_curriculum": "Inject augmentation improved some OOD performance relative to concat but was substantially less effective than the diverse (support-uniform) strategy; precise gains vary by model and task. The paper reports that merely injecting more questions is less helpful than increasing diversity and is less data-efficient.",
            "performance_without_curriculum": "Baseline concat training (no inject) yields near-perfect IID performance (&gt;99%) but poor OOD generalization; inject gives some gains but not as large as diverse.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Compared to diverse: inject (very large datasets) produced inferior compositional generalization relative to diverse splits which used rejection sampling to balance supporting-fact compositions; diverse was more data-efficient.",
            "transfer_generalization": "Inject fails to reliably enable compositional generalization to held-out combinations despite large dataset size; models still struggle on novel support compositions, especially complex ones (n&gt;=3) and those involving GIVE.",
            "key_findings": "Adding more questions per passage (inject) is less effective than increasing the diversity of supporting-fact compositions (diverse). Data quantity alone (even very large injection) does not substitute for targeted diversity in training examples when aiming for compositional generalization.",
            "uuid": "e1622.1",
            "source_info": {
                "paper_title": "Dyna-bAbI: unlocking bAbI’s potential with dynamic synthetic benchmarking",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "diverse (support-uniform)",
            "name_full": "Diverse training via rejection sampling (support-composition uniformization)",
            "brief_description": "A curriculum / data-sampling strategy that uses rejection sampling to produce training data with a roughly uniform distribution over number of supporting facts per question and varied support compositions, thereby exposing models to a wider variety of compositional patterns while holding out certain combinations to test OOD generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "T5-base (representative); other models evaluated too",
            "agent_description": "T5-base (220M) fine-tuned on diverse splits (e.g., diverse(T12) size ~24,772); other models (RoBERTa, BiDAF, EntNet, STM) also evaluated on same splits.",
            "agent_size": "220M",
            "environment_name": "Dyna-bAbI / bAbI",
            "environment_description": "Synthetic story micro-world where diverse splits were created by rejection sampling to make the number of supporting facts per question roughly uniform and to increase diversity of support compositions; complex linguistic constructs were intentionally only seen with MOVE events at training (to hold out some combinations).",
            "procedure_type": "commonsense procedures (story-level entity and object state tracking and event reasoning)",
            "procedure_examples": "Questions that require 1, 2, 3, or &gt;=4 supporting facts drawn from mixed event types (MOVE, GIVE, POSS) and linguistic constructs (COREF, CONJ, NEGATE, INDEF).",
            "compositional_structure": "Training emphasizes a wide variety of support compositions (which events and constructs form supporting facts); composition complexity operationalized by number of supporting facts (n) and by novel combinations withheld at training.",
            "uses_curriculum": true,
            "curriculum_name": "support-composition uniform curriculum (diverse)",
            "curriculum_description": "Rejection sampling used to generate training samples so that the distribution over number of supporting facts per question is roughly uniform (avoiding predominance of trivial 1–2 fact examples). Certain combinations (e.g., complex linguistic constructs with non-MOVE events) are held out at training to create compositional test challenges.",
            "curriculum_ordering_principle": "compositional complexity / support-fact variety (not temporal ordering)",
            "task_complexity_range": "Exposes tasks across support sizes n=1..&gt;=4; diverse(T7) training size 17,000; diverse(T12) training size 24,772 (examples in paper).",
            "performance_with_curriculum": "Diverse training substantially improved compositional generalization and was more data-efficient than inject; T5 trained on diverse(T12) shows near-perfect performance for simple compositions (n &lt;= 2) but degraded performance for n &gt;= 3. Example breakdowns (mix(T7) reported): n=1: T5 86.8%; n=2 (w/o GIVE) 71.8%; n=2 (with GIVE) 5.05%; n=3 (w/o GIVE) 44.3%; n=3 (with GIVE) 15.2%. Overall, complex compositions remain under ~70% accuracy.",
            "performance_without_curriculum": "Without the diverse sampling (e.g., concat baseline), models achieve high IID accuracy but exhibit larger OOD failures. Diverse outperforms concat and inject in OOD compositional generalization despite smaller training set sizes.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Direct comparison: diverse &gt;&gt; inject in terms of OOD compositional generalization and data efficiency; concat (original) performs worst on mix (compositional) splits. Exact numeric gains vary by task and model but trends are consistent across experiments.",
            "transfer_generalization": "Diverse training improves transfer to novel support compositions but does not fully solve compositionality; generalization degrades with increasing n and for specific phenomena (GIVE events, double disjunctions, question-format paraphrases).",
            "key_findings": "Balancing training data by support-composition (diverse) yields better compositional generalization and is more data-efficient than simply increasing dataset size (inject). However, even diverse training leaves significant failure modes for complex compositions (n&gt;=3) and particular event types (GIVE), indicating that curriculum/data design helps but is insufficient alone to achieve robust systematic generalization.",
            "uuid": "e1622.2",
            "source_info": {
                "paper_title": "Dyna-bAbI: unlocking bAbI’s potential with dynamic synthetic benchmarking",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "inoculation finetuning",
            "name_full": "Inoculation by fine-tuning on small OOD subsets",
            "brief_description": "A rapid-adaptation strategy (inoculation) where a model trained on original tasks is fine-tuned on a small number of OOD (compositional challenge) examples per question type to measure learnability of withheld patterns; adapted from the methodology of Liu et al. (2019) and applied here.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "T5-base (representative)",
            "agent_description": "Pre-trained T5-base fine-tuned first on original bAbI/conct data and then further fine-tuned (inoculated) on small numbers of OOD compositional samples per question type.",
            "agent_size": "220M",
            "environment_name": "Dyna-bAbI / bAbI",
            "environment_description": "Synthetic story micro-world; inoculation samples are drawn disjointly from the challenge test data (mix splits) but contain the withheld compositional patterns to which the model must adapt.",
            "procedure_type": "commonsense story procedures and compositional QA patterns",
            "procedure_examples": "Small sets of compositional QA examples that exemplify held-out concept combinations (e.g., questions involving support compositions with GIVE, COREF and multiple supporting facts).",
            "compositional_structure": "Not a curriculum across increasing difficulty per se; inoculation is targeted fine-tuning on the specific withheld compound patterns to test if the model can rapidly learn them.",
            "uses_curriculum": true,
            "curriculum_name": "targeted OOD fine-tuning (inoculation)",
            "curriculum_description": "After base training on original tasks, the model is fine-tuned on small amounts of OOD challenge data (increasing 'dose' of inoculation) per question type; performance is evaluated as a function of the number of inoculation examples.",
            "curriculum_ordering_principle": "small targeted exposure to withheld compositional patterns (not ordered progression)",
            "task_complexity_range": "Inoculation doses tested up to a few hundred examples per question type (paper reports ~500 samples per question type as effective).",
            "performance_with_curriculum": "Performance quickly improves with small inoculation doses: the paper reports that with only ~500 additional inoculation samples per question type, performance reaches &gt;90% accuracy on both mix(T7) and mix(T12) challenge sets (Figure 7).",
            "performance_without_curriculum": "Without inoculation (i.e., only original training), models perform poorly on held-out compositional tests (large OOD drops as reported elsewhere).",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "The inoculation experiment demonstrates that small targeted fine-tuning is highly effective at recovering performance on compositional challenges compared to training only on original data; it is complementary to diverse sampling but addresses residual failures quickly.",
            "transfer_generalization": "Inoculation shows that withheld compositional patterns are learnable with small amounts of targeted data: models can generalize to the compositional challenge set rapidly after limited OOD exposure, indicating that failures are largely due to insufficient training-pattern coverage rather than fundamental incapacity.",
            "key_findings": "Small amounts of targeted OOD fine-tuning (inoculation) can rapidly remediate compositional generalization failures (&gt;=90% with ~500 samples per question type), implying that curriculum/data coverage is critical; however, reliance on inoculation suggests that standard training distributions do not naturally induce robust composition learning.",
            "uuid": "e1622.3",
            "source_info": {
                "paper_title": "Dyna-bAbI: unlocking bAbI’s potential with dynamic synthetic benchmarking",
                "publication_date_yy_mm": "2021-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A benchmark for systematic generalization in grounded language understanding",
            "rating": 2
        },
        {
            "paper_title": "gSCAN: Generalization in grounded SCAN (Ruis et al., 2020)",
            "rating": 2
        },
        {
            "paper_title": "Measuring compositional generalization: A comprehensive method on realistic data",
            "rating": 2
        },
        {
            "paper_title": "Can small and synthetic benchmarks drive modeling innovation? a retrospective study of question answering modeling approaches",
            "rating": 2
        },
        {
            "paper_title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
            "rating": 1
        },
        {
            "paper_title": "CLUTRR: A diagnostic benchmark for inductive reasoning from text",
            "rating": 1
        }
    ],
    "cost": 0.016965,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Dyna-bAbI: unlocking bAbI's potential with dynamic synthetic benchmarking</h1>
<p>Ronen Tamari ${ }^{\dagger <em>}$ Kyle Richardson</em> Noam Kahlon ${ }^{\dagger}$ Aviad Sar-Shalom ${ }^{\triangle}$ Nelson F. Liu<em> Reut Tsarfaty</em> ${ }^{\ddagger}$ Dafna Shahaf ${ }^{\dagger}$<br>${ }^{\dagger}$ The Hebrew University of Jerusalem <em>Allen Institute for AI<br>${ }^{\ddagger}$ Bar-Ilan University ${ }^{\triangle}$ Tel-Aviv University ${ }^{\text {</em> }}$ Stanford University<br>{ronent, dshahaf}@cs.huji.ac.il, {reutt, kyler}@allenai.org</p>
<h4>Abstract</h4>
<p>While neural language models often perform surprisingly well on natural language understanding (NLU) tasks, their strengths and limitations remain poorly understood. Controlled synthetic tasks are thus an increasingly important resource for diagnosing model behavior. In this work we focus on story understanding, a core competency for NLU systems. However, the main synthetic resource for story understanding, the bAbI benchmark, lacks such a systematic mechanism for controllable task generation. We develop Dyna-bAbI, a dynamic framework providing fine-grained control over task generation in bAbI. We demonstrate our ideas by constructing three new tasks requiring compositional generalization, an important evaluation setting absent from the original benchmark. We tested both special-purpose models developed for bAbI as well as state-of-the-art pre-trained methods, and found that while both approaches solve the original tasks ( $&gt;99 \%$ accuracy), neither approach succeeded in the compositional generalization setting, indicating the limitations of the original training data. We explored ways to augment the original data, and found that though diversifying training data was far more useful than simply increasing dataset size, it was still insufficient for driving robust compositional generalization (with $&lt;70 \%$ accuracy for complex compositions). Our results underscore the importance of highly controllable task generators for creating robust NLU systems through a virtuous cycle of model and data development. ${ }^{\dagger}$</p>
<h2>1 Introduction</h2>
<p>Considerable progress has been made recently in natural language understanding (NLU), driven largely by advances in model pre-training (Devlin</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) Low task configurability leads to static datasets, benchmark saturation \&amp; unreliable model development. (b) We propose a dynamic benchmarking approach; developing models and tasks in a tight feedback loop using (c) Dyna-bAbI task generator. Dyna-bAbI provides fine-grained control over task structure, composition and difficulty, yielding challenging new test sets exposing limitations of state-of-the-art models.
et al., 2019; Raffel et al., 2020) and the development of large-scale NLU benchmarks across a wide range of tasks (Wang et al., 2018, 2019; Liang et al., 2020). Such successes, however, have coincided with the discovery of various shortcomings in existing human curated datasets, largely related to annotation artifacts (Gururangan</p>
<p>et al., 2018), or systematic biases that create shortcuts that can inflate model performance and harm generalization.</p>
<p>In order to overcome these issues, two avenues of research have recently gained traction: 1) development of dynamic benchmarks (Potts et al., 2021; Kiela et al., 2021) where, in contrast to conventional static benchmarks, evaluation and data collection are conducted interactively with humans and models in a rapidly evolving feedback loop and; 2) renewed interest in synthetic benchmarks (Lake and Baroni, 2018; Sinha et al., 2019; Clark et al., 2020; Ruis et al., 2020) that allow for absolute control over the data creation process in order to help understand the strengths and weaknesses of existing models on targeted tasks and language phenomena.</p>
<p>Story understanding is a particularly important domain for research on dynamic and synthetic benchmarks; it is a core competency for NLU systems (McClelland et al., 2020; Dunietz et al., 2020), but the scale and annotation detail required make human data collection prohibitively costly. However, the main synthetic resource for story understanding remains the bAbI task suite (Weston et al., 2016), which is saturated by models reaching near-perfect performance (Liu et al., 2021), and further limited by exploitable biases in the data (Kaushik and Lipton, 2018). Despite its creators' initial intentions, bAbI has largely remained a static benchmark limited to a small subset of the tasks potentially possible to generate within the bAbI "micro-world". Accordingly, two natural questions arise: (Q1) is near-perfect model performance on the original bAbI tasks a reliable indicator of story understanding competence?; (Q2) are there still interesting challenges to discover inside the broader bAbI task space that help identify weaknesses in current models and drive modelling innovation?</p>
<p>To answer these questions, we employ a dynamic synthetic benchmarking approach on bAbI , combining the benefits of the agile approach of recent dynamic benchmarks with the scale and control provided by synthetic datasets. As illustrated in Figure 1, in dynamic synthetic benchmarks the data generator itself is designed for agile development, enabling experimentation with increasingly complex tasks and a wider range of linguistic phenomena. ${ }^{2}$ Constructing</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>challenging tasks is a challenge in and of itself, requiring precise control over the reasoning patterns underlying each question. To meet these requirements, we developed a new task generator for bAbI called Dyna-bAbI ${ }^{3}$.</p>
<p>Using Dyna-bAbI, we first devise new splits that systematically test compositional generalization across tasks; as shown in Fig. 1c, we test models on novel combinations (right side, line 10) of concepts seen at training, like co-reference and object tracking (left). We find that training on the original bAbI tasks (hereafter: bAbI 1.0) is not sufficient for models to attain good compositional generalization. Though general purpose pre-trained models far outperform special-purpose (non-pretrained) architectures developed for bAbI, they still suffer a $20-50 \%$ drop in accuracy compared to the non-pre-trained models which suffer a $50-80 \%$ drop. Both types attain near perfect performance on the original tasks, suggesting that bAbI 1.0 is not challenging enough to differentiate between the two classes of models (Q1).</p>
<p>We next investigate how different enhancements of training data affect compositional generalization: (a) injecting more questions into bAbI 1.0, and (b) generating new, more diverse training samples. Compared to question injection, we find that diverse training data better facilitates compositional generalization, as well as being more data efficient. However, neither approach drives reliable compositional generalization; a representative state-of-the-art (SOTA) model, T5 (Raffel et al., 2020), demonstrates a lack of robustness to novel combinations and also exhibits knowledge inconsistency, for example, by correctly answering certain types of questions but systematically failing to answer equivalent paraphrases. These results suggest that there remain many important challenges within the broader bAbI task space (Q2) which can be discovered through more careful control of task generation.</p>
<p>To sanity-check the quality of our new tests compared with bAbI 1.0, we employ the notion of concurrence proposed by Liu et al. (2021);</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>concurrence is a measure of correlation between models' performance on a synthetic task and their performance on an existing, non-synthetic NLU benchmark. We find high concurrence between our new challenge tasks and the widely used SQuAD dataset (Rajpurkar et al., 2016), in contrast to bAbI 1.0 , which achieved low concurrence.</p>
<p>Giving the continued interest in using bAbI 1.0 to evaluate new modelling approaches (Banino et al., 2020, 2021; Schlag et al., 2021), our new challenge splits and the Dyna-bAbI task generator contribute to more reliably guiding future efforts. While we focused on bAbI, our results apply more generally, telling a cautionary tale about the limits of static synthetic datasets, and motivating the development of controllable task generators for dynamic synthetic benchmarking.</p>
<h2>2 Related Work</h2>
<p>Our work brings together two promising areas of current research: dynamic benchmarking such as Dynabench (Kiela et al., 2021) that address many existing issues with static benchmarks (Bowman and Dahl, 2021), and synthetic benchmarking, which is widely used for high-precision and dataintensive problems such as relational and logical reasoning (Sinha et al., 2019; Clark et al., 2020; Betz et al., 2021; Richardson and Sabharwal, 2022), robot planning (Banerjee et al., 2020), instruction following and language grounding (Long et al., 2016; Lake and Baroni, 2018) among many others (Richardson et al., 2020; Khot et al., 2021). Most approaches to synthetic benchmarking focus on model development on a static benchmark, and are not designed to facilitate agile and highly controlled task space exploration, which is our focus here.</p>
<p>The recent gSCAN dataset (Ruis et al., 2020) and later extensions (Qiu et al., 2021; Wu et al., 2021) can be seen as an example of a synthetic benchmark "going dynamic". Our work differs in terms of target domain (story understanding as opposed to multi-modal language grounding), and we further focus attention on a more general research direction of intentional, a-priori design of NLU benchmarks for agile development. In this regard, our work can be seen as part of a trend towards data-centric research efforts in response to prevailing modelcentric research, which generally focuses heavily on architectural design and novelty (Kaushik and Lipton, 2018), at the expense of work on the data
side (Sambasivan et al., 2021; Rogers, 2021).
We address the domain of story understanding as a particularly core (and data-intensive) capacity underlying language use (McClelland et al., 2020), thought to require constructing and manipulating situation models of entities and their relations as they unfold throughout discourse (Zwaan, 2016; Tamari et al., 2020). Procedural text datasets (Dalvi et al., 2018; Tandon et al., 2020) are closely related in that they provide detailed annotation of entities and state changes, and have mostly focused on relatively small and static benchmarks using human collected data. Overall, recent works identify a lack of benchmarks which systematically probe the situation models constructed by systems processing discourse-level texts (Sugawara et al., 2021).</p>
<p>The bAbI benchmark (Weston et al., 2016) is seen as highly relevant in terms of objective (targeting situation modelling) (Dunietz et al., 2020), but has been viewed critically due to its constrained nature and exploitable artifacts (Kaushik and Lipton, 2018). Our work focuses on improving the evaluation in bAbI through compositional generalization, widely used across NLP to more rigorously probe model robustness (Finegan-Dollak et al., 2018; Keysers et al., 2020; Gontier et al., 2020; Yanaka et al., 2021), but to our knowledge still not applied to story understanding or bAbI.</p>
<h2>3 Synthetic Dynamic Benchmarking on bAbI</h2>
<h3>3.1 Dyna-bAbI</h3>
<p>What makes a synthetic benchmark dynamic? We think of a dynamic synthetic benchmark as a highly controllable task generator, enabling rapid exploration of interesting areas of a task space. The original bAbI 1.0 simulator code does not readily facilitate such exploration; each of the bAbI 1.0 tasks is generated by a hard-coded script which does not enable parametric manipulation of interesting generation aspects such as question difficulty or compositionality.</p>
<p>Accordingly, we developed Dyna-bAbI, a Python-based version of the original simulator. Dyna-bAbI facilitates control of task generation through a configuration file, effectively abstracting away much of the underlying implementation complexity. The configuration file allows users to specify high-level task parameters such as the set of target concepts, passage length, and filtering</p>
<p>conditions to mine for harder/rarer examples. We also modularized the code to facilitate adding new questions and other concepts more easily.</p>
<p>In this next sections we describe the underlying structure of the bAbI 1.0 tasks, and how we combine them using Dyna-bAbI to create more complex compositional generalization tasks.</p>
<h2>3.2 bAbI task structure</h2>
<p>A task in bAbI 1.0 is a set of train, validation and test splits. Each split is a set of instances, where an instance is a tuple $(p, q, a)=($ passage, question, answer). Passages are generated using a micro-world simulator by sampling a valid sequence of world events from an event set $\mathcal{E}$ and generating a linguistic description of them. By default, linguistic descriptions are generated by a simple sentence-level mapping from an event to a natural language sentence. For example, the event move (john, park) could be translated to "John moved to the park."</p>
<p>Some tasks also incorporate more complex linguistic mappings between events and sentences, such as co-reference: the event sequence (move (john, park), move (john, kitchen)) could be mapped to "John moved to the park. Then he went to the kitchen." We denote the set of possible linguistic mappings by $\mathcal{L}$.</p>
<p>Finally, a valid question-answer pair $(q, a)$ over $p$ is sampled from question set $\mathcal{Q}$. In bAbI, each split is generated using some particular subset of all possible events, linguistic constructs and questions (§3.3); for a given split we can then define its concept set, $\mathcal{C}=\mathcal{E} \cup \mathcal{L} \cup \mathcal{Q}$. Instances also include a set of supporting facts $(f)$, or the relevant lines from which $a$ can be derived (see Fig. 1). The support composition $\left(f_{c}\right)$ is the set of events and linguistic constructs contained in $f$ (see examples in $\S 4.2 .1$ ), and is useful for characterizing compositionality performance (§3.4).</p>
<h3>3.3 Original bAbI 1.0 tasks</h3>
<p>Our focus here is on a particular subset of 12 bAbI 1.0 tasks evaluating aspects of story understanding. Table 1 summarizes them, detailing $\mathcal{E}, \mathcal{L}, \mathcal{Q}$ for each task. For $\mathcal{L}$, we list only complex constructs beyond the default event-sentence mapping (which is present in every task). See appendix A. 1 for additional details on task construction. Not all of the story understanding tasks are considered. For example, tasks 14 and 20 address time</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Events <br> $(\mathcal{E})$</th>
<th style="text-align: center;">Linguistic <br> Constructs <br> $(\mathcal{L})$</th>
<th style="text-align: center;">Questions <br> $(\mathcal{Q})$</th>
<th style="text-align: center;">Avg. sents. \&amp; <br> supp. facts <br> per story</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">MOVE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">where-P</td>
<td style="text-align: center;">6,1</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">MOVE, <br> POSS</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">where-O</td>
<td style="text-align: center;">$15.52,2$</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">MOVE, <br> POSS <br> MOVE,</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">where-was-O</td>
<td style="text-align: center;">$51.9,3$</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">GIVE, <br> POSS</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">give-qs</td>
<td style="text-align: center;">20.1,1</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">MOVE <br> MOVE,</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">yes-no</td>
<td style="text-align: center;">$6.27,1$</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">GIVE, <br> POSS</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">counting</td>
<td style="text-align: center;">$8.67,2.33$</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">MOVE, <br> POSS</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">list</td>
<td style="text-align: center;">$8.75,1.94$</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">MOVE</td>
<td style="text-align: center;">NEGATE</td>
<td style="text-align: center;">yes-no</td>
<td style="text-align: center;">6,1</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">MOVE</td>
<td style="text-align: center;">INDEX</td>
<td style="text-align: center;">yes-no</td>
<td style="text-align: center;">6,1</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">MOVE</td>
<td style="text-align: center;">CO-REF</td>
<td style="text-align: center;">where-P</td>
<td style="text-align: center;">6,2</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">MOVE</td>
<td style="text-align: center;">CONJ.</td>
<td style="text-align: center;">where-P</td>
<td style="text-align: center;">6,1</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">MOVE</td>
<td style="text-align: center;">CONJ., <br> CO-REF</td>
<td style="text-align: center;">where-P</td>
<td style="text-align: center;">6,2</td>
</tr>
</tbody>
</table>
<p>Table 1: Subset of 12 bAbI 1.0 tasks considered here. Each task is characterized by the possible events, linguistic constructs and questions that can occur in instances. POSS (possession) is short for GRAB and DROP events. Statistics based on training sets. A large space of task configurations remains unexplored.
reasoning and agent motivations, and we leave their integration for future work.</p>
<h3>3.4 Compositional generalization on bAbI</h3>
<p>As can be seen in Table 1, many possible task configurations are not covered by the original benchmark; which directions should be explored? We focus on out-of-distribution (OOD) robustness, which is increasingly seen as a vital evaluation criteria across AI/NLP research (Shanahan et al., 2020; Hendrycks et al., 2020). We target compositional generalization, a particularly important class of OOD problems (Lake et al., 2017; Lake and Baroni, 2018). Compositional generalization refers to the ability to systematically generalize to test inputs containing novel combinations of more basic elements seen at training time (Partee et al., 1995). For example, a model that has learned basic object tracking and co-reference separately (tasks 2 and 11, see Fig. 1c) could be expected to solve tasks requiring a mixture of both object tracking and co-reference (Fig. 1c, line 10 question on right side). Compositional tasks are absent from bAbI 1.0 which features only IID test sets (independent,</p>
<p>identically distributed). ${ }^{4}$
Compositional task generation. To create compositional generalization tasks in practice, we create training (and validation) splits composed of $M$ sub-tasks with concept sets $\left{\mathcal{C}<em i="1">{\text {train }}^{\prime}\right}</em>}^{M}$, and a test set $\mathcal{C<em _test="{test" _text="\text">{\text {test }}$ such that $\mathcal{C}</em>}} \neq \mathcal{C<em _test="{test" _text="\text">{\text {train }}^{\prime} \forall i$, but $\mathcal{C}</em>}}=$ $\bigcup_{i=1}^{M} \mathcal{C<em c="c">{\text {train }}^{\prime}$. In other words, each training subtask can be thought of focusing on a particular subset of test concepts, so models are exposed to all test concepts at training time, but not to all combinations of them (Yanaka et al., 2021).
Task difficulty. We hypothesize that support composition $\left(f</em>\right)$ and supporting fact set size $(|f|)$ are main factors underlying a particular instance's difficulty, and especially novel support compositions not seen at training time. Additionally, the difference between train and test splits results in potentially harder distractors, as test-time distractors appear in novel contexts.</p>
<p>Our notions of concepts and support composition resemble atoms and compounds in DBCA, a related study on compositionality (Keysers et al., 2020). While DBCA enables automatic creation of compositional train and test splits, we opt here for a more human-interpretable representation that allows more precise manual control of the combinations of concepts a model is exposed to at train and test time.
Quality comparison vs. bAbI 1.0 tasks. Intuitively, good synthetic datasets help drive the development of better modelling approaches. Our new compositional tasks might be harder than bAbI 1.0, but how do we know whether they are a more useful target? To provide a preliminary answer to this question, we adopt the notion of concurrence as a quality measure (Liu et al., 2021). Two benchmarks are said to have high concurrence when they rank a set of modelling approaches similarly. Concurrence offers a way to formalize the intuition above, as high concurrence between a synthetic and natural language benchmark suggests that the synthetic benchmark could have driven similar innovations. We follow the setup of Liu et al. (2021) using SQuAD for the natural language benchmark. ${ }^{5}$ Notably, bAbI 1.0 achieved very low concurrence with SQuAD; for example, pre-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Split | Type | Avg. <br> length | Size | Avg. supp. <br> fact set size |
| :-- | :-- | :-- | :-- | --: |
| concat(T2) | Train | 10.76 | 18,000 | 2 |
| concat(T7) | Train | 13.5 | 63,000 | 1.68 |
| inject(T7) | Train | 23.25 | 190,158 | 1.42 |
| diverse(T7) | Train | 20 | 17,000 | 2.17 |
| concat(T12) | Train | 10.8 | 108,000 | 1.42 |
| inject(T12) | Train | 15.97 | 368,831 | 1.28 |
| diverse(T12) | Train | 20 | 24,772 | 2.45 |
| mix(T2) | Test | 13.25 | 1,000 | 2.05 |
| mix(T7) | Test | 20 | 3,000 | 2.50 |
| mix(T12) | Test | 20 | 6,000 | 3.70 |</p>
<p>Table 2: Splits used for our experiments. All except the original data (concat) are created with Dyna-bAbI.
training consistently yields large gains on SQuAD, but on bAbI 1.0, both pre-trained and non-pretrained models achieve perfect performance on many tasks. The low concurrence thus suggests that bAbI 1.0 may be an unreliable benchmark for model development, and highlights the importance of improving its quality.</p>
<h2>4 Experiments</h2>
<p>With the controllable task generation afforded by Dyna-bAbI, we can now create datasets probing deeper story understanding capabilities of models.</p>
<p>We present two main experiments targeting the following questions:</p>
<ul>
<li>Exp. 1: (q1.a) What role does model architecture play in the capacity for compositional generalization? (q1.b) What is the concurrence of our compositional tasks with real datasets, compared with bAbI 1.0?</li>
<li>Exp. 2: (q2) How do training data quantity and diversity affect compositional generalization?</li>
</ul>
<h2>Data</h2>
<p>For our experiments we created 4 kinds of splits over three subsets of bAbI 1.0 tasks, summarized in Table 2. We denote a subset of tasks $T$, and consider $T_{2}={2,11}$, $T_{7}={1,2,3,5,11,12,13}$, and $T_{12}=$ ${1,2,3,5, \ldots, 13}$.</p>
<ul>
<li>concat splits are simply concatenations of the official data for the tasks $T$. We considered the larger version where each task consists of 9,000/1,000 training/development examples; e.g., concat $\left(T_{2}\right)$ consists of 18,000 training examples and 2,000 development examples.</li>
<li>inject splits enrich the concat data as follows:</li>
</ul>
<p>for each question in the original data, we supplement it with all possible additional questions of the specified types. In this work, the supplement question types were where- $P$ and where- $O$ (to provide location information of objects and agents).</p>
<ul>
<li>diverse splits use rejection sampling to generate more diverse samples, such that the number of supporting facts per question is roughly uniform across all sub-task instances for a given question type. Without rejection sampling, most generated questions would be trivial (e.g., 1-2 supporting facts). Compositionality is retained by holding out certain combinations. In particular, at training time, complex linguistic constructs (e.g., coreference) are only seen with MOVE events.</li>
<li>mix are test splits generated using rejection sampling like diverse, and consist of instances which may feature elements from any of the considered tasks. As a result, questions in mix splits require novel/more complex reasoning patterns compared to those seen during training.
See appendix A. 1 for examples and extended details on task generation.</li>
</ul>
<h3>4.1 Exp. 1: Can training on bAbI 1.0 facilitate compositional generalization?</h3>
<p>For this experiment, we compared models on $T_{2}$ and $T_{7}$, since they allow for a direct conversion to an extractive QA format, ${ }^{6}$ enabling us to use the same concurrence framework of Liu et al. (2021).
Models. We considered 3 classes of models:</p>
<ul>
<li>Non-pre-trained specialized architectures for bAbI 1.0 including EntNet (Henaff et al., 2017) and STM (Le et al., 2020), the latter being current SOTA on bAbI $1.0^{7}$.</li>
<li>Non-pretrained general-purpose QA methods, such as BiDAF (Seo et al., 2017).</li>
<li>General purpose pre-trained approaches including RoBERTa (Liu et al., 2020) and T5 (base) (Raffel et al., 2020).
The last two categories are comprised of the 20 models evaluated in Liu et al. (2021), with the addition of T5 to the last group. For implementation details, see appendix A.2.</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Results \&amp; Analysis</h2>
<p>Experiment results are summarized in Table 3. All models perform well in IID settings, but performance drops considerably in OOD settings
Architecture alone is not a significant compositionality driver (q1.a). The large OOD performance gap between pre-trained and non-pre-trained models indicates that pre-training plays a much greater role than specialized architectures for QA performance, adding to similar findings in other NLP domains (Hendrycks et al., 2020). These results raise questions about special purpose relational reasoning architectures that continue to be developed today: the poor OOD performance suggests that such models may not be fulfilling their intended design. Either way, these results underscore the importance of rigorous evaluation to verify that modelling motivations are borne out in practice (Aina et al., 2019).
Compositionality increases concurrence (q1.b). As can be seen in the Fig. 2 plots $^{8}$, increasing compositionality is correlated with increased concurrence. In contrast to the original bAbI 1.0 tasks which exhibited virtually no correlation with SQuAD, our compositional task $\operatorname{mix}\left(T_{7}\right)$ exhibits high concurrence of $r=0.92, \tau=0.78$ (Pearson and Kendall correlation functions, resp.). These results are comparable to other natural language as well as purpose-built synthetic datasets considered in Liu et al. (2021), which feature $r, \tau$ in the ranges $[0.87,0.99]$ and $[0.77,0.94]$, respectively. Our results thus extend the findings of Liu et al. (2021); they demonstrated the existence of high concurrence synthetic benchmarks, we additionally suggest a guiding principle for how to create them (incorporate compositionality evaluation).</p>
<h3>4.2 Exp. 2: enriching bAbI 1.0 training data</h3>
<p>The results above suggest that the bAbI data in their current form may not be rich enough to drive compositional generalization. ${ }^{9}$ In this experiment we probe this question, enriching the training data to better understand its impact on compositional generalization. In particular, we investigate two approaches to enriching the training data while maintaining the compositionality evaluation, corresponding to the inject and diverse splits.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>| Name | Train | Test | Evaluation accuracy | | | | | SQuAD Concurrence | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Error analysis on $\operatorname{mix}\left(T_{12}\right)$ for T5 trained on diverse $\left(T_{12}\right)$ data. The sub-plots break down performance on questions requiring ${\leq 2,3, \geq 4}$ supporting facts. For each sub-plot, the left side of each row corresponds to a particular support composition $\left(f_{c}\right)$, and the right side displays accuracy over inputs sharing $f_{c}$, across various question types. Performance on $f_{c}$ seen at training time (blue frames) is generally high, but overall generalization is not systematic, as evidenced by high variance across different $f_{c}$, especially for higher complexity $(n=3, n \geq 4)$ and more novel compositions.
generalization as well as much improved data efficiency. ${ }^{10}$ However, as the error analysis of the next section shows, performance on compositional generalization is still fundamentally limited.</p>
<h3>4.2.1 Discussion and error analysis</h3>
<p>Figure 3 breaks down the performance of T5 on $\operatorname{mix}\left(T_{12}\right)$ after training on diverse $\left(T_{12}\right)$. The heatmaps plot performance across various support compositions $\left(f_{c}\right)$ occurring in the test data, subdivided by the number of required supporting facts $n$ per question. Performance on support compositions seen at training time (blue frames) is generally high, indicating the importance of training pattern diversity for better generalization. The plots indicate that T5 shows some ability to generalize to new support compositions, especially for lower $n$. Furthermore, certain question types appear to be more learned more robustly; for list and count questions, performance remains relatively high even for larger $n$ and across novel $f_{c}$. We hypothesize that such questions may be easier as simple counting rules suffice to reach an answer, and these are "close to the surface"; unlike other events that may implicitly convey</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Example $\operatorname{mix}\left(T_{12}\right)$ instance demonstrating the question phrasing sensitivity failure mode in T5: the model correctly answers the question in where- $P$ form (line 22), and incorrectly in yes-no form (line 21).
information, in our stories, changes of possession are always explicit in the text.</p>
<p>In general however, the plots indicate that T5 is far from robust compositional generalization:</p>
<h2>Performance deteriorates with increased complexity. Performance is near perfect for simple</h2>
<p>compositions $(n \leq 2)$ but deteriorates significantly for more complex cases $(n \geq 3)$.</p>
<p>Question phrasing sensitivity. The discrepancy between the relatively high performance on where- $P$ questions compared with very low performance on yes-no questions suggests that models are learning highly question-dependent story representations. E.g., if a model answers $y$ correctly to some "Where is $p$ ?" question, we would expect it to answer "yes" correctly for the same question in yes-no format, "Is $p$ at $y$ ?". Figure 4 shows a characteristic example: T5 answers correctly in the where- $P$ format, but incorrectly answers "maybe" for the yes-no format, likely thrown off by the distractor indefinite phrase in sentence 3.</p>
<p>We present further empirical support for question phrasing sensitivity in appendix A.6. These results suggest models may be learning shortcuts that work well for the story/question pairs seen at training time, but not more robust rules that also generalize to novel test instances. Such highly question-dependent story representation stands in contrast to more human-like narrative comprehension, which is thought to involve the construction of situation models, or structured representations of entities and their relations as depicted by the text. Situation models are less dependent on a-priori knowledge of a question (or its phrasing), and are often generated on-line during the course of comprehension (Graesser et al., 1994).</p>
<p>Performance below chance for certain question types. The heatmaps expose a particularly challenging class of yes-no questions involving disjunctions over indefinites (center and right plots, bottom right); accuracy for such questions is close to zero. See appendix A. 7 for an example instance.</p>
<h2>5 Future work \&amp; conclusions</h2>
<p>Our work opens up multiple new directions for future research. Our new tool, Dyna-bAbI is readily extendable for systematic probing of more diverse linguistic phenomena. A beneficial first step could include integration of additional bAbI tasks. That said, our experience suggests that the design of truly scalable synthetic and dynamic benchmarks poses significant theoretical and engineering challenges, warranting deeper research on their own right.</p>
<p>Our results raise new questions about the viability of learning robust situation models using standard question-answering training, and our
datasets present new challenges for future efforts.
Additionally, Dyna-bAbI can naturally complement parallel work probing the the situation representations constructed by neural language models (Li et al., 2021) by facilitating tailored data generation for specific questions, thus broadening and deepening the scope of possible research.</p>
<p>In conclusion, we introduced Dyna-bAbI, a new framework for highly controllable bAbI task generation. We used it to create compositional generalization datasets providing new modelling challenges for state-of-the-art neural language models. More broadly, our results underscore the importance in development of benchmarks themselves, beyond only the models solving them.</p>
<h2>Broader Impact</h2>
<p>While large, neural language models are increasingly seen as foundations for a wide array of NLP tasks, we still lack a clear understanding of their capabilities and failure modes. Our work joins many recent efforts using carefully controlled synthetic tasks to more rigorously evaluate models' language comprehension abilities.</p>
<p>While our choice of a synthetic language benchmark allows more precise control over evaluation, the synthetic nature of the data is an obvious limitation. Similar to the original bAbI benchmark, our tasks are not a substitute for real natural language datasets, but should rather complement them. Even if a method works well on our data, it should be shown to perform well on real data as well. Rather, our tasks are better thought of as comprehension "unit-tests", where poor performance on our tasks serves as a warning sign suggesting the model may exhibit limited systematicity and robustness on more difficult, naturalistic inputs.</p>
<h2>Acknowledgements</h2>
<p>We thank the Aristo team at the Allen Institute for AI for valuable support and feedback. Ronen Tamari was supported by the Center for Interdisciplinary Data-science Research at HUJI. This work was supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant no. 852686, SIAM, Shahaf). Part of this research is also supported by the European Research Council, ERC-StG grant no. 677352 (Tsarfaty), which we gratefully acknowledge.</p>
<h2>References</h2>
<p>Laura Aina, Carina Silberer, Ionut-Teodor Sorodoc, Matthijs Westera, and Gemma Boleda. 2019. What do entity-centric models learn? insights from entity linking in multi-party dialogue. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 37723783, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Pratyay Banerjee, Chitta Baral, Man Luo, Arindam Mitra, Kuntal Pal, Tran C. Son, and Neeraj Varshney. 2020. Can transformers reason about effects of actions? Computing Research Repository, arXiv:2012.09938.</p>
<p>Andrea Banino, Adrià Puigdomènech Badia, Raphael Köster, Martin J. Chadwick, Vinicius Zambaldi, Demis Hassabis, Caswell Barry, Matthew Botvinick, Dharshan Kumaran, and Charles Blundell. 2020. Memo: A deep network for flexible combination of episodic memories. In International Conference on Learning Representations.</p>
<p>Andrea Banino, Jan Balaguer, and Charles Blundell. 2021. Pondernet: Learning to ponder. In 8th ICML Workshop on Automated Machine Learning (AutoML).</p>
<p>Gregor Betz, Christian Voigt, and Kyle Richardson. 2021. Critical thinking for language models. Proceedings of IWCS.</p>
<p>Lukas Biewald. 2020. Experiment tracking with weights and biases. Software available from wandb.com.</p>
<p>Samuel R. Bowman and George Dahl. 2021. What will it take to fix benchmarking in natural language understanding? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4843-4855, Online. Association for Computational Linguistics.</p>
<p>Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 3882-3890. International Joint Conferences on Artificial Intelligence Organization. Main track.</p>
<p>Bhavana Dalvi, Lifu Huang, Niket Tandon, Wen-tau Yih, and Peter Clark. 2018. Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1595-1604, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Jesse Dunietz, Greg Burnham, Akash Bharadwaj, Owen Rambow, Jennifer Chu-Carroll, and Dave Ferrucci. 2020. To test machine comprehension, start by defining comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7839-7859, Online. Association for Computational Linguistics.</p>
<p>William Falcon et al. 2019. Pytorch lightning. GitHub. Note: https://github.com/PyTorchLightning/pytorchlightning, 3.</p>
<p>Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir Radev. 2018. Improving text-to-SQL evaluation methodology. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 351-360, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Nicolas Gontier, Koustuv Sinha, Siva Reddy, and Christopher Pal. 2020. Measuring systematic generalization in neural proof generation with transformers. In Advances in Neural Information Processing Systems 33. Curran Associates, Inc.</p>
<p>Arthur C. Graesser, Murray Singer, and Tom Trabasso. 1994. Constructing Inferences During Narrative Text Comprehension. Psychological Review, 101(3):371395.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. 2017. Tracking the world state with recurrent entity networks. 5th International Conference on Learning Representations, ICLR 2017 ; Conference date: 24-04-2017 Through 26-04-2017.</p>
<p>Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. 2020. Pretrained transformers improve out-ofdistribution robustness. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2744-2751, Online. Association for Computational Linguistics.</p>
<p>Divyansh Kaushik and Zachary C. Lipton. 2018. How much reading does reading comprehension require? a critical investigation of popular benchmarks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5010-5015, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, and Olivier Bousquet. 2020. Measuring compositional generalization: A comprehensive method on realistic data. In International Conference on Learning Representations.</p>
<p>Tushar Khot, Kyle Richardson, Daniel Khashabi, and Ashish Sabharwal. 2021. Learning to Solve Complex Tasks by Talking to Agents. arXiv preprint arXiv:2110.08542.</p>
<p>Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. 2021. Dynabench: Rethinking benchmarking in nlp.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2017. Adam: A method for stochastic optimization.</p>
<p>Brenden Lake and Marco Baroni. 2018. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International conference on machine learning, pages 2873-2882. PMLR.</p>
<p>Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. 2017. Building machines that learn and think like people. Behavioral and brain sciences, 40.</p>
<p>Hung Le, Truyen Tran, and Svetha Venkatesh. 2020. Self-attentive associative memory. In International Conference on Machine Learning, pages 5682-5691. PMLR.</p>
<p>Belinda Z. Li, Maxwell Nye, and Jacob Andreas. 2021. Implicit representations of meaning in neural language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1813-1827, Online. Association for Computational Linguistics.</p>
<p>Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan</p>
<p>Majumder, and Ming Zhou. 2020. XGLUE: A new benchmark datasetfor cross-lingual pre-training, understanding and generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6008-6018, Online. Association for Computational Linguistics.</p>
<p>Nelson F. Liu, Tony Lee, Robin Jia, and Percy Liang. 2021. Can small and synthetic benchmarks drive modeling innovation? a retrospective study of question answering modeling approaches. Computing Research Repository, arXiv:2102.01065.</p>
<p>Nelson F. Liu, Roy Schwartz, and Noah A. Smith. 2019. Inoculation by fine-tuning: A method for analyzing challenge datasets. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2171-2179, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Ro{bert}a: A robustly optimized {bert} pretraining approach.</p>
<p>Reginald Long, Panupong Pasupat, and Percy Liang. 2016. Simpler context-dependent logical forms via model projections. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1456-1465, Berlin, Germany. Association for Computational Linguistics.</p>
<p>James L. McClelland, Felix Hill, Maja Rudolph, Jason Baldridge, and Hinrich Schütze. 2020. Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models. Proceedings of the National Academy of Sciences, arXiv:1707(Xx):201910416.</p>
<p>Inbar Oren, Jonathan Herzig, and Jonathan Berant. 2021. Finding needles in a haystack: Sampling structurally-diverse training sets from synthetic data for compositional generalization.</p>
<p>Barbara Partee et al. 1995. Lexical semantics and compositionality. An invitation to cognitive science: Language, 1:311-360.</p>
<p>Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. 2021. DynaSent: A dynamic benchmark for sentiment analysis. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2388-2404, Online. Association for Computational Linguistics.</p>
<p>Linlu Qiu, Hexiang Hu, Bowen Zhang, Peter Shaw, and Fei Sha. 2021. Systematic generalization on gscan: What is nearly solved and what is next? Computing Research Repository, arXiv:2109.12243.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Kyle Richardson, Hai Hu, Lawrence Moss, and Ashish Sabharwal. 2020. Probing natural language inference models through semantic fragments. In Proceedings of AAAI.</p>
<p>Kyle Richardson and Ashish Sabharwal. 2022. Pushing the limits of rule reasoning in transformers through natural language satisfiability. Proceedings of AAAI.</p>
<p>Anna Rogers. 2021. Changing the world by changing the data. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2182-2194, Online. Association for Computational Linguistics.</p>
<p>Laura Ruis, Jacob Andreas, Marco Baroni, Diane Bouchacourt, and Brenden M Lake. 2020. A benchmark for systematic generalization in grounded language understanding. In Advances in Neural Information Processing Systems, volume 33, pages 19861-19872. Curran Associates, Inc.</p>
<p>Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. 2021. "everyone wants to do the model work, not the data work": Data cascades in high-stakes ai. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, CHI '21, New York, NY, USA. Association for Computing Machinery.</p>
<p>Imanol Schlag, Tsendsuren Munkhdalai, and Jürgen Schmidhuber. 2021. Learning associative inference using fast weight memory. In International Conference on Learning Representations.</p>
<p>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In International Conference on Learning Representations.</p>
<p>Murray Shanahan, Matthew Crosby, Benjamin Beyret, and Lucy Cheke. 2020. Artificial intelligence and the common sense of animals. Trends in Cognitive Sciences, 24(11):862-872.</p>
<p>Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. 2019. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In Proceedings of the 2019 Conference on</p>
<p>Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4506-4515, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Saku Sugawara, Pontus Stenetorp, and Akiko Aizawa. 2021. Benchmarking machine reading comprehension: A psychological perspective. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1592-1612, Online. Association for Computational Linguistics.</p>
<p>Ronen Tamari, Chen Shani, Tom Hope, Miriam R L Petruck, Omri Abend, and Dafna Shahaf. 2020. Language (re)modelling: Towards embodied language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6268-6281, Online. Association for Computational Linguistics.</p>
<p>Niket Tandon, Keisuke Sakaguchi, Bhavana Dalvi, Dheeraj Rajagopal, Peter Clark, Michal Guerquin, Kyle Richardson, and Eduard Hovy. 2020. A dataset for tracking entities in open domain procedural text. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6408-6417, Online. Association for Computational Linguistics.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, and Tomás Mikolov. 2016. Towards ai-complete question answering: A set of prerequisite toy tasks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System</p>
<p>Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Zhengxuan Wu, Elisa Kreiss, Desmond C. Ong, and Christopher Potts. 2021. ReaSCAN: Compositional reasoning in language grounding. NeurIPS 2021 Datasets and Benchmarks Track.</p>
<p>Hitomi Yanaka, Koji Mineshima, and Kentaro Inui. 2021. SyGNS: A systematic generalization testbed based on natural language semantics. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 103-119, Online. Association for Computational Linguistics.</p>
<p>Rolf A. Zwaan. 2016. Situation models, mental simulations, and abstract concepts in discourse comprehension. Psychonomic Bulletin and Review, 23(4):1028-1034.</p>
<h2>A Appendix</h2>
<h2>A. 1 Extended task construction details</h2>
<p>This section provides further details of the training and test splits used for our experiments.</p>
<p>Table 5 enumerates the basic "building blocks", or concepts underlying the tasks, as presented in §3.2.</p>
<p>Tables 6 and 7 detail the concept sets for each of the sub-tasks comprising the training and test sets, for the $T_{2}, T_{7}$ and $T_{12}$ groups of tasks.</p>
<p>As can be seen from the tables, the main sources of compositionality are:</p>
<ul>
<li>Following the bAbI 1.0 task structure, at training time, all of the more complex linguistic constructs are seen only with MOVE events (and none of the other event types).</li>
<li>Similarly, at training time, yes-no questions are always seen only with MOVE events (and none of the other event types), and with the INDEF or NEGATE linguistic constructs (but not others, such as COREF).</li>
<li>where-was-O questions are never seen in stories with GIVE events.
Language templates. For our new generated tasks we use the same language templates as used in the original bAbI 1.0 benchmark (e.g., the same entity names, verb synonyms). The only modification to the language generation engine was that we completely omit the use of "there"; in the original benchmark, "there" could be used in confusing contexts, as shown in Fig. 5.</li>
</ul>
<h2>A.1.1 Example instances</h2>
<p>Figure 6 shows examples from each of the 4 types of splits used in our experiments. The concat instance is from the original bAbI 1.0 task 5. The inject data contains the same passages as concat, but adds supplementary questions on agent and object locations. diverse instances</p>
<table>
<thead>
<tr>
<th style="text-align: left;">1 Mary journeyed to the bathroom.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">2 Sandra went to the garden.</td>
</tr>
<tr>
<td style="text-align: left;">3 Daniel went back to the garden.</td>
</tr>
<tr>
<td style="text-align: left;">4 Daniel went to the office.</td>
</tr>
<tr>
<td style="text-align: left;">5 Sandra grabbed the milk there.</td>
</tr>
<tr>
<td style="text-align: left;">6 Sandra put down the milk there.</td>
</tr>
<tr>
<td style="text-align: left;">7 Where is the milk? garden 62</td>
</tr>
</tbody>
</table>
<p>Figure 5: Example from original bAbI 1.0 benchmark with confusing usage of "there". In Dyna-bAbI we do not include "there", to avoid this confusion.
contain more diverse support compositions $\left(f_{c}\right)$, but certain combinations are held out. In particular, diverse instances only feature non-default linguistic mappings with MOVE events, never with POSS (GRAB or DROP) or GIVE. In the mix instances, all combinations of support compositions are possible, as shown in the example which features possession (POSS) events along with co-reference.</p>
<h2>A.1.2 Long instances in the bAbI 1.0 tasks</h2>
<p>For the T5 experiments, we used a slightly modified version of the bAbI 1.0 tasks, where we trimmed all training and validation examples that didn't fit into the 512 -token input window. This resulted in trimming 1,585 training instances and 175 validation instances from $T_{7}$ and $T_{1} 2$ (common to both sets). These data points are not consequential as our analysis focuses on the effects of compositionality and not story length; all instances in diverse and mix are substantially shorter than the 512 -token maximum input window size.</p>
<h2>A. 2 Implementation details</h2>
<p>T5. We use the publicly available HuggingFace pre-trained T5-base implementation (Wolf et al., 2020) which has 220M parameters. We similarly use the HuggingFace tokenization pipeline. We fine-tune T5 for 12 epochs on our bAbI data, using the Adam optimizer (Kingma and Ba, 2017), an initial learning rate of $5 * 10^{-5}$ and training batch size of 8 .
STM. We used the official STM implementation ${ }^{11}$, with the only change being a batch size of 32 instead of 128, due to technical constraints.
EntNet. We re-implemented the model in PyTorch, similarly using a batch-size of 32. Following the official Lua reference implementation ${ }^{12}$, we used 20 memory units each with dimension 100. We used the SGD optimizer.</p>
<p>For both the EntNet and STM, we trained models for 200 epochs, and took the best of 10 tries, following Henaff et al. (2017).</p>
<p>For the 20-model concurrence benchmark, refer to Liu et al. (2021) for model details, as we used the same experimental setup.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Events</th>
<th style="text-align: center;">Template</th>
<th style="text-align: center;">Example</th>
<th style="text-align: center;">Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MOVE</td>
<td style="text-align: center;">P {moved} to the L.</td>
<td style="text-align: center;">John traveled to the park.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GRAB</td>
<td style="text-align: center;">P {grabbed} the O.</td>
<td style="text-align: center;">Mary picked up the apple.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DROP</td>
<td style="text-align: center;">P {dropped} the O.</td>
<td style="text-align: center;">Daniel dropped the milk.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GIVE</td>
<td style="text-align: center;">P1 {gave} P2 the O.</td>
<td style="text-align: center;">John handed Mary the apple.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Linguistic <br> Constructs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">COREF</td>
<td style="text-align: center;">P (MOVE</td>
<td style="text-align: center;">GRAB</td>
<td style="text-align: center;">DROP) <br> Following that, ${$ he ${$ <br> (MOVE</td>
</tr>
<tr>
<td style="text-align: center;">CONJ</td>
<td style="text-align: center;">P1 and P2 {moved} to the L1.</td>
<td style="text-align: center;">Jeff and Fred went to the cinema.</td>
<td style="text-align: center;">Conjunction</td>
</tr>
<tr>
<td style="text-align: center;">COMPOUND</td>
<td style="text-align: center;">P1 and P2 {moved} to the L1.</td>
<td style="text-align: center;">Jeff and Fred went to the cinema.</td>
<td style="text-align: center;">Compound co-reference</td>
</tr>
<tr>
<td style="text-align: center;">NEGATE</td>
<td style="text-align: center;">Then they {moved} to the L2.</td>
<td style="text-align: center;">Then they traveled to the school.</td>
<td style="text-align: center;">Negation</td>
</tr>
<tr>
<td style="text-align: center;">INDEF</td>
<td style="text-align: center;">P is not at the L. <br> P is either at the L1 or the L2.</td>
<td style="text-align: center;">Julie is not in the park. <br> John is either in the park or the school.</td>
<td style="text-align: center;">Indefinite expression</td>
</tr>
<tr>
<td style="text-align: center;">Questions</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">where-P</td>
<td style="text-align: center;">Where is P ?</td>
<td style="text-align: center;">Where is John?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">where-O</td>
<td style="text-align: center;">Where is the O ?</td>
<td style="text-align: center;">Where is the football?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">where-was-O</td>
<td style="text-align: center;">Where was the O before the L ?</td>
<td style="text-align: center;">Where was the football before the hallway?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">yes-no</td>
<td style="text-align: center;">Is P at the L?</td>
<td style="text-align: center;">Is John at the park?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">list</td>
<td style="text-align: center;">What is P carrying?</td>
<td style="text-align: center;">What is John carrying?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">counting</td>
<td style="text-align: center;">How many objects is P carrying? <br> Who gave the O to P2?</td>
<td style="text-align: center;">How many objects is John carrying?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Who gave the O ?</td>
<td style="text-align: center;">Who gave the football to John?</td>
<td style="text-align: center;">Constitutes multiple question types over</td>
</tr>
<tr>
<td style="text-align: center;">give-qs</td>
<td style="text-align: center;">Who received the O ? <br> Who did P1 give the P2 to? <br> What did P1 give to P2?</td>
<td style="text-align: center;">..</td>
<td style="text-align: center;">GIVE events.</td>
</tr>
</tbody>
</table>
<p>Table 5: Details of the events, linguistic constructs and questions constituting the bAbI tasks covered in this work. Words in {brackets} are drawn from a small set of synonyms.</p>
<p>| concat(T12) + inject(T12) <br> 1 Bill travelled to the office. <br> 2 Bill picked up the football there. <br> 3 Bill went to the bedroom. <br> 4 Bill gave the football to Fred. <br> 5 What did Bill give to Fred? football (4) <br> 6 Where is the football? bedroom (3, 4) <br> 7 Where is Bill? bedroom (3) <br> 8 Where is Fred? bedroom (3, 4) | diverse(T12) <br> 1 Fred went back to the garden. <br> 2 Sandra travelled to the cinema. <br> 3 Fred went to the bathroom. <br> 4 Fred got the football. <br> 5 Fred travelled to the garden. <br> 6 Bill journeyed to the garden. <br> 7 Fred passed the football to Bill. <br> 8 Bill discarded the football. <br> 9 Jeff got the football. <br> 10 Jeff discarded the football. <br> 11 Sandra journeyed to the office. <br> 12 Fred journeyed to the kitchen. <br> 13 Bill got the football. <br> 14 Bill travelled to the office. <br> 15 Bill passed the football to Julie. <br> 16 Julie passed the football to Daniel. <br> 17 Daniel left the football. <br> 18 Mary journeyed to the bedroom. <br> 19 Bill picked up the football. <br> 20 Bill left the football. <br> 21 Where is Jeff? garden $\mathrm{f}={6,8,9}$ <br> $\mathrm{f} _\mathrm{c}={$ MOVE, POSS $}$ | mix(T12) <br> 1 John is no longer in the bedroom. <br> 2 Bill is in the bedroom. <br> 3 Bill took the apple. <br> 4 Afterwards he discarded the apple. <br> 5 Bill is no longer in the bedroom. <br> 6 Daniel is either in the kitchen or the bathroom. <br> 7 Fred and Bill journeyed to the kitchen. <br> 8 Jeff is either in the park or the office. <br> 9 Daniel is either in the garden or the kitchen. <br> 10 Sandra is in the school. <br> 11 Bill is either in the bathroom or the school. <br> 12 Mary is not in the office. <br> 13 Sandra journeyed to the hallway. <br> 14 After that she grabbed the milk. <br> 15 Julie is either in the bedroom or the office. <br> 16 Daniel is no longer in the garden. <br> 17 Jeff moved to the bathroom. <br> 18 Julie picked up the apple. <br> 19 Following that she got the football. <br> 20 Jeff is in the hallway. <br> 21 Where is the football? bedroom $\mathrm{f}={2,3,4,18,19}$ <br> $\mathrm{f} _\mathrm{c}={$ MOVE, POSS, COREF $}$ |</p>
<p>Figure 6: Example instances from each of the 4 types of splits used in our experiments.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Events</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Linguistic Constructs</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Questions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Sub-task</td>
<td style="text-align: center;">Type</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I/D</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I/D</td>
<td style="text-align: center;">I/D</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\operatorname{mix}\left(T_{2}\right)$</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\operatorname{mix}\left(T_{7}\right)$</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 6: Concept sets for the $T_{2}$ and $T_{7}$ sub-set of the original bAbI tasks, and the new tasks generated with Dyna-bAbI. Train sub-task numbering follows the original bAbI numbering. The inject and diverse tasks inherit the same concept set from the original tasks, and additionally " I ", "D" denote question types included only in the inject or diverse tasks, respectively. "I/D" denotes question types included in both.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Events</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Linguistic Constructs</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Questions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Task</td>
<td style="text-align: center;">Type</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I/D</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I/D</td>
<td style="text-align: center;">I/D</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I/D</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I/D</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">I/D</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\operatorname{mix}\left(T_{12}\right)$</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 7: Concept sets for the $T_{12}$ sub-set of the original bAbI tasks, and the new tasks generated with Dyna-bAbI. Train sub-task numbering follows the original bAbI numbering. The inject and diverse tasks inherit the same concept set from the original tasks, and additionally "I", "D" denote question types included only in the inject or diverse tasks, respectively. "I/D" denotes question types included in both.</p>
<p>For the T5 experiments, we used the PyTorch Lightning (Falcon et al., 2019) trainer implementation, and Weights \&amp; Biases (Biewald, 2020) for experiment tracking and artifacts management.</p>
<p>We used standard hyper-parameter settings for all models, with slight changes in the case of memory issues as described above.
Experimental infrastructure details. Our experiments were performed using an RTX-8000 GPU, with a total computational budget of roughly 1,000 GPU hours.</p>
<h2>A. 3 Inoculation experiment results</h2>
<p>To rule out the hypothesis that certain patterns may be too hard for models to learn, we follow the inoculation methodology presented in Liu et al. (2019): after training on the original tasks, we finetune the T5 on small amounts of OOD data (disjoint from the test data), and evaluate performance as a function of "inoculation dose". As can be seen in Fig. 7, we find that performance quickly (with only 500 additional inoculation samples per question type) reaches over $90 \%$ accuracy on both the $\operatorname{mix}\left(T_{7}\right)$ and $\operatorname{mix}\left(T_{12}\right)$ challenge sets. These results support the hypothesis that the training data is not rich enough, indicating clearly that the model is capable of quickly learning to solve the challenge tasks, given exposure to training samples with similar enough patterns.</p>
<h2>A. 4 Concurrence experiments</h2>
<p>Table 8 presents the full results for the concurrence experiments of $\S 4.1$. SQuAD and bAbI task 2 results are reproduced from Liu et al. (2021), see there also for implementation details of the models used.</p>
<h2>A. 5 Extended error analysis: GIVE events</h2>
<p>We analyze the performance of models on the $\operatorname{mix}\left(T_{7}\right)$ split after being trained on concat $\left(T_{7}\right)$, and in particular we focus on GIVE events. As noted in $\S 4.2$, compositions involving GIVE are intuitively challenging as they entail multiple inferences which are not explicit in the text: the actors share the same location, and the possession of the object being given is transferred from the giver to the recipient. The only task in concat $\left(T_{7}\right)$ featuring GIVE events is task 5, which never asks about the locations of actors or objects, but only about the participant roles in the event (e.g., who was the giver or recipient; see Fig. 1 example from task 5).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Evaluation accuracy</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">SQuAD</td>
<td style="text-align: left;">$\operatorname{mix}(\mathrm{T} 2)$</td>
<td style="text-align: left;">$\operatorname{mix}(\mathrm{T} 7)$</td>
<td style="text-align: left;">babi task 2</td>
</tr>
<tr>
<td style="text-align: left;">rasor</td>
<td style="text-align: left;">64.86</td>
<td style="text-align: left;">88.20</td>
<td style="text-align: left;">35.03</td>
<td style="text-align: left;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">bidaf</td>
<td style="text-align: left;">67.39</td>
<td style="text-align: left;">97.20</td>
<td style="text-align: left;">30.50</td>
<td style="text-align: left;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">documentreader</td>
<td style="text-align: left;">69.66</td>
<td style="text-align: left;">90.20</td>
<td style="text-align: left;">40.70</td>
<td style="text-align: left;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">documentreader</td>
<td style="text-align: left;">69.21</td>
<td style="text-align: left;">82.50</td>
<td style="text-align: left;">37.17</td>
<td style="text-align: left;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">(no_features)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">bidafplusplus</td>
<td style="text-align: left;">69.49</td>
<td style="text-align: left;">99.50</td>
<td style="text-align: left;">44.20</td>
<td style="text-align: left;">80.70</td>
</tr>
<tr>
<td style="text-align: left;">mnemonicreader</td>
<td style="text-align: left;">73.02</td>
<td style="text-align: left;">98.20</td>
<td style="text-align: left;">39.63</td>
<td style="text-align: left;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">mnemonicreader</td>
<td style="text-align: left;">72.67</td>
<td style="text-align: left;">97.50</td>
<td style="text-align: left;">38.20</td>
<td style="text-align: left;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">(no_features)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">qanet</td>
<td style="text-align: left;">72.41</td>
<td style="text-align: left;">67.70</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">fusionnet</td>
<td style="text-align: left;">72.90</td>
<td style="text-align: left;">99.50</td>
<td style="text-align: left;">39.73</td>
<td style="text-align: left;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">fusionnet</td>
<td style="text-align: left;">72.24</td>
<td style="text-align: left;">88.10</td>
<td style="text-align: left;">37.80</td>
<td style="text-align: left;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">(no_features)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">bert</td>
<td style="text-align: left;">81.46</td>
<td style="text-align: left;">95.50</td>
<td style="text-align: left;">47.63</td>
<td style="text-align: left;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">bert_large</td>
<td style="text-align: left;">84.17</td>
<td style="text-align: left;">98.30</td>
<td style="text-align: left;">59.10</td>
<td style="text-align: left;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">bert_large_wwm</td>
<td style="text-align: left;">87.33</td>
<td style="text-align: left;">98.70</td>
<td style="text-align: left;">67.63</td>
<td style="text-align: left;">99.90</td>
</tr>
<tr>
<td style="text-align: left;">albert</td>
<td style="text-align: left;">81.86</td>
<td style="text-align: left;">98.20</td>
<td style="text-align: left;">56.70</td>
<td style="text-align: left;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">albert_xxlarge</td>
<td style="text-align: left;">89.07</td>
<td style="text-align: left;">99.80</td>
<td style="text-align: left;">80.00</td>
<td style="text-align: left;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">roberta</td>
<td style="text-align: left;">83.37</td>
<td style="text-align: left;">98.70</td>
<td style="text-align: left;">57.70</td>
<td style="text-align: left;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">roberta_large</td>
<td style="text-align: left;">86.96</td>
<td style="text-align: left;">99.80</td>
<td style="text-align: left;">64.07</td>
<td style="text-align: left;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">electra</td>
<td style="text-align: left;">85.88</td>
<td style="text-align: left;">98.70</td>
<td style="text-align: left;">53.47</td>
<td style="text-align: left;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">spanbert</td>
<td style="text-align: left;">86.20</td>
<td style="text-align: left;">98.40</td>
<td style="text-align: left;">55.70</td>
<td style="text-align: left;">99.50</td>
</tr>
<tr>
<td style="text-align: left;">spanbert_large</td>
<td style="text-align: left;">88.74</td>
<td style="text-align: left;">98.60</td>
<td style="text-align: left;">62.27</td>
<td style="text-align: left;">95.40</td>
</tr>
</tbody>
</table>
<p>Table 8: Full results of concurrence experiments presented in $\S 4.1$.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 7: Inoculation experiment results.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Num. <br> supporting facts</th>
<th style="text-align: left;">Num. <br> samples</th>
<th style="text-align: left;">Evaluation accuracy</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">BiDAF</td>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: left;">T5</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">334</td>
<td style="text-align: left;">53.3</td>
<td style="text-align: left;">93.4</td>
<td style="text-align: left;">86.8</td>
</tr>
<tr>
<td style="text-align: left;">2 (w/o GIVE)</td>
<td style="text-align: left;">734</td>
<td style="text-align: left;">51.50</td>
<td style="text-align: left;">82.3</td>
<td style="text-align: left;">71.8</td>
</tr>
<tr>
<td style="text-align: left;">2 (with GIVE)</td>
<td style="text-align: left;">99</td>
<td style="text-align: left;">3.03</td>
<td style="text-align: left;">7.07</td>
<td style="text-align: left;">5.05</td>
</tr>
<tr>
<td style="text-align: left;">3 (w/o GIVE)</td>
<td style="text-align: left;">1365</td>
<td style="text-align: left;">24.6</td>
<td style="text-align: left;">47.2</td>
<td style="text-align: left;">44.3</td>
</tr>
<tr>
<td style="text-align: left;">3 (with GIVE)</td>
<td style="text-align: left;">468</td>
<td style="text-align: left;">4.27</td>
<td style="text-align: left;">7.05</td>
<td style="text-align: left;">15.2</td>
</tr>
</tbody>
</table>
<p>Table 9: Breakdown of model performance on $\operatorname{mix}\left(T_{7}\right)$ for questions including (or not) GIVE events in the supporting fact set. The poor performance on questions including GIVE indicates that training on the bAbI 1.0 data does not facilitate generalization to novel compositions of GIVE.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">where- $P(\rightarrow)$ <br> yes-no $(\downarrow)$</th>
<th style="text-align: left;">correct</th>
<th style="text-align: left;">incorrect</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">correct</td>
<td style="text-align: left;">209</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;">incorrect</td>
<td style="text-align: left;">145</td>
<td style="text-align: left;">88</td>
</tr>
</tbody>
</table>
<p>Table 10: Confusion matrix displaying question phrasing sensitivities in T5. We pose a question in two formats: (1) yes-no: "Is $X$ at $L$ ? yes" vs (2) where-P: "Where is $X$ ? $L$ ". We find performance is considerably higher for questions posed in the where- $P$ format, indicating the model isn't learning the equivalence of both forms.</p>
<p>To measure this intuition empirically, we analyze a subset of 567 questions including GIVE events in the supporting facts set. As shown in Table 9, performance for all models on questions including GIVE is extremely low, far below performance for questions without it. Qualitative analysis indicates many failure cases follow the pattern shown in the right-side example of Fig. 1c, question on line 10: the location of an entity (e.g., Daniel) must be inferred via the known (co-)location of a second participant in the GIVE event (e.g., Jeff). These results strengthen the hypothesis that standard QA training on the original bAbI data does not drive strong event comprehension in models.</p>
<h2>A. 6 Extended error analysis: question phrasing sensitivity</h2>
<p>This section presents further empirical analysis of the question phrasing sensitivities discussed in $\S 4.2 .1$, relating to the performance of the T5 model trained on the diverse $\left(T_{12}\right)$ data and evaluated on the challenge set $\operatorname{mix}\left(T_{12}\right)$.</p>
<p>We collected all yes-no questions from $\operatorname{mix}\left(T_{12}\right)$ for which the answer was "yes", yielding 446 questions in total. For each such (question, answer) pair, of the form ("Is person at the location?", "yes"), we created an equivalent pair in the format of a where- $P$ question, ("Where is person?", location). Figure 4 shows a characteristic example. Ideally, we would expect a model to be agnostic to equivalent phrasings of a question. However, as displayed in Table 10, we find that T5 is considerably more accurate for questions posed in the where- $P$ format, likely due to exposure to a larger variety of such questions at training time.</p>
<p>1 Bill grabbed the milk.
2 Bill put down the milk.
3 John is either in the bedroom or the kitchen.
4 Fred journeyed to the kitchen.
5 John grabbed the football.
6 Following that he put down the football.
7 Bill picked up the milk.
8 Following that he went to the bedroom.
9 Bill is in the office.
10 Bill is in the cinema.
11 Bill passed the milk to Julie.
12 Julie handed the milk to Bill.
13 Jeff is not in the school.
14 John took the football.
15 Fred and Jeff moved to the school.
16 Afterwards they journeyed to the bathroom.
17 Bill handed the milk to Julie.
18 John dropped the football.
19 Daniel is either in the school or the bedroom.
20 Daniel took the football.
21 Is John in the bedroom? yes 3181920</p>
<p>Figure 8: Double disjunction example from $\operatorname{mix}\left(T_{12}\right)$.</p>
<h2>A. 7 Extended error analysis: double disjunctions</h2>
<p>As the shown in the $\S 4.2 .1$ error analysis, a particularly difficult class of questions are double disjunctions over indefinite expressions. Figure 8 displays a typical example from $\operatorname{mix}\left(T_{12}\right)$, where the locations of two actors are given in indefinite form (sentences 3 and 19), and are also known to be co-located, since they share the location of the object "football", as inferred from sentences 18 and 20. Hence it is possible to infer their location as the intersection of the two indefinite expressions (here "bedroom"). Rather than answering "yes" to the question "Is John in the bedroom?", T5 invariably answers "maybe" for such cases. This pattern is likely due to the fact that in the training data "maybe" is a typical answer for yes-no questions about actors mentioned by indefinite expressions (task 10 in bAbI 1.0).</p>
<h2>B Datasheet for datasets</h2>
<h2>Motivation</h2>
<p>For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.
Few synthetic resources for probing NLP models' performance on discourse-level narrative understanding texts. Existing resources lack customizability (control over data created + amenable to extension).
Who created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?
Joint team of researchers at Hebrew University of Jerusalem (Israel) and the Allen Institute for Artifical Intelligence.
Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.
Work was supported by the Center for Interdisciplinary Data-science Research (CIDR) at HUJI. This work was also supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant no. 852686, SIAM) and NSF-BSF grant no. 2017741 (Shahaf). Part of this research is also supported by the European Research Council, ERC-StG grant no. 677352 (Tsarfaty).</p>
<p>Any other comments?</p>
<h2>Composition</h2>
<p>What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description. Instances represent variable length stories.
How many instances are there in total (of each type, if appropriate)?
Any size dataset can be created (programmatic generation).
Does the dataset contain all possible instances or is it a sample (not necessarily
random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).
Used rejection sampling for some datasets to cover more diverse instances.
What data does each instance consist of? "Raw" data (e.g., unprocessed text or images) or features? In either case, please provide a description.
Simple textual stories generated using templates ("John went to the kitchen. He grabbed the apple.").</p>
<p>Is there a label or target associated with each instance? If so, please provide a description.
Each instance is accompanied by a (question, answer) pair, both in natural language.
Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.
N/A
Are there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them. The data is organized in splits, which are explained in section 4 of the paper.
Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.
Template based language generation may result in somewhat unnatural texts.
Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset</p>
<p>(i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.
Self contained.
Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why. No.</p>
<p>Does the dataset relate to people? If not, you may skip the remaining questions in this section.
No.
Any other comments?</p>
<h2>Collection Process</h2>
<p>How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, modelbased guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.
Programmatically generated using logical rules and templates.
If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?
Rejection sampling was used in some cases, described in Section 4.
Does the dataset relate to people? If not, you may skip the remaining questions in this section.
No.
Any other comments?</p>
<h2>Preprocessing/cleaning/labeling</h2>
<p>Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remainder of the questions in this section.
No.
Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the "raw" data.
N/A
Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point.
N/A
Any other comments?</p>
<h2>Uses</h2>
<p>Has the dataset been used for any tasks already? If so, please provide a description.
Benchmark to guide model development for reading comprehension and textual reasoning tasks.</p>
<p>Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.
Not currently, we will use the https:// paperswithcode.com/ integration to track results.</p>
<p>What (other) tasks could the dataset be used for?
N/A
Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms,</p>
<p>legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms?
N/A
Are there tasks for which the dataset should not be used? If so, please provide a description.
Similar to the original bAbI benchmark, our tasks are not a substitute for real natural language datasets, but should rather complement them. Even if a method works well on our data, it should be shown to perform well on real data as well. Rather, our tasks are better thought of as comprehension "unit-tests", where poor performance on our tasks serves as a warning sign suggesting the model may exhibit limited systematicity and robustness on more difficult, naturalistic inputs.</p>
<p>Any other comments?</p>
<h2>Distribution</h2>
<p>Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.
N/A
How will the dataset will be distributed (e.g., tarball on website, API, GitHub) Does the dataset have a digital object identifier (DOI)? Github + Weights and Biases. No DOI currently.</p>
<h2>When will the dataset be distributed?</h2>
<p>Data and code-base for task generation to be uploaded upon publication.</p>
<p>Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.
Will be available with standard MIT license.
Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any
relevant licensing terms, as well as any fees associated with these restrictions.
N/A
Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.
N/A
Any other comments?</p>
<h2>Maintenance</h2>
<p>Who will be
supporting/hosting/maintaining the
dataset?
Corresponding author of paper.
How can the owner/curator/manager of the dataset be contacted (e.g., email address)?
Via email with corresponding author, and through dedicated GitHub repository.</p>
<p>Is there an erratum? If so, please provide a link or other access point.
N/A
Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)?
Extensions will be maintained via GitHub.
If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.
N/A
Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to users.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ https://github.com/thaihungle/SAM
${ }^{12}$ https://github.com/facebookarchive/ MemNN/tree/master/EntNet-babi&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{8}$ See appendix A. 4 for full numeric results.
${ }^{9}$ An alternate hypothesis is that certain patterns may be too hard for models to learn; we confirm this is not the case by using the inoculation methodology of Liu et al. (2019), see details in Appendix A.3.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
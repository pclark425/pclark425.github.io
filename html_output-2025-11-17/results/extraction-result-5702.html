<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5702 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5702</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5702</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-c9b6d0db6c2bebe0bcf593d95bea0e62b2443ef3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c9b6d0db6c2bebe0bcf593d95bea0e62b2443ef3" target="_blank">The Effect of Sampling Temperature on Problem Solving in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> Empirical results indicate that changes in temperature from 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks, and these results appear to generalize across LLMs, prompt-engineering techniques, and problem domains.</p>
                <p><strong>Paper Abstract:</strong> In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used nine popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.6. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature from 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to generalize across LLMs, prompt-engineering techniques, and problem domains. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5702.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5702.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-engineering technique that instructs the LLM to 'think step-by-step' and produce intermediate reasoning before giving a final answer; used here as a system prompt and one-shot example format.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo; also evaluated across multiple LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MCQA exams (100-question small exam; 1,000-question large exam)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice question-and-answer problems sampled from standard LLM benchmarks across domains (ARC, AQUA-RAT, Hellaswag, LogiQA, LSAT variants, MedMCQA, SAT-English, SAT-Math).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Single-request single-response one-shot in-context prompt where the system instructs the model to recite relevant knowledge, think step-by-step (CoT), self-critically evaluate, and finally output an answer in the enforced format (e.g., 'Action: Answer("[choice]")'); the one-shot example included a CoT solution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against Baseline (no engineering), Domain Expert, Self-recitation, and Composite prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy remained stable across sampling temperatures 0.0–1.0 (no significant change); for GPT-3.5 CoT on 1,000-question exam Kruskal-Wallis H(10)=10.439, p=0.403 (no significant temperature effect).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT outperformed the other four prompt types in absolute accuracy on the 100-question exam (no absolute accuracy percentages reported). Kruskal-Wallis test by prompt for GPT-3.5: CoT H=2.042, p=0.996 (temperature had no significant effect within CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (CoT > other prompt formats in accuracy), but temperature within 0.0–1.0 had no effect</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>CoT encourages procedural reasoning and yields higher accuracy on constrained MCQA tasks; however, the constrained nature of MCQA may limit the effect of sampling temperature, so reasoning improvements from format do not interact with temperature in the 0.0–1.0 range.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Effect of Sampling Temperature on Problem Solving in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5702.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5702.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline (no prompt engineering) prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A minimal prompt instructing the model to return only a single multiple-choice answer (e.g., 'Answer("C")') without additional reasoning or domain framing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MCQA exams (100-question small exam)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same set of multiple-choice problems sampled across ten benchmark problem sets.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>One-shot in-context example included; system prompt only requests a single formatted answer without recitation or reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to Domain Expert, Self-recitation, Chain-of-Thought, and Composite prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy across temperatures 0.0–1.0 remained stable (no significant effect of temperature). Kruskal-Wallis for Baseline: H=0.420, p=1.000 (GPT-3.5 on 100-question exam).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to Chain-of-Thought prompting (qualitative statement; exact numeric accuracies not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (relative to CoT); temperature had no effect</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Lack of reasoning scaffolding reduces problem-solving performance compared to CoT; however sampling temperature in 0.0–1.0 does not materially change Baseline performance on MCQA.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Effect of Sampling Temperature on Problem Solving in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5702.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5702.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain Expert</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Expertise prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system prompt framing the LLM as an expert in the problem domain (e.g., 'medicine' or 'anatomy') to bias responses toward domain-specific reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MCQA exams (100-question small exam)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice benchmarks across domains including MedMCQA and others.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>One-shot prompt where the system specifies expertise in the exam domain; includes a single example adapted for the domain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to Baseline, Self-recitation, Chain-of-Thought, and Composite prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy remained stable across temperatures 0.0–1.0; Kruskal-Wallis H=0.548, p=1.000 (GPT-3.5 on 100-question exam), indicating no significant temperature effect.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Domain Expert did not outperform CoT; CoT remained best-performing prompt overall (no absolute accuracy values provided).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>no effect (temperature) ; moderate improvement vs baseline possible but less than CoT</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Domain framing can provide helpful priors but does not change temperature sensitivity; prompt content helps quality of reasoning but temperature in 0.0–1.0 does not alter accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Effect of Sampling Temperature on Problem Solving in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5702.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5702.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-recitation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-recitation prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system prompt instructing the model to recite its internal knowledge about the question and each option before answering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MCQA exams (100-question small exam)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice QA sampled from established benchmarks across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>One-shot prompt where the system instructs the LLM to explicitly recite relevant knowledge before producing an answer; a one-shot example included a recitation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against Baseline, Domain Expert, Chain-of-Thought, and Composite prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy stable across 0.0–1.0 temperatures; Kruskal-Wallis H=1.403, p=0.999 (GPT-3.5 on 100-question exam).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to Chain-of-Thought prompting (CoT > Self-recitation qualitatively); specific accuracy numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>no effect (temperature) ; modestly worse than CoT</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Recitation may make more of the model's internal knowledge explicit but does not interact with sampling temperature for MCQA problems within 0.0–1.0.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Effect of Sampling Temperature on Problem Solving in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5702.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5702.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Composite Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Composite prompt (Domain + Recitation + CoT + self-criticism)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A combined system prompt that layers domain expertise, self-recitation, chain-of-thought reasoning, and self-criticism in a single one-shot example.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MCQA exams (100-question small exam)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice benchmarks sampling across several domains and difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Single request-response one-shot prompt combining domain framing, recitation of knowledge, step-by-step chain-of-thought, and explicit self-critique; includes an example demonstrating this combined behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to Baseline, Domain Expert, Self-recitation, and Chain-of-Thought prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy stable across temperatures 0.0–1.0; Kruskal-Wallis H=1.000, p=1.000 (GPT-3.5 on 100-question exam).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Composite prompt did not outperform Chain-of-Thought alone; CoT remained top-performing. No absolute accuracy numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>no effect (temperature) ; no improvement over CoT in this experimental setup</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Combining techniques did not yield additional gain over CoT for constrained MCQA tasks in this single-turn, one-shot setup; sampling temperature 0.0–1.0 did not change outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Effect of Sampling Temperature on Problem Solving in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5702.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5702.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>One-shot in-context format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-shot in-context learning with example problem-and-solution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing a single example (problem and solution) in the prompt to guide the model's response style and format; adapted per prompt technique (e.g., CoT example for CoT prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo (used consistently for prompt comparisons); other LLMs used with CoT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MCQA exams (100- and 1,000-question exams)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice QA from ten benchmark problem sets.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>One-shot example appended to system prompt; example included the reasoning style requested (e.g., chain-of-thought) and the final required answer formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Used in all prompt conditions; no explicit quantitative comparison reported between one-shot and zero-shot because study constrained to one-shot designs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The one-shot example aligns the model's output style (including CoT), likely improving consistency of the requested reasoning format, but the study did not compare to zero-shot or multi-shot variants.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Effect of Sampling Temperature on Problem Solving in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5702.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5702.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Answer-format enforcement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Strict output formatting requirement (e.g., 'Action: Answer("[choice]")')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strict requirement in the system prompt that the model must reply using an exact single-line answer format; used to make evaluation deterministic but caused formatting errors in some models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2 7B (noted), evaluated across multiple LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B for Llama 2 7B; other model sizes not specified</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MCQA exams (100-question small exam)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice QA drawn from ten benchmark problem sets.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>System prompt mandated final output exactly in the pattern 'Action: Answer("<choice>")'; responses evaluated for correctness only if properly formatted.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Llama 2 7B performed poorly (not statistically better than random). Failure modes: 39% of outputs incorrectly formatted, 36% correctly formatted but incorrect answers — these formatting failures substantially lowered measured accuracy for that model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (format enforcement caused apparent performance degradation for models that frequently violated the format)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Strict answer-format enforcement can artifactually reduce measured accuracy if models produce valid answers in a different surface form; Llama 2 7B's high rate of misformatted answers illustrates how presentation/formatting constraints can affect evaluation results independent of underlying problem-solving capability.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Effect of Sampling Temperature on Problem Solving in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5702.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5702.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-variability vs Temperature</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text similarity / variability analysis (TF-IDF, Jaccard, BLEU, SBERT, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Measurement of how response text similarity changes with sampling temperature using multiple metrics; used to link temperature to output diversity and coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo (primary analysis); similar trends reported across other LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MCQA exams (100-question small exam; extended sweep to 1.6 for GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice question answering; textual answers analyzed for similarity across repeated attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same prompt formats (CoT in main analysis); repeated-answer sampling at each temperature to compute similarity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Text similarity decreased (i.e., variability increased) as temperature increased; similarity metrics drop rapidly after temperature >1.0, coinciding with rapid accuracy degradation observed beyond τ=1.0.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>temperature increases -> increased variability (no prompt-format interaction reported)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Higher sampling temperature increases randomness/diversity of outputs as expected from softmax temperature theory; in the constrained MCQA setting, this variability did not produce improved accuracy up to τ=1.0 but led to incoherence and falling accuracy beyond τ≈1.0.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Effect of Sampling Temperature on Problem Solving in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Recitation-augmented language models <em>(Rating: 2)</em></li>
                <li>Self-critique prompting with large language models for inductive instructions <em>(Rating: 1)</em></li>
                <li>Improving code generation by dynamic temperature sampling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5702",
    "paper_id": "paper-c9b6d0db6c2bebe0bcf593d95bea0e62b2443ef3",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompt-engineering technique that instructs the LLM to 'think step-by-step' and produce intermediate reasoning before giving a final answer; used here as a system prompt and one-shot example format.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo; also evaluated across multiple LLMs",
            "model_size": null,
            "task_name": "MCQA exams (100-question small exam; 1,000-question large exam)",
            "task_description": "Multiple-choice question-and-answer problems sampled from standard LLM benchmarks across domains (ARC, AQUA-RAT, Hellaswag, LogiQA, LSAT variants, MedMCQA, SAT-English, SAT-Math).",
            "problem_format": "Single-request single-response one-shot in-context prompt where the system instructs the model to recite relevant knowledge, think step-by-step (CoT), self-critically evaluate, and finally output an answer in the enforced format (e.g., 'Action: Answer(\"[choice]\")'); the one-shot example included a CoT solution.",
            "comparison_format": "Compared against Baseline (no engineering), Domain Expert, Self-recitation, and Composite prompts.",
            "performance": "Accuracy remained stable across sampling temperatures 0.0–1.0 (no significant change); for GPT-3.5 CoT on 1,000-question exam Kruskal-Wallis H(10)=10.439, p=0.403 (no significant temperature effect).",
            "performance_comparison": "CoT outperformed the other four prompt types in absolute accuracy on the 100-question exam (no absolute accuracy percentages reported). Kruskal-Wallis test by prompt for GPT-3.5: CoT H=2.042, p=0.996 (temperature had no significant effect within CoT).",
            "format_effect_size": null,
            "format_effect_direction": "improved (CoT &gt; other prompt formats in accuracy), but temperature within 0.0–1.0 had no effect",
            "explanation_or_hypothesis": "CoT encourages procedural reasoning and yields higher accuracy on constrained MCQA tasks; however, the constrained nature of MCQA may limit the effect of sampling temperature, so reasoning improvements from format do not interact with temperature in the 0.0–1.0 range.",
            "counterexample_or_null_result": null,
            "uuid": "e5702.0",
            "source_info": {
                "paper_title": "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Baseline Prompt",
            "name_full": "Baseline (no prompt engineering) prompt",
            "brief_description": "A minimal prompt instructing the model to return only a single multiple-choice answer (e.g., 'Answer(\"C\")') without additional reasoning or domain framing.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo",
            "model_size": null,
            "task_name": "MCQA exams (100-question small exam)",
            "task_description": "Same set of multiple-choice problems sampled across ten benchmark problem sets.",
            "problem_format": "One-shot in-context example included; system prompt only requests a single formatted answer without recitation or reasoning steps.",
            "comparison_format": "Compared to Domain Expert, Self-recitation, Chain-of-Thought, and Composite prompts.",
            "performance": "Accuracy across temperatures 0.0–1.0 remained stable (no significant effect of temperature). Kruskal-Wallis for Baseline: H=0.420, p=1.000 (GPT-3.5 on 100-question exam).",
            "performance_comparison": "Underperformed relative to Chain-of-Thought prompting (qualitative statement; exact numeric accuracies not reported).",
            "format_effect_size": null,
            "format_effect_direction": "reduced (relative to CoT); temperature had no effect",
            "explanation_or_hypothesis": "Lack of reasoning scaffolding reduces problem-solving performance compared to CoT; however sampling temperature in 0.0–1.0 does not materially change Baseline performance on MCQA.",
            "counterexample_or_null_result": null,
            "uuid": "e5702.1",
            "source_info": {
                "paper_title": "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Domain Expert",
            "name_full": "Domain Expertise prompt",
            "brief_description": "A system prompt framing the LLM as an expert in the problem domain (e.g., 'medicine' or 'anatomy') to bias responses toward domain-specific reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo",
            "model_size": null,
            "task_name": "MCQA exams (100-question small exam)",
            "task_description": "Multiple-choice benchmarks across domains including MedMCQA and others.",
            "problem_format": "One-shot prompt where the system specifies expertise in the exam domain; includes a single example adapted for the domain.",
            "comparison_format": "Compared to Baseline, Self-recitation, Chain-of-Thought, and Composite prompts.",
            "performance": "Accuracy remained stable across temperatures 0.0–1.0; Kruskal-Wallis H=0.548, p=1.000 (GPT-3.5 on 100-question exam), indicating no significant temperature effect.",
            "performance_comparison": "Domain Expert did not outperform CoT; CoT remained best-performing prompt overall (no absolute accuracy values provided).",
            "format_effect_size": null,
            "format_effect_direction": "no effect (temperature) ; moderate improvement vs baseline possible but less than CoT",
            "explanation_or_hypothesis": "Domain framing can provide helpful priors but does not change temperature sensitivity; prompt content helps quality of reasoning but temperature in 0.0–1.0 does not alter accuracy.",
            "counterexample_or_null_result": null,
            "uuid": "e5702.2",
            "source_info": {
                "paper_title": "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Self-recitation",
            "name_full": "Self-recitation prompt",
            "brief_description": "A system prompt instructing the model to recite its internal knowledge about the question and each option before answering.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo",
            "model_size": null,
            "task_name": "MCQA exams (100-question small exam)",
            "task_description": "Multiple-choice QA sampled from established benchmarks across domains.",
            "problem_format": "One-shot prompt where the system instructs the LLM to explicitly recite relevant knowledge before producing an answer; a one-shot example included a recitation.",
            "comparison_format": "Compared against Baseline, Domain Expert, Chain-of-Thought, and Composite prompts.",
            "performance": "Accuracy stable across 0.0–1.0 temperatures; Kruskal-Wallis H=1.403, p=0.999 (GPT-3.5 on 100-question exam).",
            "performance_comparison": "Underperformed relative to Chain-of-Thought prompting (CoT &gt; Self-recitation qualitatively); specific accuracy numbers not provided.",
            "format_effect_size": null,
            "format_effect_direction": "no effect (temperature) ; modestly worse than CoT",
            "explanation_or_hypothesis": "Recitation may make more of the model's internal knowledge explicit but does not interact with sampling temperature for MCQA problems within 0.0–1.0.",
            "counterexample_or_null_result": null,
            "uuid": "e5702.3",
            "source_info": {
                "paper_title": "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Composite Prompt",
            "name_full": "Composite prompt (Domain + Recitation + CoT + self-criticism)",
            "brief_description": "A combined system prompt that layers domain expertise, self-recitation, chain-of-thought reasoning, and self-criticism in a single one-shot example.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo",
            "model_size": null,
            "task_name": "MCQA exams (100-question small exam)",
            "task_description": "Multiple-choice benchmarks sampling across several domains and difficulty levels.",
            "problem_format": "Single request-response one-shot prompt combining domain framing, recitation of knowledge, step-by-step chain-of-thought, and explicit self-critique; includes an example demonstrating this combined behavior.",
            "comparison_format": "Compared to Baseline, Domain Expert, Self-recitation, and Chain-of-Thought prompts.",
            "performance": "Accuracy stable across temperatures 0.0–1.0; Kruskal-Wallis H=1.000, p=1.000 (GPT-3.5 on 100-question exam).",
            "performance_comparison": "Composite prompt did not outperform Chain-of-Thought alone; CoT remained top-performing. No absolute accuracy numbers reported.",
            "format_effect_size": null,
            "format_effect_direction": "no effect (temperature) ; no improvement over CoT in this experimental setup",
            "explanation_or_hypothesis": "Combining techniques did not yield additional gain over CoT for constrained MCQA tasks in this single-turn, one-shot setup; sampling temperature 0.0–1.0 did not change outcomes.",
            "counterexample_or_null_result": null,
            "uuid": "e5702.4",
            "source_info": {
                "paper_title": "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "One-shot in-context format",
            "name_full": "One-shot in-context learning with example problem-and-solution",
            "brief_description": "Providing a single example (problem and solution) in the prompt to guide the model's response style and format; adapted per prompt technique (e.g., CoT example for CoT prompt).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo (used consistently for prompt comparisons); other LLMs used with CoT",
            "model_size": null,
            "task_name": "MCQA exams (100- and 1,000-question exams)",
            "task_description": "Multiple-choice QA from ten benchmark problem sets.",
            "problem_format": "One-shot example appended to system prompt; example included the reasoning style requested (e.g., chain-of-thought) and the final required answer formatting.",
            "comparison_format": null,
            "performance": "Used in all prompt conditions; no explicit quantitative comparison reported between one-shot and zero-shot because study constrained to one-shot designs.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "not applicable",
            "explanation_or_hypothesis": "The one-shot example aligns the model's output style (including CoT), likely improving consistency of the requested reasoning format, but the study did not compare to zero-shot or multi-shot variants.",
            "counterexample_or_null_result": null,
            "uuid": "e5702.5",
            "source_info": {
                "paper_title": "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Answer-format enforcement",
            "name_full": "Strict output formatting requirement (e.g., 'Action: Answer(\"[choice]\")')",
            "brief_description": "A strict requirement in the system prompt that the model must reply using an exact single-line answer format; used to make evaluation deterministic but caused formatting errors in some models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 2 7B (noted), evaluated across multiple LLMs",
            "model_size": "7B for Llama 2 7B; other model sizes not specified",
            "task_name": "MCQA exams (100-question small exam)",
            "task_description": "Multiple-choice QA drawn from ten benchmark problem sets.",
            "problem_format": "System prompt mandated final output exactly in the pattern 'Action: Answer(\"&lt;choice&gt;\")'; responses evaluated for correctness only if properly formatted.",
            "comparison_format": null,
            "performance": "Llama 2 7B performed poorly (not statistically better than random). Failure modes: 39% of outputs incorrectly formatted, 36% correctly formatted but incorrect answers — these formatting failures substantially lowered measured accuracy for that model.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "reduced (format enforcement caused apparent performance degradation for models that frequently violated the format)",
            "explanation_or_hypothesis": "Strict answer-format enforcement can artifactually reduce measured accuracy if models produce valid answers in a different surface form; Llama 2 7B's high rate of misformatted answers illustrates how presentation/formatting constraints can affect evaluation results independent of underlying problem-solving capability.",
            "counterexample_or_null_result": null,
            "uuid": "e5702.6",
            "source_info": {
                "paper_title": "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Text-variability vs Temperature",
            "name_full": "Text similarity / variability analysis (TF-IDF, Jaccard, BLEU, SBERT, etc.)",
            "brief_description": "Measurement of how response text similarity changes with sampling temperature using multiple metrics; used to link temperature to output diversity and coherence.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo (primary analysis); similar trends reported across other LLMs",
            "model_size": null,
            "task_name": "MCQA exams (100-question small exam; extended sweep to 1.6 for GPT-3.5)",
            "task_description": "Multiple-choice question answering; textual answers analyzed for similarity across repeated attempts.",
            "problem_format": "Same prompt formats (CoT in main analysis); repeated-answer sampling at each temperature to compute similarity metrics.",
            "comparison_format": null,
            "performance": "Text similarity decreased (i.e., variability increased) as temperature increased; similarity metrics drop rapidly after temperature &gt;1.0, coinciding with rapid accuracy degradation observed beyond τ=1.0.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "temperature increases -&gt; increased variability (no prompt-format interaction reported)",
            "explanation_or_hypothesis": "Higher sampling temperature increases randomness/diversity of outputs as expected from softmax temperature theory; in the constrained MCQA setting, this variability did not produce improved accuracy up to τ=1.0 but led to incoherence and falling accuracy beyond τ≈1.0.",
            "counterexample_or_null_result": null,
            "uuid": "e5702.7",
            "source_info": {
                "paper_title": "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Recitation-augmented language models",
            "rating": 2,
            "sanitized_title": "recitationaugmented_language_models"
        },
        {
            "paper_title": "Self-critique prompting with large language models for inductive instructions",
            "rating": 1,
            "sanitized_title": "selfcritique_prompting_with_large_language_models_for_inductive_instructions"
        },
        {
            "paper_title": "Improving code generation by dynamic temperature sampling",
            "rating": 1,
            "sanitized_title": "improving_code_generation_by_dynamic_temperature_sampling"
        }
    ],
    "cost": 0.0137005,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Effect of Sampling Temperature on Problem Solving in Large Language Models</h1>
<p>Matthew Renze<br>Johns Hopkins University mrenze1@jhu.edu</p>
<p>Erhan Guven<br>Johns Hopkins University<br>eguven2@jhu.edu</p>
<h4>Abstract</h4>
<p>In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used nine popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.6. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature from 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to generalize across LLMs, prompt-engineering techniques, and problem domains. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature.</p>
<h2>1 Introduction</h2>
<h3>1.1 Background</h3>
<p>In recent years, Large Language Models (LLMs) have revolutionized the field of artificial intelligence. The availability of open-source LLMs and pay-per-use APIs has allowed engineers to incorporate LLMs in their AI systems. However, prompt engineering and hyperparameter tuning are required to work effectively with LLMs.
Prompt-engineering techniques help LLMs solve complex problems, avoid hallucinations, and provide more accurate responses. For example, we can use techniques like chain-of-thought, tree-of-thought, self-criticism, and self-consistency to improve LLM performance [1,2].
In addition, several inference hyperparameters can be adjusted to modify the LLM's output at runtime. For example, hyperparameters like sampling temperature, top-k sampling, repetition penalty, and maximum token length all affect the LLM's output and performance [3-5].
Despite significant interest in LLMs and progress in LLM best practices, many open questions remain about optimal prompt-engineering techniques and inference hyperparameters for LLMs. To complicate matters, various local optima may exist for LLMs, prompt types, and problem domains [5].
The prompt-engineering community has an abundance of opinions and anecdotal evidence regarding optimal promptengineering techniques and inference hyperparameter settings. However, we currently lack systematic studies and empirical evidence to support many of these claims.
As a result, this paper aims to address the open question of the optimal LLM sampling temperature for problem-solving tasks. In addition, we aim to provide a systematic study with empirical results to add to the growing body of knowledge used to create LLM and prompt-engineering best practices.</p>
<h3>1.2 Sampling Temperature</h3>
<p>Sampling temperature is a hyperparameter of an LLM used in a temperature-based sampling process. It controls the randomness of the model's output at inference time [5-8].</p>
<p>During each step of an LLM’s decoding process, the LLM uses the previous tokens to choose the next output token. The final layer of the LLM uses a softmax function to convert raw scores (logits) into probabilities.</p>
<p>In greedy sampling, the model will always choose the most likely next token. However, for probabilistic sampling, the next token is selected from a probability distribution.</p>
<p>Temperature sampling is a modification to the softmax function, which adjusts the resulting probability mass functions. In this modified softmax function, $v_{k}$ is the $k$-th vocabulary token, $l_{k}$ is the token’s logit, and $\tau$ is a constant temperature. See equation 1.</p>
<p>$\operatorname{Pr}\left(v_{k}\right)=\frac{e^{l_{k} / \tau}}{\sum_{i} e^{l_{i} / \tau}}$</p>
<p>A lower temperature makes the output of the LLM more deterministic, thus favoring the most likely predictions. This conservativeness is captured by the model’s tendency to produce more repetitive, focused, and less diverse output based on the patterns most commonly seen in the training data [5, 7, 8].</p>
<p>A higher temperature increases the randomness of the output, thus favoring more “creative” predictions. This creativity is captured by the model’s willingness to explore more unconventional and less likely outputs. Higher temperatures can lead to novel text, diverse ideas, and creative solutions to problems [5, 7, 8].</p>
<p>In the context of problem-solving, temperature can be seen as a trade-off between exploring and exploiting possible solutions within the solution space. Lower temperatures tend to exploit more probable solutions; higher temperatures explore the solution space more broadly.</p>
<h3>1.3 Choosing a Sampling Temperature</h3>
<p>Within the prompt-engineering community, there are a variety of opinions and best practices regarding the ideal sampling temperature for various problem-solving tasks [9, 10].</p>
<p>Low sampling temperatures are recommended for tasks requiring precision and factual accuracy, such as technical writing, code generation, or question-answering [11, 12]. However, higher temperatures are recommended for tasks requiring creativity, such as writing poetry, creating stories, or brainstorming.</p>
<p>Higher temperatures also increase the probability of model hallucination. Hallucination is a phenomenon where an LLM produces statistically probable responses that are factually incorrect or nonsensical. As a result, optimal temperature selection is also a balance between creativity and hallucination [13].</p>
<p>Practical guidelines for choosing a sampling temperature for a specific task or problem domain are often vague or anecdotal. Prompt-engineering guides often provide hypothetical examples of optimal sampling temperatures for various tasks. However, they rarely cite any sources or provide empirical evidence.</p>
<p>As a result, the current state of choosing the optimal sampling temperature for specific problems is largely based on guesswork, gut instinct, non-systematic experimentation, and iterative refinement.^{2,3}</p>
<h2>2 Methods</h2>
<h3>2.1 Models</h3>
<p>The models used in this research project comprise nine widely-used foundational LLMs. To complement our analysis, we also conducted experiments using five prompts created using commonly used prompt-engineering techniques.</p>
<p>First, we reviewed the prior literature to identify candidate LLMs commonly used for problem-solving tasks. We limited our candidate models to those that allowed the model’s sampling temperature to be specified via their API [4, 16–18]. See Table 1 for a list of LLMs used in the experiment.</p>
<p>^{1}A few empirical studies exist that indicate sampling temperature does have an effect on LLM performance on some types of problem-solving tasks (e.g., code generation, engineering exams, etc.) [11, 12, 14].</p>
<p>^{2}For example, OpenAI’s GPT-3.5 API allowed users to set the sampling temperature from 0.0 to 1.0 with a default of 0.7. GPT-4’s API expanded this range from 0.0 to 2.0 with a default of 1.0. No explanation from OpenAI has been provided for these default values or their change from GPT-3.5 to GPT-4 [15].</p>
<p>^{3}Even the GPT-4 Technical Report explains that the authors used their “best-guess” when choosing sampling temperatures while evaluating GPT-4 on various benchmarks. See Appendix A in the GPT-4 Technical Report [16].</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Vendor</th>
<th>Released</th>
<th>License</th>
<th>Source</th>
</tr>
</thead>
<tbody>
<tr>
<td>Claude 3 Opus</td>
<td>Anthropic</td>
<td>2024-03-04</td>
<td>Closed</td>
<td>$[19,20]$</td>
</tr>
<tr>
<td>Command R+</td>
<td>Cohere</td>
<td>2024-04-04</td>
<td>Open</td>
<td>$[21,22]$</td>
</tr>
<tr>
<td>Gemini 1.0 Pro</td>
<td>Google</td>
<td>2023-12-06</td>
<td>Closed</td>
<td>$[23,24]$</td>
</tr>
<tr>
<td>Gemini 1.5 Pro (Preview)</td>
<td>Google</td>
<td>2024-02-15</td>
<td>Closed</td>
<td>$[25,26]$</td>
</tr>
<tr>
<td>GPT-3.5 Turbo</td>
<td>OpenAI</td>
<td>2022-11-30</td>
<td>Closed</td>
<td>$[17,27]$</td>
</tr>
<tr>
<td>GPT-4</td>
<td>OpenAI</td>
<td>2023-03-14</td>
<td>Closed</td>
<td>$[16,18]$</td>
</tr>
<tr>
<td>Llama 2 7B Chat</td>
<td>Meta</td>
<td>2023-07-18</td>
<td>Open</td>
<td>$[4,28]$</td>
</tr>
<tr>
<td>Llama 2 70B Chat</td>
<td>Meta</td>
<td>2023-07-18</td>
<td>Open</td>
<td>$[4,28]$</td>
</tr>
<tr>
<td>Mistral Large</td>
<td>Mistral AI</td>
<td>2024-02-26</td>
<td>Closed</td>
<td>$[29]$</td>
</tr>
</tbody>
</table>
<p>Table 1: LLMs used in the experiment.</p>
<p>Next, we reviewed the existing literature for commonly used prompt-engineering techniques. We limited our candidate prompts to those that could be performed in a single request-and-response cycle with one-shot in-context learning. We excluded multi-step agents, few-shot learning, and model fine-tuning.
As a result, we selected five prompt-engineering techniques to construct our system prompts:</p>
<ul>
<li>Baseline - no prompt engineering; the LLM is instructed to return only a single multiple-choice answer as its output (e.g., 'Answer("C")' ).</li>
<li>Domain Expertise - the system prompt specifies that the LLM is an expert in the problem domain of the exam (e.g., "medicine") or the topic of the problem (e.g., "anatomy") [2].</li>
<li>Self-recitation - the system prompt instructs the LLM to recite its own internal knowledge about the problem before answering the question $[2,30]$.</li>
<li>Chain-of-Thought (CoT) - the system prompt instructs the LLM to "think step-by-step" to encourage it to reason through the problem procedurally $[31,32]$.</li>
<li>Composite - the system prompt combines domain expertise, self-recitation, chain-of-thought, and adds self-criticism $[33,34]$.</li>
</ul>
<p>Finally, we provided the LLM with a single example problem-and-solution pair for one-shot in-context learning. The example solution was adapted for each prompt based on the prompt-engineering technique used. For example, the CoT prompt included a chain of thought in its solution. See Figure 10 in the Appendix for a sample prompt.</p>
<h1>2.2 Data</h1>
<p>The test dataset used in this research study consists of a series of Multiple-Choice Question-and-Answer (MCQA) exams derived from widely used LLM performance benchmarks.
First, we reviewed the prior literature to identify benchmarks frequently used to evaluate LLMs. We limited our candidate benchmarks to those containing MCQA problems so that we could use correct-answer accuracy as our primary performance metric.
Next, we selected a set of problems that covered a range of problem domains (e.g., math, science, law, etc.) and difficulty levels (e.g., secondary school, university, etc.) These problem sets can be seen in Table 2.
Then, we converted the benchmark problems from their original data format into a standardized data structure using the JSON Lines (JSON-L) format [41]. Our standardized set of exams allowed us to use the exams interchangeably without modifying the code in the test harness. See Figure 11 in the Appendix for a sample of an MCQA problem.
Finally, we created two MCQA exams of different sizes. We created a large exam with 1,000 questions by randomly sampling 100 problems from each of the ten problem sets. This 1,000 -question (large) exam was used with GPT-3.5 to perform a detailed analysis of temperature across problem domains.
Additionally, we created a smaller exam of 100 questions by randomly sampling ten questions from each of the ten domain-specific problem sets. This 100 -question (small) exam was used for our high-level analysis of sampling temperature across all nine models, all five prompt-engineering techniques, and extended temperature range ( $0.0-1.6) .^{5}$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Problem Set</th>
<th>Benchmark</th>
<th>Domain</th>
<th>Questions</th>
<th>License</th>
<th>Source</th>
</tr>
</thead>
<tbody>
<tr>
<td>ARC Challenge Test</td>
<td>ARC</td>
<td>Science</td>
<td>1,173</td>
<td>CC BY-SA</td>
<td>[35]</td>
</tr>
<tr>
<td>AQUA-RAT</td>
<td>AGI Eval</td>
<td>Math</td>
<td>254</td>
<td>Apache v2.0</td>
<td>[36]</td>
</tr>
<tr>
<td>Hellaswag Val</td>
<td>Hellaswag</td>
<td>Common Sense Reasoning</td>
<td>10,042</td>
<td>MIT</td>
<td>[37]</td>
</tr>
<tr>
<td>LogiQA (English)</td>
<td>AGI Eval</td>
<td>Logic</td>
<td>651</td>
<td>GitHub</td>
<td>$[36,38]$</td>
</tr>
<tr>
<td>LSAT-AR</td>
<td>AGI Eval</td>
<td>Law (Analytic Reasoning)</td>
<td>230</td>
<td>MIT</td>
<td>$[36,39]$</td>
</tr>
<tr>
<td>LSAT-LR</td>
<td>AGI Eval</td>
<td>Law (Logical Reasoning)</td>
<td>510</td>
<td>MIT</td>
<td>$[36,39]$</td>
</tr>
<tr>
<td>LSAT-RC</td>
<td>AGI Eval</td>
<td>Law (Reading Comprehension)</td>
<td>260</td>
<td>MIT</td>
<td>$[36,39]$</td>
</tr>
<tr>
<td>MedMCQA Valid</td>
<td>MedMCQA</td>
<td>Medicine</td>
<td>6,150</td>
<td>MIT</td>
<td>[40]</td>
</tr>
<tr>
<td>SAT-English</td>
<td>AGI Eval</td>
<td>English</td>
<td>206</td>
<td>MIT</td>
<td>[36]</td>
</tr>
<tr>
<td>SAT-Math</td>
<td>AGI Eval</td>
<td>Math</td>
<td>220</td>
<td>MIT</td>
<td>[36]</td>
</tr>
</tbody>
</table>
<p>Table 2: Problem sets used to create the multi-domain MCQA exam.
Note: The GitHub repository for LogiQA does not include a license file. However, both the paper and readme.md file states that "The dataset is freely available."</p>
<h1>2.3 Process</h1>
<p>Our experiment was designed to test the problem-solving performance of LLMs across ten models, five promptengineering techniques, ten problem domains, 100 problems within each problem domain, and all viable sampling temperatures. For each combination of model, prompt, exam, and temperature, we instructed the LLM to answer each question ten times so we could assess the average correct-answer accuracy.
The full experiment setup can be seen in Figure 1 and Algorithm 1. However, due to cost and runtime considerations, we conducted a subset of the full experiment designed to capture the most valuable information as efficiently as possible.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Diagram of the full experiment.</p>
<p>Algorithm 1 Full LLM Temperature Experiment</p>
<table>
<thead>
<tr>
<th style="text-align: left;">1: for each model $m$ in $M$ do</th>
<th style="text-align: left;">$\triangleright 10$ models</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">2: for each prompt $p$ in $P$ do</td>
<td style="text-align: left;">$\triangleright 5$ prompts</td>
</tr>
<tr>
<td style="text-align: left;">3: for each exam $e$ in $E$ do</td>
<td style="text-align: left;">$\triangleright 10$ exams</td>
</tr>
<tr>
<td style="text-align: left;">4: for each temperature $\tau$ in $T$ do</td>
<td style="text-align: left;">$\triangleright 16$ temps</td>
</tr>
<tr>
<td style="text-align: left;">5: for each problem $q$ in $Q$ do</td>
<td style="text-align: left;">$\triangleright 100$ probs</td>
</tr>
<tr>
<td style="text-align: left;">6: for each attempt $a$ in $A$ do</td>
<td style="text-align: left;">$\triangleright 10$ attempts</td>
</tr>
<tr>
<td style="text-align: left;">7: Create the prompt</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">8: Answer the question</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">9: Record the answer</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">10: end for</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">11: end for</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">12: end for</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">13: Save the results</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">14: end for</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">15: end for</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">16: end for</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">17: Process the results</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">18: Analyze the results</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>First, we instructed GPT-3.5 to complete the 100-question (small) exam using the CoT prompt with temperatures ranging from 0.0 to 2.0 in increments of 0.1 . This allowed us to determine the range of viable sampling temperatures to explore. ${ }^{5}$
Performance began to drop rapidly after a temperature of 1.0 until the generated text became incoherent at 1.6. As a result, we stopped the initial temperature sweep at 1.6 and limited the rest of our sweeps from 0.0 to 1.0 .
Next, we instructed the other eight LLMs to complete the 100-question (small) exam using the CoT prompt with temperatures from 0.0 to 1.0. This allowed us to determine if the results generalize to other LLMs.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Then, we instructed GPT-3.5 to complete the 100-question (small) exam using each of the five prompts over temperatures from 0.0 to 1.0. This allowed us to determine if the results generalize over various prompt-engineering techniques.
Finally, we instructed GPT-3.5 to complete the 1,000-question (large) exam using the CoT prompt with temperatures from 0.0 to 1.0. This allowed us to determine if the results were domain-specific or generalized across problem domains.</p>
<h1>2.4 Metrics</h1>
<p>To test our hypothesis, we measured the LLM's correct-answer accuracy as our primary performance metric. For each combination of model, prompt, exam, and temperature, we calculated the accuracy as the number of correct answers from ten attempts at each problem. Then, we computed the average (mean) accuracy across all problems.
To further support our findings, we also measured the similarity of the LLM's responses using a series of text-similarity metrics. These metrics are defined as follows:</p>
<ul>
<li>Jaccard similarity - the ratio of the intersection to the union of word sets in the output text [42].</li>
<li>Bag-of-Words (BoW) similarity - comparison of the frequency of each word, ignoring order [43].</li>
<li>TF-IDF similarity - comparison of word frequency weighted by inverse document frequency [44].</li>
<li>Levenshtein similarity - the number of edits needed to change one string of text into the other [45].</li>
<li>BLEU score - comparison of similarity based on n-gram overlap [46].</li>
<li>SBERT similarity - semantic similarity computed using Sentence-BERT embeddings [47].</li>
</ul>
<h3>2.5 Analysis</h3>
<p>We used the Kruskal-Wallis test to evaluate the statistical significance of any changes in accuracy as a function of temperature [48]. We chose the Kruskal-Wallis test because the data (i.e., correct-answer accuracy by question) were not normally distributed. Rather, they were bimodally distributed with centers at 0.0 and 1.0.</p>
<h2>3 Results</h2>
<h3>3.1 Accuracy vs. Temperature</h3>
<p>Our analysis revealed that the problem-solving performance of LLMs remained relatively stable across sampling temperatures from 0.0 to 1.0 for all LLMs, prompt-engineering techniques, and problem domains. Using GPT-3.5 with a CoT prompt on the 1,000-question exam from 0.0 to 1.0, the Kruskal-Wallis test yielded $H(10)=10.439, p=0.403$.
First, we analyzed the performance of GPT-3.5 using the CoT prompt on the 100-question exam. Accuracy remained stable over temperatures from 0.0 to 1.0. However, after a temperature of 1.0, the text rapidly became incoherent, and the accuracy began to drop until it reached zero around a temperature of 1.6. See Figure 2.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Accuracy by temperature from 0.0 to 1.6 for GPT-3.5 using the CoT prompt on the 100-question exam.</p>
<p>Second, we analyzed the performance of all nine LLMs using the CoT prompt on the 100-question exam. Accuracy also remained stable across all of the LLMs, except for Llama 2 7B. The performance of most LLMs showed a gradual (non-significant) decrease in performance as a function of temperature. See Figure 3 and Table 3.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">$\mathbf{H ( 1 0 )}$</th>
<th style="text-align: center;">$\mathbf{p}$-value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Claude 3 Opus</td>
<td style="text-align: right;">1.735</td>
<td style="text-align: center;">0.998</td>
</tr>
<tr>
<td style="text-align: left;">Command R+</td>
<td style="text-align: right;">1.771</td>
<td style="text-align: center;">0.998</td>
</tr>
<tr>
<td style="text-align: left;">Gemini Pro 1.0</td>
<td style="text-align: right;">7.379</td>
<td style="text-align: center;">0.689</td>
</tr>
<tr>
<td style="text-align: left;">Gemini Pro 1.5</td>
<td style="text-align: right;">3.119</td>
<td style="text-align: center;">0.978</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 Turbo</td>
<td style="text-align: right;">2.042</td>
<td style="text-align: center;">0.996</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: right;">3.789</td>
<td style="text-align: center;">0.956</td>
</tr>
<tr>
<td style="text-align: left;">Llama 2 70B</td>
<td style="text-align: right;">3.677</td>
<td style="text-align: center;">0.961</td>
</tr>
<tr>
<td style="text-align: left;">Llama 2 7B</td>
<td style="text-align: right;">17.086</td>
<td style="text-align: center;">0.072</td>
</tr>
<tr>
<td style="text-align: left;">Mistral Large</td>
<td style="text-align: right;">3.069</td>
<td style="text-align: center;">0.980</td>
</tr>
</tbody>
</table>
<p>Table 3: Kruskal-Wallis test results by model using the CoT prompt on the 100-question exam.</p>
<p>Figure 3: Accuracy by temperature and model using the CoT prompt on the 100-question exam.</p>
<p>Llama 2 7B did not perform better than statistically random guesses. Its poor performance was due to generating incorrectly formatted answers (39\%) and correctly formatted but incorrect answers (36\%). Its all-or-nothing behavior at a temperature of 0.0 versus more random behavior from 0.1 to 1.0 led to a much lower, yet still non-significant, p-value.
Third, we analyzed the performance of GPT-3.5 using each of the five prompts on the 100-question exam. Accuracy remained stable for all temperatures across all prompt-engineering techniques. The CoT prompt outperformed the other four prompts. As a result, we used the CoT prompt for all single-prompt experiments. See Figure 4 and Table 4.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompt</th>
<th style="text-align: center;">$\mathbf{H ( 1 0 )}$</th>
<th style="text-align: center;">$\mathbf{p}$-value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Baseline</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">Domain Expert</td>
<td style="text-align: center;">0.548</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">Self-recitation</td>
<td style="text-align: center;">1.403</td>
<td style="text-align: center;">0.999</td>
</tr>
<tr>
<td style="text-align: left;">Chain of Thought</td>
<td style="text-align: center;">2.042</td>
<td style="text-align: center;">0.996</td>
</tr>
<tr>
<td style="text-align: left;">Composite</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
</tr>
</tbody>
</table>
<p>Table 4: Kruskal-Wallis test results by prompt for GPT-3.5 on the 100-question exam.</p>
<p>Figure 4: Accuracy by temperature and prompt for GPT-3.5 on the 100-question exam.</p>
<p>Finally, we analyzed the performance of GPT-3.5 using the CoT prompt on all ten exams. Accuracy remained stable for all temperatures across all problem domains based on visual analysis. However, the LSAT-AR and SAT-Math exams showed statistically significant differences in the Kruskal-Wallis p-values. ${ }^{6}$ See Figure 5 and Table 5.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Accuracy by temperature and exam for GPT-3.5 using the CoT prompt.</p>
<p>We performed the Dunn-Bonferroni test on the LSAT-AR and SAT-Math results [49]. It revealed that the all-or-nothing behavior of responses generated at a temperature of 0.0 versus the more random responses from 0.1 to 1.0 caused the anomaly. The correct-answer accuracy for each individual problem varied significantly when evaluated pairwise across temperatures. However, the average accuracy for all problems remained similar across temperatures.</p>
<h1>3.2 Text Variability vs. Temperature</h1>
<p>To further support our results, we analyzed text variability as a function of temperature. Our findings show a clear trend of decreasing text similarity (thus increasing text variability) as temperature increases. Text similarity decreases rapidly after a temperature of 1.0, corresponding to the rapid decrease in accuracy observed above $\tau=1.0$. See Figure 6.
These results are consistent with our understanding of sampling temperature, indicating that higher temperatures produce more widely varied outputs. Furthermore, these results hold regardless of the LLM, prompt-engineering technique, or problem domain. See Figures 7, 8, and 9.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Text similarity by temperature and metric for GPT3.5 using CoT prompting on the 100 -question exam over sampling temperatures from 0.0 to 1.6 .
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: TF-IDF text similarity by temperature and model using the CoT prompt on the 100 -question exam over sampling temperatures from 0.0 to 1.0</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: TF-IDF text similarity by temperature and prompt for GPT-3.5 on the 100-question exam over sampling temperatures from 0.0 to 1.0 .
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: TF-IDF text similarity by temperature and exam for GPT-3.5 using the CoT prompt over sampling temperatures from 0.0 to 1.0</p>
<h1>4 Discussion</h1>
<h3>4.1 Interpretation</h3>
<p>Based on these results, changes in temperature from 0.0 to 1.0 do not have a statistically significant effect on the problemsolving performance of LLMs. These results appear to generalize across LLMs, prompt-engineering techniques, and problem domains. However, there are a few potential exceptions to these general findings.
Therefore, we recommend setting an LLM's sampling temperature to 0.0 for problem-solving tasks. This temperature maximizes reproducibility without compromising accuracy. In addition, it helps avoid the inevitable drop-off in performance that occurs beyond a temperature of 1.0. However, exceptions to this guidance should be taken into consideration.</p>
<h3>4.2 Limitations</h3>
<p>There were several limitations to our research study due to cost and runtime considerations:
First, our study was limited to a subset of popular LLMs. As a result, our findings may not hold for other LLMs that were excluded from our experiment.
Second, we only explored a subset of prompt-engineering techniques using a single prompt-and-response cycle with one-shot in-context learning. As a result, there may be more complex prompts or agent architectures that better leverage sampling temperature for creativity in their problem-solving capabilities.
Third, our study was limited to a subset of problems, problem domains, and problem-solving tasks. As a result, our findings may not hold for larger data sets, different problem domains, or other types of problem-solving tasks.
Fourth, due to time and cost constraints, we limited our study to two test sets of 1,000 and 100 randomly selected questions from standard benchmarks. These limited sample sizes may have introduced bias into the results. Utilizing a larger and more diverse test set would enhance the statistical reliability of our findings.
Fifth, we had to limit the sampling temperature range we explored from 0.0 to 1.0 for all combinations of models, prompts, and exams, except for GPT-3.5 using CoT prompting on the 100-question exam. As a result, the temperature hyperparameter of other LLMs may operate differently at temperatures above 1.0.
Sixth, we fixed all other sampling parameters (e.g., top-p, top-k, repetition penalty, etc.) to isolate the effect of sampling temperature. As a result, there may be combinations of sampling parameters that result in different outcomes.</p>
<p>Finally, we could only explore a subset of the various combinations of models, prompts, exams, and temperatures. As a result, other combinations of LLMs, prompt-engineering techniques, and problem domains may exist where temperature plays a more important role in problem-solving performance.</p>
<h1>4.3 Implications</h1>
<p>This research study provides empirical evidence that changes in sampling temperature in the range of 0.0 to 1.0 do not significantly impact the problem-solving capabilities of LLMs on MCQA problems.
Answering this question may save AI engineers significant time and resources evaluating various sampling temperatures for their LLM agents and applications. In addition, it may reduce unproductive debates in the prompt-engineering community regarding the optimal sampling temperatures for various problem-solving tasks.
This research also provides broader insights for AI researchers studying model hallucination and problem-solution state-space search with LLMs. Our results show that increasing LLM temperature up to 1.0 does not cause the LLM to hallucinate in ways that lead to incorrect MCQA solutions. In addition, higher temperatures do not appear to improve MCQA solution-space search in ways that lead to correct solutions more often than lower temperatures.</p>
<h3>4.4 Future Research</h3>
<p>To improve upon this research, we propose the following follow-up experiments:
First, we recommend conducting this experiment with additional LLMs. Other proprietary and open-source LLMs may utilize temperature in ways that benefit their specific models but did not benefit the LLMs we tested.
Second, we recommend expanding beyond MCQA problems to other types of problem-solving tasks whose correct answers are more open-ended. The limited effects of sampling temperature in our experiments may have simply resulted from the constraints imposed by the structure of MCQA problems.
Third, we recommend conducting additional experiments with more MCQA problems and problem domains. We recommend specifically targeting tasks and problem domains that require more creative solutions or lateral "out-of-thebox" thinking.
Fourth, we recommend extending the sampling temperature range until accuracy drops to zero for each LLM, prompt, and exam. However, it should be noted that as the generated text becomes more random, the number of tokens in each response increases significantly, leading to a considerable increase in runtime and cost to explore temperatures above 1.0.</p>
<p>Finally, we recommend a more in-depth error analysis to determine if any sub-types of problems within these problem domains benefit from changes to sampling temperature. It is possible that statistical noise or averaging may have hidden individual problems that were sensitive to changes in sampling temperature.</p>
<h2>5 Conclusion</h2>
<p>This research study empirically investigated the effect of sampling temperature on the problem-solving performance of LLMs across multiple problem domains.
We demonstrated that changes in sampling temperature from 0.0 to 1.0 do not produce statistically significant differences in problem-solving performance on MCQA problems across multiple LLMs, prompt-engineering techniques, and problem domains.
These results have practical implications for AI engineers using LLMs to develop new AI systems. Additionally, they have theoretical implications for AI researchers studying model hallucination and solution-space search with LLMs.</p>
<h2>References</h2>
<p>[1] G. Mialon, R. Dessì, M. Lomeli, C. Nalmpantis, R. Pasunuru, R. Raileanu, B. Rozière, T. Schick, J. Dwivedi-Yu, A. Celikyilmaz, E. Grave, Y. LeCun, and T. Scialom, "Augmented language models: a survey," arXiv, 2 2023.
[2] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. Elnashar, J. Spencer-Smith, and D. C. Schmidt, "A prompt pattern catalog to enhance prompt engineering with ChatGPT," arXiv, 22023.
[3] OpenAI, "OpenAI - API reference," 2023. [Online]. Available: https://platform.openai.com/docs/api-reference/ chat/create</p>
<p>[4] Llama-2-Team, "Llama 2: Open foundation and fine-tuned chat models," arXiv, 72023.
[5] C. Wang, S. X. Liu, and A. H. Awadallah, "Cost-effective hyperparameter optimization for large language model generation inference," 2023.
[6] D. H. Ackley, G. E. Hinton, and T. J. Sejnowski, "A learning algorithm for Boltzmann machines," Cognitive Science, vol. 9, pp. 147-169, 1985.
[7] G. Hinton, O. Vinyals, and J. Dean, "Distilling the knowledge in a neural network," arXiv, 32015.
[8] P.-H. Wang, S.-I. Hsieh, S.-C. Chang, Y.-T. Chen, J.-Y. Pan, W. Wei, and D.-C. Juan, "Contextual temperature for language modeling," arXiv, 122020.
[9] Microsoft, "Completions - learn how to generate or manipulate text," 2023. [Online]. Available: https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/completions
[10] J. Shieh, "Best practices for prompt engineering with OpenAI API," 2024. [Online]. Available: https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api
[11] F. F. Xu, U. Alon, G. Neubig, and V. J. Hellendoorn, "A systematic evaluation of large language models of code," in Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. Association for Computing Machinery, 2022, pp. 1-10.
[12] Y. Zhu, J. Li, G. Li, Y. Zhao, J. Li, Z. Jin, and H. Mei, "Improving code generation by dynamic temperature sampling," arXiv, 92023.
[13] M. Lee, "A mathematical investigation of hallucination and creativity in GPT models," Mathematics, vol. 11, p. 2320, 52023.
[14] V. Pursnani, Y. Sermet, and I. Demir, "Performance of ChatGPT on the US fundamentals of engineering exam: Comprehensive assessment of proficiency and potential implications for professional environmental engineering practice," arXiv, 42023.
[15] OpenAI, "Api temperature change from 0 to 1 to 0 to 2 in 'playground'," 2023. [Online]. Available: https://community.openai.com/t/api-temperature-change-from-0-to-1-to-0-to-2-in-playground/217755
[16] —<em>, "GPT-4 technical report," arXiv, 3 2023. [Online]. Available: https://arxiv.org/abs/2303.08774
[17] —</em>, "Introducing ChatGPT," 11 2022. [Online]. Available: https://openai.com/blog/chatgpt
[18] —_, "GPT-4," 3 2023. [Online]. Available: https://openai.com/research/gpt-4
[19] Anthropic, "Introducing the next generation of claude anthropic," 2024. [Online]. Available: https: //www.anthropic.com/news/claude-3-family
[20] , "The claude 3 model family: Opus, sonnet, haiku," 2024. [Online]. Available: https: //www.anthropic.com/claude-3-model-card
[21] Cohere, "Command r+," 2024. [Online]. Available: https://docs.cohere.com/docs/command-r-plus
[22] , "Model card for c4ai command r+," 2024. [Online]. Available: https://huggingface.co/CohereForAI/ c4ai-command-r-plus
[23] S. Pichai and D. Hassabis, "Introducing gemini: Google's most capable ai model yet," 2023. [Online]. Available: https://blog.google/technology/ai/google-gemini-ai/
[24] Gemini-Team, "Gemini: A family of highly capable multimodal models," arXiv, 122023.
[25] S. Pichai and D. Hassabis, "Introducing gemini 1.5, google's next-generation ai model," 2024. [Online]. Available: https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/
[26] Gemini-Team, "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context," 2024. [Online]. Available: https://arxiv.org/abs/2403.05530
[27] OpenAI, "Models - openai api." [Online]. Available: https://platform.openai.com/docs/models/gpt-3-5-turbo
[28] Meta, "Meta and microsoft introduce the next generation of llama I meta," 2023. [Online]. Available: https://about.meta.com/news/2023/07/llama-2/
[29] Mistral-AI-Team, "Au large I mistral ai I frontier ai in your hands," 2024. [Online]. Available: https://mistral.ai/news/mistral-large/
[30] Z. Sun, X. Wang, Y. Tay, Y. Yang, and D. Zhou, "Recitation-augmented language models," in The Eleventh International Conference on Learning Representations, 102023.
[31] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, "Large language models are zero-shot reasoners," in Advances in Neural Information Processing Systems, vol. 35, 5 2022, pp. 22 199-22 213.
[32] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou, "Chain-of-thought prompting elicits reasoning in large language models," arXiv, 12022.
[33] S. Huo, N. Arabzadeh, and C. L. A. Clarke, "Retrieving supporting evidence for generative question answering," arXiv, 92023.</p>
<p>[34] R. Wang, H. Wang, F. Mi, Y. Chen, R. Xu, and K.-F. Wong, "Self-critique prompting with large language models for inductive instructions," arXiv, 52023.
[35] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord, "Think you have solved question answering? Try ARC, the AI2 reasoning challenge," ArXiv, 32018.
[36] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan, "AGIEval: A human-centric benchmark for evaluating foundation models," ArXiv, 42023.
[37] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, "HellaSwag: Can a machine really finish your sentence?" in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.
[38] J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, and Y. Zhang, "Logiqa: A challenge dataset for machine reading comprehension with logical reasoning," in International Joint Conference on Artificial Intelligence, 2020.
[39] S. Wang, Z. Liu, W. Zhong, M. Zhou, Z. Wei, Z. Chen, and N. Duan, "From lsat: The progress and challenges of complex reasoning," IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 30, pp. 2201-2216, 82021.
[40] A. Pal, L. K. Umapathi, and M. Sankarasubbu, "MedMCQA: A large-scale multi-subject multi-choice dataset for medical domain question answering," in Proceedings of the Conference on Health, Inference, and Learning. PMLR, 2022, pp. 248-260.
[41] I. Ward, "JSON lines," 2014. [Online]. Available: https://jsonlines.org/
[42] P. Jaccard, "The distribution of flora in the alpine zone," New Phytologist, vol. 11, pp. 37-50, 21912.
[43] Z. S. Harris, "Distributional structure," WORD, vol. 10, pp. 146-162, 81954.
[44] K. S. Jones, "A statistical interpretation of term specificity and its application in retrieval," Journal of Documentation, vol. 28, pp. 11-21, 11972.
[45] V. Levenshtein, "Binary codes capable of correcting deletions, insertions and reversals," Soviet Physics Doklady, vol. 10, pp. 707-710, 1966.
[46] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, "BLEU," in Proceedings of the 40th Annual Meeting on Association for Computational Linguistics - ACL '02. Association for Computational Linguistics, 2001, p. 311.
[47] N. Reimers and I. Gurevych, "Sentence-BERT: Sentence embeddings using Siamese BERT-networks," in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 82019.
[48] W. H. Kruskal and W. A. Wallis, "Use of ranks in one-criterion variance analysis," Journal of the American Statistical Association, vol. 47, no. 260, pp. 583-621, 1952. [Online]. Available: https://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483441
[49] O. J. Dunn, "Multiple comparisons using rank sums," Technometrics, vol. 6, no. 3, pp. 241-252, 1964. [Online]. Available: https://www.tandfonline.com/doi/abs/10.1080/00401706.1964.10490181</p>
<h1>A Appendix</h1>
<p>[System Prompt]
You are an expert in {{expertise}}.
Your task is to answer the following multiple-choice questions.
First, you should recite all of the relevant knowledge you have about the question and each option.
Next, you should think step-by-step through the problem to ensure you have the correct answer.
Then, you should critically evaluate your thoughts to identify any flaws in your facts, logic, and reasoning.
Finally, you MUST answer the question using the following format 'Action: Answer("[choice]")'
The parameter [choice] is the letter or number of the answer you want to select (e.g. "A", "B", "C", or "D")
For example, 'Answer("C")' will select choice "C" as the best answer.
The answer MUST ALWAYS be one of the available choices; it CANNOT be "None of the Above".
If you think the answer is "none of the above", then you MUST select the most likely answer.
[Example Problem]
Question: What is the capital of the state where Johns Hopkins University is located?
Choices:
A: Baltimore
B: Annapolis
C: Des Moines
D: Las Vegas
[Example Solution]
Knowledge:
Johns Hopkins University is located in Baltimore, Maryland.
A: Baltimore is a city located in the State of Maryland, but it is not the capital of Maryland.
B: Annapolis is the capital of the State of Maryland.
C: Des Moines is a city located in the State of Iowa and is the capital of Iowa.
D: Las Vegas is located in the State of Nevada, but it is not the capital of Nevada.
Thought:
Johns Hopkins University is located in Baltimore.
Baltimore is a city located in the state of Maryland.
The capital of Maryland is Baltimore.
Therefore, the capital of the state where Johns Hopkins University is located is Baltimore.
The answer is A: Baltimore.
Criticism:
You are correct that Johns Hopkins is located in the State of Maryland.
However, the capital of Maryland is Annapolis, not Baltimore.
So, the correct answer is actually B: Annapolis.
Action: Answer("B")
Figure 10: Sample of the composite system prompt with a one-shot example (i.e., problem-and-solution pair).</p>
<div class="codehilite"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="s">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;arc/arc-challenge-test&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;source_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;topic&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Science&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;context&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;question&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;An astronomer observes that a planet rotates faster</span>
<span class="s">                                    after a meteorite impact. Which is the most likely effect</span>
<span class="s">                                    of this increase in rotation?&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;choices&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s">&quot;A&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Planetary density will decrease.&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;B&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Planetary years will become longer.&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;C&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Planetary days will become shorter.&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;D&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Planetary gravity will become stronger.&quot;</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="s">&quot;answer&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;solution&quot;</span><span class="p">:</span><span class="s">&quot;&quot;</span>
<span class="p">}</span>
</code></pre></div>

<p>Figure 11: Sample of an MCQA problem in JSON-L format - with whitespace added for readability.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ We considered the ARC Challenge results to be non-significant since they were greater than the significance threshold of 0.05 .&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2740 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2740</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2740</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-268856673</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.02039v2.pdf" target="_blank">A Survey on Large Language Model-Based Game Agents</a></p>
                <p><strong>Paper Abstract:</strong> Game environments provide rich, controllable settings that simulate many aspects of real-world complexity. As such, game agents offer a valuable testbed for exploring capabilities relevant to Artificial General Intelligence [176]. Recently, the emergence of Large Language Models (LLMs) provides new opportunities to endow these agents with generalizable reasoning, memory, and adaptability in complex game environments. This survey offers an up-to-date review of LLM-based game agents (LLMGAs) through a unified reference architecture. At the single-agent level, we synthesize existing studies around three core components: memory, reasoning, and perception–action interfaces, which jointly characterize how language enables agents to perceive, think, and act. At the multi-agent level, we outline how communication protocols and organizational models support coordination, role differentiation, and large-scale social behaviors. To contextualize these designs, we introduce a challenge-centered taxonomy linking six major game genres to their dominant agent requirements, from low-latency control in action games to open-ended goal formation in sandbox worlds. A curated list of related papers is available at: https://github.com/git-disl/awesome-LLM-game-agent-papers.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2740.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2740.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/agent paradigm that interleaves chain-of-thought style reasoning traces with environment actions to guide stepwise decision-making in interactive/text games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-driven agent using few-shot prompting to alternate explicit reasoning (CoT) and action generation; LLM is central decision maker and is prompted to produce 'thought' and 'action' tokens in an interleaved loop.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Text-based games (e.g., ALFWorld, TextWorld mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-adventure / instruction-following environments where the agent issues textual commands and receives natural-language observations; tasks include navigation, object manipulation, and multi-step household tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Survey notes that simple ReAct-style interleaving of reasoning and actions is insufficient by itself for long-horizon text-adventure tasks because agents commonly fail to maintain an accurate record of explored state and become stuck.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Does not provide explicit long-term memory or consolidation; in-episode CoT reasoning can drift and lacks persistence across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Large Language Model-Based Game Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2740.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2740.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent framework that produces self-reflections (self-critiques) on failed attempts and uses those reflections to modify subsequent behavior, effectively creating a reflective memory trace across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-based agent that records outcomes of actions and generates textual self-analyses which are then stored and consumed in future planning steps to avoid repeating mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>ALFWorld (and other interactive/text environments referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Embodied instruction-following / text grounding environments where the agent executes sequences of textual actions to complete household or procedural tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic / reflective memory (summary-of-failures and reflections)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>textual summaries / reflection records appended to an episodic buffer (chunk-like entries)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>self-critiques, failure analyses, summaries of previous attempts and corrections</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>retrieve recent or relevant reflections when planning new attempts (temporal/importance bias implied)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>write reflection summaries after failed attempts or at episode boundaries (pause-and-reflect mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>avoid repeating past mistakes, incorporate lessons from failures into future planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Writing and storing self-critiques enables agents to extract insights from errors and avoid repeating them, turning episodic failures into persistent corrections of world knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Reflection tends to be coarse-grained (post-hoc summaries) rather than providing step-level dense feedback; may not provide fine-grained credit assignment for intermediate reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Large Language Model-Based Game Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2740.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2740.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PokéLLMon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PokéLLMon: A grounding and reasoning benchmark for large language models in Pokémon battles</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark and agent work studying LLMs playing turn-based Pokémon Battles; identifies short-term decision inconsistency and proposes active maintenance (Last-Thoughts) to improve stability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PokéLLMon: A grounding and reasoning benchmark for large language models in pokémon battles.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PokéLLMon agent (LLM-based Pokémon battle agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-driven agent (evaluated with GPT-4o in the survey's Table 2) that generates battle actions; evaluated with and without intermediate reasoning (CoT) and with active maintenance strategies (Last-Thoughts).</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Pokémon Battles (PokéLLMon benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Turn-based battle environment where each player selects moves or switches per turn; requires grounding to game mechanics (type matchups, move effects) and short-horizon consistency across turns.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working-memory active maintenance + external game-knowledge retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>in-prompt carryover of previous reasoning traces (Last-Thoughts) and retrieval from external knowledge (game mechanics) when needed</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>most-recent chain-of-thought trace, previous action, known game rules/type-effect knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>carry forward last-step reasoning into next prompt (temporal continuity); retrieve external game knowledge by lookup when relevant</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>update per turn by including previous 'thought' in the next prompt (Last-Thoughts); external knowledge static or retrieved on demand</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>stabilize short-term decision-making, reduce consecutive illogical switches, ground actions in game-specific knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Last-Thoughts (GPT-4o): Win Rate 46.67% ; Switch Rate 0.2227 ; Consecutive Switch Rate 0.0861 (Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline LLM (GPT-4o) without Last-Thoughts: Win Rate 42.17% ; Switch Rate 0.3356 ; Consecutive Switch Rate 0.2442 (Table 2). CoT and SC-CoT variants reported intermediate values in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Active maintenance (carrying previous reasoning into the next step) substantially reduces short-term inconsistency (consecutive switch rate) and improves win rate; chain-of-thought increases stochasticity and can worsen immediate consistency unless paired with continuity mechanisms like Last-Thoughts or self-consistency voting.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Intermediate chain-of-thought increases stochastic divergence across steps; without active maintenance, reasoning tokens introduce cumulative randomness and short-term action instability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Last-Thoughts (explicit carryover of previous reasoning) outperformed CoT-alone and Self-Consistency CoT in reducing consecutive-switch behavior and improving win rate because it anchors current decisions to prior deliberation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Large Language Model-Based Game Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2740.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2740.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenerativeAgents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cognitive-architecture style system that scores incoming observations by recency/importance, consolidates salient events into long-term memory summaries, and retrieves them to drive believable agent behavior in simulated towns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Population of LLM-based agents augmented with memory modules that score importance, reflect, and summarize episodes into a streaming long-term memory (memory tree) used for future retrieval and dialog generation.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Small Village / social-simulation environments (not a text-adventure benchmark per se)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Multi-agent social simulation where agents maintain diaries/memories, plan daily activities, and interact with each other in a simulated town; tasks include scheduling, conversing, and behavior consistent with personalities.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term episodic memory (streaming, importance-scored summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>chunk-based entries with metadata, hierarchical memory tree (streaming tree of summaries and higher-level nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>summaries of observations, events, activities, relationships, and self-reflections</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>metadata weighting combining recency and importance; semantic search (embedding similarity) when retrieving relevant memories for dialogue or planning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>signal-triggered consolidation: when cumulative importance passes a threshold, the agent pauses to reflect and writes a summarized memory entry</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>maintain coherent long-term persona, enable believable dialog and planning, support role fidelity and emergent social behavior</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Importance scoring and periodic reflection/consolidation leads to more coherent, persistent agent behavior and better believability in social simulation; memory summaries permit retrieval that approximates human-like forgetting curves.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Consolidation policies and update frequency matter; naive storing of all events is infeasible — selective consolidation and adaptive forgetting are required. Scalability concerns for large agent populations and streaming updates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Streaming hierarchical memory trees with importance-based consolidation and metadata-weighted retrieval were highlighted as effective for maintaining coherent, retrievable long-term knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Large Language Model-Based Game Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2740.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2740.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AriGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AriGraph: Learning knowledge graph world models with episodic memory for LLM agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that encodes episodic experiences plus semantic facts into a knowledge-graph-style memory to support retrieval and interpretable world modeling for LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Arigraph: Learning knowledge graph world models with episodic memory for llm agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AriGraph-based agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM agent that converts episodic text chunks into a knowledge graph where nodes are entities/facts and edges capture relations; episodic leaves are linked and higher-level summaries are maintained for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Adventure/text environments (method described as applicable to world-modeling tasks such as text-based adventure)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Environments requiring structured world knowledge (entities, relations, preconditions/effects) to solve puzzles and long-horizon quests.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-structured long-term memory (episodic + semantic)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>knowledge graph where nodes are entities/facts and episodic chunks attach as leaves; supports multi-hop relations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>episodic experiences, semantic facts, extracted triplets (who-what-where relations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>graph-based retrieval: identify relevant nodes via semantic/lexical cues and traverse multi-hop neighborhoods to assemble subgraphs for the LLM</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>new experiences traverse the graph, merging with existing nodes or forming new branches (update mechanism varies across implementations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>provide structured, interpretable world knowledge for planning and to connect precondition-effect relations across episodes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Graph-structured memory yields an interpretable and retrievable representation of the environment that helps world-modeling for planning and long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Update mechanics (offline vs streaming vs batched) affect timeliness; graph extraction quality depends on reliable triplet/entity extraction from noisy observations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Knowledge-graph memory paired with episodic leaves and summary nodes recommended for interpretability and multi-hop retrieval, though exact best config depends on update strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Large Language Model-Based Game Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2740.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2740.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoryBank (Memory system for LLM agents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A long-term memory design that commits experiences based on salience/relevance thresholds and prioritizes retrieval of goal-relevant memories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoryBank (memory module used by LLM agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>External long-term memory module that scores experiences by relevance-to-goal and writes salient experiences into persistent storage; retrieval is relevance-scored at query time.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term episodic memory with importance-based consolidation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>chunk-based persistent store augmented with metadata (importance/salience scores)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>salient experiences, task-relevant events, metadata annotations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>metadata-based ranking (relevance to goal) when retrieving memories</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>commit experiences when their relevance to current goals exceeds a salience threshold</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>help agents focus retrieval on task-relevant past experiences and avoid storing low-value details</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Signal-triggered consolidation (salience threshold) helps maintain tractable memory that emphasizes goal-relevant content.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Choosing thresholds and salience metrics is nontrivial; poor salience estimation can either omit important experiences or bloat memory with noise.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Large Language Model-Based Game Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2740.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2740.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adapt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adapt: As-needed decomposition and planning with language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent methodology that performs on-demand decomposition of tasks into subgoals and tracks preconditions during execution, implicitly relying on short-term records of progress (memory of subgoals and preconditions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adapt: As-needed decomposition and planning with language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Adapt agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM planner that decomposes goals into subgoals as needed and monitors execution by tracking which preconditions are satisfied, effectively using working-memory-like summaries of subgoals.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>ALFWorld and other adventure-style environments referenced</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Long-horizon text/embodied tasks where success requires tracking preconditions, items, and multi-step recipes.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory / short-term subgoal buffer</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>subgoal chunks and precondition trackers stored in working buffer (prompt-level summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>current subgoals, tracked preconditions, recent progress summaries</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>use the working buffer (prompt) holding current subgoals when generating next actions</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>update subgoal buffer dynamically as preconditions are satisfied or tasks complete</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>keep plans aligned with evolving world state and enable coherent re-planning when branches fail</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Explicitly decomposing quests and tracking preconditions maintains alignment between plans and world state, improving ability to re-plan when branches fail.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Relies on prompt-level buffers (working memory) and thus faces capacity limits and drift without compression or consolidation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Large Language Model-Based Game Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2740.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2740.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SwiftSAGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SwiftSAGE: A generative agent with fast and slow thinking for complex interactive tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid 'fast and slow' agent that separates rapid reactive reasoning from slower reflective planning; it uses memory summarization to stabilize multi-step interactive behavior in complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SwiftSAGE agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-based agent implementing fast (reactive) and slow (reflective) thinking streams; slow stream writes summaries/reflections into memory to inform the fast loop.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Complex interactive tasks including ALFWorld and similar environments</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Multi-step interactive tasks requiring both quick local decisions and longer-term coherence across many steps.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hybrid working + reflective memory (summaries and subgoal chunks)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>streaming summaries and subgoal chunks (hierarchical-like), maintained to bridge fast and slow thinking</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>recent plans, reflections, subgoal summaries, and execution outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>bring slow-stream summaries into the fast stream when context-relevant; prioritize recent and important summaries</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>slow stream periodically consolidates recent events into summaries; working buffer updated continuously</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>stabilize behavior across long interactions and supply fast reactive loop with higher-level context</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Combining fast reactive decisions with slower reflective summaries helps maintain coherence in complex interactive tasks by preserving higher-level context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Coordination between fast and slow streams and deciding what to consolidate are open design questions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Large Language Model-Based Game Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2740.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2740.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KWM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KWM (task knowledge/world knowledge model - cited as KWM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that extracts task knowledge from expert demonstrations and distills a world-knowledge model to guide planning and future action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KWM (world knowledge model)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-augmented component that distills patterns from successful trajectories into a compact world-knowledge model used as a retrieval source during planning.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term world-knowledge memory (distilled from successful trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>dedicated distilled model or knowledge artifact (world knowledge model) rather than raw chunks</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>generalized task knowledge and common patterns distilled from expert trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>retrieve distilled task patterns or heuristics relevant to current planning context</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>learn from newly observed successful trajectories and integrate into the distilled model</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>provide reusable heuristics and task knowledge to guide planning and reduce need for repeated trial-and-error</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Distilling knowledge from successful trajectories yields reusable world knowledge that improves future planning and reduces repeated failures.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Quality depends on expert-trajectory coverage; distillation may omit rare but crucial corner-case knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Large Language Model-Based Game Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PokéLLMon: A grounding and reasoning benchmark for large language models in pokémon battles <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models. <em>(Rating: 2)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior. <em>(Rating: 2)</em></li>
                <li>Arigraph: Learning knowledge graph world models with episodic memory for llm agents. <em>(Rating: 2)</em></li>
                <li>MemoryBank: Enhancing large language models with long-term memory <em>(Rating: 2)</em></li>
                <li>Adapt: As-needed decomposition and planning with language models. <em>(Rating: 1)</em></li>
                <li>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2740",
    "paper_id": "paper-268856673",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing reasoning and acting in language models",
            "brief_description": "A prompting/agent paradigm that interleaves chain-of-thought style reasoning traces with environment actions to guide stepwise decision-making in interactive/text games.",
            "citation_title": "React: Synergizing reasoning and acting in language models.",
            "mention_or_use": "mention",
            "agent_name": "ReAct agent",
            "agent_description": "LLM-driven agent using few-shot prompting to alternate explicit reasoning (CoT) and action generation; LLM is central decision maker and is prompted to produce 'thought' and 'action' tokens in an interleaved loop.",
            "base_model_size": null,
            "game_benchmark_name": "Text-based games (e.g., ALFWorld, TextWorld mentioned)",
            "game_description": "Text-adventure / instruction-following environments where the agent issues textual commands and receives natural-language observations; tasks include navigation, object manipulation, and multi-step household tasks.",
            "uses_memory": false,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Survey notes that simple ReAct-style interleaving of reasoning and actions is insufficient by itself for long-horizon text-adventure tasks because agents commonly fail to maintain an accurate record of explored state and become stuck.",
            "memory_limitations": "Does not provide explicit long-term memory or consolidation; in-episode CoT reasoning can drift and lacks persistence across episodes.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2740.0",
            "source_info": {
                "paper_title": "A Survey on Large Language Model-Based Game Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "An agent framework that produces self-reflections (self-critiques) on failed attempts and uses those reflections to modify subsequent behavior, effectively creating a reflective memory trace across episodes.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning.",
            "mention_or_use": "mention",
            "agent_name": "Reflexion agent",
            "agent_description": "LLM-based agent that records outcomes of actions and generates textual self-analyses which are then stored and consumed in future planning steps to avoid repeating mistakes.",
            "base_model_size": null,
            "game_benchmark_name": "ALFWorld (and other interactive/text environments referenced)",
            "game_description": "Embodied instruction-following / text grounding environments where the agent executes sequences of textual actions to complete household or procedural tasks.",
            "uses_memory": true,
            "memory_type": "episodic / reflective memory (summary-of-failures and reflections)",
            "memory_structure": "textual summaries / reflection records appended to an episodic buffer (chunk-like entries)",
            "memory_content": "self-critiques, failure analyses, summaries of previous attempts and corrections",
            "memory_capacity": null,
            "memory_retrieval_strategy": "retrieve recent or relevant reflections when planning new attempts (temporal/importance bias implied)",
            "memory_update_strategy": "write reflection summaries after failed attempts or at episode boundaries (pause-and-reflect mechanism)",
            "memory_usage_purpose": "avoid repeating past mistakes, incorporate lessons from failures into future planning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Writing and storing self-critiques enables agents to extract insights from errors and avoid repeating them, turning episodic failures into persistent corrections of world knowledge.",
            "memory_limitations": "Reflection tends to be coarse-grained (post-hoc summaries) rather than providing step-level dense feedback; may not provide fine-grained credit assignment for intermediate reasoning steps.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2740.1",
            "source_info": {
                "paper_title": "A Survey on Large Language Model-Based Game Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "PokéLLMon",
            "name_full": "PokéLLMon: A grounding and reasoning benchmark for large language models in Pokémon battles",
            "brief_description": "A benchmark and agent work studying LLMs playing turn-based Pokémon Battles; identifies short-term decision inconsistency and proposes active maintenance (Last-Thoughts) to improve stability.",
            "citation_title": "PokéLLMon: A grounding and reasoning benchmark for large language models in pokémon battles.",
            "mention_or_use": "mention",
            "agent_name": "PokéLLMon agent (LLM-based Pokémon battle agent)",
            "agent_description": "An LLM-driven agent (evaluated with GPT-4o in the survey's Table 2) that generates battle actions; evaluated with and without intermediate reasoning (CoT) and with active maintenance strategies (Last-Thoughts).",
            "base_model_size": null,
            "game_benchmark_name": "Pokémon Battles (PokéLLMon benchmark)",
            "game_description": "Turn-based battle environment where each player selects moves or switches per turn; requires grounding to game mechanics (type matchups, move effects) and short-horizon consistency across turns.",
            "uses_memory": true,
            "memory_type": "working-memory active maintenance + external game-knowledge retrieval",
            "memory_structure": "in-prompt carryover of previous reasoning traces (Last-Thoughts) and retrieval from external knowledge (game mechanics) when needed",
            "memory_content": "most-recent chain-of-thought trace, previous action, known game rules/type-effect knowledge",
            "memory_capacity": null,
            "memory_retrieval_strategy": "carry forward last-step reasoning into next prompt (temporal continuity); retrieve external game knowledge by lookup when relevant",
            "memory_update_strategy": "update per turn by including previous 'thought' in the next prompt (Last-Thoughts); external knowledge static or retrieved on demand",
            "memory_usage_purpose": "stabilize short-term decision-making, reduce consecutive illogical switches, ground actions in game-specific knowledge",
            "performance_with_memory": "Last-Thoughts (GPT-4o): Win Rate 46.67% ; Switch Rate 0.2227 ; Consecutive Switch Rate 0.0861 (Table 2)",
            "performance_without_memory": "Baseline LLM (GPT-4o) without Last-Thoughts: Win Rate 42.17% ; Switch Rate 0.3356 ; Consecutive Switch Rate 0.2442 (Table 2). CoT and SC-CoT variants reported intermediate values in Table 2.",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Active maintenance (carrying previous reasoning into the next step) substantially reduces short-term inconsistency (consecutive switch rate) and improves win rate; chain-of-thought increases stochasticity and can worsen immediate consistency unless paired with continuity mechanisms like Last-Thoughts or self-consistency voting.",
            "memory_limitations": "Intermediate chain-of-thought increases stochastic divergence across steps; without active maintenance, reasoning tokens introduce cumulative randomness and short-term action instability.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "Last-Thoughts (explicit carryover of previous reasoning) outperformed CoT-alone and Self-Consistency CoT in reducing consecutive-switch behavior and improving win rate because it anchors current decisions to prior deliberation.",
            "uuid": "e2740.2",
            "source_info": {
                "paper_title": "A Survey on Large Language Model-Based Game Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GenerativeAgents",
            "name_full": "Generative Agents: Interactive simulacra of human behavior",
            "brief_description": "A cognitive-architecture style system that scores incoming observations by recency/importance, consolidates salient events into long-term memory summaries, and retrieves them to drive believable agent behavior in simulated towns.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior.",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents",
            "agent_description": "Population of LLM-based agents augmented with memory modules that score importance, reflect, and summarize episodes into a streaming long-term memory (memory tree) used for future retrieval and dialog generation.",
            "base_model_size": null,
            "game_benchmark_name": "Small Village / social-simulation environments (not a text-adventure benchmark per se)",
            "game_description": "Multi-agent social simulation where agents maintain diaries/memories, plan daily activities, and interact with each other in a simulated town; tasks include scheduling, conversing, and behavior consistent with personalities.",
            "uses_memory": true,
            "memory_type": "long-term episodic memory (streaming, importance-scored summaries)",
            "memory_structure": "chunk-based entries with metadata, hierarchical memory tree (streaming tree of summaries and higher-level nodes)",
            "memory_content": "summaries of observations, events, activities, relationships, and self-reflections",
            "memory_capacity": null,
            "memory_retrieval_strategy": "metadata weighting combining recency and importance; semantic search (embedding similarity) when retrieving relevant memories for dialogue or planning",
            "memory_update_strategy": "signal-triggered consolidation: when cumulative importance passes a threshold, the agent pauses to reflect and writes a summarized memory entry",
            "memory_usage_purpose": "maintain coherent long-term persona, enable believable dialog and planning, support role fidelity and emergent social behavior",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Importance scoring and periodic reflection/consolidation leads to more coherent, persistent agent behavior and better believability in social simulation; memory summaries permit retrieval that approximates human-like forgetting curves.",
            "memory_limitations": "Consolidation policies and update frequency matter; naive storing of all events is infeasible — selective consolidation and adaptive forgetting are required. Scalability concerns for large agent populations and streaming updates.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "Streaming hierarchical memory trees with importance-based consolidation and metadata-weighted retrieval were highlighted as effective for maintaining coherent, retrievable long-term knowledge.",
            "uuid": "e2740.3",
            "source_info": {
                "paper_title": "A Survey on Large Language Model-Based Game Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "AriGraph",
            "name_full": "AriGraph: Learning knowledge graph world models with episodic memory for LLM agents",
            "brief_description": "A system that encodes episodic experiences plus semantic facts into a knowledge-graph-style memory to support retrieval and interpretable world modeling for LLM agents.",
            "citation_title": "Arigraph: Learning knowledge graph world models with episodic memory for llm agents.",
            "mention_or_use": "mention",
            "agent_name": "AriGraph-based agent",
            "agent_description": "LLM agent that converts episodic text chunks into a knowledge graph where nodes are entities/facts and edges capture relations; episodic leaves are linked and higher-level summaries are maintained for retrieval.",
            "base_model_size": null,
            "game_benchmark_name": "Adventure/text environments (method described as applicable to world-modeling tasks such as text-based adventure)",
            "game_description": "Environments requiring structured world knowledge (entities, relations, preconditions/effects) to solve puzzles and long-horizon quests.",
            "uses_memory": true,
            "memory_type": "graph-structured long-term memory (episodic + semantic)",
            "memory_structure": "knowledge graph where nodes are entities/facts and episodic chunks attach as leaves; supports multi-hop relations",
            "memory_content": "episodic experiences, semantic facts, extracted triplets (who-what-where relations)",
            "memory_capacity": null,
            "memory_retrieval_strategy": "graph-based retrieval: identify relevant nodes via semantic/lexical cues and traverse multi-hop neighborhoods to assemble subgraphs for the LLM",
            "memory_update_strategy": "new experiences traverse the graph, merging with existing nodes or forming new branches (update mechanism varies across implementations)",
            "memory_usage_purpose": "provide structured, interpretable world knowledge for planning and to connect precondition-effect relations across episodes",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Graph-structured memory yields an interpretable and retrievable representation of the environment that helps world-modeling for planning and long-horizon tasks.",
            "memory_limitations": "Update mechanics (offline vs streaming vs batched) affect timeliness; graph extraction quality depends on reliable triplet/entity extraction from noisy observations.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "Knowledge-graph memory paired with episodic leaves and summary nodes recommended for interpretability and multi-hop retrieval, though exact best config depends on update strategy.",
            "uuid": "e2740.4",
            "source_info": {
                "paper_title": "A Survey on Large Language Model-Based Game Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "MemoryBank",
            "name_full": "MemoryBank (Memory system for LLM agents)",
            "brief_description": "A long-term memory design that commits experiences based on salience/relevance thresholds and prioritizes retrieval of goal-relevant memories.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "MemoryBank (memory module used by LLM agents)",
            "agent_description": "External long-term memory module that scores experiences by relevance-to-goal and writes salient experiences into persistent storage; retrieval is relevance-scored at query time.",
            "base_model_size": null,
            "game_benchmark_name": null,
            "game_description": null,
            "uses_memory": true,
            "memory_type": "long-term episodic memory with importance-based consolidation",
            "memory_structure": "chunk-based persistent store augmented with metadata (importance/salience scores)",
            "memory_content": "salient experiences, task-relevant events, metadata annotations",
            "memory_capacity": null,
            "memory_retrieval_strategy": "metadata-based ranking (relevance to goal) when retrieving memories",
            "memory_update_strategy": "commit experiences when their relevance to current goals exceeds a salience threshold",
            "memory_usage_purpose": "help agents focus retrieval on task-relevant past experiences and avoid storing low-value details",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Signal-triggered consolidation (salience threshold) helps maintain tractable memory that emphasizes goal-relevant content.",
            "memory_limitations": "Choosing thresholds and salience metrics is nontrivial; poor salience estimation can either omit important experiences or bloat memory with noise.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2740.5",
            "source_info": {
                "paper_title": "A Survey on Large Language Model-Based Game Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Adapt",
            "name_full": "Adapt: As-needed decomposition and planning with language models",
            "brief_description": "An agent methodology that performs on-demand decomposition of tasks into subgoals and tracks preconditions during execution, implicitly relying on short-term records of progress (memory of subgoals and preconditions).",
            "citation_title": "Adapt: As-needed decomposition and planning with language models.",
            "mention_or_use": "mention",
            "agent_name": "Adapt agent",
            "agent_description": "LLM planner that decomposes goals into subgoals as needed and monitors execution by tracking which preconditions are satisfied, effectively using working-memory-like summaries of subgoals.",
            "base_model_size": null,
            "game_benchmark_name": "ALFWorld and other adventure-style environments referenced",
            "game_description": "Long-horizon text/embodied tasks where success requires tracking preconditions, items, and multi-step recipes.",
            "uses_memory": true,
            "memory_type": "working memory / short-term subgoal buffer",
            "memory_structure": "subgoal chunks and precondition trackers stored in working buffer (prompt-level summaries)",
            "memory_content": "current subgoals, tracked preconditions, recent progress summaries",
            "memory_capacity": null,
            "memory_retrieval_strategy": "use the working buffer (prompt) holding current subgoals when generating next actions",
            "memory_update_strategy": "update subgoal buffer dynamically as preconditions are satisfied or tasks complete",
            "memory_usage_purpose": "keep plans aligned with evolving world state and enable coherent re-planning when branches fail",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Explicitly decomposing quests and tracking preconditions maintains alignment between plans and world state, improving ability to re-plan when branches fail.",
            "memory_limitations": "Relies on prompt-level buffers (working memory) and thus faces capacity limits and drift without compression or consolidation.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2740.6",
            "source_info": {
                "paper_title": "A Survey on Large Language Model-Based Game Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "SwiftSAGE",
            "name_full": "SwiftSAGE: A generative agent with fast and slow thinking for complex interactive tasks",
            "brief_description": "A hybrid 'fast and slow' agent that separates rapid reactive reasoning from slower reflective planning; it uses memory summarization to stabilize multi-step interactive behavior in complex tasks.",
            "citation_title": "Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks.",
            "mention_or_use": "mention",
            "agent_name": "SwiftSAGE agent",
            "agent_description": "LLM-based agent implementing fast (reactive) and slow (reflective) thinking streams; slow stream writes summaries/reflections into memory to inform the fast loop.",
            "base_model_size": null,
            "game_benchmark_name": "Complex interactive tasks including ALFWorld and similar environments",
            "game_description": "Multi-step interactive tasks requiring both quick local decisions and longer-term coherence across many steps.",
            "uses_memory": true,
            "memory_type": "hybrid working + reflective memory (summaries and subgoal chunks)",
            "memory_structure": "streaming summaries and subgoal chunks (hierarchical-like), maintained to bridge fast and slow thinking",
            "memory_content": "recent plans, reflections, subgoal summaries, and execution outcomes",
            "memory_capacity": null,
            "memory_retrieval_strategy": "bring slow-stream summaries into the fast stream when context-relevant; prioritize recent and important summaries",
            "memory_update_strategy": "slow stream periodically consolidates recent events into summaries; working buffer updated continuously",
            "memory_usage_purpose": "stabilize behavior across long interactions and supply fast reactive loop with higher-level context",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Combining fast reactive decisions with slower reflective summaries helps maintain coherence in complex interactive tasks by preserving higher-level context.",
            "memory_limitations": "Coordination between fast and slow streams and deciding what to consolidate are open design questions.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2740.7",
            "source_info": {
                "paper_title": "A Survey on Large Language Model-Based Game Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "KWM",
            "name_full": "KWM (task knowledge/world knowledge model - cited as KWM)",
            "brief_description": "A system that extracts task knowledge from expert demonstrations and distills a world-knowledge model to guide planning and future action generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "KWM (world knowledge model)",
            "agent_description": "An LLM-augmented component that distills patterns from successful trajectories into a compact world-knowledge model used as a retrieval source during planning.",
            "base_model_size": null,
            "game_benchmark_name": null,
            "game_description": null,
            "uses_memory": true,
            "memory_type": "long-term world-knowledge memory (distilled from successful trajectories)",
            "memory_structure": "dedicated distilled model or knowledge artifact (world knowledge model) rather than raw chunks",
            "memory_content": "generalized task knowledge and common patterns distilled from expert trajectories",
            "memory_capacity": null,
            "memory_retrieval_strategy": "retrieve distilled task patterns or heuristics relevant to current planning context",
            "memory_update_strategy": "learn from newly observed successful trajectories and integrate into the distilled model",
            "memory_usage_purpose": "provide reusable heuristics and task knowledge to guide planning and reduce need for repeated trial-and-error",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Distilling knowledge from successful trajectories yields reusable world knowledge that improves future planning and reduces repeated failures.",
            "memory_limitations": "Quality depends on expert-trajectory coverage; distillation may omit rare but crucial corner-case knowledge.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2740.8",
            "source_info": {
                "paper_title": "A Survey on Large Language Model-Based Game Agents",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PokéLLMon: A grounding and reasoning benchmark for large language models in pokémon battles",
            "rating": 2,
            "sanitized_title": "pokéllmon_a_grounding_and_reasoning_benchmark_for_large_language_models_in_pokémon_battles"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models.",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior.",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Arigraph: Learning knowledge graph world models with episodic memory for llm agents.",
            "rating": 2,
            "sanitized_title": "arigraph_learning_knowledge_graph_world_models_with_episodic_memory_for_llm_agents"
        },
        {
            "paper_title": "MemoryBank: Enhancing large language models with long-term memory",
            "rating": 2,
            "sanitized_title": "memorybank_enhancing_large_language_models_with_longterm_memory"
        },
        {
            "paper_title": "Adapt: As-needed decomposition and planning with language models.",
            "rating": 1,
            "sanitized_title": "adapt_asneeded_decomposition_and_planning_with_language_models"
        },
        {
            "paper_title": "Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks.",
            "rating": 1,
            "sanitized_title": "swiftsage_a_generative_agent_with_fast_and_slow_thinking_for_complex_interactive_tasks"
        }
    ],
    "cost": 0.02393575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Large Language Model-Based Game Agents
3 Nov 2025</p>
<p>Sihao Hu 
Tiansheng Huang 
Gaowen Liu 
Cisco Research 
Usa Ramana 
Rao Kompella 
Yichang Xu 
Ling Liu </p>
<p>Georgia Institute of Technology
USA</p>
<p>Georgia Institute of Technology
USA</p>
<p>FATIH ILHAN
Georgia Institute of Technology
USA, USA</p>
<p>SELIM FURKAN TEKIN
Georgia Institute of Technology
USA</p>
<p>Georgia Institute of Technology
USA</p>
<p>ZACHARY YAHN
Georgia Institute of Technology
USA</p>
<p>Georgia Institute of Technology
USA</p>
<p>A Survey on Large Language Model-Based Game Agents
3 Nov 2025B420A03DCB7E61D879781D95D0899FA2arXiv:2404.02039v4[cs.AI]
Game environments provide rich, controllable settings that simulate many aspects of real-world complexity.As such, game agents offer a valuable testbed for exploring capabilities relevant to Artificial General Intelligence[176].Recently, the emergence of Large Language Models (LLMs) provides new opportunities to endow these agents with generalizable reasoning, memory, and adaptability in complex game environments.This survey offers an up-to-date review of LLM-based game agents (LLMGAs) through a unified reference architecture.At the single-agent level, we synthesize existing studies around three core components: memory, reasoning, and perception-action interfaces, which jointly characterize how language enables agents to perceive, think, and act.At the multi-agent level, we outline how communication protocols and organizational models support coordination, role differentiation, and large-scale social behaviors.To contextualize these designs, we introduce a challenge-centered taxonomy linking six major game genres to their dominant agent requirements, from low-latency control in action games to open-ended goal formation in sandbox worlds.A curated list of related papers is available at: https://github.com/git-disl/awesome-LLM-game-agent-papers.</p>
<p>Introduction</p>
<p>By scaling model capacity and training on massive, diverse text corpora, large language models (LLMs) have demonstrated strong capabilities in language understanding, knowledge generalization, and conversational dialogue [5,18,107].Despite these advances, current LLMs are primarily optimized on fixed, static text corpora [105].Human intelligence, in contrast, develops through continuous sensorimotor engagement with the environment [131], for example, by forming perceptual representations from repeated interactions that capture the structure and dynamics of the world [13], and by adjusting behavior in response to feedback from action outcomes that gradually improves performance [30].In general, the literature on embodied cognition emphasizes that human intelligence arises from situated interaction with the environment rather than from disembodied symbol manipulation [29,131,150].</p>
<p>Unlike humans, LLM-based agents lack a physical body, making deep participation in realworld interactions difficult and costly.In contrast, game environments provide a natural testbed for realizing the coupling between agent and environment, and offer a richer, more embodied alternative compared to typical settings of current LLM-based agents, such as dialogue, web navigation, or API tool use [155].By granting avatars to agents in the interactive world with perception and action modules, digital games approximate aspects of real-world while remaining safe, controllable, and cost-effective.In addition, they are reproducible and span a wide range of complexity, making them an effective platform for advancing LLMs toward interactive intelligence.</p>
<p>Traditional game agents follow a control-based paradigm, where decision-making is coupled through predefined or learned state-action mappings [176].Finite state machines, behavior trees, and reinforcement learning agents [70,125,138] exemplify this design.In contrast, language serves as a unified medium for LLM-based agents to represent goals, contexts, and interactions, enabling explicit reasoning, reflection, and communication beyond traditional systems.</p>
<p>Existing surveys [48,155] touch on the topic from different angles yet largely treat games as a downstream application alongside dialogue, tool use, or web automation, leaving the field of LLMbased game agents (LLMGAs) underexplored.The complexity and openness of game environments distinguish them from narrowly defined tasks.For instance, while a web-based agent may complete a query or transaction through a handful of API calls, a sandbox game enables researchers to cultivate entire agent societies and allows agents to freely explore, interact, and build within physics-driven worlds.These environments afford a degree of freedom that enables emergent behaviors far beyond constrained, task-oriented interactions.On the other hand, game-focused surveys [47,139] emphasize areas such as game development, educational applications, or content generation.They neither examine the design challenges specific to LLMGAs nor explore the role of games as environments for advancing interactive intelligence.As a result, a dedicated survey of LLMGAs as a distinct research area is still missing.</p>
<p>Scope and Contributions.To fill this gap, we present this survey with two main objectives.First, we categorize existing LLMGA studies under a unified reference architecture, which integrates two complementary parts: an LLMGA framework that enables component-level analysis of a single agent, and a multi-LLMGA framework that captures communication and organization within populations of agents.The LLMGA framework abstracts common choices into three modules: memory system, reasoning mechanism, and perception-action interface, each associated with a fundamental challenge.For instance, within the memory system, working memory faces limitations of capacity and temporal consistency, while long-term memory centers on when and what observations to consolidate and how to structure them for effective retrieval.The multi-LLMGA framework provides a complementary perspective that examines how agents communicate, coordinate, and self-organize under constraints such as partial observability, limited bandwidth, and evolving social dynamics.It distinguishes between agent-level communication, which governs message generation, interpretation, and belief alignment, and organization-level structure, which shapes topology, role assignment, and collective stability.</p>
<p>Second, we introduce a challenge-centered taxonomy that maps six representative game genres [81,135] to the distinct demands they impose on agent design.For example, role-playing games center on the problem of role fidelity, i.e., how to encode and maintain consistent personas in memory so that dialogue and actions remain aligned with character identity over extended interactions.These genre-challenge mappings offer a structured lens on prior work and practical guidance for developing future LLMGAs.The broader aim of this survey is to position game environments as experimental grounds for examining whether sustained interaction between agents and their environments can foster more general and adaptive forms of intelligence.</p>
<p>This survey focuses exclusively on LLM-based agents in game environments.We include papers that (i) employ an LLM or MLLM as a central decision-making component and (ii) involve interaction with a game or game-like environment.We exclude both traditional non-LLM game AI approaches (e.g., deep reinforcement learning, symbolic systems) and LLM agents applied in non-game domains such as dialogue, web navigation, or API tool use.To construct the paper corpus, we searched four sources: ACM Digital Library, IEEE Xplore, Google Scholar, and arXiv-for the period 1 Jan 2018 to 31 Jul 2025 using the Boolean string: ("large language model" OR LLM) AND (game OR environment).Duplicates and irrelevant studies were removed, and additional works were identified through citation snowballing.Given the rapid pace of this field, we also maintain a curated and continuously updated collection of relevant literature at https://github.com/git-disl/awesome-LLMgame-agent-papers.Multi-LLMGA framework that extends the architecture to populations of agents, including the communication protocol that governs message exchange and the organizational structure that determines topology, task allocation, and role differentiation.</p>
<p>Overview</p>
<p>LLM-based Game Agent (LLMGA) Framework</p>
<p>Cognitive science views intelligence as an integrated system in which perception-action, memory, and reasoning processes interact to produce adaptive behavior [80,104].In line with this view, we find that existing studies on LLMGAs primarily introduce techniques that fall into three components: memory, reasoning, and perception-action [63,109,178].Building on this perspective, we categorize existing LLMGA studies under a unified framework that instantiates these cognitive principles through the three components.Figure 1(a) illustrates the overall architecture: a central LLM connects the three components in continuous interaction with the game environment.At each step of gameplay, the environment evolves and produces new observations, which the agent perceives, interprets, and acts upon, completing a closed perception-action loop.</p>
<p>The perception interface transforms these observations into representations that the LLM can interpret [97].In Section 5, we discuss how different modalities of observations, including textual, symbolic, and visual inputs, are handled by the agent.</p>
<p>The memory system provides a temporal mechanism that links past, present, and future, allowing information to persist across time and guide ongoing decisions.Following classic distinctions in cognitive psychology [11,12], we divide it into working memory and long-term memory.Working memory offers a short-term buffer that supports immediate processing and coordination across steps, with technical considerations centered on extending its capacity and maintaining consistency over time.Long-term memory, by contrast, accumulates knowledge and experience across episodes.In Section 3, we will focus on how to decide when and what to consolidate from transient experiences into long-term memory, and how stored content can be structured and retrieved.</p>
<p>Building on observations and memories, the reasoning mechanism defines how the LLM generates reasoning traces, such as plans, explanations, or self-critiques, that guide action proposals [129,166,178].In cognitive science, reasoning is understood as constructing and operating on Table 1.Gameplay taxonomy: game genres, core challenges, and representative environments.</p>
<p>Genre Core Challenge Representative Environments</p>
<p>Action games Low-latency control Atari 2600 games [2]; Procgen [32]; ViZDoom [77]; DeepMind Lab [15]; Street Fighter [106] Adventure games Stateful world modeling TextWorld [33]; Jericho [56]; ALFWorld [1]; ScienceWorld [157]; Red Dead Redemption II [140] Role-playing games Role fidelity AvalonBench [88]; Werewolf [174]; Diplomacy [42]; Pokémon [76];</p>
<p>Strategy games Opponent-aware planning Chess/Go [44,144]; Poker [54,65];</p>
<p>Pokémon Battles [63]; StarCraft II [97] Simulation games Real-world fidelity</p>
<p>Generative Agents [109]; Humanoid Agents [164]; AgentSims [91]; LyfeGame [74]; CivRealm [114]; Artificial Leviathan [35] Sandbox games Open-ended goal progression Minecraft [101]; MineDojo [43]; Crafter [55] internal representations to draw inferences beyond the given information [41,72].In Section 4, we outline two complementary approaches: prompting strategies, which elicit diverse reasoning paths at inference time, ranging from single linear chains to multiple parallel explorations and iterative refinements; Training paradigms, which improve reasoning ability by learning from expert demonstrations and from trial-and-error interaction with the environment.Finally, the action interface functions as the agent's hand and foot, translating language-based action proposals into concrete interactions with the environment [154].In Section 5, we discuss how high-level, free-form language decisions are transformed into executable behaviors, including constrained natural language commands, symbolic actions, and sequences of low-level controls.These actions in turn alter the game state, producing new observations and completing the cycle of interaction.</p>
<p>Multi-LLMGA Framework</p>
<p>Building on the single-agent framework, the multi-agent framework introduces an additional layer of complexity: agents not only interact with the environment but also with each other.Such settings naturally call for mechanisms of coordination and communication [67,167].Compared to generic LLM-based multi-agent systems, game environments impose additional constraints such as partial observability, limited communication bandwidth, and the need to preserve realistic gameplay boundaries, making their design challenges distinct.To analyze how existing works address these challenges, we consider two complementary dimensions of multi-agent design, as shown in Figure 1 (b).</p>
<p>At the agent level, the communication protocol specifies how information flows between peers and how it is integrated into ongoing cognition.Directly transmitting raw observations is often overwhelming and noisy.Therefore, messages should be filtered and abstracted into higher-level forms such as beliefs or intentions.Upon receiving a message, an agent must reconcile the new content with its own memory and internal state, particularly when inconsistencies arise.</p>
<p>At the organizational level, the organizational structure governs how a collection of agents functions as a coherent system.Topology determines the pattern of connections, centralized, decentralized, hierarchical, or partitioned, that constrain how decisions propagate and where authority resides.Task and role differentiation, whether predefined, dynamically reassigned, or emergent through interaction, dictates the division of labor that underpins efficiency and adaptability.Finally, scalability and stability mechanisms determine whether the system can sustain large populations in practice and prevent the collective from collapsing into incoherence or disorder.</p>
<p>Game Taxonomy for LLMGA Design</p>
<p>The way a game agent is designed cannot be isolated from the environment in which it operates: Different game genres foreground distinct capabilities and place different challenges on agent design.For example, action games like Street Fighter demand far quicker reactions than strategy games like Poker, while requiring much less reasoning.Therefore, a taxonomy that captures how these characteristics shape agent design is essential.</p>
<p>Clarke et al. [31] critically examine how conventional video game genre classifications often mix orthogonal dimensions such as mechanics and player structures, thereby lacking conceptual clarity.Building on this insight, we ground our taxonomy in established game studies literature through a gameplay-oriented perspective, drawing on the top-level groupings from SteamDB [135] and the classification proposed by Lee et al. [81].To maintain coherence with existing LLMGA studies, we merge narrower categories (e.g., driving/racing, fighting) and additionally include sandbox games, resulting in six major genres as depicted in Figure 1.Building on this categorization, we further introduce a challenge-centered view, where each genre is linked to the core design challenge that most strongly drives agent development.</p>
<p>As shown in Table 1, we identify six representative game genres, each posing distinct design challenges for LLM-based agents.(1) Action games [2,106] unfold in real time and emphasize reflexive control, such as aiming, dodging, or chaining combos under tight temporal constraints.The core challenge is low-latency control, which shapes agent design by requiring fast action and hybrid architectures that reconcile LLM reasoning with frame-level responsiveness; (2) Adventure games [56,157] emphasize exploration and long-horizon quests, where progress depends on remembering locations, items, and unresolved preconditions.The challenge is stateful world modeling, pushing agents to develop memory structures that maintain coherent records of evolving environments and dependencies; (3) Role-playing games [42,174] center on character embodiment, where players assume predefined roles with distinct traits and narrative trajectories.The key challenge is role fidelity, shaping agent design toward embedding role profiles into memory and reasoning so that dialogue and actions remain persona-consistent over extended horizons; (4) Strategy games [62,97] involve multi-step planning against adaptive adversaries, ranging from fully observable board games to imperfect-information settings with hidden states.Their central challenge is opponent-aware planning, which requires agents to integrate multi-step reasoning with theory-of-mind style opponent modeling; (5) Simulation games [91,109] approximate real-world or systemic processes, from individual social life to the evolution of societies.The challenge is realworld fidelity, shaping agent design to ensure that behaviors remain credible and human-like rather than drifting into unrealistic patterns; (6) Sandbox games [55,101] offer open-ended environments where players set their own objectives, explore, and build.The challenge is open-ended goal progression, which drives designs where agents can generate self-directed goals, decompose them hierarchically, and accumulate reusable skills to sustain long-term play.</p>
<p>3 Memory System of LLMGA LLMGAs require memory systems that encode and retain prior experience to ensure coherent and efficient interaction.Following classic distinctions in cognitive psychology [11,12], we conceptualize an agent's memory as two complementary components: working memory and long-term memory.</p>
<p>In cognitive psychology, working memory functions as a transient and limited-capacity buffer that temporarily stores and manipulates information needed for ongoing cognitive processing [11,12].In LLMGAs, this role is fulfilled by the model's short context window and auxiliary mechanisms that keep recent observations "in mind" [124].For working memory, we examine three key mechanisms.The first is context extension, which enlarges the effective context window so that recent events can be accommodated within short-term processing.The second is memory compression, which condenses lengthy inputs into compact representations, reducing capacity limits while preserving essential content.The third is active maintenance, which explicitly preserves recent bindings, plans, and intermediate states, preventing short-term drift and inconsistency caused by temporal decay.</p>
<p>In contrast, long-term memory refers to the durable store of information that persists over extended periods and can be retrieved to guide future behavior [134,149].It enables the accumulation of experience and knowledge that extend beyond the limited span of working memory [134].In LLMGAs, long-term memory is primarily realized through external storage systems that persist across interactions, such as vector databases, knowledge graphs, or serialized logs that record and retrieve past experience [109].In addition, long-term memory can also be embedded within the parameters of the model itself, encoding generalized knowledge and experience that can be implicitly retrieved during generation [127].For long-term memory, we introduce mechanisms that enable agents to persist and exploit information across episodes.The first is memory consolidation, which decides when and what to commit from working memory to durable storage.The second is memory structuring, which determines how stored content is organized to facilitate abstraction and efficient access.The third is memory retrieval, which reactivates relevant past knowledge so that prior experience can inform ongoing decision-making.Figure 2 presents the structure of this section of different components within the memory system.Fig. 2. Overview of the memory system of LLMGAs.</p>
<p>Working Memory</p>
<p>As shown in Figure 4, recent studies can be grouped into three categories based on functionality.First, capacity extension enlarges the effective span of working memory by expanding positional encodings or restructuring attention.Second, information refinement distills lengthy or redundant input into more salient representations, mirroring the cognitive process of recoding multiple stimuli into higher-order units to overcome capacity limits [34].Finally, active maintenance explicitly preserves variable bindings and states over short time scales, mirroring the human use of rehearsal to prevent rapid forgetting and inconsistency due to temporal decay [12,34].</p>
<p>Context Extension.Context refers to the input tokens that the LLM can access when generating a new token, that is, the range of preceding text the model can attend to during generation, which is bounded by its context length [18].To overcome this, recent research focuses on extending the effective scope of the context window without full retraining.</p>
<p>In LLM, position refers to the relative order of tokens within this context, typically represented through positional encodings or embeddings that allow the model to distinguish token order in a sequence [151].Position-based techniques leverage adjustments in positional encoding to allow extrapolation to much longer sequences.One of the earliest, Position Interpolation (PI) rescales Rotary Position Embedding (RoPE) [136] positional indices linearly, allowing models to handle up to ~32K tokens with minimal fine-tuning and maintaining performance on shorter inputs [25].Building on this, YaRN (Yet another RoPE extensioN) introduces nonlinear mappings that require only a small fraction of data (~0.1% of original pre-training) to support context lengths up to 128K tokens [110].More recently, LongRoPE further pushes context windows up to 2 million tokens through progressive fine-tuning and intelligent RoPE scaling strategies [37].</p>
<p>In addition to positional interpolation, another line of work extends effective context length by restructuring how attention is computed over long sequences.Parallel Context Windows (PCW) divides inputs into disjoint segments with shared embeddings, enabling off-the-shelf LLMs to process texts beyond their native window without additional training [119].Similarly, PoSE introduces a skip-wise positional encoding scheme that allows models trained with short contexts to generalize to longer sequences while reducing memory overhead [196].Together, these methods demonstrate that segmenting and coordinating attention can serve as a practical alternative to purely extending positional encodings.</p>
<p>Memory Compression.LLMs often struggle to juggle large amounts of information simultaneously.Experiments using the n-back paradigm show that performance deteriorates sharply as the number of items increases, resembling the human short-term memory limit in which accuracy drops abruptly once n-back exceeds three or four [34,50].To address this bottleneck, recent work has developed techniques that refine long inputs into compact, salient representations, thereby reducing redundancy while preserving essential information.</p>
<p>One line of research focuses on soft token compression, which introduces a small set of trainable tokens to stand in for much longer text spans [49,82,103].By attaching lightweight learned parameters [57,86], the model conditions on these compact tokens instead of repeatedly processing the entire sequence.For example, AutoCompressor produces summary vectors segment by segment through an unsupervised objective [27]; the In-Context Autoencoder transforms lengthy documents into dedicated "memory slots" [49]; GIST modifies the attention mask so that the model learns to compress an entire prompt into a few gist tokens, trainable virtual tokens inserted between the prompt and the input that encode the essential information of the prompt for reuse instead of reprocessing the entire text [103].</p>
<p>A complementary direction is summarization, which organizes long contexts into multi-level abstractions.For example, the chain-of-summarization approach [97] incrementally condenses game-state trajectories by segmenting the temporal sequence into short windows and recursively summarizing them into higher-level representations.This hierarchical compression enables the model to retain the strategic context of long games while keeping the effective input size within the context window.Methods such as NUGGET cluster adjacent tokens into higher-level semantic "nuggets", compact representations that compress contiguous text segments for efficient retrieval and long-context reasoning [116].WDMM constructs a memory tree and traverses it iteratively to surface only the most relevant segments [22].These approaches echo the cognitive strategy of chunking, in which humans reduce information load by recoding multiple stimuli into higher-order units, thereby overcoming the intrinsic limits of working memory [34].</p>
<p>Active Maintenance.In cognitive psychology, the persistence of working memory is constrained by rapid decay and interference.Active maintenance refers to keeping the contents of working memory available over short intervals to preserve continuity in reasoning and action [12,34,100].Basic LLM game agents face an analogous problem: they "forget" what just happened and acted even though the historical events are included in the context window.A motivating case comes from the PokéLLMon paper [62].As shown in Figure 3, LLMs exhibit action inconsistency, such as switching Pokémon in consecutive turns instead of attacking when facing powerful opponents.The inconsistency becomes even more pronounced when chain-of-thought [166] (CoT) reasoning is adopted, as shown in Table 2, where the switch rate measures the overall frequency of switching actions, and the consecutive switch rate specifically counts switches made in successive turns, an indicator of short-term instability in decision-making.From the perspective of generation, reasoning introduces cumulative stochasticity that can lead to divergent decisions.Self-Consistency CoT (SC-CoT) [160] attempts to mitigate this inconsistency by applying majority voting across reasoning paths in every step.A simple and effective alternative, termed Last-Thoughts [62], explicitly carries the reasoning trace (the thought from the previous step) into the next prompt, ensuring that the model's decision remains anchored to its prior deliberation.This lightweight continuity mechanism substantially reduces the consecutive switch rate and improves overall win rate, as shown in Table 2.A related approach is belief-state maintenance [83]: agents explicitly summarize their current understanding of the environment as a belief state, and then feed it into subsequent steps, which has been shown to improve consistency and collaboration in multi-agent tasks.</p>
<p>Beyond prompt-level carryover, active maintenance can also be realized through other mechanisms.MEM1 [195] employs a reinforcement-learning-based controller that updates a compact shared memory state at every step, retaining salient information while discarding redundancy.HiAgent [59] manages the working-memory buffer as subgoal chunks, dynamically overwriting completed chunks with concise summaries to ensure that only the most relevant reasoning traces remain active without relying on retrieval.</p>
<p>Long-Term Memory</p>
<p>Recent agent architectures emphasize three fundamental processes in the design of long-term memory systems.First, memory consolidation determines when and what to commit from transient buffers to durable storage, often triggered by event boundaries, importance scoring, or successful event execution [109,194].Second, memory structuring addresses how stored content is organized, whether as raw chunks, key-value stores, hierarchical trees, knowledge graphs, or even implicit parametric memories fine-tuned into the model [10,154].Finally, memory retrieval specifies how past knowledge is re-activated to guide ongoing decision-making, leveraging metadata filtering, semantic search, or traversal of graph/tree structures [85,109].These components together ensure that long-term memory effectively archives past experience and supports future behavior.</p>
<p>Memory Consolidation.In cognitive psychology, the transfer of information from working memory into long-term memory is termed memory consolidation, a selective process that determines which experiences persist beyond the immediate moment [11,134].For LLMGAs, the analogous process is to decide when and what to commit from transient buffers to durable storage so that memory remains useful and tractable.</p>
<p>A common paradigm is signal-triggered consolidation, where specific signals determine whether new information should be committed.In Generative Agents [109], each incoming observation is assigned an importance score by an LLM, and once the cumulative importance of recent events exceeds a threshold, the agent pauses to reflect, producing a summary that is then written into long-term memory.MemoryBank [194] applies a similar principle, committing experiences when their relevance to the goal surpasses a salience threshold.Voyager [154] instead uses task outcomes as signals: successful code executions are committed into a skill library, while failed attempts are excluded or down-weighted.</p>
<p>More recent works extend write-back into more flexible learning-based schemes.For instance, CoALA [137] models "learning" as an explicit internal action within the agent's action space, leaving it to the control policy (e.g., LLM) to decide when to encode new information into long-term memory.Self-Controlled Memory [152] introduces a trainable memory controller that adaptively decides whether to write or use memory at each step.The controller is optimized jointly with the LLM through task-level supervision, such that memory updates are triggered only when they improve downstream performance.</p>
<p>Memory Structuring.After deciding when to commit new information into long-term storage, an important design choice is how the memory is structured.Existing representative structures range from simple text fragments to highly organized graphs and implicit parametric storage, as shown in Figure 4.</p>
<p>The most direct approach is to store observations as chunks, a simple yet flexible unit for inserting new memories [109].To facilitate later retrieval, each chunk can be augmented with metadata such as timestamps, importance scores, or Q-values [188].Moving beyond raw fragments, many systems adopt a key-value representation, where keys encode identifiers or semantic descriptors, and values store the corresponding content.This allows fast lookups and supports multimodal inputs: for example, Voyager represents keys as program descriptions paired with code snippets as values [154], while JARVIS-1 stores visual observations as keys and successful execution plans as values [163].</p>
<p>To capture hierarchical relations, memories can be recursively clustered into a tree structure.Generative Agents [109], RAPTOR [122], and MemTree [121] all build memory trees where raw chunks form the leaves, and higher layers summarize increasingly abstract topics.Although the update mechanism differs (offline in RAPTOR, batched in Generative Agents, and streaming in MemTree), the underlying idea is to let new experiences traverse the tree, merging with existing nodes or forming new branches, while recursively updating parent summaries.</p>
<p>An alternative design is to use graph-structured memory.In knowledge graph approaches, nodes correspond to entities and edges correspond to semantic relations, typically extracted as triplets from text chunks, emphasizing fact representation [10,40,85].In contrast, A-MeM [173] organizes memory into a network of atomic notes enriched with tags and context, and edges represent semantic links between related notes, emphasizing interlinked note-taking and allowing updates to existing nodes.</p>
<p>Finally, some work explores parametric storage, where memory is encoded implicitly in the model's parameters rather than explicitly as external data.This perspective aligns with human cognition, which does not store verbatim text but instead internalizes experience.Fine-tuning on domain knowledge or episodic data can thus endow LLMs with embedded semantic or procedural memory [45,143].For instance, CharacterLLM fine-tunes on synthetic character experiences so that the resulting model can recall detailed knowledge of people, events, and objects in a role-consistent manner [127].</p>
<p>Memory Retrieval.Memory retrieval is the process of reactivating stored information to guide current reasoning and action, and is tightly coupled with the data structures used for storage.In cognitive psychology, retrieval has long been studied as a cue-driven process, often distinguished into recall, where information is reconstructed without external cues, and recognition, where cues assist in reactivating stored traces [8,148].Human studies also highlight that retrieval is selective and subject to recency, salience, and interference effects [34,39].These insights resonate with the design of LLMGAs, which rely on structured retrieval strategies to decide what portion of past experience should be brought back into working memory.</p>
<p>One common strategy is metadata retrieval, where each memory entry is annotated with auxiliary attributes such as timestamps, importance scores, or Q-values.During retrieval, agents rank memories using such metadata: for example, Generative Agents weight recency and importance to approximate the Ebbinghaus forgetting curve [109], while MemoryBank employs relevance scoring to prioritize salient experiences [194].REMEMBERER further records observation-action pairs with associated Q-values and retrieves both highly rewarded and strongly penalized experiences to guide behavior [188].</p>
<p>A second approach is semantic retrieval, where queries are embedded into a vector space and compared with stored representations.Generative Agents, for instance, compute cosine similarity between a self-instructed query and stored text memories [109].In key-value settings, similarity is measured between the query and the key, with the associated value returned.This design allows flexibility across modalities: Voyager retrieves executable code by comparing program descriptions [154], while JARVIS-1 retrieves action plans from multimodal keys that combine task descriptions and visual observations [163].</p>
<p>For more structured memories, retrieval can exploit graph or tree topologies.Graph-based retrieval begins by identifying relevant nodes using semantic or lexical cues, then traverses edges to explore multi-hop neighborhoods, finally synthesizing the resulting subgraph into a coherent narrative for the LLM to consume [10,85].Tree-based retrieval instead performs hierarchical traversal: starting from the root, the agent selects top- relevant nodes at each level based on similarity, gradually descending to finer-grained leaves.Some variants collapse the hierarchy into a flat pool of summaries and retrieve based purely on semantic similarity [121,122].</p>
<p>Finally, for parametric storage, knowledge is embedded implicitly in model weights rather than explicit structures.Such retrieval resembles implicit or procedural memory in humans, in which skills and habits are expressed without deliberate recall [127].</p>
<p>Table 3 summarizes representative LLMGAs by their memory design, showing the diversity of memory mechanisms across different game environments.</p>
<p>Reasoning of LLMGA</p>
<p>In cognitive science, reasoning is understood as the process of constructing and manipulating internal representations of known information to uncover implicit relations and abstract structures,  [174] Werewolf In-episode experience Reflective experience for retrieval PokéLLMon [61] Pokémon Battles Active maintenance (last-step thoughts) External game knowledge for retrieval TextStarCraft [97] StarCraft II Memory compression (chain-of-summarization) SuspicionAgent [53] Leduc Hold'em In-episode experience Reflection on previous episodes ProAgent [187] Overcooked-AI Active Maintenance (Intention and belief) Past experience for retrieval Voyager [154] Minecraft Short-term code feedback Successful code for retrieval GTIM [197] Minecraft Short-term action feedback Successful plan for retrieval JARVIS-1 [163] Minecraft Short-term situational context Successful multimodal plan for retrieval GenerativeAgents [109] Small Village Memory compression (tree-based reflection) Streaming memory with metadata E2WM [171] VirtualHome In-context dialogue Exploration experience for fine-tuning LLMPlanner [132] ALFRED In-episode experience Exemplar plan for retrieval CharacterLLM [127] Role-playing QA In-context dialogue Synthetic experience for fine-tuning thereby enabling conclusions that extend beyond what is explicitly given [41,72].In LLMGAs, reasoning serves as the central mechanism that transforms perceived and retrieved information into coherent plans, decisions, and explanations.It unfolds through language, by generating intermediate thought sequences that externalize internal deliberation and guide subsequent actions [79,166].</p>
<p>For instruction-guided reasoning, designed prompts elicit reasoning behavior directly at inference time.The first mechanism is chain-of-thought, which guides the model to articulate intermediate steps before arriving at an answer.The second is search-based reasoning, which explores multiple reasoning paths in parallel and selects among them to ensure consistency.The third is reflective reasoning, which iteratively improves reasoning across steps by incorporating internal self-critique or external signals.</p>
<p>For fine-tuning paradigms, reasoning abilities are improved through optimization on data or experience interacted with the game environments.The first mechanism is supervised finetuning, where agents imitate expert demonstrations to acquire reasoning behaviors.The second is reinforcement learning, which updates policies or value models to optimize reasoning with task rewards.The third is preference optimization, which contrasts preferred and dispreferred generations to bias reasoning toward desirable outcomes.Figure 5 presents the structure of this section of different components within the reasoning mechanism.</p>
<p>Instruction-Guided Reasoning</p>
<p>Prior studies have demonstrated that reasoning abilities can be elicited and amplified by deliberate prompting strategies at inference time, which guide models to externalize intermediate steps rather than relying solely on direct answer generation [79,166].We categorize existing methods into three groups.Chain-of-thought prompting elicits a single linear reasoning path, but is prone to error propagation.Search-based reasoning mitigates this by generating and organizing multiple trajectories to enhance robustness.Reflective reasoning emphasizes temporal refinement, where reasoning is iteratively improved using signals from prior experience or the environment.Chain-of-Thought.CoT [166] is the basic approach that prompts LLMs to conduct intermediate reasoning before generating the answers, as shown in Figure 6.Since generation can be seen as an auto-regressive process of searching the next token in the latent space, the introduction of intermediate reasoning enhances the ability to traverse greater distances in that latent space, making LLMs capable of addressing more complex tasks.The ReAct [178] agent interleaves CoT reasoning and actions using few-shot prompting in text-based games.In their approach, reasoning acts as a mechanism for the agent to periodically check its task progress and plan its next steps.</p>
<p>Intermediate reasoning introduces additional stochasticity, which can lead to inconsistent outputs.For instance, in Pokémon Battles, CoT may cause agents to panic-switch Pokémon in consecutive turns [62], as shown in Figure 3.Moreover, once an early step deviates, subsequent tokens may inherit and magnify the error [98].Self-Refine [98], GPTLens [60] and RCI [78] aim to mitigate error propagation through self-criticism, first generating reasoning thoughts and then evaluating and refining them to improve the reasoning generation.</p>
<p>Search-based Reasoning.A major limitation of single-path chain-of-thought is fragility: randomness in sampling may yield inconsistent outputs, and early errors can propagate through the chain [98,160].Search-based methods mitigate this by generating multiple intermediate reasoning candidates and then selecting, aggregating, or revising them.As shown in Figure 6, Self-Consistency [160] alleviates inconsistency by prompting LLMs to generate multiple chains of thoughts independently, and conduct majority voting on the final answer to find the most consistent reasoning path.Tree-of-Thoughts [3] focuses on preventing error propagation by proposing multiple intermediate thoughts and selecting the correct one.Specifically, it decomposes a task into multiple steps, generates candidate thoughts for each step, and selects the most promising one, making the reasoning process resemble traversing a tree of thoughts.Extending this idea, Graph-of-Thoughts [17] aggregates thoughts across different reasoning paths, converting a tree structure into a directed acyclic graph (DAG).SPRING [170] constructs a template DAG in which each node corresponds to a question or instruction used to prompt LLMs for progressive reasoning.In their study, the authors prompt LLMs to summarize the Crafter paper [55] into a DAG and then progressively traverse the DAG to answer these questions, thereby guiding the model through a step-by-step reasoning process.</p>
<p>Reflective Reasoning.Unlike generic LLM agents often evaluated on single-turn tasks, game agents operate within an observation-action-feedback loop, continuously perceiving the environment, taking actions, and adjusting decisions based on the resulting outcomes.Reflective reasoning builds on this loop by allowing agents to analyze the outcomes of their own actions and incorporate these reflections into future reasoning and behavior, as Reflexion [129] shown in Figure 6.This introduces a temporal dimension to reasoning, enabling the integration of experience over time.</p>
<p>Studies have shown that such temporal interaction enables LLMGAs to evolve over time by integrating feedback from past trajectories.The most direct form is reflection on failure: when an action fails, the agent can reuse the error signal to avoid repeating the same mistake.For instance, environments may provide explicit feedback such as "I cannot make a stone shovel because I need 2 more sticks" in MineCraft, which agents like Voyager [154] and GTIM [197] exploit to iteratively refine their plans.Beyond explicit signals, reflective mechanisms such as Reflexion [129], DEPS [162], and ProAgent [187] guide agents to analyze their own chain-of-thought traces, identify where reasoning went wrong, and incorporate these insights into subsequent decisions.Even in environments with sparse feedback, agents can still benefit from heuristic signals [129].</p>
<p>In addition to learning from failures, reflective reasoning can also benefit from reflecting on successes.Successful trajectories not only consolidate effective strategies but also provide contrastive signals when compared against failures.ExpeL [191] leverages this idea by retrieving the most relevant successful experiences, summarizing common patterns, and deriving insights through success-failure comparisons.Similarly, KWM [115] extracts task knowledge from expert-demonstrated trajectories and distills it into a dedicated world knowledge model, which is then used to guide the agent's planning in future episodes.In summary, reflective reasoning shares the basic idea of reinforcement learning that uses feedback to correct mistakes and reinforce successful strategies, embodying the principle of learning through interaction with the environment.</p>
<p>Fine-tuning for Improving Reasoning</p>
<p>In this subsection, we examine fine-tuning techniques for optimizing reasoning and action generation.Based on the training strategy, existing methods can be grouped into three categories.Supervised fine-tuning learns from expert trajectories to imitate reasoning and action generation.Reinforcement learning updates policies with reward feedback, reinforcing reasoning and actions that lead to favorable outcomes.Preference optimization leverages comparisons between better and worse trajectories to align models without the need for explicit reward models.It is worth noting that some methods mentioned below optimize only the final action without explicit reasoning, however, they can be extended to improve reasoning by eliciting chain-of-thought, allowing reasoning to be shaped through its effect on action outcomes [73].</p>
<p>Supervised Fine-Tuning.Supervised fine-tuning trains LLM agents on collected trajectories to maximize the likelihood of reproducing demonstrated reasoning and actions.The most common approach is behavior cloning, where agents directly imitate expert demonstrations.Such trajectories may come from human experts [120], from state-of-the-art agents [90], or from teacher LLMs that generate rollouts for training student models [21,184].Behavior cloning is widely adopted as an initialization strategy, providing a strong prior policy that can later be refined by reinforcement learning [4,133].</p>
<p>Building on this idea, rejection sampling fine-tuning introduces a selection stage before training.Instead of imitating all trajectories, the model generates multiple candidates and filters them according to predefined criteria, such as binary success/failure signals or reward estimates.RFT [183], for example, fine-tunes models only on successful trajectories, while other works employ environmentprovided or model-estimated rewards to guide sample selection [145].Although this improves data quality, it can be inefficient when the agent initially produces few successful rollouts.</p>
<p>Reinforcement Learning.Reinforcement learning (RL) provides another major paradigm for improving reasoning and action generation in LLM agents.Existing game agents [4,20,38,141] mainly adopt the Proximal Policy Optimization (PPO) algorithm [123], where the model is trained as a policy  (  |   ) (without explicit reasoning) and updated using advantage-weighted gradients to favor actions leading to higher rewards.Alongside the policy model, PPO also learns a value function to estimate the relative quality of state-action pairs.While effective, applying RL to LLMs faces the challenge of an enormous generation space, which often leads to inadmissible actions.To address this, some methods compute the probability distribution of admissible actions by the chain rule before sampling, ensuring that the generated actions remain valid [20,141].</p>
<p>Recent works further integrate explicit reasoning into RL training, where the LLM is trained as a policy  (  ,   |   ).Reinforced Fine-Tuning (ReFT) [146] introduces chain-of-thought supervision into PPO, encouraging the model to generate reasoning paths that lead to correct answers.However, because reasoning tokens are often much longer than action tokens, naive optimization can overweight reasoning relative to actions.Zhai et al. [185] propose downscaling the likelihood of reasoning steps, showing that moderate scaling achieves better balance between planning and acting.Beyond policy optimization, value-based methods such as Q-learning extend RL to reasoning by treating partial generations as states and token expansions as actions.This formulation allows the use of search algorithms, such as Best-of-N sampling or Monte Carlo tree search, to evaluate and expand reasoning paths guided by the Q-function [92,153].</p>
<p>A challenge is that conventional reward signals (and the value estimates derived from them) are provided only at the action level, providing no feedback on the intermediate reasoning steps.This causes error to propagate through the reasoning until the final outcome is known.To address this limitation, Process Reward Modeling (PRM) [89] supplies dense feedback by explicitly evaluating intermediate reasoning steps.</p>
<p>Preference Optimization.The idea of preference optimization was first explored in games, where OpenAI demonstrated that human preference comparisons could be used to train reward models for Dota 2 [28].This principle of optimizing agents by favoring trajectories preferred by humans rather than relying on hand-crafted rewards later became the foundation for aligning language models.Building on this, Direct Preference Optimization (DPO) [118] enables contrastive training without an explicit reward model by maximizing the margin between preferred and non-preferred generations, thereby simplifying the optimization process and reducing cost.In the context of game agents, this preference-based framework can also be applied at the trajectory or step level: ETO [133] alternates between exploration and fine-tuning with DPO on successful vs. failed rollouts, while IPR [172] extends this to step-wise preference optimization, pairing reasoning steps according to the average reward calculated via Monte Carlo method.</p>
<p>In Table 4, we list representative LLMGAs by their reasoning mechanism design, aligned with the two dimensions of our categorization.</p>
<p>Perception and Action Interfaces of LLMGA</p>
<p>LLMGAs differ from generic LLM systems in that they operate within a continuous perceptionaction loop.To support this loop, agents rely on perception and action interfaces that serve as their eyes and hands for interacting with the environment [63,154].On the input side, the perception interface determines how raw game states are abstracted into representations that can be processed by the LLM, handling textual, symbolic, and visual observations.On the output side, the action interface ensures that the model's decisions are translated into admissible in-game operations by grounding the LLM outputs into high-level, low-level, and code-based actions.Figure 7 outlines the structure of this section.</p>
<p>Perception Interface</p>
<p>The perception interface defines how an LLMGA accesses and processes information from the game environment.The most direct and widely adopted way to categorize input-processing methods is based on the modality of the game observation, such as textual, symbolic, or visual forms.CoT Reflexion [129] ALFWorld, etc.</p>
<p>CoT + Reflective reasoning ADAPT [111] ALFWorld, etc.</p>
<p>As-needed CoT (planning) SwiftSAGE [90] ScienceWorld As-needed CoT (planning) ETO [133] ALFWorld, etc.</p>
<p>Trajectory-level preference optimization IPR [172] ALFWorld, etc.</p>
<p>Step-level preference optimization GLAM [20] BabyAI-Text RL fine-tuning TWOSOME [141] Overcooked-AI RL fine-tuning Xu et al. [174] Werewolf Reflective reasoning Xu et al. [175] Werewolf RL-based candidate selection Thinker [169] Werewolf RL-guided dialogue generation ReCon [159] Avalon Theory-of-mind reasoning CodeAct [128] Avalon Reasoning as code generation WarAgent [64] Diplomacy-like Structural reasoning PokéLLMon [63] Pokémon Battles Search-based reasoning ChessGPT [44] Chess Supervised fine-tuning PokerGPT [65] Texas Hold'em RL from human feedback SuspicionAgent [53] Leduc Hold'em Theory-of-mind reasoning HLA [93] Overcooked As-needed CoT (planning) S-Agents [23] Minecraft Goal decomposition, evaluation HAC [192] Minecraft Goal decomposition, correction, evaluation Voyager [154] Minecraft Code as policy, correction DEPS [162] Minecraft Goal decomposition, reflection, selection GTIM [197] Minecraft Goal decomposition, correction JARVIS-1 [163] Minecraft Goal decomposition, reflection Plan4MC [181] Minecraft Goal decomposition RL-GPT [95] Minecraft Reasoning as code generation LLaMARider [45] Minecraft Novelty-driven Supervised fine-tuning Project Sid [7] Minecraft Social awareness reasoning GenerativeAgents [109] Sims-like game Tree-based reflection &amp; planning HumanoidAgents [164] Social Affective-driven planning LLMPlanner [132] ALFRED Planning &amp; re-planning Octopus [4] OctoVerse Reasoning as code generation RL fine-tuning ELLM [38] Crafter Situated goal generation SPRING [170] Crafter Structural reasoning</p>
<p>Perception &amp; Action Interfaces  Textual Observations.In text-based or dialogue-centric games [1,68,174], the environment state is natively presented in natural language.In such cases, the agent can directly consume text descriptions as observations without additional preprocessing [129,178].This modality is straightforward, as it aligns with the input format of LLMs, but it is restricted to environments where language is the primary medium of interaction.</p>
<p>Symbolic Observations.Some video game environments provide structured state information through APIs or game engines [63,87,97,101].These symbolic variables (e.g., avatar health, inventory, world coordinates or object properties) can be transformed into a form that the LLM can process, often through textual summaries or structured prompt templates.For example, Mineflayer [112] exposes a Minecraft character's stats and surrounding entities, which can then be summarized into a natural-language prompt [63].Symbolic observations are efficient when the selected variables can sufficiently capture the essential context, but they risk losing fidelity in complex environments where subtle but critical distinctions, such as object textures, spatial relations, or small visual cues, are omitted from the symbolic representation.</p>
<p>Visual Observations.In video games, the agent typically perceives the environment as a sequence of rendered images.Since LLMs cannot directly operate on raw pixels, the perception interface requires a translator that converts visual signals into interpretable representations.One approach is vision-to-text translation, where object detectors or pretrained encoders such as CLIP [117] produce captions or object lists that can be inserted into prompt templates.For example, an agent in a 3D environment can use an object detector to list visible objects ("a key on the floor, a locked door ahead") and is inserted into the prompt template [132,189].The agent can also adopt a visual encoder to map images into pre-defined text descriptions [38,162,163], or a text decoder to generate the caption [38,102] to summarize the scene.</p>
<p>An alternative is to use multimodal LLMs to directly process raw frames.These models align visual and textual information in a shared representation space, allowing an agent to feed raw images or pixels to the model and get an immediate understanding.Recent works [36,140,186] leverage general-purpose multimodal LLMs (e.g., GPT-4 Vision [5]) to interpret game visuals.This direct approach can generalize well to new games, but still requires additional mechanisms to correct errors or uncertainties in its perceptions [4,140].Game-specific multimodal models have also been introduced, e.g., LLMs finetuned on paired image-instruction data for a particular game, such as SteveEye [193] or learned from environmental feedback through RL such as Octopus [4].</p>
<p>Action Interface</p>
<p>The action interface determines how an LLM-based agent's decisions are grounded into executable operations within the game environment.Unlike generic LLM outputs that produce unconstrained text, games require actions that conform to specific control formats.Accordingly, action interfaces are categorized by the type of action required by games: high-level actions represent semantic or logical operations (e.g., "open the door"); low-level actions specify concrete control signals such as keystrokes or mouse movements; and programmatic actions output structured commands or API calls that the environment can directly execute.</p>
<p>High-Level Actions.In games where actions are expressed as discrete choices [62,65], the generation problem can be reformulated as a selection task.In this case, the model can simply select one of the provided options as the action.In parser-based environments, such as text adventure games or interactive narratives [56,99], the LLM must generate a command that follows specific syntax, such as "open the door" or "pick up the sword".Outputs that deviate from the expected syntax are treated as invalid and ignored.Therefore, the core challenge is to ensure that output actions are admissible.Recent work has introduced correction mechanisms, such as mapping generated phrases to the closest permissible action [66].An alternative is constrained decoding: instead of unconstrained token-by-token decoding, it computes the joint likelihood of each valid action sequence using the chain rule, and then normalize across the entire action set [20].However, such token-level probabilities penalize longer commands disproportionately, leading to systematic bias against valid but longer actions.To mitigate this problem, TWOSOME [141] introduces length normalization by scaling log-likelihoods with the action's token count, thereby balancing the probability distribution over admissible actions.</p>
<p>Low-Level Actions.Low-level actions operate at the control layer, such as keystrokes, mouse movements, joystick inputs, and are executed at each timestep.A low-level controller (policy) is responsible for translating a high-level action from the LLM into a sequence of control signals.</p>
<p>One approach is heuristic planning [6,93,109]: given an intent such as "chop a tree," the system invokes a path planner (e.g., A * ) to locate the nearest tree and then issues the necessary movement and interaction commands [197].Another approach is to learn a low-level controller (policy) that generates the required action sequences to realize the LLM's high-level decisions.Such policies can be trained either through imitation learning from expert demonstrations or through reinforcement learning with environment feedback, often aided by goal-conditioned rewards or semantic similarity between goals and observed transitions [95].</p>
<p>Code-based Actions.Code-based actions express agent decisions as structured code or API calls that can be executed directly in the environment [140,154].Their structured nature provides explicit semantics and eliminates ambiguity, allowing complex operations to be specified with precision (e.g., bot.equip(sword); through a modding API [112] or key_press("M") at the system level).A further advantage is verifiability: code outputs can be parsed and checked before execution, and compilers or interpreters supply syntax feedback that enables automatic detection and correction of invalid commands [154].In addition, programmatic actions support reusability by enabling agents to maintain a library of high-level primitives that encapsulate recurring skills.These functions can be flexibly composed, reducing redundant low-level generation and facilitating scalable, compositional behavior [140].</p>
<p>Table 5 lists representative LLMGAs, categorized by their perception and action interfaces.Textual input High-level action SwiftSAGE [90] ScienceWorld Textual input High-level action Cradle [140] RDR2 Visual input (MLLM) Low-level action (via keyboard-mouse control APIs) Xu et al. [174] Werewolf Textual input High-level action ReCon [159] Avalon Textual input High-level action CodeAct [128] Avalon Text input Code-based action PokéLLMon [63] Pokémon Battles Symbolic input High-level action TextStarCraft [97] StarCraft II Symbolic input Low-level action (rule-based controller) ChessGPT [44] Chess Symbolic input High-level action PokerGPT [65] Texas Hold'em Symbolic input High-level action SuspicionAgent [53] Leduc Hold'em Symbolic input High-level action ProAgent [187] Overcooked-AI Symbolic input Low-level action (via path search + API calls) TWOSOME [141] Overcooked-AI Symbolic input High-level action (admissible action generation) Voyager [154] Minecraft Symbolic input Code-based action (via Mineflayer code execution) GTIM [197] Minecraft Symbolic input Low-level action (via API calls) JARVIS-1 [163] Minecraft Visual and symbolic input Low-level action (via controller and API calls) CoELA [189] TDW-T&amp;WAH Visual input (object detector) Low-level action (via rule-based controller) GenerativeAgents [109] Small Village Textual input High-level actions ZeroShotPlanner [66] VirtualHome Symbolic input High-level actions (semantic translation) ELLM [38] Crafter Visual input (visual encoder) Low-level action (RL-based controller)</p>
<p>6 Multi-LLMGA Framework In this section, we extend the single agent framework to multi-agent settings.Designing a multiagent system in games is different from generic multi-agent systems because games impose unique constraints: states are partially observable, communication channels are often bandwidth-limited, and in certain scenarios direct memory sharing is disallowed to preserve realistic simulation [189].</p>
<p>To analyze how existing work addresses these challenges, we distinguish two complementary dimensions.</p>
<p>At the agent level, we examine how agents exchange information and integrate it into their decision-making.Communication protocols specify what messages are generated (e.g., observations, beliefs, or intentions) and how they are interpreted by receivers.At the organization level, we study three aspects: the topology of connections that shape communication flow, the allocation of tasks and roles that governs functional division of labor, and the mechanisms for ensuring scalability and robustness as groups expand.Figure 8 presents the structure of this section of different components within the multi-LLMGA system.Fig. 8. Overview of the multi-LLMGA framework.</p>
<p>Multi-Agent Framework</p>
<p>Communication Protocol</p>
<p>In game and simulation environments, communication is likely constrained by partial observability, limited bandwidth, and asynchronous execution [189], which makes communication protocol design crucial for coordination.A communication protocol defines the rules that regulate peerto-peer information exchange at the agent level, which specifies what message the sender should share, and how it is integrated by the receiver.</p>
<p>Message Generation.</p>
<p>Senders determine what type of information is worth exchanging, which can be broadly categorized into three classes: The first is observation, referring to the raw and local signals each agent perceives from the environment.Observations are typically partial, such as perceiving only a limited visual field in environment [189], sharing observations allows teammates to directly expand each other's perceptual fields.Since raw perceptual input is often redundant or low-value, practical systems [189] apply summarization to compress observations into compact, salient statements.The second is belief, which represents an agent's internal inference or probability distribution over the hidden state of the world, based on its own observations and prior knowledge [6,187].Compared to raw observations, beliefs provide higher-level interpretations.For example, an agent may observe scattered leaves and tree trunks, and infer that the environment contains sufficient wood resources nearby.The third is intention, where agents communicate their planned actions or subgoals.Intention propagation is especially important in tasks that require complementary execution to reduce redundant effort (e.g., multiple agents pursuing the same subtask) and prevent conflicts (e.g., two agents competing for the same resource) [6,189].In addition, when there is no communication mechanism/channel available, agents need to infer collaborators' hidden intentions based on their actions observed.</p>
<p>Message Interpretation.Once communication takes place, agents need to integrate the exchanged information into their memory and ongoing decision process.In general, received messages can be directly adopted to guide actions.However, inconsistencies may arise when the new information conflicts with an agent's existing internal state.To address this, agents must reconcile external messages and internal models.For instance, ProAgent [187] infers the belief of a partner through the reasoning of theory of mind and subsequently corrects its estimate when the partner's observed actions reveal mismatches.ReConcile [24] provides a debate-based approach by engaging agents in multi-round discussions, where they attempt to convince each other with corrective explanations and aggregate responses through confidence-weighted voting to reach consensus.ECON [180] models this reconciliation as a Bayesian game, where agents treat each other's beliefs and intentions as uncertain types and update them until they converge on a joint profile that all parties can consistently follow.</p>
<p>Organizational Structure</p>
<p>Organizational structure defines how agents are arranged and coordinated within a multi-agent system, including the topology of their connections, the allocation of tasks and roles, and the mechanisms that ensure scalability and stability as the population grows.</p>
<p>Organizational Topology.Organizational topology refers to the structural constraints that determine how decisions flow, how agents connect for communication, and where authority over world state resides.Rather than a free design choice, topology is an architectural constraint that shapes the trade-off between scalability, robustness, and latency [52].</p>
<p>Centralized organization rely on a single planner or coordinator to aggregate information and allocate subtasks.This design ensures strong consistency and efficiency but creates bottlenecks and single points of failure, which limit scalability.For example, MindAgent [51] adopts a single foundation model as the central dispatcher that issues the step-by-step commands to all agents.Decentralized organization remove central authority and let agents act based on local observations and peer communication.Such topology is robust and can avoid global bottlenecks, but can suffer from coordination conflicts and redundant actions.CoELA [189] follows this paradigm, framing cooperation as decentralized planning under costly communication channels.TeamCraft [96] also includes a decentralized setting where each agent needs to coordinate from partial observability.To balance coherence with local flexibility, hierarchical organizations introduce multiple layers of control, with higher-level agents assigning goals or subtasks and lower-level agents refining them layer-by-layer.HAS [192] exemplifies a three-tier hierarchy: a top-level manager sets global plans, intermediate conductors translate and distribute these plans, and bottom-level action agents execute concrete steps.Similarly, S-Agents [23] use a tree structure where a root node provides coordination and leaf nodes carry out subtasks.Partitioned or sharded systems divide persistent environments into regions, each governed by local authority with cross-shard coordination handled by bridging protocols.This design enables scalability and fault tolerance, but weakens global consistency.Project Sid [7] illustrates in a large-scale setting: thousands of Minecraft agents self-organize into civilizations where division of labor and institutions emerge, showing that centralized control is infeasible at such scale.</p>
<p>Task &amp; Role Allocation.Task and role allocation determines how subtasks are mapped to agents, shaping both efficiency and adaptability in multi-agent system.Allocation specifies the functional division of labor within the organizational topology.Three patterns are commonly observed: prefixed, dynamic, and emergent.</p>
<p>Prefixed allocation specifies roles or tasks in advance, often through a central planner or a leader.This ensures clear division of labor and prevents conflicts, making it reliable for structured environments but rigid under open-ended or rapidly changing conditions.MindAgent [51] follows this approach: a single foundation model centrally dispatches per-step actions for all agents, directly specifying each agent's next move.Similarly, S-Agents [23] predefine a root-leaf hierarchy, where the root serves as coordinator and leaves as executors, though the specific subtasks are still assigned dynamically during execution.TeamCraft [96] also provides prefixed task allocation in its expert demonstration data, where planners assign optimal actions to each agent.Dynamic allocation allows agents to determine their roles during execution, with assignments decided in real time by monitoring the environment or coordinating with peers.This increases adaptability and robustness but may produce redundancy when multiple agents converge on the same role.Overcooked-AI [19] illustrates this challenge, as frequent task changes require agents to split and reassign responsibilities on the fly.CoELA [189] provides another example, where decentralized agents negotiate via natural language under costly communication channels to decide which subtasks to pursue.HAS [192] also falls into this category: while roles such as manager and conductors are predefined, the system dynamically reorganizes action groups and reallocates responsibilities as tasks evolve.Emergent allocation does not predefine the set of roles but lets them arise through repeated interaction.At scale, Project Sid [7] demonstrates how thousands of Minecraft agents spontaneously differentiate into specialized professions such as farmers, miners, builders, and traders, stabilizing cooperation without central control.This diversification arises from social awareness, where agents adjust goals in response to others' activities, thereby reducing redundancy and enabling stable specialization.</p>
<p>Scalability &amp; Robustness.Scaling multi-agent systems beyond small groups remains challenging.Early studies such as Generative Agents typically support only dozens of agents, since agents execute cognition through a sequential pipeline with a single thread.This serialized design becomes the bottleneck for scaling [109].Project Sid addresses the per-agent bottleneck with the PIANO architecture, which runs six modules in parallel to update the agent state at different time scales.To prevent incoherence between simultaneous outputs, a cognitive controller [74] selects an option from the candidate outputs of concurrent modules and transmits this decision to other modules for execution.</p>
<p>During the emergence of roles, certain factors are critical for ensuring organizational stability.Project Sid [7] demonstrates that social awareness plays a critical role in sustaining division of labor: when agents observe many of their peers performing one task, they are more likely to select a different one.Through memory and repeated behavior, these roles become reinforced, allowing agents to form stable identities such as "farmer" or "miner" and yielding a more persistent specialization structure.In social simulation experiments, Artificial Leviathan [35] demonstrate that memory depth is the key factor for the emergence of a commonwealth (i.e., the rise of a sovereign), under which social disorder is significantly reduced.This suggests that memory acts as a stabilizing mechanism by turning short-term interactions into long-term understanding of agents' relative strengths and weaknesses, thereby forming group consensus.</p>
<p>Gameplay Taxonomy for LLMGA Design</p>
<p>The design of game agents is inseparable from the environments in which they operate: different genres foreground different capabilities, from fast perception-action cycles in action games to long-horizon planning in strategy games.A taxonomy that connects the properties of games with the demands they impose on agents is therefore valuable for this field.Here we adopt a challenge-centered game taxonomy: for each major category, we highlight the design challenge that most strongly drives LLMGA design.The genre axis itself draws on established categorizations, combining top-level groupings from SteamDB [135] with the gameplay-oriented classification of Lee et al. [81].To make the taxonomy more coherent to covered studies, we exclude narrower genres such as driving/racing or fighting, and instead introduce sandbox as a category to capture open-ended and emergent play, with Minecraft as the canonical example.</p>
<p>Building on this taxonomy, we sketch how different game genres map into distinct design challenges.Action games require low-latency control, where agents are challenged to reconcile the slow deliberation of language models with the frame-level demands of real-time play.Adventure games highlight stateful world modeling, where progress depends on maintaining coherent memories of evolving environments, quests, and object dependencies.Role-playing games raise the issue of role fidelity, in which agents are expected to sustain consistent personas and align dialogue and actions with character identity.Strategy games emphasize opponent-aware planning, where the key difficulty lies in anticipating and adjusting to adversaries' potential intentions under imperfect information.Simulation games emphasize real-world fidelity, evaluating whether agents can display behavior that remains credible rather than drifting into unrealistic patterns.Finally, sandbox games expose the challenge of open-ended goal progression, where agents are tasked with generating their own objectives, decomposing them hierarchically, and accumulating reusable skills to sustain long-term play.</p>
<p>Action Games: Low-Latency Control</p>
<p>Action games are characterized by real-time, time-critical interaction, where success hinges on executing precise movements such as aiming, dodging, or chaining combos within narrow temporal windows.This creates a fundamental demand for low-latency control, and the design challenge is therefore to reconcile the reasoning strengths of LLMs with the immediacy required by real-time gameplay.</p>
<p>Environments.Atari 2600 games in the Arcade Learning Environment [2] provide a canonical benchmark for reflexive control, where agents map raw pixel observations to joystick inputs at 60 Hz.Procgen [32] extends this setup with procedurally generated levels, requiring agents to generalize their responses across unseen layouts rather than memorizing fixed patterns.Moving into 3D, ViZDoom [77] and DeepMind Lab [15] present first-person 3D environments where perception is partial and high-dimensional, requiring agents to aim, strafe, and dodge in real time.Fighting games such as Street Fighter III [106] further sharpen the requirement for low-latency control: the timing of counters and combos is so precise that even minimal decision delays can flip the outcome of an exchange.</p>
<p>Methods.Across action game environments, a consistent finding is that LLMs alone cannot sustain frame-level decision speed.Evaluations of multimodal LLMs as low-level controllers in Atari 2600 games show that models fall far short of reinforcement learning agents and humans, often approaching random-play performance, primarily due to inference latency and limited visuospatial grounding [165].Similar evidence comes from latency-sensitive games such as Street Fighter, where empirical studies demonstrate that achieving competent play requires explicitly trading off reasoning quality for faster inference [75].To mitigate this bottleneck, researchers have adopted hybrid designs.One line of work delegates reflexive control to low-level policies trained through reinforcement or imitation learning, while reserving the LLM for high-level reasoning and strategy, as illustrated by two-tier agent systems in fighting games [158].Empirical studies further show that latency-sensitive environments such as Street Fighter expose a fundamental trade-off between reasoning quality and decision speed: deeper reasoning produces stronger local decisions but increases inference latency to the point of losing more frequently, while shallower reasoning improves responsiveness and overall win rates [75].In the recent Black Myth: Wukong, VARP samples frames at second-level intervals for multi-step action generation instead of conducting per-frame inference, thereby maintaining timely control under visually complex action settings [26].</p>
<p>Adventure Games: Stateful World Modeling</p>
<p>Adventure games are defined by partial observability and long-horizon quests: progress depends on remembering what has been explored, which preconditions of puzzles or storylines remain unsatisfied, and understanding how objects, actions, and rules of the world.For LLMGAs, this creates a fundamental demand: they should be able to record, update, and retrieve both the evolving environment state and the underlying knowledge of how these elements can be used or combined.Without such modeling, agents lose track of progress, repeat past actions, or fail to connect prerequisites with goals.Empirically, GPT-3.5 struggles to construct coherent maps in partially known text-adventure environments, and state-prediction benchmarks indicate that even stronger LLMs are unreliable as implicit world simulators [56,147].</p>
<p>Environments.Adventure game benchmarks such as TextWorld [33], Jericho [56], ALFWorld [1], and ScienceWorld [157] provide text-based environments in which players interact with the world through natural language, exploring rooms, collecting objects, and completing quests of varying complexity.For instance, TextWorld procedurally generates synthetic quests by varying the number of rooms, objects, and goals [99,182].Jericho includes 56 human-authored classics such as the Zork series [68,69] and Hitchhiker's Guide to the Galaxy [14].ALFWorld aligns to the embodied ALFRED benchmark [130], requiring agents to follow household instructions.ScienceWorld [157] simulates primary-school science curricula, highlighting basic knowledge from physics and chemistry in order to complete experiments.</p>
<p>Methods.Recent work has gradually converged on the view that memory should operate as the backbone of world modeling in adventure settings.Early agents such as ReAct [178] showed that simple interleaving of and actions is not sufficient, as the agent often fails to maintain an accurate view of the environment and becomes stuck.By incorporating reasoning, the agent can periodically summarize recent progress, ensuring that short-term records of explored locations, obtained items, and pending subgoals remain stable across steps.Reflexion [129] further demonstrates that writing self-critiques of failed attempts enables agents to extract insights from errors and avoid repeating them, thereby transforming episodic failures into persistent corrections of world knowledge.Subsequent agents, including Adapt [111] and SwiftSage [90] further explicitly decompose quests into subgoals and tracking preconditions during execution.This keeps plans aligned with an evolving world state and enables coherent re-planning when branches fail.KWM [115] leverages successful trajectories to learn a knowledge-augmented world model, allowing agents to internalize regularities of environment dynamics and use the world model to guide future planning.AriGraph [10] encodes episodic experiences alongside semantic facts in a knowledge-graph memory, yielding a retrievable and interpretable representation of the game environment.At a larger scale, Cradle [140] demonstrates the same principle in the visually rich adventure setting of Red Dead Redemption II, where the key difficulty lies in aligning perception with quest progress and narrative state.By maintaining memory as an explicit record of explored context and completed steps, Cradle enables the agent to keep exploration and story advancement coherent across long-horizon play, which stabilizes behavior in sprawling 3D environments.</p>
<p>Role-Playing Games: Role Fidelity</p>
<p>Role-playing games (RPGs) require players to assume pre-defined characters with distinct abilities, knowledge, experiences, and objectives.Although RPGs may also incorporate elements of action or adventure, our focus here is on a common characteristic that underpins this genre: role fidelity.Role fidelity means that agents should internalize their assigned role and generate dialogue and actions that remain consistent with the character's identity and capabilities.Failure to do so causes agents to lose consistency in speech and action, or even contradict their assigned role, undermining both immersion and gameplay effectiveness.</p>
<p>Environments.Social deduction board games provide natural testbeds for role fidelity.In Werewolf, each player receives a hidden role such as seer, guard, or werewolf, and must preserve secrecy while engaging in persuasion, deception, and coordinated voting [174].Similarly, Avalon assigns asymmetric roles with private knowledge (e.g., Merlin knowing the bad team), requiring agents to participate in multi-round discussions without revealing confidential information while still influencing team decisions [88].Negotiation games like Diplomacy, where each player embodies a nation with its own objectives [42], and scripted murder-mystery games such as Jubensha [168], reinforce the same demand: agents must consistently inhabit a pre-defined persona and objectives, balancing what to disclose and what to withhold across multiple turns to preserve immersion and effectiveness.</p>
<p>Classic RPGs also emphasize role fidelity through long-horizon progression.For example, in Pokémon Red, the trainer role requires remembering the current storyline position, the Pokémon owned, the items carried, and the towns and paths visited.PokéAgent introduces exploration tasks to test whether agents can remain coherent as trainers throughout the gameplay [76].Beyond the main character, role fidelity is even more critical for non-player characters (NPCs), which must sustain consistent personas across repeated interactions and emergent narratives, as exemplified by recent studies evaluating personality fidelity in role-playing [161].</p>
<p>Methods.The simplest approach adds the role card directly into the prompt, listing traits and goals as initial memory [109].While this establishes in-character openings, it quickly breaks down over multi-turn play, i.e., the role drift problem.Empirical studies show that in Avalon, LLMs may reveal their secret identity [88] or fail to sustain deception across rounds [159].To mitigate such inconsistencies, approaches condition generation on explicit intentions or structured reasoning: in Avalon, code-based reasoning constrains utterances to follow hidden-role logic [128], while in Diplomacy, Cicero anchors dialogue in private strategic plans to ensure alignment between language and action [42].These methods improve local consistency but are not designed to preserve longterm role fidelity.More recent approaches target role fidelity directly by integrating role profiles as a persistent component of the memory system.RoleLLM [156] introduces structured role memory that separates private belief states (e.g., hidden identities) from public discourse records, ensuring that agents regulate what to disclose versus conceal across turns.CharacterLLM [127] adopts parametric adaptation, fine-tuning LLMs on curated role-play data to internalize persona traits and generate consistent style and objectives without continual reminders.These frameworks shift the focus from dialogue-level consistency to persistent memory management.</p>
<p>Sandbox Games: Open-Ended Goal Progression</p>
<p>Sandbox games are characterized by open-ended environments and emergent play rather than fixed quests or roles.Players can freely explore, collect resources, and set their own objectives from survival to large-scale construction.For LLMGAs, this creates unique demands for both generating meaningful goals in the absence of external instructions and decomposing goals into actionable plans.Without such mechanisms, agents either become stuck in aimless wandering or fail to coordinate long-horizon plans into coherent progression.</p>
<p>Environments.Minecraft and Crafter are two sandbox games that have been widely studied for game agents.Minecraft [101] is a 3D sandbox game that offer players the great freedom to traverse a world made up of blocky, pixelated landscapes, facilitated by the procedurally generated worlds.The resource-based crafting system enables players to transform collected materials into tools, build elaborate structures and complex machines.Built on Minecraft, MineDojo [43] provides a large-scale research platform with thousands of open-ended tasks, multimodal data from community sources, and the MineCLIP reward model.Crafter [55] offers a lightweight 2D open-world environment with procedurally generated maps.It challenges players to manage their resources carefully to ensure sufficient water, food, and rest, while also defending against threats like zombies.</p>
<p>Methods.In sandbox settings, agents need to first determine what goals to pursue before they can decide how to achieve them.Existing works can be divided into two complementary directions.The first direction emphasizes goal generation through intrinsically motivated exploration.With LLMs, agents can propose adaptive goals conditioned on their current state, skills, and environment for curriculum learning.Voyager [154] exemplifies this idea by prompting an LLM to continually generate new objectives, building a self-directed curriculum and accumulating a library of reusable skills.OMNI [190] utilizes LLMs to determine interesting tasks for curriculum design, overcoming the previous challenge of quantifying "interestingness".ELLM [38] queries LLMs for next goals given an agent's current context, and rewards agents for accomplishing those suggestions in the sparse-reward setting; SPRING [170] uses LLMs to summarize useful knowledge from the Crafter paper [55] and progressively prompts the LLM to generate next action.</p>
<p>The second direction is hierarchical planning for task execution.Sandbox objectives such as constructing tools or building structures require agents to gather dispersed resources and follow multi-step recipes with strict dependencies.DEPS [162] introduces plan correction: the LLM generates candidate subgoals, monitors execution outcomes, and self-explains failures in order to iteratively repair its plans, while leaving the final action execution to goal-conditioned controllers.Subsequent work emphasized making planning more reusable.GITM [197] prompts LLM to decompose goals and retrieves external knowledge such as crafting recipes, while long-term memory preserves common subplans that can be reused across tasks.JARVIS-1 [163] extend this idea by integrating multimodal perception and memory, grounding subgoal generation in visual context.Later work such as Plan4MC [181] and RL-GPT [95] extend hierarchical planning by coupling high-level LLM planners with low-level controllers trained via reinforcement learning.Finally, multi-agent frameworks such as HAS [192] and S-Agents [23] extend hierarchical planning to cooperative settings, dispatching subgoals across multiple agents to parallelize progress on complex objectives.</p>
<p>Strategy Games: Opponent-Aware Planning</p>
<p>Strategy games span a spectrum of complexity, from turn-based, deterministic, perfect information game to real-time, stochastic imperfect information games.A common requirement is opponentaware planning: agents need to infer opponents' possible intentions and conduct multi-step planning conditioned on these possibilities.</p>
<p>Environments.Board games like Chess and Go are fully observable, where agents need to search vast move trees while anticipating optimal counter-moves [44,84,144].Pokémon battles [63] add uncertainty: players select moves or switches without knowing the opponent's choice, and success depends on exploiting type matchups.Poker, exemplified by Texas Hold'em, deals each player two private hole cards, followed by betting rounds as community cards are revealed.The winning strategy is not simply holding the best hand, but managing information asymmetry through bluffing, pot control, and reasoning about what cards the opponent may have [54,65].StarCraft II is a real-time strategy game where players collect resources, expand bases, build armies, and fight under the fog of war.Winning requires players to infer the opponent's strategy from limited scouting, adapt build orders and timing attacks accordingly, and still control units precisely in battle.For agents, the challenge is therefore twofold: modeling and planning against an adaptive opponent as in other strategy games, and at the same time coordinating across macro, tactical, and micro levels under strict temporal constraints [97,126].</p>
<p>Methods.In perfect-information games such as Chess and Go, opponent-aware planning reduces to deterministic search over long move sequences.ChessGPT [44] demonstrates that training on textual game corpora allows LLMs to evaluate positions and propose continuations, while blindfold-play studies [84,144] reveal that models can implicitly reconstruct board states from move sequences, approximating the effect of explicit lookahead search.In imperfect-information games, the challenge is reasoning over probability trees defined by partially observable states and uncertain opponent actions.Here, opponent modeling, often framed as theory-of-mind (ToM) thinking, is crucial.Suspicion-Agent [53] shows that prompting LLMs for higher-order ToM in Leduc Hold'em leads to more aggressive raises and fewer passive calls, improving long-term chip gains.PokéLLMon [63] shows that LLM agents are still vulnerable to human misdirection strategies exploiting their limited higher-order ToM.For instance, a player may bait the agent by sending out a seemingly weak Pokémon, then switch to an immune one just before the attack lands, causing the agent to waste its move.</p>
<p>Simulation Games: Real-World Fidelity</p>
<p>Simulation games approximate aspects of the real world, ranging from individual social life to large-scale civilizations.They are generally open-ended, allowing diverse trajectories and outcomes rather than fixed solutions.We therefore center this section on real-world fidelity, the extent to which an agent's behavior remains credible within the simulated dynamics.This requirement is especially salient in human and social simulations: the higher the fidelity, the more convincingly LLM-based architectures approximate human cognitive models.</p>
<p>Environments.Human simulation environments construct virtual societies for studying emergent social behavior.Generative Agents [109] places 25 agents in a sandbox town with cognitive modules for everyday interaction.Humanoid Agents [164] extends this setting by incorporating physiological needs, emotions, and relationship closeness.AgentSims [91] provides a programmable multi-agent framework, while LyfeGame [74] situates agents in a 3D virtual town for scenariodriven testing (e.g., school events, crises).More recent platforms such as Project Sid [7] scale to hundreds or thousands of agents in a Minecraft-based world, while Artificial Leviathan [35] creates a survival sandbox for exploring the emergence of social contracts and authority.Beyond human simulation, CivRealm [114] is a Civilization-style simulation environment focusing on the macro-scale evolution of societies across historical eras.</p>
<p>Methods.Maintaining real-world fidelity in simulation requires that agents behave in ways consistent with human or societal patterns rather than drifting into unrealistic behavior.Generative Agents [109] achieved this by introducing cognitive architectures with memory, reflection, and planning.Its memory system scores experiences by recency, relevance, and importance, allowing salient events to be repeatedly recalled and consolidated, mirroring core patterns of human memory.Humanoid Agents [164] further improved fidelity by embedding physiological needs, emotions, and relationship closeness into decision-making, leading agents to display more human-like variability.</p>
<p>At larger scales, Project Sid [7] constructed an agent society in a Minecraft-based world, inhabited by hundreds to thousands of agents who shared limited resources and interacted concurrently.Under conditions of scarcity and continual co-presence, the agents competed and cooperated, spontaneously developing specialized roles, adapting collective rules, and propagating cultural practices such as religion.Artificial Leviathan [35] approaches fidelity through a survival sandbox in which agents, driven by psychological needs under resource pressure, choose among farming, trading, or robbing each day.This design replicates Hobbes's state-of-nature scenario: agents start in conflict but eventually form social contracts, authorize a sovereign, and transition to peaceful cooperation.Experiments further show that parameters like memory depth has a large impact on the speed and nature of social evolution.</p>
<p>8 Discussion and Open Challenges</p>
<p>Memory System</p>
<p>Working and long-term memory serve distinct yet interdependent functions ( §3).Working memory stabilizes short-horizon decision-making under limited context by (i) extending the effective input span, (ii) compressing redundant information, and (iii) maintaining recent bindings and plans.These mechanisms mitigate short-term drift and prevent inconsistent actions [62].Long-term memory ensures continuity across episodes when organized into structured and retrievable forms such as chunks with metadata, key-value pairs, hierarchical trees, graphs, or skill libraries.The interaction between the two hinges on three functions: consolidation, which determines when transient traces are committed to durable storage; structuring, which organizes stored content for efficient access; and retrieval, which reactivates relevant information through metadata filtering, semantic search, or traversal of structured memories.</p>
<p>An open challenge for current memory systems is to move beyond "storing more" toward developing a true "world-model" memory that consolidates fragmented experiences into a coherent mental model of the game world [71].To distinguish a world-model memory system from a mere database, three design principles are essential.(i) Predictive dynamics: memory should not only replay past events but also predict what might happen next.In cognitive science, mental models are understood as internal simulations that help people anticipate outcomes and detect errors, rather than as static records [71].(ii) Structural compositionality: experiences need to be stored in organized forms, such as schemas or graphs that link entities, relations, and precondition-effect rules, so that knowledge from different situations can be combined and reused.This idea aligns with schema and situation-model theories, which show that humans build integrated "who-what-where-when-why" representations to reason beyond literal experiences [198].(iii) Selective consolidation and adaptive forgetting: long-term memory should decide what to keep and what to discard.Instead of saving every detail, it should preserve experiences that are important for understanding or improving the current model of the world, while letting irrelevant or low-value details fade.Research on human memory shows that people tend to remember information that is useful or frequently encountered and forget what rarely matters [9].</p>
<p>Reasoning Mechanism</p>
<p>Reasoning in LLMGAs is not merely about producing intermediate thoughts, but about ensuring that those thoughts improve decision quality ( §4).Prompting strategies such as chain-of-thought, structural reasoning, and feedback reasoning highlight recurring challenges: reasoning should avoid error propagation and remain consistent across steps.Training paradigms such as supervised finetuning, reinforcement learning, and preference optimization strengthen these abilities by grounding reasoning in experience and feedback.Despite these advances, a fundamental limitation remains: current approaches rely on narrow forms of feedback or numeric rewards.Multi-path reasoning improves robustness by exploring diverse reasoning trajectories, yet it provides no learning signal about which paths are preferable or why.Reflective reasoning enables self-correction across episodes but remains coarse-grained, offering post-hoc summaries rather than actionable, step-level feedback.Process Reward Models (PRMs) attempt to provide this supervision by assigning stepwise rewards, but rely heavily on costly human annotation or handcrafted heuristics, making feedback sparse, rigid, and poorly aligned with the linguistic nature of reasoning.</p>
<p>The deeper challenge lies in the mismatch between the form of reinforcement and the medium of reasoning.Traditional reinforcement learning depends on numeric rewards, whereas reasoning in LLMs unfolds through language, where success, failure, and state changes appear as semantic cues.Humans, however, are capable of assigning credit even from weak or indirect feedback: they adjust their reasoning based on partial signals such as environmental changes, the outcome of intermediate goals, or the perceived coherence of an explanation.Cognitive studies on metacognition and error monitoring show that such internal evaluation enables people to refine reasoning continuously through semantic and contextual signals rather than explicit numeric reinforcement [16,46,179].By virtue of their linguistic grounding, LLMs can transform textual feedback, environmental descriptions, and self-critiques into implicit reinforcement signals, generalizing traditional reward learning beyond numeric values and enabling reasoning to improve through understanding rather than scoring.</p>
<p>Perception-Action Interface</p>
<p>The perception &amp; action interface grounds how agents see the environment and fulfill their decisions ( §5).A key challenge is how effectively perception and action are aligned to support decision quality.Perception should highlight decision-relevant features such as object states, affordances, and strategic cues so that the agent does not waste capacity on irrelevant detail.Action interfaces, in turn, balance expressivity and reliability: high-level actions simplify decision space, low-level controls allow fine precision, and programmatic actions offer structure, verifiability, and reusability.Overall, perception and action should be co-designed as a coupled system, since they form a single loop where perception shapes possible actions and actions in turn shape what must be perceived.Ensuring this alignment while keeping the loop efficient and scalable remains an open problem for future research.</p>
<p>Multi-LLMGA System</p>
<p>LLM-based multi-agent systems extend game environments from single-agent decision making to collective behavior, introducing new challenges such as partial observability, communication bandwidth limits, and the need to preserve realistic interaction constraints ( §6).In our framework, we analyze these systems across two complementary levels.At the micro level, communication protocols determine what information agents exchange and how it is integrated under these constraints, while at the macro level, organizational structures govern decision flow (topology), guide division of labor (task allocation), and determine whether societies can scale and remain stable.</p>
<p>Prior studies have demonstrated the potential of multi-agent systems in large-scale simulations, where agents exhibit emergent behaviors.However, current large-scale multi-agent simulations remain constrained by structural and methodological limitations.Many "emergent" phenomena, such as role differentiation, norm formation, or collective planning, are closely tied to task initialization and rule design.In practice, agents are often seeded with shared goals, cooperation-oriented prompts, or predefined role templates that guide subsequent division of labor and coordination patterns.Prior studies of multi-agent societies have shown that such structural priors are widespread, from small-scale social environments [109] to hierarchical and large-scale simulations [7,23,192], where coordination often reflects the constraints of task setup rather than fully autonomous selforganization.Moreover, the lack of open and reproducible large-scale platforms further limits systematic evaluation, making it difficult to test under what specific conditions such collective dynamics genuinely arise.</p>
<p>Game Environments and Benchmarks</p>
<p>Current widely used benchmarks (e.g., TextWorld [56], ALFWorld [1], ScienceWorld [157]) were primarily developed before the rise of LLMs.Their tasks are generated from templated rules and constrained by a limited set of admissible actions and shallow dynamics, which result in highly similar instantiated tasks and low interactive complexity.In ALFWorld, for example, tasks are constructed from household instruction templates over a fixed action set (e.g., pick up, open, put, heat), producing many near-duplicate instances that only substitute objects or receptacles [1].</p>
<p>High-quality game environments/benchmarks are crucial for advancing the capabilities of LLM-GAs.Such environments should not only be more complex, but complex in targeted ways that expose the distinctive weaknesses of current architectures.This entails: (i) tasks with deeper compositional structure and long-horizon dependencies, ensuring that success cannot be reduced to pattern-matching templates; (ii) world dynamics governed by consistent physical or social rules, requiring agents to acquire and exploit regularities rather than memorize isolated instances; and (iii) scalability in both breadth (diverse tasks and domains) and depth (persistent settings spanning multiple days or large populations of agents).</p>
<p>Most existing benchmarks evaluate game agents with coarse-grained metrics such as win rate and task success rate [1,157].While these high-level measures capture overall gameplay performance, they obscure where and why agents fail.Moving forward, the field requires fine-grained  [33] Text-based Games Adventure (mini) Single Text GitHub 2019/09 Jericho [177] Interactive Fictions Adventure Single Text GitHub 2019/12 Overcooked-AI [19] Overcooked-like game Action Multi Symbolic GitHub 2020/03 ALFRED [130] Household Tasks Adventure (mini) Single Mixed GitHub 2020/10 ALFWorld [178] Household evaluation protocols that can diagnose the core components of agent design, memory, reasoning, perception-action translation, and multi-agent coordination, thus linking empirical evaluation to theoretical progress.One practical approach is game-specific metric design.Such metrics leverage domain knowledge to expose failure modes that aggregate scores cannot reveal.For example, PokéLLMon introduces the consecutive switch rate, measuring the proportion of turns where the agent switches Pokémon consecutively as a proxy for short-term inconsistency [63].Voyager uses map coverage and number of unique items collected to quantify exploration breadth and inventory management [154].At a larger scale, Project Sid [7] invite new metrics, such as persistence of social norms or stability of emergent institutions, providing outcome measures with diagnostic signals for interpreting agent behavior.However, not all evaluation targets lend themselves to direct quantification.Aspects such as role fidelity, believability, or the coherence of emergent behavior often require judgment-based protocols.In Generative Agents [109], for example, agents were interviewed about their recent activities, relationships, or future plans, and their answers were cross-checked against internal memory logs.Human evaluators then rated responses for consistency, plausibility, and coherence, providing a qualitative assessment of role fidelity.This procedure can be extended through LLMbased judgments, where a strong LLM serves as the evaluator to assess the quality of agent behaviors, offering scalability and reproducibility.To mitigate bias, a practical solution is to adopt hybrid protocols, where LLM judgments are guided by rubrics defined by human experts and their outputs are validated through human spot-checking.</p>
<p>Conclusion</p>
<p>This survey provides an up-to-date review of LLMGAs through a unified analytical framework.At the single agent level, we synthesize prior work across three core components, memory, reasoning, and perception-action interfaces, that together describe how agents perceive, think, and act through language.Extending this foundation, we introduce a complementary multi-agent framework for analyzing communication protocols and organizational structures that govern coordination, task allocation, and large-scale stability.To connect these design dimensions with gameplay contexts, we further introduce a challenge-centered taxonomy that maps six major game genres to their dominant agent design requirements, from low-latency control in action games to open-ended goal generation in sandbox worlds.Together, these perspectives present a coherent view of how language-enabled agents operate in interactive game environments and outline key challenges that define the next stage of research.</p>
<p>Fig. 1 .
1
Fig. 1.(a) Single-agent framework for LLMGAs, consisting of a memory system, a reasoning mechanism, and interfaces for perception and action.These modules are connected through the central LLM, driving a continuous gameplay loop where the agent perceives the evolving environment and acts in response.(b) Multi-LLMGA framework that extends the architecture to populations of agents, including the communication protocol that governs message exchange and the organizational structure that determines topology, task allocation, and role differentiation.</p>
<p>Fig. 3 .
3
Fig. 3. Illustration of temporal inconsistency: When facing a powerful opponent, the LLM game agent tends to switch different Pokémon in consecutive steps rather than taking attack, even though it has the memory that it switches in the current Pokémon from last step.Figure is obtained from the PokeLLMon paper[63]</p>
<p>Fig. 4 .
4
Fig. 4. Illustration of representative memory structuring approaches.</p>
<p>Fig. 5 .
5
Fig. 5. Categorization of reasoning mechanisms of LLMGAs.</p>
<p>Fig. 6 .
6
Fig. 6.Illustration of representative instruction-guided reasoning approaches.</p>
<p>Fig. 7 .
7
Fig. 7. Overview of perception and action interfaces in LLMGAs.</p>
<p>Table 2 .
2
[62]uation of decision consistency in PokéLLMon Battles<a href="GPT-4o is adopted as the LLM">62</a>.
MethodWin Rate↑Switch RateCon. Switch Rate↓LLM (GPT-4o)0.42170.33560.2442CoT [166]0.37130.33440.2647SC-CoT [160]0.40650.36430.0954LastThoughts [61]0.46670.22270.0861</p>
<p>Table 3 .
3
Summary of representative LLMGAs in terms of memory design.
LLMGAEnvironmentWorking MemoryLong-Term MemoryReflexion [129]ALFWorldIn-episode experienceReflection on previous episodesXu et al.</p>
<p>Table 4 .
4
Summary of representative LLMGAs in terms of reasoning mechanism.
LLMGAEnvironmentInstruction-guided ReasoningFine-tuning for Improving ReasoningReAct [178]ALFWorld, etc.</p>
<p>Table 5 .
5
Summary of representative LLMGAs in terms of perception &amp; action interfaces.
AgentGamePerception InterfaceAction InterfaceReAct [178]ALFWorld, etc.</p>
<p>Table 6 .
6
Open-sourced Benchmark/Environments for LLMGAs
DateBenchmark/Environment Game ContentGenre ClassificationPlayer Mode Modality Code Link2018/06 VirtualHome [113]Household TasksAdventure (mini)SingleMixedGitHub2018/07 TextWorld</p>
<p>Aligning Text and Embodied Environments for Interactive Learning. Alfworld, booktitle=International Conference on Learning Representations, year=2021. Xingdi Author=mohit Shridhar, Marc-Alexandre Yuan, Yonatan Cote, Adam Bisk, Matthew Trischler, Hausknecht, </p>
<p>The arcade learning environment: An evaluation platform for general agents. Journal of artificial intelligence research. 472013</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems (NeurIPS). 2023</p>
<p>Octopus: Embodied vision-language programmer for daily tasks. European Conference on Computer Vision (ECCV). Springer2024</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.087742023arXiv preprint</p>
<p>LLM-coordination: Evaluating and analyzing multi-agent coordination abilities in large language models. S Agashe, Y Fan, A Reyna, X E Wang, Findings of the Association for Computational Linguistics: NAACL 2025. L Chiruzzo, A Ritter, L Wang, Albuquerque, New MexicoAssociation for Computational LinguisticsApr. 2025</p>
<p>Project sid: Many-agent simulations toward ai civilization. A Al, A Ahn, N Becker, S Carroll, N Christie, M Cortes, A Demirci, M Du, F Li, S Luo, arXiv:2411.001142024arXiv preprint</p>
<p>Cognitive Psychology and Its Implications. J R Anderson, 2010Worth PublishersNew York7th edition</p>
<p>Reflections of the environment in memory. J R Anderson, L J Schooler, Psychological science. 261991</p>
<p>Arigraph: Learning knowledge graph world models with episodic memory for llm agents. P Anokhin, N Semenov, A Sorokin, D Evseev, M Burtsev, E Burnaev, arXiv:2407.043632024arXiv preprint</p>
<p>Working memory: Theories, models, and controversies. Annual review of psychology. A Baddeley, 201263</p>
<p>Working memory. A D Baddeley, G Hitch, Psychology of Learning and Motivation. G. A. Bower81974Academic Press</p>
<p>Perceptual symbol systems. L W Barsalou, Behavioral and brain sciences. 2241999</p>
<p>The hitchhiker's guide to the galaxy text adventure. BBC30th anniversary edition</p>
<p>. C Beattie, J Z Leibo, D Teplyashin, T Ward, M Wainwright, H Küttler, A Lefrancq, S Green, V Valdés, A Sadik, arXiv:1612.038012016Deepmind lab. arXiv preprint</p>
<p>Foundations of metacognition. M J Beran, 2012Oxford University Press</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. M Besta, N Blach, A Kubicek, R Gerstenberger, M Podstawski, L Gianinazzi, J Gajda, T Lehmann, H Niewiadomski, P Nyczyk, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202438</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>On the utility of learning about humans for human-ai coordination. M Carroll, R Shah, M K Ho, T Griffiths, S Seshia, P Abbeel, A Dragan, Advances in neural information processing systems. 201932</p>
<p>Grounding large language models in interactive environments with online reinforcement learning. T Carta, C Romac, T Wolf, S Lamprier, O Sigaud, P.-Y Oudeyer, International Conference on Machine Learning. PMLR2023</p>
<p>B Chen, C Shu, E Shareghi, N Collier, K Narasimhan, S Yao, arXiv:2310.05915Fireact: Toward language agent fine-tuning. 2023arXiv preprint</p>
<p>Walking down the memory maze: Beyond context limit through interactive reading. H Chen, R Pasunuru, J Weston, A Celikyilmaz, arXiv:2310.050292023arXiv preprint</p>
<p>S-agent: self-organizing agents in open-ended environment. J Chen, Y Jiang, J Lu, L Zhang, ICLR 2024 Workshop on Large Language Model (LLM) Agents. 2024</p>
<p>Reconcile: Round-table conference improves reasoning via consensus among diverse llms. J Chen, S Saha, M Bansal, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Extending context window of large language models via positional interpolation. K Chen, D F Wong, L S Chao, Z Tu, arXiv:2306.155952023arXiv preprint</p>
<p>Can vlms play action role-playing games? take black myth: Wukong as a study case. P Chen, P Bu, J Song, Y Gao, B Zheng, arXiv:2409.128892024arXiv preprint</p>
<p>Adapting language models to compress contexts. A Chevalier, A Wettig, A Ajith, D Chen, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>Deep reinforcement learning from human preferences. Advances in neural information processing systems. P F Christiano, J Leike, T Brown, M Martic, S Legg, D Amodei, 201730</p>
<p>Being there: Putting brain, body, and world together again. A Clark, 1998MIT press</p>
<p>Whatever next? predictive brains, situated agents, and the future of cognitive science. A Clark, Behavioral and brain sciences. 3632013</p>
<p>Why video game genres fail: A classificatory analysis. R I Clarke, J H Lee, N Clark, Games and Culture. 1252017</p>
<p>Leveraging procedural generation to benchmark reinforcement learning. K Cobbe, C Hesse, J Hilton, J Schulman, International conference on machine learning. PMLR2020</p>
<p>Textworld: A learning environment for text-based games. M.-A Côté, A Kádár, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, M Hausknecht, L El Asri, M Adada, Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI. Revised Selected Papers. Stockholm, SwedenSpringer2018. July 13. 2018. 20197</p>
<p>The magical number 4 in short-term memory: A reconsideration of mental storage capacity. N Cowan, Behavioral and brain sciences. 2412001</p>
<p>Artificial leviathan: Exploring social evolution of llm agents through the lens of hobbesian social contract theory. G Dai, W Zhang, J Li, S Yang, S Rao, A Caetano, M Sra, arXiv:2406.143732024arXiv preprint</p>
<p>Will gpt-4 run doom?. A De Wynter, IEEE Transactions on Games. 1722025</p>
<p>Longrope: Extending llm context window beyond 2 million tokens. Y Ding, L L Zhang, C Zhang, Y Xu, N Shang, J Xu, F Yang, M Yang, International Conference on Machine Learning. PMLR2024</p>
<p>Guiding pretraining in reinforcement learning with large language models. Y Du, O Watkins, Z Wang, C Colas, T Darrell, P Abbeel, A Gupta, J Andreas, International Conference on Machine Learning. PMLR2023</p>
<p>Memory: A Contribution to Experimental Psychology. H Ebbinghaus, Henry A. Ruger and Clara E. Bussenius1913New YorkTeachers College, Columbia University</p>
<p>From local to global: A graph rag approach to query-focused summarization. D Edge, H Trinh, N Cheng, J Bradley, A Chao, A Mody, S Truitt, J Larson, arXiv:2404.161302024arXiv preprint</p>
<p>Dual-processing accounts of reasoning, judgment, and social cognition. J S B Evans, Annu. Rev. Psychol. 5912008</p>
<p>Human-level play in the game of diplomacy by combining language models with strategic reasoning. M F A R D T Fair) †, A Bakhtin, N Brown, E Dinan, G Farina, C Flaherty, D Fried, A Goff, J Gray, H Hu, Science. 37866242022</p>
<p>Minedojo: Building open-ended embodied agents with internet-scale knowledge. L Fan, G Wang, Y Jiang, A Mandlekar, Y Yang, H Zhu, A Tang, D.-A Huang, Y Zhu, A Anandkumar, Advances in Neural Information Processing Systems. 202235</p>
<p>Chessgpt: Bridging policy learning and language modeling. X Feng, Y Luo, Z Wang, H Tang, M Yang, K Shao, D Mguni, Y Du, J Wang, Advances in Neural Information Processing Systems. 362024</p>
<p>Llama-rider: Spurring large language models to explore the open world. Y Feng, Y Wang, J Liu, S Zheng, Z Lu, Findings of the Association for Computational Linguistics: NAACL 2024. 2024</p>
<p>The neural basis of metacognitive ability. S M Fleming, R J Dolan, Philosophical Transactions of the Royal Society B: Biological Sciences. 3671594. 2012</p>
<p>Large language models and games: A survey and roadmap. R Gallotta, G Todd, M Zammit, S Earle, A Liapis, J Togelius, G N Yannakakis, IEEE Transactions on Games. 2024</p>
<p>Large language models empowered agent-based modeling and simulation: A survey and perspectives. C Gao, X Lan, N Li, Y Yuan, J Ding, Z Zhou, F Xu, Y Li, Humanities and Social Sciences Communications. 1112024</p>
<p>In-context autoencoder for context compression in a large language model. T Ge, H Jing, L Wang, X Wang, S.-Q Chen, F Wei, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Working memory capacity of chatgpt: An empirical study. D Gong, X Wan, D Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Mindagent: Emergent gaming interaction. R Gong, Q Huang, X Ma, Y Noda, Z Durante, Z Zheng, D Terzopoulos, L Fei-Fei, J Gao, H Vo, Findings of the Association for Computational Linguistics: NAACL 2024. 2024</p>
<p>F Grötschla, L Müller, J Tönshoff, M Galkin, B Perozzi, arXiv:2507.08616Agentsnet: Coordination and collaborative reasoning in multi-agent llms. 2025arXiv preprint</p>
<p>Suspicion-agent: Playing imperfect information games with theory of mind aware gpt-4. J Guo, B Yang, P Yoo, B Y Lin, Y Iwasawa, Y Matsuo, Proceedings of the 1st Conference on Language Modeling (COLM). the 1st Conference on Language Modeling (COLM)2024</p>
<p>Are chatgpt and gpt-4 er players?-a pre-flop analysis. A Gupta, arXiv:2308.124662023arXiv preprint</p>
<p>Benchmarking the spectrum of agent capabilities. D Hafner, arXiv:2109.067802021arXiv preprint</p>
<p>Interactive fiction games: A colossal adventure. M Hausknecht, P Ammanabrolu, M.-A Côté, X Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>LoRA: Low-rank adaptation of large language models. E J Hu, P Shen, Z Wallis, Y Allen-Zhu, S Li, L Wang, W Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>L Hu, M Huo, Y Zhang, H Yu, E P Xing, I Stoica, T Rosing, H Jin, H Zhang, arXiv:2505.15146lmgame-bench: How good are llms at playing games? arXiv preprint. 2025</p>
<p>Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model. M Hu, T Chen, Q Chen, Y Mu, W Shao, P Luo, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025). the 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025)2025</p>
<p>Large language model-powered smart contract vulnerability detection: New perspectives. S Hu, T Huang, F İlhan, S F Tekin, L Liu, arXiv:2310.011522023arXiv preprint</p>
<p>Pokéllmon: A grounding and reasoning benchmark for large language models in pokémon battles. S Hu, T Huang, G Liu, R Kompella, L Liu, ACM Transactions on Internet Technology. 2025</p>
<p>S Hu, T Huang, G Liu, R R Kompella, L Liu, Pokéllmon: A grounding and reasoning benchmark for large language models in pokémon battles. </p>
<p>Pokéllmon: A human-parity agent for pokémon battles with large language models. S Hu, T Huang, L Liu, 2024</p>
<p>War and peace (waragent): Large language model-based multi-agent simulation of world wars. W Hua, L Fan, L Li, K Mei, J Ji, Y Ge, L Hemphill, Y Zhang, arXiv:2311.172272023arXiv preprint</p>
<p>Pokergpt: An end-to-end lightweight solver for multi-player texas hold'em via large language model. C Huang, Y Cao, Y Wen, T Zhou, Y Zhang, arXiv:2401.067812024arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, International Conference on Machine Learning. PMLR2022</p>
<p>Cognition in the Wild. E Hutchins, 1995MIT press</p>
<p>. Infocom, I Zork, 1980</p>
<p>. Infocom, Zork, 1982</p>
<p>A survey of behavior trees in robotics and ai. M Iovino, E Scukins, J Styrud, P Ögren, C Smith, Robotics and Autonomous Systems. 1541040962022</p>
<p>Mental models: Towards a cognitive science of language, inference, and consciousness. Number 6. P N Johnson-Laird, 1983Harvard University Press</p>
<p>Mental models and human reasoning. P N Johnson-Laird, Proceedings of the National Academy of Sciences. 107432010</p>
<p>D Kahneman, Thinking, Fast and Slow. Farrar, Straus and Giroux. 2011</p>
<p>Z Kaiya, M Naim, J Kondic, M Cortes, J Ge, S Luo, G R Yang, A Ahn, arXiv:2310.02172Lyfe agents: Generative agents for low-cost real-time social interactions. 2023arXiv preprint</p>
<p>Win fast or lose slow: Balancing speed and accuracy in latency-sensitive decisions of llms. H Kang, Q Zhang, H Cai, W Xu, T Krishna, Y Du, T Weissman, arXiv:2505.194812025arXiv preprint</p>
<p>The pokeagent challenge: Competitive and long-context learning at scale. S Karten, J Grigsby, S Milani, K Vodrahalli, A Zhang, F Fang, Y Zhu, C Jin, NeurIPS Competition Track. Apr. 2025</p>
<p>Vizdoom: A doom-based ai research platform for visual reinforcement learning. M Kempka, M Wydmuch, G Runc, J Toczek, W Jaśkowski, 2016 IEEE conference on computational intelligence and games (CIG). </p>
<p>Language models can solve computer tasks. G Kim, P Baldi, S Mcaleer, Advances in Neural Information Processing Systems. 202436</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>40 years of cognitive architectures: core cognitive abilities and practical applications. I Kotseruba, J K Tsotsos, Artificial Intelligence Review. 5312020</p>
<p>Facet analysis of video game genres. J H Lee, N Karlova, R I Clarke, K Thornton, A Perti, IConference. 2014. 2014Proceedings</p>
<p>The power of scale for parameter-efficient prompt tuning. B Lester, R Al-Rfou, N Constant, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2021</p>
<p>Theory of mind for multi-agent collaboration via large language models. H Li, Y Chong, S Stepputtis, J Campbell, D Hughes, C Lewis, K Sycara, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>Emergent world representations: Exploring a sequence model trained on a synthetic task. K Li, A K Hopkins, D Bau, F Viégas, H Pfister, M Wattenberg, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Building graph-based agent to enhance long-context abilities of large language models. S Li, Y He, H Guo, X Bu, G Bai, J Liu, J Liu, X Qu, Y Li, W Ouyang, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. X L Li, P Liang, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAugust 20211</p>
<p>Cooperative open-ended learning framework for zero-shot coordination. Y Li, S Zhang, J Sun, Y Du, Y Wen, X Wang, W Pan, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>From text to tactic: Evaluating LLMs playing the game of avalon. J Light, M Cai, S Shen, Z Hu, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023</p>
<p>Let's verify step by step. H Lightman, V Kosaraju, Y Burda, H Edwards, B Baker, T Lee, J Leike, J Schulman, I Sutskever, K Cobbe, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. B Y Lin, Y Fu, K Yang, F Brahman, S Huang, C Bhagavatula, P Ammanabrolu, Y Choi, X Ren, Advances in Neural Information Processing Systems. 202436</p>
<p>Agentsims: An open-source sandbox for large language model evaluation. J Lin, H Zhao, A Zhang, Y Wu, H Ping, Q Chen, arXiv:2308.040262023arXiv preprint</p>
<p>Q* agent: Optimizing language agents with q-guided exploration. Z Lin, Y Tang, D Yin, S X Yao, Z Hu, Y Sun, K.-W Chang, </p>
<p>Llm-powered hierarchical language agent for real-time human-ai coordination. J Liu, C Yu, J Gao, Y Xie, Q Liao, Y Wu, Y Wang, Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems. the 23rd International Conference on Autonomous Agents and Multiagent Systems2024</p>
<p>S Liu, Y Li, K Zhang, Z Cui, W Fang, Y Zheng, T Zheng, M Song, Odyssey, arXiv:2407.15325Empowering minecraft agents with open-world skills. 2024arXiv preprint</p>
<p>Rl-gpt: Integrating reinforcement learning and code-as-policy. S Liu, H Yuan, M Hu, Y Li, Y Chen, S Liu, Z Lu, J Jia, Advances in Neural Information Processing Systems (NeurIPS). 2024</p>
<p>Teamcraft: A benchmark for multi-modal multi-agent systems in minecraft. Q Long, Z Li, R Gong, Y N Wu, D Terzopoulos, X Gao, arXiv:2412.052552024arXiv preprint</p>
<p>Large language models play starcraft II:benchmarks and a chain of summarization approach. W Ma, Q Mi, Y Zeng, X Yan, R Lin, Y Wu, J Wang, H Zhang, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>First textworld problems: The competition using text-based games to advance capabilities of ai agents. 2019Microsoft Research</p>
<p>The cognitive revolution: a historical perspective. G A Miller, Trends in cognitive sciences. 732003</p>
<p>. Mojang Studios, Minecraft, </p>
<p>R Mokady, A Hertz, A H Bermano, arXiv:2111.09734Clipcap: Clip prefix for image captioning. 2021arXiv preprint</p>
<p>Learning to compress prompts with gist tokens. J Mu, X Li, N Goodman, Advances in Neural Information Processing Systems. 19327-19352, 202336</p>
<p>Unified theories of cognition. A Newell, 1994Harvard University Press</p>
<p>Large language models and cognitive science: A comprehensive review of similarities, differences, and challenges. Q Niu, J Liu, Z Bi, P Feng, B Peng, K Chen, M Li, L K Yan, Y Zhang, C H Yin, arXiv:2409.023872024arXiv preprint</p>
<p>Opengenerativeai, Llm colosseum: Benchmark llms by fighting in street fighter iii. 2024</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>D Park, M Kim, B Choi, J Kim, K Lee, J Lee, I Park, B.-U Lee, J Hwang, J Ahn, arXiv:2506.03610A foundational benchmark for training and evaluating llm agents on diverse video games. 2025arXiv preprint</p>
<p>Generative agents: Interactive simulacra of human behavior. J S Park, J O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>YaRN: Efficient context window extension of large language models. B Peng, J Quesnelle, H Fan, E Shippole, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Adapt: As-needed decomposition and planning with language models. A Prasad, A Koller, M Hartmann, P Clark, A Sabharwal, M Bansal, T Khot, Findings of the Association for Computational Linguistics: NAACL 2024. 2024</p>
<p>Prismarinejs, Mineflayer, Create minecraft bots with a powerful, stable, and high level javascript api. 2013</p>
<p>Virtualhome: Simulating household activities via programs. X Puig, K Ra, M Boben, J Li, T Wang, S Fidler, A Torralba, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>Civrealm: A learning and reasoning odyssey in civilization for decision-making agents. S Qi, S Chen, Y Li, X Kong, J Wang, B Yang, P Wong, Y Zhong, X Zhang, Z Zhang, N Liu, Y Yang, S.-C Zhu, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Agent planning with world knowledge model. S Qiao, R Fang, N Zhang, Y Zhu, X Chen, S Deng, Y Jiang, P Xie, F Huang, H Chen, Proceedings of the 38th International Conference on Neural Information Processing Systems. the 38th International Conference on Neural Information Processing Systems2024</p>
<p>Nugget: Neural agglomerative embeddings of text. G Qin, B Van Durme, International Conference on Machine Learning. PMLR2023</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PMLR2021</p>
<p>Direct preference optimization: Your language model is secretly a reward model. R Rafailov, A Sharma, E Mitchell, C D Manning, S Ermon, C Finn, Advances in Neural Information Processing Systems. 362023</p>
<p>N Ratner, Y Levine, Y Belinkov, O Ram, I Magar, O Abend, E Karpas, A Shashua, K Leyton-Brown, Y Shoham, arXiv:2212.10947Parallel context windows for large language models. 2022arXiv preprint</p>
<p>. S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Giménez, Y Sulsky, J Kay, J T Springenberg, T Eccles, J Bruce, A Razavi, A Edwards, N Heess, Y Chen, R Hadsell, O Vinyals, M Bordbar, N De Freitas, A generalist agent. Transactions on Machine Learning Research. 2022Featured Certification, Outstanding Certification</p>
<p>From isolated conversations to hierarchical schemas: Dynamic tree memory representation for LLMs. A Rezazadeh, Z Li, W Wei, Y Bao, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>P Sarthi, S Abdullah, A Tuli, S Khanna, A Goldie, C D Manning, arXiv:2401.18059Raptor: Recursive abstractive processing for tree-organized retrieval. 2024arXiv preprint</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>L Shan, S Luo, Z Zhu, Y Yuan, Y Wu, arXiv:2504.02441Cognitive memory in large language models. 2025arXiv preprint</p>
<p>A survey of deep reinforcement learning in video games. K Shao, Z Tang, Y Zhu, N Li, D Zhao, arXiv:1912.109442019arXiv preprint</p>
<p>X Shao, W Jiang, F Zuo, M Liu, Swarmbrain, arXiv:2401.17749Embodied agent for real-time strategy game starcraft ii via large language models. 2024arXiv preprint</p>
<p>Character-llm: A trainable agent for role-playing. Y Shao, L Li, J Dai, X Qiu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Z Shi, M Fang, S Zheng, S Deng, L Chen, Y Du, arXiv:2312.17515Cooperation on the fly: Exploring language agents for ad hoc teamwork in the avalon game. 2023arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, E Berman, A Gopinath, K Narasimhan, S Yao, Advances in Neural Information Processing Systems (NeurIPS). 2023</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>The development of embodied cognition: Six lessons from babies. L Smith, M Gasser, Artificial life. 111-22005</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. C H Song, J Wu, C Washington, B M Sadler, W.-L Chao, Y Su, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Trial and error: Exploration-based trajectory optimization of llm agents. Y Song, D Yin, X Yue, J Huang, S Li, B Y Lin, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Memory systems of the brain: a brief history and current perspective. L R Squire, Neurobiology of learning and memory. 8232004</p>
<p>Steam tags and genres. Steamdb, 2025-08-20</p>
<p>Roformer: Enhanced transformer with rotary position embedding. J Su, M Ahmed, Y Lu, S Pan, W Bo, Y Liu, Neurocomputing. 5681270632024</p>
<p>Cognitive architectures for language agents. T Sumers, S Yao, K Narasimhan, T Griffiths, Transactions on Machine Learning Research. 2024Survey Certification</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 1998MIT press Cambridge1</p>
<p>Large language models and video games: A preliminary scoping review. P Sweetser, Proceedings of the 6th ACM Conference on Conversational User Interfaces. the 6th ACM Conference on Conversational User Interfaces2024</p>
<p>Towards general computer control: A multimodal agent for red dead redemption II as a case study. W Tan, Z Ding, W Zhang, B Li, B Zhou, J Yue, H Xia, J Jiang, L Zheng, X Xu, Y Bi, P Gu, X Wang, B F Karlsson, B An, Z Lu, ICLR 2024 Workshop on Large Language Model (LLM) Agents. 2024</p>
<p>True knowledge comes from practice: Aligning large language models with embodied environments via reinforcement learning. W Tan, W Zhang, S Liu, L Zheng, X Wang, B An, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Mars: Situated inductive reasoning in an open-world environment. X Tang, J Li, Y Liang, S -C. Zhu, M Zhang, Z Zheng, Advances in Neural Information Processing Systems. 202437</p>
<p>Redpajama: An open source recipe to reproduce llama training dataset. April 2023Together Computer</p>
<p>Chess as a testbed for language model state tracking. S Toshniwal, S Wiseman, K Livescu, K Gimpel, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Reft: Reasoning with reinforced fine-tuning. L Trung, X Zhang, Z Jie, P Sun, X Jin, H Li, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Can large language models play text games well? current state-of-the-art and open questions. C F Tsai, X Zhou, S S Liu, J Li, M Yu, H Mei, arXiv:2304.028682023arXiv preprint</p>
<p>Episodic and semantic memory. E Tulving, Organization of Memory. E Tulving, W Donaldson, Academic Press1972</p>
<p>Episodic and semantic memory. E Tulving, Organization of memory. 111972</p>
<p>The embodied mind, revised edition: Cognitive science and human experience. F J Varela, E Thompson, E Rosch, 2017MIT press</p>
<p>Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, 201730Attention is all you need</p>
<p>Enhancing large language model with self-controlled memory framework. B Wang, X Liang, J Yang, H Huang, S Wu, P Wu, L Lu, Z Ma, Z Li, arXiv:2304.133432023arXiv preprint</p>
<p>C Wang, Y Deng, Z Lyu, L Zeng, J He, S Yan, B An, arXiv:2406.14283Q*: Improving multi-step reasoning for llms with deliberative planning. 2024arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, Transactions on Machine Learning Research. 2024</p>
<p>A survey on large language model based autonomous agents. L Wang, C Ma, X Feng, Z Zhang, H Yang, J Zhang, Z Chen, J Tang, X Chen, Y Lin, Frontiers of Computer Science. 1861863452024</p>
<p>Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. N Wang, Z Peng, H Que, J Liu, W Zhou, Y Wu, H Guo, R Gan, Z Ni, J Yang, M Zhang, Z Zhang, W Ouyang, K Xu, W Huang, J Fu, J Peng, Findings of the Association for Computational Linguistics: ACL 2024. L.-W Ku, A Martins, V Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAug. 2024</p>
<p>Scienceworld: Is your agent smarter than a 5th grader?. R Wang, P Jansen, M.-A Côté, P Ammanabrolu, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu DhabiAssociation for Computational Linguistics2022</p>
<p>Enhancing player enjoyment with a two-tier drl and llm-based agent system for fighting games. S Wang, Z Jiang, F Sliva, S Earle, J Togelius, arXiv:2504.074252025arXiv preprint</p>
<p>Avalon's game of thoughts: Battle against deception through recursive contemplation. S Wang, C Liu, Z Zheng, S Qi, S Chen, Q Yang, A Zhao, C Wang, S Song, G Huang, arXiv:2310.013202023arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q V Le, E H Chi, S Narang, A Chowdhery, D Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Evaluating personality fidelity in role-playing agents through psychological interviews. X Wang, Y Xiao, J.-T Huang, S Yuan, R Xu, H Guo, Q Tu, Y Fei, Z Leng, W Wang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Describe, explain, plan and select: Interactive planning with LLMs enables open-world multi-task agents. Z Wang, S Cai, G Chen, A Liu, X Ma, Y Liang, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models. Z Wang, S Cai, A Liu, Y Jin, J Hou, B Zhang, H Lin, Z He, Z Zheng, Y Yang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4732025</p>
<p>Humanoid agents: Platform for simulating human-like generative agents. Z Wang, Y Y Chiu, Y C Chiu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations2023</p>
<p>N R Waytowich, D White, M Sunbeam, V G Goecks, arXiv:2408.15950Atari-gpt: Benchmarking multimodal large language models as low-level policies in atari games. 2024arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>An introduction to multiagent systems. M Wooldridge, 2009John wiley &amp; sons</p>
<p>Deciphering digital detectives: Understanding llm behaviors and capabilities in multi-agent mystery games. D Wu, H Shi, Z Sun, B Liu, Findings of the Association for Computational Linguistics ACL 2024. 2024</p>
<p>S Wu, L Zhu, T Yang, S Xu, Q Fu, Y Wei, H Fu, arXiv:2402.02330Enhance reasoning for large language models in the game werewolf. 2024arXiv preprint</p>
<p>Spring: Studying papers and reasoning to play games. Y Wu, S Y Min, S Prabhumoye, Y Bisk, R R Salakhutdinov, A Azaria, T M Mitchell, Y Li, Advances in Neural Information Processing Systems. 202436</p>
<p>Language models meet world models: Embodied experiences enhance language models. J Xiang, T Tao, Y Gu, T Shu, Z Wang, Z Yang, Z Hu, Advances in neural information processing systems. 202436</p>
<p>Watch every step! LLM agent learning via iterative step-level process refinement. W Xiong, Y Song, X Zhao, W Wu, X Wang, K Wang, C Li, W Peng, S Li, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>W Xu, K Mei, H Gao, J Tan, Z Liang, Y Zhang, arXiv:2502.12110A-mem: Agentic memory for llm agents. 2025arXiv preprint</p>
<p>Y Xu, S Wang, P Li, F Luo, X Wang, W Liu, Y Liu, arXiv:2309.04658Exploring large language models for communication games: An empirical study on werewolf. 2023arXiv preprint</p>
<p>Language agents with reinforcement learning for strategic play in the werewolf game. Z Xu, C Yu, F Fang, Y Wang, Y Wu, Forty-first International Conference on Machine Learning. 2024</p>
<p>Artificial Intelligence and Games. G N Yannakakis, J Togelius, 2018Springer</p>
<p>Keep calm and explore: Language models for action generation in text-based games. S Yao, R Rao, M Hausknecht, K Narasimhan, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K R Narasimhan, Y Cao, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Metacognition in human decision-making: confidence and error monitoring. N Yeung, C Summerfield, Philosophical Transactions of the Royal Society B: Biological Sciences. 3671594. 2012</p>
<p>From debate to equilibrium: Belief-driven multi-agent llm reasoning via bayesian nash equilibrium. X Yi, Z Zhou, C Cao, Q Niu, T Liu, B Han, Forty-second International Conference on Machine Learning. </p>
<p>Skill reinforcement learning and planning for open-world long-horizon tasks. H Yuan, C Zhang, H Wang, F Xie, P Cai, H Dong, Z Lu, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023</p>
<p>X Yuan, M.-A Côté, A Sordoni, R Laroche, R T D Combes, M Hausknecht, A Trischler, arXiv:1806.11525Counting to explore and generalize in text-based games. 2018arXiv preprint</p>
<p>Z Yuan, H Yuan, C Li, G Dong, K Lu, C Tan, C Zhou, J Zhou, arXiv:2308.01825Scaling relationship on learning mathematical reasoning with large language models. 2023arXiv preprint</p>
<p>Agenttuning: Enabling generalized agent abilities for llms. A Zeng, M Liu, R Lu, B Wang, X Liu, Y Dong, J Tang, Findings of the Association for Computational Linguistics ACL 2024. 2024</p>
<p>Fine-tuning large vision-language models as decision-making agents via reinforcement learning. S Zhai, H Bai, Z Lin, J Pan, P Tong, Y Zhou, A Suhr, S Xie, Y Lecun, Y Ma, Advances in Neural Information Processing Systems. 202537</p>
<p>Creative agents: Empowering agents with imagination for creative tasks. C Zhang, P Cai, Y Fu, H Yuan, Z Lu, arXiv:2312.025192023arXiv preprint</p>
<p>Proagent: Building proactive cooperative agents with large language models. C Zhang, K Yang, S Hu, Z Wang, G Li, Y Sun, C Zhang, Z Zhang, A Liu, S.-C Zhu, X Chang, J Zhang, F Yin, Y Liang, Y Yang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Large language models are semi-parametric reinforcement learning agents. D Zhang, L Chen, S Zhang, H Xu, Z Zhao, K Yu, Advances in Neural Information Processing Systems. 202436</p>
<p>Building cooperative embodied agents modularly with large language models. H Zhang, W Du, J Shan, Q Zhou, Y Du, J B Tenenbaum, T Shu, C Gan, The Twelfth International Conference on Learning Representations. 2024</p>
<p>OMNI: Open-endedness via models of human notions of interestingness. J Zhang, J Lehman, K Stanley, J Clune, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Expel: Llm agents are experiential learners. A Zhao, D Huang, Q Xu, M Lin, Y.-J Liu, G Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Z Zhao, K Chen, D Guo, W Chai, T Ye, Y Zhang, G Wang, arXiv:2403.08282Hierarchical auto-organizing system for open-ended multi-agent navigation. 2024arXiv preprint</p>
<p>Steve-eye: Equipping llm-based embodied agents with visual perception in open worlds. S Zheng, Y Feng, Z Lu, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Memorybank: Enhancing large language models with long-term memory. W Zhong, L Guo, Q Gao, H Ye, Y Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Z Zhou, A Qu, Z Wu, S Kim, A Prakash, D Rus, J Zhao, B K H Low, P P Liang, arXiv:2506.15841Mem1: Learning to synergize memory and reasoning for efficient long-horizon agents. 2025arXiv preprint</p>
<p>Pose: Efficient context window extension of llms via positional skip-wise training. D Zhu, N Yang, L Wang, Y Song, W Wu, F Wei, S Li, The Twelfth International Conference on Learning Representations (ICLR). 2024</p>
<p>Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. X Zhu, Y Chen, H Tian, C Tao, W Su, C Yang, G Huang, B Li, L Lu, X Wang, arXiv:2305.171442023arXiv preprint</p>
<p>Situation models in language comprehension and memory. R A Zwaan, G A Radvansky, Psychological bulletin. 12321621998</p>            </div>
        </div>

    </div>
</body>
</html>
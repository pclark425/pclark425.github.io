<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1375 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1375</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1375</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-e9f32b34fde7d0034a147d5996470634c9e19002</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e9f32b34fde7d0034a147d5996470634c9e19002" target="_blank">Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> A new algorithm MINERVA is proposed, which addresses the much more difficult and practical task of answering questions where the relation is known, but only one entity, and significantly outperforms prior methods.</p>
                <p><strong>Paper Abstract:</strong> Knowledge bases (KB), both automatically and manually constructed, are often incomplete --- many valid facts can be inferred from the KB by synthesizing existing information. A popular approach to KB completion is to infer new relations by combinatory reasoning over the information found along other paths connecting a pair of entities. Given the enormous size of KBs and the exponential number of paths, previous path-based models have considered only the problem of predicting a missing relation given two entities or evaluating the truth of a proposed triple. Additionally, these methods have traditionally used random paths between fixed entity pairs or more recently learned to pick paths between them. We propose a new algorithm MINERVA, which addresses the much more difficult and practical task of answering questions where the relation is known, but only one entity. Since random walks are impractical in a setting with combinatorially many destinations from a start node, we present a neural reinforcement learning approach which learns how to navigate the graph conditioned on the input query to find predictive paths. Empirically, this approach obtains state-of-the-art results on several datasets, significantly outperforming prior methods.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1375.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1375.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MINERVA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MINERVA (Meandering In Networks of Entities to Reach Verisimilar Answers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning agent that answers KB queries by walking a directed, labeled knowledge-graph from a query entity, selecting outgoing labeled edges conditioned on the query and path history to arrive at answer nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Knowledge-Graph Query Answering (various KB datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Knowledge bases represented as directed, labeled multigraphs where nodes are entities and labeled edges are binary relations (inverse edges added). Domains are real-world KBs (Countries, UMLS, Kinship, WN18RR, FB15K-237, NELL-995, WikiMovies).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Directed labeled multigraph, generally sparse; connectivity statistics vary by dataset (average and median degree reported per dataset). The model adds inverse edges for bidirectional traversal and a NO_OP self-edge to allow variable-length paths.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Multiple datasets (examples from Table 1): COUNTRIES: 272 entities, 1,158 facts (avg degree 4.35, median 4); UMLS: 135 entities, 5,216 facts (avg degree 38.63, median 28); KINSHIP: 104 entities, 10,686 facts (avg degree 82.15); WN18RR: 40,945 entities, 86,835 facts (avg degree 2.19); NELL-995: 75,492 entities, 154,213 facts (avg degree 4.07); FB15K-237: 14,505 entities, 272,115 facts (avg degree 19.74); WikiMovies: 43,230 entities, 196,453 facts (avg degree 6.65).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MINERVA (LSTM policy trained with REINFORCE)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>History-dependent stochastic policy: LSTM encodes path history; entity and relation embeddings; at each step the agent scores outgoing edges (relation label + destination embedding) with a small MLP conditioned on the query relation and history, samples actions (edges), trained with policy gradients (REINFORCE) with a moving-average baseline and entropy regularization; uses NO_OP and inverse relations; inference uses beam search (beam width 50).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Reported metrics relevant to exploration/efficiency: training convergence time (wall-clock vs baselines), inference wall-clock time, dependence of inference cost on local degree (instead of O(|E|) ranking), task performance metrics (HITS@k, MRR, AUC-PR, MAP), and path-length sensitivity (accuracy vs required path length).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Representative reported values: inference wall-clock time on WN18RR test set -- MINERVA 63s vs DistMult 211s; on NELL-995 test set -- MINERVA 35s vs DistMult 115s. Training uses 20 rollouts per training example; beam width 50 at inference. No single scalar sample-efficiency number given, but plots (Fig. 5) show MINERVA reaches higher HITS@10 faster than DistMult.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported query-answering performance (which reflects navigation-to-answer): WN18RR HITS@1 = 0.413, HITS@3 = 0.456, HITS@10 = 0.513, MRR = 0.448; NELL-995 HITS@1 = 0.663, HITS@3 = 0.773, HITS@10 = 0.831; FB15K-237 HITS@1 = 0.217, HITS@3 = 0.329, HITS@10 = 0.456. (See Tables 3 and 4 for more dataset-specific metrics.)</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>History-dependent (memory-based) policy with path encoding (LSTM), ability to stay (NO_OP) and to backtrack (inverse edges); beam search at inference for ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Multiple relationships reported: (1) Inference cost and average runtime depend on node degree distribution rather than total number of entities; for power-law degree graphs average inference time scales with degree-distribution parameter (paper gives O(alpha/(alpha-1)) for alpha>1 and median O(1)). (2) The frequency/repetition of predictive path types strongly affects learnability: datasets where useful path-types repeat often (e.g., NELL-995) enable MINERVA to learn generalizable path policies; datasets with many relation types and rare repeating path-types (e.g., FB15K-237) make learning predictive generalized path policies harder and reduce performance. (3) Required path length (longer reasoning chains) affects policy complexity: MINERVA handles longer chains (Countries S3, synthetic grid tasks) better than some baselines. (4) Removing history (no LSTM) substantially degrades performance, especially on datasets/tasks with longer required paths.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>The paper compares performance across datasets with differing graph characteristics: MINERVA outperforms baselines on datasets where predictive path patterns repeat (e.g., NELL-995, Countries S3) and achieves strong performance on WN18RR; performance is weaker on FB15K-237, attributed to many relation types and low repetition of path-types. They also vary path-lengths (Countries S1-S3, synthetic grid) and observe MINERVA is robust to longer path requirements compared to NeuralLP and other path-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Key findings tying policy structure to topology: (a) History encoding (LSTM) is crucial for environments requiring multi-hop reasoning—without it, HITS@1 and other metrics drop substantially and grid-world long-path accuracy collapses. (b) NO_OP actions let the policy handle variable-length reasoning and allow early-stopping behavior by staying at an answer node. (c) Inverse relations enable backtracking to recover from wrong steps. (d) A single shared policy across relations can leverage cross-relation correlations and performs competitively with per-relation trained policies, though per-relation models can sometimes slightly outperform for particular relations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1375.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1375.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GridWorld-16x16</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>16-by-16 Synthetic Grid World (Yang et al., 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic 2D grid navigation environment where each cell is an entity and directed labeled edges correspond to cardinal (and diagonal) directions; queries are sequences of directions and the agent must navigate from a random start cell to a target cell following the query.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Differentiable learning of logical rules for knowledge base reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>16x16 Grid World</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Synthetic spatial grid: nodes are grid cells, edges encode directional adjacency (e.g., North, SouthWest, East). Domain: synthetic path-finding to test long multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td>30 steps (maximum Manhattan shortest path between opposite corners in a 16x16 grid).</td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Regular 2D lattice connectivity: each node connects to its neighboring cells according to allowed directions (up to 4 or more with diagonals), sparse local connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>256 nodes (16 x 16 grid); edges connect adjacent cells according to direction labels.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MINERVA (LSTM REINFORCE) and baseline NeuralLP</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>MINERVA: LSTM-history policy selecting labeled direction edges conditioned on the query; trained with REINFORCE. Compared against NeuralLP and other baselines to test robustness to longer path queries.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Final accuracy (success rate) as a function of required path length (i.e., robustness to longer multi-hop navigation), plus standard classification/accuracy metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>MINERVA final accuracy reported as 96.7% (overall), compared to NeuralLP 94.6%; MINERVA shows minimal degradation even for the longest path classes in the dataset (figure reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>96.7% accuracy (MINERVA) on the grid-world benchmark as reported in Table 6 / Fig. 3.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory-based (history-encoding) policy; policies that remember past actions and state (LSTM) perform best for long-path navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Higher path length (greater effective diameter between start and target) increases the need for memory/history in the policy. MINERVA (history-based) maintains high performance as path length increases, whereas methods without explicit path-history (e.g., ablated or some baselines) degrade sharply.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>The experiment varies required path length (different difficulty classes). Results show MINERVA is significantly more robust to increasing path length than NeuralLP, with minimal performance degradation for the longest paths.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Successful navigation on long chains requires policies that encode action history (LSTM); memory-less policies or policies without path-history perform poorly on longer paths. No special door/lock mechanics were necessary; the ability to represent sequences and backtrack was key.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1375.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1375.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WikiNav / Wikispeedia (related work mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>End-to-end goal-driven web navigation (WikiNav) / Wikispeedia: An online game for inferring semantic distances between concepts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related tasks mentioned in the paper where nodes are Wikipedia pages and agents navigate via hyperlinks using page text as observations; these are natural-language web-navigation environments used in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>End-to-end goal-driven web navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>WikiNav / Wikispeedia (web-page hyperlink navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Web-page navigation tasks: nodes represent Wikipedia pages, edges are hyperlinks; agents must traverse hyperlinks towards a target page using page text as observations (domain: web navigation / natural-language navigation).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Large hyperlink graph (sparse) with natural hyperlink connectivity; paper only cites these environments in related work and does not provide topology statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>End-to-end web-navigation agents (e.g., Nogueira & Cho's RL agent) - mentioned as related work</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned methods operate on natural-language page text as observations and train agents to navigate hyperlink graphs end-to-end; this paper only references these works and does not use them experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>End-to-end goal-driven web navigation <em>(Rating: 2)</em></li>
                <li>Wikispeedia: An online game for inferring semantic distances between concepts <em>(Rating: 2)</em></li>
                <li>Deeppath: A reinforcement learning method for knowledge graph reasoning <em>(Rating: 2)</em></li>
                <li>Differentiable learning of logical rules for knowledge base reasoning <em>(Rating: 2)</em></li>
                <li>Traversing knowledge graphs in vector space <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1375",
    "paper_id": "paper-e9f32b34fde7d0034a147d5996470634c9e19002",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "MINERVA",
            "name_full": "MINERVA (Meandering In Networks of Entities to Reach Verisimilar Answers)",
            "brief_description": "A reinforcement-learning agent that answers KB queries by walking a directed, labeled knowledge-graph from a query entity, selecting outgoing labeled edges conditioned on the query and path history to arrive at answer nodes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Knowledge-Graph Query Answering (various KB datasets)",
            "environment_description": "Knowledge bases represented as directed, labeled multigraphs where nodes are entities and labeled edges are binary relations (inverse edges added). Domains are real-world KBs (Countries, UMLS, Kinship, WN18RR, FB15K-237, NELL-995, WikiMovies).",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Directed labeled multigraph, generally sparse; connectivity statistics vary by dataset (average and median degree reported per dataset). The model adds inverse edges for bidirectional traversal and a NO_OP self-edge to allow variable-length paths.",
            "environment_size": "Multiple datasets (examples from Table 1): COUNTRIES: 272 entities, 1,158 facts (avg degree 4.35, median 4); UMLS: 135 entities, 5,216 facts (avg degree 38.63, median 28); KINSHIP: 104 entities, 10,686 facts (avg degree 82.15); WN18RR: 40,945 entities, 86,835 facts (avg degree 2.19); NELL-995: 75,492 entities, 154,213 facts (avg degree 4.07); FB15K-237: 14,505 entities, 272,115 facts (avg degree 19.74); WikiMovies: 43,230 entities, 196,453 facts (avg degree 6.65).",
            "agent_name": "MINERVA (LSTM policy trained with REINFORCE)",
            "agent_description": "History-dependent stochastic policy: LSTM encodes path history; entity and relation embeddings; at each step the agent scores outgoing edges (relation label + destination embedding) with a small MLP conditioned on the query relation and history, samples actions (edges), trained with policy gradients (REINFORCE) with a moving-average baseline and entropy regularization; uses NO_OP and inverse relations; inference uses beam search (beam width 50).",
            "exploration_efficiency_metric": "Reported metrics relevant to exploration/efficiency: training convergence time (wall-clock vs baselines), inference wall-clock time, dependence of inference cost on local degree (instead of O(|E|) ranking), task performance metrics (HITS@k, MRR, AUC-PR, MAP), and path-length sensitivity (accuracy vs required path length).",
            "exploration_efficiency_value": "Representative reported values: inference wall-clock time on WN18RR test set -- MINERVA 63s vs DistMult 211s; on NELL-995 test set -- MINERVA 35s vs DistMult 115s. Training uses 20 rollouts per training example; beam width 50 at inference. No single scalar sample-efficiency number given, but plots (Fig. 5) show MINERVA reaches higher HITS@10 faster than DistMult.",
            "success_rate": "Reported query-answering performance (which reflects navigation-to-answer): WN18RR HITS@1 = 0.413, HITS@3 = 0.456, HITS@10 = 0.513, MRR = 0.448; NELL-995 HITS@1 = 0.663, HITS@3 = 0.773, HITS@10 = 0.831; FB15K-237 HITS@1 = 0.217, HITS@3 = 0.329, HITS@10 = 0.456. (See Tables 3 and 4 for more dataset-specific metrics.)",
            "optimal_policy_type": "History-dependent (memory-based) policy with path encoding (LSTM), ability to stay (NO_OP) and to backtrack (inverse edges); beam search at inference for ranking.",
            "topology_performance_relationship": "Multiple relationships reported: (1) Inference cost and average runtime depend on node degree distribution rather than total number of entities; for power-law degree graphs average inference time scales with degree-distribution parameter (paper gives O(alpha/(alpha-1)) for alpha&gt;1 and median O(1)). (2) The frequency/repetition of predictive path types strongly affects learnability: datasets where useful path-types repeat often (e.g., NELL-995) enable MINERVA to learn generalizable path policies; datasets with many relation types and rare repeating path-types (e.g., FB15K-237) make learning predictive generalized path policies harder and reduce performance. (3) Required path length (longer reasoning chains) affects policy complexity: MINERVA handles longer chains (Countries S3, synthetic grid tasks) better than some baselines. (4) Removing history (no LSTM) substantially degrades performance, especially on datasets/tasks with longer required paths.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "The paper compares performance across datasets with differing graph characteristics: MINERVA outperforms baselines on datasets where predictive path patterns repeat (e.g., NELL-995, Countries S3) and achieves strong performance on WN18RR; performance is weaker on FB15K-237, attributed to many relation types and low repetition of path-types. They also vary path-lengths (Countries S1-S3, synthetic grid) and observe MINERVA is robust to longer path requirements compared to NeuralLP and other path-based methods.",
            "policy_structure_findings": "Key findings tying policy structure to topology: (a) History encoding (LSTM) is crucial for environments requiring multi-hop reasoning—without it, HITS@1 and other metrics drop substantially and grid-world long-path accuracy collapses. (b) NO_OP actions let the policy handle variable-length reasoning and allow early-stopping behavior by staying at an answer node. (c) Inverse relations enable backtracking to recover from wrong steps. (d) A single shared policy across relations can leverage cross-relation correlations and performs competitively with per-relation trained policies, though per-relation models can sometimes slightly outperform for particular relations.",
            "uuid": "e1375.0",
            "source_info": {
                "paper_title": "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning",
                "publication_date_yy_mm": "2017-11"
            }
        },
        {
            "name_short": "GridWorld-16x16",
            "name_full": "16-by-16 Synthetic Grid World (Yang et al., 2017)",
            "brief_description": "A synthetic 2D grid navigation environment where each cell is an entity and directed labeled edges correspond to cardinal (and diagonal) directions; queries are sequences of directions and the agent must navigate from a random start cell to a target cell following the query.",
            "citation_title": "Differentiable learning of logical rules for knowledge base reasoning",
            "mention_or_use": "use",
            "environment_name": "16x16 Grid World",
            "environment_description": "Synthetic spatial grid: nodes are grid cells, edges encode directional adjacency (e.g., North, SouthWest, East). Domain: synthetic path-finding to test long multi-hop reasoning.",
            "graph_diameter": "30 steps (maximum Manhattan shortest path between opposite corners in a 16x16 grid).",
            "clustering_coefficient": null,
            "dead_ends_present": false,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": null,
            "graph_connectivity": "Regular 2D lattice connectivity: each node connects to its neighboring cells according to allowed directions (up to 4 or more with diagonals), sparse local connectivity.",
            "environment_size": "256 nodes (16 x 16 grid); edges connect adjacent cells according to direction labels.",
            "agent_name": "MINERVA (LSTM REINFORCE) and baseline NeuralLP",
            "agent_description": "MINERVA: LSTM-history policy selecting labeled direction edges conditioned on the query; trained with REINFORCE. Compared against NeuralLP and other baselines to test robustness to longer path queries.",
            "exploration_efficiency_metric": "Final accuracy (success rate) as a function of required path length (i.e., robustness to longer multi-hop navigation), plus standard classification/accuracy metrics.",
            "exploration_efficiency_value": "MINERVA final accuracy reported as 96.7% (overall), compared to NeuralLP 94.6%; MINERVA shows minimal degradation even for the longest path classes in the dataset (figure reported in paper).",
            "success_rate": "96.7% accuracy (MINERVA) on the grid-world benchmark as reported in Table 6 / Fig. 3.",
            "optimal_policy_type": "Memory-based (history-encoding) policy; policies that remember past actions and state (LSTM) perform best for long-path navigation.",
            "topology_performance_relationship": "Higher path length (greater effective diameter between start and target) increases the need for memory/history in the policy. MINERVA (history-based) maintains high performance as path length increases, whereas methods without explicit path-history (e.g., ablated or some baselines) degrade sharply.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "The experiment varies required path length (different difficulty classes). Results show MINERVA is significantly more robust to increasing path length than NeuralLP, with minimal performance degradation for the longest paths.",
            "policy_structure_findings": "Successful navigation on long chains requires policies that encode action history (LSTM); memory-less policies or policies without path-history perform poorly on longer paths. No special door/lock mechanics were necessary; the ability to represent sequences and backtrack was key.",
            "uuid": "e1375.1",
            "source_info": {
                "paper_title": "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning",
                "publication_date_yy_mm": "2017-11"
            }
        },
        {
            "name_short": "WikiNav / Wikispeedia (related work mention)",
            "name_full": "End-to-end goal-driven web navigation (WikiNav) / Wikispeedia: An online game for inferring semantic distances between concepts",
            "brief_description": "Related tasks mentioned in the paper where nodes are Wikipedia pages and agents navigate via hyperlinks using page text as observations; these are natural-language web-navigation environments used in prior work.",
            "citation_title": "End-to-end goal-driven web navigation",
            "mention_or_use": "mention",
            "environment_name": "WikiNav / Wikispeedia (web-page hyperlink navigation)",
            "environment_description": "Web-page navigation tasks: nodes represent Wikipedia pages, edges are hyperlinks; agents must traverse hyperlinks towards a target page using page text as observations (domain: web navigation / natural-language navigation).",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "Large hyperlink graph (sparse) with natural hyperlink connectivity; paper only cites these environments in related work and does not provide topology statistics.",
            "environment_size": null,
            "agent_name": "End-to-end web-navigation agents (e.g., Nogueira & Cho's RL agent) - mentioned as related work",
            "agent_description": "Mentioned methods operate on natural-language page text as observations and train agents to navigate hyperlink graphs end-to-end; this paper only references these works and does not use them experimentally.",
            "exploration_efficiency_metric": null,
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": null,
            "topology_performance_relationship": null,
            "comparison_across_topologies": null,
            "topology_comparison_results": null,
            "policy_structure_findings": null,
            "uuid": "e1375.2",
            "source_info": {
                "paper_title": "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning",
                "publication_date_yy_mm": "2017-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "End-to-end goal-driven web navigation",
            "rating": 2
        },
        {
            "paper_title": "Wikispeedia: An online game for inferring semantic distances between concepts",
            "rating": 2
        },
        {
            "paper_title": "Deeppath: A reinforcement learning method for knowledge graph reasoning",
            "rating": 2
        },
        {
            "paper_title": "Differentiable learning of logical rules for knowledge base reasoning",
            "rating": 2
        },
        {
            "paper_title": "Traversing knowledge graphs in vector space",
            "rating": 1
        }
    ],
    "cost": 0.017419499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases USING REINFORCEMENT LEARNING</h1>
<p>Rajarshi Das ${ }^{<em>, 1}$, Shehzaad Dhuliawala ${ }^{</em>, 1}$, Manzil Zaheer ${ }^{*, 2}$<br>Luke Vilnis ${ }^{1}$, Ishan Durugkar ${ }^{3}$, Akshay Krishnamurthy ${ }^{1}$, Alex Smola ${ }^{4}$, Andrew McCallum ${ }^{1}$<br>{rajarshi, sdhuliawala, luke, akshay, mccallum}@cs.umass.edu<br>manzil@cmu.edu, ishand@cs.utexas.edu, alex@smola.org<br>${ }^{1}$ University of Massachusetts, Amherst, ${ }^{2}$ Carnegie Mellon University<br>${ }^{3}$ University of Texas at Austin, ${ }^{4}$ Amazon Web Services</p>
<h4>Abstract</h4>
<p>Knowledge bases (KB), both automatically and manually constructed, are often incomplete - many valid facts can be inferred from the KB by synthesizing existing information. A popular approach to KB completion is to infer new relations by combinatory reasoning over the information found along other paths connecting a pair of entities. Given the enormous size of KBs and the exponential number of paths, previous path-based models have considered only the problem of predicting a missing relation given two entities, or evaluating the truth of a proposed triple. Additionally, these methods have traditionally used random paths between fixed entity pairs or more recently learned to pick paths between them. We propose a new algorithm, MINERVA, which addresses the much more difficult and practical task of answering questions where the relation is known, but only one entity. Since random walks are impractical in a setting with unknown destination and combinatorially many paths from a start node, we present a neural reinforcement learning approach which learns how to navigate the graph conditioned on the input query to find predictive paths. On a comprehensive evaluation on seven knowledge base datasets, we found MINERVA to be competitive with many current state-of-the-art methods.</p>
<h2>1 INTRODUCTION</h2>
<p>Automated reasoning, the ability of computing systems to make new inferences from observed evidence, has been a long-standing goal of artificial intelligence. We are interested in automated reasoning on large knowledge bases (KB) with rich and diverse semantics (Suchanek et al., 2007; Bollacker et al., 2008; Carlson et al., 2010). KBs are highly incomplete (Min et al., 2013), and facts not directly stored in a KB can often be inferred from those that are, creating exciting opportunities and challenges for automated reasoning. For example, consider the small knowledge graph in Figure 1. We can answer the question "Who did Malala Yousafzai share her Nobel Peace prize with?" from the following reasoning path: Malala Yousafzai $\rightarrow$ WonAward $\rightarrow$ Nobel Peace Prize $2014 \rightarrow$ AwardedTo $\rightarrow$ Kailash Satyarthi. Our goal is to automatically learn such reasoning paths in KBs. We frame the learning problem as one of query answering, that is to say, answering questions of the form (Malala Yousafzai, SharesNobelPrizeWith, ?).</p>
<p>From its early days, the focus of automated reasoning approaches has been to build systems that can learn crisp symbolic logical rules (McCarthy, 1960; Nilsson, 1991). Symbolic representations have also been integrated with machine learning especially in statistical relational learning (Muggleton et al., 1992; Getoor \&amp; Taskar, 2007; Kok \&amp; Domingos, 2007; Lao et al., 2011), but due to poor generalization performance, these approaches have largely been superceded by distributed vector representations. Learning embedding of entities and relations using tensor factorization or neural methods has been a popular approach (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013, inter alia), but these methods cannot capture chains of reasoning expressed by KB paths. Neural multi-hop models (Neelakantan et al., 2015; Guu et al., 2015; Toutanova et al., 2016) address the aforementioned problems to some extent by operating on KB paths embedded in vector space. However, these models take as input a set of paths which are gathered by performing random walks</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A small fragment of a knowledge base represented as a knowledge graph. Solid edges are observed and dashed edges are part of queries. Note how each query relation (e.g. SharesNobelPrizeWith, Nationality, etc.) can be answered by traversing the graph via "logical" paths between entity 'Malala Yousafzai' and the corresponding answer.
independent of the query relation. Additionally, models such as those developed in Neelakantan et al. (2015); Das et al. (2017) use the same set of initially collected paths to answer a diverse set of query types (e.g. MarriedTo, Nationality, WorksIn etc.).
This paper presents a method for efficiently searching the graph for answer-providing paths using reinforcement learning (RL) conditioned on the input question, eliminating any need for precomputed paths. Given a massive knowledge graph, we learn a policy, which, given the query (entity ${ }<em 1="1">{1}$, relation, ?), starts from entity ${ }</em>$ and learns to walk to the answer node by choosing to take a labeled relation edge at each step, conditioning on the query relation and entire path history. This formulates the query-answering task as a reinforcement learning (RL) problem where the goal is to take an optimal sequence of decisions (choices of relation edges) to maximize the expected reward (reaching the correct answer node). We call the RL agent MINERVA for "Meandering In Networks of Entities to Reach Verisimilar Answers."</p>
<p>Our RL-based formulation has many desirable properties. First, MINERVA has the built-in flexibility to take paths of variable length, which is important for answering harder questions that require complex chains of reasoning (Shen et al., 2017). Secondly, MINERVA needs no pretraining and trains on the knowledge graph from scratch with reinforcement learning; no other supervision or fine-tuning is required representing a significant advance over prior applications of RL in NLP. Third, our path-based approach is computationally efficient, since by searching in a small neighborhood around the query entity it avoids ranking all entities in the KB as in prior work. Finally, the reasoning paths found by our agent automatically form an interpretable provenance for its predictions.</p>
<p>The main contributions of the paper are: (a) We present agent MINERVA, which learns to do query answering by walking on a knowledge graph conditioned on an input query, stopping when it reaches the answer node. The agent is trained using reinforcement learning, specifically policy gradients (§ 2). (b) We evaluate MINERVA on several benchmark datasets and compare favorably to Neural Theorem Provers (NTP) (Rocktäschel \&amp; Riedel, 2017) and Neural LP (Yang et al., 2017), which do logical rule learning in KBs, and also state-of-the-art embedding based methods such as DistMult (Yang et al., 2015) and ComplEx (Trouillon et al., 2016) and ConvE (Dettmers et al., 2018). (c) We also extend MINERVA to handle partially structured natural language queries and test it on the WikiMovies dataset (§ 3.3) (Miller et al., 2016).</p>
<p>We also compare to DeepPath (Xiong et al., 2017) which uses reinforcement learning to pick paths between entity pairs. The main difference is that the state of their RL agent includes the answer entity since it is designed for the simpler task of predicting if a fact is true or not. As such their method cannot be applied directly to our more challenging query answering task where the second entity is unknown and must be inferred. Nevertheless, MINERVA outperforms DeepPath on their benchmark NELL-995 dataset when compared in their experimental setting (§ 3.2.2).</p>
<h1>2 TASK AND MODEL</h1>
<p>We formally define the task of query answering in a KB. Let $\mathcal{E}$ denote the set of entities and $\mathcal{R}$ denote the set of binary relations. A KB is a collection of facts stored as triplets $\left(\mathrm{e}<em 2="2">{1}, \mathrm{r}, \mathrm{e}</em>}\right)$ where $\mathrm{e<em 2="2">{1}, \mathrm{e}</em>} \in \mathcal{E}$ and $\mathrm{r} \in \mathcal{R}$. From the KB , a knowledge graph $\mathcal{G}$ can be constructed where the entities $\mathrm{e<em 2="2">{1}, \mathrm{e}</em> \times V$. Also, following previous approaches}$ are represented as the nodes and relation $r$ as labeled edge between them. Formally, a knowledge graph is a directed labeled multigraph $\mathcal{G}=(V, E, \mathcal{R})$, where $V$ and $E$ denote the vertices and edges of the graph respectively. Note that $V=\mathcal{E}$ and $E \subseteq V \times \mathcal{R</p>
<p>(Bordes et al., 2013; Neelakantan et al., 2015; Xiong et al., 2017), we add the inverse relation of every edge, i.e. for an edge $\left(\mathrm{e}<em 2="2">{1}, \mathrm{r}, \mathrm{e}</em>}\right) \in E$, we add the edge $\left(\mathrm{e<em 1="1">{2}, \mathrm{r}^{-1}, \mathrm{e}</em>$ as well.)}\right)$ to the graph. (If the set of binary relations $\mathcal{R}$ does not contain the inverse relation $\mathrm{r}^{-1}$, it is added to $\mathcal{R</p>
<p>Since KBs have a lot of missing information, two natural tasks have emerged in the information extraction community - fact prediction and query answering. Query answering seeks to answer questions of the form $\left(\mathrm{e}_{1}, \mathrm{r}, ?\right)$, e.g. Toronto, locatedIn, ?, whereas fact prediction involves predicting if a fact is true or not, e.g. (Toronto, locatedIn, Canada)?. Algorithms for fact prediction can be used for query answering, but with significant computation overhead, since all candidate answer entities must be evaluated, making it prohibitively expensive for large KBs with millions of entities. In this work, we present a query answering model, that learns to efficiently traverse the knowledge graph to find the correct answer to a query, eliminating the need to evaluate all entities.</p>
<p>Query answering reduces naturally to a finite horizon sequential decision making problem as follows: We begin by representing the environment as a deterministic partially observed Markov decision process on a knowledge graph $\mathcal{G}$ derived from the KB (§2.1). Our RL agent is given an input query of the form $\left(\mathrm{e}<em _mathrm_q="\mathrm{q">{1 \mathrm{q}}, \mathrm{r}</em>$, the agent follows a path in the graph stopping at a node that it predicts as the answer (§ 2.2). Using a training set of known facts, we train the agent using policy gradients more specifically by REINFORCE (Williams, 1992) with control variates (§ 2.3). Let us begin by describing the environment.}}, ?\right)$. Starting from vertex corresponding to $e_{1 q}$ in $\mathcal{G</p>
<h1>2.1 Environment - States, Actions, Transitions and Rewards</h1>
<p>Our environment is a finite horizon, deterministic partially observed Markov decision process that lies on the knowledge graph $\mathcal{G}$ derived from the KB. On this graph we will now specify a deterministic partially observed Markov decision process, which is a 5-tuple $(\mathcal{S}, \mathcal{O}, \mathcal{A}, \delta, R)$, each of which we elaborate below.</p>
<p>States. The state space $\mathcal{S}$ consists of all valid combinations in $\mathcal{E} \times \mathcal{E} \times \mathcal{R} \times \mathcal{E}$. Intuitively, we want a state to encode the query $\left(\mathrm{e}<em _mathrm_q="\mathrm{q">{1 \mathrm{q}}, \mathrm{r}</em>}}\right)$, the answer $\left(\mathrm{e<em _mathrm_t="\mathrm{t">{2 \mathrm{q}}\right)$, and a location of exploration $\mathrm{e}</em>}}$ (current location of the RL agent). Thus overall a state $S \in \mathcal{S}$ is represented by $S=\left(\mathrm{e<em 1="1" _mathrm_q="\mathrm{q">{\mathrm{t}}, \mathrm{e}</em>}}, \mathrm{r<em 2="2" _mathrm_q="\mathrm{q">{\mathrm{q}}, \mathrm{e}</em>\right)$ and the state space consists of all valid combinations.}</p>
<p>Observations. The complete state of the environment is not observed. Intuitively, the agent knows its current location $\left(\mathrm{e}<em 1="1" _mathrm_q="\mathrm{q">{\mathrm{t}}\right)$ and $\left(\mathrm{e}</em>}}, \mathrm{r<em 2="2" _mathrm_q="\mathrm{q">{\mathrm{q}}\right)$, but not the answer $\left(\mathrm{e}</em>}}\right)$, which remains hidden. Formally, the observation function $\mathcal{O}: \mathcal{S} \rightarrow \mathcal{E} \times \mathcal{E} \times \mathcal{R}$ is defined as $\mathcal{O}\left(s=\left(\mathrm{e<em 1="1" _mathrm_q="\mathrm{q">{\mathrm{t}}, \mathrm{e}</em>}}, \mathrm{r<em 2="2" _mathrm_q="\mathrm{q">{\mathrm{q}}, \mathrm{e}</em>}}\right)\right)=\left(\mathrm{e<em 1="1" _mathrm_q="\mathrm{q">{\mathrm{t}}, \mathrm{e}</em>}}, \mathrm{r<em S="S">{\mathrm{q}}\right)$.
Actions. The set of possible actions $\mathcal{A}</em>}$ from a state $S=\left(\mathrm{e<em 1="1" _mathrm_q="\mathrm{q">{\mathrm{t}}, \mathrm{e}</em>}}, \mathrm{r<em 2="2" _mathrm_q="\mathrm{q">{\mathrm{q}}, \mathrm{e}</em>}}\right)$ consists of all outgoing edges of the vertex $\mathrm{e<em S="S">{\mathrm{t}}$ in $\mathcal{G}$. Formally $\mathcal{A}</em>}=\left{\left(\mathrm{e<em _mathrm_t="\mathrm{t">{\mathrm{t}}, r, v\right) \in E: S=\left(\mathrm{e}</em>}}, \mathrm{e<em _mathrm_q="\mathrm{q">{1 \mathrm{q}}, \mathrm{r}</em>, v \in V\right} \cup{(s, \varnothing, s)}$. Basically, this means an agent at each state has option to select which outgoing edge it wishes to take having the knowledge of the label of the edge $r$ and destination vertex $v$.}}, \mathrm{e}_{2 \mathrm{q}}\right), r \in \mathcal{R</p>
<p>During implementation, we unroll the computation graph up to a fixed number of time steps T. We augment each node with a special action called 'NO_OP' which goes from a node to itself. Some questions are easier to answer and needs fewer steps of reasoning than others. This design decision allows the agent to remain at a node for any number of time steps. This is especially helpful when the agent has managed to reach a correct answer at a time step $t&lt;\mathrm{T}$ and can continue to stay at the 'answer node' for the rest of the time steps. Alternatively, we could have allowed the agent to take a special 'STOP' action, but we found the current setup to work sufficiently well. As mentioned before, we also add the inverse relation of a triple, i.e. for the triple $\left(e_{1}, r, e_{2}\right)$, we add the triple $\left(e_{2}, r^{-1}, e_{1}\right)$ to the graph. We found this important because this actually allows our agent to undo a potentially wrong decision.</p>
<p>Transition. The environment evolves deterministically by just updating the state to the new vertex incident to the edge selected by the agent. The query and answer remains the same. Formally, the transition function is $\delta: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$ defined by $\delta(S, A)=\left(v, \mathrm{e}<em _mathrm_q="\mathrm{q">{1 \mathrm{q}}, \mathrm{r}</em>}}, \mathrm{e<em _mathrm_t="\mathrm{t">{2 \mathrm{q}}\right)$, where $S=\left(\mathrm{e}</em>}}, \mathrm{e<em _mathrm_q="\mathrm{q">{1 \mathrm{q}}, \mathrm{r}</em>}}, \mathrm{e<em _mathrm_t="\mathrm{t">{2 \mathrm{q}}\right)$ and $A=\left(\mathrm{e}</em>, r, v\right)$ ).
Rewards. We only have a terminal reward of +1 if the current location is the correct answer at the end and 0 otherwise. To elaborate, if $S_{T}=\left(\mathrm{e}}<em 1="1" _mathrm_q="\mathrm{q">{\mathrm{t}}, \mathrm{e}</em>}}, \mathrm{r<em 2="2" _mathrm_q="\mathrm{q">{\mathrm{q}}, \mathrm{e}</em>}}\right)$ is the final state, then we receive a reward of +1 if $\mathrm{e<em 2="2" _mathrm_q="\mathrm{q">{\mathrm{t}}=\mathrm{e}</em>}}$ else 0 ., i.e. $R\left(S_{T}\right)=\mathbb{I}\left{\mathrm{e<em 2="2" _mathrm_q="\mathrm{q">{\mathrm{t}}=\mathrm{e}</em>\right}$.}</p>
<h1>2.2 Policy Network</h1>
<p>To solve the finite horizon deterministic partially observable Markov decision process described above, we design a randomized non-stationary history-dependent policy $\pi=\left(\mathbf{d}<em _mathbf_2="\mathbf{2">{\mathbf{1}}, \mathbf{d}</em>}}, \ldots, \mathbf{d<em _mathbf_t="\mathbf{t">{\mathbf{T}-\mathbf{1}}\right)$, where $\mathbf{d}</em>}}: H_{t} \rightarrow \mathcal{P}\left(\mathcal{A<em t="t">{S</em>\right)$ is just the sequence of observations and actions taken. We restrict ourselves to policies parameterized by long short-term memory network (LSTM) (Hochreiter \&amp; Schmidhuber, 1997).
An agent based on LSTM encodes the history $H_{t}$ as a continuous vector $\mathbf{h}}}\right)$ and history $H_{t}=\left(H_{t-1}, A_{t-1}, O_{t<em t="t">{\mathbf{t}} \in \mathbb{R}^{2 d}$. We also have embedding matrix $\mathbf{r} \in \mathbb{R}^{|\mathcal{R}| \times d}$ and $\mathbf{e} \in \mathbb{R}^{|\mathcal{E}| \times d}$ for the binary relations and entities respectively. The history embedding for $H</em>\right)$ is updated according to LSTM dynamics:}=\left(H_{t-1}, A_{t-1}, O_{t</p>
<p>$$
\mathbf{h}<em _mathbf_t="\mathbf{t">{\mathbf{t}}=\operatorname{LSTM}\left(\mathbf{h}</em>}-\mathbf{1}},\left[\mathbf{a<em _mathbf_t="\mathbf{t">{\mathbf{t}-\mathbf{1}} ; \mathbf{o}</em>\right]\right)
$$}</p>
<p>where $\mathbf{a}<em _mathbf_t="\mathbf{t">{\mathbf{t}-\mathbf{1}} \in \mathbb{R}^{d}$ and $\mathbf{o}</em>}} \in \mathbb{R}^{d}$ denote the vector representation for action/relation at time $t-1$ and observation/entity at time $t$ respectively and $[;]$ denote vector concatenation. To elucidate, $\mathbf{a<em A__t-1="A_{t-1">{\mathbf{t}-\mathbf{1}}=\mathbf{r}</em>}}$, i.e. the embedding of the relation corresponding to label of the edge the agent chose at time $t-1$ and $\mathbf{o<em _mathrm_e="\mathrm{e">{\mathbf{t}}=\mathbf{e}</em><em t="t">{\mathrm{t}}}$ if $O</em>}=\left(\mathrm{e<em 1="1" _mathrm_q="\mathrm{q">{\mathrm{t}}, \mathrm{e}</em>}}, \mathrm{r<em _mathbf_t="\mathbf{t">{\mathrm{q}}\right)$ i.e. the embedding of the entity corresponding to vertex the agent is at time $t$.
Based on the history embedding $\mathbf{h}</em>}}$, the policy network makes the decision to choose an action from all available actions $\left(\mathcal{A<em t="t">{S</em>}}\right)$ conditioned on the query relation. Recall that each possible action represents an outgoing edge with information of the edge relation label $l$ and destination vertex/entity $d$. So embedding for each $A \in \mathcal{A<em t="t">{S</em>}}$ is $\left[\mathbf{r<em _mathbf_d="\mathbf{d">{\mathbf{l}} ; \mathbf{e}</em>}}\right]$, and stacking embeddings for all the outgoing edges we obtain the matrix $\mathbf{A<em _mathbf_t="\mathbf{t">{\mathbf{t}}$. The network taking these as inputs is parameterized as a two-layer feedforward network with ReLU nonlinearity which takes in the current history representation $\mathbf{h}</em>$ and outputs a probability distribution over the possible actions from which a discrete action is sampled. In other words,}}$ and the embedding for the query relation $\mathbf{r}_{\mathbf{q}</p>
<p>$$
\begin{aligned}
&amp; \mathbf{d}<em _mathbf_t="\mathbf{t">{\mathbf{t}}=\operatorname{softmax}\left(\mathbf{A}</em>}}\left(\mathbf{W<em _mathbf_1="\mathbf{1">{\mathbf{2}} \operatorname{ReLU}\left(\mathbf{W}</em>}}\left[\mathbf{h<em _mathbf_t="\mathbf{t">{\mathbf{t}} ; \mathbf{o}</em>}} ; \mathbf{r<em t="t">{\mathbf{q}}\right]\right)\right)\right) \
&amp; A</em>\right)
\end{aligned}
$$} \sim \text { Categorical }\left(\mathbf{d}_{\mathbf{t}</p>
<p>Note that the nodes in $\mathcal{G}$ do not have a fixed ordering or number of edges coming out from them. The size of matrix $\mathbf{A}<em S__t="S_{t">{\mathbf{t}}$ is $\left|\mathcal{A}</em>}}\right| \times 2 d$, so the decision probabilities $d_{t}$ lies on simplex of size $\left|\mathcal{A<em t="t">{S</em>}}\right|$. Also the procedure above is invariant to order in which edges are presented as desired and falls in purview of neural networks designed to be permutation invariant (Zaheer et al., 2017). Finally, to summarize, the parameters of the LSTM, the weights $\mathbf{W<em _mathbf_2="\mathbf{2">{\mathbf{1}}, \mathbf{W}</em>$, the corresponding biases (not shown above for brevity), and the embedding matrices form the parameters $\theta$ of the policy network.}</p>
<h3>2.3 Training</h3>
<p>For the policy network $\left(\pi_{\theta}\right)$ described above, we want to find parameters $\theta$ that maximize the expected reward:</p>
<p>$$
J(\theta)=\mathbb{E}<em 1="1">{\left(e</em>}, r, e_{2}\right) \sim D} \mathbb{E<em 1="1">{A</em>\right)\right]
$$}, \ldots, A_{T-1} \sim \pi_{\theta}}\left[R\left(S_{T}\right) \mid S_{1}=\left(e_{1}, e_{1}, r, e_{2</p>
<p>where we assume there is a true underlying distribution $\left(\mathrm{e}<em 2="2">{1}, \mathrm{r}, \mathrm{e}</em>\right) \sim D$. To solve this optimization problem, we employ REINFORCE (Williams, 1992) as follows:</p>
<ul>
<li>The first expectation is replaced with empirical average over the training dataset.</li>
<li>For the second expectation, we approximate by running multiple rollouts for each training example. The number of rollouts is fixed and for all our experiments we set this number to 20.</li>
<li>For variance reduction, a common strategy is to use an additive control variate baseline (Hammersley, 2013; Fishman, 2013; Evans \&amp; Swartz, 2000). We use a moving average of the cumulative discounted reward as the baseline. We tune the weight of this moving average as a hyperparameter. Note that in our experiments we found that using a learned baseline performed similarly, but we finally settled for cumulative discounted reward as the baseline owing to its simplicity.</li>
<li>To encourage diversity in the paths sampled by the policy at training time, we add an entropy regularization term to our cost function scaled by a constant $(\beta)$.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;">#entities</th>
<th style="text-align: right;">#relations</th>
<th style="text-align: right;">#facts</th>
<th style="text-align: right;">#queries</th>
<th style="text-align: right;">#degree</th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;">avg.</td>
<td style="text-align: right;">median</td>
</tr>
<tr>
<td style="text-align: left;">COUNTRIES</td>
<td style="text-align: right;">272</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">1158</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">4.35</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: left;">UMLS</td>
<td style="text-align: right;">135</td>
<td style="text-align: right;">49</td>
<td style="text-align: right;">5,216</td>
<td style="text-align: right;">661</td>
<td style="text-align: right;">38.63</td>
<td style="text-align: right;">28</td>
</tr>
<tr>
<td style="text-align: left;">KINSHIP</td>
<td style="text-align: right;">104</td>
<td style="text-align: right;">26</td>
<td style="text-align: right;">10686</td>
<td style="text-align: right;">1074</td>
<td style="text-align: right;">82.15</td>
<td style="text-align: right;">82</td>
</tr>
<tr>
<td style="text-align: left;">WN18RR</td>
<td style="text-align: right;">40,945</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">86,835</td>
<td style="text-align: right;">3134</td>
<td style="text-align: right;">2.19</td>
<td style="text-align: right;">2</td>
</tr>
<tr>
<td style="text-align: left;">NELL-995</td>
<td style="text-align: right;">75,492</td>
<td style="text-align: right;">200</td>
<td style="text-align: right;">154,213</td>
<td style="text-align: right;">3992</td>
<td style="text-align: right;">4.07</td>
<td style="text-align: right;">1</td>
</tr>
<tr>
<td style="text-align: left;">FB15K-237</td>
<td style="text-align: right;">14,505</td>
<td style="text-align: right;">237</td>
<td style="text-align: right;">272,115</td>
<td style="text-align: right;">20,466</td>
<td style="text-align: right;">19.74</td>
<td style="text-align: right;">14</td>
</tr>
<tr>
<td style="text-align: left;">WikiMovies</td>
<td style="text-align: right;">43,230</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">196,453</td>
<td style="text-align: right;">9952</td>
<td style="text-align: right;">6.65</td>
<td style="text-align: right;">4</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of various datasets used in experiments.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">ComplEx</th>
<th style="text-align: right;">ConvE</th>
<th style="text-align: right;">DistMult</th>
<th style="text-align: right;">NTP</th>
<th style="text-align: right;">NTP- $\lambda$</th>
<th style="text-align: right;">NeuralLP</th>
<th style="text-align: right;">MINERVA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">S1</td>
<td style="text-align: center;">$99.37 \pm 0.4$</td>
<td style="text-align: right;">$\mathbf{1 0 0 . 0} \pm \mathbf{0 . 0 0}$</td>
<td style="text-align: right;">$97.91 \pm 0.01$</td>
<td style="text-align: right;">$90.83 \pm 15.4$</td>
<td style="text-align: right;">$\mathbf{1 0 0 . 0} \pm \mathbf{0 . 0 0}$</td>
<td style="text-align: right;">$\mathbf{1 0 0 . 0} \pm \mathbf{0 . 0}$</td>
<td style="text-align: right;">$\mathbf{1 0 0 . 0} \pm \mathbf{0 . 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">S2</td>
<td style="text-align: center;">$87.95 \pm 2.8$</td>
<td style="text-align: right;">$\mathbf{9 9 . 0} \pm \mathbf{1 . 0 0}$</td>
<td style="text-align: right;">$69.18 \pm 2.38$</td>
<td style="text-align: right;">$87.40 \pm 11.7$</td>
<td style="text-align: right;">$93.04 \pm 0.40$</td>
<td style="text-align: right;">$75.1 \pm 0.3$</td>
<td style="text-align: right;">$92.36 \pm 2.41$</td>
</tr>
<tr>
<td style="text-align: left;">S3</td>
<td style="text-align: center;">$48.44 \pm 6.3$</td>
<td style="text-align: right;">$86.0 \pm 5.00$</td>
<td style="text-align: right;">$15.79 \pm 0.64$</td>
<td style="text-align: right;">$56.68 \pm 17.6$</td>
<td style="text-align: right;">$77.26 \pm 17.0$</td>
<td style="text-align: right;">$92.2 \pm 0.2$</td>
<td style="text-align: right;">$\mathbf{9 5 . 1 0} \pm \mathbf{1 . 2 0}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance on three tasks of COUNTRIES dataset with AUC-PR metric. MINERVA significantly outperforms all other methods on the hardest task (S3). Also variance across runs for MINERVA is lower compared to other methods.</p>
<h1>3 EXPERIMENTS</h1>
<p>We now present empirical studies for MINERVA in order to establish that (i) MINERVA is competitive for query answering on small (Sec. 3.1.1) as well as large KBs (Sec. 3.1.2), (ii) MINERVA is superior to a path based models that do not search the KB efficiently or train query specific models (Sec. 3.2), (iii) MINERVA can not only be used for well formed queries, but can also easily handle partially structured natural language queries (Sec 3.3), (iv) MINERVA is highly capable of reasoning over long chains, and (v) MINERVA is robust to train and has much faster inference time (Sec. 3.5).</p>
<h3>3.1 Knowledge Base Query Answering</h3>
<p>To gauge the reasoning capability of MINERVA, we begin with task of query answering on KB, i.e. we want to answer queries of the form $\left(e_{1}, r, ?\right)$. Note that, as mentioned in Sec. 2, this task is subtly different from fact checking in a KB. Also, as most of the previous literature works in the regime of fact checking, their ranking includes variations of both $\left(e_{1}, r, x\right)$ and $\left(x, r, e_{2}\right)$. However, since we do not have access to $e_{2}$ in case of question answering scenario the same ranking procedure does not hold for us - we only need to rank on $\left(e_{1}, r, x\right)$. This difference in ranking made it necessary for us to re-run all the implementations of previous work. We used the implementation or the best pre-trained models (whenever available) of Rocktäschel \&amp; Riedel (2017); Yang et al. (2017) and Dettmers et al. (2018). For MINERVA to produce a ranking of answer entities during inference, we do a beam search with a beam width of 50 and rank entities by the probability of the trajectory the model took to reach the entity and remaining entities are given a rank of $\infty$.</p>
<p>Method We compare MINERVA with various state-of-the-art models using HITS @ $1,3,10$ and mean reciprocal rank (MRR), which are standard metrics for KB completion tasks. In particular we compare against embedding based models - DistMult (Yang et al., 2015), ComplEx (Trouillon et al., 2016) and ConvE (Dettmers et al., 2018). For ConvE and ComplEx, we used the implementation released by Dettmers et al. (2018) ${ }^{1}$ on the best hyperparameter settings reported by them. For DistMult, we use our highly tuned implementation (e.g. which performs better than the state-of-the-art results of Toutanova et al. (2015)). We also compare with two recent work in learning logical rules in KB namely Neural Theorem Provers (NTP) (Rocktäschel \&amp; Riedel, 2017) and NeuralLP (Yang et al., 2017). Rocktäschel \&amp; Riedel (2017) also reports a NTP model which is trained with an additional objective function of ComplEx (NTP- $\lambda$ ). For these models, we used the implementation released by corresponding authors ${ }^{23}$, again on the best hyperparameter settings reported by them.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Data</th>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">ComplEx</th>
<th style="text-align: center;">ConvE</th>
<th style="text-align: center;">DistMult</th>
<th style="text-align: center;">NTP</th>
<th style="text-align: center;">NTP- $\lambda$</th>
<th style="text-align: center;">NeuralLP</th>
<th style="text-align: center;">MINERVA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">KINSHIP</td>
<td style="text-align: center;">HITS@1</td>
<td style="text-align: center;">0.754</td>
<td style="text-align: center;">0.697</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.759</td>
<td style="text-align: center;">0.475</td>
<td style="text-align: center;">0.605</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HITS@3</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.942</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.798</td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.812</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HITS@10</td>
<td style="text-align: center;">0.980</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.979</td>
<td style="text-align: center;">0.777</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.924</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MRR</td>
<td style="text-align: center;">0.838</td>
<td style="text-align: center;">0.797</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.612</td>
<td style="text-align: center;">0.793</td>
<td style="text-align: center;">0.619</td>
<td style="text-align: center;">0.720</td>
</tr>
<tr>
<td style="text-align: center;">UMLS</td>
<td style="text-align: center;">HITS@1</td>
<td style="text-align: center;">0.823</td>
<td style="text-align: center;">0.894</td>
<td style="text-align: center;">0.916</td>
<td style="text-align: center;">0.817</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">0.643</td>
<td style="text-align: center;">0.728</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HITS@3</td>
<td style="text-align: center;">0.962</td>
<td style="text-align: center;">0.964</td>
<td style="text-align: center;">0.967</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">0.983</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">0.900</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HITS@10</td>
<td style="text-align: center;">0.995</td>
<td style="text-align: center;">0.992</td>
<td style="text-align: center;">0.992</td>
<td style="text-align: center;">0.970</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.962</td>
<td style="text-align: center;">0.968</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MRR</td>
<td style="text-align: center;">0.894</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.778</td>
<td style="text-align: center;">0.825</td>
</tr>
</tbody>
</table>
<p>Table 3: Query answering results on KINSHIP and UMLS datasets.</p>
<h1>3.1.1 Smaller Datasets</h1>
<p>Dataset We use three standard datasets: Countries (Bouchard et al., 2015), KINSHIP, and UMLS (Kok \&amp; Domingos, 2007). The Countries dataset ontains countries, regions, and subregions as entities and is carefully designed to explicitly test the logical rule learning and reasoning capabilities of link prediction models. The queries are of the form LocatedIn(c, ?) and the answer is a region (e.g. LocatedIn(Egypt, ?) with the answer as Africa). The dataset has 3 tasks (S1-3 in table 2) each requiring reasoning steps of increasing length and difficulty (see Rocktäschel \&amp; Riedel (2017) for more details about the tasks). Following the design of the Countries dataset, for task S1 and S2, we set the maximum path length $T=2$ and for S3, we set $T=3$. The Unified Medical Language System (UMLS) dataset, is from biomedicine. The entities are biomedical concepts (e.g. disease, antibiotic) and relations are like treats and diagnoses. The KINSHIP dataset contains kinship relationships among members of the Alyawarra tribe from Central Australia. For these two task we use maximum path length $T=2$. Also, for MINERVA we turn off entity in (1) in these experiments.</p>
<p>Observations For the countries dataset, in Table 2 we report a stronger metric - the area under the precision-recall curve - as is common in the literature. We can see that MINERVA compares favorably or outperforms all the baseline models except on the task S2 of COUnTRIES, where the ensemble model NTP- $\lambda$ and ConvE outperforms it, albeit with a higher variance across runs. Our gains are much more prominent in task S3, which is the hardest among all the tasks.</p>
<p>The Kinship and UMLS datasets are small KB datasets with around 100 entities each and as we see from Table 3, embedding based methods (ConvE, ComplEx and DistMult) perform much better than methods which aim to learn logical rules (NTP, NeuralLP and MINERVA). On Kinship, MINERVA outperforms both NeuralLP and NTP and matches the HITS@10 performance of NTP on UMLS. Unlike COUnTRIES, these datasets were not designed to test the logical rule learning ability of models and given the small size, embedding based models are able to get really high performance. Combination of both methods gives a slight increase in performance as can be seen from the results of NTP- $\lambda$. However, when we initialized MINERVA with pre-trained embeddings of ComplEx, we did not find a significant increase in performance.</p>
<h3>3.1.2 LARGER DATASETS</h3>
<p>Dataset Next we evaluate MINERVA on three large KG datasets - WN18RR, FB15K-237 and NELL995. The WN18RR (Dettmers et al., 2018) and FB15K-237 (Toutanova et al., 2015) datasets are created from the original WN18 and FB 15 K datasets respectively by removing various sources of test leakage, making the datasets more realistic and challenging. The NELL-995 dataset released by Xiong et al. (2017) has separate graphs for each query relation, where a graph for a query relation can have triples from the test set of another query relation. For the query answering experiment, we combine all the graphs and removed all test triples (and the corresponding triples with inverse relations) from the graph. We also noticed that several triples in the test set had an entity (source or target) that never appeared in the graph. Since, there will be no trained embeddings for those entities, we removed them from the test set. This reduced the size of test set from 3992 queries to 2818 queries. ${ }^{4}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Data</th>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">ComplEx</th>
<th style="text-align: center;">ConvE</th>
<th style="text-align: center;">DistMult</th>
<th style="text-align: center;">NeuralLP</th>
<th style="text-align: center;">Path-Baseline</th>
<th style="text-align: center;">MINERVA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">WN18RR</td>
<td style="text-align: center;">HITS@1</td>
<td style="text-align: center;">0.382</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">0.410</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.017</td>
<td style="text-align: center;">0.413</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HITS@3</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">0.025</td>
<td style="text-align: center;">0.456</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HITS@10</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.475</td>
<td style="text-align: center;">0.657</td>
<td style="text-align: center;">0.046</td>
<td style="text-align: center;">0.513</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MRR</td>
<td style="text-align: center;">0.415</td>
<td style="text-align: center;">0.438</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.463</td>
<td style="text-align: center;">0.027</td>
<td style="text-align: center;">0.448</td>
</tr>
<tr>
<td style="text-align: center;">FB15K-237</td>
<td style="text-align: center;">HITS@1</td>
<td style="text-align: center;">0.303</td>
<td style="text-align: center;">0.313</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">0.166</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">0.217</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HITS@3</td>
<td style="text-align: center;">0.434</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.329</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HITS@10</td>
<td style="text-align: center;">0.572</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.348</td>
<td style="text-align: center;">0.357</td>
<td style="text-align: center;">0.456</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MRR</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.410</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.293</td>
</tr>
<tr>
<td style="text-align: center;">NELL-995</td>
<td style="text-align: center;">HITS@1</td>
<td style="text-align: center;">0.612</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.663</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HITS@3</td>
<td style="text-align: center;">0.761</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.773</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HITS@10</td>
<td style="text-align: center;">0.827</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">0.795</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.497</td>
<td style="text-align: center;">0.831</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MRR</td>
<td style="text-align: center;">0.694</td>
<td style="text-align: center;">0.747</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.725</td>
</tr>
</tbody>
</table>
<p>Table 4: Query answering results on WN18RR, FB15K-237 and NELL-995 datasets. NeuralLP does not scale to NELL-995 and hence the entries are kept blank.</p>
<p>Observations Table 4 reports the query answering results on the larger WN18RR, FB15K-237 and NELL-995 datasets. We could not include the results of NeuralLP on NELL-995 since it didn't scale to that size. Similarly NTP did not scale to any of the larger datasets. Apart from these, we are the first to report a comprehensive summary of performance of all baseline methods on these datasets.</p>
<p>On NELL-995, MINERVA performs comparably to embedding based methods such as DistMult and ComplEx and performs comparably with ConvE on the stricter HITS@1 metric. ConvE, however outperforms us on HITS@ 10 on NELL-995. On WN18RR, logic based based methods (NeuralLP, MINERVA) generally outperform embedding based methods, with MINERVA achieving the highest score on HITS@1 metric and NeuralLP significantly outperforming on HITS@10.
We observe that on FB15K-237, however, embedding based methods dominate over MINERVA and NeuralLP. Upon deeper inspection, we found that the query relation types of FB15K-237 knowledge graph differs significantly from others.
Analysis of query relations of FB15k-237: We analyzed the type of query relation types on the FB15K-237 dataset. Following Bordes et al. (2013), we categorized the query relations into (M)any to 1, 1 to M or 1 to 1 relations. An example of a M to 1 relation would be '/people/profession' (What is the profession of person ' $\mathrm{X}^{\prime}$ '?). An example of 1 to M relation would be /music/instrument/instrumentalists ('Who plays the music instrument X ?') or '/people/ethnicity/people' ('Who are people with ethnicity X ?'). From a query answering point of view, the answer to these questions is a list of entities. However, during evaluation time, the model is evaluated based on whether it is able to predict the one target entity which is in the query triple. Also, since MINERVA outputs the end points of the paths as target entities, it is sometimes possible that the particular target entity of the triple does not have a path from the source entity (however there are paths to other 'correct' answer entities). Table 9 (in appendix) shows few other examples of relations belonging to different classes.
Following Bordes et al. (2013), we classify a relation as 1-to-M if the ratio of cardinality of tail to head entities is greater than 1.5 and as M-to-1 if it is lesser than 0.67 . In the validation set of FB15K-237, $54 \%$ of the queries are 1-to-M, whereas only $26 \%$ are M-to-1. Contrasting it with NELL-995, 27\% are 1-to-M and $36 \%$ are M-to-1 or UMLS where only $18 \%$ are 1-to-M. Table 10 (in appendix) shows few relations from FB15K-237 dataset which have high tail-to-head ratio. The average ratio for 1-TO-M relations in FB 15 K - 237 is 13.39 (substantially higher than 1.5). As explained before, the current evaluation scheme is not suited when it comes to 1-to-M relations and the high percentage of 1-to-M relations in FB 15 K - 237 also explains the sub optimal performance of MINERVA.</p>
<p>We also check the frequency of occurrence of various unique path types. We define a path type as the sequence of relation types (ignoring the entities) in a path. Intuitively, a predictive path which generalizes across queries will occur many number of times in the graph. Figure 2 shows the plot. As we can see, the characteristics of FB 15 K - 237 is quite different from other datasets. For example, in NELL-995, more than 1000 different path types occur more than 1000 times. WN18RR has only 11 different relation types which means there are only $11^{3}$ possible path types of length 3 and even fewer number of them would be predictive. As can be seen, there are few path types which occur more than $10^{4}$ times and around 50 of them occur more than 1000 times. However in FB 15 K - 237,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Count of number of unique path types of length 3 which occur more than ' $x$ ' times in various datasets. For example, in NELL-995 there are more than $10^{3}$ path types which occur more than $10^{3}$ times. However, for FB15k-237, we see a sharp decrease as ' $x$ ' becomes higher, suggesting that path types do not repeat often.
which has the highest number of relation types, we observe a sharp decrease in the number of path types which occur a significant number of times. Since MINERVA cannot find path types which repeat often, it finds it hard to learn path types that generalize.</p>
<h1>3.2 COMPARISON WITH PATH BASED MODELS</h1>
<h3>3.2.1 With Random Walk Models</h3>
<p>In this experiment, we compare to a model which gathers path based on random walks and tries to predict the answer entity. Neural multi-hop models (Neelakantan et al., 2015; Toutanova et al., 2016), operate on paths between entity pairs in a KB. However these methods need to know the target entity in order to pre-compute paths between entity pairs. (Guu et al., 2015) is an exception in this regard as they do random walks starting from a source entity ' $e_{1}$ ' and then using the path, they train a classifier to predict the target answer entity. However, they only consider one path starting from a source entity. In contrast, Neelakantan et al. (2015); Toutanova et al. (2016) use information from multiple paths between the source and target entity. We design a baseline model which combines the strength of both these approaches. Starting from ' $e_{1}$ ' , the model samples $(k=100)$ random paths of up to a maximum length of $T=3$. Following Neelakantan et al. (2015), we encode each paths with an LSTM followed by a max-pooling operation to featurize the paths. This feature is concatenated with the source entity and query relation vector which is then passed through a feed forward network which scores all possible target entities. The network is trained with a multi-class cross entropy objective based on observed triples and during inference we rank target entities according to the model score.
The PATH-BASELINE column of table 4 shows the performance of this model on the three datasets. As we can see MINERVA outperforms this baseline significantly. This shows that a model which predicts based on a set of randomly sampled paths does not do as well as MINERVA because it either loses important paths during random walking or it fails to aggregate predictive features from all the $k$ paths, many of which would be irrelevant to answer the given query. The latter is akin to the problem with distant supervision (Mintz et al., 2009), where important evidence gets lost amidst a plethora of irrelevant information. However, by taking each step conditioned on the query relation, MINERVA can effectively reduce the search space and focus on paths relevant to answer the query.</p>
<h3>3.2.2 With DeepPath</h3>
<p>We also compare MINERVA with DeepPath which uses RL to pick paths between entity pairs. For a fair comparison, we only rank the answer entities against the negative examples in the dataset used in their experiments ${ }^{5}$ and report the mean average precision (MAP) scores for each query relation. DeepPath feeds the paths its agent gathers as input features to the path ranking algorithm (PRA) (Lao et al., 2011), which trains a per-relation classifier. But unlike them, we train one model which learns for all query relations so as to enable our agent to leverage from correlations and more data. If our agent is not able to reach the correct entity or one of the negative entities, the corresponding entities gets a score of negative infinity. If MINERVA fails to reach any of the entities in the set of correct and negative entities. then we fall back to a random ordering of the entities. As show in</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">DeepPath</th>
<th style="text-align: center;">MINERVA</th>
<th style="text-align: center;">MINERVA $^{\mathrm{a}}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">athleteplaysinleague</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;">$\mathbf{0 . 9 7 0}$</td>
<td style="text-align: center;">0.940</td>
</tr>
<tr>
<td style="text-align: left;">worksfor</td>
<td style="text-align: center;">0.711</td>
<td style="text-align: center;">$\mathbf{0 . 8 2 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 1 0}$</td>
</tr>
<tr>
<td style="text-align: left;">organizationhiredperson</td>
<td style="text-align: center;">0.742</td>
<td style="text-align: center;">$\mathbf{0 . 8 5 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 5 6}$</td>
</tr>
<tr>
<td style="text-align: left;">athleteplaysport</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">$\mathbf{0 . 9 8 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 8 0}$</td>
</tr>
<tr>
<td style="text-align: left;">teamplayssport</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">$\mathbf{0 . 8 4 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 8 0}$</td>
</tr>
<tr>
<td style="text-align: left;">personborninlocation</td>
<td style="text-align: center;">0.757</td>
<td style="text-align: center;">$\mathbf{0 . 7 9 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 8 0}$</td>
</tr>
<tr>
<td style="text-align: left;">personleadsorganization</td>
<td style="text-align: center;">0.795</td>
<td style="text-align: center;">$\mathbf{0 . 8 5 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 7 7}$</td>
</tr>
<tr>
<td style="text-align: left;">athletehomestadium</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">$\mathbf{0 . 8 9 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 9 8}$</td>
</tr>
<tr>
<td style="text-align: left;">organizationheadquarteredincity</td>
<td style="text-align: center;">0.790</td>
<td style="text-align: center;">$\mathbf{0 . 9 4 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 4 0}$</td>
</tr>
<tr>
<td style="text-align: left;">athleteplaysforteam</td>
<td style="text-align: center;">0.750</td>
<td style="text-align: center;">$\mathbf{0 . 8 2 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 0 0}$</td>
</tr>
</tbody>
</table>
<p>Table 5: MAP scores for different query relations on the NELL-995 dataset. Note that in this comparison, MINERVA refers to only a single learnt model for all query relations which is competitive with individual DeepPath models trained separately for each query relation. We also trained MINERVA in the setting of DeepPath, i.e. training per-relation models (MINERVA ${ }^{\mathrm{a}}$ )
table 5, we outperform them or achieve comparable performance for all the query relations For this experiment, we set the maximum length $T=3$. Although training per-relation models is cumbersome and does not scale to massive KBs with thousands of relation types, we also train per-relation models of MINERVA replicating the settings of DeepPath (MINERVA ${ }^{\mathrm{a}}$ in table 5). MINERVA ${ }^{\mathrm{a}}$ outperforms DeepPath and performs similarly to MINERVA which is an encouraging result since training one model which performs well for all relation is highly desirable.</p>
<h1>3.3 Partially Structured Queries</h1>
<p>Queries in KBs are structured in the form of triples. However, this is unsatisfactory since for most real applications, the queries appear in natural language. As a first step in this direction, we extend MINERVA to take in "partially structured" queries. We use the WikiMovies dataset (Miller et al., 2016) which contains questions in natural language albeit generated by templates created by human annotators. An example question is "Which is a film written by Herb Freed?". WikiMovies also has an accompanying KB which can be used to answer all the questions.</p>
<p>We link the entity occurring in the question to the KB via simple string matching. To form the vector representation of the query relation, we design a simple question encoder which computes the average of the embeddings of the question words. The word embeddings are learned from scratch and we do not use any pretrained embeddings. We compare our results with those reported in Yang et al. (2017) (table 6). For this experiment, we found that $T=1$ sufficed, suggesting that WikiMovies is not the best testbed for multihop reasoning, but this experiment is a promising first step towards the realistic setup of using KBs to answer natural language question.</p>
<h3>3.4 GRID WORLD PATH Finding</h3>
<p>While chains in KB need not be very long to get good empirical results (Neelakantan et al., 2015; Das et al., 2017; Yang et al., 2017), in principle MINERVA can be used to learn long reasoning chains. To evaluate the same, we test our model on a synthetic 16-by-16 grid world dataset created by Yang et al. (2017), where the task is to navigate to a particular cell (answer entity) starting from a random cell (start entity) by following a set of directions (query relation). The KB consists of atomic triples of the form $((2,1)$, North, $(1,1))$ - entity $(1,1)$ is north of entity $(2,1)$. The queries consists of a sequence of directions (e.g. North, SouthWest, East). The queries are classified into classes based on the path lengths. Figure 3 shows the accuracy on varying path lengths. Compared to Neural LP, MINERVA is much more robust to queries, which require longer path, showing minimal degradation in performance for even the longest path in the dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Memory Network</td>
<td style="text-align: center;">78.5</td>
</tr>
<tr>
<td style="text-align: left;">QA system</td>
<td style="text-align: center;">93.5</td>
</tr>
<tr>
<td style="text-align: left;">Key-Value Memory Network</td>
<td style="text-align: center;">93.9</td>
</tr>
<tr>
<td style="text-align: left;">Neural LP</td>
<td style="text-align: center;">94.6</td>
</tr>
<tr>
<td style="text-align: left;">MINERVA</td>
<td style="text-align: center;">$\mathbf{9 6 . 7}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Performance on WikiMovies
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Grid world experiment: We significantly outperform NeuralLP for longer path lengths.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Based on the query relation our agent assigns different probabilities to different actions. The dashed edges in the top row denote query relation. Examples in the bottom row are from the WikiMovies dataset and hence the questions are partially structured.</p>
<h1>3.5 FURTHER ANALYSIS</h1>
<p>Training time. Figure 5 plots the HITS@10 scores on the development set against the training time comparing MINERVA with DistMult. It can be seen that MINERVA converges to a higher score much faster than DistMult. It is also interesting to note that even during the early stages of the training, MINERVA has much higher performance than that of DistMult, as during these initial stages, MINERVA would just be doing random walks in the neighborhood of the source entity $\left(e_{1}\right)$. This implies that MINERVA's approach of searching for an answer in the neighborhood of $e_{1}$ is a much more efficient and smarter strategy than ranking all entities in the knowledge graph (as done by DistMult and other related methods).</p>
<p>Inference Time. At test time, embedding based methods such as ConvE, ComplEx and DistMult rank all entities in the graph. Hence, for a test-time query, the running time is always $\mathcal{O}(|\mathcal{E}|)$ where $\mathcal{R}$ denotes the set of entities ( $=$ nodes) in the graph. MINERVA, on the other hand is efficient at inference time since it has to essentially search for answer entities in its local neighborhood. The many cost at inference time for MINERVA is to compute probabilities for all outgoing edges along the path. Thus inference time of MINERVA only depends on degree distribution of the graph. If we assume the knowledge graph to obey a power law degree distribution, like many natural graphs, then for MINERVA the average inference time can be shown to be $O\left(\frac{\alpha}{\alpha-1}\right)$, when the coefficient of the power law $\alpha&gt;1$. The median inference time for MINERVA is $O(1)$ for all values of $\alpha$. Note that these quantities are independent of size of entities $|\mathcal{E}|$. For instance, on the test dataset of WN18RR, the wall clock inference time of MINERVA is $\mathbf{6 3 s}$ whereas that of a GPU implementation of DistMult, which is the simplest among the lot, is 211s. Similarly the wall-clock inference time on the test set of NELL-995 for a GPU implementation of DistMult is 115 s whereas that of MINERVA is 35 s .</p>
<p>Query based Decision Making. At each step before making a decision, our agent conditions on the query relation. Figure 4 shows examples, where based on the query relation, the probabilities are peaked on different actions. For example, when the query relation is WorksFor, MINERVA assigns a much higher probability of taking the edge CoachesTeam than AthletePlaysInLeague. We also see similar behavior on the WikiMovies dataset where the query consists of words instead of fixed schema relation.</p>
<p>Model Robustness. Table 7 also reports the mean and standard deviation across three independent runs of MINERVA. We found it easy to obtain/reproduce the highest scores across several runs as can be seen from the low deviations in scores.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">HITS@1</th>
<th style="text-align: center;">HITS@3</th>
<th style="text-align: center;">HITS@10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NELL-995</td>
<td style="text-align: center;">$0.66 \pm 0.029$</td>
<td style="text-align: center;">$0.77 \pm 0.0016$</td>
<td style="text-align: center;">$0.83 \pm 0.0030$</td>
</tr>
<tr>
<td style="text-align: left;">FB15K-237</td>
<td style="text-align: center;">$0.22 \pm 0.002$</td>
<td style="text-align: center;">$0.33 \pm 0.0008$</td>
<td style="text-align: center;">$0.46 \pm 0.0006$</td>
</tr>
<tr>
<td style="text-align: left;">WN18RR</td>
<td style="text-align: center;">$0.41 \pm 0.030$</td>
<td style="text-align: center;">$0.45 \pm 0.0180$</td>
<td style="text-align: center;">$0.51 \pm 0.0005$</td>
</tr>
</tbody>
</table>
<p>Table 7: Mean and Standard deviation across runs for various datasets.</p>
<p>Effectiveness of Remembering Path History. MINERVA encodes the history of decisions it has taken in the past using LSTMs. To test the importance of remembering the sequence of decisions, we did an ablation study in which the agent chose the next action based on only local information i.e. current entity and query and did not have access to the history $h_{t}$. For the KINSHIP dataset, we observe a $27 \%$ points decrease in HITS@ 1 and $13 \%$ decrease in HITS@ 10 . For grid-world, it is also not surprising that we see a big drop in performance. The final accuracy is 0.23 for path lengths 2-4 and 0.04 for lengths $8-10$. For FB 15 K - 237 the HITS@ 10 performance dropped from 0.456 to 0.408 .</p>
<p>NO-OP and Inverse Relations. At each step, MINERVA can choose to take a NO-OP edge and remain at the same node. This gives the agent the flexibility of taking paths of variable lengths. Some questions are easier to answer than others and require fewer steps of reasoning and if the agent reaches the answer early, it can choose to remain there. Example (i) in table 8 shows such an example. Similarly inverse relation gives the agent the ability to recover from a potentially wrong decision it has taken before. Example (ii) shows such an example, where the agent took a incorrect decision at the first step but was able to revert the decision because of the presence of inverted edges.</p>
<h1>4 Related Work</h1>
<p>Learning vector representations of entities and relations using tensor factorization Nickel et al., 2011; 2012; Bordes et al., 2013; Riedel et al., 2013; Nickel et al., 2014; Yang et al., 2015) or neural methods (Socher et al., 2013; Toutanova et al., 2015; Verga et al., 2016) has been a popular approach to reasoning with a knowledge base. However, these methods cannot capture more complex reasoning patterns such as those found by following inference paths in KBs. Multi-hop link prediction approaches (Lao et al., 2011; Neelakantan et al., 2015; Guu et al., 2015; Toutanova et al., 2016; Das et al., 2017) address the problems above, but the reasoning paths that they operate on are gathered by performing random walks independent of the type of query relation. Lao et al. (2011) further filters paths from the set of sampled paths based on the restriction that the path must end at one of the target entities in the training set and are within a maximum length. These constraints make them query
(i) Can learn general rules:
(S1) LocatedIn $(X, Y) \leftarrow$ LocatedIn $(X, Z) \&amp;$ LocatedIn $(Z, Y)$
(S2) LocatedIn $(X, Y) \leftarrow$ NeighborOf $(X, Z) \&amp;$ LocatedIn $(Z, Y)$
(S3) LocatedIn $(X, Y) \leftarrow$ NeighborOf $(X, Z) \&amp;$ NeighborOf $(Z, W) \&amp;$ LocatedIn $(W, Y)$
(ii) Can learn shorter path: Richard F. Velky $\xrightarrow{\text { WorksFor }}$ ?
Richard F. Velky $\xrightarrow{\text { PersonLeadsOrg }}$ Schaghticokes $\xrightarrow{\text { NO-OP }}$ Schaghticokes $\xrightarrow{\text { NO-OP }}$ Schaghticokes
(iii) Can recover from mistakes: Donald Graham $\xrightarrow{\text { WorksFor }}$ ?
Donald Graham $\xrightarrow{\text { OrgTerminatedPerson }}$ TNT Post $\xrightarrow{\text { OrgTerminatedPerson }-2}$ Donald Graham $\xrightarrow{\text { OrghiredPerson }}$ Wash Post</p>
<p>Table 8: A few example of paths found by MINERVA on the COUNTRIES and NELL. MINERVA can learn general rules as required by the COUNTRIES dataset (example (i)). It can learn shorter paths if necessary (example (ii)) and has the ability to correct a previously taken decision (example (iii))</p>
<p>dependent but they are heuristic in nature. Our approach eliminates any necessity to pre-compute paths and learns to efficiently search the graph conditioned on the input query relation.</p>
<p>Inductive Logic Programming (ILP) (Muggleton et al., 1992) aims to learn general purpose predicate rules from examples and background knowledge. Early work in ILP such as FOIL (Quinlan, 1990), PROGOL (Muggleton, 1995) are either rule-based or require negative examples which is often hard to find in KBs (by design, KBs store true facts). Statistical relational learning methods (Getoor \&amp; Taskar, 2007; Kok \&amp; Domingos, 2007; Schoenmackers et al., 2010) along with probabilistic logic (Richardson \&amp; Domingos, 2006; Broecheler et al., 2010; Wang et al., 2013) combine machine learning and logic but these approaches operate on symbols rather than vectors and hence do not enjoy the generalization properties of embedding based approaches.</p>
<p>There are few prior work which treat inference as search over the space of natural language. Nogueira \&amp; Cho (2016) propose a task (WikiNav) in which each the nodes in the graph are Wikipedia pages and the edges are hyperlinks to other wiki pages. The entity is to be represented by the text in the page and hence the agent is required to reason over natural language space to navigate through the graph. Similar to WikiNav is Wikispeedia (West et al., 2009) in which an agent needs to learn to traverse to a given target entity node (wiki page) as quickly as possible. Angeli \&amp; Manning (2014) propose natural logic inference in which they cast the inference as a search from a query to any valid premise. At each step, the actions are one of the seven lexical relations introduced by MacCartney \&amp; Manning (2007).</p>
<p>Neural Theorem Provers (NTP) (Rocktäschel \&amp; Riedel, 2017) and Neural LP (Yang et al., 2017) are methods to learn logical rules that can be trained end-to-end with gradient based learning. NTPs are constructed by Prolog's backward chaining inference method. It operates on vectors rather than symbols, thereby providing a success score for each proof path. However, since a score can be computed between any two vectors, the computation graph becomes quite large because of such soft-matching during substitution step of backward chaining. For tractability, it resorts to heuristics such as only keeping the top-K scoring proof paths trading-off guarantees for exact gradients. Also the efficacy of NTPs has yet to be shown on large KBs. Neural LP introduces a differential rule learning system using operators defined in TensorLog (Cohen, 2016). It has a LSTM based controller with a differentiable memory component (Graves et al., 2014; Sukhbaatar et al., 2015) and the rule scores are calculated via attention. Even though, differentiable memory allows end to end training, it necessitates accessing the entire memory, which can be computationally expensive. RL approaches capable of hard selection of memory (Zaremba \&amp; Sutskever, 2015) are computationally attractive. MINERVA uses a similar hard selection of relation edges to walk on the graph. More importantly, MINERVA outperforms both these methods on their respective benchmark datasets.</p>
<p>DeepPath (Xiong et al., 2017) uses RL based approaches to find paths in KBs. However, the state of their MDP requires the target entity to be known in advance and hence their path finding strategy is dependent on knowing the answer entity. MINERVA does not need any knowledge of the target entity and instead learns to find the answer entity among all entities. DeepPath, additionally feeds its gathered paths to Path Ranking Algorithm (Lao et al., 2011), whereas MINERVA is a complete system trained to do query answering. DeepPath also uses fixed pretrained embeddings for its entity and relations. Lastly, on comparing MINERVA with DeepPath in their experimental setting on the NELL dataset, we match their performance or outperform them. MINERVA is also similar to methods for learning to search for structured prediction (Collins \&amp; Roark, 2004; Daumé III \&amp; Marcu, 2005; Daumé III et al., 2009; Ross et al., 2011; Chang et al., 2015). These methods are based on imitating a reference policy (oracle) which make near-optimal decision at every step. In our problem setting, it is unclear what a good reference policy would be. For example, a shortest path oracle between two entities would be unideal, since the answer providing path should depend on the query relation.</p>
<h1>5 CONCLUSION</h1>
<p>We explored a new way of automated reasoning on large knowledge bases in which we use the knowledge graphs representation of the knowledge base and train an agent to walk to the answer node conditioned on the input query. We achieve state-of-the-art results on multiple benchmark knowledge base completion tasks and we also show that our model is robust and can learn long chains-ofreasoning. Moreover it needs no pretraining or initial supervision. Future research directions include applying more sophisticated RL techniques and working directly on textual queries and documents.</p>
<h1>ACKNOWLEDGEMENTS</h1>
<p>We are grateful to Patrick Verga for letting us use his implementation of DistMult. This work was supported in part by the Center for Data Science and the Center for Intelligent Information Retrieval, in part by DARPA under agreement number FA8750-13-2-0020, in part by Defense Advanced Research Agency (DARPA) contract number HR0011-15-2-0036, in part by the National Science Foundation (NSF) grant numbers DMR-1534431 and IIS-1514053 and in part by the Chan Zuckerberg Initiative under the project Scientific Knowledge Base Construction. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.</p>
<h1>REFERENCES</h1>
<p>Gabor Angeli and Christopher D Manning. Naturalli: Natural logic inference for common sense reasoning. In EMNLP, 2014.</p>
<p>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: A collaboratively created graph database for structuring human knowledge. In ICDM, 2008.</p>
<p>Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In NIPS, 2013.</p>
<p>Guillaume Bouchard, Sameer Singh, and Theo Trouillon. On approximate reasoning capabilities of low-rank vector spaces. AAAI Spring Symposium, 2015.</p>
<p>Matthias Broecheler, Lilyana Mihalkova, and Lise Getoor. Probabilistic similarity logic. In UAI, 2010.</p>
<p>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka, Jr., and Tom M. Mitchell. Toward an Architecture for Never-ending Language Learning. In AAAI, 2010.</p>
<p>Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daume, and John Langford. Learning to search better than your teacher. In ICML, 2015.</p>
<p>William Cohen. Tensorlog: A differentiable deductive database. arXiv:1605.06523, 2016.
Michael Collins and Brian Roark. Incremental parsing with the perceptron algorithm. In ACL, 2004.
Rajarshi Das, Arvind Neelakantan, David Belanger, and Andrew McCallum. Chains of reasoning over entities, relations, and text using recurrent neural networks. In EACL, 2017.</p>
<p>Hal Daumé III and Daniel Marcu. Learning as search optimization: Approximate large margin methods for structured prediction. In ICML, 2005.</p>
<p>Hal Daumé III, John Langford, and Daniel Marcu. Search-based structured prediction. Machine learning, 2009.</p>
<p>Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge graph embeddings. In AAAI, 2018.</p>
<p>Michael Evans and Timothy Swartz. Approximating integrals via Monte Carlo and deterministic methods. OUP Oxford, 2000.</p>
<p>George Fishman. Monte Carlo: concepts, algorithms, and applications. Springer Science \&amp; Business Media, 2013.</p>
<p>Lise Getoor and Ben Taskar. Introduction to statistical relational learning. MIT press, 2007.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv:1410.5401, 2014.
Kelvin Guu, John Miller, and Percy Liang. Traversing knowledge graphs in vector space. In EMNLP, 2015.</p>
<p>John Hammersley. Monte carlo methods. Springer Science \&amp; Business Media, 2013.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 1997.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.</p>
<p>Stanley Kok and Pedro Domingos. Statistical predicate invention. In ICML, 2007.
Ni Lao, Tom Mitchell, and William Cohen. Random walk inference and learning in a large scale knowledge base. In EMNLP, 2011.</p>
<p>Bill MacCartney and Christopher D Manning. Natural logic for textual inference. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. Association for Computational Linguistics, 2007.</p>
<p>John McCarthy. Programs with common sense. RLE and MIT Computation Center, 1960.
Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. Key-value memory networks for directly reading documents. EMNLP, 2016.</p>
<p>Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. Distant supervision for relation extraction with an incomplete knowledge base. In HLT-NAACL, 2013.</p>
<p>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. Distant supervision for relation extraction without labeled data. In $A C L, 2009$.</p>
<p>Stephen Muggleton. Inverse entailment and progol. New generation computing, 1995.
Stephen Muggleton, Ramon Otero, and Alireza Tamaddoni-Nezhad. Inductive logic programming. Springer, 1992.</p>
<p>Arvind Neelakantan, Benjamin Roth, and Andrew McCallum. Compositional vector space models for knowledge base completion. In $A C L, 2015$.</p>
<p>Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning on multi-relational data. In ICML, 2011.</p>
<p>Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. Factorizing yago: scalable machine learning for linked data. In WWW, 2012.</p>
<p>Maximilian Nickel, Xueyan Jiang, and Volker Tresp. Reducing the rank in relational factorization models by including observable patterns. In NIPS, 2014.</p>
<p>Nils J Nilsson. Logic and artificial intelligence. Artificial intelligence, 1991.
Rodrigo Nogueira and Kyunghyun Cho. End-to-end goal-driven web navigation. In NIPS, 2016.
J Ross Quinlan. Learning logical definitions from relations. Machine learning, 1990.
Matthew Richardson and Pedro Domingos. Markov logic networks. Machine learning, 2006.
Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M. Marlin. Relation extraction with matrix factorization and universal schemas. In NAACL, 2013.</p>
<p>Tim Rocktäschel and Sebastian Riedel. End-to-end differentiable proving. In NIPS, 2017.
Stéphane Ross, Geoffrey J Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, 2011.</p>
<p>Stefan Schoenmackers, Oren Etzioni, Daniel Weld, and Jesse Davis. Learning first-order horn clauses from web text. In EMNLP, 2010.</p>
<p>Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. Reasonet: Learning to stop reading in machine comprehension. In $K D D, 2017$.</p>
<p>Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. Reasoning with neural tensor networks for knowledge base completion. In NIPS, 2013.</p>
<p>Fabian Suchanek, Gjergji Kasneci, and Gerhard Weikum. Yago: A core of semantic knowledge. In WWW, 2007.</p>
<p>Sainbayar Sukhbaatar, Jason Weston, and Rob Fergus. End-to-end memory networks. In NIPS, 2015.
Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and Michael Gamon. Representing text for joint embedding of text and knowledge bases. In EMNLP, 2015.</p>
<p>Kristina Toutanova, Victoria Lin, Wen-tau Yih, Hoifung Poon, and Chris Quirk. Compositional learning of embeddings for relation paths in knowledge base and text. In $A C L, 2016$.</p>
<p>Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In ICML, 2016.</p>
<p>Patrick Verga, David Belanger, Emma Strubell, Benjamin Roth, and Andrew McCallum. Multilingual relation extraction using compositional universal schema. In NAACL, 2016.</p>
<p>William Yang Wang, Kathryn Mazaitis, and William W Cohen. Programming with personalized pagerank: a locally groundable first-order probabilistic logic. In CIKM, 2013.</p>
<p>Robert West, Joelle Pineau, and Doina Precup. Wikispeedia: An online game for inferring semantic distances between concepts. In IJCAI, 2009.</p>
<p>Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 1992.</p>
<p>Wenhan Xiong, Thien Hoang, and William Yang Wang. Deeppath: A reinforcement learning method for knowledge graph reasoning. In EMNLP, 2017.</p>
<p>Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. In ICLR, 2015.</p>
<p>Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical rules for knowledge base reasoning. In NIPS, 2017.</p>
<p>Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and Alexander Smola. Deep sets. In NIPS, 2017.</p>
<p>Wojciech Zaremba and Ilya Sutskever. Reinforcement learning neural turing machines. arXiv:1505.00521, 2015.</p>
<p>(i) M to 1</p>
<p>Los Angeles Rams $\xrightarrow[\text { country of origin }]{\text { team plays sport }}$ American Football
The Walking Dead $\xrightarrow{\text { USA }}$
(ii) 1 to M
CEO $\xrightarrow[\text { job position in organization }]{\text { job position in organization }}$ Merck \&amp; Co.
Traffic collision $\xrightarrow{\text { cause of death }}$ Albert Camus
Harmonica $\xrightarrow{\text { instrument played by musician }}$ Greg Graffin</p>
<p>Table 9: Few example facts belonging to m to 1,1 to m relations in FB 15 K - 237</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Relation</th>
<th style="text-align: left;">tail/head</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">/people/marriage_union_type/unions_of_this_type_/people/marriage/location_of_ceremony</td>
<td style="text-align: left;">129.75</td>
</tr>
<tr>
<td style="text-align: left;">/organization/role/leaders./organization/leadership/organization</td>
<td style="text-align: left;">65.15</td>
</tr>
<tr>
<td style="text-align: left;">/location/country/second_level_divisions</td>
<td style="text-align: left;">49.18</td>
</tr>
<tr>
<td style="text-align: left;">/user/ktrueman/default_domain/international_organization/member_states</td>
<td style="text-align: left;">36.5</td>
</tr>
<tr>
<td style="text-align: left;">/base/marchmadness/ncaa_basketball_tournament/seeds./base/marchmadness/ncaa_tournament_seed/team</td>
<td style="text-align: left;">33.6</td>
</tr>
</tbody>
</table>
<p>Table 10: Few example 1-to-M relations from FB 15 K - 237 with high cardinality ratio of tail to head.</p>
<h1>6 APPENDIX</h1>
<h3>6.1 HYPERPARAMETERS</h3>
<p>Experimental Details We choose the relation and embedding dimension size as 200. The action embedding is formed by concatenating the entity and relation embedding. We use a 3 layer LSTM with hidden size of 400 . The hidden layer size of MLP (weights $\mathbf{W}<em _mathbf_2="\mathbf{2">{\mathbf{1}}$ and $\mathbf{W}</em>$ ) is set to 400 . We use Adam (Kingma \&amp; Ba, 2014) with the default parameters in REINFORCE for the update.}</p>
<p>In our experiments, we tune our model over two hyper parameters, viz., $\beta$ which is the entropy regularization constant and $\lambda$ which is the moving average constant for the REINFORCE baseline. The table 11 lists the best hyper parameters for all the datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">$\beta$</th>
<th style="text-align: center;">$\lambda$</th>
<th style="text-align: left;">Path Length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">UMLS</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">KINSHIP</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">Countries S1</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">Countries S2</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">Countries S3</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: left;">3</td>
</tr>
<tr>
<td style="text-align: left;">WN18RR</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: left;">3</td>
</tr>
<tr>
<td style="text-align: left;">NELL-995</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: left;">3</td>
</tr>
<tr>
<td style="text-align: left;">FB15K-237</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: left;">3</td>
</tr>
<tr>
<td style="text-align: left;">WIKIMOVIES</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0</td>
<td style="text-align: left;">1</td>
</tr>
</tbody>
</table>
<p>Table 11: Best hyper parameters</p>
<h3>6.2 ADDENDUM TO NELL RESULTS</h3>
<p>The NELL dataset released by Xiong et al. (2017) includes two additional tasks for which the scores were not reported in the paper and so we were unable to compare them against DeepPath. Nevertheless, we ran MINERVA on these tasks and report our results in table 12 for completeness.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Single Model</th>
<th style="text-align: left;">DeepPath setup</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">agentbelongstoorganization</td>
<td style="text-align: left;">0.86</td>
<td style="text-align: left;">0.87</td>
</tr>
<tr>
<td style="text-align: left;">teamplaysinleague</td>
<td style="text-align: left;">0.97</td>
<td style="text-align: left;">0.95</td>
</tr>
</tbody>
</table>
<p>Table 12: NELL results for the remaining tasks</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ We are grateful to Xiong et al. (2017) for releasing the negative examples used in their experiments.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1572 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1572</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1572</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-f68d11fe12254b780e543126d453f3504976f463</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f68d11fe12254b780e543126d453f3504976f463" target="_blank">Toward Automated Quest Generation in Text-Adventure Games</a></p>
                <p><strong>Paper Venue:</strong> CCNLG</p>
                <p><strong>Paper TL;DR:</strong> This paper considers the problem of procedurally generating a quest, defined as a series of actions required to progress towards a goal, in a text-adventure game, and presents and evaluates two quest generation techniques: a Markov model, and a neural generative model.</p>
                <p><strong>Paper Abstract:</strong> Interactive fictions, or text-adventures, are games in which a player interacts with a world entirely through textual descriptions and text actions. Text-adventure games are typically structured as puzzles or quests wherein the player must execute certain actions in a certain order to succeed. In this paper, we consider the problem of procedurally generating a quest, defined as a series of actions required to progress towards a goal, in a text-adventure game. Quest generation in text environments is challenging because they must be semantically coherent. We present and evaluate two quest generation techniques: (1) a Markov model, and (2) a neural generative model. We specifically look at generating quests about cooking and train our models on recipe data. We evaluate our techniques with human participant studies looking at perceived creativity and coherence.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1572.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1572.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCS/MCC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Markov Chain Quest Generator (Simple / Complex)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A quest/content generator that builds an undirected weighted ingredient graph from ~20k recipes and generates ingredient sets by probabilistic graph walks; MCS produces 4-ingredient recipes (simple) and MCC 8-ingredient recipes (complex). Generated ingredient lists are then mapped to executable game actions using a hand-authored action graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Markov Chain Quest Generator</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Undirected weighted ingredient graph learned from recipes (edge weights = co-occurrence counts). Generation: seed ingredient sampled by global weight distribution, probabilistic neighbor selection with heuristics to encourage novelty (alpha, beta terms) and post-check against corpus for novelty. Resulting ingredient lists are converted to game actions via an action graph that encodes affordances and required tools (knife, peeler, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (home theme; 1-room and 5-room maps)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-adventure environment (TextWorld) with a 'home' theme. Interactions are textual commands; world composed of rooms, objects, and actions. Two map variants: 1R (kitchen only) and 5R (kitchen + dining room, garage, backyard, garden). Object placement is determined by an object graph; action affordances by an action graph.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>household tasks (cooking procedures)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Making recipes such as peanut butter cookies; retrieving eggs, peanut butter, flour, baking soda; preparing ingredients (cutting carrots, peeling, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Compositional: recipes decompose into sets of ingredients and then into preparation actions (retrieve ingredient -> apply required tool/action -> combine). Markov generator produces ingredient sets which are then translated to compositional action sequences via the action graph.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Two explicit complexity settings used in experiments: 'Simple' = 4-ingredient recipes (MCS); 'Complex' = 8-ingredient recipes (MCC). Additionally, environment complexity varies: 1-room vs 5-room maps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>No curriculum learning was applied. Human-subject evaluation (n=75, ~15 per game) shows Markov-chain-generated quests (MCS/MCC) produce higher perceived surprise and novelty compared to the neural LM in many cases; MCS (4 ingredients) is slightly more coherent than MCC (8 ingredients). All generated methods (including Markov variants) achieve coherence comparable to human-designed games when grounded with semantic graphs; random-assignment baseline performs worst. Perception varies with environment length/complexity: 5-room maps rated more surprising and original but less coherent than 1-room maps. Statistical tests: per-metric one-way ANOVAs significant (p < 0.05); Tukey HSD post-hoc comparisons reported (p thresholds reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Automated Quest Generation in Text-Adventure Games', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1572.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1572.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM (GPT-2 fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Language Model Quest Generator (LSTM + GPT-2 fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two-stage neural generator: (1) a 4-layer LSTM that generates ingredient sequences (treating multi-word ingredients as single tokens), and (2) a fine-tuned GPT-2 (345M) that generates titles and full instructions conditioned on ingredients.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Neural Language Model (LSTM + GPT-2 fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Ingredient generator: 4-layer LSTM trained on ingredient sets (EOI token) with top-k sampling. Instruction generator: GPT-2 (345M parameter pretrained model) fine-tuned on recipes (title, ingredients, instructions unstructured) to produce full recipe text conditioned on generated ingredients. The LM directly outputs natural-language actions, so the action graph is not used for LM-generated quests (only the object graph for placement).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td>345m</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (home theme; 1-room and 5-room maps)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same as above: text-adventure interactions via textual commands, object and action affordances encoded in graphs; two map sizes to vary task length and complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>household tasks (cooking procedures)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Full recipe instructions (e.g., steps to prepare and combine ingredients for cookies, cutting/peeling instructions if generated), retrieving and preparing ingredients located in environment.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Procedures are composed as natural-language instruction sequences generated conditioned on ingredient lists; the structure can include multi-step preparations and implicit subtask requirements (tools, locations). LM does not explicitly use an action graph, so compositional constraints are learned in the language model's outputs rather than enforced explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Varied implicitly by ingredient count (via LSTM ingredient generator) and explicitly by map size (1-room vs 5-room). No staged curriculum training was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>No curriculum learning applied. Human evaluation shows the neural LM achieves higher perceived coherence and value (sense of accomplishment) than the Markov-chain models and is comparable to human-designed quests; LM often outperforms Markov models on coherence and reported value. LM tends to require less hand-authored domain knowledge because it generates full instructions, whereas the Markov method requires an action graph. Complexity (5-room) increases surprise/originality but decreases coherence; LM maintains coherence better across complexity levels. Statistical analyses (ANOVA and Tukey HSD) reported significant differences across models and map sizes (details and pairwise meandiffs provided in paper appendices).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Automated Quest Generation in Text-Adventure Games', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li>Transfer in deep reinforcement learning using knowledge graphs <em>(Rating: 2)</em></li>
                <li>Playing by the book: An interactive game approach for action graph extraction from text <em>(Rating: 1)</em></li>
                <li>Learning to speak and act in a fantasy text adventure game <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1572",
    "paper_id": "paper-f68d11fe12254b780e543126d453f3504976f463",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "MCS/MCC",
            "name_full": "Markov Chain Quest Generator (Simple / Complex)",
            "brief_description": "A quest/content generator that builds an undirected weighted ingredient graph from ~20k recipes and generates ingredient sets by probabilistic graph walks; MCS produces 4-ingredient recipes (simple) and MCC 8-ingredient recipes (complex). Generated ingredient lists are then mapped to executable game actions using a hand-authored action graph.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Markov Chain Quest Generator",
            "agent_description": "Undirected weighted ingredient graph learned from recipes (edge weights = co-occurrence counts). Generation: seed ingredient sampled by global weight distribution, probabilistic neighbor selection with heuristics to encourage novelty (alpha, beta terms) and post-check against corpus for novelty. Resulting ingredient lists are converted to game actions via an action graph that encodes affordances and required tools (knife, peeler, etc.).",
            "agent_size": null,
            "environment_name": "TextWorld (home theme; 1-room and 5-room maps)",
            "environment_description": "Text-adventure environment (TextWorld) with a 'home' theme. Interactions are textual commands; world composed of rooms, objects, and actions. Two map variants: 1R (kitchen only) and 5R (kitchen + dining room, garage, backyard, garden). Object placement is determined by an object graph; action affordances by an action graph.",
            "procedure_type": "household tasks (cooking procedures)",
            "procedure_examples": "Making recipes such as peanut butter cookies; retrieving eggs, peanut butter, flour, baking soda; preparing ingredients (cutting carrots, peeling, etc.).",
            "compositional_structure": "Compositional: recipes decompose into sets of ingredients and then into preparation actions (retrieve ingredient -&gt; apply required tool/action -&gt; combine). Markov generator produces ingredient sets which are then translated to compositional action sequences via the action graph.",
            "uses_curriculum": false,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": null,
            "task_complexity_range": "Two explicit complexity settings used in experiments: 'Simple' = 4-ingredient recipes (MCS); 'Complex' = 8-ingredient recipes (MCC). Additionally, environment complexity varies: 1-room vs 5-room maps.",
            "performance_with_curriculum": null,
            "performance_without_curriculum": null,
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": null,
            "key_findings": "No curriculum learning was applied. Human-subject evaluation (n=75, ~15 per game) shows Markov-chain-generated quests (MCS/MCC) produce higher perceived surprise and novelty compared to the neural LM in many cases; MCS (4 ingredients) is slightly more coherent than MCC (8 ingredients). All generated methods (including Markov variants) achieve coherence comparable to human-designed games when grounded with semantic graphs; random-assignment baseline performs worst. Perception varies with environment length/complexity: 5-room maps rated more surprising and original but less coherent than 1-room maps. Statistical tests: per-metric one-way ANOVAs significant (p &lt; 0.05); Tukey HSD post-hoc comparisons reported (p thresholds reported in paper).",
            "uuid": "e1572.0",
            "source_info": {
                "paper_title": "Toward Automated Quest Generation in Text-Adventure Games",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "LM (GPT-2 fine-tuned)",
            "name_full": "Neural Language Model Quest Generator (LSTM + GPT-2 fine-tuned)",
            "brief_description": "Two-stage neural generator: (1) a 4-layer LSTM that generates ingredient sequences (treating multi-word ingredients as single tokens), and (2) a fine-tuned GPT-2 (345M) that generates titles and full instructions conditioned on ingredients.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Neural Language Model (LSTM + GPT-2 fine-tuned)",
            "agent_description": "Ingredient generator: 4-layer LSTM trained on ingredient sets (EOI token) with top-k sampling. Instruction generator: GPT-2 (345M parameter pretrained model) fine-tuned on recipes (title, ingredients, instructions unstructured) to produce full recipe text conditioned on generated ingredients. The LM directly outputs natural-language actions, so the action graph is not used for LM-generated quests (only the object graph for placement).",
            "agent_size": "345m",
            "environment_name": "TextWorld (home theme; 1-room and 5-room maps)",
            "environment_description": "Same as above: text-adventure interactions via textual commands, object and action affordances encoded in graphs; two map sizes to vary task length and complexity.",
            "procedure_type": "household tasks (cooking procedures)",
            "procedure_examples": "Full recipe instructions (e.g., steps to prepare and combine ingredients for cookies, cutting/peeling instructions if generated), retrieving and preparing ingredients located in environment.",
            "compositional_structure": "Procedures are composed as natural-language instruction sequences generated conditioned on ingredient lists; the structure can include multi-step preparations and implicit subtask requirements (tools, locations). LM does not explicitly use an action graph, so compositional constraints are learned in the language model's outputs rather than enforced explicitly.",
            "uses_curriculum": false,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": null,
            "task_complexity_range": "Varied implicitly by ingredient count (via LSTM ingredient generator) and explicitly by map size (1-room vs 5-room). No staged curriculum training was reported.",
            "performance_with_curriculum": null,
            "performance_without_curriculum": null,
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": null,
            "key_findings": "No curriculum learning applied. Human evaluation shows the neural LM achieves higher perceived coherence and value (sense of accomplishment) than the Markov-chain models and is comparable to human-designed quests; LM often outperforms Markov models on coherence and reported value. LM tends to require less hand-authored domain knowledge because it generates full instructions, whereas the Markov method requires an action graph. Complexity (5-room) increases surprise/originality but decreases coherence; LM maintains coherence better across complexity levels. Statistical analyses (ANOVA and Tukey HSD) reported significant differences across models and map sizes (details and pairwise meandiffs provided in paper appendices).",
            "uuid": "e1572.1",
            "source_info": {
                "paper_title": "Toward Automated Quest Generation in Text-Adventure Games",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 2
        },
        {
            "paper_title": "Transfer in deep reinforcement learning using knowledge graphs",
            "rating": 2
        },
        {
            "paper_title": "Playing by the book: An interactive game approach for action graph extraction from text",
            "rating": 1
        },
        {
            "paper_title": "Learning to speak and act in a fantasy text adventure game",
            "rating": 1
        }
    ],
    "cost": 0.0097495,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Toward Automated Quest Generation in Text-Adventure Games</h1>
<p>Prithviraj Ammanabrolu, William Broniec, Alex Mueller, Jeremy Paul, and Mark O. Riedl<br>School of Interactive Computing<br>Georgia Institute of Technology<br>Atlanta, GA, USA<br>{raj.ammanabrolu, wbroniec3, alexmueller, jeremypaul, riedl}@gatech.edu</p>
<h4>Abstract</h4>
<p>Interactive fictions, or text-adventures, are games in which a player interacts with a world entirely through textual descriptions and text actions. Text-adventure games are typically structured as puzzles or quests wherein the player must execute certain actions in a certain order to succeed. In this paper, we consider the problem of procedurally generating a quest, defined as a series of actions required to progress towards a goal, in a text-adventure game. Quest generation in text environments is challenging because they must be semantically coherent. We present and evaluate two quest generation techniques: (1) a Markov chains, and (2) a neural generative model. We specifically look at generating quests about cooking and train our models on recipe data. We evaluate our techniques with human participant studies looking at perceived creativity and coherence.</p>
<h2>1 Introduction</h2>
<p>Natural language can be used to express creativity in the form of narrative. Prior research has shown that narrative is used in everything from environmental understanding (Bruner, 1991) to developing language (Johnston, 2008). Given this wide ranging impact, using narrative in language to help us understand human perceptions of creativity and what it takes to replicate this through computational models is natural. Text-adventure games or interactive fiction, in which a player interacts with a world entirely through text, provide us with a platform on which to explore these ideas on creativity in language. These games are usually structured as puzzles or quests in which a player must complete a sequence of actions in order to succeed. Text games allow us to factorize the problem of creative language generation and focus on developing more fine-grained, data-driven models.</p>
<p>Automated generation of text-adventure games can broadly be split into two considerations: (1) the structure of the world, including the layout of rooms, textual description of rooms, objects, and non-player characters; and (2) the quest, consisting of the partial ordering of activities that the player must engage in to make progress toward the end of the game. In this work, we focus on methods of automatically generating such a quest and how it can be used to better understand narrative intelligence, specifically looking at perceived creativity and coherence. Quest generation requires narrative intelligence as a quest must maintain coherence throughout and progress towards a goal. Maintaining quest coherence also means following the constraints of the given game world. The quest has to fit within the confines of the world in terms of both genre and given affordancese.g. using magic in a fantasy world. This is further complicated in the case of a text-adventure as a consequence of all interactions being in natural language-the potential output space is combinatorial in size. Because the player "sees" and "acts" entirely through text, any quest generation system must also take into account the lack of visual information and generate sufficiently descriptive text accordingly.</p>
<p>There are multiple variables that could potentially affect a player's perception of creativity in a text-adventure game such as the vocabulary used, the structure of the world, stylistic variations in writing, etc. We use the TextWorld framework (Côté et al., 2018) which lets us generate textadventure game worlds based on a grammar. It lets us fix variables concerned with game world and logic generation and focus only on the generation of quests within this world. We use this framework's "home" theme-providing us with a textual simulation of a house-and restrict the types of quests that can be generated to those in-</p>
<p>volving the completion of a cooking recipe. We then attempt to learn how to generate a quest to complete a recipe-as well as how to create the recipe itself-using a large scale knowledge base of recipes. In these quests, players are provided with a list of ingredients and their locations, and they have to navigate the environment to find and prepare those ingredients to complete the given recipe. For example, given a recipe to make peanut butter cookies the quest would first tell the player to find eggs, peanut butter, flour, and baking soda. The player would then have to figure out that the first ingredient is in the fridge while the others are in the pantry and prepare each item accordingly. Generating this sort of quest requires knowledge of the ingredients, how they fit together, and how those ingredients interact with the environment.</p>
<p>The contribution of this work is thus twofold. We first detail a framework, and variations thereof, that can learn to generate creative quests in a textadventure game. This framework includes two quest generation models using Markov chains as well as a neural language model. It also uses a semantically grounded knowledge graph to improve overall quest coherence. Our second contribution provides human subject evaluations that give us insight into how each variation of this framework affects human perception of creativity and coherence in such games.</p>
<h2>2 Related Work</h2>
<p>Although there has been much work recently on text-adventure gameplay (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Yang et al., 2018; Haroush et al., 2018; Côté et al., 2018; Tao et al., 2018; Ammanabrolu and Riedl, 2019a; Hausknecht et al., 2019a; Ammanabrolu and Riedl, 2019b; Hausknecht et al., 2019b), these works focus on creating agents that can play a given game as opposed to being able to automatically generate content for them.</p>
<p>Outside of this, there has been some work on learning to create content in the context of interactive narrative. These systems mainly work to overcome a significant bottleneck in the form of the human authoring required to create such works. Permar and Magerko (2013) present a method of generating cognitive scripts required for freeform activities in the form of pretend play. Specifically, they use interactive narrative-a form of pretend play that requires a high level of impro-
visation and creativity and uses cognitive scripts acquired from multiple experience sources. They take existing cognitive scripts and blend them in the vein of more traditional conceptual blending (Veale et al., 2000; Zook et al., 2011) to create new blended scripts. Closely related is Magerko et al. (2014) who present a Co-Creative Cognitive Architecture (CoCoA), detailing the set of components that support the design of co-creative agents in the context of interactive narrative. These methods all follow singular cognitive models that do not learn to generate content automatically.</p>
<p>Li et al. (2012) present Scheherazade, a system which learns a plot graph based on stories written by crowd sourcing the task of writing short stories through Amazon Mechanical Turk. This plot graph contains details relevant for the coherence of the story and includes: plot events, temporal precedence, and mutual exclusion relations. The generated narrative contains events that can be executed from this plot graph by both players and non-player characters. Guzdial et al. (2015) introduce Scheherazade-IF, a system that learns to generate choose-your-own-adventure style interactive fictions in which the player chooses from prescribed options. More recently, Martin et al. (2017) introduce a pipeline systems for improvisational storytelling agents capable of collaboratively creating stories. These agents first focus on creating a plot for the story and then expand that plot into natural language sentences. Urbanek et al. (2019) introduce Light, a dataset of crowdsourced text-adventure game dialogs focusing on giving collaborative agents the ability to generate contextually relevant dialog and emotes.</p>
<p>Giannatos et al. (2011) use genetic algorithms to create new story plot points for an existing game of interactive fiction using an encoding known as a precedence-constraint graph. This graph gives the system information regarding the ordering of events that must happen in the game in order to advance. They demonstrate the workings of their system by generating additional content for the popular interactive fiction game Anchorhead, and show that this can be integrated into the original game. This work, however, is offline and relies on existing interactive fiction games and having knowledge of the precedence-constraint graph for this existing game.</p>
<p>The Game Forge system (Hartsook et al., 2011) also uses genetic algorithms to generate a game</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example of ingredient connections.
world and plot line for related type of game, a computer role playing game (CRPG). This work focuses on generating layouts and plot structures to create novel game worlds through with a fitness function based on a transition graph that encodes pre-built game requirements. Tamari et al. (2019) focus on extracting action graphs for sequential decision making problems such as material science experiments and turn them into textadventure games. Although these works use graph structures in order to constrain the generation of the game, we use these graph structures only to maintain coherence and focus on content creation.</p>
<p>Although there are works that attempt to automatically evaluate the creativity of the output of a generative process by computationally modeling potential human responses - such as with story telling (Purdy et al., 2018), etc. - we choose to rely on a human subject study based on the definition of creativity as presented in Boden (2007). Specifically we focus on the concepts of novelty and value, despite collecting data for other defined metrics as well. We use the definition of novelty stemming from the idea of p-creativity, i.e. a concept that is entirely new to a single agent - in this case a subject in our evaluation study. Value, as a component of computational creativity, however, is not defined concretely in Boden's work for a general domain. Our definition of value in the context of text-adventure games relies on accomplishment or achievement.</p>
<h2>3 Content Generation</h2>
<p>In this section, we present Markov chain and neural language model based models to generate content, i.e. recipes, for our quests. Content generation for a quest in a text-adventure game, in this case a recipe, can be thought of as being equivalent to generating a sequence of events in
which prior elements affect the probability of subsequent events. Markov chains present a simplified and well studied method to generate such content. Neural language models, designed to predict an element of a sequence conditioned on a given number of prior elements, let us generate sequences of events with more prior context-i.e. in the absence of the Markov assumption.</p>
<h3>3.1 Markov Chains</h3>
<p>Our first quest generation model is based on the use of Markov chains. This generation process consists of two steps. We first learn a weighted ingredient graph, a Markov chain, from a large scale knowledge base of recipes and then probabilistically walk along this graph to generate the instructions for the recipe.</p>
<h3>3.1.1 Ingredient Graph</h3>
<p>Generating the recipe requires domain knowledge. For example, creating a recipe for peanut butter cookies requires an understanding that an ingredient like peanut butter fits well with eggs, flour, and sugar while something like fish does not. We represent this knowledge with an undirected graph of ingredients. Our ingredient graph is based off of recipes scraped from allrecipes.com. ${ }^{1}$ The raw, uncleaned dataset included over 20,000 recipes with over 4000 unique ingredients. A list of ingredients was extracted from each recipe, and each of these lists was converted into a set of ingredient pairs (Fig. 2). In total, there were 118,116 unique ingredient pairings, and 73,088 of those pairings ( $62 \%$ ) only occurred once. We reduced the number of distinct ingredients from 4460 to 1703 by merging items with the same base ingredient and by replacing name-brand items with a generic equivalent.</p>
<p>Each of the nodes within the graph represents a possible ingredient, and weighted connections between these nodes represent how well the ingredients go together. The weight of each edge is the total number of occurrences of that ingredient pair within the recipe corpus. The edge connecting eggs and white sugar would have a weight of 3774 while the edge between hot milk and orange juice would have a weight of 1 . Ingredient pairings that do not occur within the recipe corpus did not have an edge within this network, and this helped</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Ingredient extraction process.</p>
<p>prevent our model from generating completely incoherent recipe pairings (e.g. hot sauce and baby food). Take the graph in Fig. 1 as an example. In this complete graph, all of the ingredients go well with each other except for fish and sugar, which is indicated by the low weight connection between them. The weak connection between sugar and fish suggest that they would likely not go well together in a recipe.</p>
<h3>3.1.2 Instruction Generation</h3>
<p>With the ingredient graph created, we begin the process of instruction generation based on subgraph mining and prior generative methods based on probabilistic graph walks <em>Fleishman (1978)</em>. We start by selecting an initial random ingredient 'x' weighted by its distribution in the graph.</p>
<p>$$p(x_1) = \frac{\sum_{i=1}^k w(v_i, x_1)}{\sum_{i=1}^k \sum_{j=1}^k w(v_i, v_j)}\tag{1}$$</p>
<p>We probabilistically select one of its neighbors based on the conditional frequency of the pair. Each iteration further computes conditional probabilities and selects them. We exclude all ingredients in which any bag of words token is contained by any other, ensuring that a variety of different ingredients are selected.</p>
<p>$$\alpha = \begin{cases} 0 &amp; B_{x_i} \subseteq B_{x_{n+1}} \lor B_{x_{n+1}} \subseteq B_{x_i} \ 1 &amp; \text{else} \end{cases} \tag{2}$$</p>
<p>In Eq. 2, B<sub>x<sub>i</sub></sub> refers to the 1-gram bag of words model.</p>
<p>However, just computing complete conditional probabilities would remove the chance for entirely new combinations to emerge. Therefore, we calculate just the partial probability of having shared ingredients with a bias designed to favor such combinations.</p>
<p>$$\beta = (\sum_{i=1}^{n} \text{Shared}(x_i, x_{n+1}))^2 \tag{3}$$</p>
<p>$$\text{Shared}(x_1, x_2) = \begin{cases} 1 &amp; w(x_1, x_2) &gt; 0 \ 0 &amp; \text{else} \end{cases} \tag{4}$$</p>
<p>This process repeated recursively to generate a recipe with the desired number of ingredients.</p>
<p>$$p(x_{n+1}) = \sum_{i=1}^{n} \alpha \beta \frac{w(x_{n+1}, v_j)}{\sum_{j=1}^k w(x_i, v_j)} \tag{5}$$</p>
<p>Finally, resultant combinations are referenced back against the original corpus to guarantee novelty in the result.</p>
<h3>3.2 Neural Language Model</h3>
<p>Here we use a neural language model to generate both the ingredients for a recipe and the steps of the ingredients as well. We use the same knowledge base as described in Sec. 3.1.1 and train two separate language models: one to generate the ingredients, and the other to generate the recipe given a set of ingredients.</p>
<p>The first language model uses a simple 4-layer LSTM to generate a sequence of ingredients, treating all the words in a single ingredient as a single token. For example, "peanut butter" would be considered a single token in this model. We train this model using the sets of ingredients found in each recipe for the entire recipe dataset, with each set ending with an <EOI> or End of Ingredients tag. Once trained, the model then generates a sequence of ingredients until the <EOI> is reached using the top-k sampling technique <em>Holtzman et al. (2019)</em>.</p>
<p>To generate the actual recipe, we use GPT-2 <em>Radford et al. (2019)</em> and fine-tune their pretrained 345m parameter model on the recipe data. The data to fine-tune this model is designed to contain the recipe title, ingredients, and instructions in an unstructured text-form. Once this model has been fine-tuned, we use it to generate the title and instructions for the recipe conditioned on the ingredients generated by the first language model. The entire generated recipe consists of the ingredients, title, and instructions.</p>
<h3>4 Quest Assembly</h3>
<p>We now use the generated content, i.e. the recipe, to assemble a quest—grounding the generated in-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Object graph in the one room map.
gredients and instructions in the game world. This requires us to first determine the structure of the game world and the locations of objects within this world in addition to transforming the set of generated instructions into executable actions. We use two types of semantically grounded knowledge graphs to represent this information: the object and action graphs.</p>
<p>The object graph is used to determine the structure of the world and the most likely locations of objects within this world. For example, we could have information that says that vegetables must be stored in a refrigerator. If a recipe requires carrots, then the carrots would automatically be placed in a refrigerator at the start of the game. This graph is constructed by hand and is built to make the game world and resulting quest as coherent as possible.</p>
<p>We construct object graphs for two different room layouts. The first, the one room (1R) map, consists of a kitchen as well as the objects and actions that exist within it. The second map, the five room (5R) map, is an extension of the first map and contains four additional rooms.</p>
<p>The object graph for the 1R map as shown in Fig. 3 is largely inspired by the simple, prebuilt game provided within TextWorld (Côté et al., 2018). This object graph determines how and where objects are placed within the environment during game generation, and the action graph (Fig.6) dictates how generated instructions are transformed into executable actions in the game. The object graph was constructed logically: tools and utensils go in the drawer, meat and dairy belong in the refrigerator, and so on. Food item placements are deterministic and coherent. Vegetables always go in the refrigerator, and fruit always goes on the kitchen island. The action graph was also designed to prevent the player from conducting illogical actions.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Room layout in the five room map.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Object graph in the five room map.</p>
<p>The 5R map included a dining room, garage, backyard, and garden in addition to the kitchen (Fig. 4). The map (Fig. 5) is designed to maintain the same levels of coherency as the 1R map while allowing for more diverse gameplay, which could in turn lead to higher levels of perceived creativity. The additional rooms are selected based on their possible relationships to the domain of food and cooking, and each new room has its own unique objects that players can interact with. For example, the garage has an old refrigerator that can be used to store meat. These new rooms and objects also allow for dynamic food placement. Meat can be placed in one of two refrigerators, and fruits and vegetables can possibly be found in the garden. The existence of these new locations is not immediately clear to the player. The garage and backyard are additionally obscured by closed doors, adding to quest complexity. While the additional rooms and dynamic food placement allow for more diverse gameplay, they do not sacrifice coherency.</p>
<p>The action graph contains information regarding the affordances of the objects in the world and what kinds of objects are required to complete a given generated instruction. For example, if a generated instruction tells us to prepare vegetables, i.e. cut them, then this graph tells us that there must be a knife somewhere in this world. This graph is partially extracted from static cooking guides online using a mixture of OpenIE (Angeli</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Example action graph for both maps.
et al., 2015) and hand-authored rules to account for the irregularities of cooking guides. An example of an action graph is given in Fig. 6. A player can peel fruit and vegetables, for example, but cannot peel a steak. There are also strict rules on what tools are required for certain actions. A player can only cut something if they have a knife and can only peel something with a peeler. While this restricts how players can interact with the environment, it ultimately reinforces game coherency.</p>
<p>We also note that when generating the quests, both the Markov chain and the neural language model based generation systems use the object graph to determine object placement but only the Markov chain based model uses the action graph. This is because the instructions generated by the Markov chain model is in the form of a sequence of ingredients which then requires the action graph to determine the actions and additional objects required to turn this list of ingredients into a playable quest. The action graph would thus take an ingredient such as a carrot and determine first that it needs to be cut and that a knife is required for this task. The neural language model on the other hand already generates the full action, including potentially required objects, that can be executed and so does not make use of this graph.</p>
<h2>5 Experiments</h2>
<p>Our experiments were designed to compare perceived creativity and coherence, specifically testing our models in addition to factors such as complexity. We tested five types of designs: Human Designed (HD), Random Assignment (RA), Markov Chains Simple (MCS), Markov Chains Complex (MCC), and Language Model (LM). HD is simply what it sounds like, a game that was created by a person. In this game, a human cre-
ates both the ingredients and the instructions for a recipe and is additionally responsible for quest assembly, i.e. grounding the generated content in a given game world. We do not consider experience in designing text-adventure games when picking a human to create this game as this task can be performed even by novices given the easily understandable "home" theme of the game world. The game is manually crafted in terms of decided what ingredients to put where and what the final recipe would come together to be. RA is on the opposite end of the spectrum where, as the name suggests, everything is placed in a random location, and the recipe could be totally random with ingredients and instructions that might not normally be seen. MCS and MCC use our Markov chains approach to generate quest content. The difference between MCS and MCC are that the former has four ingredients involved in its recipe while the latter has eight. This was to vary the complexity to see how that affected perceived creativity. LM refers to the games generated using the recipes generated by the language model. We additionally had one-room and five-room variants for each of the models to test how the structure and length of the game would affect the players.</p>
<p>Evaluating the creativity of the output of any computational generation process is a difficult task which requires concrete definitions of the metrics being used. We thus setup the experiment by having our game designs deployed on Amazon Mechanical Turk for people to play and provide feedback. Specifically, they would play one randomly selected game from the 1 room layout and then fill out a survey for that game, and then play one randomly selected game from the 5 room layout and fill out an identical survey. Subjects were provided with a simple practice game that they could play beforehand to familiarize themselves with TextWorld and its interface. We had 75 total participants for the entire study and had an average of 15 people play each game. The only restrictions that we had for participants was that they had to be fluent in English-this was determined by means of prebuilt restrictions on Amazon Mechanical Turk and game completion verification.</p>
<p>The users were asked questions pertaining to two metrics: coherence and creativity. We looked at creativity as a metric in the survey using the components of creativity as defined by Boden: novelty, surprise, and value. The survey detailed</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Coherence scores for each game. Error bars indicate one standard deviation.</p>
<p>questions that measured our defined metrics, using Likert Scale values along a scale of 1-7. For example, it posed questions such as "How original was the quest you played? 1: not at all novel, 7: exceptionally novel" when measuring novelty. The other factors were also measured using similarly phrased questions. A one-way ANOVA test was then conducted followed by Tukey HSD posthoc analysis to determine significance. The results of the raw scores for each group as well as the significant results between pairs of different models are presented below.</p>
<h2>6 Results and Discussion</h2>
<p>We present results for four metrics: coherence, unpredictability (or surprise), novelty (or originality), and value (or accomplishment) for each of the games. Additionally, we also show the p-value result of a one-way ANOVA test for the distributions in each of the categories to determine statistical significance. This test tells us if the differences in the means across the different games are significant for each of the categories separately. The Tukey HSD post-hoc analysis further tells us which specific pairs of results are significant. We hypothesized that semantic grounding using the knowledge graph would enable our models to maintain coherence on par with the human-designed games. Further, given the stochastic nature of our generative models, we further predicted that our models would also rate as being comparable in terms of creativity to the human-designed games—with all models relatively outperforming the randomly generated games. We see below that these predictions hold.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Unpredictability (surprise) scores for each game. Error bars indicate one standard deviation.</p>
<p>We find that the results for each individual category are significant—p &lt; 0.05 in all the cases. Additionally, all the specific pairwise comparisons we make are significant with p &lt; 0.1—a full set of these pairwise results can be found in Appendix A. The rest of this section will discuss each of these metrics in more detail.</p>
<p>Fig. 7 displays trends in the players' perception of coherence for each of the games. We first see that the one-room games were consistently rated to be more coherent than the five-room games, indicating that overall quest coherence—and thus the coherence of our generative system—degrades the longer and more complex the quest. Across the games, we see that the RA models were the considered to be the least coherent. The MCS model slightly outperforms the MCC model, showing that the Markov chain-based models are more coherent the less complex the output. The LM achieves a higher score than both of the Markov chain models and maintains coherence more easily than either. Most importantly, all of these methods are comparable in coherence to the human-authored games, i.e., our semantically grounded knowledge graph ensures that coherence is not lost when generating content.</p>
<p>Similarly, Fig. 8 describes how surprising the game was to the players. The difference between the one-room and five-room games here is much more pronounced. The players find the five-room, the longer and more complex game, much more surprising than their one-room counterparts, showing that complexity is an important factor in determining surprise. Another indication of this is that the MCC model is rated as more surpris</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Originality (novelty) scores for each game. Error bars indicate one standard deviation.</p>
<p>ing than the less complex MCS model. The LM achieves comparable performance to MCC and once again they all perform as well as the HD games.</p>
<p>Originality (Fig. 9), which we use as a proxy to measure novelty, exhibits similar trends as surprise. The more longer, more complex games are deemed more original. Despite being random, the RA games are seen to be less original than the the rest of the games perhaps indicating that there is a link between perceptions of coherence and originality. The gaps in performance here are much less pronounced, however, and the Markov chain models slightly edge out the LM — with all three being comparable to the HD games.</p>
<p>To measure value, or utility, in a text-adventure game, we asked the players if they felt a sense of accomplishment after finishing the game (Fig. 10). We see players reported a higher sense of accomplishment after finishing more complex games in general with the exception of the RA games, both of which performed poorly—likely due to them being relatively incoherent. We also note that the LM showed the highest values here, surpassing the HD games. We hypothesize that this might be due to the player having to perform a wider range of actions, some relatively unintuitive, that are not constrained by our action graph.</p>
<h2>7 Conclusions</h2>
<p>We have demonstrated a framework to automatically generate cooking quests in a "home" themed text-adventure game, although our framework can be generalized to other themes as well. Quest generation in a given game world is a subset</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Accomplishment (value) scores for each game. Error bars indicate one standard deviation.</p>
<p>of the overall problem of generating entire text-adventure games. Content generated by both the Markov chains and the neural language models can be grounded into a given game world using domain knowledge encoded in the form of a knowledge graph. The models each excel on different metrics: the Markov chains model produces quests that are more surprising and novel while the neural language model offers greater value and coherence. We also note, however, that the neural language model requires less domain knowledge than the Markov chains and is thus potentially more generalizable to other themes and types of quests.</p>
<p>Our human subject study shows us that there is an inverse relationship between creativity and coherence but only when a certain threshold of coherence is passed. In other words, the less coherent a game the more creative it is, but incoherent games—such as those generated by the RA model—are perceived to be less creative. Furthermore, our automatically generated games consistently perform at least as well as human designed games in this setting, both in terms of coherence and creativity—implying that the generative process can be automated without a loss in perceived game quality.</p>
<h2>8 Acknowledgements</h2>
<p>The authors would like to thank Dr. Ashok Goel and Fred Bane from the Cognitive Science class at Georgia Tech for their advice and encouragement of the ideas in this work.</p>
<h2>References</h2>
<p>Prithviraj Ammanabrolu and Mark O. Riedl. 2019a. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACLHLT 2019.</p>
<p>Prithviraj Ammanabrolu and Mark O. Riedl. 2019b. Transfer in deep reinforcement learning using knowledge graphs. CoRR, abs/1908.06556.</p>
<p>Gabor Angeli, Johnson Premkumar, Melvin Jose, and Christopher D. Manning. 2015. Leveraging Linguistic Structure For Open Domain Information Extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).</p>
<p>Margaret A Boden. 2007. Creativity in a nutshell. Think, 5(15):83-96.</p>
<p>Antoine Bordes, Nicolas Usunier, Ronan Collobert, and Jason Weston. 2010. Towards understanding situated natural language. In Proceedings of the 2010 International Conference on Artificial Intelligence and Statistics.</p>
<p>Jerome Bruner. 1991. The narrative construction of reality. Critical Inquiry, 18(1):1-21.</p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. 2018. Textworld: A learning environment for text-based games. arXiv preprint arXiv:1806.11532.</p>
<p>Allen I Fleishman. 1978. A method for simulating nonnormal distributions. Psychometrika, 43(4):521532.</p>
<p>Nancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. 2017. What can you do with a rock? affordance extraction via word embeddings. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, pages 1039-1045.</p>
<p>Spyridon Giannatos, Mark J Nelson, Yun-Gyung Cheong, and Georgios N Yannakakis. 2011. Suggesting New Plot Elements for an Interactive Story. In In Workshop on Intelligent Narrative Technologies (INT'11).</p>
<p>Matthew Guzdial, Brent Harrison, Boyang Li, and Mark Riedl. 2015. Crowdsourcing open interactive narrative. In $F D G$.</p>
<p>Matan Haroush, Tom Zahavy, Daniel J Mankowitz, and Shie Mannor. 2018. Learning How Not to Act in Text-Based Games. In Workshop Track at ICLR 2018, pages 1-4.
K. Hartsook, A. Zook, S. Das, and M. O. Riedl. 2011. Toward supporting stories with procedurally generated game worlds. In 2011 IEEE Conference on Computational Intelligence and Games (CIG'11), pages 297-304.</p>
<p>Matthew Hausknecht, Prithviraj Ammanabrolu, MarcAlexandre Côté, and Xingdi Yuan. 2019a. Interactive fiction games: A colossal adventure. CoRR, abs/1909.05398.</p>
<p>Matthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D. Williams. 2019b. Nail: A general interactive fiction agent. CoRR, abs/1902.04259.</p>
<p>Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. 2016. Deep Reinforcement Learning with a Natural Language Action Space. In Association for Computational Linguistics (ACL).</p>
<p>Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. CoRR, abs/1904.09751.</p>
<p>Judith R Johnston. 2008. Narratives Twenty-Five Years Later. In Top Lang Disorders, volume 28, pages 9398 .</p>
<p>Boyang Li, Stephen Lee-Urban, Darren Scott Appling, and Mark O. Riedl. 2012. Crowdsourcing Narrative Intelligence . In Advances in Cognitive Systems, volume 1, pages 1-18.</p>
<p>Brian Magerko, Justin Permar, Mikhail Jacob, Margeaux Comerford, and Justin Smith. 2014. An Overview of Computational Co-creative Pretend Play with a Human. In Proceedings of IVA 2014.</p>
<p>Lara J. Martin, Prithviraj Ammanabrolu, Xinyu Wang, Shruti Singh, Brent Harrison, Murtaza Dhuliawala, Pradyumna Tambwekar, Animesh Mehta, Richa Arora, Nathan Dass, Chris Purdy, and Mark O. Riedl. 2017. Improvisational Storytelling Agents. In Workshop on Machine Learning for Creativity and Design (NeurIPS 2017), Long Beach, CA.</p>
<p>Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. 2015. Language Understanding for Textbased Games Using Deep Reinforcement Learning. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Justin Permar and Brian Magerko. 2013. A Conceptual Blending Approach to the Generation of Cognitive Scripts for Interactive Narrative. In 9th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE13).</p>
<p>Christopher Purdy, Xinyu Wang, Larry He, and Mark Riedl. 2018. Predicting Generated Story Quality with Quantitative Measures. In In Proceedings of 14th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE18).</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Ronen Tamari, Hiroyuki Shindo, Dafna Shahaf, and Yuji Matsumoto. 2019. Playing by the book: An interactive game approach for action graph extraction from text. In Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications, pages 62-71, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Ruo Yu Tao, Marc-Alexandre Côté, Xingdi Yuan, and Layla El Asri. 2018. Towards solving text-based games by producing adaptive action spaces. In Proceedings of the 2018 NeurIPS Workshop on Wordplay: Reinforcement and Language Learning in Text-based Games.</p>
<p>Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rocktäschel, Douwe Kiela, Arthur Szlam, and Jason Weston. 2019. Learning to speak and act in a fantasy text adventure game. CoRR, abs/1903.03094.</p>
<p>Tony Veale, Diarmuid O'donoghue, and Mark T Keane. 2000. Computation and Blending. Cognitive Linguistics, 11(3/4):253282.</p>
<p>Zhilin Yang, Saizheng Zhang, Jack Urbanek, Will Feng, Alexander H. Miller, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Mastering the dungeon: Grounded language learning by mechanical turker descent. In In proceedings of the The Sixth International Conference on Learning Representations (ICLR'18).</p>
<p>Alexander Zook, Brian Magerko, and Mark Riedl. 2011. Formally modeling pretend object play. In Proceedings of the 8th ACM Conference on Creativity and Cognition, pages 147-156, New York, NY, USA. ACM.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Coherence scores for each game. Error bars indicate one standard deviation.</p>
<table>
<thead>
<tr>
<th>Group 1</th>
<th>Group 2</th>
<th>Meandiff</th>
</tr>
</thead>
<tbody>
<tr>
<td>HD-1</td>
<td>RA-5</td>
<td>-1.3958</td>
</tr>
<tr>
<td>LM-1</td>
<td>RA-5</td>
<td>-1.5</td>
</tr>
<tr>
<td>MCC-5</td>
<td>RA-5</td>
<td>-0.5333</td>
</tr>
<tr>
<td>MCS-1</td>
<td>RA-5</td>
<td>-1.1667</td>
</tr>
<tr>
<td>RA-1</td>
<td>RA-5</td>
<td>-1.2157</td>
</tr>
<tr>
<td>LM-5</td>
<td>MCS-1</td>
<td>0.1275</td>
</tr>
<tr>
<td>LM-5</td>
<td>MCS-5</td>
<td>-0.4392</td>
</tr>
<tr>
<td>LM-5</td>
<td>RA-5</td>
<td>-1.0392</td>
</tr>
<tr>
<td>LM-1</td>
<td>RA-1</td>
<td>-0.2843</td>
</tr>
</tbody>
</table>
<p>Table 1: Coherence results for the post-hoc Tukey HSD test with <em>p</em> = 0.1, only significant results are shown.</p>
<table>
<thead>
<tr>
<th>Group 1</th>
<th>Group 2</th>
<th>Meandiff</th>
</tr>
</thead>
<tbody>
<tr>
<td>HD-1</td>
<td>HD-5</td>
<td>1.8125</td>
</tr>
<tr>
<td>HD-1</td>
<td>LM-1</td>
<td>1.2292</td>
</tr>
<tr>
<td>HD-1</td>
<td>LM-5</td>
<td>1.4007</td>
</tr>
<tr>
<td>HD-1</td>
<td>MCC-5</td>
<td>1.6125</td>
</tr>
<tr>
<td>HD-1</td>
<td>MCS-1</td>
<td>0.5347</td>
</tr>
<tr>
<td>HD-1</td>
<td>MCS-5</td>
<td>1.5458</td>
</tr>
<tr>
<td>HD-1</td>
<td>RA-1</td>
<td>1.5772</td>
</tr>
<tr>
<td>HD-1</td>
<td>RA-5</td>
<td>0.9458</td>
</tr>
<tr>
<td>MCC-1</td>
<td>MCC-5</td>
<td>1.05</td>
</tr>
<tr>
<td>MCC-1</td>
<td>RA-1</td>
<td>1.0147</td>
</tr>
</tbody>
</table>
<p>Table 2: Originality results for the post-hoc Tukey HSD test with <em>p</em> = 0.1, only significant results are shown.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Originality (novelty) scores for each game. Error bars indicate one standard deviation.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Unpredictability (surprise) scores for each game. Error bars indicate one standard deviation.</p>
<table>
<thead>
<tr>
<th>Group 1</th>
<th>Group 2</th>
<th>Meandiff</th>
</tr>
</thead>
<tbody>
<tr>
<td>HD-1</td>
<td>HD-5</td>
<td>2.6106</td>
</tr>
<tr>
<td>HD-1</td>
<td>LM-5</td>
<td>2.7463</td>
</tr>
<tr>
<td>HD-1</td>
<td>MCC-5</td>
<td>2.6875</td>
</tr>
<tr>
<td>HD-1</td>
<td>MCS-5</td>
<td>1.8875</td>
</tr>
<tr>
<td>HD-1</td>
<td>MCC-1</td>
<td>0.9375</td>
</tr>
<tr>
<td>HD-1</td>
<td>RA-1</td>
<td>0.9228</td>
</tr>
<tr>
<td>HD-1</td>
<td>RA-5</td>
<td>1.4208</td>
</tr>
<tr>
<td>HD-5</td>
<td>LM-1</td>
<td>-2.3397</td>
</tr>
<tr>
<td>HD-5</td>
<td>MCC-1</td>
<td>-1.6731</td>
</tr>
<tr>
<td>HD-5</td>
<td>MCS-1</td>
<td>-2.5342</td>
</tr>
<tr>
<td>LM-1</td>
<td>LM-5</td>
<td>2.4755</td>
</tr>
<tr>
<td>LM-1</td>
<td>MCC-1</td>
<td>0.6667</td>
</tr>
<tr>
<td>LM-1</td>
<td>MCC-5</td>
<td>2.4167</td>
</tr>
<tr>
<td>LM-1</td>
<td>MCS-5</td>
<td>1.6167</td>
</tr>
<tr>
<td>LM-5</td>
<td>MCC-1</td>
<td>-1.8088</td>
</tr>
<tr>
<td>LM-5</td>
<td>MCS-1</td>
<td>-2.6699</td>
</tr>
<tr>
<td>LM-5</td>
<td>RA-1</td>
<td>-1.8235</td>
</tr>
<tr>
<td>LM-5</td>
<td>RA-5</td>
<td>-1.3255</td>
</tr>
<tr>
<td>MCC-1</td>
<td>MCC-5</td>
<td>1.75</td>
</tr>
<tr>
<td>MCC-1</td>
<td>MCS-1</td>
<td>-0.8611</td>
</tr>
<tr>
<td>MCC-5</td>
<td>RA-1</td>
<td>-1.7647</td>
</tr>
<tr>
<td>MCC-5</td>
<td>RA-5</td>
<td>-1.2667</td>
</tr>
<tr>
<td>MCS-1</td>
<td>MCS-5</td>
<td>1.8111</td>
</tr>
<tr>
<td>MCS-1</td>
<td>RA-1</td>
<td>0.8464</td>
</tr>
<tr>
<td>MCS-1</td>
<td>RA-5</td>
<td>1.3444</td>
</tr>
</tbody>
</table>
<p>Table 3: Surprise results for the post-hoc Tukey HSD test with <em>p</em> = 0.1, only significant results are shown.</p>
<table>
<thead>
<tr>
<th>Group 1</th>
<th>Group 2</th>
<th>Meandiff</th>
</tr>
</thead>
<tbody>
<tr>
<td>HD-1</td>
<td>HD-5</td>
<td>1.899</td>
</tr>
<tr>
<td>HD-1</td>
<td>LM-1</td>
<td>2.1042</td>
</tr>
<tr>
<td>HD-1</td>
<td>LM-5</td>
<td>2.7904</td>
</tr>
<tr>
<td>HD-1</td>
<td>MCC-5</td>
<td>2.0375</td>
</tr>
<tr>
<td>HD-1</td>
<td>MCS-1</td>
<td>1.6042</td>
</tr>
<tr>
<td>HD-1</td>
<td>MCS-5</td>
<td>1.7708</td>
</tr>
<tr>
<td>HD-1</td>
<td>RA-1</td>
<td>1.6728</td>
</tr>
<tr>
<td>HD-5</td>
<td>LM-5</td>
<td>0.8914</td>
</tr>
<tr>
<td>HD-5</td>
<td>RA-5</td>
<td>-1.3282</td>
</tr>
<tr>
<td>LM-1</td>
<td>MCC-1</td>
<td>-1.25</td>
</tr>
<tr>
<td>LM-1</td>
<td>MCS-1</td>
<td>-0.5</td>
</tr>
<tr>
<td>LM-1</td>
<td>RA-5</td>
<td>-1.5333</td>
</tr>
<tr>
<td>LM-5</td>
<td>MCC-1</td>
<td>-1.9363</td>
</tr>
<tr>
<td>LM-5</td>
<td>MCS-1</td>
<td>-1.1863</td>
</tr>
<tr>
<td>LM-5</td>
<td>MCS-5</td>
<td>-1.0196</td>
</tr>
<tr>
<td>LM-5</td>
<td>RA-1</td>
<td>-1.1176</td>
</tr>
<tr>
<td>LM-5</td>
<td>RA-5</td>
<td>-2.2196</td>
</tr>
<tr>
<td>MCC-1</td>
<td>MCC-5</td>
<td>1.1833</td>
</tr>
<tr>
<td>MCC-5</td>
<td>RA-5</td>
<td>-1.4667</td>
</tr>
<tr>
<td>MCS-5</td>
<td>RA-5</td>
<td>-1.2</td>
</tr>
</tbody>
</table>
<p>Figure 14: Accomplishment (value) scores for each game. Error bars indicate one standard deviation.</p>
<p>Table 4: Value results for the post-hoc Tukey HSD test with <em>p</em> = 0.1, only significant results are shown.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/kbrohkahn/ recipe-parser&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
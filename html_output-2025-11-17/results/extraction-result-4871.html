<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4871 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4871</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4871</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-264805345</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.emnlp-main.568.pdf" target="_blank">What’s “up” with vision-language models? Investigating their struggle with spatial reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Recent vision-language (VL) models are powerful, but can they reliably distinguish “right” from “left”? We curate three new corpora to quantify model comprehension of such basic spatial relations. These tests isolate spatial reasoning more precisely than existing datasets like VQAv2, e.g., our What’sUp benchmark contains sets of photographs varying only the spatial relations of objects, keeping their identity fixed (see Figure 1: models must comprehend not only the usual case of a dog under a table, but also, the same dog on top of the same table). We evaluate 18 VL models, finding that all perform poorly, e.g., BLIP fine-tuned on VQAv2, which nears human parity on VQAv2, achieves 56% accuracy on our benchmarks vs. humans at 99%. We conclude by studying causes of this surprising behavior, finding: 1) that popular vision-language pre-training corpora like LAION-2B contain little reliable data for learning spatial relationships; and 2) that basic modeling interventions like up-weighting preposition-containing instances or fine-tuning on our corpora are not sufficient to address the challenges our benchmarks pose. We are hopeful that these corpora will facilitate further research, and we release our data and code at https://github.com/amitakamath/ whatsup_vlms .</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4871.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4871.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language–Image Pretraining) variants (ViT-B/32, ViT-L/14, OpenCLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Contrastive vision-language models trained to align image and text embeddings (variants evaluated include ViT-B/32 and ViT-L/14 OpenCLIP models trained on LAION). Used in zero-shot and finetuned settings to select captions that differ only by spatial prepositions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP ViT-B/32 and CLIP ViT-L/14 (OpenCLIP variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Contrastive image-text encoder models using ViT image backbones (ViT-B/32 and ViT-L/14 variants). OpenCLIP versions trained on large-scale scraped corpora (LAION-2B). Trained contrastively to maximize image-text dot-product for matching pairs across large batches.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial reasoning benchmarks: What'sUp, COCO-spatial, GQA-spatial</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Multiple-choice image captioning tasks where each image is paired with several captions that differ only by a spatial preposition (e.g., 'mug on table' vs 'mug under table'); What'sUp is tightly controlled with 4-option sets per object pair, COCO- and GQA-spatial are curated two-option tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Image-text matching via dot-product between image and caption embeddings; scoring captions and selecting the highest-scoring caption. Also used in experiments with renormalized caption priors, prompt rewriting (e.g., replacing 'behind' with 'in the background'), and finetuning on spatially filtered data and on automatically generated hard negatives (switched prepositions).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Quantitative evaluation on the three benchmarks showed poor performance, indicating weak spatial reasoning. Representational probing (linear analogies) on What'sUp preposition pairs for CLIP variants produced only ~9% analogy accuracy for preposition transformations versus 61% for color analogies, suggesting spatial relations are weakly encoded compared to color. Additional evidence: poor pair- and set-wise accuracies on What'sUp and failure to fit datasets augmented with hard negative preposition captions (very high training loss).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zero-shot example (CLIP ViT-B/32 from finetune experiments table): What'sUp 31.0% accuracy, COCO-spatial 47.4%, GQA-spatial 46.9% (average ~41.8%). These are only a few points above random on What'sUp (random=25% for 4-option) and below human estimates (~98–100%). Renormalizing by caption priors gave small, inconsistent changes. Finetuning on curated COCO/GQA training splits improved COCO/GQA but not What'sUp in some settings (see paper Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Models often behave near-random on tightly controlled spatial tasks; sometimes collapse to predicting only 1–2 prepositions across inputs. Contrastive training with large batches does not reliably force learning of prepositions because exact preposition-bearing captions are rare in batches. LAION captions contain spatial prepositions <0.2% of the time and are often ambiguous/extraneous. CLIP struggled to fit training data augmented with hard negative switched-preposition captions (very high training loss), indicating inability to learn fine-grained prepositional distinctions under the studied regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Performs far worse than humans (human estimate ~97–100% across datasets). Sometimes performs worse than simple text-only or random baselines on specific metrics (e.g., random/text-only baselines reported). CLIP variants underperform cross-attention one-stack models like XVLM in zero-shot on these spatial benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What’s “up” with vision-language models? Investigating their struggle with spatial reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4871.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4871.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLIP / BLIP-VQA / BLIP2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLIP (Bootstrapping Language–Image Pretraining), BLIP-VQA (BLIP finetuned on VQAv2), BLIP2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vision-language models trained with image-text objectives and used for both generative and discriminative V+L tasks; BLIP-VQA is BLIP finetuned for visual question answering and BLIP2 exposes ITM and ITC heads.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLIP (14M and 129M variants), BLIP-VQA (finetuned on VQAv2), BLIP2 (ITM/ITC heads)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models combining image encoders with text decoders/encoders trained on large-scale captioning corpora; BLIP has generative and discriminative heads (image-text matching), BLIP2 uses frozen image encoders with language models to perform multimodal tasks. BLIP models evaluated both zero-shot and after finetuning on downstream datasets (Flickr30K, COCO retrieval, VQAv2).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial reasoning benchmarks: What'sUp, COCO-spatial, GQA-spatial</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same as above: image + multiple captions differing only by a spatial preposition; the model must choose the correct preposition describing the spatial relation in the image.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Image-text matching scoring (ITM/ITC) for caption selection; for generative heads (BLIP-VQA/BLIP2), posed as yes/no questions and model probability of 'yes'/'no' is used to choose the correct option. Experiments include finetuning on VQAv2 and other retrieval/captioning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Quantitative evaluation shows BLIP (including BLIP-VQA) struggles: BLIP finetuned on VQAv2, despite nearing human parity on VQAv2, attains only ~56% on these spatial benchmarks (paper reports this explicitly). Pair/set analyses show poor consistency across complementary spatial images (e.g., getting all four prepositions correct for a set is rare), indicating lack of robust prepositional concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported: BLIP finetuned on VQAv2 achieves ~56% accuracy on the proposed benchmarks (paper abstract). Other BLIP variants often fall near chance; BLIP-VQA outperforms BLIP in some cases but still falls far short of human performance (~98–100%). Exact per-benchmark numbers are in the paper's tables (summary: poor relative to humans).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Even strong finetuning (e.g., VQAv2) yields limited gains; models can be brittle and inconsistent across paired images. Generative heads require framing as queries and still rely on surface probabilities, which do not reliably reflect spatial understanding. BLIP variants also suffer from data shortcomings (rare/ambiguous prepositions in pretraining corpora) and do not generalize strongly to tightly controlled spatial distributions like What'sUp.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>BLIP-VQA (finetuned) performs better than zero-shot BLIP but still substantially worse than human estimated performance; models often perform worse on spatial benchmarks than on general VQA benchmarks (where BLIP-VQA can approach parity).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What’s “up” with vision-language models? Investigating their struggle with spatial reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4871.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4871.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XVLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XVLM (Cross-view Vision-Language Model variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>One-stack cross-attention vision-language model that incorporates finer-grained supervision (including bounding box level) and achieves the strongest zero-shot performance among models evaluated on the spatial benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XVLM (4M and 16M parameter variants evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cross-attention, one-stack vision-language models trained with mixed objectives; variants differ in parameter counts (paper mentions 4M and 16M versions) and use more fine-grained supervision including bounding-box-level alignment in addition to image-level captions.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial reasoning benchmarks: What'sUp, COCO-spatial, GQA-spatial</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Multiple-choice caption selection where only spatial prepositions differ, requiring the model to resolve 'left/right/above/below/on/under/in front of/behind' relations between objects in images.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Image-text cross-attention scoring enabling more detailed alignment between words and image regions; evaluated zero-shot primarily, and contrasted against two-stack contrastive models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>XVLM was the highest-performing model in the zero-shot evaluations among the 18 VL models studied, suggesting that cross-attention and bounding-box level supervision helps spatial distinctions. However, even XVLM's absolute performance remained far below human estimates and insufficient on pair/set metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The paper reports XVLM outperforms other zero-shot models but does not give a single aggregate number in text; overall finding: all models, including XVLM, fall well below human performance (~98–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Despite being strongest among evaluated models, XVLM still performs poorly on many instances and fails to reliably generalize prepositional concepts across sets; bounding-box supervision helps but is not sufficient under the data/scale regimes evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Outperforms two-stack contrastive models (e.g., CLIP variants) in zero-shot on these spatial tasks, but still far worse than human performance and often not robust across controlled What'sUp sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What’s “up” with vision-language models? Investigating their struggle with spatial reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4871.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4871.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoCa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoCa (Contrastive Captioner — combined generative and contrastive objectives)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mixed-objective vision-language model trained with both generative captioning and contrastive alignment used to evaluate spatial relation understanding; included in the suite of evaluated models but does not outperform simpler contrastive models on spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoCa (Contrastive Captioner)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Model trained with both generative captioning and contrastive objectives intending to capture richer multimodal signals; evaluated zero-shot and with COCO captioning finetuning in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial reasoning benchmarks: What'sUp, COCO-spatial, GQA-spatial</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same caption-selection tasks focused on distinguishing spatial prepositions; requires visual grounding of spatial language.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Uses image-text scoring from its contrastive and generative heads to select among caption options; finetuning experiments include COCO captioning for downstream adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>CoCa did not show consistent advantage on spatial reasoning despite generative objectives; paper notes that generative objectives did not provide clear benefit for spatial relations compared to contrastive-only models (CoCa scored less than CLIP ViT-B/32 in some evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in aggregate: CoCa scores lower than some CLIP variants on the spatial benchmarks in zero-shot; finetuning on COCO captioning did not consistently improve spatial relation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Generative training objective alone did not yield improved spatial understanding; CoCa underperforms some contrastive models on the spatial benchmarks, indicating that mixed objectives in the studied setups are insufficient for reliable spatial relation comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Performs worse than CLIP ViT-B/32 on the spatial benchmarks in some zero-shot comparisons; does not approach human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What’s “up” with vision-language models? Investigating their struggle with spatial reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4871.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4871.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAVA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAVA (Foundational Language and Vision Alignment Model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A foundational multimodal model aligning visual and textual modalities, included among the evaluated VL models; like others, it exhibits poor performance on tightly controlled spatial relation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAVA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A unified vision-and-language model trained to align and integrate multimodal signals for downstream tasks, evaluated in a zero-shot capacity on the paper's spatial benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial reasoning benchmarks: What'sUp, COCO-spatial, GQA-spatial</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Caption-selection tasks constructed to isolate spatial prepositions, requiring distinguishing spatial relations between objects in images.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Uses learned multimodal representations to score image-caption pairs; evaluated zero-shot for caption selection tasks and compared with other VL models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Like most evaluated models, FLAVA shows poor accuracy on the paper's spatial benchmarks, suggesting weak encoding of prepositional spatial relations despite multimodal training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregate results place FLAVA among poorly performing models on the spatial benchmarks—specific per-benchmark numbers appear in the paper's tables (all evaluated models fall far below human performance).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>FLAVA struggles with tightly controlled spatial distinctions; pretraining data sparsity and ambiguity of prepositions in corpora likely hamper spatial learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Performs substantially worse than humans and comparably poorly to other VL baselines; no evidence it substantially outperforms CLIP-style models on these tasks in the reported evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What’s “up” with vision-language models? Investigating their struggle with spatial reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4871.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4871.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP variants (negCLIP, RoBERTaCLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>negCLIP and RoBERTa-initialized CLIP variants</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variants of CLIP trained or initialized with modified objectives/weights (negCLIP trained on word-order-shuffled data; RoBERTaCLIP initialized with RoBERTa text weights); evaluated to probe sensitivity to language modeling changes for spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>negCLIP (word-order shuffling finetuned CLIP), RoBERTaCLIP (RoBERTa-initialized CLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>negCLIP: CLIP ViT-B/32 finetuned with word-order shuffling augmentation; RoBERTaCLIP: CLIP initialized with RoBERTa-pretrained text encoder weights. Both maintain CLIP's contrastive alignment objective but vary text encoder properties/training.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial reasoning benchmarks: What'sUp, COCO-spatial, GQA-spatial</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Caption-selection tasks differing only by spatial prepositions requiring grounding of language to spatial relations in images.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Same image-text dot-product scoring as CLIP; tested to see whether altering text encoder pretraining or augmentations affects spatial sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>The variants achieved low analogy and caption-selection accuracies on spatial tasks; representation analogy experiments showed ~9% accuracy for preposition analogies across CLIP variants, indicating limited structured spatial knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports these CLIP variants perform poorly on spatial benchmarks and often only marginally above random; exact numbers are in the paper's detailed tables, but generally they do not approach human levels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Changing text encoder initialization or word-order augmentations did not meaningfully remediate spatial reasoning failures; these variants still suffer from training-data sparsity/ambiguity for prepositions and contrastive objective limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Perform similarly to standard CLIP variants; do not outperform cross-attention models like XVLM; far worse than humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What’s “up” with vision-language models? Investigating their struggle with spatial reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4871.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4871.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP (prompt/prompting & priors experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experiments: renormalizing caption priors, prompt rewriting (e.g., 'behind' -> 'in the background'), and hard-negative finetuning on CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Data-informed interventions applied to CLIP to test whether distributional/textual factors explain poor spatial performance: re-normalizing caption priors, replacing rare prepositions with more common words, finetuning on preposition-containing LAION subsets and using automated hard negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP ViT-B/32 with caption-prior renormalization, prompt rewriting, and finetuning variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same base CLIP model used with post-hoc probability renormalization by estimated text prior, alternative textual prompts (e.g., replacing 'behind' with 'in the background'), finetuning on curated LAION subsets containing prepositions, and finetuning with programmatically created hard negative captions (switched prepositions).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial reasoning benchmarks: What'sUp (tight 4-option sets), COCO-spatial, GQA-spatial</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Tasks require resolving fine-grained spatial prepositions where textual priors and distributional mismatch might bias predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>1) Re-normalize caption probabilities by subtracting an estimated caption prior (text-only prior) to counter high-probability captions; 2) prompt rewriting to substitute rare prepositions with more common spatial-indicative words (case study: 'behind' vs 'in the background'); 3) finetuning on LAION subset filtered for prepositions and on LAION-4M with augmented hard negatives (switched prepositions).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Prompt rewriting ('background' instead of 'behind') improved average accuracy on that subset from ~52% to ~67% for Open-CLIP variants, indicating some latent spatial information can be surfaced with better wording. Renormalization of caption priors yielded small, inconsistent gains. Finetuning on preposition-rich LAION-4M produced only marginal improvements; adding hard negatives caused extremely high training loss and failure to fit, suggesting models cannot easily learn those fine-grained distinctions under studied regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Prompt rewriting (CLIP ViT-B/32 & ViT-L/14): average performance on 'behind' vs 'background' case study increased from 52% to 67%. Renormalization had slight benefits on COCO/GQA but not on What'sUp overall. Finetuning on LAION-4M-prep gave modest increases on some splits (see Table 2 summarized results: + LAION-4M-prep avg ~42.2% vs base ~41.8%), but results were inconsistent and far below human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Renormalization cannot fully correct for lack of grounded spatial knowledge. Prompt rewriting success depended on availability of frequently observed alternative phrasing in pretraining corpora (e.g., 'background' is unusually common). Finetuning with hard negatives produced prohibitively high training loss (models could not fit), demonstrating difficulty of learning prepositional disambiguation with the studied objectives and data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Prompt rewriting outperformed the original rare preposition wording in the case study; however, overall approaches did not yield consistent improvements to reach human-level performance. Hard-negative finetuning failed while naive finetuning yielded only modest gains in COCO/GQA but sometimes hurt performance on the controlled What'sUp dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What’s “up” with vision-language models? Investigating their struggle with spatial reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>BLIP: Bootstrapping language-image pretraining for unified vision-language understanding and generation <em>(Rating: 2)</em></li>
                <li>BLIP-2: bootstrapping language-image pretraining with frozen image encoders and large language models <em>(Rating: 2)</em></li>
                <li>CoCa: Contrastive Captioners are image-text foundation models <em>(Rating: 2)</em></li>
                <li>Multigrained vision language pre-training: Aligning texts with visual concepts (XVLM) <em>(Rating: 2)</em></li>
                <li>LAION-5b: An open large-scale dataset for training next generation image-text models <em>(Rating: 2)</em></li>
                <li>When and why vision-language models behave like bags-of-words, and what to do about it? <em>(Rating: 1)</em></li>
                <li>VALSE: A task-independent benchmark for vision and language models centered on linguistic phenomena <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4871",
    "paper_id": "paper-264805345",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "CLIP",
            "name_full": "CLIP (Contrastive Language–Image Pretraining) variants (ViT-B/32, ViT-L/14, OpenCLIP)",
            "brief_description": "Contrastive vision-language models trained to align image and text embeddings (variants evaluated include ViT-B/32 and ViT-L/14 OpenCLIP models trained on LAION). Used in zero-shot and finetuned settings to select captions that differ only by spatial prepositions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CLIP ViT-B/32 and CLIP ViT-L/14 (OpenCLIP variants)",
            "model_description": "Contrastive image-text encoder models using ViT image backbones (ViT-B/32 and ViT-L/14 variants). OpenCLIP versions trained on large-scale scraped corpora (LAION-2B). Trained contrastively to maximize image-text dot-product for matching pairs across large batches.",
            "puzzle_name": "Spatial reasoning benchmarks: What'sUp, COCO-spatial, GQA-spatial",
            "puzzle_description": "Multiple-choice image captioning tasks where each image is paired with several captions that differ only by a spatial preposition (e.g., 'mug on table' vs 'mug under table'); What'sUp is tightly controlled with 4-option sets per object pair, COCO- and GQA-spatial are curated two-option tasks.",
            "mechanism_or_strategy": "Image-text matching via dot-product between image and caption embeddings; scoring captions and selecting the highest-scoring caption. Also used in experiments with renormalized caption priors, prompt rewriting (e.g., replacing 'behind' with 'in the background'), and finetuning on spatially filtered data and on automatically generated hard negatives (switched prepositions).",
            "evidence_of_spatial_reasoning": "Quantitative evaluation on the three benchmarks showed poor performance, indicating weak spatial reasoning. Representational probing (linear analogies) on What'sUp preposition pairs for CLIP variants produced only ~9% analogy accuracy for preposition transformations versus 61% for color analogies, suggesting spatial relations are weakly encoded compared to color. Additional evidence: poor pair- and set-wise accuracies on What'sUp and failure to fit datasets augmented with hard negative preposition captions (very high training loss).",
            "performance_metrics": "Zero-shot example (CLIP ViT-B/32 from finetune experiments table): What'sUp 31.0% accuracy, COCO-spatial 47.4%, GQA-spatial 46.9% (average ~41.8%). These are only a few points above random on What'sUp (random=25% for 4-option) and below human estimates (~98–100%). Renormalizing by caption priors gave small, inconsistent changes. Finetuning on curated COCO/GQA training splits improved COCO/GQA but not What'sUp in some settings (see paper Table 2).",
            "limitations_or_failure_cases": "Models often behave near-random on tightly controlled spatial tasks; sometimes collapse to predicting only 1–2 prepositions across inputs. Contrastive training with large batches does not reliably force learning of prepositions because exact preposition-bearing captions are rare in batches. LAION captions contain spatial prepositions &lt;0.2% of the time and are often ambiguous/extraneous. CLIP struggled to fit training data augmented with hard negative switched-preposition captions (very high training loss), indicating inability to learn fine-grained prepositional distinctions under the studied regimes.",
            "comparison_baseline": "Performs far worse than humans (human estimate ~97–100% across datasets). Sometimes performs worse than simple text-only or random baselines on specific metrics (e.g., random/text-only baselines reported). CLIP variants underperform cross-attention one-stack models like XVLM in zero-shot on these spatial benchmarks.",
            "uuid": "e4871.0",
            "source_info": {
                "paper_title": "What’s “up” with vision-language models? Investigating their struggle with spatial reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "BLIP / BLIP-VQA / BLIP2",
            "name_full": "BLIP (Bootstrapping Language–Image Pretraining), BLIP-VQA (BLIP finetuned on VQAv2), BLIP2",
            "brief_description": "Vision-language models trained with image-text objectives and used for both generative and discriminative V+L tasks; BLIP-VQA is BLIP finetuned for visual question answering and BLIP2 exposes ITM and ITC heads.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BLIP (14M and 129M variants), BLIP-VQA (finetuned on VQAv2), BLIP2 (ITM/ITC heads)",
            "model_description": "Models combining image encoders with text decoders/encoders trained on large-scale captioning corpora; BLIP has generative and discriminative heads (image-text matching), BLIP2 uses frozen image encoders with language models to perform multimodal tasks. BLIP models evaluated both zero-shot and after finetuning on downstream datasets (Flickr30K, COCO retrieval, VQAv2).",
            "puzzle_name": "Spatial reasoning benchmarks: What'sUp, COCO-spatial, GQA-spatial",
            "puzzle_description": "Same as above: image + multiple captions differing only by a spatial preposition; the model must choose the correct preposition describing the spatial relation in the image.",
            "mechanism_or_strategy": "Image-text matching scoring (ITM/ITC) for caption selection; for generative heads (BLIP-VQA/BLIP2), posed as yes/no questions and model probability of 'yes'/'no' is used to choose the correct option. Experiments include finetuning on VQAv2 and other retrieval/captioning datasets.",
            "evidence_of_spatial_reasoning": "Quantitative evaluation shows BLIP (including BLIP-VQA) struggles: BLIP finetuned on VQAv2, despite nearing human parity on VQAv2, attains only ~56% on these spatial benchmarks (paper reports this explicitly). Pair/set analyses show poor consistency across complementary spatial images (e.g., getting all four prepositions correct for a set is rare), indicating lack of robust prepositional concepts.",
            "performance_metrics": "Reported: BLIP finetuned on VQAv2 achieves ~56% accuracy on the proposed benchmarks (paper abstract). Other BLIP variants often fall near chance; BLIP-VQA outperforms BLIP in some cases but still falls far short of human performance (~98–100%). Exact per-benchmark numbers are in the paper's tables (summary: poor relative to humans).",
            "limitations_or_failure_cases": "Even strong finetuning (e.g., VQAv2) yields limited gains; models can be brittle and inconsistent across paired images. Generative heads require framing as queries and still rely on surface probabilities, which do not reliably reflect spatial understanding. BLIP variants also suffer from data shortcomings (rare/ambiguous prepositions in pretraining corpora) and do not generalize strongly to tightly controlled spatial distributions like What'sUp.",
            "comparison_baseline": "BLIP-VQA (finetuned) performs better than zero-shot BLIP but still substantially worse than human estimated performance; models often perform worse on spatial benchmarks than on general VQA benchmarks (where BLIP-VQA can approach parity).",
            "uuid": "e4871.1",
            "source_info": {
                "paper_title": "What’s “up” with vision-language models? Investigating their struggle with spatial reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "XVLM",
            "name_full": "XVLM (Cross-view Vision-Language Model variants)",
            "brief_description": "One-stack cross-attention vision-language model that incorporates finer-grained supervision (including bounding box level) and achieves the strongest zero-shot performance among models evaluated on the spatial benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "XVLM (4M and 16M parameter variants evaluated)",
            "model_description": "Cross-attention, one-stack vision-language models trained with mixed objectives; variants differ in parameter counts (paper mentions 4M and 16M versions) and use more fine-grained supervision including bounding-box-level alignment in addition to image-level captions.",
            "puzzle_name": "Spatial reasoning benchmarks: What'sUp, COCO-spatial, GQA-spatial",
            "puzzle_description": "Multiple-choice caption selection where only spatial prepositions differ, requiring the model to resolve 'left/right/above/below/on/under/in front of/behind' relations between objects in images.",
            "mechanism_or_strategy": "Image-text cross-attention scoring enabling more detailed alignment between words and image regions; evaluated zero-shot primarily, and contrasted against two-stack contrastive models.",
            "evidence_of_spatial_reasoning": "XVLM was the highest-performing model in the zero-shot evaluations among the 18 VL models studied, suggesting that cross-attention and bounding-box level supervision helps spatial distinctions. However, even XVLM's absolute performance remained far below human estimates and insufficient on pair/set metrics.",
            "performance_metrics": "The paper reports XVLM outperforms other zero-shot models but does not give a single aggregate number in text; overall finding: all models, including XVLM, fall well below human performance (~98–100%).",
            "limitations_or_failure_cases": "Despite being strongest among evaluated models, XVLM still performs poorly on many instances and fails to reliably generalize prepositional concepts across sets; bounding-box supervision helps but is not sufficient under the data/scale regimes evaluated.",
            "comparison_baseline": "Outperforms two-stack contrastive models (e.g., CLIP variants) in zero-shot on these spatial tasks, but still far worse than human performance and often not robust across controlled What'sUp sets.",
            "uuid": "e4871.2",
            "source_info": {
                "paper_title": "What’s “up” with vision-language models? Investigating their struggle with spatial reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CoCa",
            "name_full": "CoCa (Contrastive Captioner — combined generative and contrastive objectives)",
            "brief_description": "A mixed-objective vision-language model trained with both generative captioning and contrastive alignment used to evaluate spatial relation understanding; included in the suite of evaluated models but does not outperform simpler contrastive models on spatial tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CoCa (Contrastive Captioner)",
            "model_description": "Model trained with both generative captioning and contrastive objectives intending to capture richer multimodal signals; evaluated zero-shot and with COCO captioning finetuning in the paper's experiments.",
            "puzzle_name": "Spatial reasoning benchmarks: What'sUp, COCO-spatial, GQA-spatial",
            "puzzle_description": "Same caption-selection tasks focused on distinguishing spatial prepositions; requires visual grounding of spatial language.",
            "mechanism_or_strategy": "Uses image-text scoring from its contrastive and generative heads to select among caption options; finetuning experiments include COCO captioning for downstream adaptation.",
            "evidence_of_spatial_reasoning": "CoCa did not show consistent advantage on spatial reasoning despite generative objectives; paper notes that generative objectives did not provide clear benefit for spatial relations compared to contrastive-only models (CoCa scored less than CLIP ViT-B/32 in some evaluations).",
            "performance_metrics": "Reported in aggregate: CoCa scores lower than some CLIP variants on the spatial benchmarks in zero-shot; finetuning on COCO captioning did not consistently improve spatial relation performance.",
            "limitations_or_failure_cases": "Generative training objective alone did not yield improved spatial understanding; CoCa underperforms some contrastive models on the spatial benchmarks, indicating that mixed objectives in the studied setups are insufficient for reliable spatial relation comprehension.",
            "comparison_baseline": "Performs worse than CLIP ViT-B/32 on the spatial benchmarks in some zero-shot comparisons; does not approach human performance.",
            "uuid": "e4871.3",
            "source_info": {
                "paper_title": "What’s “up” with vision-language models? Investigating their struggle with spatial reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "FLAVA",
            "name_full": "FLAVA (Foundational Language and Vision Alignment Model)",
            "brief_description": "A foundational multimodal model aligning visual and textual modalities, included among the evaluated VL models; like others, it exhibits poor performance on tightly controlled spatial relation tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "FLAVA",
            "model_description": "A unified vision-and-language model trained to align and integrate multimodal signals for downstream tasks, evaluated in a zero-shot capacity on the paper's spatial benchmarks.",
            "puzzle_name": "Spatial reasoning benchmarks: What'sUp, COCO-spatial, GQA-spatial",
            "puzzle_description": "Caption-selection tasks constructed to isolate spatial prepositions, requiring distinguishing spatial relations between objects in images.",
            "mechanism_or_strategy": "Uses learned multimodal representations to score image-caption pairs; evaluated zero-shot for caption selection tasks and compared with other VL models.",
            "evidence_of_spatial_reasoning": "Like most evaluated models, FLAVA shows poor accuracy on the paper's spatial benchmarks, suggesting weak encoding of prepositional spatial relations despite multimodal training.",
            "performance_metrics": "Aggregate results place FLAVA among poorly performing models on the spatial benchmarks—specific per-benchmark numbers appear in the paper's tables (all evaluated models fall far below human performance).",
            "limitations_or_failure_cases": "FLAVA struggles with tightly controlled spatial distinctions; pretraining data sparsity and ambiguity of prepositions in corpora likely hamper spatial learning.",
            "comparison_baseline": "Performs substantially worse than humans and comparably poorly to other VL baselines; no evidence it substantially outperforms CLIP-style models on these tasks in the reported evaluations.",
            "uuid": "e4871.4",
            "source_info": {
                "paper_title": "What’s “up” with vision-language models? Investigating their struggle with spatial reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CLIP variants (negCLIP, RoBERTaCLIP)",
            "name_full": "negCLIP and RoBERTa-initialized CLIP variants",
            "brief_description": "Variants of CLIP trained or initialized with modified objectives/weights (negCLIP trained on word-order-shuffled data; RoBERTaCLIP initialized with RoBERTa text weights); evaluated to probe sensitivity to language modeling changes for spatial tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "negCLIP (word-order shuffling finetuned CLIP), RoBERTaCLIP (RoBERTa-initialized CLIP)",
            "model_description": "negCLIP: CLIP ViT-B/32 finetuned with word-order shuffling augmentation; RoBERTaCLIP: CLIP initialized with RoBERTa-pretrained text encoder weights. Both maintain CLIP's contrastive alignment objective but vary text encoder properties/training.",
            "puzzle_name": "Spatial reasoning benchmarks: What'sUp, COCO-spatial, GQA-spatial",
            "puzzle_description": "Caption-selection tasks differing only by spatial prepositions requiring grounding of language to spatial relations in images.",
            "mechanism_or_strategy": "Same image-text dot-product scoring as CLIP; tested to see whether altering text encoder pretraining or augmentations affects spatial sensitivity.",
            "evidence_of_spatial_reasoning": "The variants achieved low analogy and caption-selection accuracies on spatial tasks; representation analogy experiments showed ~9% accuracy for preposition analogies across CLIP variants, indicating limited structured spatial knowledge.",
            "performance_metrics": "Paper reports these CLIP variants perform poorly on spatial benchmarks and often only marginally above random; exact numbers are in the paper's detailed tables, but generally they do not approach human levels.",
            "limitations_or_failure_cases": "Changing text encoder initialization or word-order augmentations did not meaningfully remediate spatial reasoning failures; these variants still suffer from training-data sparsity/ambiguity for prepositions and contrastive objective limitations.",
            "comparison_baseline": "Perform similarly to standard CLIP variants; do not outperform cross-attention models like XVLM; far worse than humans.",
            "uuid": "e4871.5",
            "source_info": {
                "paper_title": "What’s “up” with vision-language models? Investigating their struggle with spatial reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CLIP (prompt/prompting & priors experiments)",
            "name_full": "Experiments: renormalizing caption priors, prompt rewriting (e.g., 'behind' -&gt; 'in the background'), and hard-negative finetuning on CLIP",
            "brief_description": "Data-informed interventions applied to CLIP to test whether distributional/textual factors explain poor spatial performance: re-normalizing caption priors, replacing rare prepositions with more common words, finetuning on preposition-containing LAION subsets and using automated hard negatives.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CLIP ViT-B/32 with caption-prior renormalization, prompt rewriting, and finetuning variants",
            "model_description": "Same base CLIP model used with post-hoc probability renormalization by estimated text prior, alternative textual prompts (e.g., replacing 'behind' with 'in the background'), finetuning on curated LAION subsets containing prepositions, and finetuning with programmatically created hard negative captions (switched prepositions).",
            "puzzle_name": "Spatial reasoning benchmarks: What'sUp (tight 4-option sets), COCO-spatial, GQA-spatial",
            "puzzle_description": "Tasks require resolving fine-grained spatial prepositions where textual priors and distributional mismatch might bias predictions.",
            "mechanism_or_strategy": "1) Re-normalize caption probabilities by subtracting an estimated caption prior (text-only prior) to counter high-probability captions; 2) prompt rewriting to substitute rare prepositions with more common spatial-indicative words (case study: 'behind' vs 'in the background'); 3) finetuning on LAION subset filtered for prepositions and on LAION-4M with augmented hard negatives (switched prepositions).",
            "evidence_of_spatial_reasoning": "Prompt rewriting ('background' instead of 'behind') improved average accuracy on that subset from ~52% to ~67% for Open-CLIP variants, indicating some latent spatial information can be surfaced with better wording. Renormalization of caption priors yielded small, inconsistent gains. Finetuning on preposition-rich LAION-4M produced only marginal improvements; adding hard negatives caused extremely high training loss and failure to fit, suggesting models cannot easily learn those fine-grained distinctions under studied regimes.",
            "performance_metrics": "Prompt rewriting (CLIP ViT-B/32 & ViT-L/14): average performance on 'behind' vs 'background' case study increased from 52% to 67%. Renormalization had slight benefits on COCO/GQA but not on What'sUp overall. Finetuning on LAION-4M-prep gave modest increases on some splits (see Table 2 summarized results: + LAION-4M-prep avg ~42.2% vs base ~41.8%), but results were inconsistent and far below human performance.",
            "limitations_or_failure_cases": "Renormalization cannot fully correct for lack of grounded spatial knowledge. Prompt rewriting success depended on availability of frequently observed alternative phrasing in pretraining corpora (e.g., 'background' is unusually common). Finetuning with hard negatives produced prohibitively high training loss (models could not fit), demonstrating difficulty of learning prepositional disambiguation with the studied objectives and data regimes.",
            "comparison_baseline": "Prompt rewriting outperformed the original rare preposition wording in the case study; however, overall approaches did not yield consistent improvements to reach human-level performance. Hard-negative finetuning failed while naive finetuning yielded only modest gains in COCO/GQA but sometimes hurt performance on the controlled What'sUp dataset.",
            "uuid": "e4871.6",
            "source_info": {
                "paper_title": "What’s “up” with vision-language models? Investigating their struggle with spatial reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2,
            "sanitized_title": "learning_transferable_visual_models_from_natural_language_supervision"
        },
        {
            "paper_title": "BLIP: Bootstrapping language-image pretraining for unified vision-language understanding and generation",
            "rating": 2,
            "sanitized_title": "blip_bootstrapping_languageimage_pretraining_for_unified_visionlanguage_understanding_and_generation"
        },
        {
            "paper_title": "BLIP-2: bootstrapping language-image pretraining with frozen image encoders and large language models",
            "rating": 2,
            "sanitized_title": "blip2_bootstrapping_languageimage_pretraining_with_frozen_image_encoders_and_large_language_models"
        },
        {
            "paper_title": "CoCa: Contrastive Captioners are image-text foundation models",
            "rating": 2,
            "sanitized_title": "coca_contrastive_captioners_are_imagetext_foundation_models"
        },
        {
            "paper_title": "Multigrained vision language pre-training: Aligning texts with visual concepts (XVLM)",
            "rating": 2,
            "sanitized_title": "multigrained_vision_language_pretraining_aligning_texts_with_visual_concepts_xvlm"
        },
        {
            "paper_title": "LAION-5b: An open large-scale dataset for training next generation image-text models",
            "rating": 2,
            "sanitized_title": "laion5b_an_open_largescale_dataset_for_training_next_generation_imagetext_models"
        },
        {
            "paper_title": "When and why vision-language models behave like bags-of-words, and what to do about it?",
            "rating": 1,
            "sanitized_title": "when_and_why_visionlanguage_models_behave_like_bagsofwords_and_what_to_do_about_it"
        },
        {
            "paper_title": "VALSE: A task-independent benchmark for vision and language models centered on linguistic phenomena",
            "rating": 1,
            "sanitized_title": "valse_a_taskindependent_benchmark_for_vision_and_language_models_centered_on_linguistic_phenomena"
        }
    ],
    "cost": 0.0160285,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>What's "up" with vision-language models? Investigating their struggle with spatial reasoning</p>
<p>Amita Kamath kamatha@cs.ucla.edu 
University of California
Los Angeles</p>
<p>Jack Hessel jackh@allenai.org 
Allen Institute for AI</p>
<p>Kai-Wei Chang kwchang@cs.ucla.edu 
University of California
Los Angeles</p>
<p>What's "up" with vision-language models? Investigating their struggle with spatial reasoning
2B353EE845F94BE9041300DCEAA683C5
Recent vision-language (VL) models are powerful, but can they reliably distinguish "right" from "left"?We curate three new corpora to quantify model comprehension of such basic spatial relations.These tests isolate spatial reasoning more precisely than existing datasets like VQAv2, e.g., our What'sUp benchmark contains sets of photographs varying only the spatial relations of objects, keeping their identity fixed (see Figure1: models must comprehend not only the usual case of a dog under a table, but also, the same dog on top of the same table).We evaluate 18 VL models, finding that all perform poorly, e.g., BLIP finetuned on VQAv2, which nears human parity on VQAv2, achieves 56% accuracy on our benchmarks vs. humans at 99%.We conclude by studying causes of this surprising behavior, finding: 1) that popular vision-language pretraining corpora like LAION-2B contain little reliable data for learning spatial relationships; and 2) that basic modeling interventions like up-weighting preposition-containing instances or fine-tuning on our corpora are not sufficient to address the challenges our benchmarks pose.We are hopeful that these corpora will facilitate further research, and we release our data and code at https://github.com/amitakamath/whatsup_vlms.</p>
<p>Introduction</p>
<p>Pre-trained vision-language models perform well on complex tasks such as VQAv2 (Goyal et al., 2016) and Nocaps (Agrawal et al., 2019), even in the zero-shot setting (Li et al., 2023).However, recent work has re-surfaced a concern that has long plagued vision-language models (Yatskar et al., 2016;Johnson et al., 2017): new multimodal models still exhibit poor behavior on simple tasks like attribute attachment, counting, etc. (Yamada et al., 2022;Thrush et al., 2022;Yuksekgonul et al., 2023;Parcalabescu et al., 2021).Despite improvements, models still fail to reliably capture even  basic spatial factors of images, a prerequisite for more precise and complex reasoning benchmarks.</p>
<p>But why?In this work, we study vision-language models' performance on basic spatial relations, such as "left of" and "right of".Existing benchmarks which aim to operationalize spatial understanding such as VQAv2 and GQA (Hudson and Manning, 2019) often conflate the evaluation of spatial reasoning with other types of reasoning, such as in the GQA question "Is there a woman to the left of the person that is wearing a wetsuit?".</p>
<p>Hence, we first curate COCO-spatial and GQA-spatial based on the COCO (Lin et al., 2014) and GQA datasets respectively, to isolate and assess more strictly only basic spatial relations.In addition, we collect a third evaluation corpus, What'sUp, with even tighter controls.The images within COCO and GQA often contain many objects/relations, and exhibit biases that reflect our usual world (e.g., a mug is usually on a table, not under it).We manually capture controlled photographs of household objects in various positions: e.g., to overcome the social bias of dogs being photographed under tables, we (carefully, gently, and with many treats) placed a dog on a table and took a picture of her (see Figure 1).What'sUp consists of 205 sets of four images each, resulting in 820 images in total.Each set of images varies the underlying preposition that describes the relationship between two objects, e.g., one set of images contains a mug on, under, left of, and right of a table.Furthermore, background objects are minimized, so there is no ambiguity.</p>
<p>For all three datasets, our setup is as follows: for a given image, the model is given a correct caption and 1 or 3 distractor captions, which differ only by a preposition: it must select the correct one.We evaluate 18 popular vision-language models, covering various architectures (e.g., one-stack vs. two-stack), training objectives (e.g., generative vs. contrastive models), and training data.All models perform poorly across benchmarks, with many performing just a few points above random chance and all models falling far behind human performance.</p>
<p>Next, we investigate why these models fail to learn much about spatial relationships.All models we consider are pre-trained on large-scale imagecaption corpora.We perform a corpus study of the LAION-2B dataset (Schuhmann et al., 2022), which was used to train OpenCLIP (Ilharco et al., 2021).We see that (1) common spatial prepositions occur in less than 0.2% of the training data; (2) when they do occur, they can be ambiguous or extraneous to the image, e.g., "left" defined from the viewer's perspective vs the subject's; and (3) they can often be guessed without looking at the image, e.g., "a house above water".</p>
<p>We consider several modeling improvements based on these findings, including: (1) renormalizing model probabilities to account for the implicit text-only prior of captions in LAION-2B;</p>
<p>(2) replacing the preposition "behind" with one more frequent in the training data, "in the background", as a case study to investigate if models may indeed "understand" spatial relationships (but are not surfacing that knowledge due to distribution mismatches); and (3) finetuning on several different relevant training sets (e.g., COCO-spatial/GQAspatial training sets, preposition-containing subsets of LAION-2B, and auto-generated hard negatives with switched prepositions).None of these approaches dramatically improves model performance on understanding spatial relations.</p>
<p>In summary, our contributions are: (1) three new benchmarks evaluating spatial relations in vision-language models, alongside results of 18 VL models on them; (2) a study of the training data of some of these models, with observations that could explain poor model performance on the benchmarks; and (3) a study of various methods to improve model performance, with insights that could guide future research in overcoming this issue.We release code and data to encourage the same at https://github.com/amitakamath/whatsup_vlms.</p>
<p>Benchmarks</p>
<p>Existing benchmarks include spatial reasoning questions, such as VQAv2 (Goyal et al., 2016) and GQA (Hudson and Manning, 2019).However, instances in these corpora often conflate several types of reasoning: in GQA, over 92% of the validation questions do so.For example, the GQA question "Are there men to the left of the person that is holding the umbrella?"conflates evaluation of spatial reasoning, object relationships, and object detection -in contrast, our questions require only spatial reasoning about one or two objects.</p>
<p>Our three new evaluation corpora are presented in the same format: an image paired with several captions which differ only by a preposition.What'sUp consists of tightly controlled photographs we captured ourselves, whereas COCOspatial and GQA-spatial are curated from wellrecognized image datasets.One key contribution is that all instances in all of our corpora require only spatial reasoning about one or two objects, e.g., in What'sUp, we circumvent the part-andwhole problem discussed in Yamada et al. (2022) by careful construction.</p>
<p>Figure 2 contains examples of images from each of our three benchmarks, along with the caption options each image is paired with.</p>
<p>Collection and statistics</p>
<p>What'sUp We captured 820 images of pairs of household objects in unambiguous spatial relation to each other.408 of these (Subset A) contain an object on, under, left of, or right of a table, chair or armchair.The other 412 (Subset B) contain an object in front of, behind, left of or right of another object on a black tabletop.For a given object pair, each preposition is represented; thus each subset of What'sUp has equal representation of each preposition.These images were captured with a tripod, with minimal changes between images in terms of position and lighting, except for the placement of the objects.This allows the benefit of real-world images, while exhibiting the controlled nature of synthetic images.This control has several advantages:</p>
<p>(1) we are able to evaluate model performance on pairs or sets of images, as described in §2.2; (2) we overcome textual biases that could falsely improve model performance, e.g.always guessing that the mug is on the table based on training priors; and (3) we are able to run specialized experiments studying model representations such as in §2.4.The primary differences between the two subsets are: (1) in Subset B, the two objects are closer in size than in Subset A; and (2) in Subset B, there is no obvious prior on the spatial relationship between the two objects, whereas in Subset A, e.g., a mug would usually go on a table.</p>
<p>COCO-spatial</p>
<p>We created a benchmark from the validation set of COCO (Lin et al., 2014) using detection annotation data.We select images with only one instance of each object mentioned in the text input, where the area of each is at least 3% the area of the image.Unlike in What'sUp, these images contain objects that may embody multiple spatial relations, e.g., an object that is both to the top of and to the left of another object.Thus, we provide only caption options that are mutually ex-clusive (to the left of vs to the right of, above vs below).Similarly for one-object images, we only test for mutually exclusive spatial relations (on the left vs on the right, on the top vs on the bottom).This benchmark contains 2687 images, with two caption options each.</p>
<p>GQA-spatial</p>
<p>We isolated questions targeting basic spatial relations from the GQA validation dataset (Hudson and Manning, 2019), which is sourced from Visual Genome (Krishna et al., 2016).The questions we isolate are of the form "Is the object on the preposition of the image?" or "Is the object 1 to the preposition of object 2 ?", when the object(s) mentioned are all present in the image, to avoid conflation with object detection.We retain attribute-object pairs (e.g., "white car") only if the attribute does not affect the answer (e.g., there is only one car in the image), to avoid conflation with attribute detection.Similar to COCO-spatial, we select images where the area of each object in the question is at least 3% of the image.We manually filtered out noisy images, e.g., those with multiple instances of objects in the question with different spatial relations.Finally, we convert these questions to a templated caption format.This benchmark contains 1451 images, with two caption options each, due to the same ambiguity as in COCOspatial of objects having multiple spatial relations.</p>
<p>Evaluation</p>
<p>Task.For all three benchmarks, the input is an image paired with several caption options that differ only by the preposition they contain.The model must select the caption with the correct preposition.As shown in Figure 2, for What'sUp, there are four caption options; for COCO-spatial and GQA-spatial, there are two.</p>
<p>Metric.The primary metric we use is the percentage of images for which the image-text matching score is highest for the correct caption compared to the incorrect caption(s).The controlled and balanced structure of What'sUp enables two additional metrics for that corpus: pair-wise and set-wise accuracy.Pair-wise accuracy is the accuracy on pairs of images that contain opposing prepositions.For example, if the model guesses correctly for "mug on table" and "mug under table", it gets one point.Set-wise accuracy is similar, but is awarded only when all four prepositions for a given object pair are guessed correctly.</p>
<p>Human estimated performance.We also estimate human performance on our three benchmarks.We sample 100 data points from each benchmark and, to ensure quality of the annotations, invite experts to voluntarily annotate the data.The annotators have all taken at least one graduate course in NLP.They are asked to determine whether the correct caption is an obvious choice, or if there is any scope for ambiguity.This estimate of human performance is 97.3% on COCO-spatial, 99% on GQA-spatial, and 100% on What'sUp.</p>
<p>Models.The models we study in the zero-shot setting are: CLIP (Radford et al., 2021) ViT-B/32 and ViT-L/14; a version of CLIP ViT-B/32 that has been finetuned on word order shuffling data called negCLIP (Yuksekgonul et al., 2023); a version of CLIP ViT-B/32 that has been initialized with RoBERTa-pretrained weights (Ilharco et al., 2021); CoCa, a model trained with generative and contrastive objectives (Yu et al., 2022); XVLM (Zeng et al., 2022) with 4M and 16M parameters; BLIP (Li et al., 2022) with 14M and 129M parameters; BLIP2 (Li et al., 2023) image-text matching head (ITM) and image-text contrastive learning head (ITC); and FLAVA (Singh et al., 2022).These models span various modeling choices: one-and two-stack models, generative and contrastive training objectives, different training data, etc.</p>
<p>We also study several models that have been finetuned on downstream tasks: CoCa which has been finetuned on COCO captioning; two versions of XVLM-16M that have been respectively finetuned on Flickr30K retrieval and COCO retrieval; and three versions of BLIP-14M that have been respectively finetuned on Flickr30K retrieval, COCO retrieval, and VQAv2.Almost all of these models are capable of yielding a score representing how well a given caption matches a given image.We use this score to evaluate whether the model "selects" the correct caption from the given options for an image.As BLIP-VQA and BLIP2-ITC have a text generation head rather than a scoring head, we phrase the input as a set of questions, e.g."Is the mug on the table?","Is the mug under the table?",etc, and evaluate the model by measuring the probability of the responses "yes" and "no": if the probability of "yes" is highest for the gold option (or "no" is lowest for the gold option if all option responses are "no"), we award a point.Table 1: Results of varied VL models on our benchmarks: models in the first section are evaluated zeroshot, and models in the second section have been finetuned on some downstream task: COCO captioning, retrieval on Flickr30K or COCO, or VQA.All models perform poorly on basic spatial relations.</p>
<p>Results</p>
<p>The performance of the models on our benchmarks is listed in Table 1.All models fall far behind human-estimated performance, with many models scoring within a few points of random chance.The number of models we evaluate allows us to draw inferences about various aspects of model design and training, as discussed below.</p>
<p>Model architecture.XVLM and BLIP2 perform better than other models in the zero-shot setting, hinting that the increased expressiveness of onestack, cross-attention models vs the two-stack models may indeed matter in this case.</p>
<p>Model size in parameters.Scaling up model size does not necessarily improve spatial reasoning capabilities.In the case of XVLM, the 16M model outperforms the 4M model; however, CLIP ViT-B/32 outperforms CLIP ViT-L/14 and BLIP 14M outperforms BLIP 129M averaged across our three benchmarks.</p>
<p>Training objective.Despite helping on other zero-shot tasks such as ImageNet-1K (Deng et al., 2009;Yu et al., 2022), the generative training objective does not seem to encourage spatial reasoning abilities more than a contrastive objective: CoCa scores less than CLIP ViT-B/32, and BLIP2-ITC scores less than BLIP2-ITM.</p>
<p>Supervision.XVLM is the highest-performing model of those we evaluate, likely due to its more fine-grained supervision at the bounding-box level in addition to the image-level.</p>
<p>Finetuning.Finetuning on downstream tasks appears to improve model performance sometimes, e.g.BLIP-VQA outperforms BLIP significantly, but not always, e.g.CoCa-Captioning underperforms CoCa.</p>
<p>Pair/Set and One-object/Two-object accuracy.</p>
<p>Detailed results including pair and set accuracy for What'sUp, and one-and two-object accuracy for COCO-spatial and GQA-spatial are presented in Appendix Table 3.All models show very poor pair and set accuracy, showing their lack of understanding of the concept of each preposition.</p>
<p>There does not seem to be a uniform trend of model performance on one-object images vs two-object images.</p>
<p>Inspection of the failure cases shows some models always predicting 1-2 prepositions for all inputs, and others predicting seemingly randomly.Overall, our data allows a very precise evaluation of spatial reasoning, revealing that these models exhibit a failure to understand basic spatial relations, despite nearing human performance on VQAv2, as in the case of BLIP-VQA.</p>
<p>Visual analogies</p>
<p>Next, we study the representations of CLIP models on the What'sUp Benchmark.The models are able to get some examples correct (e.g."dog on a table", "dog under a table"), but as they are not able to get higher performance, particularly on the pair and set metrics, it hints that they are not learning the generalizable concept of "under" or other spatial relations.To study whether the representations encode these concepts in a generalizable manner, we study whether the image representations of these images exhibit the same linear analogies as studied in NLP (king − man + woman = queen) (Mikolov et al., 2013).We study only CLIP variants in this setting, as they alone of the models we study are trained in a manner to encourage linear recoverability.Specifically, we evaluate CLIP ViT-B/32, ViT-L/14, NegCLIP and RoBERTaCLIP.Prepositions.We select 25 sets of 4 from What'sUp Subset A: specifically, images where objects are placed around a table.We now evaluate whether I(mug on table) − I(mug under table) + I(bowl under table) is the closest to I(bowl on table), compared to I(bowl left/right/under table), where I(•) is the image representation.Given 25 objects and 4 preposition options, there are 7200 such analogies.We measure the percentage of these where our condition holds.On average, the four CLIP-based models we study achieve an analogy accuracy of only 9%.The average performance of the models when directly evaluated on the images according to our usual accuracy metric is 31%.</p>
<p>Colors.As a control test for our setup, we next study whether these linear analogies appear in the representation of various colors, which CLIP has been shown to generalize to very well (e.g., correctly identifying a blue cow).We isolate 25 objects from the What'sUp Benchmark, and edit the images to attribute one of four different colors to the object: red, yellow, green or blue, as in Figure 3.We now evaluate whether I(red mug) − I(yellow mug) + I(yellow bowl) is the closest to I(red bowl), compared to I(yellow/green/blue bowl), where I(•) is the image representation.Here, again, we have 7200 analogies and measure the percentage of times the condition holds.On average, the four CLIP-based models we study achieve an accuracy of 61%1 -much higher than for prepositions.They also achieve 100% accuracy when directly evaluated on the color options in the same format as our basic evaluation (given one image and four caption options with different colors, select the correct caption).These experiments suggest that models appear to learn the concept of color attachments more effectively than spatial relations.</p>
<p>Prepositions occur rarely.We find that captions in the corpus contain common spatially specific prepositions like "under" or "left of" only 0.2% of the time (we additionally filter spatial prepositions that are used in non-spatial contexts, e.g., "under $25").The individual frequency of each preposition is given in Appendix Table 4.</p>
<p>There are several reasons why this may be the case: alt-text authors may choose not to specify prepositions they feel are obvious (e.g., a house "above" the water) or ambiguous (e.g., "left" from the viewer's perspective, or from the subject of the image's perspective?); the preposition may not be important in the writer's eyes when trying to capture holistic information about the entire image in a short caption (e.g., "a cluttered kitchen", rather than "a fork to the left of a knife on a kitchen counter"); the writer may choose more casual language (e.g., "next to" rather than "to the left of").See Berg et al. (2012) for a discussion of how descriptions manifest according to similar factors in crowdsourced image captioning corpora.</p>
<p>Prepositions can be ambiguous.Of the spatial prepositions that do occur in LAION, examination of the associated images reveals ambiguity.For example, the frame of reference could be defined from the perspective of the viewer of the photo, or of the subject of the photo -in our benchmarks, we follow the same convention as CLEVR (Johnson et al., 2017), i.e., the perspective of the viewer; however, image-text pairs in LAION are scraped from the internet, and thus follow no single convention.As another example, "in front of" could mean closer to the viewer of the photo, or ahead of a subject that is facing in a certain direction in the photo.Even the same preposition with the same meaning could have very different visual appearances, e.g."a ball under the desk" vs "a ball under the water".</p>
<p>A few examples are discussed in Figure 4.  Prepositions are rarely needed to satisfy the contrastive learning objective.CLIP and similar models trained contrastively rely on a large batch size to obtain negative examples that require more precise visual representations.For example, the model learns a visual representation of "Bernese Mountain Dog" rather than just "dog", as there could be several types of dogs in the 32K batch.However, this is not the case for prepositions.Given the combinatorial space of all possible sentences, it is unlikely that the exact same description would apply to two images in a batch with the exception of a specific preposition.Furthermore, some preposition-object combinations are much more common, e.g., "dog under table" vs. "dog on table".Thus, we hypothesize that the model can perform well on the contrastive training objective despite ignoring spatial relationships between objects in the image.</p>
<p>Data-informed attempts at improvement</p>
<p>In this section, we operationalize our hypotheses detailed above to yield potential solutions to models' struggle with learning spatial relations.</p>
<p>Incorporating Caption Priors</p>
<p>The first method we consider is a re-normalization of probabilities.Intuitively, some captions are more likely on average across all images.We estimate the prior for a caption by calculating its average dot product with a large set of images from a different source to avoid test set contamination (e.g.COCO to estimate priors of a VG caption).We then use that prior to re-normalize the caption probability for a given image.Specifically, we compute a re-normalized caption probability as the difference between the un-normalized probability and the caption's calculated prior.This process is similar to the text-only normalization of Holtzman et al. (2021).This normalization encodes that P (caption|image) should not depend on P (caption).</p>
<p>Tables 5 and 6 in the Appendix contain the results of models with and without considering caption priors from different datasets.Overall, it seems that normalizing by caption priors does not tend to improve performance on What'sUp much (although a slight improvement is observed in pair and set accuracies).The priors are slightly helpful for performance on COCO-spatial and GQAspatial, likely because those two image distributions are closer to each other than either is to What'sUp.However, overall, this approach did not drastically improve model performance on any of the benchmarks.Thus, poor performance of vision-language models cannot be attributed entirely to difficult-to-overcome text-only priors on correct options of the captions we evaluate.</p>
<p>Better prompts: don't fall (for) "behind"</p>
<p>From our study of the LAION-2B dataset, we see one word that is not a basic spatial preposition, but gives information about spatial relations, and has relatively high prevalence in the data: "background".This word alone appears in 0.84% of the captions, four times more than all of the other prepositions we study combined.Many of these captions describe synthetic images (e.g., "the words happy new year on a red background"), but others provide spatial information (e.g., "two people talking with some flowers in the background").The most similar preposition we evaluate is "behind", in What'sUp Subset B.</p>
<p>To determine whether models understand the concept of "behind" (but this knowledge may not be accessible by using that particular word), we do a case study of whether models trained on LAION perform better when given a prompt of "background" or "behind".We take the "in front of" and "behind" images from What'sUp Subset B (disregarding the "left of" and "right of" images), changing the text input options to (1) "object 1 behind object 2 " and "object 2 behind object 1 ", or (2) "object 2 with object 1 in the background" and "object 1 with object 2 in the background".This allows us to evaluate only performance on "behind" vs "background" without conflating other factors such as performance on other prepositions.For CLIP ViT-B/32 and CLIP ViT-L/14 (both Open-CLIP versions trained on LAION), performance on ( 1) is an average of 52%, just two points above random chance, whereas performance on ( 2) is an average of 67%.</p>
<p>Discussion.This is a significant jump, and shows that spatial information may indeed be present in these models, but may have to be teased out more carefully.A strong caveat to these results is that the word "background" seems to be a special case: we are able to run this experiment because it appears very frequently in LAION, but we did not come across any other such words that appear frequently and provide spatial understanding.Thus, while this is an interesting thought experiment and provides hope that with more data, the issue can be mitigated, we do not believe it is the solution for models' poor performance on all spatial reasoning tasks.</p>
<p>Finetuning</p>
<p>Finally, we run several experiments with finetuning.Ideally, models should be able to understand basic spatial relations without finetuning, especially as finetuning tends to lose some benefits from pretraining and is tedious and expensive to do for various downstream tasks.However, we experiment with some finetuning settings with CLIP ViT-B/32 to determine whether spatial reasoning can be easily learned by our models with extra training.The results are presented in Table 2.</p>
<p>Finetuning on the train equivalents of COCOspatial and GQA-spatial.We repeat the automated process to curate spatial relations data from GQA and COCO on the training set (rather than the validation set, which was used to create the benchmarks), dropping the filter for the objects to be at least 3% the area of the image, and dropping the human quality filter.We also combine an equal weight of COCO captions, so the model does not Finetuning on a subset of LAION including prepositions.We next isolate a subset of LAION including the prepositions we evaluate across our benchmarks.After filtering noise, this subset contains 4M image-text pairs.When finetuned on this data, performance improvements are marginal.The reasons for this could be as discussed in Section 3prepositions in LAION are ambiguous and rarely required to identify the image, even from a large batch (we finetune with a batch size of 2048 across 4 NVIDIA RTX A6000 GPUs).</p>
<p>Finetuning on LAION-4M with hard negative captions.</p>
<p>Related work</p>
<p>Spatial reasoning has long been evaluated by visionlanguage benchmarks: VQAv2 (Goyal et al., 2016), GQA (Hudson and Manning, 2019), NLVR2 (Suhr et al., 2018), CLEVR (Johnson et al., 2017) and ShapeWorld (Kuhnle and Copestake, 2017) all contain questions requiring spatial reasoning.However, many of these questions conflate several types of reasoning.Performance on these benchmarks therefore masks VL models' struggle with spatial understanding specifically.More recently, vision-language benchmarks evaluating more specific phenomena have been proposed, testing understanding of word order (Thrush et al., 2022;Yuksekgonul et al., 2023), counting (Parcalabescu et al., 2021), object-attribute association (Yamada et al., 2022), and compositionality (Kamath et al., 2023;Ma et al., 2022).Other work including VALSE (Parcalabescu et al., 2022), VSR (Liu et al., 2023), VL-Checklist (Zhao et al., 2023) and ReCLIP (Subramanian et al., 2022) evaluate spatial reasoning in isolation, as we do in our three corpora, testing VL models' ability to match an image to the more fitting of two captions where only the spatial preposition is flipped.They show that models have room for improvement in both zero-shot and finetuned settings.</p>
<p>However, all of the non-synthetic benchmarks above testing spatial reasoning are based on COCO (Lin et al., 2014) or Visual Genome (Krishna et al., 2016), which are sourced from Flickr.These images tend to have many objects, usually in cluttered environments, which can confuse models trained with only image-level supervision (Yamada et al., 2022).The images also reflect biases in our usual world, such as mugs usually being on tables and not under them3 .Models may learn these priors and attain high scores on these benchmarks without actually attending to the images (Hsieh et al., 2023)e.g., text-only GPT-1 (Radford et al., 2018) scores 27 accuracy points above random chance on spatial reasoning questions in VALSE.In contrast, we capture sets of photographs for What'sUp which are uncluttered, unambiguous, and contain all four preposition options for any pair of objects -thus exposing any bias models may have for the "usual" relation between two objects, as well as preventing models with such a bias from leveraging it to mask their spatial understanding abilities.</p>
<p>Text-to-image generation has also been shown to struggle with correctly depicting spatial relations (Gokhale et al., 2022;Hu et al., 2023).Our work sheds light on why this could be the case: e.g., DALL-E 2 (Ramesh et al., 2022) uses a frozen CLIP backbone, and as we show in our work, CLIP itself struggles with spatial reasoning.</p>
<p>Conclusion</p>
<p>In this work, we propose three new benchmarks: What'sUp, COCO-spatial and GQA-spatial, to evaluate VL models on basic spatial relations in a range of environments, with the controlled nature of What'sUp allowing us to evaluate pairs and sets of prepositions for a given object pair.We observe that all 18 models we evaluate perform poorly on these benchmarks in a zero-shot fashion.Next, we study the LAION dataset which was used to train OpenCLIP, revealing that prepositions are rare, ambiguous, and extraneous in the captions.Finally, we explore potential remedies, ultimately finding that CLIP models, at least in the regime of scale we consider, fail to even fit a large-scale training set that requires precise spatial reasoning.</p>
<p>How might models solve our newly proposed evaluations going forward?Three promising future directions include: (1) Auto-generation of hard negatives for spatial prepositions (and beyond) during pre-training; (2) Consideration of more expressive fine-tuned models that support image-text crossattention and mixes of contrastive and generation objectives; and (3) Thorough scaling experiments to probe for potentially promising relationships between increasing compute of vision-language models vs. performance on our benchmarks.</p>
<p>Limitations</p>
<p>First, the benchmarks we propose, especially What'sUp, are restricted in scale compared to benchmarks like ARO (Yuksekgonul et al., 2023) and GQA (Hudson and Manning, 2019).Second, our paper focuses on investigating how and why vision-language models struggle with basic spatial relations: our methods to improve models, while grounded in observations from our investigation, do not improve model performance significantly on all of our benchmarks.Third, our work is restricted to spatial reasoning.It would be interesting to perform a wide-scale study tackling several types of reasoning.</p>
<p>A Appendix</p>
<p>This section contains additional results.Table 3 contains detailed results of VL models on our three proposed benchmarks.Table 4 breaks down the prevalence of various prepositions in the LAION-2B dataset, before and after removing noisy prepositions such as "under $25" -to emphasize that a direct count of word occurrence is not sufficient to understand the low prevalence of spatial relations in LAION captions.Tables 5 and 6 contain results of the experiments targeting re-normalization of caption priors.Table 7 contains detailed results of different types of finetuning on our three benchmarks.Figures 5 and 6 contain loss curves from finetuning with and without hard negative captions targeting prepositions -the train loss from the latter is about 500x smaller than the former, and the loss on the gold caption and hard negative caption is about the same, showing that the model struggles to disambiguate between the correct caption and the hard distractor, amongst the entire batch.</p>
<p>Figure 1 :
1
Figure 1: We propose three tightly controlled benchmarks to assess model capacity for fine-grained spatial reasoning, showing that popular vision-language models fall far behind human performance when asked to select the correct spatial relation between two objects in an image (real examples shown).</p>
<p>A</p>
<p>mug behind a plate A mug to the left of a plate A mug to the right of a plate A mug in front of a plate A mug behind a plate A mug to the left of a plate A mug to the right of a plate A mug in front of a plate A mug behind a plate A mug to the left of a plate A mug to the right of a plate A mug in front of a plate A mug behind a plate A mug to the left of a plate A mug to the right of a plate A boy to the left of a racket A boy to the right of a racket What'sUp (Subset A) What'sUp (Subset B) COCO-spatial A dog to the left of a bench A dog to the right of a bench GQA-spatial A person on the left A person on the right An umpire on the left An umpire on the right</p>
<p>Figure 2 :
2
Figure 2: Examples from our three proposed benchmarks.Each image is paired with four text options in What'sUp and two text options in COCO-spatial and GQA-spatial.Given a single image and the corresponding text options, a VL model must select the correct option.</p>
<p>Figure 3 :
3
Figure 3: Example of edited images with four colors.</p>
<p>this startrail.Only managing approx 5hrs of darkness because of the long days.Taken between 1030pm and sunrise following day.May 31 2009 in Sth Leics, UK.Love the opposite curvature of the trails above and below the celestial equator.Olympus E3, 7-14mm lens.Just over 1000 exposures stacked in startrails.The celestial equator is not obvious in this image, and thus the description of trails above and below it does not provide much information.Maury Determined That Was a Lie you said the next bus/train was coming up right behind you the half an hour wait determined that was a lie , made with livememe meme creatorThe caption is a transcription of the text overlaid on the image; the image does not contain a bus or train at all.Learning objects.Fabric with sewing item and accesories which are required to learn to sew on wooden table background.Directly above and copy space.Unclear what the preposition refers to.</p>
<p>Figure 4 :
4
Figure 4: Examples of ambiguity in spatial prepositions used in LAION captions, alongside discussions thereof.</p>
<p>Table 2 :
2
Results of different types of finetuning on CLIP ViT-B/32.Even with finetuning, the results do not increase by a large margin across all benchmarks.forget standard English.This gives us 900,000 data points, which we downsample to 300,000 for compute reasons.When we finetune on this data, we see the model improves on COCO-spatial and GQA-spatial by an average of 14.6 accuracy points.But performance drops on What'sUp by 4.3 accuracy points.Plausible explanations include the image distributions being different, and that the What'sUp data contains unusual placements of objects.Also, even with significant supervised indistribution data, performance on COCO-spatial and GQA-spatial still lag significantly behind human performance (by ∼50 accuracy points).
ModelWhats-UpCOCO-spatialGQA-spatialAvgCLIP ViT-B/3231.047.446.941.8+ train COCO-spatial and GQA-spatial26.763.959.550.0+ LAION-4M-prep33.146.047.642.2+ LAION-4M-prep with neg. cap.29.344.446.540.1Random / Text-only25.050.050.041.7Human Estimate100.097.399.098.8</p>
<p>We additionally track how the model allocates its probability across the batch: loss on the positive caption is similar to the loss on the negative caption, which suggests that CLIP is able to narrow text options down to those two captions, but cannot consistently learn which is correct of the two.Experiments with ViT-B/32, ViT-B/16 and ViT-L/14 all show this pattern when finetuned on both 50% and 100% of the data, implying that, at least for the training regime we consider, scaling the data or model size does not help.It is likely that an inductive bias or denser supervision is needed to enable the model to learn this, as in XVLM.The train loss curves are provided in the Appendix.
Taking inspiration from Yuksekgonulet al. (2023), we add hard negative captions to theLAION-4M subset we curate, by programmaticallyswitching the preposition with its opposite. Thisensures that the model is forced to distinguish be-tween the two in order to meet the training objec-tive. For CLIP ViT-B/32, we observe a very hightraining loss, suggesting that the model cannot fit this augmented corpus. 2
The linear analogy accuracy is not very high, but this is perhaps not too surprising given that even performing JPEG compression before encoding changes the image representation significantly for CLIP (see https://github.com/ allenai/mmc4/issues/12).
All models we consider in Section 2.2 utilize largescale image-caption corpora for pretraining. Here, we investigate one popular such corpus, LAION-2B(Schuhmann et al., 2022), to better understand why spatial relations might not be learned by models when trained on this type of data. LAION was also used to train OpenCLIP(Ilharco et al., 2021).
Across several hyperparameter settings, we consistently observed loss of 5.0, compared to the loss of 0.01 for the same configuration without the very hard negatives.
As of the time of this writing, even querying Google Image Search with "a mug under/left of/right of a table" did not yield any accurate images.
AcknowledgementsWe thank John Hewitt, Akhila Yerukola, Liunian Harold Li, and the anonymous reviewers for helpful discussion and feedback, as well as Travis McGuire and Emily Chua for helping us take photographs of Lucy, the star of Figure1.This work was funded by the Allen Institute for AI.AK was additionally supported by the UCLA Computer Science Department First-Year Fellowship.KC was supported in part by DARPA MCS under contract number N660011924032, ONR N00014-23-1-2780, and a Sloan Fellowship.The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, or the U.S. Government.Table5: Summarized results of the experiments incorporating caption priors.For each model we have shown the best performance from different methods of calculating caption priors.Incorporating the low caption priors improves performance in some cases, but not by a large margin overall -in many cases, even with improvement the model still performs below random chance.Detailed results are shown in Table6.
Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, Peter Anderson, nocaps: novel object captioning at scale. International Conference on Computer Vision. 2019</p>
<p>Understanding and predicting importance in images. Alexander C Berg, Tamara L Berg, Hal Daumé, Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch, Margaret Mitchell, Aneesh Sood, Karl Stratos, Kota Yamaguchi, 10.1109/CVPR.2012.62481002012 IEEE Conference on Computer Vision and Pattern Recognition. 2012</p>
<p>Imagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 10.1109/CVPR.2009.52068482009 IEEE Conference on Computer Vision and Pattern Recognition. 2009</p>
<p>Benchmarking spatial relationships in text-to-image generation. Tejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, Eric Horvitz, Ece Kamar, Chitta Baral, Yezhou Yang, arXiv:2212.100152022arXiv preprint</p>
<p>Making the v in vqa matter: Elevating the role of image understanding in visual question answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, International Journal of Computer Vision. 1272016</p>
<p>Surface form competition: Why the highest probability answer isn't always right. Ari Holtzman, Peter West, Vered Schwartz, Yejin Choi, Luke Zettlemoyer, Conference on Empirical Methods in Natural Language Processing. 2021</p>
<p>Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, Ranjay Krishna, Thirty-Seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Tifa: Accurate and interpretable text-toimage faithfulness evaluation with question answering. Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, Noah A Smith, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2023. 2023</p>
<p>GQA: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, CVPR. 2019</p>
<p>. Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, Ludwig Schmidt, 10.5281/zenodo.51437732021Openclip</p>
<p>CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, CVPR. 2017</p>
<p>Text encoders are performance bottlenecks in contrastive vision-language models. Amita Kamath, Jack Hessel, Kai-Wei Chang, EMNLP. 2023</p>
<p>Visual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael S Bernstein, Li Fei-Fei, International Journal of Computer Vision. 1232016</p>
<p>Shapeworld -a new test methodology for multimodal language understanding. Alexander Kuhnle, Ann Copestake, 2017</p>
<p>BLIP-2: bootstrapping language-image pretraining with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, ICML. 2023</p>
<p>BLIP: Bootstrapping language-image pretraining for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, Proceedings of the 39th International Conference on Machine Learning. the 39th International Conference on Machine LearningPMLR2022162</p>
<p>Microsoft COCO: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, Lawrence Zitnick, European conference on computer vision. Springer2014</p>
<p>Visual spatial reasoning. Fangyu Liu, Guy Edward Toh Emerson, Nigel Collier, 2023Transactions of the Association for Computational Linguistics</p>
<p>Crepe: Can vision-language foundation models reason com. Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, Ranjay Krishna, CVPR. 2022</p>
<p>Distributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S Corrado, Jeffrey Dean, 2013NeurIPS</p>
<p>VALSE: A task-independent benchmark for vision and language models centered on linguistic phenomena. Letitia Parcalabescu, Michele Cafagna, Lilitta Muradjan, Anette Frank, Iacer Calixto, Albert Gatt, 10.18653/v1/2022.acl-long.567Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Seeing past words: Testing the cross-modal capabilities of pretrained V&amp;L models on counting tasks. Letitia Parcalabescu, Albert Gatt, Anette Frank, Iacer Calixto, Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR). the 1st Workshop on Multimodal Semantic Representations (MMSR)Groningen, NetherlandsAssociation for Computational Linguistics2021</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International Conference on Machine Learning. PMLR2021</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen, arXiv:2204.06125Hierarchical textconditional image generation with CLIP latents. 2022arXiv preprint</p>
<p>LAION-5b: An open large-scale dataset for training next generation image-text models. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Ross Cade W Gordon, Mehdi Wightman, Theo Cherti, Aarush Coombes, Clayton Katta, Mitchell Mullis, Patrick Wortsman, Schramowski, Katherine Srivatsa R Kundurthy, Ludwig Crowson, Robert Schmidt, Jenia Kaczmarczyk, Jitsev, Thirtysixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2022</p>
<p>FLAVA: A foundational language and vision alignment model. Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, Douwe Kiela, CVPR. 2022</p>
<p>ReCLIP: A strong zero-shot baseline for referring expression comprehension. Sanjay Subramanian, William Merrill, Trevor Darrell, Matt Gardner, Sameer Singh, Anna Rohrbach, 10.18653/v1/2022.acl-long.357Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, Yoav Artzi, arXiv:1811.00491A corpus for reasoning about natural language grounded in photographs. 2018arXiv preprint</p>
<p>Winoground: Probing vision and language models for visio-linguistic compositionality. Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, Candace Ross, CVPR. 2022</p>
<p>Yutaro Yamada, Yingtian Tang, Ilker Yildirim, arXiv:2212.12043When are lemons purple? the concept association bias of clip. 2022arXiv preprint</p>
<p>Situation recognition: Visual semantic role labeling for image understanding. Mark Yatskar, Luke Zettlemoyer, Ali Farhadi, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016. 2016</p>
<p>Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, Yonghui Wu, Coca: Contrastive captioners are image-text foundation models. 2022</p>
<p>When and why vision-language models behave like bags-of-words, and what to do about it?. Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, James Zou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Multigrained vision language pre-training: Aligning texts with visual concepts. Yan Zeng, Xinsong Zhang, Hang Li, Proceedings of the 39th International Conference on Machine Learning. the 39th International Conference on Machine LearningPMLR2022Proceedings of Machine Learning Research</p>
<p>Vl-checklist: Evaluating pre-trained visionlanguage models with objects, attributes and relations. Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng Lu, Jianwei Yin, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
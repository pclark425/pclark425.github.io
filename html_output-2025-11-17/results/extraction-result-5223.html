<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5223 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5223</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5223</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-267751452</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.11641v2.pdf" target="_blank">Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Graph-structured data are the commonly used and have wide application scenarios in the real world. For these diverse applications, the vast variety of learning tasks, graph domains, and complex graph learning procedures present challenges for human experts when designing versatile graph learning approaches. Facing these challenges, large language models (LLMs) offer a potential solution due to the extensive knowledge and the human-like intelligence. This paper proposes a novel conceptual prototype for designing versatile graph learning methods with LLMs, with a particular focus on the"where"and"how"perspectives. From the"where"perspective, we summarize four key graph learning procedures, including task definition, graph data feature engineering, model selection and optimization, deployment and serving. We then explore the application scenarios of LLMs in these procedures across a wider spectrum. In the"how"perspective, we align the abilities of LLMs with the requirements of each procedure. Finally, we point out the promising directions that could better leverage the strength of LLMs towards versatile graph learning methods.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5223.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5223.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphText</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphText (Graph reasoning in text space)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that converts graph neighborhoods into textual descriptions (k-hop subgraph text) and uses closed-source LLMs (e.g., ChatGPT/GPT-4) in a few-shot manner to perform node-level prediction by prompting on the textualized subgraph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphtext: Graph reasoning in text space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>k-hop subgraph textualization / subgraph linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For a target (centric) node, extract its k-hop subgraph, serialize relevant node/edge attributes and connectivity into a natural-language description (text prompt) representing the local structure and features, then feed that text to an LLM in few-shot prompting to produce node label predictions or explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text-attributed graph / node-centric subgraphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Makes graph data compatible with LLMs (text modality); interpretable (natural language); compactness controlled by k; depends on quality of natural-language description; introduces potential information loss from graph→text conversion and sensitivity to how the subgraph is described (prompt formulation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification / node label prediction (few-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Qualitatively presented as an approach that enables zero/few-shot LLM prediction on textualized graph inputs; contrasted with native GNNs which operate on adjacency/features directly. Paper reports this line of work as able to produce explainable predictions and be used in few-shot setups, but provides no numeric head-to-head results in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance is sensitive to the prompt/textualization design and to how subgraphs are described; textualization may lose structural details (limits in faithfully encoding graph is noted); robustness and reliability concerns; not systematically compared across textualization strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5223.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5223.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT4Graph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical evaluation/benchmark that uses LLM self-prompting to produce textual descriptions of graphs and measures LLM competence on graph-structure reasoning tasks (e.g., node degree, diameter, clustering coefficient recognition).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>self-prompted graph description</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>LLMs are prompted (self-prompting) to describe graph structures or compute graph properties by converting graph structural information into natural-language queries or descriptions; these textual descriptions are then used for LLM-based reasoning about graph properties.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>generic graphs used for graph-structure reasoning (synthetic/benchmark graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Enables direct use of LLMs for structural reasoning tasks; representation fidelity depends on the expressiveness of the generated textual description and prompt; subject to variability introduced by description style.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph structural reasoning tasks: recognizing node degree, graph diameter, clustering coefficient and other structural statistics/labels.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Survey notes experiments like GPT4Graph and TLG that evaluate LLMs on graph-structure tasks; highlights that textual descriptions greatly impact LLM performance and that current textualization approaches may not fully capture structural information compared with graph-native methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Descriptions are sensitive to wording leading to reliability/robustness issues; textualization may not capture full structural expressivity; lack of theoretical validation (e.g., Weisfeiler–Lehman tests) to assess whether LLM-text encodings preserve structural distinctions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5223.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5223.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TLG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TLG (Textual-graph LLM approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that describes graphs, nodes and edges with natural language and applies large LMs (e.g., PaLM 62B) in zero-shot to predict graph properties or perform reasoning over graph data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-to-text descriptions for zero-shot LLM prediction</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Generate textual descriptions of graph structures (nodes, edges, and their relations) and formulate reasoning prompts so that an LLM like PaLM can answer queries or predict graph statistics in a zero-shot manner.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs / graph reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Leverages LLM's language understanding for zero-shot adaptation across tasks; representation relies on natural-language expressiveness and LLM pretraining knowledge; may be interpretable and human-readable.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Predicting graph statistics and semantic labels in zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Reported as part of the class of textualization approaches (alongside GraphText and GPT4Graph); survey notes that such LLM-as-predictor methods can be adapted across tasks easily but are influenced by prompt design and example selection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No numeric performance reported in survey; sensitive to prompt engineering, example selection; issues with robustness, reliability, and potential data leakage from LLM pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5223.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5223.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OFA / TAPE / GraphGPT (text augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OFA, TAPE, GraphGPT (LLM-based textual augmentation methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that use LLMs (GPT-3.5 / other LMs) to generate textual descriptions, explanations, or labels for nodes/subgraphs to augment graphs with textual attributes which are then encoded for downstream graph models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>LLM-generated node/edge/subgraph descriptions (textual augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use an LLM to generate natural language descriptions, explanations, or labels for nodes, edges or entire graphs (e.g., chemical functional group explanations, node descriptions), and attach those texts as attributes; optionally encode texts with Sentence-BERT / DeBERTa / other encoders for GNN consumption or jointly tune.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text-attributed graphs (text-augmented nodes/edges); molecular graphs in some applications</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Augments sparse or non-text features with rich natural-language descriptions; increases interpretability; allows leveraging pretrained text encoders; may improve downstream performance when text is informative.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node-level prediction, molecular property prediction, graph reasoning and explanation generation (varies by cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Survey describes three cooperation categories: GLA-centric (encode text then feed to GNNs), alignment-based (contrastive/alignment between graph and text embeddings), and LLM-centric (map graph structure into LLM prefix/tokens). Text-augmentation methods are reported to improve performance when combined appropriately but no numeric comparisons are provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quality of generated text governs usefulness; text-sparse graphs (e.g., traffic, power networks) are challenging to describe; potential mismatches between LLM-generated descriptions and structural signals; lack of systematic comparison between textual augmentation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5223.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5223.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Textual Feature Encoding (Sentence-BERT / RoBERTa / PaLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Textual Feature Encoding using pre-trained language encoders (Sentence-BERT, RoBERTa, PaLM, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard practice of encoding natural-language descriptions attached to graph elements into dense embeddings using pretrained transformers or sentence encoders, either frozen or co-trained with graph models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>text embedding of graph-attached text</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Feed LLM/LM-generated textual attributes into an encoder (Sentence-BERT, RoBERTa, PaLM, vanilla Transformer) to obtain vector representations which are used as node/edge features (frozen or fine-tuned), or used in cross-modal training/alignment with graph encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text-attributed graphs / structured-textual graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Leverages strong pretrained semantic encoders; produces dense, continuous features that are easy to integrate with GNNs/Transformers; encoding quality depends on encoder trained objective; can be frozen or co-trained.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used broadly in downstream tasks such as node classification, graph classification, link prediction, or as inputs to alignment and pretraining objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Survey notes methods using these encoders as components across GLA-centric and alignment-based approaches; alignment-based pretraining (contrastive) between graph encoders and text encoders is cited as beneficial but no numeric results are given in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Choice of encoder and co-training strategy affects performance; may not capture structural relations unless alignment objectives or co-training are applied; textual encoders can introduce bias or leakage from pretraining corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5223.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5223.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Structured-Textual Graph (G_ST) representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured-Textual Graph (G_ST)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation formalizing the combination of structured graphs (nodes, edges) and textual descriptions attached to nodes/edges/subgraphs, enabling multimodal graph learning by LLMs and GNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>structured-textual graph (combined graph+text modality)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Define G_S = (V,E) as the structured graph and G_T as textual descriptions; G_ST is the union where textual attributes V_T, E_T or graph-level text G_T are associated with graph elements. These textual attributes can be generated or augmented by LLMs and encoded (Sentence-BERT, DeBERTa) and then combined with graph neural methods via GLA-centric, alignment-based, or LLM-centric strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text-attributed graphs / multimodal graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Enables leveraging both structural and semantic textual signals; flexible integration patterns (encode-then-GNN, alignment, map-to-LLM); can improve explainability and transferability; complexity increases with multimodal fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used across node/edge/graph-level tasks, pretraining/transfer learning, and explainable prediction tasks described in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Survey categorizes three manners of combining modalities (GLA-centric, alignment-based, LLM-centric) and qualitatively contrasts their trade-offs: GLA-centric emphasizes graph learners with text features; alignment-based focuses on joint embedding/pretraining; LLM-centric maps graph info into LLM input/prefix. No quantitative ranking is provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Design choices (how to attach/encode text, alignment objectives, joint training) lack systematic comparison; for text-sparse domains it's unclear how to generate meaningful text; theoretical understanding of how textualization preserves graph structure is lacking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graphtext: Graph reasoning in text space <em>(Rating: 2)</em></li>
                <li>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. <em>(Rating: 2)</em></li>
                <li>Boosting graph reasoning ability of large language model <em>(Rating: 1)</em></li>
                <li>TLG <em>(Rating: 1)</em></li>
                <li>OFA <em>(Rating: 1)</em></li>
                <li>GraphGPT <em>(Rating: 1)</em></li>
                <li>TAPE <em>(Rating: 1)</em></li>
                <li>NLGraph <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5223",
    "paper_id": "paper-267751452",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "GraphText",
            "name_full": "GraphText (Graph reasoning in text space)",
            "brief_description": "A method that converts graph neighborhoods into textual descriptions (k-hop subgraph text) and uses closed-source LLMs (e.g., ChatGPT/GPT-4) in a few-shot manner to perform node-level prediction by prompting on the textualized subgraph.",
            "citation_title": "Graphtext: Graph reasoning in text space",
            "mention_or_use": "mention",
            "representation_name": "k-hop subgraph textualization / subgraph linearization",
            "representation_description": "For a target (centric) node, extract its k-hop subgraph, serialize relevant node/edge attributes and connectivity into a natural-language description (text prompt) representing the local structure and features, then feed that text to an LLM in few-shot prompting to produce node label predictions or explanations.",
            "graph_type": "text-attributed graph / node-centric subgraphs",
            "representation_properties": "Makes graph data compatible with LLMs (text modality); interpretable (natural language); compactness controlled by k; depends on quality of natural-language description; introduces potential information loss from graph→text conversion and sensitivity to how the subgraph is described (prompt formulation).",
            "evaluation_task": "Node classification / node label prediction (few-shot prompting)",
            "performance_metrics": null,
            "comparison_to_other_representations": "Qualitatively presented as an approach that enables zero/few-shot LLM prediction on textualized graph inputs; contrasted with native GNNs which operate on adjacency/features directly. Paper reports this line of work as able to produce explainable predictions and be used in few-shot setups, but provides no numeric head-to-head results in the survey.",
            "limitations_or_challenges": "Performance is sensitive to the prompt/textualization design and to how subgraphs are described; textualization may lose structural details (limits in faithfully encoding graph is noted); robustness and reliability concerns; not systematically compared across textualization strategies.",
            "uuid": "e5223.0",
            "source_info": {
                "paper_title": "Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT4Graph",
            "name_full": "GPT4Graph",
            "brief_description": "An empirical evaluation/benchmark that uses LLM self-prompting to produce textual descriptions of graphs and measures LLM competence on graph-structure reasoning tasks (e.g., node degree, diameter, clustering coefficient recognition).",
            "citation_title": "Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking.",
            "mention_or_use": "mention",
            "representation_name": "self-prompted graph description",
            "representation_description": "LLMs are prompted (self-prompting) to describe graph structures or compute graph properties by converting graph structural information into natural-language queries or descriptions; these textual descriptions are then used for LLM-based reasoning about graph properties.",
            "graph_type": "generic graphs used for graph-structure reasoning (synthetic/benchmark graphs)",
            "representation_properties": "Enables direct use of LLMs for structural reasoning tasks; representation fidelity depends on the expressiveness of the generated textual description and prompt; subject to variability introduced by description style.",
            "evaluation_task": "Graph structural reasoning tasks: recognizing node degree, graph diameter, clustering coefficient and other structural statistics/labels.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Survey notes experiments like GPT4Graph and TLG that evaluate LLMs on graph-structure tasks; highlights that textual descriptions greatly impact LLM performance and that current textualization approaches may not fully capture structural information compared with graph-native methods.",
            "limitations_or_challenges": "Descriptions are sensitive to wording leading to reliability/robustness issues; textualization may not capture full structural expressivity; lack of theoretical validation (e.g., Weisfeiler–Lehman tests) to assess whether LLM-text encodings preserve structural distinctions.",
            "uuid": "e5223.1",
            "source_info": {
                "paper_title": "Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "TLG",
            "name_full": "TLG (Textual-graph LLM approach)",
            "brief_description": "A method that describes graphs, nodes and edges with natural language and applies large LMs (e.g., PaLM 62B) in zero-shot to predict graph properties or perform reasoning over graph data.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "graph-to-text descriptions for zero-shot LLM prediction",
            "representation_description": "Generate textual descriptions of graph structures (nodes, edges, and their relations) and formulate reasoning prompts so that an LLM like PaLM can answer queries or predict graph statistics in a zero-shot manner.",
            "graph_type": "general graphs / graph reasoning benchmarks",
            "representation_properties": "Leverages LLM's language understanding for zero-shot adaptation across tasks; representation relies on natural-language expressiveness and LLM pretraining knowledge; may be interpretable and human-readable.",
            "evaluation_task": "Predicting graph statistics and semantic labels in zero-shot settings.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Reported as part of the class of textualization approaches (alongside GraphText and GPT4Graph); survey notes that such LLM-as-predictor methods can be adapted across tasks easily but are influenced by prompt design and example selection.",
            "limitations_or_challenges": "No numeric performance reported in survey; sensitive to prompt engineering, example selection; issues with robustness, reliability, and potential data leakage from LLM pretraining.",
            "uuid": "e5223.2",
            "source_info": {
                "paper_title": "Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "OFA / TAPE / GraphGPT (text augmentation)",
            "name_full": "OFA, TAPE, GraphGPT (LLM-based textual augmentation methods)",
            "brief_description": "Approaches that use LLMs (GPT-3.5 / other LMs) to generate textual descriptions, explanations, or labels for nodes/subgraphs to augment graphs with textual attributes which are then encoded for downstream graph models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "LLM-generated node/edge/subgraph descriptions (textual augmentation)",
            "representation_description": "Use an LLM to generate natural language descriptions, explanations, or labels for nodes, edges or entire graphs (e.g., chemical functional group explanations, node descriptions), and attach those texts as attributes; optionally encode texts with Sentence-BERT / DeBERTa / other encoders for GNN consumption or jointly tune.",
            "graph_type": "text-attributed graphs (text-augmented nodes/edges); molecular graphs in some applications",
            "representation_properties": "Augments sparse or non-text features with rich natural-language descriptions; increases interpretability; allows leveraging pretrained text encoders; may improve downstream performance when text is informative.",
            "evaluation_task": "Node-level prediction, molecular property prediction, graph reasoning and explanation generation (varies by cited work).",
            "performance_metrics": null,
            "comparison_to_other_representations": "Survey describes three cooperation categories: GLA-centric (encode text then feed to GNNs), alignment-based (contrastive/alignment between graph and text embeddings), and LLM-centric (map graph structure into LLM prefix/tokens). Text-augmentation methods are reported to improve performance when combined appropriately but no numeric comparisons are provided in the survey.",
            "limitations_or_challenges": "Quality of generated text governs usefulness; text-sparse graphs (e.g., traffic, power networks) are challenging to describe; potential mismatches between LLM-generated descriptions and structural signals; lack of systematic comparison between textual augmentation strategies.",
            "uuid": "e5223.3",
            "source_info": {
                "paper_title": "Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Textual Feature Encoding (Sentence-BERT / RoBERTa / PaLM)",
            "name_full": "Textual Feature Encoding using pre-trained language encoders (Sentence-BERT, RoBERTa, PaLM, etc.)",
            "brief_description": "Standard practice of encoding natural-language descriptions attached to graph elements into dense embeddings using pretrained transformers or sentence encoders, either frozen or co-trained with graph models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "text embedding of graph-attached text",
            "representation_description": "Feed LLM/LM-generated textual attributes into an encoder (Sentence-BERT, RoBERTa, PaLM, vanilla Transformer) to obtain vector representations which are used as node/edge features (frozen or fine-tuned), or used in cross-modal training/alignment with graph encoders.",
            "graph_type": "text-attributed graphs / structured-textual graphs",
            "representation_properties": "Leverages strong pretrained semantic encoders; produces dense, continuous features that are easy to integrate with GNNs/Transformers; encoding quality depends on encoder trained objective; can be frozen or co-trained.",
            "evaluation_task": "Used broadly in downstream tasks such as node classification, graph classification, link prediction, or as inputs to alignment and pretraining objectives.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Survey notes methods using these encoders as components across GLA-centric and alignment-based approaches; alignment-based pretraining (contrastive) between graph encoders and text encoders is cited as beneficial but no numeric results are given in the survey.",
            "limitations_or_challenges": "Choice of encoder and co-training strategy affects performance; may not capture structural relations unless alignment objectives or co-training are applied; textual encoders can introduce bias or leakage from pretraining corpus.",
            "uuid": "e5223.4",
            "source_info": {
                "paper_title": "Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Structured-Textual Graph (G_ST) representation",
            "name_full": "Structured-Textual Graph (G_ST)",
            "brief_description": "A representation formalizing the combination of structured graphs (nodes, edges) and textual descriptions attached to nodes/edges/subgraphs, enabling multimodal graph learning by LLMs and GNNs.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "representation_name": "structured-textual graph (combined graph+text modality)",
            "representation_description": "Define G_S = (V,E) as the structured graph and G_T as textual descriptions; G_ST is the union where textual attributes V_T, E_T or graph-level text G_T are associated with graph elements. These textual attributes can be generated or augmented by LLMs and encoded (Sentence-BERT, DeBERTa) and then combined with graph neural methods via GLA-centric, alignment-based, or LLM-centric strategies.",
            "graph_type": "text-attributed graphs / multimodal graphs",
            "representation_properties": "Enables leveraging both structural and semantic textual signals; flexible integration patterns (encode-then-GNN, alignment, map-to-LLM); can improve explainability and transferability; complexity increases with multimodal fusion.",
            "evaluation_task": "Used across node/edge/graph-level tasks, pretraining/transfer learning, and explainable prediction tasks described in the survey.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Survey categorizes three manners of combining modalities (GLA-centric, alignment-based, LLM-centric) and qualitatively contrasts their trade-offs: GLA-centric emphasizes graph learners with text features; alignment-based focuses on joint embedding/pretraining; LLM-centric maps graph info into LLM input/prefix. No quantitative ranking is provided in the survey.",
            "limitations_or_challenges": "Design choices (how to attach/encode text, alignment objectives, joint training) lack systematic comparison; for text-sparse domains it's unclear how to generate meaningful text; theoretical understanding of how textualization preserves graph structure is lacking.",
            "uuid": "e5223.5",
            "source_info": {
                "paper_title": "Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graphtext: Graph reasoning in text space",
            "rating": 2,
            "sanitized_title": "graphtext_graph_reasoning_in_text_space"
        },
        {
            "paper_title": "Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking.",
            "rating": 2,
            "sanitized_title": "gpt4graph_can_large_language_models_understand_graph_structured_data_an_empirical_evaluation_and_benchmarking"
        },
        {
            "paper_title": "Boosting graph reasoning ability of large language model",
            "rating": 1,
            "sanitized_title": "boosting_graph_reasoning_ability_of_large_language_model"
        },
        {
            "paper_title": "TLG",
            "rating": 1
        },
        {
            "paper_title": "OFA",
            "rating": 1
        },
        {
            "paper_title": "GraphGPT",
            "rating": 1
        },
        {
            "paper_title": "TAPE",
            "rating": 1
        },
        {
            "paper_title": "NLGraph",
            "rating": 1
        }
    ],
    "cost": 0.010173999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models
23 Feb 2024</p>
<p>Lanning Wei weilanning1997@gmail.com 
Institute of Computing Technology
Chinese Academy of Sciences</p>
<p>University of Chinese Academy of Sciences</p>
<p>Jun Gao imgaojun@gmail.com 
Harbin Institute of Technology</p>
<p>Huan Zhao zhaohuan@4paradigm.com 
Quanming Yao 
Department of Electronic Engineering
Tsinghua University</p>
<p>Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models
23 Feb 20248E073AFDC04404EA3C2F124001ADEC75arXiv:2402.11641v2[cs.LG]
Graph-structured data are the commonly used and have wide application scenarios in the real world.For these diverse applications, the vast variety of learning tasks, graph domains, and complex graph learning procedures present challenges for human experts when designing versatile graph learning approaches.Facing these challenges, large language models (LLMs) offer a potential solution due to the extensive knowledge and the human-like intelligence.This paper proposes a novel conceptual prototype for designing versatile graph learning methods with LLMs, with a particular focus on the "where" and "how" perspectives.From the "where" perspective, we summarize four key graph learning procedures, including task definition, graph data feature engineering, model selection and optimization, deployment and serving.We then explore the application scenarios of LLMs in these procedures across a wider spectrum.In the "how" perspective, we align the abilities of LLMs with the requirements of each procedure.Finally, we point out the promising directions that could better leverage the strength of LLMs towards versatile graph learning methods.The related source can be found at: https://github.com/wei-ln/versatilegraph-learning-approaches. 1</p>
<p>Introduction</p>
<p>Graph-structured data are commonly used and applied in real-world applications, e.g., social networks [Hamilton et al., 2017], chemistry and biomedical molecules [Gilmer et al., 2017].Existing methods have achieved great success in understanding and solving the single task, e.g., formulating the molecular property prediction as graph classification task [Gilmer et al., 2017], conducting graph sampling when facing large-scale graphs [Hamilton et al., 2017], designing expressive graph learning algorithms (GLAs) to extract graph structural information [Min et al., 2022;Wu et al., 2020], and selecting appropriate hyper-parameters in evaluation stage [Zhang et al., 2022].Despite their success on one 1 Work are done when Lanning and Jun are interns in 4Paradigm.</p>
<p>single task, there are still challenges in graph learning that are yet to be addressed.Firstly, the graph domains and the learning problems are largely different in real world.Subsequently, human experts need to configure the complex procedures [You et al., 2020] individually, which brings about substantial requirements for professionalism and domain knowledge on graph-structured data.All these aspects lead to the challenge for human experts in handling the graph learning problems that suited for different tasks, graphs in diverse domains, and can efficiently conduct the pipeline with as few as possible assistance from humans, i.e., towards versatile graph learning methods.</p>
<p>Large language models (LLMs) are treated as the key point in designing versatile graph learning algorithms due to the maintained knowledge and human-like intelligence.LLMs, refer to the large-sized (billions-level) pre-trained language models (PLM) in general, have undergone a rapid succession of breakthroughs in recent years [Zhao et al., 2023b].By pre-training with comprehensive text data and prompt-tuning on downstream tasks, LLMs pose a vast store of knowledge and show the ability in achieving the human-level decisionmaking ability [Zhao et al., 2023b;Wang et al., 2023c].For instance, LLMs can reason on mathematical problems with chain-of-thoughts [Wei et al., 2022], span different tasks and achieve superior performance [Mialon et al., 2023], solve computer vision tasks from task planning to algorithm selection and execution like human experts [Shen et al., 2023].Based on these abilities, LLMs show promising potential to serve as Artificial General Intelligence (AGI) [Ge et al., 2023] and general research assistant [Huang et al., 2023b].Therefore, it is natural to directly use or draw on the successful experience of LLMs when constructing versatile graph learning methods.</p>
<p>The numerous advantages of LLMs have triggered an increasing interest in using them for graph learning problems.[Li et al., 2023;Guo et al., 2023;Shen et al., 2023;Zhang et al., 2023a].For instance, through harnessing the preserved knowledge , different graph learning tasks across various domains can be unified into textual descriptions, which subsequently employ LLMs, such as ChatGPT, to acquire predictions [Liu et al., 2023a;Fatemi et al., 2023].LLMs enhance the textural-attributed graphs (TAGs) which subsequently employ LLMs, such as ChatGPT, to acquire predictions [He et al., 2023;Guo et al., 2023;Chen et al., 2023].These methods have catalyzed impressive progress in designing effective techniques for different graph learning problems.</p>
<p>With such a development of graph learning methods joint with LLMs, it is necessary to explore the systematical way towards versatile graph learning methods using LLMs.In this paper, we conduct a comprehensive review on existing methods and develop conceptual prototype by aligning the advantages and strong abilities of LLMs with the different requirements maintained in versatile graph learning methods.More specific, the key insights are the identification of: Where can be used.Different requirements for LLMs are arise when human experts manage the graph learning pipeline.Towards versatile graph learning approach, it is a vital to examine the feasibility of LLMs in diverse graph learning procedures to fully unleash the versatile abilities of LLMs.Consequently, we summarize four key procedures based on the requirements according to a standard machine learning pipeline, i.e., task definition, feature engineering, model selection and optimization, deployment and serving, as shown in Fig. 1.Based on these procedures, we can explore a wider spectrum of application scenarios when employing LLMs, thereby enhancing the versatility of existing graph learning methods.How to use.It is important to align the capabilities of LLMs with the requirements of the four procedures.We summarize three levels of increased requirements for LLMs, depicted as three rows in Fig. 1, and then provide a detailed analysis of existing methods.The human-designed methods, represented in the first row, place no requirements on LLMs, The methods situated in the second row emphasize the fundamental ability of LLMs to understand, encode, and reason out the natural language queries.The methods in the third row require a higher level of human-level decision-making ability and the rich domain-specific knowledge from LLMs, similar to human experts.The varying capabilities of LLMs encourage us to investigate their potential applications in each procedure, prompting the development of versatile graph learning methods.</p>
<p>It is significant to highlight the proposed conceptual prototype in Fig. 1 when designing versatile graph learning methods based on LLMs.The "Where" perspective is spanned along with four key procedures in graph learning pipeline, and the "How" perspective is organized based on the abilities of LLMs in different levels.Then, the graph learning methods joint with LLMs can be designed by choosing the desired ability in each procedure, which is detailed shown in Table 1.Based on these two perspectives, we provide comprehensive overview of the graph learning methods jointed with LLMs, and emphasize the potential for a broad exploration spectrum and the usability of LLMs' various abilities towards versatile graph learning approaches.Finally, we suggest promising future directions on the basis of underexplored ability of LLMs and the property of graph-structured data, i.e., the effectiveness in understanding the graph structure, large graph foundation model, and universal graph learning agents.</p>
<p>Overview</p>
<p>Machine Learning on Graphs</p>
<p>The graph-structured data is presented as G = (V, E), where V and E are nodes and edges.A ∈ R |V|×|V| is the adja-cency matrix of this graph, and H ∈ R |V|×d is the feature matrix.The graph learning procedures generate the results R of graphs learning problem P on graph G.</p>
<p>Considering the considerable efforts expended by human experts to solve these diverse graphs and applications, we summarize the key points in graph learning pipeline as shown in Fig. 1.</p>
<p>• Task definition.Experts formulate the problems P into specific graph learning tasks T , e.g., the predictions on nodes, edges, sub-graphs or graphs.</p>
<p>• Feature engineering.Experts select, combine and transform features towards better-performed performance by themselves [Liu et al., 2022]</p>
<p>Conceptual Prototype</p>
<p>In this paper, we propose a conceptual prototype when designing versatile graph learning methods jointed with LLMs, following the development of graph learning procedures and the different abilities of LLMs as illustrated in Fig. 1. "Where can be used" We analyze and summarize the common requirements in each key graph learning procedure, which are represented as four columns in Fig. 1.This analysis led us to explore the potential application scenarios of LLMs in each procedure, which could broaden the spectrum of potential applications of LLMs in various graph learning methods.The results show that existing methods are extensively used in the feature engineering and model selection procedures, attributing to the need for enhancing features and representations of textual data."How to use" Considering the different requirements in each procedure, we explore the feasibility of using LLMs according to a hierarchy of requirements for LLMs' abilities.This hierarchy is visualized as different rows in Fig. 1, i.e., no requirements for the use of LLMs (with human experts merely); requiring the fundamental ability in understanding and reasoning the natural languages; and requisition of advanced human-like intelligence and domain-specific knowledge as human experts.The majority methods, represented in the second row, employ LLMs to assist the human experts due to their proficiency in question answer, understanding and encoding the natural language queries.Conversely, a minority methods, as illustrated in the final row, exploit the potential of LLMs in planning, decomposing and completing tasks with themselves due to the advanced ability in achieving humanlike intelligence.It is necessary to note that the potential abilities of LLMs are still being explored and the conceptual prototype represented in the figure may be extended to include more rows as the development of LLMs progresses.</p>
<p>Comparisons with Contemporaneous Surveys</p>
<p>There exists several contemporaneous surveys that explore how to use LLMs in feature engineering and model selection procedures.To be specific, [Li et al., 2023;Jin et al., 2023a] categorized the methods based on the role that LLM played, and [Zhang et al., 2023b;Liu et al., 2023b] et al., 2023c].This suggestion aligns with recent global research topics [He et al., 2023].</p>
<p>Graph Learning with LLMs</p>
<p>In this section, we overview the existing methods following the sequential graph learning pipeline, and then the methods used in each procedure are introduced following the involvement of LLMs.</p>
<p>Task Definition</p>
<p>Task definition transforms the learning problems in realworld applications into possible solvable tasks with current machine learning techniques.It is the first step to addressing the machine learning problems and can only be effectively done through human expertise.LLMs have advantages in understanding the different applications in real-world, and they have the ability to assist the complex task planning, decomposition and completion [Mialon et al., 2023;Huang et al., 2023b].</p>
<p>Efforts have been made by combing LLMs with other modalities and graph-structured data.In computer vision, HuggingGPT [Shen et al., 2023] used LLMs to understand the complex computer vision tasks and complete them sequentially with the help of LLMs, which can be achieved due to the human-like decision-making capability of LLMs.However, graph-structured data, which stems from different domains, presents a unique challenge.The learning tasks related to graph-structured data are in varied formats, necessitating diverse configurations for individual tasks [Liu et al., and GPT4Graph [Guo et al., 2023] evaluated LLMs in understanding and formulating the graph reasoning tasks over different graph datasets.Instruction2GL [Wei et al., 2023] proposed a LLM-based planning agent and then mapped the users' instructions into different graph learning tasks.Given the experiments of these methods over different tasks, the feasibility of LLMs in planning and understanding graph learning tasks is obvious.</p>
<p>In conclusion, defining the learning tasks is the first step when facing the real-world applications on graph-structured data.It can be solved by LLMs with the ability in understanding the natural languages and the maintained domain knowledge over graphs.With these abilities, LLMs bring more convenient interaction manner with users and could alleviate the stress over human experts.It indicates the potential of LLMs to define, planning, decomposing the complex graph learning tasks.</p>
<p>Feature Engineering</p>
<p>Graph feature engineering is responsible for combing or transforming data to more effective features which are built based on the original graph-structured data.LLMs have the ability to generate descriptions of the graph using natural language, and can also encode text into an embedding space, serving as a useful tool in this regard.By introducing this new modality to graphs, an abundance of textual information can be incorporated, which may lead to a significant improvement in the effectiveness of the algorithm.</p>
<p>Textual Feature Construction</p>
<p>As shown in Fig. 2, two types of engineering strategies are designed to combine textual information based on the graphstructured data.In this paper, for the clear justification, we denote the structured graph as G S = (V, E), and the textual graph G T as a word sequence {w 1 , • • • } which describes the nodes and edges in graph.The structured-textual graphs G ST is defined as the combination of structures G S and tex-
tual description {w 1 , • • • } of nodes V T , edges E T , or the graph G T .
Firstly, for structured-textual graph G ST , the textual descriptions and explanations of nodes, edges or sub-graphs can be generated by LLMs and used in the following procedures to improve the effectiveness [Chen et al., 2023].To be specific, Chen et al. [Chen et al., 2023] highlighted the text attributes for nodes, GraphGPT [Tang et al., 2023] described the graph structures with natural languages, TAPE [He et al., 2023] used GPT-3.5 to generate the node descriptions, prediction as well as the explanations to enrich the text information; OFA [Liu et al., 2023a] further provided the descriptions In conclusion, existing methods enhance the textual information with the help of LLMs, the enhancement include but not limited to the descriptions on nodes and connection relationship and explanations on the rich domain information maintained in nodes, edges or graphs.It is achieved with the abilities of answering the questions of users based on the vast knowledge maintained in LLMs.</p>
<p>Textual Feature Encoding</p>
<p>Existing methods employ open-source LMs or LLMs to obtaining the text embedding with vanilla Transformer [Vaswani et al., 2017], Sentence-BERT [Reimers and Gurevych, 2019], RoBERTa [Liu et al., 2019] or PaLM [Chowdhery et al., 2023].These models can be frozen to obtaining the embedding directly, or co-trained with the graph learning algorithms which will be introduced in the following.</p>
<p>Summary and Discussion</p>
<p>In conclusion, existing methods explore the feasibility of LLMs in generating novel text modality for graphs.As illustrated in Fig. 1, by combing the expertise of human experts with the versatile explore of LLMs, these methods construct and encode the textural information, on top of which the performance improvements of graph learning methods can be expected.Apart from these methods, AutoKG [Chen and Bertozzi, 2023] adopted LLMs to construct the entities of knowledge graphs based on natural languages, thereby empowering decisions akin to human expertise.Despite the success of these methods, there is a notable absence of systematic comparisons and discussions about effective construction of textual components in the graph, especially for text-sparse graphs such as traffic and power transmission graphs [Jin et al., 2023a], graph with signal or image features that are challenging to describe using natural languages.</p>
<p>Model Selection and Optimization</p>
<p>The core objective of graph learning procedures is selecting and training models that will be used for the downstream tasks, i.e., selecting and optimizing A based on the given data G and tasks T .LLMs are indispensable in encoding the textual information and have advantages in making decisions due to the maintained knowledge.In the following, we will introduce how LLMs are utilized given different types of data mentioned in Section 3.2.In conclusion, when dealing with the textual graph, the utilization of LLMs is indispensable.These methods pose superior capabilities in generating explainable predictions compared with general graph learning methods such as GNNs and graph transformer [Zhao et al., 2023a].Furthermore, they could used in zero-shot manner, which could adapt to different tasks and data easily due to the strong ability in understanding natural language queries.Nonetheless, the prompt design and the usage of examples still have large influence on the model performance [Fatemi et al., 2023].Moreover, the robustness, reliability, and data leakage problems of LLMs are the potential issues that need to be solved [Huang et al., 2023a].</p>
<p>LLMs as Predictor for Textual Graph</p>
<p>LLMs as Co-operator for Structured-Textual Graphs G ST</p>
<p>With textual and structured graph, LLMs A LLM s and graph learning algorithms A GLAs , like GNNs and graph Transformers [Kipf and Welling, 2016;Min et al., 2022;Wu et al., 2020;Wang et al., 2023d], are co-operated towards accurate predictions.Existing methods are organized following three categories as illustrated in Fig. 3. Firstly, the GLAcentric methods first use LLMs to encode the textual information, and then employ graph learning algorithms to obtain predictions based on these enriched features.By encoding the textual information of nodes and edges with Sentence-BERT firstly, OFA [Liu et al., 2023a] adopted R-GCN [Schlichtkrull et al., 2018] to aggregate the features given different types of edges.TAPE [He et al., 2023] adopted Deberta to encode the textual descriptions and explanations , and then used RevGAT to learning graph representations.Secondly, the alignment-based methods are co-operated by aligning the textual and structured embedding space.For instance, G2P2 [Wen and Fang, 2023] adopted three levels contrastive learning when pre-trained the GCN [Kipf and Welling, 2016] and Transformer [Vaswani et al., 2017].Patton [Jin et al., 2023b] trained GNNs and BERT in a nested manner, and GRAD [Mavromatis et al., 2023] distilled the structured information from GNNs to enhance BERT towards better understanding on graphs.Finally, the LLM-centric methods use graph learning algorithms to extract the graph structure information, and then mapped into the text space of LLMs.These methods could leverage the advantages of graph learning algorithms in understanding the inherent structural characteristics and LLMs in feature transformation [Li et al., 2023].The representative GraphLLM [Chai et al., 2023] first learned graph representations with Graph Transformer, and then mapped into prefix and tuned with LLaMA.In conclusion, LLMs and graph learning algorithms are cooperated when facing the structured-textual graphs.Three categories of combinations manners are explored in the literature, i.e., GLA-centric, alignment-based and LLM-centric methods, as shown in Fig. 3.In these methods, LLMs are treated as an pure large models or as language models to enhancing the information extraction, which can be frozen or trained with GNNs.In the future work, the improvements can be developed towards larger graph learning models given the model size of LLMs.</p>
<p>LLMs as Advisor for Structured graph G S</p>
<p>Based on the structured graphs without textual information, the LLM-assisted graph learning methods, i.e., LLMs serve as the graph learning research assist in selecting models, are proposed considering its strong ability in reasoning and human-like decision making ability.GPT4GNAS [Wang et al., 2023a] used GPT-4 to guide the designing of GNNs with AutoML.It used a fixed search space and then let GPT-4 select and revise GNNs from this space towards betterperformed architectures.Instruction2GL [Wei et al., 2023] employs GPT-3.5 to configure the search space and search algorithms to automated conducting graph learning procedures based on the AutoML technique.</p>
<p>Moreover, motivated by the advantages of LLMs in handling different tasks by training on large-scale data from different tasks, the development of graph foundation models (GFMs) are popular in recent years.To be specific, the constructions of foundation models following the topics of architecture backbone, pre-training and adaption to downstream tasks [Liu et al., 2023b;Zhang et al., 2023b;Jin et al., 2023a].Motivated by the "pre-train and prompt" large model train paradigm, existing methods construct pretraining data based on different tasks and then propose graph prompt strategy towards versatile graph learning models.The representative methods GraphPrompt [Liu et al., 2023c] and GPPT [Sun et al., 2022] unified the graph learning tasks on node, edge and graph levels with link prediction task and then pre-train the GNNs on different tasks.In the prompt tuning stage, GraphPrompt learned the prompt vector and GPPT designed prompt function on tasks and structures when facing the different downstream tasks.</p>
<p>In conclusion, when facing the structured graph, LLMs could serve as advisor to support the graph learning as shown in Fig. 4, i.e., serve as an research assistant with the humanlike decision-making capabilities, and motivate the construction and training of graph foundation models.Compared with the latter, the LLM-assisted methods necessitate a higher level of LLM proficiency in human-like intelligence.All these methods underscore the potential of LLMs to accomplish the complex graph learning tasks, marking a new trend and a critical technique in the development of user-friendly, accessible graph learning methods [Wang et al., 2023c;Wei et al., 2023].</p>
<p>Summary and Discussions</p>
<p>In graph learning model selection and optimization procedures, LLMs play different roles depending on the type of graph data encountered.They are treated as the predictor when facing the textual graph, and serve as the co-operator towards better combination from two modalities in structuredtextual graphs.When facing these graph graphs, LLMs are explored due to its ability in understanding the text modality and transforming features.When facing the structured graph, LLMs are treated as the advisor to assist the graph learning by leveraging their human-level intelligence and utilizing innovative model training strategies.Expected for the benefits by incorporating new modalities, LLMs have difficulties in understanding the graph structures [Fatemi et al., 2023;Guo et al., 2023].Such deficiencies accumulate over time, contributing to weak robustness and a lack of theoretical justifications in graph learning tasks.A crucial challenge that needs to be addressed involves harnessing the power of LLMs by integrating them with existing graph learning methods, which have a superior understanding of graph structural information.</p>
<p>Deployment and Serving</p>
<p>Given the user request, deploying solutions and then generating response for users are the final step of the graph learning pipeline, and they are tightly connected with users, which indicating that more human engagements are required to improve the usability and accessibility [Yao et al., 2018].LLMs could generate codes based on the users requirements and call proper APIs related to the data [Wang et al., 2023c;Huang et al., 2023b].Following this paradigm, LLMs are used as general AI research assistant in writing files, executing code and inspecting outputs [Huang et al., 2023b;Mialon et al., 2023;Zhang et al., 2023a], displaying high usability and interpretability in conducting the graph learning procedures.On graph-structured data, the deployment and serving are related to the tools and packages they used.Graph Tool-former [Zhang, 2023] used external API tools to load graph data, and generate prediction as well as explanations on graphs under different tasks.Instruction2GL [Wei et al., 2023] learned to call APIs in Pytorch Geometric package to load data, execute graph learning code, and generate the response based on the users' requests and code execution logs.As a conclusion, LLMs serve as development assistants, with human-like ability to use various tools.They also act as a new interface due to their ability to generate natural language responses.In the future, LLMs could play even more vital role after familiar with the external graph developments tools.This advancement will make graph-based machine learning more accessible and easy to use, thereby lowering the barriers to entry.</p>
<p>Summary and Discussions</p>
<p>In this section, we have provided detailed analysis on how LLMs are involved in different graph learning procedures.Given the versatile abilities of LLMs, it is feasible to integrate them towards versatile graph learning algorithms as shown in Fig. 1 and Table 1.Considering the rapid progression and yet under-explored potential of LLMs, significant research opportunities still exist to explore how to effectively integrate LLMs into graph learning problems.</p>
<p>Future Directions</p>
<p>The developments of LLMs and the potential applications are fast growing in recent years.Beyond existing methods, there are still several promising directions that could be explored on the graph-structured data.The ability quantification of LLMs in understanding graphs-structured data.There is an absence of systematic evaluations regarding the ability of LLMs to comprehend graph-structured data.To be specific, LLMs are trained with natural languages, while the graph structure, the key in graph learning, are represented as the adjacency matrix in general and are new modality for LLMs.Existing methods, e.g., GPT4Graph [Guo et al., 2023] and TLG [Fatemi et al., 2023], attempt to describe graph structures with natural languages and then evaluate the effectiveness on graph structure reasoning tasks, e.g., recognizing the node degree, graph diameter or clustering coefficient.However, these measures are not adequate to entirely capture the capacity of LLMs to understand graph structures.Firstly, the descriptions can greatly impact performance, which may result in insufficient robustness and reliability.Moreover, Weisfeiler-Lehman test, which is seminal in assessing the comprehension of graph structures, should be considered to theoretically validate the effectiveness.Consequently, both from the theoretical and empirical points of view, there is a need for a benchmark and detailed analysis to evaluate the competence of LLMs in understanding graph structures.Large graph foundation models.Excepted for employing LLMs in solving graph learning tasks, designing graph foundation models and scaling up the model parameters is a promising direction in handling the different tasks and data from different domains.To be specific, existing methods have explored the GNNs and Graph Transformer methods on the given single task [Wu et al., 2020;Min et al., 2022].Compared with foundation models in natural language, challenges in designing large graph models are the scarcity of extensive pre-training data and the lack of large-scale backbone architectures [Liu et al., 2023b;Zhang et al., 2023b].Furthermore, designing the graph foundation models for specific graph domains is more effective and feasible considering the application scenarios [Zhang et al., 2023b], e.g., recommendation systems, molecules and finance.Universal Graph learning agents.LLM-based autonomous agents are the prominent research topic in both academic and industry due to its potential emulating human-like decisionmaking processes [Wang et al., 2023c].In graph learning, LLMs could serve as the research assistant when facing the different learning problems.For example, decomposing the graph learning tasks, writing and executing graph learning procedures step by step [Mialon et al., 2023;Wei et al., 2023;Huang et al., 2023b], or arranging the graph learning tools when facing the tasks.Consequently, such an approach holds immense potential to facilitate advancements in current graph learning methods.</p>
<p>Conclusion</p>
<p>Exploring the feasibility of employing LLMs in graph learning problems is a promising direction considering the applications of graphs in the real world.In this paper, we propose a novel conceptual prototype for versatile graph learning approaches, and provide a comprehensive overview of existing methods jointed with LLMs following the graph learning procedures and the different ability requirements for LLMs.To be specific, we first explore the feasibility of LLMs used in the graph learning pipeline, including task definition, graph feature engineering, model selection and optimization, deployment and serving procedures.In each procedure, we categorize methods following the involvements of LLMs and provide detailed discussions on these methods.Finally, we suggest future directions for LLM-based graph learning approaches, potentially expanding the methodology towards universal graph learning methods.</p>
<p>Figure 1 :
1
Figure 1: The conceptual prototype of versatile graph learning methods joint with LLMs.LLMs can be used in sequential graph learning procedures in these columns, with increased requirements for LLMs in different rows for each procedure.The rows can be further developed along with the exploration of different abilities of LLMs.</p>
<p>Figure 2 :
2
Figure 2: Illustrations of graph data feature engineering strategies that jointed with LLMs.</p>
<p>G T Existing methods employ open or closed source LLMs, denoted as A LLM s , to generate the prediction results R of graph learning problem P. GraphText [Zhao et al., 2023a], a representative method in the field, used closed-source ChatGPT to predict the node labels with k-hop sub-graph text in a fewshot manner.TLG [Fatemi et al., 2023] employed opensource PaLM 62B [Chowdhery et al., 2023] to predict different graph properties via the text descriptions, which could be conducted in zero-shot manner.</p>
<p>Figure 3 :
3
Figure 3: Illustrations of LLM-related graph learning algorithms on structured-textual graphs.</p>
<p>Figure 4 :
4
Figure 4: Illustrations of LLM-empowered graph learning algorithms on graph-structured data.</p>
<p>Table 1 :
1
A summary of graph learning methods jointed with LLMs under the proposed conceptual prototype.represents cells that were created by human experts and do not have any relation to the following procedures that use LLMs.
MethodsTask DefinitionTaxonomyFeature Engineering DescriptionLLMsTaxonomyModel Selection and Optimization DescriptionLLMsDeployment &amp; ServingGraphtext [Zhao et al., 2023a]-G TThe features and node labels of k-hop subgraphs-Predictorpredicting labels of centric nodeChatGPT / GPT-4Using predictions as responseNLGraph [Wang et al., 2023b]Formulating graph reasoning tasks with human expertsG TDescribing the graphs, nodes, edges and learning tasks.-PredictorPredicting the statistics of graphs.GPT-3.5 / GPT4Using predictions as responseGPT4Graph [Guo et al., 2023]-G Tself-prompting to describe the graphs.InstructGPT-3 PredictorPredicting the statistics and Semantic labels of graphs.InstructGPT-3Using predictions as responseTLG [Fatemi et al., 2023]Formulating graph reasoning tasks with human expertsG TDescribing the graphs, nodes, edges and learning tasks.-PredictorPredicting the statistics of graphs.PaLM 62BUsing predictions as responseLLM4Mol [Qian et al., 2023]-G TCombining SMILES text and explanations of functional groups, chemical pharmaceutical applications properties, and potentialChatGPTPredictorFine-tune a pre-trained downstream tasks language model on various molecule relatedRoBERTa-OFA [Liu et al., 2023a]-G STExplanations on V T and E TChatGPTGLA-centricco-trained LLMs and R-GCNsSentence-bert-Combining[Chen et al., 2023]-G STGenerating and encoding textual information of nodesOpen-source LLMs Deberta / LLaMAGLA-centric / Predictor Alignment-based /Combining using Deberta as predictor GCN/GAT/RevGAT with Sentence-BERT/ Deberta;GCN/GAT/RevGAT using Deberta as with Sentence-BERT/ Deberta; -predictorGRAD [Mavromatis et al., 2023] -G STUsing the raw texts in graphs -Alignment-basedco-trained GNN teacher and LLM studentBERT-Applying contrastiveGraphGPT [Tang et al., 2023]-G STUsing the raw texts in graphs -Alignment-basedalignment objective on the predictions of pre-trainedBert-GNNs and LLMsGraphPrompt [Liu et al., 2023c] -G SSampling and unifying data and tasks-GFMsPre-train and prompt on downstream tasks--All in One [Sun et al., 2022]Unified with graph-levelG SConstructing prompt graphs -GFMsPre-trained with multi-task meta-earning and prompted--taskson downstream tasksConfiguring codesInstruction2GL [Wei et al., 2023]Formulating tasks with ChatGPT-3.5G SSlecting engineering strategy following user instructionsChatGPT-3.5LLM-assistedConfiguring AutoML andChatGPT-3.5and generating response with ChatGPT and theagentsGPT4GNAS [Wang et al., 2023a] -G S--LLM-assistedSelecting and evaluating GNNs on AutoML withGPT-4-LLMs.
[Wang et al., 2023b]ture, NLGraph[Wang et al., 2023b]</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Chai, arXiv:2310.05845arXiv:2304.04370Boosting graph reasoning ability of large language model. Wenyue Hua2023. 2023. 2023. 2023. 2023. 2023. 2023. 2023. 2023. 202324Chen and BertozziarXiv preprintJianchao Ji. Juntao Tan, Shuyuan Xu, and Yongfeng Zhang. Openagi: When llm meets domain experts</p>
<p>Neural message passing for quantum chemistry. Gilmer, ICML. 2017. 2017</p>
<p>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. Guo, arXiv:2305.15066NeurIPS2023. 2023. 2017. 2017arXiv preprintInductive representation learning on large graphs</p>
<p>Explanations as features: Llmbased features for text-attributed graphs. He, arXiv:2305.195232023. 2023arXiv preprint</p>
<p>Can llms effectively leverage graph structural information: when and why. Huang, arXiv:2309.165952023a. 2023arXiv preprint</p>
<p>Semi-supervised classification with graph convolutional networks. Huang, arXiv:2310.03302arXiv:2305.12268Benchmarking large language models as ai research agents. ICLR2023b. 2023. 2023a. 2023. 2023b. 2023. 2016. 2016arXiv preprintLarge language models on graphs: A comprehensive survey</p>
<p>Li, arXiv:2311.12399arXiv:1907.11692A survey of graph meets large language model: Progress and future directions. 2023. 2023. 2019. 2019arXiv preprintRoberta: A robustly optimized bert pretraining approach</p>
<p>Taxonomy of benchmarks in graph representation learning. Liu, Learning on Graphs Conference. PMLR2022. 2022</p>
<p>One for all: Towards training one graph model for all classification tasks. Liu, arXiv:2310.00149arXiv:2310.118292023a. 2023. 2023b. 2023arXiv preprintTowards graph foundation models: A survey and beyond</p>
<p>Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. Liu, arXiv:2304.10668arXiv:2310.13023Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. Mingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, Xin Wang, Heraklion, Crete, GreeceSpringer2023c. 2023. 2023. 2023. 2023. 2022. 2022. 2023. 2023. 2019. 2019. 2018. June 3-7, 2018. 2018. 2023. 2022. 2023. 2023. 201715arXiv preprintAdvances in neural information processing systems</p>
<p>Automated 3d pre-training for molecular property prediction. Wang , arXiv:2310.01436arXiv:2308.11432Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023a. 2023. 2023b. 2023. 2023c. 2023. 2023d. 2023arXiv preprintA survey on large language model based autonomous agents</p>
<p>Unleashing the power of graph learning through llm-based autonomous agents. Wei, arXiv:2309.04565arXiv:2307.10230Advances in Neural Information Processing Systems. 2022. 2022. 2023. 2023. 202335arXiv preprintChain-of-thought prompting elicits reasoning in large language models. Prompt tuning on graph-augmented low-resource text classification</p>
<p>A comprehensive survey on graph neural networks. Wu, IEEE Transactions on Neural Networks and Learning Systems (TNNLS). 2020. 2020</p>
<p>Jiaxuan You, Zhitao Ying, and Jure Leskovec. Design space for graph neural networks. Yao, arXiv:1810.13306Advances in Neural Information Processing Systems. 2018. 2018. 202033arXiv preprintTaking human out of learning applications: A survey on automated machine learning</p>
<p>Efficient hyper-parameter search for knowledge graph embedding. Zhang, arXiv:2305.02499arXiv:2308.14522Shujian Zhang, Chengyue Gong, Lemeng Wu, Xingchao Liu, and Mingyuan Zhou. Automl-gpt: Automatic machine learning with gpt. Long Papers. 2021. 2021. 2022. 2022. 2023a. 2023. 2023b. 20231arXiv preprintProceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Xin Wang, and Wenwu Zhu. Large graph models: A perspective</p>
<p>Automl for deep recommender systems: A survey. Jiawei Zhang, ; Zhang, Zhao, arXiv:2304.11116arXiv:2303.18223Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt. Zheng, 2023. 2023. 2023a. 2023. 2023b. 2023. 202341arXiv preprintGraphtext: Graph reasoning in text space</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1234 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1234</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1234</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-267499902</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.03570v4.pdf" target="_blank">Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently. As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive queries. We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM. In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling. Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data. Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation. In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44\%$ performance gain, and is comparable to or slightly surpassing their model-free counterparts.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1234.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1234.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DWM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diffusion World Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conditional diffusion-based world model that predicts a multistep sequence of future states and rewards in a single forward sampling pass, conditioned on current state, action and a return-to-go (RTG) target.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Diffusion World Model (DWM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conditional diffusion probabilistic model (classifier-free guidance) that models x0 = (r_t, s_{t+1}, r_{t+1}, ..., s_{t+T-1}, r_{t+T-1}) conditioned on (s_t, a_t, g_t). Noise predictor implemented as a temporal U-Net (6 residual blocks) that outputs denoised sequences; conditioning via RTG, action and timestep embeddings; does not model future actions. Uses stride/accelerated sampling (train K steps, infer N steps) and low-temperature sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural generative sequence world model (diffusion)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Offline continuous-control locomotion (D4RL suite) — general offline RL</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Prediction mean-squared error (MSE) on predicted state and reward sequences; compounding-error measured by downstream return vs simulation horizon; downstream normalized return (0-1) on D4RL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Substantially lower long-horizon MSE than one-step dynamics (reported qualitatively and in Appendix tables); robust to simulation horizons up to H=31; downstream average normalized return 0.643 ± 0.07 (across 9 D4RL tasks) vs one-step average 0.446 ± 0.116 (≈44% relative gain). DWM also outperforms Transformer-based world model baselines (≈37.5% relative gain).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural generative model; no interpretable latent factors or symbolic structure are reported; model is not presented as intrinsically interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned in the paper (no latent-factor visualization, attention analysis, or symbolic extraction applied).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training: diffusion model trained for 2e6 iterations (EMA used); reported wall-clock training time for DWM-based RL methods ≈ 18000 s (on NVIDIA V100) for full runs described. Sampling: trained with K=5 diffusion steps, accelerated inference using N=3 steps (stride sampling); per-batch (128 trajectories) sampling time reported ≈ 0.031 s in the reported timing experiment. Inference still more expensive than trivial one-step models but much cheaper than some autoregressive diffusion baselines at long horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Inference: DWM inference is constant-time with respect to horizon (sample whole sequence once) and reported ~4.6× faster inference than Decision Diffuser (which requires sampling at inference time plus inverse dynamics). Training and sampling are more expensive than one-step models (one-step training ≈ 2300 s vs DWM ≈ 18000 s in reported runs), but sampling scales far better with horizon than autoregressive diffusion (AD).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>On 9 D4RL locomotion tasks, DWM-based algorithms (DWM-TD3BC, DWM-IQL, DWM-PQL) achieve an average normalized return of ~0.643 ± 0.07, outperforming one-step dynamics model counterparts (average ~0.446 ± 0.116) and Transformer-based world-model substitutes; comparable to or slightly better than model-free baselines in several settings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Higher multistep predictive fidelity (reduced compounding error) translates into improved value-estimation (Diffusion-MVE) and thus better downstream policy learning in offline RL. The DWM can be used to generate imagined short-term returns for critic targets and as a synthetic data source for offline Q-learning; conditioning on optimistic/out-of-distribution RTGs provides useful value-regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Clear trade-off: DWM reduces compounding error and improves task performance at the expense of heavier computation (longer training, more complex sampling). It is task/environment-specific (trained per environment) and remains a black-box (low interpretability). Accelerated inference (reduced N) speeds sampling but may harm fidelity if too aggressive (r_infer < 0.5 hurts accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Predicts sequences of states+rewards (T default 8, also tested T=32) conditioned on s_t, a_t and RTG g_eval; omits future actions to keep inference cheap; temporal U-Net architecture; classifier-free guidance and low-temperature sampling; stride sampling for accelerated inference; EMA of parameters; hyperparams: K=5 (train), N=3 (inference), p_uncond=0.25; RTG conditioning (often optimistic OOD values) used to regularize value estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Versus one-step Gaussians: DWM yields much improved long-horizon predictions and downstream returns by avoiding iterative rollouts, but costs more to train and to sample per trajectory (although DWM's cost does not grow with horizon). Versus Transformer sequence models: DWM outperforms Transformer-based world models in experiments (less autoregressive compounding error), but Transformer training/inference can be faster. Versus Decision Diffuser: similar task performance but DWM is ~4.6× faster at inference because it does not need to run the world model at action selection time. Versus Autoregressive Diffusion (AD): DWM sampling time is constant in horizon while AD scales linearly and becomes computationally prohibitive for long horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends sequence length T=8 as a default (T=32 gives marginal gains but higher cost); K=5 training diffusion steps and N=3 inference steps (stride sampling) to balance accuracy and speed, with inference-step ratio r_infer >= 0.5 as critical; condition on moderately optimistic OOD RTG values for better offline policy improvement; use λ-return when integrating with IQL for better results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1234.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1234.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>One-step</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-step dynamics model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard one-step parametric dynamics model that predicts next state and reward p(s_{t+1}, r_t | s_t, a_t), typically trained as a Gaussian conditional model and used recursively for multi-step rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>One-step dynamics model (Gaussian MLP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deterministic/stochastic per-step dynamics parameterized as Gaussian p(s_{t+1}, r_t | s_t, a_t) with mean and diagonal covariance predicted by MLPs (4-layer MLPs with 256 units in this paper). Used in classic MVE by recursive application to simulate multi-step trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>one-step parametric dynamics model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Offline continuous-control locomotion (D4RL), general MBRL</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Per-step MSE on next-state and reward predictions; downstream normalized return under multi-step rollout (compounding error measured as return decay vs horizon).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reasonable one-step accuracy but rapid accumulation of error with iterative rollouts: downstream average normalized return reported ~0.446 ± 0.116 across tasks (paper's one-step baselines). Performance drops quickly as simulation horizon increases (compounding error).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural net (MLP) outputs parametric Gaussian; relatively simple and transparent in parameterization but no explicit interpretability analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned beyond the simple parametric (Gaussian) structure.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training: 1e6 iterations in experiments; wall-clock training time reported ~2300 s (per the paper comparisons). Inference: cheap per-step but requires H separate model queries for H-step rollout (linear scaling with horizon).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Faster to train and cheaper per-model-evaluation than DWM; however, because rollout requires recursive calls, sampling cost scales with horizon and compounding error makes long-horizon simulation unreliable compared to DWM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Good for short-horizon model-based rollouts but inferior for long-horizon value expansion; downstream average return lower than DWM in reported experiments (average ~0.446).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Useful for short-term imagination and sample-efficient MBRL, but poor long-horizon fidelity leads to unreliable value-estimates and degraded policy learning when using longer rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Low compute and simple implementation vs severe compounding error with horizon and degraded long-horizon task utility; more efficient but less faithful for long-term prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Gaussian parameterization, 4-layer MLPs with 256 hidden units, diagonal covariance predicted with SoftPlus on final layer; trained via log-likelihood maximum.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>DWM reduces compounding error and improves long-horizon downstream performance vs one-step; Transformers and diffusion-based autoregressive approaches suffer from autoregressive compounding as well, but differ in computational trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests one-step models work best with short rollout horizons (1–2 steps) for MVE; longer horizons are not recommended due to compounding errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1234.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1234.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer WM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-based world model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sequence-modeling approach using Transformer architectures to predict future states (and possibly rewards/actions) autoregressively; evaluated as a substitute to DWM in the same pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-based world model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer conditioned on initial state-action pairs trained to predict subsequent state/reward sequence; masking used for unknown future actions and autoregressive decoding used for rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive sequence world model (transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Offline continuous-control (D4RL locomotion) and general sequence-conditioning RL</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Sequence prediction MSE and downstream normalized return; compounding-error measured as drop in return with longer autoregressive rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Underperforms DWM in experiments: DWM-based algorithms surpass Transformer-based substitutes by ~37.5% (relative) in reported benchmarks; Transformer-based methods show more pronounced compounding error due to autoregressive rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box Transformer; no interpretability analyses reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Reported wall-clock training time for Transformer-based model-based algorithms ~8000 s (between one-step and DWM in the reported comparisons). Inference requires autoregressive steps and thus scales with rollout horizon.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Slower to train than one-step models but faster than DWM in reported training wall-clock time; inference cost scales with horizon unlike DWM's single-query sequence sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Worse than DWM on average; sometimes competitive with one-step baselines for short horizons but suffers with long-horizon planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Autoregressive nature makes it susceptible to compounding errors; although Transformers are powerful sequence models, their autoregressive decoding here limits long-horizon predictive fidelity and downstream policy gains relative to DWM's joint-sequence diffusion approach.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Transformer gives flexible sequence modeling and often faster training than DWM, but autoregressive structure induces compounding error that reduces utility for long-horizon model-based value estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Autoregressive decoding, masking of future actions, 4-layer transformer with 4 attention heads (per experiments), losses include state and reward prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>DWM offers better long-horizon fidelity and downstream improvement; one-step models are cheaper but degrade quickly with horizon; Transformers are intermediate with their own compounding issues.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified beyond experimental hyperparameters; paper reports Transformer as a baseline with same conditioning choices to compare compounding error effects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1234.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1234.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DecisionDiffuser</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decision Diffuser (DD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion-based offline RL method that models state-only future trajectories and then predicts actions via an inverse dynamics model at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decision Diffuser (state-only diffusion + inverse dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unconditioned/conditioned diffusion model that generates state-only trajectories; actions at inference are recovered using a separately trained inverse dynamics model (IDM). The IDM is a large MLP trained to predict actions from consecutive states.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>diffusion-based trajectory model + inverse dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Offline continuous-control (D4RL) and conditional behavior synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Downstream normalized return; quality of generated state trajectories and IDM action prediction accuracy; computational timing at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Task performance reported as comparable to DWM on many tasks (paper states comparable).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Model components are neural (diffusion + IDM); no interpretability methods discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Computationally inefficient at inference because the diffusion model must be sampled at inference time and an inverse dynamics model evaluated to recover actions; paper reports DWM is ~4.6× faster at inference averaged over 600 evaluation episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Similar task performance to DWM but significantly slower inference; requires training an IDM (large 3-layer MLP with 1024 units per layer in experiments) and longer IDM training (2e6 iterations in the reported setup).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Comparable to DWM in many environments per experiments, but slower to run at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Predicting state-only trajectories can be effective, but requiring IDM to recover actions couples performance to IDM accuracy and increases inference cost; not as convenient for integration into MF actor-critic pipelines where fast action generation is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Similar fidelity to DWM on value estimation but with major inference-time computational cost; design splits trajectory generation and action recovery, increasing complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>State-only diffusion conditioning and separate inverse dynamics model for action conversion; uses N=3 internal sampling steps for comparability in timing experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>DWM achieves similar returns but much faster inference because DWM conditions on (s_t,a_t) and is used only for critic training while actor execution does not require sampling the world model.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends DWM when inference speed is important; DD may be acceptable if inference cost is not critical or IDM is highly accurate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1234.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1234.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autoregressive Diffusion (AD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive diffusion approach used as a baseline where diffusion is applied per-step and rolled out autoregressively for multi-step trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Autoregressive Diffusion (AD)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Diffusion model applied at the transition level and rolled out autoregressively: generate next-step via diffusion, feed it back, then generate subsequent step, etc. Same model architecture as diffusion but used in autoregressive loop for rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive diffusion world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Offline RL (D4RL locomotion) used as a baseline for sampling cost experiments</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Prediction MSE and downstream return (but emphasis in paper is on sampling time and scaling), sampling wall-clock time per batch of trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Prediction quality can be good per step but overall MBRL integration is computationally impractical for large horizons; paper does not report better downstream returns than DWM.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural generative model; no interpretability methods described.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Sampling time grows linearly with horizon: reported sampling times for a batch of 128 trajectories on walker2d-medium-v2 (A6000 GPU): H=1 -> 0.030 s, H=3 -> 0.087 s, H=5 -> 0.145 s, H=7 -> 0.198 s, while DWM is constant ~0.031 s. Extrapolated to training usage, generating trajectories with AD would take ≈27.5 hours vs ≈4 hours with DWM in the reported setup.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Far less efficient for multi-step imagination than DWM because of linear scaling in horizon; DWM's single-sequence sample is constant-time regardless of H (up to sampled sequence length).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not favored due to computational inefficiency in MBRL pipelines despite potential per-step fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Autoregressive diffusion's per-step fidelity is overshadowed by impractical sampling cost for generating large numbers of imagined trajectories used in model-based RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Possibly high per-step fidelity but prohibitive computational cost with growing horizon; not practical as a general MBRL world model in this paper's regime.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Apply diffusion to single-step transition-level distribution and autoregressively roll out; small number of diffusion steps for speed (K=3 in timing experiments) but still scales with H.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>DWM is far more sampling-time efficient for multi-step imagination; one-step Gaussian models are cheaper per prediction but suffer compounding error; AD is closest in expressivity to diffusion approaches but inefficient for MBRL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1234.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1234.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SynthER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SynthER / SynTHER (transition-level diffusion for data augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unconditional transition-level diffusion model used to generate synthetic transitions for data augmentation (augment offline dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SynthER (unconditioned transition-level diffusion)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unconditioned diffusion model trained on single-step transitions (state, action, reward, next state) to generate synthetic transitions to augment offline datasets; used as a data-augmentation baseline (DA-TD3BC / DA-IQL) in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>unconditioned transition-level diffusion model (data-augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Offline RL (D4RL locomotion) as augmentation baseline</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Quality of generated transitions (not deeply quantified in main text) and downstream policy performance after augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In experiments, DWM consistently outperforms SynthER-style data augmentation baselines across tasks; SynthER required different preprocessing in original paper but was re-run under DWM normalizations for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Neural generative model; no interpretability work reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>To enable fast sampling in comparisons, trained with low diffusion steps (K=5) and sampled with N=3. Computational cost still exists but used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less useful than DWM for generating multi-step trajectories because it produces single transitions (augmentation) rather than joint multi-step imagination; overall DWM-based imagination delivered better downstream value estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Augmented models (DA-TD3BC / DA-IQL) underperformed compared to DWM-based algorithms in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Transition-level augmentation helps but does not substitute for accurate multistep sequence modeling when the goal is improved long-horizon value estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Simpler to implement and sample than full-sequence diffusion but lacks DWM's multistep imagination utility; may be less computationally expensive but less effective for MVE-style uses.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Unconditioned transition diffusion, lower diffusion steps to speed sampling for baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>DWM yields better downstream RL improvements by modeling multi-step sequences directly; SynthER is useful for augmentation but not as effective for diffusion-based MVE.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1234.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1234.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PolyGRAD/PGD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PolyGRAD / Policy-guided Diffusion (PGD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Diffusion-based methods that generate on-policy trajectories by alternating state/reward denoising and action sampling (PolyGRAD) or using classifier guidance with policy-gradient (PGD).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PolyGRAD / Policy-guided Diffusion (PGD)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PolyGRAD predicts sequences of future states and rewards conditioning on initial state but treats future actions as unknown and alternates between sampling actions via stochastic Langevin dynamics guided by policy score and denoising states/rewards; PGD trains an unconditional diffusion and uses classifier/policy-gradient guidance during sampling to produce on-policy trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>policy-guided diffusion / on-policy diffusion world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Online and offline RL settings in prior work; on-policy trajectory generation for policy optimization</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Quality of generated on-policy trajectories and downstream policy-learning performance (not directly quantified here beyond references).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Prior works demonstrate policy-guided diffusion can produce on-policy-like trajectories; in this paper these methods are discussed as alternatives—DWM is distinguished by being off-policy (sampling independent of current policy) and used for offline RL.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Neural black-box; no interpretability analysis discussed in paper with regard to these methods.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>On-policy sampling requires iterative action sampling (Langevin dynamics or classifier guidance) interleaved with denoising, which increases sampling complexity compared to DWM's off-policy single-pass sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>PGD/PolyGRAD generate on-policy trajectories but at extra sampling cost and complexity; DWM's off-policy single-shot sequence generation is simpler for offline MVE use.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not directly compared in experiments here; cited as prior work with different goals (on-policy data generation).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>On-policy diffusion is useful when integrating with policy optimization loops that require trajectories matched to current policy; DWM is designed for offline Q/value estimation where off-policy imagined trajectories are desirable.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>On-policy guidance yields trajectories aligned with a target policy but increases sampling and algorithmic complexity; DWM favors simplicity and offline stability.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Policy guidance (classifier guidance or Langevin dynamics) used during diffusion sampling to steer samples toward policy-typical actions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>DWM differs by conditioning on a_t and RTG and by producing off-policy multi-step sequences in a single pass, enabling efficient offline Diffusion-MVE without policy-in-the-loop sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Decision Diffuser <em>(Rating: 2)</em></li>
                <li>Diffuser <em>(Rating: 2)</em></li>
                <li>PolyGRAD <em>(Rating: 2)</em></li>
                <li>SynthER <em>(Rating: 2)</em></li>
                <li>MOPO: Model-based Offline Policy Optimization <em>(Rating: 2)</em></li>
                <li>Dreamer <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1234",
    "paper_id": "paper-267499902",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "DWM",
            "name_full": "Diffusion World Model",
            "brief_description": "A conditional diffusion-based world model that predicts a multistep sequence of future states and rewards in a single forward sampling pass, conditioned on current state, action and a return-to-go (RTG) target.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Diffusion World Model (DWM)",
            "model_description": "Conditional diffusion probabilistic model (classifier-free guidance) that models x0 = (r_t, s_{t+1}, r_{t+1}, ..., s_{t+T-1}, r_{t+T-1}) conditioned on (s_t, a_t, g_t). Noise predictor implemented as a temporal U-Net (6 residual blocks) that outputs denoised sequences; conditioning via RTG, action and timestep embeddings; does not model future actions. Uses stride/accelerated sampling (train K steps, infer N steps) and low-temperature sampling.",
            "model_type": "neural generative sequence world model (diffusion)",
            "task_domain": "Offline continuous-control locomotion (D4RL suite) — general offline RL",
            "fidelity_metric": "Prediction mean-squared error (MSE) on predicted state and reward sequences; compounding-error measured by downstream return vs simulation horizon; downstream normalized return (0-1) on D4RL tasks.",
            "fidelity_performance": "Substantially lower long-horizon MSE than one-step dynamics (reported qualitatively and in Appendix tables); robust to simulation horizons up to H=31; downstream average normalized return 0.643 ± 0.07 (across 9 D4RL tasks) vs one-step average 0.446 ± 0.116 (≈44% relative gain). DWM also outperforms Transformer-based world model baselines (≈37.5% relative gain).",
            "interpretability_assessment": "Black-box neural generative model; no interpretable latent factors or symbolic structure are reported; model is not presented as intrinsically interpretable.",
            "interpretability_method": "None mentioned in the paper (no latent-factor visualization, attention analysis, or symbolic extraction applied).",
            "computational_cost": "Training: diffusion model trained for 2e6 iterations (EMA used); reported wall-clock training time for DWM-based RL methods ≈ 18000 s (on NVIDIA V100) for full runs described. Sampling: trained with K=5 diffusion steps, accelerated inference using N=3 steps (stride sampling); per-batch (128 trajectories) sampling time reported ≈ 0.031 s in the reported timing experiment. Inference still more expensive than trivial one-step models but much cheaper than some autoregressive diffusion baselines at long horizons.",
            "efficiency_comparison": "Inference: DWM inference is constant-time with respect to horizon (sample whole sequence once) and reported ~4.6× faster inference than Decision Diffuser (which requires sampling at inference time plus inverse dynamics). Training and sampling are more expensive than one-step models (one-step training ≈ 2300 s vs DWM ≈ 18000 s in reported runs), but sampling scales far better with horizon than autoregressive diffusion (AD).",
            "task_performance": "On 9 D4RL locomotion tasks, DWM-based algorithms (DWM-TD3BC, DWM-IQL, DWM-PQL) achieve an average normalized return of ~0.643 ± 0.07, outperforming one-step dynamics model counterparts (average ~0.446 ± 0.116) and Transformer-based world-model substitutes; comparable to or slightly better than model-free baselines in several settings.",
            "task_utility_analysis": "Higher multistep predictive fidelity (reduced compounding error) translates into improved value-estimation (Diffusion-MVE) and thus better downstream policy learning in offline RL. The DWM can be used to generate imagined short-term returns for critic targets and as a synthetic data source for offline Q-learning; conditioning on optimistic/out-of-distribution RTGs provides useful value-regularization.",
            "tradeoffs_observed": "Clear trade-off: DWM reduces compounding error and improves task performance at the expense of heavier computation (longer training, more complex sampling). It is task/environment-specific (trained per environment) and remains a black-box (low interpretability). Accelerated inference (reduced N) speeds sampling but may harm fidelity if too aggressive (r_infer &lt; 0.5 hurts accuracy).",
            "design_choices": "Predicts sequences of states+rewards (T default 8, also tested T=32) conditioned on s_t, a_t and RTG g_eval; omits future actions to keep inference cheap; temporal U-Net architecture; classifier-free guidance and low-temperature sampling; stride sampling for accelerated inference; EMA of parameters; hyperparams: K=5 (train), N=3 (inference), p_uncond=0.25; RTG conditioning (often optimistic OOD values) used to regularize value estimation.",
            "comparison_to_alternatives": "Versus one-step Gaussians: DWM yields much improved long-horizon predictions and downstream returns by avoiding iterative rollouts, but costs more to train and to sample per trajectory (although DWM's cost does not grow with horizon). Versus Transformer sequence models: DWM outperforms Transformer-based world models in experiments (less autoregressive compounding error), but Transformer training/inference can be faster. Versus Decision Diffuser: similar task performance but DWM is ~4.6× faster at inference because it does not need to run the world model at action selection time. Versus Autoregressive Diffusion (AD): DWM sampling time is constant in horizon while AD scales linearly and becomes computationally prohibitive for long horizons.",
            "optimal_configuration": "Paper recommends sequence length T=8 as a default (T=32 gives marginal gains but higher cost); K=5 training diffusion steps and N=3 inference steps (stride sampling) to balance accuracy and speed, with inference-step ratio r_infer &gt;= 0.5 as critical; condition on moderately optimistic OOD RTG values for better offline policy improvement; use λ-return when integrating with IQL for better results.",
            "uuid": "e1234.0",
            "source_info": {
                "paper_title": "Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "One-step",
            "name_full": "One-step dynamics model",
            "brief_description": "Standard one-step parametric dynamics model that predicts next state and reward p(s_{t+1}, r_t | s_t, a_t), typically trained as a Gaussian conditional model and used recursively for multi-step rollouts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "One-step dynamics model (Gaussian MLP)",
            "model_description": "Deterministic/stochastic per-step dynamics parameterized as Gaussian p(s_{t+1}, r_t | s_t, a_t) with mean and diagonal covariance predicted by MLPs (4-layer MLPs with 256 units in this paper). Used in classic MVE by recursive application to simulate multi-step trajectories.",
            "model_type": "one-step parametric dynamics model",
            "task_domain": "Offline continuous-control locomotion (D4RL), general MBRL",
            "fidelity_metric": "Per-step MSE on next-state and reward predictions; downstream normalized return under multi-step rollout (compounding error measured as return decay vs horizon).",
            "fidelity_performance": "Reasonable one-step accuracy but rapid accumulation of error with iterative rollouts: downstream average normalized return reported ~0.446 ± 0.116 across tasks (paper's one-step baselines). Performance drops quickly as simulation horizon increases (compounding error).",
            "interpretability_assessment": "Black-box neural net (MLP) outputs parametric Gaussian; relatively simple and transparent in parameterization but no explicit interpretability analysis provided.",
            "interpretability_method": "None mentioned beyond the simple parametric (Gaussian) structure.",
            "computational_cost": "Training: 1e6 iterations in experiments; wall-clock training time reported ~2300 s (per the paper comparisons). Inference: cheap per-step but requires H separate model queries for H-step rollout (linear scaling with horizon).",
            "efficiency_comparison": "Faster to train and cheaper per-model-evaluation than DWM; however, because rollout requires recursive calls, sampling cost scales with horizon and compounding error makes long-horizon simulation unreliable compared to DWM.",
            "task_performance": "Good for short-horizon model-based rollouts but inferior for long-horizon value expansion; downstream average return lower than DWM in reported experiments (average ~0.446).",
            "task_utility_analysis": "Useful for short-term imagination and sample-efficient MBRL, but poor long-horizon fidelity leads to unreliable value-estimates and degraded policy learning when using longer rollouts.",
            "tradeoffs_observed": "Low compute and simple implementation vs severe compounding error with horizon and degraded long-horizon task utility; more efficient but less faithful for long-term prediction.",
            "design_choices": "Gaussian parameterization, 4-layer MLPs with 256 hidden units, diagonal covariance predicted with SoftPlus on final layer; trained via log-likelihood maximum.",
            "comparison_to_alternatives": "DWM reduces compounding error and improves long-horizon downstream performance vs one-step; Transformers and diffusion-based autoregressive approaches suffer from autoregressive compounding as well, but differ in computational trade-offs.",
            "optimal_configuration": "Paper suggests one-step models work best with short rollout horizons (1–2 steps) for MVE; longer horizons are not recommended due to compounding errors.",
            "uuid": "e1234.1",
            "source_info": {
                "paper_title": "Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Transformer WM",
            "name_full": "Transformer-based world model",
            "brief_description": "Sequence-modeling approach using Transformer architectures to predict future states (and possibly rewards/actions) autoregressively; evaluated as a substitute to DWM in the same pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Transformer-based world model",
            "model_description": "Autoregressive Transformer conditioned on initial state-action pairs trained to predict subsequent state/reward sequence; masking used for unknown future actions and autoregressive decoding used for rollouts.",
            "model_type": "autoregressive sequence world model (transformer)",
            "task_domain": "Offline continuous-control (D4RL locomotion) and general sequence-conditioning RL",
            "fidelity_metric": "Sequence prediction MSE and downstream normalized return; compounding-error measured as drop in return with longer autoregressive rollouts.",
            "fidelity_performance": "Underperforms DWM in experiments: DWM-based algorithms surpass Transformer-based substitutes by ~37.5% (relative) in reported benchmarks; Transformer-based methods show more pronounced compounding error due to autoregressive rollouts.",
            "interpretability_assessment": "Black-box Transformer; no interpretability analyses reported in the paper.",
            "interpretability_method": "None mentioned.",
            "computational_cost": "Reported wall-clock training time for Transformer-based model-based algorithms ~8000 s (between one-step and DWM in the reported comparisons). Inference requires autoregressive steps and thus scales with rollout horizon.",
            "efficiency_comparison": "Slower to train than one-step models but faster than DWM in reported training wall-clock time; inference cost scales with horizon unlike DWM's single-query sequence sampling.",
            "task_performance": "Worse than DWM on average; sometimes competitive with one-step baselines for short horizons but suffers with long-horizon planning.",
            "task_utility_analysis": "Autoregressive nature makes it susceptible to compounding errors; although Transformers are powerful sequence models, their autoregressive decoding here limits long-horizon predictive fidelity and downstream policy gains relative to DWM's joint-sequence diffusion approach.",
            "tradeoffs_observed": "Transformer gives flexible sequence modeling and often faster training than DWM, but autoregressive structure induces compounding error that reduces utility for long-horizon model-based value estimation.",
            "design_choices": "Autoregressive decoding, masking of future actions, 4-layer transformer with 4 attention heads (per experiments), losses include state and reward prediction.",
            "comparison_to_alternatives": "DWM offers better long-horizon fidelity and downstream improvement; one-step models are cheaper but degrade quickly with horizon; Transformers are intermediate with their own compounding issues.",
            "optimal_configuration": "Not specified beyond experimental hyperparameters; paper reports Transformer as a baseline with same conditioning choices to compare compounding error effects.",
            "uuid": "e1234.2",
            "source_info": {
                "paper_title": "Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "DecisionDiffuser",
            "name_full": "Decision Diffuser (DD)",
            "brief_description": "A diffusion-based offline RL method that models state-only future trajectories and then predicts actions via an inverse dynamics model at inference time.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Decision Diffuser (state-only diffusion + inverse dynamics)",
            "model_description": "Unconditioned/conditioned diffusion model that generates state-only trajectories; actions at inference are recovered using a separately trained inverse dynamics model (IDM). The IDM is a large MLP trained to predict actions from consecutive states.",
            "model_type": "diffusion-based trajectory model + inverse dynamics",
            "task_domain": "Offline continuous-control (D4RL) and conditional behavior synthesis",
            "fidelity_metric": "Downstream normalized return; quality of generated state trajectories and IDM action prediction accuracy; computational timing at inference.",
            "fidelity_performance": "Task performance reported as comparable to DWM on many tasks (paper states comparable).",
            "interpretability_assessment": "Model components are neural (diffusion + IDM); no interpretability methods discussed.",
            "interpretability_method": "None mentioned.",
            "computational_cost": "Computationally inefficient at inference because the diffusion model must be sampled at inference time and an inverse dynamics model evaluated to recover actions; paper reports DWM is ~4.6× faster at inference averaged over 600 evaluation episodes.",
            "efficiency_comparison": "Similar task performance to DWM but significantly slower inference; requires training an IDM (large 3-layer MLP with 1024 units per layer in experiments) and longer IDM training (2e6 iterations in the reported setup).",
            "task_performance": "Comparable to DWM in many environments per experiments, but slower to run at inference.",
            "task_utility_analysis": "Predicting state-only trajectories can be effective, but requiring IDM to recover actions couples performance to IDM accuracy and increases inference cost; not as convenient for integration into MF actor-critic pipelines where fast action generation is needed.",
            "tradeoffs_observed": "Similar fidelity to DWM on value estimation but with major inference-time computational cost; design splits trajectory generation and action recovery, increasing complexity.",
            "design_choices": "State-only diffusion conditioning and separate inverse dynamics model for action conversion; uses N=3 internal sampling steps for comparability in timing experiments.",
            "comparison_to_alternatives": "DWM achieves similar returns but much faster inference because DWM conditions on (s_t,a_t) and is used only for critic training while actor execution does not require sampling the world model.",
            "optimal_configuration": "Paper recommends DWM when inference speed is important; DD may be acceptable if inference cost is not critical or IDM is highly accurate.",
            "uuid": "e1234.3",
            "source_info": {
                "paper_title": "Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "AD",
            "name_full": "Autoregressive Diffusion (AD)",
            "brief_description": "An autoregressive diffusion approach used as a baseline where diffusion is applied per-step and rolled out autoregressively for multi-step trajectories.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Autoregressive Diffusion (AD)",
            "model_description": "Diffusion model applied at the transition level and rolled out autoregressively: generate next-step via diffusion, feed it back, then generate subsequent step, etc. Same model architecture as diffusion but used in autoregressive loop for rollouts.",
            "model_type": "autoregressive diffusion world model",
            "task_domain": "Offline RL (D4RL locomotion) used as a baseline for sampling cost experiments",
            "fidelity_metric": "Prediction MSE and downstream return (but emphasis in paper is on sampling time and scaling), sampling wall-clock time per batch of trajectories.",
            "fidelity_performance": "Prediction quality can be good per step but overall MBRL integration is computationally impractical for large horizons; paper does not report better downstream returns than DWM.",
            "interpretability_assessment": "Black-box neural generative model; no interpretability methods described.",
            "interpretability_method": "None mentioned.",
            "computational_cost": "Sampling time grows linearly with horizon: reported sampling times for a batch of 128 trajectories on walker2d-medium-v2 (A6000 GPU): H=1 -&gt; 0.030 s, H=3 -&gt; 0.087 s, H=5 -&gt; 0.145 s, H=7 -&gt; 0.198 s, while DWM is constant ~0.031 s. Extrapolated to training usage, generating trajectories with AD would take ≈27.5 hours vs ≈4 hours with DWM in the reported setup.",
            "efficiency_comparison": "Far less efficient for multi-step imagination than DWM because of linear scaling in horizon; DWM's single-sequence sample is constant-time regardless of H (up to sampled sequence length).",
            "task_performance": "Not favored due to computational inefficiency in MBRL pipelines despite potential per-step fidelity.",
            "task_utility_analysis": "Autoregressive diffusion's per-step fidelity is overshadowed by impractical sampling cost for generating large numbers of imagined trajectories used in model-based RL training.",
            "tradeoffs_observed": "Possibly high per-step fidelity but prohibitive computational cost with growing horizon; not practical as a general MBRL world model in this paper's regime.",
            "design_choices": "Apply diffusion to single-step transition-level distribution and autoregressively roll out; small number of diffusion steps for speed (K=3 in timing experiments) but still scales with H.",
            "comparison_to_alternatives": "DWM is far more sampling-time efficient for multi-step imagination; one-step Gaussian models are cheaper per prediction but suffer compounding error; AD is closest in expressivity to diffusion approaches but inefficient for MBRL.",
            "uuid": "e1234.4",
            "source_info": {
                "paper_title": "Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "SynthER",
            "name_full": "SynthER / SynTHER (transition-level diffusion for data augmentation)",
            "brief_description": "An unconditional transition-level diffusion model used to generate synthetic transitions for data augmentation (augment offline dataset).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "SynthER (unconditioned transition-level diffusion)",
            "model_description": "Unconditioned diffusion model trained on single-step transitions (state, action, reward, next state) to generate synthetic transitions to augment offline datasets; used as a data-augmentation baseline (DA-TD3BC / DA-IQL) in comparisons.",
            "model_type": "unconditioned transition-level diffusion model (data-augmentation)",
            "task_domain": "Offline RL (D4RL locomotion) as augmentation baseline",
            "fidelity_metric": "Quality of generated transitions (not deeply quantified in main text) and downstream policy performance after augmentation.",
            "fidelity_performance": "In experiments, DWM consistently outperforms SynthER-style data augmentation baselines across tasks; SynthER required different preprocessing in original paper but was re-run under DWM normalizations for fair comparison.",
            "interpretability_assessment": "Neural generative model; no interpretability work reported.",
            "interpretability_method": "None mentioned.",
            "computational_cost": "To enable fast sampling in comparisons, trained with low diffusion steps (K=5) and sampled with N=3. Computational cost still exists but used as baseline.",
            "efficiency_comparison": "Less useful than DWM for generating multi-step trajectories because it produces single transitions (augmentation) rather than joint multi-step imagination; overall DWM-based imagination delivered better downstream value estimation.",
            "task_performance": "Augmented models (DA-TD3BC / DA-IQL) underperformed compared to DWM-based algorithms in the paper's experiments.",
            "task_utility_analysis": "Transition-level augmentation helps but does not substitute for accurate multistep sequence modeling when the goal is improved long-horizon value estimation.",
            "tradeoffs_observed": "Simpler to implement and sample than full-sequence diffusion but lacks DWM's multistep imagination utility; may be less computationally expensive but less effective for MVE-style uses.",
            "design_choices": "Unconditioned transition diffusion, lower diffusion steps to speed sampling for baseline comparisons.",
            "comparison_to_alternatives": "DWM yields better downstream RL improvements by modeling multi-step sequences directly; SynthER is useful for augmentation but not as effective for diffusion-based MVE.",
            "uuid": "e1234.5",
            "source_info": {
                "paper_title": "Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "PolyGRAD/PGD",
            "name_full": "PolyGRAD / Policy-guided Diffusion (PGD)",
            "brief_description": "Diffusion-based methods that generate on-policy trajectories by alternating state/reward denoising and action sampling (PolyGRAD) or using classifier guidance with policy-gradient (PGD).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "PolyGRAD / Policy-guided Diffusion (PGD)",
            "model_description": "PolyGRAD predicts sequences of future states and rewards conditioning on initial state but treats future actions as unknown and alternates between sampling actions via stochastic Langevin dynamics guided by policy score and denoising states/rewards; PGD trains an unconditional diffusion and uses classifier/policy-gradient guidance during sampling to produce on-policy trajectories.",
            "model_type": "policy-guided diffusion / on-policy diffusion world model",
            "task_domain": "Online and offline RL settings in prior work; on-policy trajectory generation for policy optimization",
            "fidelity_metric": "Quality of generated on-policy trajectories and downstream policy-learning performance (not directly quantified here beyond references).",
            "fidelity_performance": "Prior works demonstrate policy-guided diffusion can produce on-policy-like trajectories; in this paper these methods are discussed as alternatives—DWM is distinguished by being off-policy (sampling independent of current policy) and used for offline RL.",
            "interpretability_assessment": "Neural black-box; no interpretability analysis discussed in paper with regard to these methods.",
            "interpretability_method": "None mentioned.",
            "computational_cost": "On-policy sampling requires iterative action sampling (Langevin dynamics or classifier guidance) interleaved with denoising, which increases sampling complexity compared to DWM's off-policy single-pass sampling.",
            "efficiency_comparison": "PGD/PolyGRAD generate on-policy trajectories but at extra sampling cost and complexity; DWM's off-policy single-shot sequence generation is simpler for offline MVE use.",
            "task_performance": "Not directly compared in experiments here; cited as prior work with different goals (on-policy data generation).",
            "task_utility_analysis": "On-policy diffusion is useful when integrating with policy optimization loops that require trajectories matched to current policy; DWM is designed for offline Q/value estimation where off-policy imagined trajectories are desirable.",
            "tradeoffs_observed": "On-policy guidance yields trajectories aligned with a target policy but increases sampling and algorithmic complexity; DWM favors simplicity and offline stability.",
            "design_choices": "Policy guidance (classifier guidance or Langevin dynamics) used during diffusion sampling to steer samples toward policy-typical actions.",
            "comparison_to_alternatives": "DWM differs by conditioning on a_t and RTG and by producing off-policy multi-step sequences in a single pass, enabling efficient offline Diffusion-MVE without policy-in-the-loop sampling.",
            "uuid": "e1234.6",
            "source_info": {
                "paper_title": "Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Decision Diffuser",
            "rating": 2,
            "sanitized_title": "decision_diffuser"
        },
        {
            "paper_title": "Diffuser",
            "rating": 2
        },
        {
            "paper_title": "PolyGRAD",
            "rating": 2
        },
        {
            "paper_title": "SynthER",
            "rating": 2
        },
        {
            "paper_title": "MOPO: Model-based Offline Policy Optimization",
            "rating": 2,
            "sanitized_title": "mopo_modelbased_offline_policy_optimization"
        },
        {
            "paper_title": "Dreamer",
            "rating": 1
        }
    ],
    "cost": 0.0240005,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning
15 Oct 2024</p>
<p>Zihan Ding 
Amy Zhang 
Yuandong Tian 
Meta Qinqing 
Zheng Meta </p>
<p>Princeton University</p>
<p>University of Texas at Austin Meta</p>
<p>Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning
15 Oct 2024B8F95D8E97B048B633871ED2952FFA16arXiv:2402.03570v4[cs.LG]
We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently.As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive queries.We integrate DWM into model-based value estimation[14], where the short-term return is simulated by future trajectories sampled from DWM.In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling.Alternatively, it can be seen as a data source that enables offline Qlearning with synthetic data.Our experiments on the D4RL [15] dataset confirm the robustness of DWM to long-horizon simulation.In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a 44% performance gain, and is comparable to or slightly surpassing their model-free counterparts.</p>
<p>Introduction</p>
<p>World models are foundational blocks of AI systems to perform planning and reasoning [21].They serve as simulators of real environments that predict the future outcome of certain actions will produce, and policies can be derived from them.Representative example usages of them in model-based reinforcement learning (MBRL) include action searching [61,82], policy optimization within such simulators [9,14,22,68], or a combination of both [8,27,28].The prediction accuracy of a world model is critical to the final performance of model-based RL approaches.Traditional MB methods builds a one-step dynamics model p one ps t<code>1 , r t |s t , a t q that predicts reward r t and next state s t</code>1 based on the current state s t and the current action a t [22,24,25,26,34,39,40,83]. When planning for multiple steps into the future, p one is recursively invoked, leading to a rapid accumulation of errors and unreliable predictions for long-horizon rollouts.Figure 1.1 plots the performance of an MB approach with a onestep dynamics model.The return quickly collapses as the rollout length increases, highlighting the issue of compounding errors for such models [4,45,78].</p>
<p>Recently, there has been growing interest in utilizing sequence modeling techniques to solve decision making problems, as seen in various studies [1,6,33,35,50,59,86,87].Under this theme, a number of works have proposed Transformer based [5,50,59] or diffusion model based [2,33,48,58,81,84] Method</p>
<p>RL Setup Diffusion Model Model Usages Action Prediction</p>
<p>SynthER [48] Offline/Online ppst, at, st<code>1, rtq transition-level data augmentation MF methods DWMs [2] Online ppst</code>1|st, at, . . ., st´T <code>1, at´T</code>1q step-by-step planning REINFORCE [75] PolyGrad [58] Online pprt, st<code>1, . . ., st</code>T ´1, rt<code>T ´1|st, at, . . ., at</code>T ´1q generate on-policy trajectories for policy optimization stochastic Langevine dynamics PGD [32] Offline ppst, at, rt, . . ., st<code>T ´1, at</code>T ´1, rt<code>T ´1, st</code>T q generate on-policy trajectories with policy gradient guidance for data augmentation</p>
<p>MF offline methods</p>
<p>UniSim ˚ [81] Offline ppst<code>1|st, atq step-by-step planning REINFORCE [75] Diffuser [33] Offline ppat, . . ., st</code>T ´1, at<code>T ´1|stq extract at from the sample extract from the sample DD [1] Offline ppst</code>1, . . ., st<code>T ´1|st, gtq extract st</code>1 from the sample inverse dynamics model DWM (ours) Offline pprt, st<code>1, rt</code>1, . . ., st<code>T ´1, rt</code>T ´1|st, at, gtq multistep planning MF offline methods *The observation of UniSim might contain multiple frames, yet the nature of their diffusion model is still a one-step model.dynamics models, or closely related approaches.As we will review at the end of this section, as well as in Section 2, while most existing approaches leverage these sequence models as dynamics models for planning, they model one-step future outcome s t`1 and r t using information of current and previous steps.At planning time, they still plan step by step.This raises an intriguing question that our paper seeks to answer: Can sequence modeling tools effectively reduce the compounding error in longhorizon prediction via jointly predicting multiple steps into the future?</p>
<p>In this paper, we introduce Diffusion World Model (DWM).Conditioning on current state s t , action a t , and expected return g t , DWM simultaneously predicts multistep future states and rewards.Namely, it models p θ pr t , s t<code>1 , r t</code>1 , . . ., s t<code>T ´1, r t</code>T ´1|s t , a t , g t q where T is the sequence length of the diffusion model.For planning H pH ă T q steps into the future, DWM only needs to be called once, whereas the traditional one-step model p one needs to be invoked H times.This greatly reduces the source of compounding error.As illustrated in Figure 1.1, diffusion world model is robust to long-horizon simulation, where the performance does not deteriorate even with simulation horizon 31.</p>
<p>To verify the proposed DWM, we consider the offline RL setup, where the objective is to learn a policy from a static dataset without online interactions.The detachment from online training circumvents the side effects of exploration and allows us to investigate the quality of world models thoroughly.We propose a generic Dyna-type [68] model-based framework.In brief, we first train a diffusion world model using the offline dataset, then train a policy using imagined data generated by the diffusion world model, in an actor-critic manner.Particularly, to generate the target value for training the critic, we introduce Diffusion Model Based Value Expansion (Diffusion-MVE) that uses diffusion world model generated future trajectories to simulate the return up to a chosen horizon.As we will elaborate later, Diffusion-MVE can be interpreted as a value regularization for offline RL through generative modeling, or alternatively, a way to conduct offline Q-learning with synthetic data.Our framework is flexible to carry any MF actor-critic RL method of choice, and the output policy is efficient at inference time, as the world model does not intervene with action generation.</p>
<p>Empirically, we benchmark diffusion-based and traditional one-step world models on 9 locomotion tasks from the D4RL datasets [15], where all the tasks are in continuous action and observation spaces.The predominant results are:</p>
<ol>
<li>Our results confirm that DWM outperform one-step models, where DWM-based algorithms achieves a 44% performance gain.2. We further consider a variant of our approach where the diffusion model is substituted with a Transformer architecture [70].Although Transformer is a sequence model, its inherent autoregressive structure is more prone to compounding error.We confirm that DWM-based algorithms surpass Transformer-based algorithms with a 37.5% performance gain.3. We also compare our algorithm with Decision Diffuser [1], a closely related model-based offline RL method that simulates the state-only trajectory, while predicting actions using an inverse dynamics model.The performance of the two methods are comparable.4. Meanwhile, due to inevitable modeling error, MB methods typically exhibit worse final performance compared with their model-free (MF) counterparts that directly learn policies from interacting with the true environment.Our results show that DWM-based MB algorithms is comparable to or even slightly outperforming its MF counterparts.We believe this stimulates us to conduct research in the space of model-based RL approaches, which come with an advantage of sample efficiency [9,10] and thus are potentially more suitable for practical real-world problems.</li>
</ol>
<p>Key Differences with Other Diffusion-Based Offline RL Methods More recently, various forms of diffusion models like [1,2,32,33,48,58,81,84] have been introduced for world modeling and related works.These works have targeted different data setups (offline or online RL), and utilize diffusion models to model different types of data distributions.When applying to downstream RL tasks, they also have distinct ways to derive a policy.While Section 2 will review them in details, we summarize our key distinctions from these works in Table 1.1.</p>
<p>Related Work</p>
<p>Model-Based RL One popular MB technique is action searching.Using the world model, one simulates the outcomes of candidate actions, which are sampled from proposal distributions or policy priors [53,74], and search for the optimal one.This type of approaches has been successfully applied to games like Atari and Go [61,82] and continuous control problems with pixel observations [23].</p>
<p>Alternatively, we can optimize the policy through interactions with the world model.This idea originally comes from the Dyna algorithm [68].The primary differences between works in this regime lie in their usages of the model-generated data.For example, Dyna-Q [67] and MBPO [34] augment the true environment data by world model generated transitions, and then conduct MF algorithms on either augmented or generated dataset.Feinberg et al. [14] proposes to improve the value estimation by unrolling the policy within the world model up to a certain horizon.The Dreamer series of work [22,24,25] use the rollout data for both value estimation and policy learning.More recently, Hansen et al. [27,28], Chitnis et al. [8] combine both techniques to solve continuous control problems.As we cannot go over all the MB approaches, we refer readers to Wang et al. [71], Amos et al. [3] for more comprehensive review and benchmarks of them.</p>
<p>Most of the aforementioned approaches rely on simple one-step world models p one ps t`1 , r t |s t , a t q.</p>
<p>The Dreamer series of work [22,24,25] use recurrent neural networks (RNN) to engage in past information for predicting the next state.Lately, Robine et al. [59], Micheli et al. [50], Chen et al. [5] have independently proposed Transformer-based world models as a replacement of RNN.Janner et al. [36] uses a generative model to learn the occupancy measure over future states, which can perform long-horizon rollout with a single forward pass.</p>
<p>Offline RL Directly applying online RL methods to offline RL usually leads to poor performance.The failures are typically attributed to the extrapolation error [18].To address this issue, a number of conservatism notions have been introduced to encourage the policy to stay close to the offline data.For model-free methods, these notions are applied to the value functions [44,42,19] or to the policies [77,37,43,16].Conservatism has also been incorporated into MB techniques through modified MDPs.For instance, MOPO [83] builds upon MBPO and relabels the predicted reward when generating transitions.It subtracts the uncertainty of the world model's prediction from the predicted reward, thereby softly promoting state-action pairs with low-uncertainty outcome.In a similar vein, MOReL [40] trains policies using a constructed pessimistic MDP with terminal state.The agent will be moved to the terminal state if the prediction uncertainty of the world model is high, and will receive a negative reward as a penalty.</p>
<p>Sequence Modeling for RL There is a surge of recent research interest in applying sequence modeling tools to RL problems.[6,35] first consider the offline trajectories as autoregressive sequences and model them using Transformer architectures [70].This has inspired a line of follow-up research, including [49,46].Normalizing flows like diffusion model [30,64,66], flow matching [47] and consistency model [65] have also been incorporated into various RL algorithms, see e.g., [7,11,12,29,38,52,72,79].Several recent works have utilized the diffusion model (DM) for world modeling in a variety of ways.Here we discuss them and highlight the key differences between our approach and theirs, see also  [81].In essence, these models still plan step by step while incorporating information from preivous steps, whereas our model plans multiple future steps at once.Similarly, Zhang et al. [84] trains a discretized DM with masked and noisy input.</p>
<p>Despite still predicting step by step at inference time, this work mainly focuses on prediction tasks and does not conduct RL experiments.SynthER [48] is in the same spirit as MBPO [34], which models the collected transition-level data distribution via an unconditioned diffusion model, and augments the training dataset by its samples.We focus on simulating the future trajectory for enhancing the model-based value estimation, and our diffusion model is conditioning on s t and a t .PolyGRAD [58] learns a DM to predict a sequence of future states s t<code>1 , . . ., s t</code>T ´1 and rewards r t , . . ., r t<code>T ´1, conditioning on the initial state s t and corresponding actions a t , . . ., a t</code>T ´1.Given that the actions are also unknown, PolyGRAD alternates between predicting the actions (via stochastic Langevin dynamics using policy score) and denoising the states and rewards during the DM's sampling process.This approach results in generating on-policy trajectories.In contrast, our approach is off-policy, since it does not interact with the policy during the sampling process.Policy-guided diffusion (PGD) [32] shares the same intention as PolyGrad, which is to generate on-policy trajectories.To achieve this, it trains an unconditioned DM using the offline dataset but samples from it under the (classifier) guidance of policy-gradient.Diffuser [33] and Decision Diffuser (DD) [1] are most close to our work, as they also predict future trajectories.However, the modeling details and the usage of generated trajectories significantly differs.Diffuser trains an unconditioned model that predicts both states and actions, resulting in a policy that uses the generated next action directly.DD models state-only future trajectories conditioning on s t , while we model future states and rewards conditioning on s t and a t .DD predicts the action by an inverse dynamics model given current state and predicted next state, hence the diffusion model needs to be invoked at inference time.Our approach, instead, can connect with any MF offline RL methods that is fast to execute for inference.</p>
<p>Preliminaries</p>
<p>Offline RL.We consider an infinite-horizon Markov decision process (MDP) defined by pS, A, R, P, p 0 , γq, where S is the state space, A is the action space.Let ∆pSq be the probability simplex of the state space.R : S ˆA Þ Ñ R is a deterministic reward function, P : S ˆA Þ Ñ ∆pSq defines the probability distribution of transition, p 0 : S Þ Ñ ∆pSq defines the distribution of initial state s 0 , and γ P p0, 1q is the discount function.The task of RL is to learn a policy π : S Þ Ñ A that maximizes its return Jpπq " E s0"p0psq,at"πp¨|stq,st`1"P p¨|st,atq "ř In offline RL, we are constrained to learn a policy solely from a static dataset generated by certain unknown policies.Throughout this paper, we use D offline to denote the offline data distribution and use D offline to denote the offline dataset.</p>
<p>Diffusion Model.Diffusion probabilistic models [30,64,66] are generative models that create samples from noises by an iterative denoising process.It defines a fixed Markov chain, called the forward or diffusion process, that iteratively adds Gaussian noise to x pkq starting from a data point x p0q : x pk<code>1q |x pkq " N</code>?1 ´βk x pkq , β k I ˘, 0 ď k ď K ´1.As the number of diffusion steps K Ñ 8, x pKq essentially becomes a random noise.We learn the corresponding reverse process that transforms random noise to data point: x pk´1q |x pkq " N <code>µθ px pkq q, Σ θ px pkq q ˘, 1 ď k ď K. Sampling from a diffusion model amounts to first sampling a random noise x pKq " N p0, Iq then running the reverse process.Let φpz; µ, Σq denote the density function of a random variable z " N pµ, Σq.To learn the reverse process, we parameterize p θ px pk´1q |x pkq q " φ</code>xpk´1q ; µ θ px pkq q, Σ θ px pkq q ˘, 1 ď k ď K, and optimize the variational lower bound of the marginal likelihood p θ px p0q:pKq q.There are multiple equivalent ways to optimize the lower bound [41], and we take the noise prediction route as follows.One can rewrite x pkq " ?s α k x p0q <code>?1 ´s α k ε, where s α k " ś K k 1 "1 p1 ´βk 1 q, and ε " N p0, Iq is the noise injected for x pkq (before scaling).We then parameterize a neural network ε θ px pkq , kq to predict ε injected for x pkq .Moreover, a conditional variable y can be easily added into both processes via formulating the corresponding density functions qpx pk</code>1q |x pkq , yq and p θ px pk´1q |x pkq , yq, respectively.We further deploy classifier-free guidance [31] to promote the conditional information, which essentially learns both conditioned and unconditioned noise predictors.More precisely, we optimize the following loss function: where x p0q and y are the true data point and conditional information sampled from data distribution, ε " N p0, Iq is the injected noise, k is the diffusion step sampled uniformly between 1 and K, b " Bernoullipp uncond q is used to indicate whether we will use null condition, and finally, x pkq " ?s α k x 0 <code>?1 ´s α k ε.Algorithm A.1 details how to sample from a guided diffusion model.In section 4, we shall introduce the form of x p0q and y in the context of offline RL, and discuss how we utilize diffusion models to ease planning.
E px p0q ,yq,k,ε,b › › ›εθ</code>xpkq px p0q , εq, k, p1 ´bq ¨y `b ¨∅˘´ε › › › 2 2 ,(1)</p>
<p>Diffusion World Model</p>
<p>In this section, we introduce a general recipe for model-based offline RL with diffusion world model.Our framework consists of two training stages, which we will detail in Section 4.1 and 4.2, respectively.In the first stage, we train a diffusion model to predict a sequence of future states and rewards, conditioning on the current state, action and target return.Next, we train an offline policy using an actor-critic method, where we utilize the pretrained diffusion model for model-based value estimation.Algorithm 4.1-4.2presents this framework with a simple actor-critic algorithm with delayed updates, where we assume a deterministic offline policy.Our framework can be easily extended in a variety of ways.First, we can generalize it to account for stochastic policies.Moreover, the actor-critic algorithm we present is of the simplest form.It can be extended to combine with various existing offline learning algorithms.In Section 5, we discuss three instantiations of Algorithm 4.2, which embeds TD3+BC [16], IQL [42] and Q-learning with pessimistic reward [83] respectively.</p>
<p>Conditional Diffusion Model</p>
<p>We train a return-conditioned diffusion model p θ px p0q |s t , a t , yq on length-T subtrajectories, where the conditioning variable is the RTG of a subtrajectory.That is, y " g t and x p0q " pr t , s t<code>1 , r t</code>1 , . . ., s t<code>T ´1, r t</code>T ´1q.As introduced in Section 3, we employ classifier-free guidance to promote the role of RTG.Stage 1 of Algorithm 4.2 describes the training procedure in detail.For the actual usage of the trained diffusion model in the second stage of our pipeline, we predict future T ´1 states and rewards based on a target RTG g eval and also current state s t and action a t .These predicted states and rewards are used to facilitate the value estimation in policy training, see Section 4.2.As the future actions are not needed, we do not model them in our world model.To enable the conditioning of s t and a t , we slightly adjust the standard sampling procedure (Algorithm A.1), where we fix s t and a t as conditioning for every denoising step in the reverse process, see Algorithm A.2.</p>
<p>Model-Based RL with Diffusion World Model</p>
<p>As summarized in Algorithm 4.2, we propose an actor-critic algorithm where the critic is trained on synthetic data generated by the diffusion model, and the actor is then trained with policy evaluation based on the critic.In a nutshell, we estimate the Q-value by the sum of a short-term return, simulated by the DWM, and a long-return value, estimated by a proxy state-action value function p Q learned through temporal difference (TD) learning.It is worth noting that in our framework, DWM only intervenes the critic training, and Algorithm 4.2 is general to connect with any MF value-based algorithms.We shall present 3 different instantiations of it in Section 5.</p>
<p>Definition 4.1 (H-step Diffusion Model Value Expansion</p>
<p>).Let ps t , a t q be a state-action pair.Sample p r t , p s t<code>1 , p r t</code>1 , . . .,p s t<code>T ´1, p r t</code>T ´1 from the diffusion model p θ p¨|s t , a t , g eval q.Let H be the simulation horizon, where H ă T .The H-step diffusion model value expansion estimate of the value of ps t , a t q is given by
p Q H diff ps t , a t q " E p θ p¨|st,at,gevalq " ř H´1 h"0 γ h p r t<code>h</code>γH p Qpp s t<code>H , p a t</code>H q ı ,(2)
where p a t<code>H " πpp s t</code>H q and p Qpp s t<code>H , p a t</code>H q is the proxy value for the final state-action pair.</p>
<p>We employ this expansion to compute the target value in TD learning, see Algorithm 4.2.This mechanism is key to the success of our algorithm and has several appealing properties.</p>
<p>1.In deploying the standard model-based value expansion (MVE, Feinberg et al. [14]), the imagined trajectory is derived by recursively querying the one-step dynamics model p one ps t`1 , r t |s t , a t q, which is the root cause of error accumulation.As an advantage over MVE, our DWM generates the imagined trajectory (without actions) as a whole.</p>
<ol>
<li>More interestingly, MVE uses the policy predicted action p a t " πpp s t q when querying p one .This can be viewed as an on-policy value estimation of π in a simulated environment.In contrast, Diffusion-MVE operates in an off-policy manner, as π does not influence the sampling process.As we will explore in Section 5, the off-policy diffusion-MVE excels in offline RL, significantly surpassing the performance of one-step-MVE.We will now delve into two interpretations of this, each from a unique perspective.</li>
</ol>
<p>(a) Our approach can be viewed as a policy iteration algorithm, alternating between policy evaluation (Algorithm 4.2 line 7-9) and policy improvement (line 10) steps.Here, p Q H diff is the estimator of the policy value function Q π , with adjustable lookahead horizon H and pessimistic or optimistic estimation through changing g eval .Parameterized Q ϕ is optimized towards the target p Q H diff through a mean squared error.In the context of offline RL, TD learning often lead to overestimation of Q π [69,44].This is because π might produce out-of-distribution actions, leading to erroneous values for p Q, and the policy is defined to maximize p Q.Such overestimation negatively impacts the generalization capability of the resulting policy when it is deployed online.To mitigate this, a broad spectrum of offline RL methods apply various forms of regularization to the value function [19,42,44], to ensure the resulting policy remains close to the data.As the DWM is trained exclusively on offline data, it can be seen as a synthesis of the behavior policy that generates the offline dataset.In other words, diffusion-MVE introduces a type of value regularization for offline RL through generative modeling.</p>
<p>Moreover, our approach significantly differs from existing value pessimism notions.One challenge of offline RL is that the behavior policy that generates the offline dataset is often of low-to-moderate quality, so that the resulting dataset might only contain trajectories with low-to-moderate returns.As a result, many regularization techniques introduced for offline RL are often overly pessimistic [20,54].To address this issue, we typically condition on large out-of-distribution (OOD) values of g eval when sampling from the DWM.Putting differently, we ask the DWM to output an imagined trajectory under an optimistic goal.</p>
<p>(b) Alternatively, we can also view the approach as an offline Q-learning algorithm [73], where p Q is estimating the optimal value function Q ˚using off-policy data.Again, the off-policy data is generated by the diffusion model, conditioning on OOD RTG values.In essence, our approach can be characterized as offline Q-learning on synthetic data.</p>
<p>Comparison with Transformer-based World Models.Curious readers may wonder about the key distinctions between DMW and existing Transformer-based world models [5,50,59].These models, given the current state s t and action a t , leverage the autoregressive structure of Transformer to incorporate past information to predict s t`1 .To forecast multiple steps into the future, they must make iterated predictions.In contrast, DWM makes long-horizon predictions in a single query.It is worth noting that it is entirely possible to substitute the diffusion model in our work with a Transformer, and we justify our design choice in Section 5.4.</p>
<p>Experiments</p>
<p>Our experiments are designed to answer the following questions.To answer these questions, we consider three instantiations of Algorithm 4.2, where we integrate TD3+BC [16], IQL [42] and Q-learning with pessimistic reward (which we refer to as PQL) as the offline RL algorithm in the second stage.These algorithms come with different conservatism notions defined on the action (TD3+BC), the value function (IQL), and the reward (PQL), respectively.Specifically, the PQL algorithm is inspired by the MOPO algorithm [83], where we penalize the world model predicted reward by the uncertainty of its prediction.Nonetheless, it is distinct from MOPO in the critic learning.MOPO uses standard TD learning on model-generated transitions, whereas we use MVE or Diff-MVE for value estimation.In the sequel, we refer to our algorithms as DWM-TD3BC, DWM-IQL and DWM-PQL respectively.For DWM-IQL, we have observed performance enhancement using a variant of Diff-MVE based on the λ-return technique [62], therefore we incorporate it as a default feature.Detailed descriptions of these algorithms are deferred to Appendix C. We present the comparisons in Section 5.1-5.3, and ablate the design choices we made for DWM in Section 5.4.In Appendix E.4, we conduct experimental comparison with additional baselines including data augmentation [48] and autoregressive diffusion [58] methods.</p>
<p>Benchmark and Hyperparameters.We conduct experiments on 9 datasets of locomotion tasks from the D4RL [15] benchmark, and report the obtained normalized return (0-1 with 1 as expert performance).Throughout the paper, we train each algorithm for 5 instances with different random seeds, and evaluate them for 10 episodes.All reported values are means and standard deviations aggregated over 5 random seeds.We set the sequence length of DWM to be T " 8 (discussed in Section 5.1).The number of diffusion steps is K " 5 for training.For DWM inference, an accelerated inference technique is applied with a reduced number of diffusion steps N " 3, as detailed in Section 5.4.The training and sampling details of DWM refer to Appendix A, and the training details of each offline algorithm refer to Appendix D. We further conduct extensive experiments on sparse-reward tasks, and results are detailed in Appendix Sec.E.5.</p>
<p>DWM v.s. One-Step Dynamics Model</p>
<p>We first investigate the effectiveness of DWM in reducing the compounding error for MBRL, and compare it with the counterparts using one-step dynamics model.Next, we evaluate the performance of our proposed Algorithm 4.2 and the one-step dynamics model counterparts, where we substitute DWM by one-step dynamics models and use standard MVE.We call these baselines OneStep-TD3BC, OneStep-IQL and OneStep-PQL, correspondingly.</p>
<p>Long Horizon Planning and Compounding Error Comparison.To explore the response of different world models to long simulation horizons, we compare the performance DWM methods (DWM-TD3BC and DWM-IQL) with their one-step counterparts (OneStep-TD3BC and OneStep-IQL) when the simulation horizon H used in policy training changes.To explore the limit of DWM models, we train another set of DWMs with longer sequence length T " 32 and investigate the performance of downstream RL algorithms for H P t1, 3, 7, 15, 31u.The algorithms with one-step dynamics models have simulation horizon from 1 to 5. Figure 5.1 plots the results across 9 tasks.OneStep-IQL and OneStep-TD3BC exhibit a clearly performance drop as the simulation horizon increases.For most tasks, their performances peak with relatively short simulation horizons, like one or two.This suggests that longer model-based rollout with one-step dynamics models suffer from severe compounding errors.On the contrary, DWM-TD3BC and DWM-IQL maintain relatively high returns without signif-  icant performance degradation, even using horizon length 31.Note that in the final result Table 5.1, we report results using DWM with sequence length T " 8, because the performance gain of using T " 32 is marginal.See Appendix E.7 for details.We additionally conduct experiments on analyzing the compounding error for DWM and one-step model predictions.The results in Appendix E.2 indicate the superior performance of DWM in reducing the compounding errors, which verifies our hypothesis.</p>
<p>Offline RL Performance.Table 5.1 reports the performance of Algorithm 4.2 using DWM and onestep dynamics models on the D4RL datasets.We sweep over the simulation horizon H P t1, 3, 5, 7u and a set of evaluation RTG values.The RTG values we search vary across environments, see Table D.2.The predominant trends we found are: the proposed DWM significantly outperforms the one-step counterparts, with a notable 44% performance gain.This is attributed to the strong expressivity of diffusion models and the prediction of entire sequences all at once, which circumvents the compounding error issue in multistep rollout.This point will be further discussed in the studies of simulation horizon as next paragraph.</p>
<p>DWM v.s. Decision Diffuser</p>
<p>We further compare DWM-TD3BC (the best-performing DWM-based algorithms) with Decision Diffuser (DD) [1], another closely related approach that also use diffusion models to model the trajectory in the offline dataset.As noted in Section 1, our approach is significantly different from theirs, though.DWM conditions on both state s t and action a t , where DD only conditions on s t .More importantly, we train a downstream model-free policy using imagined rollout, whereas DD predicts the action via an inverse dynamics model, using current state s t and predicted next state p s t`1 .This means, at inference time, DD needs to generate the whole trajectory, which is computationally inefficient.On the contrary, DWM based approaches are efficient as we do not need to sample from the trained DWM anymore.Table 5.2 reports the performance and the inference time of DWM-TD3BC and DD.The inference time is averaged over 600 evaluation episodes.The performance of DWM-TD3BC is comparable to Decision Diffuser, and it enjoys 4.6x faster inference speed.We anticipate the difference in speed amplifies for higher dimensional problems.</p>
<p>DWM v.s. Model-Free Counterparts</p>
<p>Finally, we compare DWM-based algorithms with their MF counterparts, namely, TD3+BC vs DWM-TD3BC, and IQL vs DWM-IQL.To verify this hypothesis, we replace the diffusion model with Transformer in our proposed algorithms, and compare the resulting performance with DWM methods.We particularly consider the combination with TD3+BC and IQL, where we call the obtained algorithms T-TD3BC and T-IQL.We test T-TD3BC and T-IQL with parameter sweeping over simulation horizon H P t1, 3, 5, 7u, as the same as DWM methods.For the evaluation RTG, we take the value used in Decision Transformer [6] and apply the same normalization as used for DWM.</p>
<p>As shown in Table 5.4, DWM consistently outperforms Transformer-based world models across offline RL algorithm instantiations and environments.The experiment details refer to Appendix E.3.</p>
<p>Additional Ablation Experiments.Due to the space limit, we refer the readers to Appendix for other ablation experiments regarding other design choices of our approach.Appendix E.6 discusses the number of diffusion steps we use in training DWM and trajectory sampling.Appendix E.8 discusses the evaluation RTG values we use when sampling from the DWM.Appendix E.9 ablates the λ-return technique we incorporate for DWM-IQL.Last, we also investigate the effects of fine-tuning DWM with relabelled RTGs [80].We have found this technique is of limited utility, so we did not include it in the final design for the simplicity of our algorithm.See the results in Appendix E.10.</p>
<p>Conclusion and Future Work</p>
<p>We present a general framework of leveraging diffusion models as world models, in the context of offline RL.This framework can be easily extended to accommodate online training.Specifically, we utilize DWM generated trajectories for model-based value estimation.Our experiments show that this approach effectively reduces the compounding error in MBRL.We benchmarked DWM against the traditional one-step dynamics model, by training 3 different types of offline RL algorithms using imagined trajectories generated by each of them.DWM demonstrates a notable performance gain.DWM-based approaches are also caparable with or marginally outperforming their MF counterparts.However, there are also limitations of our work.Currently, DWM is trained for each individual environment and is task-specific.An intriguing avenue for future research would be extending DWM to multi-environment and multi-task settings.Additionally, to circumvent the side effects of exploration, we only investigate DWM in the offline RL setting.This raises an interesting question regarding the performance of DWM in online settings.Lastly but most importantly, although we adopt the stride sampling technique to accelerate the inference, the computational demand of DWM remains high.Further enhancements to speed up the sampling process could be crucial for future usages of DWM to tackle larger scale problems.</p>
<p>A Implementation Details of Diffusion World Model</p>
<p>We summarize the architecture and hyperparameters used for our experiments.For all the experiments, we use our own PyTorch implementation that is heavily influenced by the following codebases: (3)</p>
<p>There are two reasons we choose not to model future actions in the sequence.First, our proposed diffusion model value expansion (Definition 4.1) does not require the action information for future steps.Second, previous work have found that modeling continuous action through diffusion is less accurate [1].</p>
<p>Throughout the paper, we train guided diffusion models for state-reward sequences of length T " 8.</p>
<p>The number of diffusion steps is K " 5.The probability of null conditioning p uncond is set to 0.25, and the batch size is 64.We use the cosine noise schedule proposed by Nichol and Dhariwal [56].The discount factor is γ " 0.99, and we normalize the discounted RTG by a task-specific reward scale, which is 400 for Hopper, 550 for Walker, and 1200 for Halfcheetah tasks.</p>
<p>Following Ajay et al. [1], our noise predictor ε θ is a temporal U-net [33,60] that consists of 6 repeated residual blocks, where each block consists of 2 temporal convolutions followed by the group norm [76] and a final Mish nonlinearity activation [51].The diffusion step k is first transformed to its sinusoidal position encoding and projected to a latent space via a 2-layer MLP, and the RTG value is transformed into its latent embedding via a 3-layer MLP.In our diffusion world model, the initial action a t as additional condition is also transformed into latent embedding via a 3-layer MLP, and further concatenated with the embeddings of the diffusion step and RTG.</p>
<p>Optimization.We optimize our model by the Adam optimizer with a learning rate 1 ˆ10 ´4 for all the datasets.The final model parameter s θ we consider is an exponential moving average (EMA) of the obtained parameters over the course of training.For every 10 iteration, we update s θ " β s θ `p1 ´βqθ, where the exponential decay parameter β " 0.995.We train the diffusion model for 2 ˆ10 6 iterations.</p>
<p>Sampling with Guidance.To sample from the diffusion model, we need to first sample a random noise xpKq " N p0, Iq and then run the reverse process.Algorithm A.1 presents the general process of sampling from a diffusion model trained under classifier-free guidance.</p>
<p>In the context of offline RL, the diffusion world model generates future states and rewards based on the current state s t , the current action a t and the target return g eval , see Section 4. Therefore, the sampling process is slightly different from Algorithm A.1, as we need to constrain the initial state and initial action to be s t and a t , respectively.The adapted algorithm is summarized in Algorithm A.2.Following Ajay et al. [1], we apply the low temperature sampling technique for diffusion models.The temperature is set to be α " 0.5 for sampling at each diffusion step from Gaussian N pp µ θ , α 2 p Σ θ q, with p µ θ and p Σ θ being the predicted mean and covariance.</p>
<p>Accelerated Inference.Algorithm A.1 and A.2 run the full reverse process, Building on top of them, we further apply the stride sampling technique as in Nichol and Dhariwal [56] to speed up sampling process.Formally, in the full reverse process, we generates x pk´1q by x pkq one by one, from k " K till k " 1:
x pk´1q " ? s α k´1 β k 1 ´s α k p x p0q <code>?α k p1 ´s α k´1 q 1 ´s α k x pkq</code>σk ε, ε " N p0, Iq,(4)
where p x p0q is the prediction of the true data point (line 5 of Algorithm A.1), σ k " c β k p1 ´s α k´1 q 1 ´s α k is the standard deviation of noise at step k (line 8 of in Algorithm A.1).We note that s α k " ś K k 1 "1 p1 ´βk 1 q where the noise schedule tβ k u K k"1 is predefined, see Section 3 and Appendix A.</p>
<p>Running a full reverse process amounts to evaluating Equation (4) for K times, which is time consuming.To speed up sampling, we choose N diffusion steps equally spaced between 1 and K, namely, τ 1 , . . ., τ N , where τ N " K.We then evaluate Equation (4) for the chosen steps τ 1 , . . ., τ N .This effectively reduces the inference time to approximately N {K of the original.In our experiments, we train the diffusion model with K " 5 diffusion steps and sample with N " 3 inference steps, see Section 5.4 for a justification of this number.Sample from the posterior distribution qpx pk´1q |x pkq , x p0q q // See Equation ( 6) and ( 7) of Ho et al. [30]  x pKq r0 : dimps t q <code>dimpa t qs Ð concatenateps t , a t q for k " K, . . ., 1 do p ε Ð ω ¨εθ px pkq , k, g eval q</code>p1 ´ωq ¨εθ px pkq , k, ∅q // estimate true data point x p0q</p>
<p>Algorithm
p µ Ð ? s α k´1 β k 1 ´s α k p x p0q <code>?α k p1 ´s α k´1 q 1 ´s α k x pkq p Σ Ð β k p1 ´s α k´1 q 1 ´s α k I x pk´1q " N pp µ,p x p0q Ð 1 ? s α k</code>xpkq ´?1 ´s α t p ε //
Sample from the posterior distribution qpx pk´1q |x pkq , x p0q q // See Equation ( 6) and ( 7) of Ho et al. [30]
p µ Ð ? s α k´1 β k 1 ´s α k p x p0q `?α k p1 ´s α k´1 q 1 ´s α k x pkq p Σ Ð β k p1 ´s α k´1 q 1 ´s α k I
x pk´1q " N pp µ, p Σq // apply conditioning of st and at</p>
<p>x pk´1q r0 : dimps t q `dimpa t qs Ð concatenateps t , a t q Output: x p0q</p>
<p>B Implementation Details of One-step Dynamics Model</p>
<p>The traditional one-step dynamics model f θ ps t<code>1 , r t |s t , a t q is typically represented by a parameterized probability distribution over the state and reward spaces, and optimized through log-likelihood maximization of the single-step transitions: max θ E pst,at,rt,st</code>1q"Doffline rlog f θ ps t`1 , r t |s t , a t qs ,</p>
<p>where ps t , a t , r t , s t`1 q is sampled from the offline data distribution D offline .As in Kidambi et al. [40], we model f θ as a Gaussian distribution N pµ θ , Σ θ q, where the mean µ θ and the diagonal covariance matrix Σ θ are parameterized by two 4-layer MLP neural networks with 256 hidden units per layer.We use the ReLU activation function for hidden layers.The final layer of Σ θ is activated by a SoftPlus function to ensure validity.We train the dynamics models for 1 ˆ10 6 iterations, using the Adam optimizer with learning rate 1 ˆ10 ´4.</p>
<p>C Diffusion World Model Based Offline RL Methods</p>
<p>In Section 5, we consider 3 instantiations of Algorithm 4.2 where we integrate TD3+BC, IQL, Q-learning with pessimistic reward (PQL) into our framework.These algorithms are specifically designed for offline RL, with conservatism notions defined on actions (TD3+BC), value function (IQL), and rewards (PQL) respectively.In the sequel, we refer to our instantiations as DWM-TD3BC, DWM-IQL and DWM-PQL.The detailed implementation of them will be introduced below.</p>
<p>C.1 DWM-TD3BC: TD3+BC with Diffusion World Model</p>
<p>Building on top of the TD3 algorithm [17], TD3+BC [16] employs explicit behavior cloning regularization to learn a deterministic policy.The algorithm works as follows.</p>
<p>The critic training follows the TD3 algorithm exactly.We learn two critic networks Q ϕ1 and Q ϕ2 as double Q-learning [17] through TD learning.The target value in TD learning for a transition ps, a, r, s 1 q is given by:
y " r <code>γ min iPt1,2u Q ϕi</code>s1 , a 1 " Clippπ s ψ ps 1 q <code>ε, ´C, Cq ˘,(6)
where ε " N p0, σ 2 Iq is a random noise, π s ψ is the target policy and Clip(¨) is an operator that bounds the value of the action vector to be within r´C, Cs for each dimension.Both Q ϕ1 and Q ϕ2 will regress into the target value y by minimizing the mean squared error (MSE), which amounts to solving the following problem: min ϕi E ps,a,r,s 1 q"Doffline "</code>ypr, s 1 q ´Qϕi ps, aq ˘2ı , i P t1, 2u.</p>
<p>For training the policy, TD3+BC optimizes the following regularized problem:
max ψ E ps,aq"Doffline " λQ ϕ1 ps, π ψ psqq ´}a ´πψ psq} 2 ı ,(8)
which D offline is the offline data distribution.Without the behavior cloning regularization term }a ´πψ psq} 2 , the above problem reduces to the objective corresponding to the deterministic policy gradient theorem [63].Note that π is always trying to maximize one fixed proxy value function.</p>
<p>Both the updates of target policy π</p>
<p>Algorithm C.1: DWM-TD3BC</p>
<p>Inputs: offline dataset D offline , pretrained diffusion world model p θ , simulation horizon H, conditioning RTG g eval , policy and target networks update frequency n, coefficient λ, parameters for action perturbation and clipping: σ, C Initialize the actor and critic networks π ψ , Q ϕ1 , Q ϕ2 Initialize the weights of target networks s ψ Ð ψ, s
ϕ 1 Ð ϕ 1 , s ϕ 2 Ð ϕ 2 for i " 1,
where L τ puq " |τ ´1uă0 |u 2 with hyperparameter τ P p0.5, 1q, As τ Ñ 1, V ξ psq is essentially estimating the maximum value of Qps, aq.This can be viewed as implicitly performing the policy improvement step, without an explicit policy.Using a hyperparameter τ ă 1 regularizes the value estimation (of an implicit policy) and thus mitigates the overestimation issue of Q function.The Q function is updated also using Eq. ( 7) but with target y " r `γV ξ ps 1 q.Finally, given the Q and the V functions, the policy is extracted by Advantage Weighted Regression [57], i.e., solving max ψ E ps,aq"Doffline rexp pβpQ ϕ ps, aq ´Vξ psqqq log π ψ pa|sqs .</p>
<p>The update of the target critic networks Q s ϕi are delayed in IQL.The whole algorithm is summarzied in Algorithm C.2.</p>
<p>C.3 DWM-PQL: Pessimistic Q-learning with Diffusion World Model</p>
<p>Previous offline RL algorithms like MOPO [83] have applied the conservatism notion directly to the reward function, which we referred to as pessimistic Q-learning (PQL) in this paper.Specifically, the original algorithm proposed by Yu et al. [83] learns an ensemble of m one-step dynamics models tp θi u iPrms , and use a modified reward rps, aq " p rps, aq ´κups, a|p θ1 , . . ., p θm q (11)</p>
<p>for learning the Q functions, where p r is the mean prediction from the ensemble, ups, a|p θ1 , . . ., p θm q is a measurement of prediction uncertainty using the ensemble.
if i mod n then s ϕ 1 Ð s ϕ 1 <code>wpϕ ´s ϕ 1 q s ϕ 2 Ð s ϕ 2</code>wpϕ ´s ϕ 2 q
Output: π ψ Yu et al. [83] parameterize each dynamics model by a Gaussian distribution, and measure the prediction uncertainty using the maximum of the Frobenious norm of each covariance matrix.Since the diffusion model does not have such parameterization, and it is computationally daunting to train an ensemble of diffusion models, we propose an alternative uncertainty measurement similar to the one used in MoRel [40].</p>
<p>Given ps t , a t q and g eval , we randomly sample m sequences from the DWM, namely, p r i t , p s i t<code>1 , p r i t</code>1 , . . ., p s i t<code>T ´1, p r i t</code>T ´1, i P rms.</p>
<p>(12) Then, we take the 1st sample as the DWM output with modified reward:
rt 1 " m ÿ i"1 1 m p r i t 1 ´κ max iPrms,jPrms ˆ› › ›p r i t 1 ´p r j t 1 › › › 2 <code>› › ›p s i t 1</code>1 ´p s j t 1 <code>1› › › 2 2
˙, t 1 " t, . . ., t</code>T ´2.(13) This provides an efficient way to construct uncertainty-penalized rewards for each timestep along the diffusion predicted trajectories.Note that this does not apply to the reward predicted for the last timestep.The rest of the algorithm follows IQL but using MSE loss instead of expectile loss for updating the value network.</p>
<p>The DWM-PQL algorithm is summarized in Algorithm C.3</p>
<p>For baseline methods with one-step dynamics model, the imagined trajectories starting from sample ps t , a t q " D offline are derived by recursively sample from the one-step dynamics model f θ p¨|s, aq and policy π ψ p¨|sq: p τ ps t , a t q " pf θ ˝πψ q H´1 ps t , a t q.By keeping the rest same as above, it produces MBRL methods with one-step dynamics, namely O-IQL, O-TD3BC and O-PQL.</p>
<p>D Training and Evaluation Details of Offline RL Algorithms D.1 Common Settings</p>
<p>We conduct primary tests on TD3+BC and IQL for selecting the best practices for data normalization.Based on the results, TD3+BC, O-TD3BC and DWM-TD3BC applies observation normalization, while other algorithms (O-PQL, DWM-PQL, IQL, O-IQL and DWM-IQL) applies both observation and reward normalization.</p>
<p>Algorithm C.3: DWM-PQL</p>
<p>Inputs: offline dataset D offline , pretrained diffusion world model p θ , simulation horizon H, conditioning RTG g eval , target network update frequency n, pessimism coefficient λ, number of samples for uncertainty estimation m Initialize the actor, critic and value networks π ψ , Q ϕ1 , Q ϕ2 , V ξ Initialize the weights of target networks s  All algorithms are trained with a batch size of 128 using a fixed set of pretrained dynamics models (one-step and diffusion).The discount factor is set as γ " 0.99 for all data.
ϕ 1 Ð ϕ 1 , s ϕ 2 Ð ϕ 2 for i " 1,if i mod n then s ϕ 1 Ð s ϕ 1 <code>wpϕ ´s ϕ 1 q s ϕ 2 Ð s ϕ 2</code></p>
<p>D.2 MF Algorithms</p>
<p>TD3+BC and IQL are trained for 1 ˆ10 6 iterations, with learning rate 3 ˆ10 ´4 for actor, critic and value networks.The actor, critic, and value networks are all parameterized by 3-layer MLPs with 256 hidden units per layer.We use the ReLU activation function for each hidden layer.IQL learns a stochastic policy which outputs a Tanh-Normal distribution, while TD3+BC has a deterministic policy with Tanh output activation.The hyperparameters for TD3+BC and IQL are provided in Table D The baseline DD [1] algorithm uses diffusion models trained with sequence length T " 32 and number of diffusion steps K " 5.It requires additionally training an inverse dynamics model (IDM) for action prediction, which is parameterized by a 3-layer MLP with 1024 hidden units for each hidden layer and ReLU activation function.The dropout rate for the MLP is 0.1.The IDMs are trained for 2 ˆ10 6 iterations for each environment.For a fair comparison with the other DWM methods, DD uses N " 3 internal sampling steps as DWM.We search over the same range of evaluation RTG g eval for DD and the other DWM methods.</p>
<p>D.3 MB algorithms</p>
<p>DWM-TD3BC, DWM-IQL and DWM-PQL are trained for 5 ˆ10 5 iterations.Table D.2 summarizes the hyperparameters we search for each experiment.The other hyperparameters and network architectures are the same as original TD3+BC and IQL in above sections.DWM-IQL with λ-return takes λ " 0.95, following Hafner et al. [25].</p>
<p>The counterparts with one-step dynamics models are trained for 2 ˆ10 5 iterations due to a relatively fast convergence from our empirical observation.Most of the hyperparameters also follow TD3+BC and IQL.The PQL-type algorithms (O-PQL and DWM-PQL) further search the pessimism coefficient κ (defined in Eq. ( 13)) among t0.01, 0.1, 1.0u.</p>
<p>Env</p>
<p>Evaluation RTG H hopper-medium-v2 [0.6, 0.</p>
<p>E Additional Experiments E.1 Detailed Results of Long Horizon Planning with DWM</p>
<p>The section provides the detailed results of the experiments for long horizon planning with DWM in Section 5.1.Table E</p>
<p>E.2 World Modeling: Prediction Error Analysis</p>
<p>An additional experiment is conducted to evaluate the prediction errors of the observations and rewards with DWM under simulation horizon H " 8, for an example task walker2d-medium-expert-v2.</p>
<p>We randomly sample a subsequence ps t , . . ., s t<code>7 q from the offline dataset, and let DWM predict the subsequent states and rewards, conditioned on the first state s t and true action a t .For the one-step model, the model iteratively predicts the reward r t and next state s t</code>1 , conditioned on the current state s t (which is predicted in the previous step) and true action a t .</p>
<p>We report the mean squared error (MSE) between the predicted samples and the ground truth for each rollout timestep in Table E.2.Each method is evaluated with five models and 100 sequences per model, and the mean and standard deviations are reported.The average prediction errors over the entire sequence are also calculated in the last row.It shows the significant reduction of prediction errors in sequence modeling by using DWM over traditional one-step models, especially when the prediction timestep is large.</p>
<p>Step Following the same protocol as DWM, the Transformer model is trained to predict future state-reward sequences, conditioning on the initial state-action pair.We use a 4-layer transformer architecture with 4 attention heads, similar to the one in Zheng et al. [87].Specifically, all the actions except for the first one are masked out as zeros in the state-action-reward sequences.Distinct from the original DT [6] where the loss function only contains the action prediction error, here the Transformer is trained with state and reward prediction loss.The Transformers are trained with optimizers and hyperparameters following ODT [87].The evaluation RTG for Transformers takes values 3600{400 " 9.0, 5000{550 « 9.1, 6000{1200 " 5.0 for hopper, walker2d and halfcheetah environments, respectively.The complete results of T-TD3BC and T-IQL are provided in Table E</p>
<p>E.4 Additional Baselines</p>
<p>E.4.1 Data Augmentation</p>
<p>As a data augmentation (DA) method, SynTHER [48] learns an unconditional diffusion model at the transition level, where the generated data are used for augmenting the training distribution.We conduct additional experiments for DWM and SynTHER-type data augmentation for this section.</p>
<p>The two data augmentation methods based on TD3+BC and IQL are referred to as DA-TD3BC and DA-IQL.For the sake of fair comparison, for DA-TD3BC and DA-IQL, the models are trained in the same manner as DWM with the same parameter sweeping for RTG values and horizons.The results show that the DWM consistently performs better than the DA algorithms across all tasks, for both TD3+BC and IQL.</p>
<p>For the sake of fair comparison with DWM, we use the same data preprocessing as DWM for this experiment, which is different from state reward normalizations as the original SynTHER paper.In our experiments, we use CDF normalizers to transform each dimension of the state vector to r´1, 1s independently, i.e., making the data uniform over each dimension by transforming with marginal CDFs.Specifically, we transform the raw reward r raw to r " 2pr raw ´rmin q{pr max ´rmin q ´1, where r min and r max are max and min raw reward of the offline dataset.SynTHER applies "whitening" that makes each (non-terminal) continuous dimension mean 0 and std 1, and the terminal states are rounded without normalization.To enable fast sampling, we use a very low diffusion steps: 5 at training and 3 at testing.The original SynthER paper uses 128 steps, which requires more computational time for sample generation.We use a set of consistent parameters like model sizes and batch sizes cross all the environments, the same as our previous experiments.</p>
<p>E.4.2 Autoregressive Diffusion</p>
<p>We conduct experiments on Autoregressive Diffusion (AD) mentioned in previous work [58] as an additional Baseline.AD is essentially a one-step model using diffusion instead of MLP, which can be autoregressively rolled out and used for Model Based Value Expansion (MVE).We find that AD is computationally inefficient to be practically integrated into the MVE framework.We have checked the wallclock time (in seconds) for sampling a batch of 128 future trajectories with different values of horizon, for the walker2d-medium-v2 environment.</p>
<p>We set the number of trajectories to be 128 because this is the batch size we use for training RL agents.Results are averaged over 100 trials.For both approaches we use diffusion models of the same model size, where the number of sampling diffusion steps is 3 (to enable fast inference).This experiment is conducted on a A6000 GPU and time unit is second.</p>
<p>The results are displayed in Table E.6.The sampling time of DWM is a constant because it's a sequence model, and in practice we diffuse the whole sequence and take a part of it according to H; while the sampling time of AD scale linearly as H increases.When H " 7, the sampling time is roughly 6.67ˆcompared with DWM.The MB methods we reported in the paper are trained for 5 ˆ10 5 iterations (see Section D.3).That means, even only generating trajectories will take 27.5 hours if we use AD (as opposed to " 4 hours for DWM).This suggests that AD is too computationally expensive to be incorporated into the MBRL framework.The number of training diffusion steps K can heavily influence the modeling quality, where a larger value of K generally leads to better performance.At the same time, sampling from the diffusion models is recognized as a slow procedure, as it involves K internal denoising steps.We apply the stride sampling technique [56] to accelerate the sampling process with reduced internal steps N , see Appendix A for more details.However, the sampling speed comes at the cost of quality.It is important to strike a balance between inference speed and prediction accuracy.We investigate how to choose the number of K and N to significantly accelerate sampling without sacrificing model performance.</p>
<p>Method</p>
<p>We train DWM with different numbers of diffusion steps K P t5, 10, 20, 30, 50, 100u, where the sequence length is T " 8.We set four inference step ratios r infer P t0.2, 0.3, 0.5, 1.0u and use N " rr infer ¨Ks internal steps in stride sampling.Figure E.1 reports the prediction errors of DMW for both observation and reward sequences, defined in Equation (14).We note that the prediction error depends on the evaluation RTG, and we report the best results across multiple values of it (listed in Table D.2).An important observation is that r infer " 0.5 is a critical ridge for distinguishing the performances with different inference steps, where N ă K{2 hurts the prediction accuracy significantly.Moreover, within the regime r infer ě 0.5, a small diffusion steps K " 5 performs roughly the same as larger values.Therefore, we choose K " 5 and r infer " 0.5 for our main</p>
<p>E.6.1 Details Results</p>
<p>Let τ denote a length-T subtrajectory ps t , a t , r t , s t<code>1 , . . ., s t</code>T ´1, r t<code>T ´1q.The average prediction errors of a DWM p θ for states and rewards along sequences are defined as:
s ϵ s " E τ "Doffline,p st"p θ p¨|s1,a1,gevalq « 1 T t</code>T ´1 ÿ t 1 "t }p s t 1 ´st 1 } 2 ff ,ands ϵ r " E τ "Doffline,p st"p θ p¨|s1,a1,gevalq « 1 T t`T ´1 ÿ t 1 "t }p r t 1 ´rt 1 } 2 ff .(14)
We first note that all the prediction errors depend on the evaluation RTG g eval .For the ease of clean presentation, we search over multiple values of g eval , listed in Therefore, we choose T " 8 for our main experiments.</p>
<p>E.8 Ablation: OOD Evaluation RTG Values</p>
<p>We found that the evaluation RTG values play a critical role in determining the performance of our algorithm.Our preliminary experiments on trajectory preidction have suggested that in distribution evaluation RTGs underperforms OOD RTGs, see Appendix E.8.2.The results show that the actual return does not always match with the specified g eval .This is a well-known issue of return-conditioned RL methods [13,87,55].Nonetheless, OOD evaluation RTGs generally performs well.Figure E.3 shows both DWM-TD3BC and DWM-IQL are robust to OOD evaluation RTGs.We emphasize the reported return is averaged over training instances with different simulation horizons, where the peak performance, reported in Table 5.1 is higher.Our intuition is to encourage the diffusion model to take an optimistic view of the future return for the current state.On the other hand, the evaluation RTG cannot be overly high.As shown in task halfcheetah-mr, increasing RTG g eval ą 0.4 will further decrease the actual performances for both methods.The optimal RTG values vary from task to task, and the complete experiment results are provided below.D.2, the data maximum is usually smaller than the evaluation RTG values that leads to higher performances, as observed in our empirical experiments.It is worth noting that the prediction error naturally grows as the horizon increases.Intuitively, given a fixed environment, the initial state of the D4RL datasets are very similar, whereas the subsequent states after multiple timesteps become quite different.</p>
<p>E.9 Ablation: λ-Return Value Estimation</p>
<p>The Dreamer series of work [22,24,25] applies the λ-return technique [62] for value estimation, used the imagined trajectory.This technique can be seamlessly embedded into our framework as a modification of the standard Diff-MVE.More precisely, given a state-action pair ps t , a t q sampled from the offline dataset, we recursively compute the λ-target value for h " H, . . ., 0:  We conduct experiments to compare the vanilla diff-MVE and the λ-return variant for DWM-TD3BC and DWM-IQL, using λ " 0.95.We search over RTG values (specified in Appendix Table D.2) and simulation horizons 1, 3, 5, 7. The results are summarized in Table E.11.The λ-return technique is beneficial for DWM-IQL, but harmful for DWM-TD3BC.We speculate that since Equation (15) iteratively invokes the Q s ϕ or the V s ψ function, it favors approaches with more accurate value estimations.While IQL regularizes the value functions, TD3+BC only has policy regularization and is shown to be more prone to the value over-estimation issue in our experiments.Based on these results, we incorporated the λ-return technique into DWM-IQL, but let DWM-TD3BC use the vanilla Diff-MVE.We let DWM-PQL uses the vanilla Diff-MVE for the sake of algorithmic simplicity.Unlike dynamic programming in traditional RL, sequential modeling methods like diffusion models and DT are suspected to fail to stitch suboptimal trajectories.RTG relabeling is proposed to alleviate this problem for DT [80], through iteratively relabeling RTG g from training dataset to be:
p Q λ t
gt " r t <code>γ maxpg t</code>1 , p V ps t<code>1 qq " maxpg t , r t</code>p V ps t`1 q,</p>
<p>where the p V function is separately learned from the offline dataset using CQL [44], and the max operator is used to prevent underestimation due to the pessimism of p V .The original formulation by Yamagata et al. [80] does not include the γ term as DT uses undiscounted RTG, i.e., γ " 1.</p>
<p>We apply the RTG relabeling for DWM fine-tuning in the policy learning phase of vanilla DWM-IQL algorithm, without the λ-return technique.The value function p V comes from the IQL algorithm.We take the first 10% steps of the entire policy learning as warm up steps, where we do not apply RTG relabeling.This is because p V can be inaccurate at the beginning of training.The modified algorithm DWM-IQL(R) achieves an average score of 0.61, improved over score 0.57 for DWM-IQL(w/o λ), under exactly the same training and test settings.Results are provided in Table E.12 with details in Table E. 13.Nonetheless, the improvement is of limited unity compared with the λ-return, thus we do not include it in the final design.</p>
<p>s. Simulation Horizon (walker2d-medium-v2)One-step Dynamics Model Diffusion World Model</p>
<p>Figure 1 . 1 :
11
Figure 1.1:The return of TD3+BC trained using diffusion world model and one-step dynamics model.</p>
<p>( 1 )
1
Compared with the onestep dynamics model, does DWM effectively reduces the compounding error and lead to better performance in MBRL? (2) How does the proposed Algorithm 4.2 compare with other diffusion model model-based methods, and (3) their model-free counterparts?</p>
<p>Figure 5 . 1 :
51
Figure 5.1: Performances of Algorithm 4.2 with DWM and one-step models, using different simulation horizons.The x-axis has range r1, 31s in a logarithm scale.</p>
<p>sψ</p>
<p>and the target critic networks Q s ϕi are delayed in TD3+BC.The whole algorithm is summarized in Algorithm C.1.</p>
<p>wpϕ ´s ϕ 2 q
2
Output: π ψ All models are trained on NVIDIA Tesla V100 PCle GPU devices.For training 200 epochs (1000 iterations per epoch), model-free algorithms like TD3+BC and IQL typically takes around 2000 seconds, DWM model-based algorithms like DWM-TD3BC and DWM-IQL typically takes around 18000 seconds, transformer model-based algorithms like T-TD3BC and T-IQL typically takes around 8000 seconds, one-step model-based algorithms like O-TD3BC and O-IQL typically takes around 2300 seconds.The specific computational time varies from task to task.</p>
<p>Figure E. 1 :
1
Figure E.1: Average observation and reward prediction errors (across 9 tasks and simulation horizon H P r7s) for DWM DWM trained with T " 8 and different diffusion steps K, as the inference step ratio r ratio changes.</p>
<p>Figure E. 2 :
2
Figure E.2: Average observation and reward prediction errors (across 9 tasks and simulation horizon H P r31s) for DWM trained with T " 32 and different diffusion steps K, as the inference step ratio r ratio changes.experiments, which leads to the number of sampling steps N " 3. We have also repeated the above experiments for DWM with longer sequence length T " 32.The results also support the choice r infer " 0.5 but favors K " 10, see Figure E.2.</p>
<p>Figure E.3 reports the return of DWM-IQL and DWM-TD3BC across 3 tasks, with different values of g eval 1 .We report the results averaged over different simulation horizons 1, 3, 5 and 7.The compared RTG values are different for each task, but are all OOD.Appendix E.8.1 shows the distributions of training RTGs for each task.</p>
<p>Figure E. 3 :
3
Figure E.3: Comparison of DWM methods using different evaluation RTG values (displayed in parenthesis).</p>
<p>63 Figure E. 4 :
634
Figure E.4: Normalized discounted returns for each environment.</p>
<p>Figure E. 5 :
5
Figure E.5: The breakdown prediction errors of DWM at each prediction timestep with different RTGs.The DWM is trained with T " 8 and K " 5.</p>
<p>Figure E. 6 :
6
Figure E.6: The breakdown prediction errors of DWM at each prediction timestep with different RTG.The DWM is trained with T " 32 and K " 10.</p>
<p>sϕ</p>
<p>function can be replaced by the V s ψ function.Worth noting, Equation(15) reduces to the vanilla Diff-MVE when λ " 1.</p>
<p>Table 1 .
1
1: A comparison of representative diffusion-model based MBRL methods.</p>
<p>Table 1
1
[2]Alonso et al.[2]trains a DM-based one-step dynamics model, which predicts the next single state s t`1 , conditioning on past states s t´T , . . ., s t and actions a t´T , . . ., a t .This concept is similarly applied in UniSim</p>
<p>Update the actor:ψ Ð ψ <code>η∇ ψ Q ϕ pst, π ψ pstqq
Algorithm 4.1: Diffusion World ModelAlgorithm 4.2: General Actor-Critic FrameworkTrainingfor Offline Model-Based RL with DWMHyperparameters: number of diffusion steps K, null conditioning probability puncond, noise parameters s α k , k P rKs while not converged do Sample a length-T subtrajectory from Doffline:1 Input: pretrained diffusion world model p θ 2 Hyperparameters: rollout length H, conditioning RTG geval, guidance parameter ω, target network update frequency n 3 Initialize the actor and critic networks π ψ , Q ϕ 4 Initialize the weights of target networks s ψ Ð ψ, s ϕ Ð ϕx p0q Ð pst, at, rt, st</code>1, rt<code>1, . . . , st</code>T ´1, rt<code>T ´1q5 for i " 1, 2, . . . until convergence do 6 Sample state-action pair pst, atq from DofflineCompute RTG gt Ð // optimize DWM via Eq. (1) ř T ´1 h"0 γ h r t</code>h Sample ε " N p0, Iq and k P rKs uniformly Compute x pkq Ð ? s α k x p0q <code>?1 ´s α k y Ð ∅ with probability puncond,7 8 9// diffusion model value expansion Sample p rt, p st</code>1, p rt<code>1, . . . , p st</code>T ´1, p rt<code>T ´1 " p θ p¨|st, at, gevalq with guidance parameter ω Compute the target Q value y Ð ř H´1 h"0 γ h p r t</code>h <code>γH Q s ϕ pp st</code>H , π s ψ pp st<code>H qq Update the critic: ϕ Ð ϕ ´η∇ ϕ }Q ϕ pst, atq ´y} 2 2otherwise y Ð gt10Take gradient step on11if i mod n then∇ θ Return: diffusion world model p θ › › ε θ px pkq , k, yq ´ε› › 2 212 13s ϕ Ð s ϕ</code>wpϕ ´s ϕq s ψ Ð s ψ `wpψ ´s ψq</p>
<p>Table 5 .
5Env.OneStep-TD3BC OneStep-IQL OneStep-PQL DWM-TD3BCDWM-IQLDWM-PQLhopper-m0.39 ˘0.040.45 ˘0.050.63 ˘0.120.65 ˘0.100.54 ˘0.110.50 ˘0.09walker2d-m0.39 ˘0.150.52 ˘0.240.74 ˘0.140.70 ˘0.150.76 ˘0.050.79 ˘0.08halfcheetah-m0.44 ˘0.050.44 ˘0.030.45 ˘0.010.46 ˘0.010.44 ˘0.010.44 ˘0.01hopper-mr0.26 ˘0.050.25 ˘0.030.32 ˘0.030.53 ˘0.090.61 ˘0.130.39 ˘0.03walker2d-mr0.23 ˘0.130.24 ˘0.070.62 ˘0.220.46 ˘0.190.35 ˘0.140.35 ˘0.13halfcheetah-mr0.43 ˘0.010.42 ˘0.020.42 ˘0.010.43 ˘0.010.41 ˘0.010.43 ˘0.01hopper-me0.31 ˘0.180.39 ˘0.190.43 ˘0.181.03 ˘0.140.90 ˘0.250.80 ˘0.18walker2d-me0.60 ˘0.250.57 ˘0.180.61 ˘0.221.10 ˘0.001.04 ˘0.101.10 ˘0.01halfcheetah-me0.27 ˘0.120.61 ˘0.220.61 ˘0.220.75 ˘0.160.71 ˘0.140.69 ˘0.13Average0.368 ˘0.1050.432 ˘0.115 0.537 ˘0.128 0.679 ˘0.098 0.641 ˘0.117 0.610 ˘0.080 0.446˘0.116 0.643˘0.07
1: Comparison of MB methods with one-step model versus DWM on the D4RL dataset.</p>
<p>Table 5
5
[70]rithm 4.2 is capable of accommodating various types of sequence models, including Transformer[70], one of the most successful sequence models.However, analogous to the compounding error issue for one-step dynamics model, Transformer is subject to inherent error accumulation due to its autoregressive structure.Therefore, we hypothesize Transformer will underperform and choose diffusion model.
Env.TD3+BCDWM-TD3BCIQLDWM-IQLhopper-m0.58 ˘0.110.65 ˘0.100.48 ˘0.080.54 ˘0.11walker2d-m0.77 ˘0.090.70 ˘0.150.75 ˘0.150.76 ˘0.05halfcheetah-m0.47 ˘0.010.46 ˘0.010.46 ˘0.070.44 ˘0.01hopper-mr0.53 ˘0.190.53 ˘0.090.25 ˘0.020.61 ˘0.13walker2d-mr0.75 ˘0.190.46 ˘0.190.48 ˘0.230.35 ˘0.14halfcheetah-mr0.43 ˘0.010.43 ˘0.010.44 ˘0.010.41 ˘0.01hopper-me0.90 ˘0.281.03 ˘0.140.86 ˘0.220.90 ˘0.25walker2d-me1.08 ˘0.011.10 ˘0.001.09 ˘0.001.04 ˘0.10halfcheetah-me0.73 ˘0.160.75 ˘0.160.60 ˘0.230.71 ˘0.14Average0.693 ˘0.116 0.679 ˘0.098 0.601 ˘0.112 0.641 ˘0.117Table 5.3: Performance of DWM methods and its MF coun-terparts.
.3 reports the results.For each group of comparison, we highlight the winning performance.</p>
<p>Table 5 .
5
4: Performance of Algorithm 4.2 using DWM and Transformer-based world models.</p>
<p>[85]sion Diffuser[1]https://github.com/anuragajay/decision-diffuser Diffuser[33]https://github.com/jannerm/diffuser/ SSORL[85]https://github.com/facebookresearch/ssorl/ Architecture.As introduced in Section 4.1, the diffusion world model p θ used in this paper is chosen to model a length-T subtrajecotires ps t , a t , r t , s t<code>1 , r t</code>1 , . . ., s t<code>T ´1, r t</code>T ´1q.At inference time, it predicts the subsequent subtrajecotry of T ´1 steps, conditioning on initial state-action pair ps t , a t q and target RTG y " g t : ´1 " p θ p¨|s t , a t , y " g t q.
p r t , p s t<code>1 , p r t</code>1 , . . . , p s t<code>T ´1, p r t</code>T</p>
<p>2, . . .until convergence do Sample state-action pair ps t , a t q from D offline ´1 " p θ p¨|s t , a t , g eval q Ð ϕ 1 ´η∇ ϕ1 }Q ϕ1 ps t , a t q ´y} 2 2 ϕ 2 Ð ϕ 2 ´η∇ ϕ2 }Q ϕ2 ps t , a t q ´y}
// diffuion model value expansionSample p r t , p s t<code>1 , p r t</code>1 , . . . , p s t<code>T ´1, p r t</code>T Sample ε " N p0, σ 2 Iqp a ε t<code>H Ð Clippπs ψ pp s t</code>H q <code>ε, ´C, CqCompute the target Q value y "ř H´1 h"0 γ hp r t</code>h<code>γH min iPt1,2u Q ϕi pp s t</code>H , p a ε s t<code>H q// update the criticϕ 1 22min ξE ps,aq"Doffline"iPt1,2u L τ ˆminQ
// update the actor (delayed) and the target networksif i mod n then ψ Ð ψ</code>η∇ ψ ´λQ ϕ1 ´st , π ψ ps t q ¯´}a t ´πψ ps t q} 2 ¯// Update the actor network s ϕ 1 Ð s ϕ 1 <code>wpϕ ´s ϕ 1 q s ϕ 2 Ð s ϕ 2</code>wpϕ ´s ϕ 2 q //Update the target networks s ψ Ð s ψ `wpψ ´s ψq Output: π ψ C.2 DWM-IQL: IQL with Diffusion World Model IQL [42] applies pessimistic value estimation on offline dataset.In addition to the double Q functions used in TD3+BC, IQL leverages an additional state-value function V ξ psq, which is estimated through expectile regression: s ϕi ps, aq ´Vξ psq ˙ȷ ,</p>
<p>Algorithm C.2: DWM-IQLInputs: offline dataset D offline , pretrained diffusion world model p θ , simulation horizon H, conditioning RTG g eval , target network update frequency n, expectile loss parameter τ Initialize the actor, critic and value networks π ψ , Q ϕ1 , Q ϕ2 , V ξ Initialize the weights of target networks s ϕ 1 Ð ϕ 1 , s ϕ 2 Ð ϕ 2 for i " 1, 2, . . .until convergence do Sample state-action pair ps t , a t q from D offline ´1 " p θ p¨|s t , a t , g eval q 1 Ð ϕ 1 ´η∇ ϕ1 }Q ϕ1 ps t , a t q ´y} 2 2 ϕ 2 Ð ϕ 2 ´η∇ ϕ2 }Q ϕ2 ps t , a t q ´y}
// diffuion model value expansionSample p r t , p s t<code>1 , p r t</code>1 , . . . , p s t<code>T ´1, p r t</code>T Compute the target Q value y " ř H´1 h"0 γ hp r t<code>h</code>γH V ξ pp s t<code>H q// update the V -value networkξ Ð ξ ´η∇ ξ L τ pmin iPt1,2u Q ϕi ps, aq ´Vξ psqq s// update the critic (Q-value networks)22// update the actor
ϕ Update the actor network: ψ Ð ψ</code>η∇ ψ exp `βpmin iPt1,2u Q ϕi ps, aq ´Vξ psqq ˘log π ψ pa|sq // update the target networks</p>
<p>2, . . .until convergence do Sample state-action pair ps t , a t q from D offline " p θ p¨|s t , a t , g eval q 1 Ð ϕ 1 ´η∇ ϕ1 }Q ϕ1 ps t , a t q ´y} 2 2 ϕ 2 Ð ϕ 2 ´η∇ ϕ2 }Q ϕ2 ps t , a t q ´y} Update the actor network: ψ Ð ψ <code>η∇ ψ exp</code>βpmin iPt1,2u Q ϕi ps, aq ´Vξ psqq ˘log π ψ pa|sq
// diffuion model value expansionSample m subtrajectories tp r j t , p s j t<code>1 , p r j t</code>1 , . . . , p s j t<code>T ´1, p r j t</code>T ´1u j"1 Modify the rewards of the first subtrajectory as in Eq. (13): mrt , p s t<code>1 , rt</code>1 , . . . , p s t<code>T ´1, p r t</code>T ´1Compute the target Q value y "ř H´1 h"0 γ h rt<code>h</code>γH V ξ pp s 1 t`H q// update the V -value networkξ Ð ξ ´η∇ ξ || min iPt1,2u Q ϕi ps, aq ´Vξ psqq|| 2 s 2// update the critic (Q-value networks)22// update the actor
ϕ // update the target networks</p>
<p>Table D . 2 :
D2
List of the hyperparameters we search for DWM methods.
7, 0.8][1,3,5,7]walker2d-medium-v2[0.6, 0.7, 0.8][1,3,5,7]halfcheetah-medium-v2[0.4, 0.5, 0.6][1,3,5,7]hopper-medium-replay-v2[0.6, 0.7, 0.8][1,3,5,7]walker2d-medium-replay-v2[0.6, 0.7, 0.8][1,3,5,7]halfcheetah-medium-replay-v2[0.4, 0.5, 0.6][1,3,5,7]hopper-medium-expert-v2[0.7, 0.8, 0.9][1,3,5,7]walker2d-medium-expert-v2[0.8, 0.9, 1.0][1,3,5,7]halfcheetah-medium-expert-v2[0.6, 0.7, 0.8][1,3,5,7]</p>
<p>.1 summarizes the normalized returns (with means and standard deviations) of DWM-IQL and DWM-TD3BC for different simulation horizons t1, 3, 7, 15, 31u.Comparison of the normalized returns with different simulation horizons for DWM-TD3BC and DWM-IQL.The reported values are the best performances across different RTG values (listed in Table D.2).
Return (mean˘std)Env.Simulation Horizon DWM-IQL DWM-TD3BC10.54 ± 0.110.68 ± 0.1230.55 ± 0.100.63 ± 0.11hopper-medium-v270.56 ± 0.090.66 ± 0.13150.58 ± 0.120.77 ± 0.15310.61 ± 0.110.79 ± 0.1510.65 ± 0.230.56 ± 0.1330.74 ± 0.110.74 ± 0.13walker2d-medium-v270.71 ± 0.130.74 ± 0.11150.66 ± 0.150.73 ± 0.13310.67 ± 0.200.75 ± 0.1210.44 ± 0.010.35 ± 0.0330.44 ± 0.010.39 ± 0.01halfcheetah-medium-v270.44 ± 0.010.40 ± 0.01150.44 ± 0.020.40 ± 0.01310.44 ± 0.010.40 ± 0.0110.18 ± 0.060.52 ± 0.2130.37 ± 0.180.44 ± 0.23hopper-medium-replay-v270.39 ± 0.140.52 ± 0.28150.37 ± 0.180.67 ± 0.25310.37 ± 0.150.59 ± 0.2210.32 ± 0.150.13 ± 0.0230.27 ± 0.240.19 ± 0.10walker2d-medium-replay-v270.25 ± 0.200.22 ± 0.14150.26 ± 0.190.22 ± 0.10310.27 ± 0.190.17 ± 0.1210.38 ± 0.050.02 ± 0.0030.39 ± 0.020.17 ± 0.05halfcheetah-medium-replay-v270.39 ± 0.020.22 ± 0.03150.38 ± 0.030.26 ± 0.03310.37 ± 0.030.26 ± 0.0510.86 ± 0.250.88 ± 0.1730.90 ± 0.190.94 ± 0.22hopper-medium-expert-v270.88 ± 0.280.93 ± 0.24150.85 ± 0.200.91 ± 0.19310.84 ± 0.230.93 ± 0.2310.80 ± 0.220.74 ± 0.2131.02 ± 0.090.89 ± 0.13walker2d-medium-expert-v270.98 ± 0.20.82 ± 0.19151.06 ± 0.050.84 ± 0.14311.05 ± 0.060.87 ± 0.0310.60 ± 0.180.39 ± 0.0130.52 ± 0.140.43 ± 0.07halfcheetah-medium-expert-v270.63 ± 0.130.44 ± 0.03150.66 ± 0.140.50 ± 0.08310.65 ± 0.170.49 ± 0.09Table E.1:</p>
<p>.3 and Table E.4 respectively.
Simulation HorizonEnv1357hopper-m0.48˘0.08 0.54˘0.10 0.55˘0.08 0.51˘0.09walker2d-m0.54˘0.18 0.62˘0.19 0.72˘0.12 0.72˘0.14halfcheetah-m0.42˘0.03 0.42˘0.02 0.43˘0.01 0.43˘0.01hopper-mr0.17˘0.05 0.24˘0.09 0.26˘0.09 0.20˘0.07walker2d-mr0.17˘0.12 0.17˘0.14 0.23˘0.12 0.16˘0.11halfcheetah-mr 0.38˘0.04 0.39˘0.01 0.38˘0.04 0.39˘0.03hopper-me0.62˘0.16 0.59˘0.21 0.47˘0.21 0.47˘0.21walker2d-me0.67˘0.23 0.87˘0.21 1.03˘0.09 0.71˘0.22halfcheetah-me 0.39˘0.19 0.43˘0.13 0.44˘0.08 0.43˘0.09Table E.4: The normalized returns of T-IQL.Simulation HorizonEnv1357hopper-m0.50˘0.050.57˘0.08 0.58˘0.08 0.57˘0.08walker2d-m0.36˘0.150.40˘0.20 0.60˘0.16 0.53˘0.17halfcheetah-m0.18˘0.07 0.41 ˘0.03 0.38˘0.08 0.42˘0.03hopper-mr0.24˘0.010.23˘0.05 0.25˘0.06 0.22˘0.08walker2d-mr0.12 ˘0.04 0.09˘0.05 0.13˘0.06 0.12˘0.05halfcheetah-mr0.40˘0.010.39˘0.02 0.39˘0.03 0.39˘0.02hopper-me0.41˘0.130.57˘0.19 0.66˘0.25 0.52˘0.15walker2d-me0.34˘0.220.58˘0.15 0.58˘0.26 0.46˘0.37halfcheetah-me0.14˘0.060.31˘0.09 0.36˘0.17 0.29˘0.12Table E.3: The normalized returns of T-TD3BC.</p>
<p>Table E .
E
6: Comparison AD and DWM for sampling time (seconds) under different horizon H values E.5 Additional Environments: Sparse-reward Tasks To further verify the method on various tasks, we conduct experiments on the maze-type tasks with sparse rewards.The methods include TD3+BC, IQL, the DWM counterparts and DD.The training and evaluation protocol follow exactly the same as the main experiments in Appendix D. The results are summarized in Table E.7, which shows the superior performance of DWM-based algorithms in sparse-reward settings.Table E.7: Comparison of different methods on sparse-reward tasks: three maze2d tasks and one antmaze task.E.6 Ablation: Number of Diffusion Steps for Training and Inference.
H=1H=3H=5H=7DWM (trained with T " 8) 0.031 0.031 0.031 0.031Autoregressive Diffusion0.030 0.087 0.145 0.198Env.DWM-TD3BC DWM-IQLTD3+BCIQLDDmaze2d-umaze0.36 ˘0.230.39 ˘0.290.05 ˘0.150.08 ˘0.16 0.40 ˘0.52maze2d-medium0.57 ˘0.500.41 ˘0.09 0.00 ˘0.006 0.10 ˘0.10 0.20 ˘0.19maze2d-large0.28 ˘0.130.11 ˘0.13 ´0.01 ˘0.03 0.01 ˘0.07 0.02 ˘0.07antmaze-umaze0.86 ˘0.290.66 ˘0.470.58 ˘0.490.64 ˘0.48 0.31 ˘0.45</p>
<p>Table E .
E
10 The average (across tasks and simulation horizon H P r31s) observation and reward prediction errors for DWM with T " 32 and different inference steps N " rr infer ¨Ks.˘0.10 0.60 ˘0.12 0.57 ˘0.09 0.61˘0.10TableE.10: The average performance of DWM algorithms across 9 tasks, using DWM with different sequence lengths.
DWM-TD3BCDWM-IQL (w/o λ)T=8 0.680.2 0.3T=321 T=8 22.815 T=32 1.0280.009 0.0010.530.8730.0011.050.8510.0010.213.1140.011100.3 0.52 31.601 1.0280.002 0.0011.050.9430.0010.213.0520.014200.3 0.52 31.595 0.9630.002 0.0011.050.8900.0010.213.1120.018300.3 0.52 31.623 0.9930.003 0.0011.050.8960.0010.213.2750.022500.3 0.52 31.726 1.0310.003 0.0011.050.9440.0010.213.2390.0231000.3 0.52 31.732 1.0210.003 0.0011.050.9230.001Error Reward Error0.217.3900.03050.3 0.52 36.003 5.0650.015 0.0101.054.8530.0100.216.6500.029100.3 0.52 35.799 4.8110.016 0.0101.055.1570.0110.216.2730.031200.3 0.52 35.254 4.7940.015 0.0111.055.0880.012
Table D.2, and report the best results.In addition to Figure E.1, the average prediction errors for diffusion models with T " 32 (longer sequence) and diffusion steps K P t5, 10, 20u are shown in Figure E.2.Based on the results, K " 10 and r infer " 0.5 are selected to strike a good balance between prediction accuracy and inference speed.The corresponding numerical results are listed in Table E.8 and E.9.Table E.8: The average (across tasks and simulation horizon H P r7s) observation and reward prediction errors for DWM with T " 8 and different inference steps N " rr infer ¨Ks.Diffusion Step K Infer Step Ratio rinfer Infer Step N Obs.E.7 Ablation: Sequence Length of Diffusion World ModelWe further compare the average performances of algorithms with DWM trained with sequence length T " 8 and T " 32.Table E.10 presents average best return across 9 tasks (searched over RTG values and simulation horizon H).Even though DWM is robust to long-horizon simulation and in certain cases we have found the optimal H is larger than 8, we found using T " 32 improves the performance of DWM-IQL, but slightly hurts the performance of DWM-TD3BC.</p>
<p>and rewards tp r t u H h"0 .We can use p Q λ t as the target Q value for TD learning, as a modification of line 8 of Algorithm 4.2.For algorithms that also learn the state-only value function, like IQL, the Q
<code>h " p r t</code>h<code>γ #p1 ´λqQ ϕ pp s t</code>h<code>1 , π s ψ pp s t</code>h<code>1 qq</code>λ p s Q λ t<code>h</code>1 Q s ϕ pp s t<code>H , π s ψ pp s t</code>H qqif h ă H if h " H(15)using DWM predicted states tp s t`h uH h"0</p>
<p>Table E .
E
˘0.10 0.68 ˘0.13 0.50 ˘0.08 0.54 ˘0.11 walker2d-m 0.70 ˘0.15 0.74 ˘0.08 0.62 ˘0.19 0.76 ˘0.05 halfcheetah-m 0.46 ˘0.01 0.40 ˘0.01 0.46 ˘0.01 0.44 ˘0.01 hopper-mr 0.53 ˘0.09 0.50 ˘0.23 0.29 ˘0.04 0.61 ˘0.13 walker2d-mr 0.46 ˘0.19 0.23 ˘0.10 0.27 ˘0.09 0.35 ˘0.14 halfcheetah-mr 0.43 ˘0.01 0.39 ˘0.02 0.43 ˘0.01 0.41 ˘0.01 hopper-me 1.03 ˘0.14 1.05 ˘0.16 0.78 ˘0.24 0.90 ˘0.25 walker2d-me 1.10 ˘0.00 0.89 ˘0.13 1.08 ˘0.03 1.04 ˘0.10 halfcheetah-me 0.75 ˘0.16 0.71 ˘0.22 0.73 ˘0.14 0.74 ˘0.16 11: Comparison of the performance of DWM methods using vanilla Diff-MVE and the λ-return variant.
DWM-TD3BCDWM-IQLEnv.w/o λw/ λw/o λw/ λhopper-m 0.65 Avg. 0.680.620.570.64
E.10 Ablation: RTG Relabeling and Model Fine-tuning</p>
<p>We note that the return and RTG are normalized in different ways: the return computed by the D4RL benchmark is undiscounted and normalized by the performance of one SAC policy, whereas the RTG we use in training is discounted and normalized by hand-selected constants.
TableE.13: The normalized returns of DWM-IQL (w/ RTG relabel) with diffusion step K " 5, sampling steps N " 3, and simulation horizon H P t1, 3, 5, 7u.E.11 Example Raw ResultsWhile we cannot enumerate all the experiment results, we provide some raw results of DWM methods.
Is conditional generative modeling all you need for decision-making?. A Ajay, Y Du, A Gupta, J Tenenbaum, T Jaakkola, P Agrawal, arXiv:2211.156572022arXiv preprint</p>
<p>Diffusion world models. E Alonso, A Jelley, A Kanervisto, T Pearce, 2024</p>
<p>On the model-based stochastic value gradient for continuous reinforcement learning. B Amos, S Stanton, D Yarats, A G Wilson, Learning for Dynamics and Control. PMLR2021</p>
<p>K Asadi, D Misra, S Kim, M L Littman, arXiv:1905.13320Combating the compounding-error problem with a multi-step model. 2019arXiv preprint</p>
<p>Transdreamer: Reinforcement learning with transformer world models. C Chen, Y.-F Wu, J Yoon, S Ahn, arXiv:2202.094812022arXiv preprint</p>
<p>Decision transformer: Reinforcement learning via sequence modeling. L Chen, K Lu, A Rajeswaran, K Lee, A Grover, M Laskin, P Abbeel, A Srinivas, I Mordatch, Thirty-Fifth Conference on Neural Information Processing Systems. 2021</p>
<p>C Chi, S Feng, Y Du, Z Xu, E Cousineau, B Burchfiel, S Song, arXiv:2303.04137Diffusion policy: Visuomotor policy learning via action diffusion. 2023arXiv preprint</p>
<p>R Chitnis, Y Xu, B Hashemi, L Lehnert, U Dogan, Z Zhu, O Delalleau, arXiv:2306.00867Iql-td-mpc: Implicit q-learning for hierarchical model predictive control. 2023arXiv preprint</p>
<p>On the sample complexity of the linear quadratic regulator. S Dean, H Mania, N Matni, B Recht, S Tu, Foundations of Computational Mathematics. 2042020</p>
<p>A survey on policy search for robotics. M P Deisenroth, G Neumann, J Peters, Foundations and Trends® in Robotics. 21-22013</p>
<p>Consistency models as a rich and efficient policy class for reinforcement learning. Z Ding, C Jin, arXiv:2309.169842023arXiv preprint</p>
<p>Learning universal policies via text-guided video generation. Y Du, M Yang, B Dai, H Dai, O Nachum, J Tenenbaum, D Schuurmans, P Abbeel, arXiv:2302.001112023arXiv preprint</p>
<p>Rvs: What is essential for offline rl via supervised learning?. S Emmons, B Eysenbach, I Kostrikov, S Levine, arXiv:2112.107512021arXiv preprint</p>
<p>Model-based value estimation for efficient model-free reinforcement learning. V Feinberg, A Wan, I Stoica, M I Jordan, J E Gonzalez, S Levine, arXiv:1803.001012018arXiv preprint</p>
<p>J Fu, A Kumar, O Nachum, G Tucker, S Levine, arXiv:2004.07219D4rl: Datasets for deep data-driven reinforcement learning. 2020arXiv preprint</p>
<p>A minimalist approach to offline reinforcement learning. S Fujimoto, S Gu, Thirty-Fifth Conference on Neural Information Processing Systems. 2021</p>
<p>Addressing function approximation error in actor-critic methods. S Fujimoto, H Hoof, D Meger, International conference on machine learning. PMLR2018</p>
<p>Off-policy deep reinforcement learning without exploration. S Fujimoto, D Meger, D Precup, International Conference on Machine Learning. PMLR2019</p>
<p>Extreme q-learning: Maxent rl without entropy. D Garg, J Hejna, M Geist, S Ermon, arXiv:2301.023282023arXiv preprint</p>
<p>Why so pessimistic? estimating uncertainties for offline rl through ensembles, and why their independence matters. K Ghasemipour, S S Gu, O Nachum, Advances in Neural Information Processing Systems. 202235</p>
<p>. D Ha, J Schmidhuber, arXiv:1803.101222018World models. arXiv preprint</p>
<p>D Hafner, T Lillicrap, J Ba, M Norouzi, arXiv:1912.01603Dream to control: Learning behaviors by latent imagination. 2019aarXiv preprint</p>
<p>Learning latent dynamics for planning from pixels. D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, International conference on machine learning. PMLR2019b</p>
<p>Mastering atari with discrete world models. D Hafner, T Lillicrap, M Norouzi, J Ba, arXiv:2010.021932020arXiv preprint</p>
<p>D Hafner, J Pasukonis, J Ba, T Lillicrap, arXiv:2301.04104Mastering diverse domains through world models. 2023arXiv preprint</p>
<p>N Hansen, Y Lin, H Su, X Wang, V Kumar, A Rajeswaran, arXiv:2212.05698Modem: Accelerating visual model-based reinforcement learning with demonstrations. 2022aarXiv preprint</p>
<p>N Hansen, H Su, X Wang, arXiv:2310.16828Td-mpc2: Scalable, robust world models for continuous control. 2023arXiv preprint</p>
<p>N Hansen, X Wang, H Su, arXiv:2203.04955Temporal difference learning for model predictive control. 2022barXiv preprint</p>
<p>Idql: Implicit q-learning as an actor-critic method with diffusion policies. P Hansen-Estruch, I Kostrikov, M Janner, J G Kuba, S Levine, arXiv:2304.105732023arXiv preprint</p>
<p>Denoising diffusion probabilistic models. J Ho, A Jain, P Abbeel, Advances in neural information processing systems. 202033</p>
<p>J Ho, T Salimans, arXiv:2207.12598Classifier-free diffusion guidance. 2022arXiv preprint</p>
<p>M T Jackson, M T Matthews, C Lu, B Ellis, S Whiteson, J Foerster, arXiv:2404.06356Policy-guided diffusion. 2024arXiv preprint</p>
<p>Planning with diffusion for flexible behavior synthesis. M Janner, Y Du, J B Tenenbaum, S Levine, arXiv:2205.099912022arXiv preprint</p>
<p>When to trust your model: Model-based policy optimization. M Janner, J Fu, M Zhang, S Levine, Advances in neural information processing systems. 201932</p>
<p>Offline reinforcement learning as one big sequence modeling problem. M Janner, Q Li, S Levine, Thirty-Fifth Conference on Neural Information Processing Systems. 2021</p>
<p>gamma-models: Generative temporal difference learning for infinite-horizon prediction. M Janner, I Mordatch, S Levine, Advances in Neural Information Processing Systems. 202033</p>
<p>N Jaques, A Ghandeharioun, J H Shen, C Ferguson, A Lapedriza, N Jones, S Gu, R Picard, arXiv:1907.00456Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. 2019arXiv preprint</p>
<p>Z Jia, F Liu, V Thumuluri, L Chen, Z Huang, H Su, arXiv:2304.00776Chain-of-thought predictive control. 2023arXiv preprint</p>
<p>Model-based reinforcement learning for atari. L Kaiser, M Babaeizadeh, P Milos, B Osinski, R H Campbell, K Czechowski, D Erhan, C Finn, P Kozakowski, S Levine, arXiv:1903.003742019arXiv preprint</p>
<p>Morel: Model-based offline reinforcement learning. R Kidambi, A Rajeswaran, P Netrapalli, T Joachims, Advances in neural information processing systems. 202033</p>
<p>Variational diffusion models. D Kingma, T Salimans, B Poole, J Ho, Advances in neural information processing systems. 202134</p>
<p>Offline reinforcement learning with implicit q-learning. I Kostrikov, A Nair, S Levine, 2021</p>
<p>A Kumar, J Fu, G Tucker, S Levine, arXiv:1906.00949Stabilizing off-policy q-learning via bootstrapping error reduction. 2019arXiv preprint</p>
<p>Conservative q-learning for offline reinforcement learning. A Kumar, A Zhou, G Tucker, S Levine, arXiv:2006.047792020arXiv preprint</p>
<p>Investigating compounding prediction errors in learned dynamics models. N Lambert, K Pister, Calandra , R , arXiv:2203.096372022arXiv preprint</p>
<p>K.-H Lee, O Nachum, M Yang, L Lee, D Freeman, W Xu, S Guadarrama, I Fischer, E Jang, H Michalewski, arXiv:2205.15241Multi-game decision transformers. 2022arXiv preprint</p>
<p>Y Lipman, R T Chen, H Ben-Hamu, M Nickel, M Le, arXiv:2210.02747Flow matching for generative modeling. 2022arXiv preprint</p>
<p>C Lu, P J Ball, J Parker-Holder, arXiv:2303.06614Synthetic experience replay. 2023arXiv preprint</p>
<p>Offline pre-trained multi-agent decision transformer: One big sequence model conquers all starcraftii tasks. L Meng, M Wen, Y Yang, C Le, X Li, W Zhang, Y Wen, H Zhang, J Wang, B Xu, arXiv:2112.028452021arXiv preprint</p>
<p>V Micheli, E Alonso, F Fleuret, arXiv:2209.00588Transformers are sample efficient world models. 2022arXiv preprint</p>
<p>A self regularized non-monotonic activation function. M D Mish, arXiv:1908.086812019arXiv preprint</p>
<p>Reorientdiff: Diffusion model based reorientation for object manipulation. U A Mishra, Y Chen, arXiv:2303.127002023arXiv preprint</p>
<p>Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. A Nagabandi, G Kahn, R S Fearing, S Levine, IEEE international conference on robotics and automation (ICRA). 2018. 2018IEEE</p>
<p>Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning. M Nakamoto, Y Zhai, A Singh, M S Mark, Y Ma, C Finn, A Kumar, S Levine, arXiv:2303.054792023arXiv preprint</p>
<p>T Nguyen, Q Zheng, A Grover, arXiv:2210.05158Conserweightive behavioral cloning for reliable offline reinforcement learning. 2022arXiv preprint</p>
<p>Improved denoising diffusion probabilistic models. A Q Nichol, P Dhariwal, International Conference on Machine Learning. PMLR2021</p>
<p>X B Peng, A Kumar, G Zhang, S Levine, arXiv:1910.00177Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. 2019arXiv preprint</p>
<p>World models via policy-guided trajectory diffusion. M Rigter, J Yamada, I Posner, arXiv:2312.085332023arXiv preprint</p>
<p>Transformer-based world models are happy with 100k interactions. J Robine, M Höftmann, T Uelwer, S Harmeling, arXiv:2303.071092023arXiv preprint</p>
<p>U-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference. Munich, GermanySpringer2015. October 5-9, 2015Proceedings, Part III 18</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. J Schrittwieser, I Antonoglou, T Hubert, K Simonyan, L Sifre, S Schmitt, A Guez, E Lockhart, D Hassabis, T Graepel, Nature. 58878392020</p>
<p>J Schulman, P Moritz, S Levine, M Jordan, P Abbeel, arXiv:1506.02438High-dimensional continuous control using generalized advantage estimation. 2015arXiv preprint</p>
<p>Deterministic policy gradient algorithms. D Silver, G Lever, N Heess, T Degris, D Wierstra, M Riedmiller, International conference on machine learning. Pmlr2014</p>
<p>Deep unsupervised learning using nonequilibrium thermodynamics. J Sohl-Dickstein, E Weiss, N Maheswaranathan, S Ganguli, International conference on machine learning. PMLR2015</p>
<p>Y Song, P Dhariwal, M Chen, I Sutskever, arXiv:2303.01469Consistency models. 2023arXiv preprint</p>
<p>Y Song, J Sohl-Dickstein, D P Kingma, A Kumar, S Ermon, B Poole, arXiv:2011.13456Score-based generative modeling through stochastic differential equations. 2020arXiv preprint</p>
<p>Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. R S Sutton, Machine learning proceedings 1990. Elsevier1990</p>
<p>Dyna, an integrated architecture for learning, planning, and reacting. R S Sutton, ACM Sigart Bulletin. 241991</p>
<p>Issues in using function approximation for reinforcement learning. S Thrun, A Schwartz, Proceedings of the 1993 connectionist models summer school. the 1993 connectionist models summer schoolPsychology Press2014</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. 201730</p>
<p>T Wang, X Bao, I Clavera, J Hoang, Y Wen, E Langlois, S Zhang, G Zhang, P Abbeel, J Ba, arXiv:1907.02057Benchmarking model-based reinforcement learning. 2019arXiv preprint</p>
<p>Z Wang, J J Hunt, M Zhou, arXiv:2208.06193Diffusion policies as an expressive policy class for offline reinforcement learning. 2022arXiv preprint</p>
<p>Q-learning. C J Watkins, P Dayan, Machine learning. 81992</p>
<p>G Williams, A Aldrich, E Theodorou, arXiv:1509.01149Model predictive path integral control using covariance variable importance sampling. 2015arXiv preprint</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. R J Williams, Machine learning. 81992</p>
<p>Group normalization. Y Wu, K He, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>Y Wu, G Tucker, O Nachum, arXiv:1911.11361Behavior regularized offline reinforcement learning. 2019arXiv preprint</p>
<p>C Xiao, Y Wu, C Ma, D Schuurmans, M Müller, arXiv:1912.11206Learning to combat compounding-error in model-based reinforcement learning. 2019arXiv preprint</p>
<p>Y Xu, N Li, A Goel, Z Guo, Z Yao, H Kasaei, M Kasaei, Z Li, arXiv:2303.05323Controllable video generation by learning the underlying dynamical system with neural ode. 2023arXiv preprint</p>
<p>Q-learning decision transformer: Leveraging dynamic programming for conditional sequence modelling in offline rl. T Yamagata, A Khalil, R Santos-Rodriguez, International Conference on Machine Learning. PMLR2023</p>
<p>M Yang, Y Du, K Ghasemipour, J Tompson, D Schuurmans, P Abbeel, arXiv:2310.06114Learning interactive real-world simulators. 2023arXiv preprint</p>
<p>Mastering atari games with limited data. W Ye, S Liu, T Kurutach, P Abbeel, Y Gao, Advances in Neural Information Processing Systems. 342021</p>
<p>Mopo: Model-based offline policy optimization. T Yu, G Thomas, L Yu, S Ermon, J Y Zou, S Levine, C Finn, T Ma, Advances in Neural Information Processing Systems. 202033</p>
<p>Learning unsupervised world models for autonomous driving via discrete diffusion. L Zhang, Y Xiong, Z Yang, S Casas, R Hu, R Urtasun, arXiv:2311.010172023arXiv preprint</p>
<p>Semi-supervised offline reinforcement learning with action-free trajectories. Q Zheng, M Henaff, B Amos, A Grover, International Conference on Machine Learning. PMLR2023a</p>
<p>Q Zheng, M Le, N Shaul, Y Lipman, A Grover, R T Chen, arXiv:2311.13443Guided flows for generative modeling and decision making. 2023barXiv preprint</p>
<p>Online decision transformer. Q Zheng, A Zhang, A Grover, international conference on machine learning. PMLR2022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5709 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5709</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5709</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-268041519</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.17944v4.pdf" target="_blank">Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5709.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5709.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Singha et al. 2023 - DFLoader/JSON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tabular representation, noisy operators, and impacts on table structure understanding tasks in LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical finding that the choice of serialized table format affects LLM table-understanding performance; DFLoader and JSON-style programmatic serializations worked better for fact-finding and table-transformation tasks than some alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tabular representation, noisy operators, and impacts on table structure understanding tasks in LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-family (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>fact-finding and table transformation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extracting factual information from tables and transforming table content (e.g., reformatting, extracting cells/columns).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Programmatic/structured serializations such as DFLoader and line-oriented JSON representations of tables instead of purely human-readable sentence templates or simple X-separated values.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Human-readable list/text templates and X-separated formats (commas/tabs) and other ad-hoc serializations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>DFLoader/JSON formats reported to perform better for fact-finding and table-transformation tasks (no numeric metrics reported in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Reported improvement over simpler X-separated or ad-hoc serializations (no numeric values reported).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Structured programmatic serializations present table structure more explicitly and reduce ambiguity for parsing tasks, which can align better with models' parsing capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5709.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5709.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sui et al. 2023 - HTML/XML vs X-separated</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluating and enhancing structural understanding capabilities of large language models on tables via input designs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Controlled experiments showing markup (HTML/XML) serializations are better interpreted by GPT-family models on tabular QA and fact-verification tasks than X-separated formats, at the cost of higher token consumption.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating and enhancing structural understanding capabilities of large language models on tables via input designs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>tabular question answering and fact verification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answering questions about table content and verifying statements against tables.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Markup-based serialization (HTML or XML table markup inserted into the prompt) vs X-separated formats (CSV/TSV-like) and plain serialized text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>X-separated formats (comma/tab delimited) and simpler text serializations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>HTML/XML serializations outperformed X-separated formats for GPT-3.5 and GPT-4 on tabular QA/FV (no numeric values reported in the survey); markup used more tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Markup > X-separated (qualitative), but markup incurred substantially higher token consumption.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>LLMs were pretrained on large amounts of web data and thus are more familiar with HTML/XML structures; markup provides explicit structural cues that help the model interpret table layout.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Markup formats require more context tokens and so may be impractical for large tables; no numeric trade-off provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5709.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5709.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sui et al. 2023c - prompt augmentation with metadata</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Including table-level metadata and statistics (e.g., dimension, measure, semantic field type, column statistics, term explanations, document references) in the prompt materially improved LLM accuracy across multiple datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-family (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>general tabular tasks (QA / FV / reasoning) across six datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various table-based reasoning and QA tasks where extra contextual metadata can aid interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt augmented with table schemas and statistics: explicit attributes such as 'dimension', 'measure', semantic field types, column statistics, and optional document references/term explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Prompts without metadata (table content only) or with minimal schema information.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Including metadata improved accuracy across the six datasets examined; statistics helped particularly on datasets with numerical/statistical cells (e.g., FEVEROUS).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Additional schema/statistical/contextual metadata supplies semantic signals and disambiguation that LLMs can leverage to reason about table contents more effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5709.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5709.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TabLLM serialization result</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TabLLM: Few-shot classification of tabular data with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic evaluation of many serialization formats for few‑shot tabular classification found that simple textual enumeration ('The column name is Value') works best and that LLM-generated serializations can hallucinate and harm performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TabLLM: Few-shot classification of tabular data with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0 / T-few / GPT-3 / BLOOM (fine-tuned variants used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>few-shot tabular classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classify tabular rows using few-shot in-context examples and various serialization/prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Textual enumeration template (e.g., 'ColumnName is Value' per feature) and multiple other hand-crafted and LLM-generated serialization templates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>LLM-based sentence serialization (LLM rewrites) and other templates vs simple enumeration/list templates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Textual enumeration ('The column name is Value') performed best across tested serializations; manually designed list/text templates outperformed LLM-based serialization methods in few-shot classification experiments (no numeric values in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Enumeration templates > LLM-generated sentence serializations (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (for simple enumeration); reduced (for LLM-based serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Simple, faithful templates avoid introducing errors; LLMs when used to serialize tables may hallucinate, inject incorrect values, or change semantics—hurting downstream classification.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>TabLLM also found that LLMs can leverage column names and relationships in few-shot medical prompts, indicating domain/task dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5709.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5709.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LIFT (Dinh et al. 2022) serialization & markers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-interfaced fine-tuning for non-language machine learning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Compared serialization variants for prediction and used special answer markers to constrain generated outputs; natural sentence feature-value templates worked better than equation-like enumerations on low-dimensional tasks and answer markers improved mapping to labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language-interfaced fine-tuning for non-language machine learning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3, GPT-J</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>tabular prediction/classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict labels from tabular rows presented as natural language prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Feature-value natural sentences (e.g., 'The column name is Value') and equation-like serialization (col1 = val1, ...); use of special markers (e.g., '###' and '@@@') to delimit answers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Equation-like short serialization vs natural sentence templates; with and without explicit answer markers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Natural sentence format yielded higher prediction accuracy on low-dimensional tasks; using explicit markers to delineate answer spans improved alignment of generated text to intended labels (qualitative reporting in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Natural language sentences fit the LLM training distribution better; explicit markers constrain generation and reduce mapping ambiguity between free text and target labels.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5709.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5709.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Serialize-LM (Jaitly et al. 2023) techniques</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Towards better serialization of tabular data for few-shot classification with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Introduced three serialization strategies (grouping related features into richer sentences, labeling covarying features, and LaTeX table encoding) that improved LLM performance on domain-specific datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards better serialization of tabular data for few-shot classification with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (fine-tuned/evaluated by authors; unspecified in survey summary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>domain-specific tabular prediction / few-shot classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classify/regress tabular rows in domain datasets where covariances and feature groupings matter.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Combine related features into single descriptive sentences (richer per-row context), label critical covarying features, and/or serialize rows as LaTeX table fragments for fine-tuning or prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Baseline simple feature-value enumeration and other plain serializations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>The proposed serialization techniques boosted LLM performance on domain-specific datasets (qualitative; no numeric metrics in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Grouping covarying or semantically related features reduces sequence length and provides higher-information density per sentence, enabling the LLM to better exploit correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5709.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5709.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model scale & long-context effects (Chen 2023 summary)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Surveyed evidence that (a) model scale strongly affects table reasoning performance (smaller models often much weaker) and (b) very long serialized table contexts degrade performance (GPT-3 performance reportedly drops to random when table inputs exceed ~1000 tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 variants (6.7B vs 175B) and GPT-3 generally</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6.7B, 175B (examples cited)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WebTableQuestions and TabFact (numerical/table reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Table QA / numerical reasoning over tables.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Model-size comparisons and experiments with long serialized table inputs (>1000 tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Smaller vs larger autoregressive models; short vs very long table serializations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported: 6.7B model achieved roughly half the score of the 175B GPT-3 on WebTableQuestions; models ≤6.7B gave near-random accuracy on TabFact; GPT-3 reportedly degrades to near-random when table input size exceeds ~1000 tokens (no exact numeric breakdown in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (for smaller models and very long contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Emergent reasoning abilities scale with model size; long contexts cause 'lost-in-the-middle' and difficulty accessing relevant items in long serialized inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5709.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5709.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Structural perturbation sensitivity (Liu et al. 2023e)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical robustness analysis showing LLMs are sensitive to structural manipulations of tables: transposing/shuffling rows or columns can substantially degrade performance, though header detection remains strong.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (evaluated in robustness experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>table interpretation / table QA robustness</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Effects of structural perturbations (transpose, column/row shuffle) on table understanding and QA accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Apply structural perturbations to serialized table inputs (transpose rows/columns, shuffle order) and evaluate model responses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original table orientation vs transposed/shuffled variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Transposing the table reduced accuracy substantially (reported ~50% accuracy in one example); however, models identified whether first row/column is header with high accuracy (94–97%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced for transposition/shuffle; robust for header-detection</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>LLMs incorporate structural biases about table orientation and layout—when orientation changes, model reasoning pipelines break; header-detection remains reliable because of strong lexical cues.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5709.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5709.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) & Self-consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting strategies that elicit step-by-step reasoning (CoT) and aggregating multiple reasoning chains (SC) improve complex multi-step numerical and symbolic reasoning; these techniques have been applied to table/numerical QA in the surveyed literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-Thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (GPT-family and other large models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>complex arithmetic, numerical QA and multi-step table reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Problems requiring multi-step reasoning over table content or numbers (e.g., compute averages, multi-step aggregations).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>CoT prompting: include one or more step-by-step reasoning exemplars or request chain-of-thought; Self-consistency: sample many CoT outputs and aggregate final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard direct-answer prompts (zero-shot/few-shot) vs chain-of-thought prompting and self-consistency aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>CoT and SC reported to improve reasoning and numerical QA performance qualitatively in multiple studies; specific numerical gains not consistently reported in the survey for tabular tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Decomposing tasks into intermediate reasoning steps reduces the burden on single-step generation; SC reduces variance by marginalizing across reasoning paths, increasing answer reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Some smaller models or formats may not benefit from CoT; task-dependent effectiveness noted in surveyed literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gpt4table: Can large language models understand structured table data? a benchmark and empirical study <em>(Rating: 2)</em></li>
                <li>Evaluating and enhancing structural understanding capabilities of large language models on tables via input designs <em>(Rating: 2)</em></li>
                <li>TabLLM: Few-shot classification of tabular data with large language models <em>(Rating: 2)</em></li>
                <li>Language-interfaced fine-tuning for non-language machine learning tasks <em>(Rating: 2)</em></li>
                <li>Towards better serialization of tabular data for few-shot classification with large language models <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning <em>(Rating: 2)</em></li>
                <li>TabMT: Generating tabular data with masked transformers <em>(Rating: 1)</em></li>
                <li>DocMath-eval: Evaluating numerical reasoning capabilities of llms in understanding long documents with tabular data <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5709",
    "paper_id": "paper-268041519",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Singha et al. 2023 - DFLoader/JSON",
            "name_full": "Tabular representation, noisy operators, and impacts on table structure understanding tasks in LLMs",
            "brief_description": "Empirical finding that the choice of serialized table format affects LLM table-understanding performance; DFLoader and JSON-style programmatic serializations worked better for fact-finding and table-transformation tasks than some alternatives.",
            "citation_title": "Tabular representation, noisy operators, and impacts on table structure understanding tasks in LLMs",
            "mention_or_use": "mention",
            "model_name": "GPT-family (unspecified)",
            "model_size": null,
            "task_name": "fact-finding and table transformation",
            "task_description": "Extracting factual information from tables and transforming table content (e.g., reformatting, extracting cells/columns).",
            "problem_format": "Programmatic/structured serializations such as DFLoader and line-oriented JSON representations of tables instead of purely human-readable sentence templates or simple X-separated values.",
            "comparison_format": "Human-readable list/text templates and X-separated formats (commas/tabs) and other ad-hoc serializations.",
            "performance": "DFLoader/JSON formats reported to perform better for fact-finding and table-transformation tasks (no numeric metrics reported in the survey).",
            "performance_comparison": "Reported improvement over simpler X-separated or ad-hoc serializations (no numeric values reported).",
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Structured programmatic serializations present table structure more explicitly and reduce ambiguity for parsing tasks, which can align better with models' parsing capabilities.",
            "counterexample_or_null_result": null,
            "uuid": "e5709.0",
            "source_info": {
                "paper_title": "Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Sui et al. 2023 - HTML/XML vs X-separated",
            "name_full": "Evaluating and enhancing structural understanding capabilities of large language models on tables via input designs",
            "brief_description": "Controlled experiments showing markup (HTML/XML) serializations are better interpreted by GPT-family models on tabular QA and fact-verification tasks than X-separated formats, at the cost of higher token consumption.",
            "citation_title": "Evaluating and enhancing structural understanding capabilities of large language models on tables via input designs",
            "mention_or_use": "use",
            "model_name": "GPT-3.5, GPT-4",
            "model_size": null,
            "task_name": "tabular question answering and fact verification",
            "task_description": "Answering questions about table content and verifying statements against tables.",
            "problem_format": "Markup-based serialization (HTML or XML table markup inserted into the prompt) vs X-separated formats (CSV/TSV-like) and plain serialized text.",
            "comparison_format": "X-separated formats (comma/tab delimited) and simpler text serializations.",
            "performance": "HTML/XML serializations outperformed X-separated formats for GPT-3.5 and GPT-4 on tabular QA/FV (no numeric values reported in the survey); markup used more tokens.",
            "performance_comparison": "Markup &gt; X-separated (qualitative), but markup incurred substantially higher token consumption.",
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "LLMs were pretrained on large amounts of web data and thus are more familiar with HTML/XML structures; markup provides explicit structural cues that help the model interpret table layout.",
            "counterexample_or_null_result": "Markup formats require more context tokens and so may be impractical for large tables; no numeric trade-off provided.",
            "uuid": "e5709.1",
            "source_info": {
                "paper_title": "Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Sui et al. 2023c - prompt augmentation with metadata",
            "name_full": "Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning",
            "brief_description": "Including table-level metadata and statistics (e.g., dimension, measure, semantic field type, column statistics, term explanations, document references) in the prompt materially improved LLM accuracy across multiple datasets.",
            "citation_title": "Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning",
            "mention_or_use": "use",
            "model_name": "GPT-family (unspecified)",
            "model_size": null,
            "task_name": "general tabular tasks (QA / FV / reasoning) across six datasets",
            "task_description": "Various table-based reasoning and QA tasks where extra contextual metadata can aid interpretation.",
            "problem_format": "Prompt augmented with table schemas and statistics: explicit attributes such as 'dimension', 'measure', semantic field types, column statistics, and optional document references/term explanations.",
            "comparison_format": "Prompts without metadata (table content only) or with minimal schema information.",
            "performance": "Including metadata improved accuracy across the six datasets examined; statistics helped particularly on datasets with numerical/statistical cells (e.g., FEVEROUS).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Additional schema/statistical/contextual metadata supplies semantic signals and disambiguation that LLMs can leverage to reason about table contents more effectively.",
            "counterexample_or_null_result": null,
            "uuid": "e5709.2",
            "source_info": {
                "paper_title": "Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "TabLLM serialization result",
            "name_full": "TabLLM: Few-shot classification of tabular data with large language models",
            "brief_description": "Systematic evaluation of many serialization formats for few‑shot tabular classification found that simple textual enumeration ('The column name is Value') works best and that LLM-generated serializations can hallucinate and harm performance.",
            "citation_title": "TabLLM: Few-shot classification of tabular data with large language models",
            "mention_or_use": "use",
            "model_name": "T0 / T-few / GPT-3 / BLOOM (fine-tuned variants used in experiments)",
            "model_size": null,
            "task_name": "few-shot tabular classification",
            "task_description": "Classify tabular rows using few-shot in-context examples and various serialization/prompting strategies.",
            "problem_format": "Textual enumeration template (e.g., 'ColumnName is Value' per feature) and multiple other hand-crafted and LLM-generated serialization templates.",
            "comparison_format": "LLM-based sentence serialization (LLM rewrites) and other templates vs simple enumeration/list templates.",
            "performance": "Textual enumeration ('The column name is Value') performed best across tested serializations; manually designed list/text templates outperformed LLM-based serialization methods in few-shot classification experiments (no numeric values in survey).",
            "performance_comparison": "Enumeration templates &gt; LLM-generated sentence serializations (qualitative).",
            "format_effect_size": null,
            "format_effect_direction": "improved (for simple enumeration); reduced (for LLM-based serialization)",
            "explanation_or_hypothesis": "Simple, faithful templates avoid introducing errors; LLMs when used to serialize tables may hallucinate, inject incorrect values, or change semantics—hurting downstream classification.",
            "counterexample_or_null_result": "TabLLM also found that LLMs can leverage column names and relationships in few-shot medical prompts, indicating domain/task dependence.",
            "uuid": "e5709.3",
            "source_info": {
                "paper_title": "Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LIFT (Dinh et al. 2022) serialization & markers",
            "name_full": "Language-interfaced fine-tuning for non-language machine learning tasks",
            "brief_description": "Compared serialization variants for prediction and used special answer markers to constrain generated outputs; natural sentence feature-value templates worked better than equation-like enumerations on low-dimensional tasks and answer markers improved mapping to labels.",
            "citation_title": "Language-interfaced fine-tuning for non-language machine learning tasks",
            "mention_or_use": "use",
            "model_name": "GPT-3, GPT-J",
            "model_size": null,
            "task_name": "tabular prediction/classification",
            "task_description": "Predict labels from tabular rows presented as natural language prompts.",
            "problem_format": "Feature-value natural sentences (e.g., 'The column name is Value') and equation-like serialization (col1 = val1, ...); use of special markers (e.g., '###' and '@@@') to delimit answers.",
            "comparison_format": "Equation-like short serialization vs natural sentence templates; with and without explicit answer markers.",
            "performance": "Natural sentence format yielded higher prediction accuracy on low-dimensional tasks; using explicit markers to delineate answer spans improved alignment of generated text to intended labels (qualitative reporting in survey).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Natural language sentences fit the LLM training distribution better; explicit markers constrain generation and reduce mapping ambiguity between free text and target labels.",
            "counterexample_or_null_result": null,
            "uuid": "e5709.4",
            "source_info": {
                "paper_title": "Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Serialize-LM (Jaitly et al. 2023) techniques",
            "name_full": "Towards better serialization of tabular data for few-shot classification with large language models",
            "brief_description": "Introduced three serialization strategies (grouping related features into richer sentences, labeling covarying features, and LaTeX table encoding) that improved LLM performance on domain-specific datasets.",
            "citation_title": "Towards better serialization of tabular data for few-shot classification with large language models",
            "mention_or_use": "use",
            "model_name": "Various LLMs (fine-tuned/evaluated by authors; unspecified in survey summary)",
            "model_size": null,
            "task_name": "domain-specific tabular prediction / few-shot classification",
            "task_description": "Classify/regress tabular rows in domain datasets where covariances and feature groupings matter.",
            "problem_format": "Combine related features into single descriptive sentences (richer per-row context), label critical covarying features, and/or serialize rows as LaTeX table fragments for fine-tuning or prompting.",
            "comparison_format": "Baseline simple feature-value enumeration and other plain serializations.",
            "performance": "The proposed serialization techniques boosted LLM performance on domain-specific datasets (qualitative; no numeric metrics in survey).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Grouping covarying or semantically related features reduces sequence length and provides higher-information density per sentence, enabling the LLM to better exploit correlations.",
            "counterexample_or_null_result": null,
            "uuid": "e5709.5",
            "source_info": {
                "paper_title": "Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Model scale & long-context effects (Chen 2023 summary)",
            "name_full": "",
            "brief_description": "Surveyed evidence that (a) model scale strongly affects table reasoning performance (smaller models often much weaker) and (b) very long serialized table contexts degrade performance (GPT-3 performance reportedly drops to random when table inputs exceed ~1000 tokens).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3 variants (6.7B vs 175B) and GPT-3 generally",
            "model_size": "6.7B, 175B (examples cited)",
            "task_name": "WebTableQuestions and TabFact (numerical/table reasoning)",
            "task_description": "Table QA / numerical reasoning over tables.",
            "problem_format": "Model-size comparisons and experiments with long serialized table inputs (&gt;1000 tokens).",
            "comparison_format": "Smaller vs larger autoregressive models; short vs very long table serializations.",
            "performance": "Reported: 6.7B model achieved roughly half the score of the 175B GPT-3 on WebTableQuestions; models ≤6.7B gave near-random accuracy on TabFact; GPT-3 reportedly degrades to near-random when table input size exceeds ~1000 tokens (no exact numeric breakdown in survey).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "reduced (for smaller models and very long contexts)",
            "explanation_or_hypothesis": "Emergent reasoning abilities scale with model size; long contexts cause 'lost-in-the-middle' and difficulty accessing relevant items in long serialized inputs.",
            "counterexample_or_null_result": null,
            "uuid": "e5709.6",
            "source_info": {
                "paper_title": "Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Structural perturbation sensitivity (Liu et al. 2023e)",
            "name_full": "",
            "brief_description": "Empirical robustness analysis showing LLMs are sensitive to structural manipulations of tables: transposing/shuffling rows or columns can substantially degrade performance, though header detection remains strong.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5 (evaluated in robustness experiments)",
            "model_size": null,
            "task_name": "table interpretation / table QA robustness",
            "task_description": "Effects of structural perturbations (transpose, column/row shuffle) on table understanding and QA accuracy.",
            "problem_format": "Apply structural perturbations to serialized table inputs (transpose rows/columns, shuffle order) and evaluate model responses.",
            "comparison_format": "Original table orientation vs transposed/shuffled variants.",
            "performance": "Transposing the table reduced accuracy substantially (reported ~50% accuracy in one example); however, models identified whether first row/column is header with high accuracy (94–97%).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "reduced for transposition/shuffle; robust for header-detection",
            "explanation_or_hypothesis": "LLMs incorporate structural biases about table orientation and layout—when orientation changes, model reasoning pipelines break; header-detection remains reliable because of strong lexical cues.",
            "counterexample_or_null_result": null,
            "uuid": "e5709.7",
            "source_info": {
                "paper_title": "Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) & Self-consistency (SC)",
            "name_full": "Chain-of-Thought prompting elicits reasoning in large language models",
            "brief_description": "Prompting strategies that elicit step-by-step reasoning (CoT) and aggregating multiple reasoning chains (SC) improve complex multi-step numerical and symbolic reasoning; these techniques have been applied to table/numerical QA in the surveyed literature.",
            "citation_title": "Chain-of-Thought prompting elicits reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "Various LLMs (GPT-family and other large models)",
            "model_size": null,
            "task_name": "complex arithmetic, numerical QA and multi-step table reasoning",
            "task_description": "Problems requiring multi-step reasoning over table content or numbers (e.g., compute averages, multi-step aggregations).",
            "problem_format": "CoT prompting: include one or more step-by-step reasoning exemplars or request chain-of-thought; Self-consistency: sample many CoT outputs and aggregate final answers.",
            "comparison_format": "Standard direct-answer prompts (zero-shot/few-shot) vs chain-of-thought prompting and self-consistency aggregation.",
            "performance": "CoT and SC reported to improve reasoning and numerical QA performance qualitatively in multiple studies; specific numerical gains not consistently reported in the survey for tabular tasks.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Decomposing tasks into intermediate reasoning steps reduces the burden on single-step generation; SC reduces variance by marginalizing across reasoning paths, increasing answer reliability.",
            "counterexample_or_null_result": "Some smaller models or formats may not benefit from CoT; task-dependent effectiveness noted in surveyed literature.",
            "uuid": "e5709.8",
            "source_info": {
                "paper_title": "Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gpt4table: Can large language models understand structured table data? a benchmark and empirical study",
            "rating": 2,
            "sanitized_title": "gpt4table_can_large_language_models_understand_structured_table_data_a_benchmark_and_empirical_study"
        },
        {
            "paper_title": "Evaluating and enhancing structural understanding capabilities of large language models on tables via input designs",
            "rating": 2,
            "sanitized_title": "evaluating_and_enhancing_structural_understanding_capabilities_of_large_language_models_on_tables_via_input_designs"
        },
        {
            "paper_title": "TabLLM: Few-shot classification of tabular data with large language models",
            "rating": 2,
            "sanitized_title": "tabllm_fewshot_classification_of_tabular_data_with_large_language_models"
        },
        {
            "paper_title": "Language-interfaced fine-tuning for non-language machine learning tasks",
            "rating": 2,
            "sanitized_title": "languageinterfaced_finetuning_for_nonlanguage_machine_learning_tasks"
        },
        {
            "paper_title": "Towards better serialization of tabular data for few-shot classification with large language models",
            "rating": 2,
            "sanitized_title": "towards_better_serialization_of_tabular_data_for_fewshot_classification_with_large_language_models"
        },
        {
            "paper_title": "Chain-of-Thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning",
            "rating": 2,
            "sanitized_title": "tap4llm_table_provider_on_sampling_augmenting_and_packing_semistructured_data_for_large_language_model_reasoning"
        },
        {
            "paper_title": "TabMT: Generating tabular data with masked transformers",
            "rating": 1,
            "sanitized_title": "tabmt_generating_tabular_data_with_masked_transformers"
        },
        {
            "paper_title": "DocMath-eval: Evaluating numerical reasoning capabilities of llms in understanding long documents with tabular data",
            "rating": 1,
            "sanitized_title": "docmatheval_evaluating_numerical_reasoning_capabilities_of_llms_in_understanding_long_documents_with_tabular_data"
        }
    ],
    "cost": 0.02712725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding -A Survey
21 Jun 2024</p>
<p>Xi Fang 
Weijie Xu weijiexu@amazon.com 
Fiona Anting Tan 
Jiani Zhang zhajiani@amazon.com 
Ziqing Hu ziqinghu@amazon.com 
Yanjun Qi yanjunqi@amazon.com 
Scott Nickleach nickleac@amazon.com 
Diego Socolinsky sclinsky@amazon.com 
Srinivasan Sengamedu sengamed@amazon.com 
Christos Faloutsos faloutso@amazon.com </p>
<p>National University of Singapore</p>
<p>AWS University of Virginia</p>
<p>Carnegie Mellon University Amazon</p>
<p>Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding -A Survey
21 Jun 20246726FBE0C5B26067209324AA2620003BarXiv:2402.17944v4[cs.CL]https:openreview .netforum? id= IZnrCGF9WI&amp;noteId= nWxFR40vnD
Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding.Each task presents unique challenges and opportunities.However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain.This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized.It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field.It also provides relevant code and datasets references.Through this comprehensive review, we hope to provide interested readers with</p>
<p>Introduction</p>
<p>Large language models (LLMs) are deep learning models trained on extensive data, endowing them with versatile problem-solving capabilities that extend far beyond the realm of natural language processing (NLP) tasks (Fu &amp; Khot, 2022).Recent research has revealed emergent abilities of LLMs, such as improved performance on few-shot prompted tasks (Wei et al., 2022b).The remarkable performance of LLMs have incited interest in both academia and industry, raising beliefs that they could serve as the foundation for Artificial General Intelligence (AGI) of this era (Chang et al., 2024;Zhao et al., 2023b;Wei et al., 2022b).A noteworthy example is ChatGPT, designed specifically for engaging in human conversation, that demonstrates the ability to comprehend and generate human language text (Liu et al., 2023g). 1efore LLMs, researchers have been investigating ways to integrate tabular data with neural network for NLP and data management tasks (Badaro et al., 2023).Today, researchers are keen to investigate the abilities of LLMs when working with tabular data for various tasks, such as prediction, table understanding, quantitative reasoning, and data generation (Hegselmann et al., 2023;Sui et al., 2023c;Borisov et al., 2023a).</p>
<p>Tabular data stands as one of the pervasive and essential data formats in machine learning (ML), with widespread applications across diverse domains such as finance, medicine, business, agriculture, education, and other sectors that heavily rely on relational databases (Sahakyan et al., 2021;Rundo et al., 2019;Hernandez et al., 2022;Umer et al., 2019;Luan &amp; Tsai, 2021).</p>
<p>In the current work, we provide a comprehensive review of recent advancements in modeling tabular data using LLMs.In the first section, we introduce the characteristics of tabular data, then provide a brief review of traditional, deep-learning and LLM methods tailored for this area.In Section 2, we introduce key techniques related to the adaptation of tabular data for LLMs.Subsequently, we cover the applications of LLMs in prediction tasks (Section 3), data augmentation and enrichment tasks (Section 4), and question answering/table understanding tasks (Section 5).Finally, Section 6 discusses limitations and future directions, while Section 7 concludes.The overview of this paper is shown in Figure 1.</p>
<p>Characteristics of tabular data</p>
<p>Tabular data, commonly known as structured data, refers to data organized into rows and columns, where each column represents a specific feature.This subsection discusses the common characteristics and inherited challenges with tabular data:</p>
<ol>
<li>
<p>Heterogeneity: Tabular data can contain different feature types: categorical, numerical, binary, and textual.Therefore, features can range from being dense numerical features to sparse or highcardinality categorical features (Borisov et al., 2022a).</p>
</li>
<li>
<p>Sparsity: Real-world applications, such as clinical trials, epidemiological research, fraud detection, etc., often deal with imbalanced class labels and missing values, which results in long-tailed distribution in the training samples (Sauber-Cole &amp; Khoshgoftaar, 2022).</p>
</li>
<li>
<p>Dependency on pre-processing: Data pre-processing is crucial and application-dependent when working with tabular data.For numerical values, common techniques include data normalization or scaling, categorical value encoding, missing value imputation, and outlier removal.For categorical values, common techniques include label encoding or one-hot encoding.Improper pre-processing may lead to information loss, a sparse matrix, and it may introduce multi-collinearity (e.g. with Figure 1: Overview of LLM on Tabular Data: the paper discusses application of LLM for prediction, data generation, and table understanding tasks one-hot encoding), or it may introduce synthetic ordering (e.g. with ordinal encoding) (Borisov et al., 2023a).</p>
</li>
</ol>
<p>Context-based interconnection:</p>
<p>In tabular data, features can be correlated.For example, age, education, and alcohol consumption from a demographic table are interconnected: it is hard to get a doctoral degree at a young age, and there is a minimum legal drinking age.Including correlated regressors in regressions lead to biased coefficients, hence, a modeler must be aware of such intricacies (Liu et al., 2023d).</p>
<ol>
<li>
<p>Order invariant: In tabular data, samples and features can be sorted.However, as opposed to textbased and image-based data that is intrinsically tied to the position of the word/token or pixel in the text or image, tabular data are relatively order-invariant.Therefore, position-based methodologies (e.g., spatial correlation, impeding inductive bias, convolutional neural networks (CNN)) are less applicable for tabular data modeling (Borisov et al., 2022a).</p>
</li>
<li>
<p>Lack of prior knowledge: In image or audio data, there is often prior knowledge about the spatial or temporal structure of the data, which can be leveraged by the model during training.However, in tabular data, such prior knowledge is often lacking, making it challenging for the model to understand the inherent relationships between features (Borisov et al., 2022a;2023a).</p>
</li>
</ol>
<p>Traditional and deep learning in tabular data</p>
<p>This survey explores the current research landscape of LLMs in tabular data prediction, with a focus on classification task, data generation, and table understanding.</p>
<p>Tabular prediction refers to classification and regression tasks.For tabular prediction, despite advancements in the field, traditional tree-based ensemble methods such as gradient-boosted decision trees (GBDT) remain the state-of-the-art (SOTA) for classification task on tabular data (Borisov et al., 2022a;Gorishniy et al., 2021)).In boosting ensemble methods, base learners are learned sequentially to reduce previous learner's error until there is no significant improvement, making it relatively stable and accurate than a single learner (Chen &amp; Guestrin, 2016).Traditional tree-based models are known for its high performance, efficiency in training, ease of tuning, and ease of interpretation.However, they have limitations compared to deep learning models: 1. Tree-based models can be sensitive to feature engineering especially with categorical features while deep learning can learn representation implicitly during training (Goodfellow et al., 2016).2. Tree-based models are not naturally suited for processing sequential data, such as time series while deep learning models such as Recurrent Neural Networks (RNNs) and transformers excel in handling sequential dependencies.3. Tree-based models sometimes struggle to generalize to unseen data particularly if the training data is not representative of the entire distribution, while deep learning methods may generalize better to diverse datasets with their ability to learn intricate representations (Goodfellow et al., 2016).</p>
<p>For deep learning methods in tabular data prediction, the methodologies can be broadly grouped into the following categories: 1.Data transformation.These models either strive to convert heterogenous tabular input into homogenous data more suitable to neural networks, like an image, on which CNN-like mechanism can be applied (SuperTML (Sun et al., 2019), IGTD (Zhu et al., 2021b), 1D-CNN (Kiranyaz et al., 2019)), or methods focusing on combining feature transformation with deep neural networks (Wide&amp;Deep (Cheng et al., 2016;Guo &amp; Berkhahn, 2016), DeepFM (Guo et al., 2017), DNN2LR (Liu et al., 2021)).2. Differentiable trees.Inspired by the performance of ensembled trees, this line of methods seeks to make trees differentiable by smoothing the decision function (NODE (Popov et al., 2019), SDTR (Luo et al., 2021), Net-DNF (Katzir et al., 2020)).Another subcategory of methods combine tree-based models with deep neural networks, thus can maintain tree's capabilities on handling sparse categorical features (Deep-GBM (Ke et al., 2019a)), borrow prior structural knowledge from the tree (TabNN (Ke et al., 2019b)), or exploit topological information by converting structured data into a directed graph (BGNN (Ivanov &amp; Prokhorenkova, 2021).3. Attention-based methods.These models incorporate attention mechanisms for feature selection and reasoning (TabNet (Arik &amp; Pfister, 2020)), feature encoding (TransTab (Wang &amp; Sun, 2022), TabTransformer (Huang et al., 2020), TP-BERTa (Yan et al., 2024b)), feature interaction modeling (DANet (Chen et al., 2022a), T2G-Former (Yan et al., 2023), ExcelFormer (Chen et al., 2023a), ARM-net (Cai et al., 2021)), or aiding intrasample information sharing (SAINT (Somepalli et al., 2021), NPT (Kossen et al., 2022)).4. Regularization methods.The importance of features varies in tabular data, in contrast to image or text data.Thus, this line of research seeks to design an optimal and dynamic regularization mechanism to adjust the sensitivity of the model to certain inputs (e.g.RLN (Shavitt &amp; Segal, 2018), Regularization Cocktails (Kadra et al., 2021).In spite of rigorous attempts in applying deep learning to tabular data modeling, GBDT algorithms, including XGBoost, LightGBM, and CatBoost (Prokhorenkova et al., 2019), still outperform deep-learning methods in most datasets with additional benefits in fast training time, high interpretability, and easy optimization (Shwartz-Ziv &amp; Armon, 2022;Gorishniy et al., 2021;Grinsztajn et al., 2022).Deep learning models, however, may have their advantages over traditional methods in some circumstances, for example, when facing very large datasets, or when the data is primarily comprised of categorical features (Borisov et al., 2022a).</p>
<p>Another important task for tabular data modeling is data synthesis.The ability to synthesize real and highquality data is essential for model development.Data generation is used for augmentation when the data is sparse (Onishi &amp; Meguro, 2023), imputing missing values (Jolicoeur-Martineau et al., 2023), and class rebalancing in imbalanced data (Sauber-Cole &amp; Khoshgoftaar, 2022).Traditional methods for synthetic data generation are mostly based on Copulas (Patki et al., 2016;Li et al., 2020b) and Bayesian networks (Zhang et al., 2017;Madl et al., 2023) while recent advancement in generative models such as Variational Autoencoders (VAEs) (Ma et al., 2020;Darabi &amp; Elor, 2021;Vardhan &amp; Kok, 2020;Liu et al., 2023d;Xu et al., 2023b)), generative adversarial networks (GANs) (Park et al., 2018;Choi et al., 2018;Baowaly et al., 2019;Xu et al., 2019), diffusion models (Kotelnikov et al., 2022;Xu et al., 2023a;Kim et al., 2022b;a;Lee et al., 2023;Zhang et al., 2023c), and LLMs, opened up many new opportunities.These deep learning approaches have demonstrated superior performance over classical methods such as Bayesian networks ( (Xu et al., 2019)).A comprehensive understanding of the strengths and weaknesses of different tabular data synthesis methods can be found in Du &amp; Li (2024).</p>
<p>Table understanding is a broad field, covering various tasks like question answering (QA), natural language inference (NLI), Text2SQL tasks, and more.Many earlier methods fine-tune BERT (Devlin et al., 2019) to become table encoders for table-related tasks, like TAPAS (Herzig et al., 2020), TABERT (Yin et al., 2020a), TURL (Deng et al., 2022a), TUTA (Wang et al., 2021) and TABBIE (Iida et al., 2021).For example, TAPAS extended BERT's masked language model objective to structured data by incorporating additional embeddings designed to capture tabular structure.It also integrates two classification layers to facilitate the selection of cells and predict the corresponding aggregation operator.A particular table QA task, Text2SQL, involves translating natural language question into structured query language (SQL).Earlier research conducted semantic parsing through hand-crafted features and grammar rules (Pasupat &amp; Liang, 2015b).Semantic parsing is also used when the table is not coming from non-database tables such as web tables, spreadsheet tables, and others (Jin et al., 2022).Seq2SQL is a sequence-to-sequence deep neural network using reinforcement-learning to generate conditions of query on WikiSQL task (Zhong et al., 2017a).Some methodologies are sketch-based, wherein a natural language question is translated into a sketch.Subsequently, programming language techniques such as type-directed sketch completion and automatic repair are utilized in an iterative manner to refine the initial sketch, ultimately producing the final query (e.g.SQLizer (Yaghmazadeh et al., 2017)).Another example is SQLNet (Xu et al., 2017) which uses column attention mechanism to synthesize the query based on a dependency graph-dependent sketch.</p>
<p>A derivative of SQLNet is TYPESQL (Yu et al., 2018a) which is also a sketch-based and slot-filling method entails extracting essential features to populate their respective slots.Unlike the previous supervised endto-end models, TableQuery is a NL2SQL model pretrained on QA on free text that obviates the necessity of loading the entire dataset into memory and serializing databases.</p>
<p>Figure 2: Tabular data characteristics and machine learning models for tabular data prediction, data synthesis and table understanding like question answering before LLMs.</p>
<p>Overview of large language models (LLMs)</p>
<p>A language model (LM) is a probabilistic model that predicts the generative likelihood of future or missing tokens in a word sequence.Zhao et al. (2023b) thoroughly reviewed the development of LMs, and characterized the it into four different stages: The first stage is Statistical Language Models (SLM), which learns the probability of word occurrence in an example sequence from previous words (e.g.N-Gram) based on Markov assumption (Saul &amp; Pereira, 1997).Although a more accurate prediction can be achieved by increasing the context window, SLM is limited by the curse of high dimensionality and high demand for computation power (Bengio et al., 2000).Next, Neural Language Models (NLM) utilize neural networks (e.g.Recurrent neural networks (RNN)) as a probabilistic classifier (Kim et al., 2015).In addition to learning the probabilistic function for word sequence, a key advantage of NLM is that they can learn the distributed representation (i.e.word embedding) of each word so that similar words are mapped close to each other in the embedding space (e.g.Word2Vec); thus, the model can generalize well to unseen sequences, as well as it avoids the curse of dimensionality (Bengio et al., 2000).Later, rather than learning a static word embedding, context-aware representation learning was introduced by pretraining the model on large-scale unannotated corpora using bidirectional LSTM that takes context into consideration (e.g., ELMo (Peters et al., 2018a)), which shows significant performance boost in various natural language processing (NLP) tasks (Wang et al., 2022a;Peters et al., 2018b).Along this line, several other Pretrained Language Models (PLM) were proposed utilizing a transformer architecture with self-attention mechanisms including BERT and GPT2 (Ding et al., 2023).The pre-training and fine-tuning paradigm, closely related to transfer learning, allows the model to gain general syntactic and semantic understanding of the text corpus and then be trained on task-specific objectives to adapt to various tasks.The final and most recent stage of LM is the Large Language Models (LLMs), and will be the focus of this paper.Motivated by the observation that scaling the data and model size usually leads to improved performance, researchers sought to test the boundaries of PLM's performance of a larger size, such as "text-to-text transfer transformers" (T5) (Raffel et al., 2023), GPT-3 (Brown et al., 2020), etc. Intriguingly, some advanced abilities emerge as a result.These large-sized PLMs (i.e.LLMs) show unprecedentedly powerful capabilities (also called emergent abilities) that go beyond traditional language modeling and start to gain capability to solve more general and complex tasks which was not seen in PLM.Formally, we define a LLM as follows:</p>
<p>Definition 1 (Large Language Model).A large language model (LLM) M , parameterized by θ, is a transformer-based model with an architecture that can be autoregressive, autoencoding, or encoder-decoder.It has been trained on a large corpus comprising hundreds of millions to trillions of tokens.LLMs encompass pre-trained models and for our survey, refers to models that have at least 1 billion parameters.</p>
<p>Several key emergent abilities of LLMs are critical for data understanding and modeling including in-context learning, instruction following, and multi-step reasoning.In-context learning refers to designing large auto-regressive language models that generate responses on unseen task without gradient update, only learning through a natural language task description and a few in-context examples provided in the prompt.The GPT3 model (Brown et al., 2020) with 175 billion parameters presented an impressive incontext learning ability that was not seen in smaller models.LLMs have also demonstrated the ability to complete new tasks by following only the instructions of the task descriptions (also known as zero-shot prompts).Some papers also fine-tuned LLMs on a variety of tasks presented as instructions (Thoppilan et al., 2022).However, instruction-tuning is reported to work best only for larger-size models (Wei et al., 2022a;Chung et al., 2022).Solving complex tasks involving multiple steps have been challenging for LLMs.By including intermediate reasoning steps, prompting strategies such as chain-of-thought (CoT) has been shown to help unlock the LLM ability to tackle complex arithmetic, commonsense, and symbolic reasoning tasks (Wei et al., 2023).These new abilities of LLMs lay the groundwork for exploring their integration into intricate tasks extending beyond traditional NLP applications across diverse data types.</p>
<p>Applications of LLMs in tabular data</p>
<p>Despite the impressive capabilities of LM in addressing NLP tasks, its utilization for tabular data learning has been constrained by differences in the inherent data structure.Some research efforts have sought to utilize the generic semantic knowledge contained in PLM, predominantly BERT-based models, for modeling tabular data (Figure 3).This involves employing PLM to learn contextual representation with semantic information taking header information into account (Chen et al., 2020c).The typical approach includes transforming tabular data into text through serialization (detailed explanation in Section 2) and employing a masked-language-modeling (MLM) approach for fine-tuning the PLM, similar to that in BERT (PTab, CT-BERT, TABERT (Liu et al., 2022a;Ye et al., 2023a;Yin et al., 2020a).In addition to being able to incorporate semantic knowledge from column names, converting heterogenous tabular data into textual representation enables PLMs to accept inputs from diverse tables, thus enabling cross-table training.Also, due to the lack of locality in tabular data, models need to be invariant to permutations of the columns (Ye et al., 2023a).In this fashion, TABERT was proposed as a PLM trained on both natural language sentence and structured data (Yin et al., 2020a), PTab demonstrated the importance of cross-table training for an enhanced representation learning (Liu et al., 2022a), CT-BERT employs masked table modeling (MTM) and contrastive learning for cross-table pretraining that outperformed tree-based models (Ye et al., 2023a).However, previous research primarily focuses on using LM for representation learning, which is quite limited.</p>
<p>Opportunities for LLMs in tabular data modeling</p>
<p>Many studies today explore the potential of using LLMs for various tabular data tasks, ranging from prediction, data generation, to data understanding (further divided into question answering and data reasoning).This exploration is driven by LLMs' unique capabilities such as in-context learning, instruction following, and step-wise reasoning.The opportunities for applying LLMs to tabular data modeling are as follows:</p>
<ol>
<li>
<p>Deep learning methods often exhibit suboptimal performance on datasets they were not initially trained on, making transfer learning using the pre-training and fine-tuning paradigm highly promising (Shwartz-Ziv &amp; Armon, 2022).</p>
</li>
<li>
<p>The transformation of tabular data into LLM-readable natural language addresses the curse of dimensionality (created by the one-hot encoding of categorical data).</p>
</li>
<li>
<p>The emergent capabilities, such as step-by-step reasoning through CoT, have transformed LM from language modeling to a more general task-solving tool.Research is needed to test the limit of LLM's emergent abilities on tabular data modeling.</p>
</li>
</ol>
<p>Contribution</p>
<p>The key contributions of this work are as follows:</p>
<p>1.A formal break down of key techniques for LLMs' applications on tabular data We split the application of LLM in tabular data to tabular data prediction, tabular data synthesis, tabular data question answering and table understanding.We further extract key techniques that can apply to all applications.We organize these key techniques in a taxonomy that researchers and practitioners can leverage to describe their methods, find relevant techniques and understand the difference between these techniques.We further subdivide each technique to subsections so that researchers can easily find relevant benchmark techniques and properly categorize their proposed techniques.</p>
<ol>
<li>
<p>A survey and taxonomy of metrics for LLMs' applications on tabular data.For each application, we categorize and discuss a wide range of metrics that can be used to evaluate the performance of that application.For each application, we documented the metric of all relevant methods, and we identify benefits/limitations of each class of metrics to capture application's performance.We also provide recommended metrics when necessary.</p>
</li>
<li>
<p>A survey and taxonomy of datasets for LLMs' applications on tabular data.For each application, we identify datasets that are commonly used for benchmark.For table understanding and question answering, we further categorize datasets by their downstream applications: Question Answering, Natural Language Generation, Classification, Natural Language Inference and Text2SQL.We further provided recommended datasets based on tasks and their GitHub link.Practitioners and researchers can look at the section and find relevant dataset easily.We share publicly-available datasets here: https://github.com/tanfiona/LLM-on-Tabular-Data-Prediction-Table-Understanding-Data-Generation4. A survey and taxonomy of techniques for LLMs' applications on tabular data.For each application, we break down an extensive range of tabular data modeling methods by steps.For example, tabular data prediction can be breakdown by pre-processing (modifying model inputs), target augmentation (modifying the outputs), fine-tuning (fine-tuning the model).We construct granular subcategories at each stage to draw similarities and trends between classes of methods, and we provide examples of main techniques.Practitioners and researchers can look at the section and understand the difference of each technique.We only recommend benchmark methods and provide GitHub link of these techniques for reference and benchmark.</p>
</li>
<li>
<p>An overview of future research directions.Future research could focus on how to solve bias problem in tabular data modeling, how to mitigate hallucinations, how to find better representations of numerical data, how to improve capacity, how to form standard benchmarks, how to improve model interpretability, how to create an integrated workflow, how to design better fine-tuning strategies and finally, how to improve the performance of downstream applications.</p>
</li>
</ol>
<p>Key techniques for LLMs' applications on tabular data</p>
<p>While conducting our survey, we noticed a few common components in modeling tabular data with LLMs across tasks.We discuss common techniques, like serialization, table manipulations, prompt engineering, and building end-to-end systems in this section.Fine-tuning LLMs is also popular, but it tends to be application-specific, thus we discuss it later, in Sections 3 and 5.</p>
<p>Figure 4: Key techniques in using LLMs for tabular data.The dotted line indicates steps that are optional.</p>
<p>Serialization</p>
<p>Since LLMs are sequence-to-sequence models, in order to feed tabular data as inputs into an LLM, we have to convert the structured tabular data into a text format (Sui et al., 2023b;Jaitly et al., 2023).2020b), TURL (Deng et al., 2022a), TUTA (Wang et al., 2021), TABBIE (Iida et al., 2021) and UTP (Chen et al., 2023b).Cong et al. (2023) discuss the pros and cons of the learned table representations of a few of these encoders.For LLMs with &gt;1B parameters, there are UniTabPT (Sarkar &amp; Lausen, 2023) with 3B parameters (based on T5 and Flan-T5 models)), TableGPT (Gong et al., 2020) with 1.5B parameters (based on GPT2), TableGPT3 (Zha et al., 2023) with 7B parameters (based on Phoenix (Chen et al., 2023c)), TableLlama (Zhang et al., 2023f) with 7B parameters (based on Llama 2 (Touvron et al., 2023b)), and Table -GPT with 350M, 3B, 13B or 175B parameters (based on various versions of OpenAI's GPT models).</p>
<p>Text-based</p>
<p>Graph-based &amp; Tree-based</p>
<p>A possible, but less explored serialization method involves converting a table to a graph or tree data structure.However, when working with sequence-to-sequence models, these structures must still be converted back to text.For Zhao et al. (2023a), after converting the table into a tree, each cell's hierarchical structure, position information, and content was represented as a tuple and fed into GPT3.5.</p>
<p>Comparisons Research has shown that LLM performance is sensitive to the input tabular formats.Singha et al. (2023) found that DFLoader and JSON formats are better for fact-finding and table transformation tasks.Meanwhile, Sui et al. (2023a) found that HTML or XML table formats are better understood by GPT models over tabular QA and FV tasks.However, they require increased token consumption.Likewise, Sui et al. (2023b) also found that markup languages, specifically HTML, outperformed X-separated formats for GPT3.5 and GPT4.Their hypothesis is that the GPT models were trained on a significant amount of web data and thus, probably exposed the LLMs to more HTML and XML formats when interpreting tables.</p>
<p>Apart from manual templates, Hegselmann et al. ( 2023) also used LLMs (Fine-tuned BLOOM on ToTTo (Parikh et al., 2020b), T0++ (Sanh et al., 2022), GPT-3 (Ouyang et al., 2022)) to generate descriptions of a table as sentences, blurring the line between a text-based and embedding-based serialization methodology.</p>
<p>Published in Transactions on Machine Learning Research (07/2024) However, for the few-shot classification task, they find that traditional list and text templates outperformed the LLM-based serialization method.Amongst LLMs, the more complex and larger the LLM, the better the performance (GPT-3 has 175B, T0 11B, and fine-tuned BLOOM model 0.56B parameters).A key reason why the LLMs are worse off at serializing tables to sentences is due to the tendency for LLMs to hallucinate: LLMs respond with unrelated expressions, adding new data, or return incorrect feature values.</p>
<p>Table Manipulations</p>
<p>Table manipulations refer to operations and transformations performed on tabular data, typically stored in databases or spreadsheets.These manipulations involve actions such as filtering, sorting, joining, aggregating, and transforming data.An important characteristic of tabular data is its heterogeneity in structure and content.They often come in large size with different dimensions encompassing various feature types.In order for LLMs to ingest tabular data efficiently, it is important to compress the tables to fit context lengths, for better performance and reduced costs.Therefore, table manipulations are required in some scenarios, as described below.</p>
<p>Compacting tables to fit context lengths, for better performance and reduced costs For smaller tables, it might be possible to include the whole table within a prompt.However, for larger tables, there are three challenges:</p>
<p>Firstly, some models have short context lengths (E.g.Flan-UL2 (Tay et al., 2023b) supports 2048 tokens, Llama 2 (Touvron et al., 2023b) supports 4096 context tokens) and even models that support large context lengths might still be insufficient for extremely large tables with over 200K rows (Claude 2.1 supports up to 200K tokens).</p>
<p>Secondly, even if the table could fit the context length, most LLMs are slow to process long sentences due to the quadratic complexity of self-attention (Sui et al., 2023b;Tay et al., 2023a;Vaswani et al., 2017).When dealing with long contexts, performance of LLMs significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models (Liu et al., 2023b).For tabular data, Cheng et al. (2023); Sui et al. (2023c) highlights that noisy information becomes an issue in large tables for LMs.Chen (2023) found that for table sizes beyond 1000 tokens, GPT-3's performance degrades to random guesses.</p>
<p>Thirdly, longer prompts incur higher costs, especially for applications built upon LLM APIs.</p>
<p>To address these issues, Herzig et al. (2020); Liu et al. (2022c) proposed methods to truncate the input based on a maximum sequence length.Sui et al. (2023b) introduced predefined certain constraints to meet the LLM call request.Another strategy is to do search and retrieval of only highly relevant tables, rows, columns or cells which we will discuss later in Section 5.</p>
<p>Additional information about tables for better performance</p>
<p>Apart from the table, some papers explored including table schemas and statistics as part of the prompt.Sui et al. (2023c) explored including additional information about the tables: Information like " dimension, measure, semantic field type" help the LLM achieve higher accuracy across all six datasets explored."Statistics features" improved performance for tasks and datasets that include a higher proportion of statistical cell contents, like FEVEROUS (Aly et al., 2021).Meanwhile, "document references" and "term explanations" add context and semantic meaning to the tables."</p>
<p>Prompt Engineering</p>
<p>A prompt is an input text that is fed into an LLM.Designing an effective prompt is a non-trivial task, and many research topics have branched out from prompt engineering alone.In this subsection, we cover the popular techniques in prompt engineering, and how researchers have used them for tasks involving tables.</p>
<p>Prompt format</p>
<p>The simplest format is concatenating task description with the serialized table as string.</p>
<p>An LLM would then attempt to perform the task described and return a text-based answer.Clearly-defined and well-formatted task descriptions are reported to be effective prompts (Marvin et al., 2023).Some other strategies to improve performance are described in the next few paragraphs.Sui et al. (2023b) recommended that external information (such as questions and statements) should be placed before the tables in prompts for better performance.</p>
<p>In-context learning As one of the emergent abilities of LLMs (see 1.3), in-context learning refers to incorporates similar examples to help the LLMs understand the desired output.Sui et al. (2023b) observed significant performance drops performance, of overall accuracy decrease of 30.38% on all tasks, when changing their prompts from a 1-shot to a 0-shot setting.In terms of choosing appropriate examples, Narayan et al. (2022) found their manually curated examples to outperform randomly selected examples by an average of 14.7 F1 points.For Chen ( 2023), increasing from 1-shot to 2-shot can often benefit the model, however, further increases did not help.</p>
<p>Chain-of-Thought and Self-consistency Chain-of-Thought (CoT) (Wei et al., 2022c) induces LLMs to decompose a task by performing step-by-step thinking, resulting in better reasoning.Program-of-Thoughts (Chen et al., 2022b) guides the LLMs using code-related comments like "Let's write a program step-by-step...".Zhao et al. (2023d) explored CoT and PoT strategies for the numerical QA task.Yang et al. (2023) prompt the LLMs with one shot CoT demonstration example to generate a reasoning and answer.Subsequently, they included the reasoning texts, indicated by special "<CoT>" token, as part of inputs to fine-tune smaller models to generate the final answer.</p>
<p>Self-consistency (SC) (Wang et al., 2023b) leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer.SC samples a diverse set of reasoning paths from an LLM, then selects the most consistent answer by marginalizing out the sampled reasoning paths.Inspired by these strategies, Zhao et al. (2023a); Ye et al. (2023b) experimented with multi-turn dialogue strategies, where they decompose the original question into sub-tasks or sub-questions to guide the LLM's reasoning.Sui et al. (2023c) instructed the LLM to "identify critical values and ranges of the last table related to the statement" to obtain additional information that were fed to the final LLM, obtaining increased scores for five datasets.Liu et al. (2023e) also investigated strategies around SC, along with self-evaluation, which guides the LLM to choose between the two reasoning approaches based on the question's nature and each answer's clarity.Deng et al. (2022b) did consensus voting across a sample a set of candidate sequences, then selected final response by ensembling the derived response based on plurality voting.</p>
<p>Chen (2023) investigated the effects of both CoT and SC on QA and FV tasks.When investigating the explainability of LLM's predictions, Dinh et al. (2022) experimented with a multi-turn approach of asking GPT3 to explain its own prediction from the previous round, and guided the explanation response using CoT by adding the line "Let's think logically.This is because".</p>
<p>Retrieval-augmented generation (RAG)</p>
<p>Retrieval-augmented generation (RAG) relies on the intuition that the LLMs are general models, but can be guided to a domain-specific answer if the user includes the relevant context within the prompts.By incorporating tables as part of the prompts, most papers described in this survey can be attributed as RAG systems.A particular challenge in RAG is to extract the most relevant information out of a large pool of data to better inform the LLMs.This challenge overlaps slightly with the strategies about</p>
<p>LLMs for predictions</p>
<p>Several studies have leveraged LLMs for prediction in tabular data.This section will delve into the existing methodologies and advancements in two categories of tabular data: standard feature-based tabular data and time series data.Time series prediction differs from normal feature-based tabular data since the predictive power heavily relies on the past.For each category, we divide it into different steps which include preprocessing, fine-tuning, and target augmentation.Preprocessing explains how different prediction methods generate input to the language model.Preprocessing includes serialization, table manipulation, and prompt engineering.Target augmentation maps the textual output from LLMs to a target label for prediction tasks.</p>
<p>At the end, we will briefly cover domain-specific prediction methods using LLMs.</p>
<p>Datasets</p>
<p>For task-specific fine-tuning, most datasets used for the prediction task are from UCI ML, OpenML, or a combo of 9 datasets created by Manikandan et al. (2023).Details of the datasets are in  F1 score is calculated from the precision and recall of the test, where the precision is the number of true positive results divided by the number of all samples predicted to be positive, including those not identified correctly, and the recall is the number of true positive results divided by the number of all samples that should have been identified as positive.CRPS is continous ranked probability score.We will introduce other metrics in relevant sections.</p>
<p>Tabular prediction</p>
<p>Preprocessing Preprocessing in LLM-based tabular prediction involves steps like table manipulation, serialization, and prompt engineering, which have been discussed earlier.Specifically, some LLM-based prediction methods incorporated a statistical summary of the tabular data as part of the input to LLM.Serialization in the prediction task is mostly Text-based (refer to Section 2.1).Prompt engineering includes incorporating task-specific cues and relevant samples into the prompt (refer to Section 2.1).The various preprocessing methods are illustrated in Table 4 and discussed in detail below.</p>
<p>As one of the earliest endeavors, LIFT (Dinh et al., 2022) tried a few different serialization methods, such as feature and value as a natural sentence such as "The column name is Value" or a set of equations, such as col 1 = val 1 , col 2 = val 2 , ....The former achieves higher prediction accuracy, especially in low-dimensional tasks.The same conclusion was drawn in TabLLM (Hegselmann et al., 2023) where they evaluated 9 different serialization methods along with a description for the classification problem.They found that a textual enumeration of all features: 'The column name is Value', performs the best.For medical prediction, they mimic the thinking process of medical professionals as prompt engineering and found that LLM makes use of column name and their relationships in few-shot learning settings.</p>
<p>In a subsequent study, TABLET (Slack &amp; Singh, 2023)  this fashion, more studies tested a more complex serialization and prompt engineering method rather than a simple concatenation of feature and value for serialization.</p>
<p>The schema-based prompt engineering usually includes background information about the dataset, a task description, a summary, and example data points.Summary Boosting (Manikandan et al., 2023) serializes data and metadata into text prompts for summary generation.This includes categorizing numerical features and using a representative dataset subset selected via weighted stratified sampling based on language embeddings.Serialize-LM (Jaitly et al., 2023) introduces 3 novel serialization techniques that boost LLM performance in domain-specific datasets.They included related features in one sentence to make the prompt more descriptive and easier for an LLM to understand.Take the task of car classification as an example: attributes like make, color, and body type are now combined into a single,richer sentence.It leverages covariance to identify the most relevant features and either label them critically or add a sentence to explain the most important features.Finally, they converted tabular data into LaTeX code format.This LaTeX representation of the table was then used as the input for fine-tuning our LLM by just passing a row representation preceded by hline tag without any headers.</p>
<p>Another work worth mentioning is UniPredict (Wang et al., 2023a), which reformats metadata by consolidating arbitrary input M to a description of the target and the semantic descriptions of features.Feature serialization follows a "column name is value" format.The objective is to minimize the difference between the output sequence generated by the adapted LLM function and the reference output sequence generated from target augmentation (represented by serialized target).To make LLMs applicable to multiple tabular datasets at the same time, Generative Tabular Learning (GTL) was proposed by Zhang et al. (2023a).It includes two parts: 1) the first part specifies the task background and description with optionally some examples as in-context examples (Prompt Engineering); 2) the second part describes feature meanings and values of the current instance to be inferred (Serialization); LIFT and TabLLM have been benchmarked by at least 3 other papers.The code for both methods is available.7</p>
<p>Some other methods leverage an LLM to rewrite the serialization or perform prompt engineering.TabLLM (Hegselmann et al., 2023) showed that LLM is not good for serialization because it is not faithful and may hallucinate.Summary Boosting (Manikandan et al., 2023) uses GPT3 to convert metadata to data description and generate a summary for a subset of datasets in each sample round.TABLET (Slack &amp; Singh, 2023) fits a simple model such as a one-layer rule set model or prototype with the 10 most important features on the task's full training data.It then serializes the logic into text using a template and revises the templates using GPT3.Based on their experiments, it was found that contrary to the naturally occurring instructions, LLM-generated instructions do not significantly improve performance.</p>
<p>Target Augmentation LLMs can solve complex tasks through text generation, however, the output is not always controllable (Dinh et al., 2022).As a result, mapping the textual output from LLMs to a target label for prediction tasks is essential.This is called target augmentation (Wang et al., 2023a).A straightforward but labor-intensive way is manual labeling, as was used by Serilize-LM (Jaitly et al., 2023).To be more automatic, LIFT (Dinh et al., 2022) utilizes ### and @@@ to demarcate question-answer pairs and signify the end of generation.These markers prompt the LLM to position answers between ### and @@@.This approach significantly aligns most generated answers with the intended labels.Additionally, to address potential inaccuracies in inference outputs, LIFT conducts five inference attempts, defaulting to the training set's average value if all attempts fail.In streamlining the two-step approach, TabLLM (Hegselmann et al., 2023) incorporates the use of Verbalizer (Cui et al., 2022) to map the answer to a valid class.To calculate AUCROC or AUCPR, the probability of the output is necessary.Thus, Verbalizer proves advantageous for closed-source models by enabling the assignment of output probability.UniPredict (Wang et al., 2023a) has the most complicated target augmentation.They transform the target label into a set of probabilities for each class via a function called "augment".Formally, for a target T in an arbitrary dataset D, they define a function augment(T ) = (C, P ), where C are new categories of targets with semantic meaning and P are the assigned probabilities to each category.They extend the target into categorical one-hot encoding and then use an external predictor to create the calibrated probability distributions.This replaces the 0/1 one-hot encoding while maintaining the final prediction outcome.Formally, given the target classes t ∈ 0, ..., |C| and target probabilities p ∈ P , they define a function serialize target(t, p) that serializes target classes and probabilities into a sequence formatted as "class t 1 : p 1 , t 2 : p 2 , . . .".We give an example for each method in 5</p>
<p>Inference-Only Prediction Some work uses LLMs directly for prediction without fine-tuning, we refer to these approaches as inference-only prediction.TABLET (Slack &amp; Singh, 2023) utilizes models like Tk-Instruct (Wang et al., 2022b), Flan-T5 (Chung et al., 2022), GPT-J (Black et al., 2022), and ChatGPT to perform inference, but it reports that a KNN approach with feature weights from XGBoost surpasses Flan-T5 11b in performance using similar examples and instructions.Summary Boosting (Manikandan et al., 2023) creates multiple inputs through the serialization step.The AdaBoost algorithm then creates an ensemble of summary-based weak learners.While non-fine-tuned LLMs struggle with continuous attributes, summary boosting is effective with smaller datasets.Furthermore, its performance is enhanced using GPTgenerated descriptions by leveraging existing model knowledge, underscoring the potential of LLMs in new domains with limited data.However, it does not perform well when there are many continuous variables.</p>
<p>For any new LLM-based prediction method without any fine-tuning, we suggest benchmarking LIFT and TABLET.LIFT is the first LLM-based method for inference-only prediction.TABLET shows significantly better performance than LIFT with 3 different base models.</p>
<p>Fine-tuning</p>
<p>For studies involving fine-tuning, they typically employ one of two distinct approaches.The first involves training an LLM model on large datasets to learn fundamental features before adapting it to specific prediction tasks.The second takes a pre-trained LLM and further trains it on a smaller, specific prediction dataset to specialize its knowledge and improve its performance on the prediction.LIFT (Dinh et al., 2022) fine-tunes pre-trained language models like GPT-3 and GPT-J using Low-Rank Adaptation (LoRA) on the training set.They found that LLM with general pretraining could improve performance.However, the performance of this method does not surpass the in-context learning result.TabLLM (Hegselmann et al., 2023) uses T0 model (Sanh et al., 2021) and T-few (Liu et al., 2022b) for fine-tuning.TabLLM has demonstrated remarkable few-shot learning capabilities, outperforming traditional deep-learning methods and gradient-boosted trees.TabLLM's efficacy is highlighted by its ability to leverage the extensive knowledge encoded in pre-trained LLMs from these models, requiring minimal labeled data.However, the sample efficiency of TabLLM is highly task-dependent.Other research also leverages T0 as the based model.Jaitly et al. (2023) uses T0 (Sanh et al., 2021).Compared to TabLLM, it is trained using Intrinsic Attention-based Prompt Tuning (IA3) (Liu et al., 2022b).However, this method only works for a few short learning, worse than baseline when the number of shots is more or equal to 128.T0 model (Sanh et al., 2021) is commonly used as the base model for tabular prediction fine-tuning.</p>
<p>PLM can be effectively adapted for diverse tabular prediction tasks, demonstrating their versatility across heterogeneous datasets (Yan et al., 2024b).UniPredict (Wang et al., 2023a) trains a single LLM (GPT2) on an aggregation of 169 tabular datasets with diverse targets and observes advantages over existing methods.This model does not require fine-tuning LLM on specific datasets.Model accuracy and ranking are better than XGBoost when the number of samples is small.The model with target augmentation performs noticeably better than the model without augmentation.It does not perform well when there are too many columns or fewer representative features.GTL (Zhang et al., 2023a) fine-tunes LLaMA to predict the next token using 115 tabular datasets.To balance the number of instances across different datasets, they randomly sample up to 2,048 instances from each tabular dataset for GTL.They employed GTL which significantly improves LLaMA in most zero-shot scenarios.Based on the current evidence, we believe that fine-tuning on large number of datasets could further improve the performance.However, both UniPredict and GTL have not released their code yet.</p>
<p>Metric Among all tabular prediction methods surveyed, AUC is mostly commonly used metric for classification prediction and RMSE is mostly commonly used metric for regression 3</p>
<p>Time Series Forecasting</p>
<p>Compared to prediction on feature-based tabular data with numerical and categorical features, time series prediction pays more attention to numerical features and temporal relations.that they use LLM for time series.However, most of these papers use LLM which is smaller than 1B.We will not discuss these methods here.Please refer to Jin et al. (2023b) for a complete introduction of these methods.</p>
<p>Preprocessing PromptCast (Xue &amp; Salim, 2022) uses raw value as input the time series data in text format and adds a minimal description of the task; as output, it uses the target after it converts it to a sentence.ZeroTS (Gruver et al., 2023) argues that numbers are not encoded well in the original LLM encoding method.Thus, it encodes numbers by breaking them down by a few digits or by each single digit for GPT-3 and LLaMA, respectively.It uses spaces and commas for separation and omitting decimal points.Time LLM (Jin et al., 2023a) involves concatenating time series sequences into embeddings and integrating them with word embeddings to create a comprehensive input.This input is complemented by dataset context, task instructions, and input statistics as a prefix.TEST (Sun et al., 2023a) introduces an embedding layer tailored for LLMs, using exponentially dilated causal convolution networks for time series processing.The embedding is generated through contrastive learning with unique positive pairs and aligning text and time series tokens using similarity measures.Serialization involves two QA templates, treating multivariate time series as univariate series for sequential template filling.</p>
<p>Target Augmentation</p>
<p>In terms of output mapping, ZeroTS (Gruver et al., 2023) proposes drawing multiple samples and using statistical methods or quantiles for point estimates or ranges.For Time-LLM (Jin et al., 2023a), the output processing is done through flattening and linear projection.The target augmentation method of ZeroTS is easy to implement8 .</p>
<p>Inference-Only Prediction Similar to feature-based tabular prediction, researchers explored LLMs' performance for time series forecasting without fine-tuning.ZeroTS (Gruver et al., 2023) examines the use of LLMs like GPT-3 (Brown et al., 2020) and LLaMA-70B (Touvron et al., 2023a) directly for time series forecasting.It evaluates models using mean absolute error (MAE), Scale MAE, and continuous ranked probability score (CRPS), noting LLMs' preference for simple rule-based completions and their tendency towards repetition and capturing trends.The study reports that LLMs are able to capture time series data distributions and to handle missing data without special treatment.However, this approach is constrained by the size of the window and the tokenization method of numerical feature, preventing it from further improvement.</p>
<p>Fine-tuning Fine-tuning the model for time series prediction is more commonly seen in current research.</p>
<p>PromptCast (Xue &amp; Salim, 2022) tried the method of inference-only prediction or fine-tuning on task-specific datasets.It shows that larger models always perform better.Time LLM (Jin et al., 2023a) presents a novel approach to time series forecasting by fine-tuning LLMs like LLaMa (Touvron et al., 2023a) and GPT-2 (Brown et al., 2020).Time-LLM is evaluated using metrics like the symmetric mean absolute percentage error (SMAPE), the mean absolute scaled error (MSAE), and the overall weighted average (OWA).It demonstrates notable performance in few-shot learning scenarios, where only 5 percent or 10 percent of the data are used.This innovative technique underscores the versatility of LLMs in handling complex forecasting tasks.For TEST (Sun et al., 2023a), soft prompts are used for fine-tuning.The paper evaluates models like Bert, GPT-2 (Brown et al., 2020) (Wang et al., 2023c) aims to create a foundation model in the medical field.For preprocessing, Meditab utilizes GPT-3.5 (Brown et al., 2020) to convert tabular data into textual format, with a focus on extracting key values.Subsequently, it employs techniques such as linearization, prompting, and sanity checks to ensure accuracy and to mitigate errors.For fine-tuning, the system further leverages multitask learning on domain-specific datasets, generates pseudo-labels for additional data, and refines them using Shapley scores.Pretraining on the refined dataset is followed by fine-tuning using the original data.The resulting model supports both zero-shot and few-shot learning for new datasets.GPT-3.5 accessed via OpenAI's API facilitates data consolidation and augmentation, while UnifiedQA-v2-T5 (Khashabi et al., 2022) is employed for sanity checks.Additionally, Meditab utilizes a pretrained BioBert classifier (Lee et al., 2019).The system undergoes thorough evaluation across supervised, few-shot, and zero-shot learning scenarios within the medical domain, demonstrating superior performance compared to gradient-boosting methods and existing LLM-based approaches.However, it may have limited applicability beyond the medical domain.The code is available.9for tabular prediction tasks specifically in the medical domain.On top of AUCROC, they also use a precision-recall curve (PRAUC) for evaluation.PRAUC is useful in imbalanced datasets, which is always the case for medical data.</p>
<p>Without any pretraining, LLM has also demonstrated superior performance.CPLLM (Shoham &amp; Rappoport, 2023) leverages LLMs (Llama2 and BioMedLM) and does fine-tuning with QLora to predict diseases using structured EHR data.CPLLM demonstrated significant improvements over the state-of-the-art in all tested disease prediction tasks.Additionally, this approach, with an extended sequence length, is also suitable for patients who were not hospitalized.LLM has also been combined with Vertical models to do medical prediction Yan et al. (2024a), showcasing remarkable performance even without any manual labels.</p>
<p>Financial Prediction FinPT (Yin et al., 2023) presents an LLM-based approach to financial risk prediction.</p>
<p>The method involves filling tabular financial data into a pre-defined template, prompting LLMs like ChatGPT and GPT-4 to generate natural-language customer profiles.These profiles are then used to fine-tune large foundation models such as BERT (Devlin et al., 2019), employing the models' official tokenizers.The process enhances the ability of these models to predict financial risks, with Flan-T5 emerging as the most effective backbone model in this context, particularly across eight datasets.For financial data, FinBench contains 10 datasets with varied training set sizes (from 2k -140k) and feature sizes (from 9 -120)10 .</p>
<p>Recommendation Prediction CTRL (Li et al., 2023e) proposes a novel method for Click Through Rate (CTR) prediction by converting tabular data into text using human-designed prompts, making it understandable for language models.The model treats tabular data and generated textual data as separate modalities, feeding them into a collaborative CTR model and a pre-trained language model such as ChatGLM (Zeng et al., 2023), respectively.CTRL employs a two-stage training process: the first stage involves cross-modal contrastive learning for fine-grained knowledge alignment, while the second stage focuses on fine-tuning a lightweight collaborative model for downstream tasks.The approach outperforms all the SOTA baselines including semantic and collaborative models over three datasets by a significant margin, showing superior prediction capabilities and proving the effectiveness of the paradigm of combining collaborative and semantic signals.However, the code for this method is not available.They use LogLoss and AUC to evaluate the method.</p>
<p>LLMs for tabular data generation</p>
<p>Tabular data synthesis serves numerous purposes across diverse domains, including augmenting training datasets for machine learning models (Fonseca &amp; Bacao, 2023) to improve models' predictive accuracy and generalization capabilities.Moreover, it's crucial for data privacy (Assefa et al., 2020), where it enables the creation of synthetic replicas of sensitive data, protecting confidential information while still preserving the statistical properties essential for analysis.Additionally, tabular data synthesis aids in data preprocessing, filling missing values (Zheng &amp; Charoenphakdee, 2022) and ensuring dataset integrity and completeness.This enhances the reliability of subsequent analyses and model building.</p>
<p>Recent studies have increasingly relied on LLMs to synthesize tabular data, leveraging their advanced generative capabilities developed through extensive training on vast text corpora, including markdown-formatted serialized tabular data.This proficiency allows LLMs to capture the intricate patterns and relationships inherent in tabular datasets (Bordt et al., 2024).Furthermore, LLMs possess rich language and data un-derstanding, enabling them to produce synthetic datasets faithful to real-world statistics, with semantic coherence and contextuality (Sui et al., 2024).</p>
<p>Methodologies</p>
<p>Table 6 summarizes different LLM-powered table synthesis methods.Except for CLLM (Seedat et al., 2023), which utilizes prior knowledge from LLMs (e.g., GPT4) to augment and enhance training data samples in low-data settings without fine-tuning the LLMs, other methods such as GReaT (Borisov et al., 2023b), TAPTAP (Zhang et al., 2023e), TabuLa (Zhao et al., 2023f), andTabMT (Gulati &amp;Roysdon, 2023) all involve fine-tuning the LLMs on a corresponding table.In standard data scenarios, fine-tuning an LLM to improve its ability to capture a table's data distribution becomes essential.This is because presenting the entire training table (often comprising millions of rows) to LLMs for in-context learning poses several challenges: 1) the low success ratio to extract the output cell values, where generated data samples may diverge from intended model output formats; 2) LLMs, acting as ICL, struggle to capture column-wise tail distributions due to the "lost-in-the-middle" phenomenon.</p>
<p>Used LLM Fine-tuned or not Serialization Metric GReaT (Borisov et al., 2023b) GPT2 In this section we survey methodologies that leverage LLMs for tabular data synthesis.We categorize the methods into two typical classes, Causal Language Modeling (CLM)-powered methods and Masked Language Modeling (MLM)-powered methods.CLM, as an autoregressive method used in GPT-based models, predicts the next token based on previous ones, focusing solely on past context.To model tabular data with unordered columns, permutation-invariant techniques are typically employed in CLM-powered methods.MLM involves masking tokens in the input sequence, with the model learning to predict these masked tokens based on surrounding context.This method benefits from bidirectional context, enabling consideration of both past and future tokens during predictions.(Zhao et al., 2023f), on the other hand, addresses the slow training of LLMs by using a randomly initialized model as the starting point; the method achieves continuous refinement through iterative fine-tuning on successive tabular data tasks13 .It introduces a token sequence compression method and a middle padding strategy to simplify training data representation and enhance performance, achieving a significant reduction in training time while maintaining or improving synthetic data quality.</p>
<p>Causal Language Modeling</p>
<p>In contrast to above methods that fine-tune an LLM on the corresponding table samples, Curated LLM (CLLM) (Seedat et al., 2023) utilizes the rich world knowledge from GPT4 to augment and enhance training data in scenarios with limited data, without the need for fine-tuning.CLLM is a framework that leverages learning dynamics and two novel curation metrics, namely confidence and uncertainty.These metrics help to filter out undesirable generated samples during the training process of a classifier, aiming to produce high-quality synthetic data.Specifically, both metrics are calculated for each sample, utilizing the classifier trained on these samples.Additionally, CLLM distinguishes itself by not requiring any fine-tuning of LLMs.</p>
<p>The generation process.After fine-tuning the model or using a standard LLM, there are three primary preconditioning methods (Borisov et al., 2023b) for designing prompts to generate new data samples for CLM-based methods, as depicted in Figure 5: 1) feature name preconditioning: This method involves providing only a feature's name, generating samples across the entire joint data distribution.2) One namevalue pair preconditioning: Here, when a single feature name along with its value is supplied, the LLM will generate a complete sample.This method produces samples from the conditional distribution.Sampling one data point from a single feature distribution is generally feasible and then use name-value pair Preconditioning to generate the rest of the features.3) Multiple Name-Value Pair Preconditioning: This involves providing multiple name-value pairs for arbitrary conditioning.The model then efficiently samples from the distribution of the remaining features.After that, we use cell value extraction methods, such as standard patternmatching algorithms and regular expressions, to transform the generated pre-defined serialized text data into a tabular format.</p>
<p>Figure 5: The data generation process for causual LMs</p>
<p>Masked Language Modeling</p>
<p>The MLM structure is suitable for generating tabular data due to its ability to capture bidirectional patterns between columns.Besides, prompting a tabular generator doesn't follow a sequential format.MLM's masking procedure enables arbitrary prompts during generation, making the generation process more efficient.Moreover, MLM can easily address the common challenge of missing data in tabular datasets by learning from missing values through a masking probability setting of 1, streamlining the generation process without requiring separate data imputation steps.</p>
<p>TabMT (Gulati &amp; Roysdon, 2023) employs a masked transformer-based architecture.The design allows efficient handling of various data types and supports missing data imputation.It leverages a masking mechanism to enhance privacy and data utility, ensuring a balance between data realism and privacy preservation.</p>
<p>TabMT's architecture is scalable, making it suitable for diverse datasets and demonstrating improved performance in synthetic data generation tasks.</p>
<p>The generation process.To minimize bias from a fixed order of column names, TabMT randomly selects column names without replacement during the generation process and subsequently samples the column values based on the predicted column distribution.TabMT initially sets the masking probability for all column values to 1 and then predicts each value gradually, as illustrated in Figure 6.</p>
<p>Figure 6: The data generation process for masked LMs</p>
<p>Evaluation</p>
<p>As outlined in Zhang et al. (2023c), the evaluation of synthetic data quality can be approached from four different dimensions: 1) Low-order statistics -column-wise density and pair-wise column correlation, estimating individual column density and the relational dynamics between pairs of columns, 2) High-order metrics -the calculation of α-precision and β-recall scores that measure the overall fidelity and diversity of synthetic data, 3) privacy preservation -DCR score, representing the median Distance to the Closest Record (DCR), to evaluate the privacy level of the original data, (Note: the similarity-based DCR score provides an average metric for the system but does not offer information about individual privacy guarantees (Ganev &amp; De Cristofaro, 2023)) and 4) Performance on downstream tasks -like machine learning efficiency (MLE) and missing value imputation.MLE is to compare the testing accuracy on real data when trained on synthetic ones.Additionally, the quality of data generation can be assessed through its performance on missing value imputation, that is, guessing the missing value(s) of a tuple, when the rest of the attributes are given.</p>
<p>LLMs for table understanding</p>
<p>In this section, we cover datasets, trends and methods explored by researchers for question answering (QA), fact verification (FV) and table reasoning tasks.There are many papers working on database manipulation, management and integration (Lobo et al., 2023;Fernandez et al., 2023;Narayan et al., 2022;Zhang et al., 2023b), which also include instructions and tabular inputs to LLMs.However, they are not typically referred to as a QA task, and they will not be covered by this paper.et al., 2023b), 90.71 (Sarkar &amp; Lausen, 2023), 87.60 (Jiang et al., 2023), 82.55 (Zhang et al., 2023g), 78.80 (Chen, 2023), 62.67 (Sui et al., 2023c) Spider (Yu et al., 2018b) 1020</p>
<p>Dataset</p>
<p>Text2-SQL</p>
<p>Table, Question</p>
<p>SQL Human annotation</p>
<p>Execution Accuracy: 87.60 (Li et al., 2024), 86.60 (Gao et al., 2024), 85.30 (Pourreza &amp; Rafiei, 2023), 82.30 (Dong et al., 2023), 79.90 (Li et al., 2023a), 78.00 (Rai et al., 2023), 77.80 (Jiang et al., 2023), 77.60 (Li et al., 2023b) (Nan et al., 2022), WikiTableQuestion (Pasupat &amp; Liang, 2015a), HybridQA (Chen et al., 2020b) and SQA (Iyyer et al., 2017) are popular options.Unlike WikiTableQuestions, which focuses on evaluating a QA system's ability to understand queries and retrieve short-form answers from tabular data, FeTaQA introduces elements that require deeper reasoning and integration of information.This includes generating free-form text answers that involve the retrieval, inference, and integration of multiple discontinuous facts from structured knowledge sources like tables.This requires the model to generate long, informative, and free-form answers.NQ-TABLES Herzig et al. (2021) is larger than the previously mentioned table.Its advantage lies in its emphasis on open-domain questions, which can be answered using structured table data.14 .Text2SQL Spider (Yu et al., 2018b), Magellan (Das et al., 2015) or WikiSQL (Zhong et al., 2017b), and BIRD (Li et al., 2023c) are suitable for training and evaluating models that generate SQL commands.Both Spider and WikiSQL have been benchmarked by many existing methods, some shown in Table 7.Compared to Spider, WikiSQL is much larger in size. 17.The BIRD (BIg Bench for LaRge-scale Database Grounded Text-to-SQL Evaluation) benchmark contains large tables and complex questions, and it been widely used by the community.et al., 2022) allows for more standardized and comparable evaluation across different NLG models and tasks, potentially leading to more reliable and consistent benchmarking in the field.The dataset is in footnote. 18.</p>
<p>Table NLI</p>
<p>InfoTabs (Gupta et al., 2020) uses Wikipedia info-boxes and is designed to facilitate understanding of semi-structured tabulated text, which involves comprehending both text fragments and their implicit relationships.InfoTabs is particularly useful for studying complex, multi-faceted reasoning over semi-structured, multi-domain, and heterogeneous data.Meanwhile, TabFact (Chen et al., 2020a) consists of human-annotated natural language statements about Wikipedia tables.It requires linguistic reasoning and symbolic reasoning to get right answer.The dataset is in footnote. 19.</p>
<p>Domain-Specific Some datasets and task focus on domain-specific applications.AIT-QA (Katsis et al., 2022) worked on airline industry specific table question answer.It highlights the unique challenges posed by domain-specific tables, such as complex layouts, hierarchical headers, and specialized terminology.For finance related table question answer, TAT-QA Zhu et al. (2021a) assesses numerical reasoning, involving operations like addition, subtraction, and comparison.SciGen (Moosavi et al., 2021) focuses on assessing the arithmetic reasoning capabilities of generation models on complex input structures, such as tables from scientific articles.TranX (Yin &amp; Neubig, 2018) investigates abstract syntax description language for the target representations, enabling high accuracy and generalizability across different types of meaning representations. 20.</p>
<p>Pretraining For pretraining on large datasets for table understanding, we recommend to use TaBERT (Yin et al., 2020b) and TAPAS (Herzig et al., 2020).Dataset in Tapas has 6.2 million tables and is useful for semantic parsing.TAPAS has 26 million tables and their associated english contexts.It can help model gain better understanding in both textual and table.The dataset is in footnote. 21.</p>
<p>Paper Task Models Explored DOCMATH-EVAL (Zhao et al., 2023d) NumQA GPT4, GPT3.5, WizardLM, Llama-2 7, 13, 70B, CodeLlama 34B, Baichuan, Qwen, WizardMath, Vicuna, Mistral, etc. Akhtar et al. (2023) NumQA TAPAS, DeBERTa, TAPEX, NT5, LUNA, PASTA, ReasTAP, FlanT5, GPT3.5, PaLM TableGPT (Gong et al., 2020) NumQA GPT2 DATER (Ye et al., 2023b) QA GPT3 Codex Chen (2023) QA GPT3 cTBLS (Sundar &amp; Heck, 2023) QA Custom: Dense  (Zha et al., 2023) QA Custom: Phoenix-7B TAP4LLM (Sui et al., 2023c) QA Instruct GPT3.5, GPT4 UniTabPT (Sarkar &amp; Lausen, 2023) QA Custom: T5, Flan-T5 Yu et al. (2023) Multi-modal QA Custom: Retrieval trained on contrastive loss, Rank by softmax, Generation built on T5 TableLlama (Zhang et al., 2023g) QA Custom: TableLlama DIVKNOWQA We only include papers that work with models of &gt;1B parameters.Models that are described as "Custom" indicates papers that finetuned specific portions of their pipeline for the task, whereas the other papers focus more on non-finetuning methods like prompt engineering.NumQA: Numerical QA.</p>
<p>General ability of LLMs in QA</p>
<p>Table 8 outlines papers that investigated the effectiveness of LLMs on QA and reasoning, and the models explored.The most popular LLM used today is GPT3.5 and GPT4.Although these GPT models were not specifically optimized for table-based tasks, many of these papers found them to be competent in performing complex table reasoning tasks, especially when combined with prompt engineering tricks like CoT.In this section, we summarize the general findings of LLMs in QA tasks and highlight models that have reported to work well.</p>
<p>Numerical QA A niche QA task involves answering questions that require mathematical reasoning.An example query could be "What is the average payment volume per transaction for American Express?"Many real-world QA applications (E.g.working with financial documents, annual reports, etc.) involve such mathematical reasoning tasks.So far, Akhtar et al. (2023) conclude that LLMs like FlanT5 and GPT3.5 perform better than other models on various numerical reasoning tasks.On the DOCMATH-EVAL (Zhao et al., 2023d)  Impact of model size on performance Chen (2023) found that size does matter: On WebTableQuestions, when comparing the 6.7B vs. 175B GPT-3 model, the smaller model achieved only half the scores of the larger one.On TabFact, they found that smaller models (&lt;=6.7B)obtained almost random accuracy.</p>
<p>Finetuning or</p>
<p>No finetuning?There are some larger models that fine-tune on various tabular tasks, some including QA and FV tasks, mentioned in Section 2.1 under embeddings-based serialization.Li et al. (2023d) found that fine-tuning always helps to improve performance across various tabular tasks.In zeroshot settings, the improvement is the most significant.For Ye et al. (2023b), they obtained higher scores on TabFact when using their framework with the PASTA (Gu et al., 2022) model (score 93.00%) as compared to the GPT-3 Codex (code-davinci-002) (scored 85.60%).PASTA was pre-trained on a synthesized corpus of 1.2 million items from WikiTables for six types of sentence-table cloze tasks.This suggests there remains some benefit in using LMs fine-tuned on tabular tasks.</p>
<p>However, compared to methodologies working on Prediction and Generation tasks, fine-tuning is not as common.This might be due to the general ability of LLMs (E.g.GPT3.5, GPT4) to perform QA tasks off-the-shelf.For SQL generation on Spider, DIN-SQL (Pourreza &amp; Rafiei, 2023) and DAIL-SQL (Gao et al., 2024) are inference-based techniques using GPT4, and surpassed previous fine-tuned smaller models.Interestingly, in the paper by Gao et al. (2024), the authors fine-tuned a Llama 2 13B model on the Text2SQL tasks.However, this model did not beat the GPT4 model that was not fine-tuned.Instead, many papers working on using LLMs for table understanding tasks focus on tweaking aspects across serialization, prompt engineering, search and retrieval, and end-to-end pipelines (user interfaces), which we describe further in the next section.</p>
<p>Key components in Tabular QA</p>
<p>In the simplest QA architecture, an LLM takes in an input prompt (query and serialized table) 23 , and returns an answer.In more involved architectures, the system might be connected to external databases or programs.Most of the times, the knowledge base might not fit in the context length or memory of the LLM.Therefore, unique challenges to tabular QA for LLMs include: query intent disambiguation, search and retrieval, output types and format, and multi-turn settings where iterative calls between programs are needed.We describe these components further in this section.Zha et al. (2023) introduced the concept of Chain-of-command (CoC), that translates user inputs into a sequence of intermediate command operations.For example, an LLM needs to first check if the task requires retrieval, mathematical reasoning, table manipulations, and/or the questions cannot be answered if the instructions are too vague.They constructed a dataset of command chain instructions to fine-tune LLMs to generate these commands.Deng et al. (2022b) proposed the QA task be split into three subtasks: Clarification Need Prediction (CNP) to determine whether to ask a question for clarifying the uncertainty; Clarification Question Generation (CQG) to generate a clarification question as the response, if CNP detects the need for clarification; and Conversational Question Answering (CQA) to directly produce the answer as the response if it is not required for clarification.They trained a UniPCQA model which unifies all subtasks in QA through multi-task learning.</p>
<p>Query intent disambiguation</p>
<p>Search and retrieval</p>
<p>The ability to accurately search and retrieve information from specific positions within structured data is crucial for LLMs.There are two types of search and retrieval use-cases: (1) to find the information (table, column, row, cell) relevant to the question, and (2) to obtain additional information and examples.</p>
<p>For main table Zhao et al. (2023d) observed that better performance of a retriever module (that returns the top-n most relevant documents) consistently enhances the final accuracy of LLMs in numerical QA.Sui et al. (2023c) explored multiple table sampling methods (of rows and columns) and table packing (based on a token-limit parameter).The best technique was the query-based sampling, which retrieves rows with the highest semantic similarity to the question, surpassing methods involving no sampling, or clustering, random, even sampling, or content snapshots.Dong et al. (2023) used ChatGPT to rank tables based on their relevance to the question using SC: they generate ten sets of retrieval results, each set containing the top four tables, then selecting the set that appears most frequently among the ten sets.To further filter the columns, all columns are ranked by relevance to the question by specifying that ChatGPT match the column names against with the question words or the foreign key should be placed ahead to assist in more accurate recall results.Similarly, SC method is used.cTBLS Sundar &amp; Heck (2023) designed a threestep architecture to retrieve and generate dialogue responses grounded on retrieved tabular information.</p>
<p>In the first step, a dual-encoder-based Dense Table Retrieval (DTR) model, initialized from RoBERTa Liu et al. (2019), identifies the most relevant table for the query.In the second step, a Coarse System State Tracking system, trained using triplet loss, is used to rank cells.Finally, GPT-3.5 is prompted to generate a natural language response to a follow-up query conditioned on cells of the table ranked by their relevance to the query as obtained from the coarse state tracker.The prompt includes the dialogue history, ranked knowledge sources, and the query to be answered.Their method produced more coherent responses than previous methods, suggesting that improvements in table retrieval, knowledge retrieval, and response generation lead to better downstream performance.Zhao et al. (2023d) used OpenAI's Ada Embedding4 and Contriever (Izacard et al., 2022) as the dense retriever along with BM25 (Robertson et al., 1995) as the sparse retriever.These retrievers help to extract the top-n most related textual and tabular evidence from the source document, which were then provided as the input context to answer the question.</p>
<p>For additional information Some papers explore techniques to curate samples for in-context learning.</p>
<p>Multi-turn tasks</p>
<p>Some papers design pipelines that call LLMs iteratively.We categorize the use-cases for doing so into three buckets: (1) to decompose a challenging task into manageable sub-tasks, (2) to update the model outputs based on new user inputs, and (3) to work-around specific constraints or to resolve errors.</p>
<p>Dialogue-based applications</p>
<p>In various applications where the users are interacting with the LLMs, like in chatbots, the pipeline must allow for LLMs to be called iteratively.Some dialogue-based Text2SQL datasets to consider are the SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a) datasets.For SParC, the authors designed subsequent follow-up questions based on Spider (Yu et al., 2018b).</p>
<p>Working around constraints or error de-bugging Zhao et al. (2023a) used multi-turn prompts to work around cases where the tables exceed the API input limit.In other cases, especially if the generated LLM output is code, an iterative process of feeding errors back to the LLM can help the LLM generate correct code.Zhang et al. (2023d) did so to improve SQL query generation.</p>
<p>Output evaluation and format</p>
<p>If</p>
<p>Limitations and future directions</p>
<p>LLMs have been used in many tabular data applications, such as predictions, data synthesis, question answering and table understanding.Here we outline some practical limitations and considerations for future research.</p>
<p>Numerical representation</p>
<p>It was revealed that LLM in house embedding is not suitable for representing intrinsic relations in numerical features (Gruver et al., 2023), and thus a careful embedding is needed.Tokenization significantly impacts pattern formation and operations in language models.Traditional methods like Byte Pair Encoding (BPE) used in GPT-3 often split numbers into non-aligned tokens (e.g., 42235630 into [422,35,630]), complicating arithmetic.Newer models like LLaMA tokenize each digit separately.Both approaches make LLM difficult to understand the whole number.Also, based on Spathis &amp; Kawsar (2023), the tokenization of integers lacks a coherent decimal representation, leading to a fragmented approach where even basic mathematical operations require memorization rather than algorithmic processing.The development of new tokenizers, like those used in LLaMA (Touvron et al., 2023b), which outperformed GPT-4 in arithmetic tasks, involves rethinking tokenizer design to handle mixed textual and numerical data more effectively, such as by splitting each digit into individual tokens for consistent number tokenization (Gruver et al., 2023).This method has shown promise in improving the understanding of symbolic and numerical data.However, it hugely increases the dimension of the input, which makes the method not practical for large datasets and many features.For future direction, it is worth to explore new tokenizer that can better represent numerical token while not increase the dimension of the input.</p>
<p>Categorical representation Tabular dataset very often contains an excessive number of categorical columns, which can lead to serialized input strings surpassing the context limit of the language model and increased cost.This is problematic as it results in parts of the data being pruned, thereby negatively impacting the model's performance.Additionally, there are issues with poorly represented categorical feature, such as nonsensical characters, which the model struggles to process and understand effectively.Another concern is inadequate or ambiguous metadata, characterized by unclear or meaningless column names and metadata, leading to confusion in the model's interpretation of inputs.Better categorical features encoding is needed to solve these problems.Traditional machine learning methods such as lightGBM require expanding dimension for categorical features (Borisov et al., 2022b) and can lead to bias categorical representation (Prokhorenkova et al., 2019).Thus, good categorical features encoding could add competitive advantage for LLM based method compared to traditional machine learning methods.</p>
<p>Standard benchmark LLMs for tabular data could greatly benefit from standardized benchmark datasets to enable fair and transparent comparisons between models.In this survey, we strive to summarize commonly used datasets/metrics and provide recommendations for dataset selection to researchers and practitioners.However, for the same dataset, the same method, and the same task, different papers report different performance.For prediction task, the performance of TabLLM (Hegselmann et al., 2023) in Blood dataset (Kadra et al., 2021) is 0.70 in GTL (Zhang et al., 2023a) (see table 2 in that paper) and 0.66 in UniPredict (Wang et al., 2023a) (see table 4 in that paper).This discrepancy in benchmark performance makes it impossible to come up with a classification performance benchmark against all methods.Therefore, there is a pressing need for more standardized and unified benchmark exercise to bridge this gap effectively.</p>
<p>Tabular-specific challenges</p>
<p>The current exploration of LLMs on tabular data remains primarily surfacelevel, lacking in-depth analysis tailored to the unique characteristics of tabular datasets.For example, there is a paucity of understanding regarding how LLMs handle class imbalanced datasets.Given that LLMs come with prior knowledge, it is reasonable to hypothesize about the synergistic or antagonistic effects between training and inference data, potentially leading to unforeseen behaviors in such scenarios (Jung &amp; van der Plas, 2024).Another unexplored aspects relates to the order invariant nature of tabular data.while language models are inherently order-variant, with word order significantly impacting predictions and contextual understanding, little is unknown how LLM performance varies when dealing with tabular data where orders of the features and records are invariant.Future research should prioritize an in-depth investigation into tabular-specific behaviors of LLMs to enhance their performance on tasks related to tabular data.</p>
<p>Bias and fairness</p>
<p>In existed tabular prediction and table understanding methods, LLMs tend to inherit social biases from their training data, which significantly impact their fairness metric.For example, Liu et al.</p>
<p>(2023f) uses GPT3.5 and do few-shot learning to evaluate the fairness of tabular prediction on in context learning.For LLMs based tabular prediction method, the research concludes that the fairness metric gap between different subgroups is larger than that in traditional machine learning model.Additionally, the research further reveals that flipping the labels of the in-context examples significantly narrows the gap in fairness metrics across different subgroups, but comes at the expected cost of a reduction in predictive performance.Other research shows that the inherent bias of LLM is hard to mitigate through prompt (Hegselmann et al., 2023).Thus, it is worth to explore through other bias mitigation methods such as pre-processing (Shah et al., 2020) or optimization (Bassi et al., 2024).</p>
<p>Hallucination LLMs sometimes produce content that is inconsistent with the real-world facts or the user inputs (Huang et al., 2023a), which raises concerns over the reliability and usefulness of LLMs in the realworld applications.For tabular prediction, especially when working with patient records and medical data, hallucinations have critical consequences.Akhtar et al. (2023) found that hallucination led to performance drops in reasoning for LLMs.To address these issues in tabular prediction, Wang et al. (2023c) incorporated an audit module that utilizes LLMs to perform self-check and self-correction.They generated pseudo-labels, then used a data audit module which filters the data based on data Shapley scores, leading to a smaller but cleaner dataset.Secondly, they removed any cells with False values, which removes the chances of the LLMs making a false inference on these invalid values.Finally, they performed a sanity check via LLM's reflection: querying the LLM with the input template "What is the {column}?{x}" to check if the answer matches the original values.If the answers do not match, the descriptions are corrected by re-prompting the LLM.However, this method requires iterative efforts and is hard to deploy in real world application.An interesting future direction would be to explore efficient and practical way to deal with hallucination in LLM based method for tabular data.</p>
<p>Model interpretability Like many deep learning algorithms, LLMs suffer from a lack of interpretability.</p>
<p>For LLM based method in tabular data, only a few systems expose a justification of their model output, such as TabLLM (Hegselmann et al., 2023).One direction is to use the Shapley values to derive interpretations.Shapley values have been used to evaluate the prompt for LLMs (Liu et al., 2023a).It could also be useful to understand how each feature influence the result.For instance, in prediction for diseases, providing explanation is crucial.In this case, an explanation based on Shapley values would list the most important features that led to the final decision.However, the performance of Shapley or other explanation methods on tabular prediction and table understanding remains unexplored.Future research is needed to explore the existed explanation mechanisms for LLM based tabular prediction and table understanding and develop more suited explanation methods.</p>
<p>Ease of use Existed LLM based tools such as ChatGPT and models on huggingface are easy to inference.Currently, most relevant tabular based LLM models require fine-tuning or data serialization, which could make these models hard to access.Pretrained models, such as Wang et al. (2023c;a) which integrate data consolidation, enrichment, and refinement, have the potential to streamline user experience.These methods still require extensive preprocessing, which makes it hard to inference.The development of a unified pipeline that incorporates these models, along with auto data prepossessing and serialization to established platforms such as Hugging Face, warrants further exploration.</p>
<p>Fine-tuning strategy design Designing appropriate tasks and learning strategies for LLMs is extensively explored.LLMs demonstrate emergent abilities such as in-context learning, instruction following, and stepby-step reasoning.However, these capabilities may not be fully evident in certain tabular data prediction and table understanding tasks, depending on the model used.LLMs are sensitive to various serialization and prompt engineering methods (Hegselmann et al., 2023), which is the primary way to adapt LLM to unseen tasks.As a future direction, researchers and practitioners need to carefully design tasks and learning strategies tailored to tabular data in order to achieve an optimal performance.</p>
<p>Model grafting</p>
<p>The performance of LLM for tabular data modeling could be improved through model grafting.Model grafting involves mapping non-text data into the same token embedding space as text using specialized encoders, as exemplified by the HeLM model (Belyaeva et al., 2023), which integrates spirogram sequences and demographic data with text tokens.This approach is efficient and allows integration with highperforming models from various domains but adds complexity due to its non-end-to-end training nature and results in communication between components that is not human-readable.This approach could be adapted to LLM for tabular data to improve the encoding of non-text data such as categorical and numerical feature.</p>
<p>Conclusion</p>
<p>This survey represents the first comprehensive investigation into the utilization of LLMs for modeling heterogeneous tabular data across various tasks, including prediction, data synthesis, question answering and table understanding.We delve into the essential steps required for tabular data to be ingested by LLM, covering serialization, table manipulation, and prompt engineering.Additionally, we systematically compare datasets, methodologies, metrics and models for each task, emphasizing the principal challenges and recent advancements in understanding, inferring, and generating tabular data.We provide recommendations for dataset and model selection tailored to specific tasks, aimed at aiding both ML researchers and practitioners in selecting appropriate solutions for tabular data modeling using different LLMs.Moreover, we examine the limitations of current approaches, such as susceptibility to hallucination, fairness concerns, data preprocessing intricacies, and result interpretability challenges.In light of these limitations, we discuss future directions that warrant further exploration in future research endeavors.</p>
<p>With the rapid development of LLMs and their impressive emergent capabilities, there is a growing demand for new ideas and research to explore their potential in modeling structured data for a variety of tasks.Through this comprehensive review, we hope it can provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.</p>
<p>Figure 3 :
3
Figure 3: Development of language models and their applications in tabular data modeling.</p>
<p>Table 1
1
(Herzig et al., 2020)oy table encoders, which were fine-tuned from PLMs, to encode tabular data into numerical representations as the input for LLMs.There are multiple table encoders, built on BERT(Devlin et al., 2019)for table-related task, like TAPAS(Herzig et al., 2020), TABERT(Yin et al.,
describes the common text-based serialization methods in the literature. A straight-forward way would be to directly input a programming-language readable data structure (E.g. PandasDataFrame Loader for Python, line-separated JSON-file format, Data Matrix represented by a list of lists,HTML code reflecting tables, etc). Alternatively, the table could be converted into X-separated values, whereX could be any reasonable delimiter like comma or tab. Some papers convert the tables into human-readablesentences using templates based on the column headers and cell values. The most common approach basedon our survey is the Markdown format.Embedding-based</p>
<p>Table 1 :
1
Text-based serialization methods.</p>
<p>Table size" had minimal improvements, while "header hierarchy" added unnecessary complexity, and hurt performance.For the Text2SQL task, Chang &amp; Fosler-Lussier (2023) also find that some table relationships and database content are useful.Huang et al. (2023b) report improvements in GPT-4's accuracy by 28.9% when incorporating documentation that disambiguate terms present in the table like data column names, value consistency, data coverage, and granularity.Zhao et al. (2023e) investigated the effects of SOTA Table QA models on manipulations on the table header, table content and natural language question (phrasing). 4They find that all examined Table QA models (TaPas, TableFormer, TaPEX, OmniTab, GPT3) are not robust under adversarial attacks.</p>
<p>Liu et al. (2023e)performance to table manipulationsLiu et al. (2023e)critically analyzed the robustness of GPT3.5 across structural perturbations in tables (transpose and shuffle).They find that LLMs suffer from structural bias in the interpretation of table orientations, and when tasked to transpose the table, LLMs performs poorly ( 50% accuracy).However, LLMs can identify if the first row or first column is the header (94-97% accuracy).</p>
<p>table sampling mentioned earlier under Section 2.2.Apart from the aforementioned methods, Sundar &amp; Heck (2023) designed a dual-encoder-based Dense Table Retrieval (DTR) model to rank cells of the table according to their relevance to the query.The ranked knowledge sources are incorporated within the prompt, and led to top ROUGE scores.Role-play Another popular prompt engineering technique is role-play, which refers to including descriptions in the prompt about the person the LLM should portray as it completes a task.For example, Zhao et al. (2023a) experimented with the prompt "Suppose you are an expert in statistical analysis.".</p>
<p>Zhang et al. (2023h)msSince LLMs can generate any text-based output, apart from generating human-readable responses, it could also generate code readable by other programs.Abraham et al. (2022) designed a model that converts natural language queries to structured queries, which can be run against a database or a spreadsheet.Liu et al. (2023e)designed a system where the LLM could interact with Python to execute commands, process data, and scrutinize results (within a Pandas DataFrame), iteratively over a maximum of five iterations.Zhang et al. (2023d)demonstrated that we can obtain errors from the SQL tool to be fed back to the LLMs.By implementing this iterative process of calling LLMs, they improved the success rate of the SQL query generation.Finally,Liu et al. (2023c)proposes a no-code data analytics platform that uses LLMs to generate data summaries, including generating pertinent questions required for analysis, and queries into the data parser.A survey byZhang et al. (2023h)covers further concepts about natural language interfaces for tabular data querying and visualization, diving deeper into recent advancements in Text-to-SQL and Text-to-Vis domains.</p>
<p>Table 2
2DatasetDataset Number Papers that used this datasetOpenML11Dinh et al. (2022); Manikandan et al. (2023)Kaggle API169Hegselmann et al. (2023); Wang et al. (2023a); Zhang et al. (2023a)Combo9Hegselmann et al. (2023); Wang et al. (2023a); Zhang et al. (2023a)UCI ML20Manikandan et al. (2023); Slack &amp; Singh (2023)DDX10Slack &amp; Singh (2023)
. OpenML has the highest number of datasets, but the size of the largest dataset is only 5600 rows.Half of the datasets in UCI ML collections are relevant to medical use cases.Thus, the combo of 9 datasets is recommended for benchmark 5 since it contains larger size datasets and more diverse feature sets.For general fine-tuning, published methods choose the Kaggle API 6 as it has 169 datasets, and its datasets are very diverse.</p>
<p>Table 2 :
2
Combo is the combination of the following dataset in the form of dataset name (number of rows,
number of features): Bank (45,211 rows, 16 feats), Blood (748, 4), California (20,640, 8), Car (1,728, 8),Creditg (1,000, 20), Income (48,842, 14), and Jungle (44,819, 6), Diabetes (768, 8) and Heart (918, 11).AlgorithmTypeMethodResource MetricUsed ModelTabletSlack &amp; Singh (2023)Tabular No FinetuneLowF1GPTJ/Tk-Instruct/Flan T5SummaryBoostManikandan et al. (2023) Tabular No FinetuneHighRMSEGPT3LIFTDinh et al. (2022)Tabular FinetuneHighMAE/RMSEGPT3/GPTJTabLLMHegselmann et al. (2023)Tabular FinetuneHighAUCGPT3/T0UnipredictWang et al. (2023a)Tabular FinetuneLowACCGPT2GTLZhang et al. (2023a)Tabular FinetuneLowACCLLaMASerializeLLMJaitly et al. (2023)Tabular FinetuneHighAUCT0MediTabWang et al. (2023c)Medical FinetuneHighPRAUC/AUCROC BioBert/GPT3.5/UnifiedQA-v2-T5CTRLLi et al. (2023e)Finance FinetuneHighAUC/LogLossRoberta/ChatGLMFinPTYin et al. (2023)CTRFinetuneHighF1 ScoreFlanT5/ChatGPT/GPT4</p>
<p>Table 3 :
3
Prediction methods.Resource is high if it has to finetune a model with size ≥ 1B even if it is PEFT.Used Model include all models used in the paper which includes serialization, preprocessing and model finetuning.ACC stands for accuracy.AUC stands for Area under the ROC Curve.MAE stands for mean absolute error.RMSE stands for root-mean-square error.</p>
<p>included naturally occurring instructions along with examples for serialization.In this case, where the task is for medical diagnosis, naturally occurring instructions are from consumer-friendly sources, such as government health websites or technical references</p>
<p>such as the Merck Manual.It includes instructions, examples, and test data points.They found that these instructions significantly enhance zero-shot F1 performance.However, experiments from TABLET revealed that LLMs tend to ignore instructions, even with examples, leading to prediction failures.Along Published in Transactions on Machine Learning Research (07/2024)</p>
<p>Table 4 :
4
Method and Example for different preprocessing for general predictive tasks.The example is to predict if a car repair claim is fraudulent or not.
MethodologyMethodExampleFeature name + Feature Value +Dinh et al. (2022); Hegsel-Car Brand is Land Rover. Year is 2017. Re-Predicted Feature Namemann et al. (2023)pair claim isTask Background + FeatureZhang et al. (2023a)The task is about fraud repair claim predic-meaning + Feature Value + Pre-tion. The brand of car is Land Rover. Thedicted Feature meaningproduce year is 2017. The repair claim of thecar isDataset Summary + LLM Pro-Manikandan et al. (2023) Larger car is always more expensive. This iscessed Feature + Taska 2017 Land Rover. Therefore, this car repairclaim is (Fraudulent or Not Fraudulent):Latex Format of features value +Jaitly et al. (2023)Is this car repair claim fraudulent? Yes or No?TaskExpert Task Understanding +Slack &amp; Singh (2023)Identify if the car repair claim is fraudulent.Label + TaskAn older car is more likely to have a fraudu-lent repair claim. Features Car Brand: LandRover Year: 2017. Answer with one of the fol-lowing: Yes | NoDataset description + FeatureWang et al. (2023a)The dataset is about fraud repair claims. Carmeaning + Feature Value + TaskBrand is the brand of car. The year is the agewhen the car is produced. The features are:Car Brand is Land Rover. The year is 2017.Predict if this car repair claim is fraudulent byYes for fraudulent, No for not fraudulent
Thus, serialization and target augmentation are more relevant to how to best represent numerical features.Many papers have claimed</p>
<p>(Zeng et al., 2023), and023), and LLaMa Touvron et al. (2023a), using metrics like classification accuracy and RMSE.However, the result shows that this method is not as efficient and accurate as training a small task-oriented model.In general, currently, LLaMa is used as the base model by most papers we surveyed.
MethodUsed PaperExampleAdding Special Token be-Dinh et al. (2022)### {Category} @@@fore and after the answerVerbalizerHegselmann et al. (2023)Output -&gt; {category1: probability1, .}Specific PrefixManikandan et al. (2023);Please answer with category 1, category 2, ...Slack &amp; Singh (2023)Predict probability andWang et al. (2023a){category1: probability1} =&gt; Calibratedrecalibrateby XGBoostTable 5: Target Augmentation methods, papers that used them, and examples3.4 Applications of Prediction using LLMMedical Prediction Medical data such as electronic health records (EHR) is a rich and complex source ofinformation about patients' medical histories, treatments, and outcomes. It has more inherent complexitythan simple tabular data. It captures information about patients' health over time, contains unstructureddata such as clinical notes, has high interconnection between variables, contains missing data and noisysignals. The LM based model could capture the long-term dependencies among events such as diabetes anddeal with unstructured data such as clinical notes. Thus, LM based models (McMaster et al., 2023; Steinberget al.
Rasmy et al., 2021;Li et al., 2020a).Another popular metric is the Continuous Ranked Probability Score (CRPS) as it captures distributional qualities, allowing for comparison of models that generate samples without likelihoods.CRPS is considered an improvement over MAE as it does not ignore the structures in data like correlations between time steps.The Symmetric Mean Absolute Percentage Error (SMAPE) measures the accuracy based on percentage errors, the Mean Absolute Scaled Error (MASE) is a scaleindependent error metric normalized by the in-sample mean absolute error of a naive benchmark model, and the Overall Weighted Average (OWA) is a combined metric that averages the ranks of SMAPE and MASE to compare the performance of different methods.Among those metrics, MAE and RMSE are used by at least half of our surveyed methods in time series., 2021;Rasmy et al., 2021;Li et al., 2020a)perform better than XGBoost.However, these models only focused on predicting a small fraction of the International Statistical Classification of Diseases and Related Health Problems (ICD) codes.Currently, Meditab</p>
<p>Table 6 :
6
LLM-powered data synthesis methods."DCR" stands for Distance to the Closest Record and "MLE" stands for Machine Learning Efficiency.
/DistilGPT2Fine-tunedSentencesDCR, MLEREaLTabFormer (Solatorio &amp; Dupriez, 2023) GPT2Fine-tunedSentencesDCR, MLETAPTAP (Zhang et al., 2023e)GPT2/DistilGPT2Fine-tunedSentencesDCR, MLETabuLa (Zhao et al., 2023f)DistilGPT2Fine-tunedX-SeparatedMLECLLM (Seedat et al., 2023)GPT4Non Fine-tunedX-SeparatedMLETabMT (Gulati &amp; Roysdon, 2023)Masked Transformers -24layerFine-tuned"[Value]"MLE</p>
<p>Zhang et al. (2023e), 2023)ses the first CLM-based table generative method, GReaT 11 (Generation of Realistic Tabular data) to generate synthetic samples with original tabular data characteristics.The GReaT data pipeline involves a textual encoding step transforming tabular data into meaningful text using the sentences serialization methods as shown in Table1, followed by fine-tuning a GPT-2 or GPT-2 distill model.Additionally, a feature order permutation step precedes the use of obtained sentences for LLM fine-tuning.REaLTabFormer(Solatorio &amp; Dupriez, 2023)extends GReaT by generating synthetic nonrelational and relational tabular data.It uses GReaT (an autoregressive GPT-2 model) to generate a parent table and a sequence-to-sequence model conditioned on the parent table for the relational dataset.The model implements target masking to prevent data copying and introduces statistical methods to detect overfitting.It demonstrates superior performance in capturing relational structures and achieves state-ofthe-art results in predictive tasks without needing fine-tuning.Folowing the similar paradigm,Zhang et al. (2023e)proposed the TAPTAP 12 (Table Pretraining for Tabular Prediction) which incorporates several enhancements.The method involves continue pretraining the GPT2 on 450 Kaggle/UCI/OpenML tables, generating label columns using a machine learning model.Other improvements improvements include a revised numerical encoding scheme and the use of external models like gradient-boosted decision trees for pseudo-label generation. Thir experimental findings demonstrate that by incorporating the additional table pre-training phase and employing machine learning models to generate labels, TAPTAP can generate superior quality training samples compared with GReaT.TabuLa</p>
<p>Table7outlines some of the popular datasets and benchmark in the literature working on tabular QA tasks.Other relevant but less commonly cited datasets are mentioned below.
Dataset#Tables TaskInputOutputDataEvaluation Metric &amp; Best ScoresTypeSourceReportedFetaQA (Nan10330QATableAnswerWikipediaBLEU: 39.05 (Zhang et al., 2023g),et al., 2022)Question35.12 (Sarkar &amp; Lausen, 2023), 30.92(Ye et al., 2023b), 27.02 (Chen, 2023),WikiTableQuestion 2108QATableAnswerWikipediaExecution Accuracy: 73.65 (Liu et al.,(Pasupat&amp;Question2023e), 67.31 (Wang et al., 2024), 65.90Liang, 2015a)(Ye et al., 2023b), 65.90 (Jiang et al.,2023), 62.45 (Sarkar &amp; Lausen, 2023),48.80 (Chen, 2023), 35.01 (Zhang et al.,2023g)HybridQA13000QATableAnswerWikipediaExact Match Accuracy: 39.38 (Zhang(Chen et al.,Questionet al., 2023g), 25.14 (Sui et al., 2023c)2020b)SQA(Iyyer982QATableAnswerWikiTable-Exact Match Accuracy: 71.23 (Sarkaret al., 2017)QuestionQuestion&amp; Lausen, 2023), 33.45 (Sui et al.,2023c)HiTAB (Cheng3597QA/Question,AnswerStatisticalExecution Accuracy: 64.71 (Zhanget al., 2022)NLGTableReport andet al., 2023g), 50.00 (Zhao et al., 2023a)WikipediaToTTo (Parikh120000NLGTableSentenceWikipediaBLEU: 53.21 (Sui et al., 2023c), 20.77et al., 2020a)(Zhang et al., 2023g)FEVEROUS28800Classifi-Claim,LabelWikipediaExact Match Accuracy: 77.22 (Chen,Aly et al. (2021)cationTable2023), 73.77 (Zhang et al., 2023g), 66.51(Sui et al., 2023c)TabFact (Chen16573NLITable,LabelWikipediaExact Match Accuracy: 93.00 (Yeet al., 2020a)State-ment</p>
<p>Table 7 :
7
Overview of popular QA/ reasoning datasets and related work for LLMs that worked on tabular data.Only datasets that have been used by more than one relevant method are included in this table.</p>
<p>Table QA
QA
For table QA datasets, FetaQA</p>
<p>Table and Conversation
and
(Eberius et al., 2015))ra et al., 2022)includes conversations grounded on both Wikipedia text and tables.This addresses a significant challenge in current dialogue systems: conversing on topics with information distributed across different modalities, specifically text and tables.15ableClassificationFEVEROUS(Alyet al., 2021)focuses on both unstructured text and structured tables for fact extraction and verification tasks.Dresden Web Tables(Eberius et al., 2015)is useful for tasks requiring the classification of web table layouts, particularly useful in data extraction and web content analysis where table structures are crucial.The dataset is in footnote.16</p>
<p>Table NLG
NLG
(Parikh et al., 2020a)020a)aims to create natural yet faithful descriptions to the source table.It is rich in size and can be used to benchmark table conditional text generation task.HiTAB (Cheng</p>
<p>Table Retrieval
Retrievalbased on RoBERTa+ Coarse State Tracking + Response based onGPT3.5GPT4Table (Sui et al., 2023b)QAGPT-3.5, GPT-4Zhao et al. (2023a)QAGPT-3.5Liu et al. (2023e)QAGPT3.5TableGPT</p>
<p>Table 8 :
8
Overview of Papers and Models for LLMs for tabular QA tasks.
Zhao et al. (2023c)QAGPT3.5, DSP, ReActJiang et al. (2023)QAGPT3.5, ChatGPT3.5Liu et al. (2023c)QA &amp; Text2SQLVicuna, GPT4Gao et al. (2024)Text2SQLGPT4Pourreza &amp; Rafiei (2023)Text2SQLGPT4Huang et al. (2023b)Text2SQLGPT4Dong et al. (2023)Text2SQLChatGPT3.5Chang &amp; Fosler-Lussier (2023)Text2SQLGPT3 Codex, ChatGPT3.5Zhang et al. (2023d)Text2SQLLLaMA2 70bAbraham et al. (2022)Text2SQLCustom: Table Selector + Known &amp; Unknown FieldsExtractor + AggFn Classifier</p>
<p>(Kong et al., 2024))CoT significantly outperforms other LLMs, while open-source LLMs (LLaMa-2, Vicuna, Mistral, Starcoder, MPT, Qwen, AquilaChat2, etc.) lag behind.Text2SQLLiu et al. (2023c)designed a question matcher that identifies three keyword types: 1) column name-related terms, 2) restriction-related phrases (e.g."top ten"), and 3) algorithm or module keywords.Once these keywords are identified, the module begins to merge the specific restrictions associated with each column into a unified combination, which is then matched with an SQL algorithm or module indicated by the third type of keyword.Zhang et al. (2023d)opted for a more straightforward approach of tasking LLaMa-2 to generate an SQL statement based on a question and table schema.Sun et al. (2023b)finetuned PaLM-2 on the Text2SQL task, achieving considerable performance on Spider.OpenTab(Kong et al., 2024)developed an LLM-based framework for the open-domain table QA tasks, incorporating a SQL generation Coder module.The top scoring models for the Spider today are Dong et al. (2023); Gao et al. (2024); Pourreza &amp; Rafiei (2023), all building off OpenAI's GPT models.SQL generation is popular in the industry, with many open-source fine-tuned models available 22 .</p>
<p>Narayan et al. (2022)coded into binary discrete syntax vectors.Narayan et al. (2022)explored manually curated and random example selection.
Gao et al. (2024) explored the a few methods: (1) random: randomly selecting k examples; (2) question
similarity selection: choosing k examples based on semantic similarity with question Q, based on a predefined distance metric (E.g.Euclidean or negative cosine similarity) of the question and example embedding, and kNN algorithm to select k closest examples from Q; (3) masked question similarity selection: similar to (2), but beforehand masking domain-specific information (the table names, column names and values) in the question; (4) query similarity selection: select k examples similar to target SQL query s * , which relies on another model to generate SQL query s ′ based on the target question and database, and so s ′ is an approximation for s * .</p>
<p>Liu et al. (2023e)tasks This section overlaps with concepts around CoT and SC discussed earlier in Section 2.3.In a nutshell, since the reasoning task might be complex, LLMs might require guidance to decompose the task into manageable sub-tasks.For example, to improve downstream tabular reasoning,Sui et al. (2023b)proposed a two-step self-augmented prompting approach: first using prompts to ask the LLM to generate additional knowledge (intermediate output) about the table, then incorporating the response into the second prompt to request the final answer for a downstream task.Ye et al. (2023b)also guided the LLM to decompose a huge table into a small table, and to convert a complex question into simpler subquestions for text reasoning.Their strategy achieved significantly better results than competitive baselines for table-based reasoning, outperforms human performance for the first time on the TabFact dataset.ForLiu et al. (2023e), in encouraging symbolic CoT reasoning pathways, they allowed the model to interact with a Python shell that could execute commands, process data, and scrutinize results, particularly within a pandas dataframe, limited to a maximum of five iterative steps.</p>
<p>Zhang et al. (2023h)umber or category, F1 or Accuracy evaluation metrics are common.If evaluating open-ended responses, apart from using typical measures for like ROUGE and BLEU, some papers also hire annotators to evaluate the Informativeness, Coherence and Fluency of the LLM responsesZhang et al. (2023h).When connected to programs like Python, Power BI, etc, LLMs' outputs are not limited to text and code.For example, creating visualizations from text and table inputs are a popular task tooZhang et al. (2023h);Zha et al. (2023).</p>
<p>We would like to thank Fanyou for his valuable contributions in discussing the project and idetifying relevant methoods.
Same name, different group of authors.
For table headers, they explored synonym and abbreviation replacement perturbations. For table content, they explored five perturbations: (1) row shuffling, (2) column shuffling, (3) extending column names content into semantically equivalent expressions, (4) masking correlated columns (E.g. "Ranking" and "Total Points" can be inferred from one another), and (5) introducing new columns that are derived from existing columns. For the question itself, they perturbed questions at the word-level or sentence-level.
GitHub repository link https://Github.com/clinicalml/TabLLM/tree/main/datasets
Link to the pre-trained data https://Github.com/Kaggle/kaggle-api
Here is the Github repo for TABLET https://Github.com/dylan-slack/Tablet, TabLLM https://Github.com/ clinicalml/TabLLM and LIFT https://Github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning
The code is in https://Github.com/ngruver/llmtime
Available at https://Github.com/RyanWangZf/MediTab.
The dataset is in https://huggingface.co/datasets/yuweiyin/FinBench and the code for FinPT is in https://Github.com/ YuweiYin/FinPT
The code is in https://github.com/kathrinse/be_great
The code is in https://github.com/ZhangTP1996/TapTap
The code is in https://github.com/zhao-zilong/Tabula
Official sites forTable QA datasets: FetaQA (https://github.com/Yale-LILY/FeTaQA), WikiTableQuestions (https://ppasupat.github.io/WikiTableQuestions/), HybridQA (https://github.com/wenhuchen/HybridQA), SQA (https:
https://huggingface.co/NumbersStation
For the scope of our paper, we do not consider images, videos and audio inputs.
https://github.com/tanfiona/LLM-on-Tabular-Data-Prediction-Table-Understanding-Data-Generation
Official site for Dresden Web Tables. 15 Official site for HybriDialogue</p>
<p>Tablequery: Querying tabular data with natural language. CoRR, abs/2202.0045419 Official site for InfoTabs. Ait-Qa, Fariz Rahman, and Damanpreet Kaur2022Official sites for the domain-specific datasets</p>
<p>Exploring the numerical reasoning capabilities of language models: A comprehensive analysis on tabular data. Mubashara Akhtar, Abhilash Reddy Shankarampeta, Vivek Gupta, Arpit Patil, Oana Cocarascu, Elena Simperl, Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 6-10, 2023. 2023</p>
<p>FEVEROUS: fact extraction and verification over unstructured and structured information. Rami Aly, Zhijiang Guo, Sejr Michael, James Schlichtkrull, Andreas Thorne, Vlachos, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021. Joaquin Vanschoren, Sai-Kit Yeung, the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021December 2021, virtual, 2021Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal</p>
<p>Tabnet: Attentive interpretable tabular learning. Sercan O Arik, Tomas Pfister, 2020</p>
<p>Generating synthetic data in finance: opportunities, challenges and pitfalls. Danial Samuel A Assefa, Mahmoud Dervovic, Robert E Mahfouz, Prashant Tillman, Manuela Reddy, Veloso, Proceedings of the First ACM International Conference on AI in Finance. the First ACM International Conference on AI in Finance2020</p>
<p>Transformers for tabular data representation: A survey of models and applications. Gilbert Badaro, Mohammed Saeed, Paolo Papotti, 10.1162/tacl_a_00544Transactions of the Association for Computational Linguistics. 112023</p>
<p>Synthesizing electronic health records using improved generative adversarial networks. Kanti Mrinal, Chia-Ching Baowaly, Chao-Lin Lin, Kuan-Ta Liu, Chen, Journal of the American Medical Informatics Association. 2632019</p>
<p>Improving deep neural network generalization and robustness to background bias via layer-wise relevance propagation optimization. Sergio Sj Pedro Ras Bassi, Andrea Dertkigil, Cavalli, Nature Communications. 1512912024</p>
<p>Multimodal llms for health grounded in individual-specific data. Anastasiya Belyaeva, Justin Cosentino, Farhad Hormozdiari, Krish Eswaran, Shravya Shetty, Greg Corrado, Andrew Carroll, Cory Y Mclean, Nicholas A Furlotte, Workshop on Machine Learning for Multimodal Healthcare Data. Springer2023</p>
<p>A neural probabilistic language model. Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Advances in neural information processing systems. 200013</p>
<p>Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle Mcdonell, Jason Phang, Michael Pieler, Shivanshu Usvsn Sai Prashanth, Laria Purohit, Jonathan Reynolds, Ben Tow, Samuel Wang, Weinbach, Gpt-neox-20b: An open-source autoregressive language model. 2022</p>
<p>Elephants never forget: Testing language models for memorization of tabular data. Sebastian Bordt, Harsha Nori, Rich Caruana, arXiv:2403.066442024arXiv preprint</p>
<p>Deep neural networks and tabular data: A survey. Vadim Borisov, Tobias Leemann, Kathrin Seßler, Johannes Haug, Martin Pawelczyk, Gjergji Kasneci, IEEE Transactions on Neural Networks and Learning Systems. 2022a</p>
<p>Language models are realistic tabular data generators. Vadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, Gjergji Kasneci, arXiv:2210.062802022barXiv preprint</p>
<p>Language models are realistic tabular data generators. Vadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, Gjergji Kasneci, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaMay 1-5, 2023OpenReview.net, 2023a</p>
<p>Language models are realistic tabular data generators. Vadim Borisov, Kathrin Sessler, Tobias Leemann, Martin Pawelczyk, Gjergji Kasneci, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, 2020Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordIlya Sutskever, and Dario Amodei. Language models are few-shot learners</p>
<p>Arm-net: Adaptive relation modeling network for structured data. Shaofeng Cai, Kaiping Zheng, Gang Chen, H V Jagadish, Beng , Chin Ooi, Meihui Zhang, 10.1145/3448016.3457321Proceedings of the 2021 International Conference on Management of Data, SIGMOD/PODS '21. the 2021 International Conference on Management of Data, SIGMOD/PODS '21ACMJune 2021</p>
<p>How to prompt llms for text-to-sql: A study in zero-shot, singledomain, and cross-domain settings. Shuaichen Chang, Eric Fosler-Lussier, 10.48550/arXiv.2305.118532023</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S Yu, Qiang Yang, Xing Xie, 10.1145/3641289ACM Trans. Intell. Syst. Technol. 2157-6904jan 2024</p>
<p>Danets: Deep abstract networks for tabular data classification and regression. Jintai Chen, Kuanlun Liao, Yao Wan, Danny Z Chen, Jian Wu, 2022a</p>
<p>Excelformer: A neural network surpassing gbdts on tabular data. Jintai Chen, Jiahuan Yan, Danny Ziyi Chen, Jian Wu, 2023a</p>
<p>Bridge the gap between language models and tabular understanding. Nuo Chen, Linjun Shou, Ming Gong, Jian Pei, Chenyu You, Jianhui Chang, Daxin Jiang, Jia Li, 10.48550/arXiv.2302.093022023b</p>
<p>Xgboost: A scalable tree boosting system. Tianqi Chen, Carlos Guestrin, 10.1145/2939672.2939785Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16. the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16New York, NY, USAAssociation for Computing Machinery2016</p>
<p>Large language models are few(1)-shot table reasoners. Wenhu Chen, 10.18653/v1/2023.findings-eacl.83Findings of the Association for Computational Linguistics: EACL 2023. Andreas Vlachos, Isabelle Augenstein, Dubrovnik, CroatiaAssociation for Computational LinguisticsMay 2-6, 20232023</p>
<p>Tabfact : A large-scale dataset for table-based fact verification. Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, William Yang, Wang , International Conference on Learning Representations (ICLR). Addis Ababa, EthiopiaApril 2020a</p>
<p>Hybridqa: A dataset of multi-hop question answering over tabular and textual data. Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, William Yang, Wang , 10.18653/v1/2020.findings-emnlp.91Findings of the Association for Computational Linguistics: EMNLP 2020. Trevor Cohn, Yulan He, Yang Liu, Association for Computational Linguistics16-20 November 2020. 2020bEMNLP 2020 of Findings of ACL</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, 10.48550/arXiv.2211.125882022b</p>
<p>Phoenix: Democratizing chatgpt across languages. Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan, Benyou Wang, Haizhou Li, 10.48550/arXiv.2304.104532023c</p>
<p>Table search using a deep contextualized language model. Zhiyu Chen, Mohamed Trabelsi, Jeff Heflin, Yinan Xu, Brian D Davison, 10.1145/3397271.3401044Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20. the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20ACMJuly 2020c</p>
<p>. Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah, 2016Wide &amp; deep learning for recommender systems</p>
<p>HiTab: A hierarchical table dataset for question answering and natural language generation. Zhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han, Jian-Guang Lou, Dongmei Zhang, 10.18653/v1/2022.acl-long.78Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Binding language models in symbolic languages. Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaMay 1-5, 2023. 2023OpenReview.net</p>
<p>Generating multi-label discrete patient records using generative adversarial networks. Edward Choi, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F Stewart, Jimeng Sun, 2018</p>
<p>. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Alex Chowdhery, Marie Castro-Ros, Kevin Pellat, Dasha Robinson, Sharan Valter, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Andrew Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, 2022Scaling instruction-finetuned language models</p>
<p>Observatory: Characterizing embeddings of relational tables. Tianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul Groth, H V Jagadish, 10.48550/arXiv.2310.077362023</p>
<p>Prototypical verbalizer for promptbased few-shot tuning. Ganqu Cui, Shengding Hu, Ning Ding, Longtao Huang, Zhiyuan Liu, 2022</p>
<p>Synthesising multi-modal minority samples for tabular data. Sajad Darabi, Yotam Elor, 2021</p>
<p>TURL: table understanding through representation learning. Sanjib Das, Anhai Doan, Paul Suganthan, G C Chaitanya Gokhale, Pradap Konda, Yash Govind, Derek Paulsen, 10.1145/3542700.3542709Xiang Deng, Huan Sun, Alyssa Lees, You Wu, and Cong Yu. SIGMOD Rec2015. 2022a51The magellan data repository</p>
<p>PACIFIC: towards proactive conversational question answering over tabular and textual data in finance. Yang Deng, Wenqiang Lei, Wenxuan Zhang, Wai Lam, Tat-Seng Chua, 10.18653/v1/2022.emnlp-main.469Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 7-11, 2022. 2022b</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. Jill Burstein, Christy Doran, Thamar Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinnesotaAssociation for Computational LinguisticsJune 20191</p>
<p>Parameter-efficient fine-tuning of large-scale pre-trained language models. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Nature Machine Intelligence. 532023</p>
<p>Lift: Language-interfaced fine-tuning for non-language machine learning tasks. Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong Sohn, Dimitris Papailiopoulos, and Kangwook Lee. 2022Advances in Neural Information Processing Systems</p>
<p>C3: zero-shot text-to-sql with chatgpt. Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, Lu Chen, Jinshu Lin, Dongfang Lou, 10.48550/arXiv.2307.073062023</p>
<p>Yuntao Du, Ninghui Li, arXiv:2402.06806Towards principled assessment of tabular data synthesis algorithms. 2024arXiv preprint</p>
<p>Building the dresden web table corpus: A classification approach. Julian Eberius, Katrin Braunschweig, Markus Hentsch, Maik Thiele, Ahmad Ahmadov, Wolfgang Lehner, 10.1109/BDC.2015.302nd IEEE/ACM International Symposium on Big Data Computing, BDC 2015. Ioan Raicu, Omer F Rana, Rajkumar Buyya, Limassol, CyprusIEEE Computer SocietyDecember 7-10, 2015. 2015</p>
<p>How large language models will disrupt data management. Raul Castro Fernandez, Aaron J Elmore, Michael J Franklin, Sanjay Krishnan, Chenhao Tan, 10.14778/3611479.3611527Proc. VLDB Endow. VLDB Endow202316</p>
<p>Tabular and latent space synthetic data generation: a literature review. Joao Fonseca, Fernando Bacao, Journal of Big Data. 1011152023Published in Transactions on Machine Learning Research (07/2024</p>
<p>How does gpt obtain its ability? tracing emergent abilities of language models to their sources. Yao Fu's Notion. Hao Fu, ; Yao, Tushar Peng, Khot, GPT-Obtain-its-Ability-Tracing-Emergent-Abilitiesof-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1. Dec 2022</p>
<p>On the inadequacy of similarity-based privacy metrics: Reconstruction attacks against" truly anonymous synthetic data. Georgi Ganev, Emiliano De, Cristofaro , arXiv:2312.051142023arXiv preprint</p>
<p>Text-to-sql empowered by large language models: A benchmark evaluation. Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, Jingren Zhou, Proc. VLDB Endow. VLDB Endow202417</p>
<p>TableGPT: Few-shot table-to-text generation with table structure reconstruction and content matching. Heng Gong, Yawei Sun, Xiaocheng Feng, Bing Qin, Wei Bi, Xiaojiang Liu, Ting Liu, 10.18653/v1/2020.coling-main.179Proceedings of the 28th International Conference on Computational Linguistics. Donia Scott, Nuria Bel, Chengqing Zong, the 28th International Conference on Computational LinguisticsBarcelona, SpainDecember 2020</p>
<p>Revisiting deep learning models for tabular data. Ian Goodfellow, Yoshua Bengio, Aaron Courville ; Yury, Ivan Gorishniy, Valentin Rubachev, Artem Khrulkov, Babenko, Advances in Neural Information Processing Systems. MIT press2016. 202134Deep learning</p>
<p>Why do tree-based models still outperform deep learning on tabular data?. Léo Grinsztajn, Edouard Oyallon, Gaël Varoquaux, 2022</p>
<p>Large language models are zero-shot time series forecasters. Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Gordon, Wilson , arXiv:2310.078202023arXiv preprint</p>
<p>PASTA: table-operations aware fact verification via sentence-table cloze pre-training. Zihui Gu, Ju Fan, Nan Tang, Preslav Nakov, Xiaoman Zhao, Xiaoyong Du, 10.18653/v1/2022.emnlp-main.331Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 7-11, 2022. 2022</p>
<p>TabMT: Generating tabular data with masked transformers. S Manbir, Gulati, Paul F Roysdon, Thirtyseventh Conference on Neural Information Processing Systems. 2023</p>
<p>Entity embeddings of categorical variables. Cheng Guo, Felix Berkhahn, 2016</p>
<p>Deepfm: A factorization-machine based neural network for ctr prediction. Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He, 2017</p>
<p>INFOTABS: Inference on tables as semistructured data. Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, Vivek Srikumar, 10.18653/v1/2020.acl-main.210Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Tabllm: Few-shot classification of tabular data with large language models. Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, David A Sontag, International Conference on Artificial Intelligence and Statistics. J R Francisco, Jennifer G Ruiz, Jan-Willem Dy, Van De Meent, Valencia, SpainPMLRApril 2023. 2023206of Proceedings of Machine Learning Research</p>
<p>Synthetic data generation for tabular health records: A systematic review. Mikel Hernandez, Gorka Epelde, Ane Alberdi, Rodrigo Cilla, Debbie Rankin, Neurocomputing. 4932022</p>
<p>Weakly supervised table parsing via pre-training. Jonathan Herzig, Krzysztof Pawel, Thomas Nowak, Francesco Müller, Julian Piccinno, Martin Eisenschlos, Tapas, 10.18653/v1/2020.acl-main.398Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel R Tetreault, the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020Association for Computational LinguisticsJuly 5-10, 2020. 2020</p>
<p>Open domain question answering over tables via dense retrieval. Jonathan Herzig, Thomas Mueller, Syrine Krichene, Julian Eisenschlos, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu, 10.48550/arXiv.2311.052322023a</p>
<p>Tabtransformer: Tabular data modeling using contextual embeddings. Xin Huang, Ashish Khetan, Milan Cvitkovic, Zohar Karnin, 2020</p>
<p>Data ambiguity strikes back: How documentation improves gpt's text-to-sql. Zezhou Huang, Pavan Kalyan Damalapati, Eugene Wu, 10.48550/arXiv.2310.187422023b</p>
<p>TABBIE: Pretrained representations of tabular data. Hiroshi Iida, Dung Thai, Varun Manjunatha, Mohit Iyyer, 10.18653/v1/2021.naacl-main.270Proceedings of the 2021 Conference of the North American Chapter. Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, Yichao Zhou, the 2021 Conference of the North American ChapterAssociation for Computational LinguisticsJune 2021</p>
<p>Boost then convolve: Gradient boosting meets graph neural networks. Sergei Ivanov, Liudmila Prokhorenkova, 2021</p>
<p>Search-based neural structured learning for sequential question answering. Mohit Iyyer, Wen-Tau Yih, Ming-Wei Chang, 10.18653/v1/P17-1167Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. Regina Barzilay, Min-Yen Kan, the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJuly 20171</p>
<p>Unsupervised dense information retrieval with contrastive learning. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave, Trans. Mach. Learn. Res. 2022. 2022</p>
<p>Towards better serialization of tabular data for few-shot classification with large language models. Sukriti Jaitly, Tanay Shah, Ashish Shugani, Razik Singh, Grewal , 2023</p>
<p>StructGPT: A general framework for large language model to reason over structured data. Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, Ji-Rong Wen, 10.18653/v1/2023.emnlp-main.574Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Time-llm: Time series forecasting by reprogramming large language models. Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, arXiv:2310.017282023aarXiv preprint</p>
<p>Large models for time series and spatio-temporal data: A survey and outlook. Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang, James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, Shirui Pan, Vincent S Tseng, Yu Zheng, Lei Chen, Hui Xiong, arXiv:2310.101962023barXiv preprint</p>
<p>A survey on table question answering: Recent advances. Nengzheng Jin, Joanna Siebert, Dongfang Li, Qingcai Chen, 2022</p>
<p>Generating and imputing tabular data via diffusion and flow-based gradient-boosted trees. Alexia Jolicoeur-Martineau, Kilian Fatras, Tal Kachman, 2023</p>
<p>Understanding the effects of language-specific class imbalance in multilingual fine-tuning. Vincent Jung, Lonneke Van Der Plas, 2024</p>
<p>Well-tuned simple nets excel on tabular datasets. Arlind Kadra, Marius Lindauer, Frank Hutter, Josif Grabocka, 2021</p>
<p>AIT-QA: Question answering dataset over complex tables in the airline industry. Yannis Katsis, Saneem Chemmengath, Vishwajeet Kumar, Samarth Bharadwaj, Mustafa Canim, Michael Glass, Alfio Gliozzo, Feifei Pan, Jaydeep Sen, Karthik Sankaranarayanan, Soumen Chakrabarti, 10.18653/v1/2022.naacl-industry.34Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track. Anastassia Loukina, Rashmi Gangadharaiah, Bonan Min, the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry TrackHybrid: Seattle, Washington + OnlineAssociation for Computational LinguisticsJuly 2022</p>
<p>Net-dnf: Effective deep modeling of tabular data. Liran Katzir, Gal Elidan, Ran El-Yaniv, International conference on learning representations. 2020</p>
<p>Deepgbm: A deep learning framework distilled by gbdt for online prediction tasks. Guolin Ke, Zhenhui Xu, Jia Zhang, Jiang Bian, Tie-Yan Liu, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2019a</p>
<p>TabNN: A universal neural network solution for tabular data. Guolin Ke, Jia Zhang, Zhenhui Xu, Jiang Bian, Tie-Yan Liu, 2019b</p>
<p>Unifiedqa-v2: Stronger generalization via broader cross-format training. Daniel Khashabi, Yeganeh Kordi, Hannaneh Hajishirzi, arXiv:2202.123592022arXiv preprint</p>
<p>Jayoung Kim, Chaejeong Lee, Noseong Park, Stasy, arXiv:2210.04018Score-based tabular data synthesis. 2022aarXiv preprint</p>
<p>Sos: Score-based oversampling for tabular data. Jayoung Kim, Chaejeong Lee, Yehjin Shin, Sewon Park, Minjung Kim, Noseong Park, Jihoon Cho, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022b</p>
<p>Character-aware neural language models. Yoon Kim, Yacine Jernite, David Sontag, Alexander M Rush, 2015</p>
<p>Serkan Kiranyaz, Onur Avci, Osama Abdeljaber, Turker Ince, Moncef Gabbouj, Daniel J Inman, 1d convolutional neural networks and applications: A survey. 2019</p>
<p>Opentab: Advancing large language models as open-domain table reasoners. Kezhi Kong, Jiani Zhang, Zhengyuan Shen, Balasubramaniam Srinivasan, Chuan Lei, Christos Faloutsos, Huzefa Rangwala, George Karypis, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Self-attention between datapoints: Going beyond individual input-output pairs in deep learning. Jannik Kossen, Neil Band, Clare Lyle, Aidan N Gomez, Tom Rainforth, Yarin Gal, 2022</p>
<p>Tabddpm: Modelling tabular data with diffusion models. Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, Artem Babenko, 2022</p>
<p>Codi: Co-evolving contrastive diffusion models for mixedtype tabular synthesis. Chaejeong Lee, Jayoung Kim, Noseong Park, arXiv:2304.126542023arXiv preprint</p>
<p>Biobert: a pre-trained biomedical language representation model for biomedical text mining. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, 10.1093/bioinformatics/btz682Bioinformatics. 1367-4811364September 2019</p>
<p>RESDSQL: decoupling schema linking and skeleton parsing for text-to-sql. Haoyang Li, Jing Zhang, Cuiping Li, Hong Chen, 10.1609/aaai.v37i11.26535Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023. Brian Williams, Yiling Chen, Jennifer Neville, Washington, DC, USAAAAI PressFebruary 7-14, 2023. 2023a</p>
<p>Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing. Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, Yongbin Li, 10.1609/aaai.v37i11.26536Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence. Brian Williams, Yiling Chen, Jennifer Neville, Washington, DC, USAAAAI PressFebruary 7-14, 202320232023Thirteenth Symposium on Educational Advances in Artificial Intelligence</p>
<p>Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, Advances in Neural Information Processing Systems. 2023c36</p>
<p>Table-gpt: Table-tuned GPT for diverse table tasks. Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri, 10.48550/arXiv.2310.092632023d</p>
<p>Ctrl: Connect collaborative and language model for ctr prediction. Xiangyang Li, Bo Chen, Lu Hou, Ruiming Tang, 2023e</p>
<p>Behrt: transformer for electronic health records. Yikuan Li, Shishir Rao, José Roberto Ayala, Abdelaali Solares, Rema Hassaine, Dexter Ramakrishnan, Yajie Canoy, Kazem Zhu, Gholamreza Rahimi, Salimi-Khorshidi, Scientific reports. 10171552020a</p>
<p>Sync: A copula based framework for generating synthetic data from aggregated sources. Zheng Li, Yue Zhao, Jialin Fu, 2020b</p>
<p>PET-SQL: A prompt-enhanced two-stage text-to-sql framework with cross-consistency. Zhishuai Li, Xiang Wang, Jingjing Zhao, Sun Yang, Guoqing Du, Xiaoru Hu, Bin Zhang, Yuxiao Ye, Ziyue Li, Rui Zhao, Hangyu Mao, 10.48550/arXiv.2403.097322024</p>
<p>Ptab: Using the pre-trained language model for modeling tabular data. Guang Liu, Jie Yang, Ledell Wu, 2022a</p>
<p>Prompt valuation based on shapley values. Hanxi Liu, Xiaokai Mao, Haocheng Xia, Jian Lou, Jinfei Liu, 10.48550/arXiv.2312.153952023a</p>
<p>Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, Colin A Raffel, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, 10.48550/arXiv.2307.031722023b</p>
<p>TAPEX: table pre-training via learning a neural SQL executor. Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. April 25-29, 2022. 2022cOpenReview.net</p>
<p>Jarvix: A LLM no code platform for tabular data analysis and optimization. Shangching Liu, Shengkun Wang, Tsungyao Chang, Wenqi Lin, Chung-Wei Hsiung, Yi-Chen Hsieh, Yu-Ping Cheng, Sian-Hong Luo, Jianwei Zhang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: EMNLP 2023 -Industry Track. Mingxuan Wang, Imed Zitouni, the 2023 Conference on Empirical Methods in Natural Language Processing: EMNLP 2023 -Industry TrackSingaporeAssociation for Computational LinguisticsDecember 6-10, 2023. 2023c</p>
<p>Goggle: Generative modelling for tabular data by learning relational structure. Tennison Liu, Zhaozhi Qian, Jeroen Berrevoets, Mihaela Van Der Schaar, The Eleventh International Conference on Learning Representations. 2023d</p>
<p>Rethinking tabular data understanding with large language models. Tianyang Liu, Fei Wang, Muhao Chen, 2023e</p>
<p>Investigating the fairness of large language models for predictions on tabular data. Yanchen Liu, Srishti Gautam, Jiaqi Ma, Himabindu Lakkaraju, Short Version in NeurIPS 2023 Workshop on Socially Responsible Language Modelling Research. 2023f</p>
<p>Summary of chatgpt-related research and perspective towards the future of large language models. Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, Zihao Wu, Lin Zhao, Dajiang Zhu, Xiang Li, Ning Qiang, Dingang Shen, Tianming Liu, Bao Ge, 10.1016/j.metrad.2023.100017Meta-Radiology. 2950-162812100017September 2023g</p>
<p>Roberta: A robustly optimized BERT pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, CoRR, abs/1907.116922019</p>
<p>Dnn2lr: Interpretation-inspired feature crossing for real-world tabular data. Zhaocheng Liu, Qiang Liu, Haoli Zhang, Yuntian Chen, 2021</p>
<p>Matching table metadata with business glossaries using large language models. Elita Lobo, Oktie Hassanzadeh, Nhan Pham, Nandana Mihindukulasooriya, Dharmashankar Subramanian, Horst Samulowitz, Hui Luan, Chin-Chung Tsai, Educational Technology &amp; Society. 2412023. 2021A review of using machine learning approaches for precision education</p>
<p>Sdtr: Soft decision tree regressor for tabular data. Haoran Luo, Fan Cheng, Heng Yu, Yuqi Yi, IEEE Access. 92021</p>
<p>Vaem: a deep generative model for heterogeneous mixed type data. Chao Ma, Sebastian Tschiatschek, José Miguel Hernández-Lobato, Richard Turner, Cheng Zhang, 2020</p>
<p>Approximate, adapt, anonymize (3a): a framework for privacy preserving training data release for machine learning. Tamas Madl, Weijie Xu, Olivia Choudhury, Matthew Howard, 2023</p>
<p>Language models are weak learners. Hariharan Manikandan, Yiding Jiang, J Zico Kolter, 2023</p>
<p>Prompt engineering in large language models. Ggaliwango Marvin, Nakayiza Hellen, Daudi Jjingo, Joyce Nakatumba-Nabende, International Conference on Data Intelligence and Cognitive Informatics. Springer2023</p>
<p>Adapting pretrained language models for solving tabular prediction problems in the electronic health record. Christopher Mcmaster, David Fl Liew, Douglas Ev Pires, 2023</p>
<p>Scigen: a dataset for reasoningaware text generation from scientific tables. Nafise Sadat Moosavi, Andreas Rücklé, Dan Roth, Iryna Gurevych, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). 2021</p>
<p>HybriDialogue: An information-seeking dialogue dataset grounded on tabular and textual data. Kai Nakamura, Sharon Levy, Yi-Lin Tuan, Wenhu Chen, William Yang, Wang , 10.18653/v1/2022.findings-acl.41Findings of the Association for Computational Linguistics: ACL 2022. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, Dublin, IrelandAssociation for Computational LinguisticsMay 2022</p>
<p>Fetaqa: Free-form table question answering. Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryscinski, Hailey Schoelkopf, Riley Kong, Xiangru Tang, Mutethia Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong, Dragomir R Radev, 10.1162/tacl_a_00446Trans. Assoc. Comput. Linguistics. 102022</p>
<p>Can foundation models wrangle your data?. Avanika Narayan, Ines Chami, Laurel J Orr, Christopher Ré, 10.14778/3574245.3574258Proc. VLDB Endow. VLDB Endow202216</p>
<p>Rethinking data augmentation for tabular data in deep learning. Soma Onishi, Shoya Meguro, 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. S Sanmi Koyejo, A Mohamed, Danielle Agarwal, K Belgrave, A Cho, Oh, NeurIPS; New Orleans, LA, USA2022. 2022. November 28 -December 9, 2022. 2022</p>
<p>ToTTo: A controlled table-to-text generation dataset. Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, Dipanjan Das, 10.18653/v1/2020.emnlp-main.89Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Bonnie Webber, Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsNovember 2020a</p>
<p>Totto: A controlled table-to-text generation dataset. P Ankur, Xuezhi Parikh, Sebastian Wang, Manaal Gehrmann, Bhuwan Faruqui, Diyi Dhingra, Dipanjan Yang, Das, CoRR, abs/2004.143732020b</p>
<p>Data synthesis based on generative adversarial networks. Noseong Park, Mahmoud Mohammadi, Kshitij Gorde, Sushil Jajodia, Hongkyu Park, Youngmin Kim, 10.14778/3231751.3231757Proceedings of the VLDB Endowment. the VLDB EndowmentJune 201811</p>
<p>Compositional semantic parsing on semi-structured tables. Panupong Pasupat, Percy Liang, 10.3115/v1/p15-1142Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language ProcessingBeijing, ChinaThe Association for Computer Linguistics2015. July 26-31, 2015. 2015a1</p>
<p>Compositional semantic parsing on semi-structured tables. Panupong Pasupat, Percy Liang ; Neha, Roy Patki, Kalyan Wedge, Veeramachaneni, 10.1109/DSAA.2016.492016 IEEE International Conference on Data Science and Advanced Analytics (DSAA). 2015b. 2016The synthetic data vault</p>
<p>Deep contextualized word representations. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, 10.18653/v1/N18-1202Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics2018a1</p>
<p>Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, Deep contextualized word representations. 2018b</p>
<p>Neural oblivious decision ensembles for deep learning on tabular data. Sergei Popov, Stanislav Morozov, Artem Babenko, 2019</p>
<p>DIN-SQL: decomposed in-context learning of text-tosql with self-correction. Mohammadreza Pourreza, Davood Rafiei, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023. Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, Sergey Levine, New Orleans, LA, USADecember 10 -16, 2023. 20232023</p>
<p>Catboost: unbiased boosting with categorical features. Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, Andrey Gulin, 2019</p>
<p>Improving generalization in language model-based text-to-sql semantic parsing: Two simple semantic boundary-based techniques. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu ; Daking Rai, Bailin Wang, Yilun Zhou, Ziyu Yao, 10.18653/v1/2023.acl-short.15Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Anna Rogers, Jordan L Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 202322023Short Papers), ACL 2023</p>
<p>Med-bert: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, Degui Zhi, NPJ digital medicine. 41862021</p>
<p>Large test collection experiments on an operational, interactive system: Okapi at TREC. Stephen E Robertson, Steve Walker, Micheline Hancock-Beaulieu, 10.1016/0306-4573(94)00051-4https://doi.org/10.1016/0306-4573(94)00051-4Inf. Process. Manag. 3131995</p>
<p>Machine learning for quantitative finance applications: A survey. Francesco Rundo, Francesca Trenta, Agatino Luigi Di Stallo, Sebastiano Battiato, Applied Sciences. 92455742019</p>
<p>Explainable artificial intelligence for tabular data: A survey. Maria Sahakyan, Zeyar Aung, Talal Rahwan, IEEE access. 92021</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Nihal Chhablani, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, Rachel Pandey, Thomas Bawden, Trishala Wang, Jos Neeraj, Abheesht Rozen, Andrea Sharma, Thibault Santilli, ; Fevry, M Alexander, Rush, 2021Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Chhablani, V Nihal, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, Rachel Pandey, Thomas Bawden, Trishala Wang, Jos Neeraj, Abheesht Rozen, Andrea Sharma, Thibault Santilli, Jason Févry, Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M Rush, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. April 25-29, 2022. 2022OpenReview.net</p>
<p>Testing the limits of unified sequence to sequence LLM pretraining on diverse table data tasks. Soumajyoti Sarkar, Leonard Lausen, 10.48550/arXiv.2310.007892023</p>
<p>The use of generative adversarial networks to alleviate class imbalance in tabular data: a survey. Rick Sauber, - Cole, Taghi M Khoshgoftaar, Journal of Big Data. 91982022</p>
<p>Aggregate and mixed-order markov models for statistical language processing. Lawrence Saul, Fernando Pereira, 1997</p>
<p>Curated llm: Synergy of llms and data curation for tabular augmentation in ultra low-data regimes. Nabeel Seedat, Nicolas Huynh, Boris Van Breugel, Mihaela Van Der Schaar, arXiv:2312.121122023arXiv preprint</p>
<p>Predictive biases in natural language processing models: A conceptual framework and overview. Deven Santosh Shah, Andrew Schwartz, Dirk Hovy, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Regularization learning networks: deep learning for tabular datasets. Ira Shavitt, Eran Segal, Advances in Neural Information Processing Systems. 312018</p>
<p>Cpllm: Clinical prediction with large language models. Ben Ofir, Nadav Shoham, Rappoport, arXiv:2309.112952023arXiv preprint</p>
<p>Tabular data: Deep learning is not all you need. Ravid Shwartz, -Ziv , Amitai Armon, Information Fusion. 812022</p>
<p>Tabular representation, noisy operators, and impacts on table structure understanding tasks in llms. Ananya Singha, José Cambronero, Sumit Gulwani, Le Vu, Chris Parnin, 10.48550/arXiv.2310.103582023</p>
<p>Tablet: Learning from instructions for tabular data. Dylan Slack, Sameer Singh, 2023</p>
<p>Realtabformer: Generating realistic relational and tabular data using transformers. V Aivin, Olivier Solatorio, Dupriez, arXiv:2302.020412023arXiv preprint</p>
<p>Saint: Improved neural networks for tabular data via row attention and contrastive pre-training. Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C Bayan Bruss, Tom Goldstein, 2021</p>
<p>The first step is the hardest: Pitfalls of representing and tokenizing temporal data for large language models. Dimitris Spathis, Fahim Kawsar, 2023</p>
<p>Language models are an effective representation learning technique for electronic health record data. Ethan Steinberg, Ken Jung, Jason A Fries, Stephen R Conor K Corbin, Pfohl, Nigam, Shah, Journal of biomedical informatics. 1131036372021</p>
<p>Evaluating and enhancing structural understanding capabilities of large language models on tables via input designs. Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, Dongmei Zhang, 10.48550/arXiv.2305.130622023a</p>
<p>Gpt4table: Can large language models understand structured table data? a benchmark and empirical study. Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, Dongmei Zhang, 2023b</p>
<p>Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning. Yuan Sui, Jiaru Zou, Mengyu Zhou, Xinyi He, Lun Du, Shi Han, Dongmei Zhang, 2023c</p>
<p>Table meets llm: Can large language models understand structured table data? a benchmark and empirical study. Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, Dongmei Zhang, Proceedings of the 17th ACM International Conference on Web Search and Data Mining. the 17th ACM International Conference on Web Search and Data Mining2024</p>
<p>Supertml: Two-dimensional word embedding for the precognition on structured tabular data. Baohua Sun, Lin Yang, Wenhan Zhang, Michael Lin, Patrick Dong, Charles Young, Jason Dong, 2019</p>
<p>Test: Text prototype aligned embedding to activate llm's ability for time series. Chenxi Sun, Yaliang Li, Hongyan Li, Shenda Hong, 2023a</p>
<p>Sql-palm: Improved large language model adaptation for text-to-sql. Ruoxi Sun, Sercan Ö Arik, Hootan Nakhost, Hanjun Dai, Rajarishi Sinha, Pengcheng Yin, Tomas Pfister, 10.48550/arXiv.2306.007392023b</p>
<p>cTBLS: Augmenting large language models with conversational tables. S Anirudh, Larry Sundar, Heck, 10.18653/v1/2023.nlp4convai-1.6Proceedings of the 5th Workshop on NLP for Conversational AI. Yun-Nung Chen, Abhinav Rastogi, the 5th Workshop on NLP for Conversational AIToronto, CanadaNLP4ConvAI 2023. July 2023Association for Computational Linguistics</p>
<p>Efficient transformers: A survey. Yi Tay, Mostafa Dehghani, Dara Bahri, Donald Metzler, 10.1145/3530811ACM Comput. Surv. 5562023a</p>
<p>UL2: unifying language learning paradigms. Yi Tay, Mostafa Dehghani, Q Vinh, Xavier Tran, Jason Garcia, Xuezhi Wei, Hyung Won Wang, Dara Chung, Tal Bahri, Huaixiu Schuster, Denny Steven Zheng, Neil Zhou, Donald Houlsby, Metzler, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaMay 1-5, 2023. 2023bOpenReview.net</p>
<p>. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Yaguang Du, Hongrae Li, Huaixiu Lee, Amin Steven Zheng, Marcelo Ghafouri, Yanping Menegali, Maxim Huang, Dmitry Krikun, James Lepikhin, Dehao Qin, Yuanzhong Chen, Zhifeng Xu, Adam Chen, Maarten Roberts, Vincent Bosma, Yanqi Zhao, Chung-Ching Zhou, Igor Chang, Will Krivokon, Marc Rusch, Pranesh Pickett, Laichee Srinivasan, Kathleen Man, Meier-Hellstern, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak2022Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben HutchinsonQuoc Le. Lamda: Language models for dialog applications</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023aarXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom, 10.48550/arXiv.2307.092882023b</p>
<p>Stock market prediction using machine learning (ml) algorithms. Muhammad Umer, Muhammad Awais, Muhammad Muzammul, ADCAIJ: Advances in Distributed Computing and Artificial Intelligence Journal. 842019</p>
<p>Generating privacy-preserving synthetic tabular data using oblivious variational autoencoders. Harsha Vivek, Stanley Vardhan, Kok, Proceedings of the Workshop on Economics of Privacy and Data Labor at the 37 th International Conference on Machine Learning (ICML). the Workshop on Economics of Privacy and Data Labor at the 37 th International Conference on Machine Learning (ICML)2020</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Isabelle Guyon, Samy Ulrike Von Luxburg, Hanna M Bengio, Rob Wallach, S V N Fergus, Roman Vishwanathan, Garnett, Long Beach, CA, USA2017. December 4-9, 2017. 2017</p>
<p>Pre-trained language models and their applications. Engineering. Haifeng Wang, Jiwei Li, Hua Wu, Eduard Hovy, Yu Sun, 2022a</p>
<p>Unipredict: Large language models are universal tabular predictors. Ruiyu Wang, Zifeng Wang, Jimeng Sun, arXiv:2310.032662023aarXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaMay 1-5, 2023. 2023bOpenReview.net</p>
<p>. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Gary Haizhi, Ishan Lai, Ishani Purohit, Jacob Mondal, Kirby Anderson, Krima Kuznia, Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabi2022bSuper-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks</p>
<p>TUTA: tree-based transformers for generally structured table pre-training. Zhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, Dongmei Zhang, 10.1145/3447548.3467434KDD '21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event. Feida Zhu, Beng , Chin Ooi, Chunyan Miao, SingaporeACMAugust 14-18, 20212021</p>
<p>Transtab: Learning transferable tabular transformers across tables. Zifeng Wang, Jimeng Sun, 2022</p>
<p>Meditab: Scaling medical tabular data predictors via data consolidation, enrichment, and refinement. Zifeng Wang, Chufan Gao, Cao Xiao, Jimeng Sun, 2023c</p>
<p>Chain-of-table: Evolving tables in the reasoning chain for table understanding. Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng Wang, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, Tomas Pfister, 10.48550/arXiv.2401.043982024</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Dai, V Quoc, Le, 2022a</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Trans. Mach. Learn. Res. 2022. 2022b</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. S Sanmi Koyejo, A Mohamed, Danielle Agarwal, K Belgrave, A Cho, Oh, NeurIPS; New Orleans, LA, USA2022. 2022. November 28 -December 9, 2022. 2022c</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Modeling tabular data using conditional gan. Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, Kalyan Veeramachaneni, 2019</p>
<p>Detime: Diffusion-enhanced topic modeling using encoder-decoder based llm. Weijie Xu, Wenxiang Hu, Fanyou Wu, Srinivasan Sengamedu, 10.18653/v1/2023.findings-emnlp.606Findings of the Association for Computational Linguistics: EMNLP 2023. Association for Computational Linguistics2023a</p>
<p>Ffpdg: Fast, fair and private data generation. Weijie Xu, Jinjin Zhao, Francis Iannacci, Bo Wang, arXiv:2307.001612023barXiv preprint</p>
<p>Sqlnet: Generating structured queries from natural language without reinforcement learning. Xiaojun Xu, Chang Liu, Dawn Song, 2017</p>
<p>Hao Xue, Flora D Salim, arXiv:2210.08964Prompt-based time series forecasting: A new task and dataset. 2022arXiv preprint</p>
<p>Sqlizer: query synthesis from natural language. Navid Yaghmazadeh, Yuepeng Wang, Isil Dillig, Thomas Dillig, 10.1145/3133887Proc. ACM Program. Lang. 1oct 2017</p>
<p>T2g-former: Organizing tabular features into relation graphs promotes heterogeneous feature interaction. Jiahuan Yan, Jintai Chen, Yixuan Wu, Danny Z Chen, Jian Wu, 2023</p>
<p>Serval: Synergy learning between vertical models and llms towards oracle-level zero-shot medical prediction. Jiahuan Yan, Jintai Chen, Chaowen Hu, Bo Zheng, Yaojun Hu, Jimeng Sun, Jian Wu, arXiv:2403.015702024aarXiv preprint</p>
<p>Making pre-trained language models great on tabular prediction. Jiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Chen, Jimeng Sun, Jian Wu, Jintai Chen, arXiv:2403.018412024barXiv preprint</p>
<p>Effective distillation of table-based reasoning ability from llms. Bohao Yang, Chen Tang, Kun Zhao, Chenghao Xiao, Chenghua Lin, 2023</p>
<p>Ct-bert: Learning better tabular representations through cross-table pre-training. Guoshan Chao Ye, Haobo Lu, Liyao Wang, Sai Li, Gang Wu, Junbo Chen, Zhao, 2023a</p>
<p>Large language models are versatile decomposers: Decomposing evidence and questions for table-based reasoning. Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li, 10.1145/3539618.3591708Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023. Hsin-Hsi Chen, Wei-Jou ( Edward) Duh, Hen-Hsen Huang, Makoto P Kato, Josiane Mothe, Barbara Poblete, the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023Taipei, TaiwanACMJuly 23-27, 2023. 2023b</p>
<p>TRANX: A transition-based neural abstract syntax parser for semantic parsing and code generation. Pengcheng Yin, Graham Neubig, 10.18653/v1/D18-2002Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Eduardo Blanco, Wei Lu, the 2018 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsBrussels, BelgiumAssociation for Computational LinguisticsNovember 2018</p>
<p>Tabert: Pretraining for joint understanding of textual and tabular data. Pengcheng Yin, Graham Neubig, Wen Tau Yih, Sebastian Riedel, 2020a</p>
<p>TaBERT: Pretraining for joint understanding of textual and tabular data. Pengcheng Yin, Graham Neubig, Wen-Tau Yih, Sebastian Riedel, 10.18653/v1/2020.acl-main.745Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020b</p>
<p>Finpt: Financial risk prediction with profile tuning on pretrained foundation models. Yuwei Yin, Yazheng Yang, Jian Yang, Qi Liu, 2023</p>
<p>Unified language representation for question answering over text, tables, and images. Bowen Yu, Cheng Fu, Haiyang Yu, Fei Huang, Yongbin Li, 10.18653/v1/2023.findings-acl.292Findings of the Association for Computational Linguistics: ACL 2023. Anna Rogers, Jordan L Boyd-Graber, Naoaki Okazaki, Toronto, CanadaAssociation for Computational LinguisticsJuly 9-14, 20232023</p>
<p>Typesql: Knowledge-based type-aware neural text-to-sql generation. Tao Yu, Zifan Li, Zilin Zhang, Rui Zhang, Dragomir Radev, 2018a</p>
<p>Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, Dragomir Radev, 10.18653/v1/D18-1425Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Ellen Riloff, David Chiang, Julia Hockenmaier, Jun'ichi Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsOctober-November 2018b</p>
<p>Cosql: A conversational text-to-sql challenge towards cross-domain natural language interfaces to databases. Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Victoria Xi, Yi Lin, Tianze Chern Tan, Zihan Shi, Youxuan Li, Michihiro Jiang, Sungrok Yasunaga, Tao Shim, Alexander R Chen, Zifan Fabbri, Luyao Li, Yuwen Chen, Shreya Zhang, Vincent Dixit, Caiming Zhang, Richard Xiong, Walter S Socher, Dragomir R Lasecki, Radev, 10.18653/v1/D19-1204Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. Kentaro Inui, Jing Jiang, Vincent Ng, Xiaojun Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaAssociation for Computational LinguisticsNovember 3-7, 2019. 2019a</p>
<p>Sparc: Cross-domain semantic parsing in context. Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent Zhang, Caiming Xiong, Richard Socher, Dragomir R Radev, 10.18653/v1/p19-1443Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Long Papers. Anna Korhonen, David R Traum, Lluís Màrquez, the 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyAssociation for Computational LinguisticsJuly 28-August 2, 2019. 2019b1</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, Jie Tang, Glm-130b: An open bilingual pre-trained model. 2023</p>
<p>Tablegpt: Towards unifying tables, nature language and commands into one GPT. Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang, Saisai Yang, Jing Yuan, Changbao Su, Xiang Li, Aofeng Su, Tao Zhang, Chen Zhou, Kaizhe Shou, Miao Wang, Wufang Zhu, Guoshan Lu, Chao Ye, Yali Ye, Wentao Ye, Yiming Zhang, Xinglong Deng, Jie Xu, Haobo Wang, Gang Chen, Junbo Zhao, 10.48550/arXiv.2307.086742023</p>
<p>Towards foundation models for learning on tabular data. Han Zhang, Xumeng Wen, Shun Zheng, Wei Xu, Jiang Bian, 2023a</p>
<p>Jellyfish: A large language model for data preprocessing. Haochen Zhang, Yuyang Dong, Chuan Xiao, Masafumi Oyamada, 10.48550/arXiv.2312.016782023b</p>
<p>Mixed-type tabular data synthesis with score-based diffusion in latent space. Hengrui Zhang, Jiani Zhang, Balasubramaniam Srinivasan, Zhengyuan Shen, Xiao Qin, Christos Faloutsos, Huzefa Rangwala, George Karypis, arXiv:2310.096562023carXiv preprint</p>
<p>Bridging the gap: Deciphering tabular data using large language model. Hengyuan Zhang, Peng Chang, Zongcheng Ji, 10.48550/arXiv.2308.118912023d</p>
<p>Privbayes: Private data release via bayesian networks. Jun Zhang, Graham Cormode, Cecilia M Procopiuc, Divesh Srivastava, Xiaokui Xiao, 10.1145/3134428ACM Trans. Database Syst. 0362-5915424oct 2017</p>
<p>Generative table pre-training empowers models for tabular prediction. Tianping Zhang, Shaowen Wang, Shuicheng Yan, Li Jian, Qian Liu, 10.18653/v1/2023.emnlp-main.917Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsDecember 2023e</p>
<p>Tablellama: Towards open large generalist models for tables. Tianshu Zhang, Xiang Yue, Yifei Li, Huan Sun, 10.48550/arXiv.2311.092062023f</p>
<p>Tianshu Zhang, Xiang Yue, Yifei Li, Huan Sun, Tablellama, Towards open large generalist models for tables. 2023g</p>
<p>Natural language interfaces for tabular data querying and visualization: A survey. Weixu Zhang, Yifei Wang, Yuanfeng Song, Junqiu Victor, Yuxing Wei, Yiyan Tian, Jonathan H Qi, Raymond Chan, Chi-Wing Wong, Haiqin Yang, 10.48550/arXiv.2310.178942023h</p>
<p>Large language models are complex table parsers. Bowen Zhao, Changkai Ji, Yuejie Zhang, Wen He, Yingwen Wang, Qing Wang, Rui Feng, Xiaobo Zhang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational LinguisticsDecember 6-10, 2023. 2023a</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, A survey of large language models. 2023b</p>
<p>Wenting Zhao, Ye Liu, Tong Niu, Yao Wan, Philip S Yu, Shafiq Joty, Yingbo Zhou, Semih Yavuz, Divknowqa: Assessing the reasoning ability of llms via open-domain question answering over knowledge base and text. 2023c</p>
<p>Docmath-eval: Evaluating numerical reasoning capabilities of llms in understanding long documents with tabular data. Yilun Zhao, Yitao Long, Hongjun Liu, Linyong Nan, Lyuhao Chen, Ryo Kamoi, Yixin Liu, Xiangru Tang, Rui Zhang, Arman Cohan, 10.48550/arXiv.2311.098052023d</p>
<p>RobuT: A systematic study of table QA robustness against human-annotated adversarial perturbations. Yilun Zhao, Chen Zhao, Linyong Nan, Zhenting Qi, Wenlin Zhang, Xiangru Tang, Boyu Mi, Dragomir Radev, 10.18653/v1/2023.acl-long.334Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 2023e1Long Papers)</p>
<p>Tabula: Harnessing language models for tabular data synthesis. Zilong Zhao, Robert Birke, Lydia Chen, arXiv:2310.127462023farXiv preprint</p>
<p>Diffusion models for missing value imputation in tabular data. Shuhan Zheng, Nontawat Charoenphakdee, arXiv:2210.171282022arXiv preprint</p>
<p>Seq2sql: Generating structured queries from natural language using reinforcement learning. Victor Zhong, Caiming Xiong, Richard Socher, 2017a</p>
<p>Seq2sql: Generating structured queries from natural language using reinforcement learning. Victor Zhong, Caiming Xiong, Richard Socher, CoRR, abs/1709.001032017b</p>
<p>TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance. Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, Tat-Seng Chua, 10.18653/v1/2021.acl-long.254Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAugust 2021a1</p>
<p>Converting tabular data into images for deep learning with convolutional neural networks. Yitan Zhu, Thomas Brettin, Fangfang Xia, Alexander Partin, Maulik Shukla, Hyunseung Yoo, Yvonne A Evrard, James H Doroshow, Rick L Stevens, Scientific reports. 111113252021b</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1393 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1393</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1393</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-da9f7ac1ba026ca493cf0cc2f9077bbec5a587fd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/da9f7ac1ba026ca493cf0cc2f9077bbec5a587fd" target="_blank">StateSpaceDiffuser: Bringing Long Context to Diffusion World Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> StateSpaceDiffuser is introduced, where a diffusion model is enabled to perform long-context tasks by integrating features from a state-space model, representing the entire interaction history, to establish that bringing state-space representations into diffusion models is highly effective in demonstrating both visual details and long-term memory.</p>
                <p><strong>Paper Abstract:</strong> World models have recently become promising tools for predicting realistic visuals based on actions in complex environments. However, their reliance on only a few recent observations leads them to lose track of the long-term context. Consequently, in just a few steps the generated scenes drift from what was previously observed, undermining the temporal coherence of the sequence. This limitation of the state-of-the-art world models, most of which rely on diffusion, comes from their lack of a lasting environment state. To address this problem, we introduce StateSpaceDiffuser, where a diffusion model is enabled to perform long-context tasks by integrating features from a state-space model, representing the entire interaction history. This design restores long-term memory while preserving the high-fidelity synthesis of diffusion models. To rigorously measure temporal consistency, we develop an evaluation protocol that probes a model's ability to reinstantiate seen content in extended rollouts. Comprehensive experiments show that StateSpaceDiffuser significantly outperforms a strong diffusion-only baseline, maintaining a coherent visual context for an order of magnitude more steps. It delivers consistent views in both a 2D maze navigation and a complex 3D environment. These results establish that bringing state-space representations into diffusion models is highly effective in demonstrating both visual details and long-term memory.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1393.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1393.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StateSpaceDiffuser</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>StateSpaceDiffuser (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid world model that fuses a discrete state-space long-context branch (Mamba) with a diffusion-based generative branch (DIAMOND UNet/EDM) to produce high-fidelity, long-horizon visual predictions while preserving long-term context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>StateSpaceDiffuser</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hybrid architecture with two branches: (1) Long-Context Branch — a discrete state-space model (Mamba) that encodes a long sequence of compact image features (Cosmos CI16 tokens concatenated with action embeddings) into a persistent state h and produces memory features m_t; (2) Generative Branch — a diffusion-based UNet (DIAMOND / EDM) that generates next-frame imagery conditioned on a short window (4 frames) and the processed state-space features via MLP fusion; includes a low-resolution primary denoiser and a secondary upsampler to 280x150.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>hybrid model (state-space + diffusion)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual world modeling: 2D maze navigation (MiniGrid) and 3D first-person gameplay (CSGO)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>PSNR and SSIM for MiniGrid quantitative evaluation; user preference scores for CSGO (human study). Also qualitative visual inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>MiniGrid (context length 16): Avg PSNR 41.01, Final PSNR 40.55, SSIM 0.98. MiniGrid (context length 50): Avg PSNR 39.68, Final PSNR 39.32, SSIM 0.98. Ablation (no state features): Avg PSNR 23.68, Final PSNR 20.95, SSIM 0.92. CSGO human study: mean preference ratings +0.20 (15th frame) and +0.24 (17th frame) favoring StateSpaceDiffuser over diffusion-only baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Relatively low-level interpretability: the state-space branch maintains an internal state vector h (discrete SSM) that can be inspected as features, but no explicit semantic or disentangled factors are reported. The paper treats the system as a neural black box with inspectable token-level features (Cosmos tokens) rather than explicitly interpretable latent variables.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None formalized; inspection is limited to qualitative visualization of generated frames and ablation (zeroing state features) showing dependence on the memory features. The state outputs m_t are used as conditioning vectors but no further visualization/semantic mapping is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Full model ~600M parameters. Baseline DIAMOND inference (4 input frames) measured at 909.515 GFLOPS. Added Long-Context Branch costs: 5.5 GFLOPS for context length 16 and 16.741 GFLOPS for context length 50 (inference, batch size 1). Trained on 8 A100 GPUs (StateSpaceDiffuser training); training iterations: MiniGrid ~77k, CSGO ~220k (details in supplement).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Authors claim minimal extra compute relative to diffusion baseline: the SSM adds only a small GFLOPS overhead (5.5–16.7 GFLOPS) compared to DIAMOND's ~909.5 GFLOPS, while providing large gains in long-term consistency. They emphasize linear-time sequence processing of SSMs vs quadratic-cost attention alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Strong improvement on long-horizon content-consistency tasks: on MiniGrid forward-backward recall the model produces significantly higher PSNR/SSIM vs diffusion-only and SSM-only baselines; user study on CSGO prefers StateSpaceDiffuser over DIAMOND on long-horizon recall frames.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Hybrid design translates improved long-term memory (from SSM) into higher-fidelity actionable frames (via diffusion): the state features enable the diffusion branch to reinstantiate far-past content (useful for planning, visual consistency), and fidelity gains are measured quantitatively (PSNR/SSIM) and qualitatively (user study). The SSM-only model retains memory but yields blurrier outputs; diffusion-only produces fidelity but lacks memory — the hybrid yields both.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-offs discussed: diffusion-only models deliver high-fidelity short-context synthesis but suffer drift and forget long context; SSM-only models capture long-range information and are computationally efficient but produce lower-fidelity, blurrier images; the hybrid adds a small compute overhead to regain long-memory while retaining diffusion fidelity. Some limitations: StateSpaceDiffuser may still miss fine-grained details and sometimes reconstructs only coarse features.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: use Cosmos CI16 tokenizer to compress frames to single feature vectors per frame; adopt Mamba as the state-space backbone (single Mamba layer, state size 256) to track long context; condition DIAMOND diffusion on 4 frames and concatenated processed SSM features via MLPs (SiLU activations); train Long-Context Branch first, then train Generative Branch with loaded SSM weights; autoregressive single-frame prediction for CSGO.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared experimentally to (1) DIAMOND (diffusion-only): DIAMOND achieves decent short-horizon fidelity (e.g., PSNR ~27) but rapidly degrades on long-horizon recall (final PSNR ~25); (2) State-Space World Model (SSM-only): retains long-range information (PSNR ~33) but generates blurrier lower-fidelity images. StateSpaceDiffuser outperforms both in long-context fidelity and visual quality (e.g., ~39–41 PSNR vs DIAMOND ~26 and SSM-only ~32). Efficiency: SSM branch adds negligible compute relative to DIAMOND.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends decoupling global context modeling (state-space backbone, e.g., Mamba) from high-fidelity synthesis (diffusion), pretraining the Long-Context Branch then training the Generative Branch, using Mamba as SSM backbone, and conditioning diffusion on both short-window frames and SSM features. Larger SSM capacity is suggested to improve fine detail reconstruction when needed; context length 50 training produced best results for long horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StateSpaceDiffuser: Bringing Long Context to Diffusion World Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1393.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1393.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DIAMOND</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DIAMOND (Diffusion for world modeling baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A UNet-based EDM diffusion world model used as the high-fidelity generative baseline (from prior work) that conditions on a short history (4 frames) and predicts next-frame imagery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diffusion for world modeling: Visual details matter in atari.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DIAMOND (diffusion-only baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Diffusion-based UNet/EDM architecture for next-frame prediction conditioned on a short sliding window of frames (K=4) and 512-d action embeddings; two-stage design with a low-resolution denoiser and a learned upsampler to high resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>diffusion-based generative world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual prediction in sequential environments (Atari cited in original DIAMOND; here evaluated on MiniGrid and CSGO)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>PSNR and SSIM on MiniGrid; human preference study on CSGO.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>MiniGrid (context length 16): Avg PSNR 27.13, Final PSNR 25.44, SSIM 0.95. MiniGrid (context length 50): Avg PSNR 26.13, Final PSNR 25.15, SSIM 0.95.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural generator; no interpretability mechanisms reported in this paper beyond visual inspection of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Measured inference cost (batch size 1, 4 input frames): 909.515 GFLOPS. Specific parameter count not stated here for DIAMOND baseline alone (StateSpaceDiffuser total ~600M params includes diffusion component).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Computationally expensive relative to the added SSM branch: DIAMOND dominates compute (909.5 GFLOPS) while SSM adds only a few GFLOPS; however DIAMOND lacks long-context capability without extra modules or conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Performs well for short-horizon high-fidelity predictions but fails to maintain temporal consistency and recall content far beyond its input window, resulting in drift during long rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High pixel fidelity on short-horizon predictions but low utility for long-context tasks requiring memory or re-instantiation of far-past observations; fails to preserve content in forward-backward recall tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High-fidelity synthesis vs lack of lasting memory: DIAMOND provides image detail but cannot maintain long-term consistency; adding memory (SSM) is necessary to overcome this at small additional compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Short conditioning window (K=4) and UNet/EDM diffusion design with two-stage denoiser + upsampler; autoregressive sliding-window generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to SSM-only and hybrid: DIAMOND yields higher-quality short-term images than SSM-only but much worse long-term recall and temporal coherence; hybrid StateSpaceDiffuser improves both fidelity and long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in detail beyond using DIAMOND as the generative backbone for the hybrid; authors suggest augmenting such diffusion models with SSM features for optimal long-horizon behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StateSpaceDiffuser: Bringing Long Context to Diffusion World Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1393.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1393.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>State-Space World Model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>State-Space World Model (SSM-only baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A world model that uses only the Long-Context Branch (SSM over Cosmos-tokenized frame features + action embeddings) to predict future tokenized features and decode them to images; trained with MSE on token features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>State-Space World Model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SSM-only predictive model that encodes each full frame to a compact Cosmos CI16 token vector, concatenates action embeddings, processes the sequence with an SSM (e.g., Mamba or S4) to predict future token features ˆf_t, and decodes predicted tokens to images using the Cosmos decoder; trained with MSE on token features.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>state-space latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual world modeling (MiniGrid, CSGO)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>PSNR and SSIM (MiniGrid); qualitative assessment (CSGO).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>MiniGrid (context length 16): Avg PSNR 33.40, Final PSNR 33.17, SSIM 0.96 (Table 1). MiniGrid (context length 50): Avg PSNR 32.64, Final PSNR 32.44, SSIM 0.96.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderate: the SSM maintains an explicit internal state h and output features m_t that can be directly inspected; however outputs decoded from tokens tend to be blurry and not semantically disentangled.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Analysis via qualitative visualization of decoded frames and via ablation (comparing decoded predictions to ground truth); internal states are described but not mapped to semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Computationally efficient relative to diffusion: SSM branch inference cost reported as 5.5 GFLOPS (context 16) and 16.741 GFLOPS (context 50). Training used fewer GPUs (State-Space World Model trained on 4 A100 GPUs for MiniGrid).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Much cheaper than the diffusion generative branch; SSMs process long sequences in linear time and add negligible compute when fused with diffusion.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Good at retaining and reproducing long-range content (higher long-horizon PSNR than DIAMOND), but generated images are lower-fidelity and blurrier than diffusion-based outputs, limiting visual quality.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>SSM-only is useful for long-range memory and for providing representations for planning, but its lower visual quality limits downstream uses where high-fidelity imagery is required; thus it serves well as a memory module but not as a standalone high-fidelity generator.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Memory and efficiency vs visual fidelity: SSMs retain long-term information and are cheap, but sacrifice fine visual detail; combining SSM with diffusion mitigates this trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of Cosmos tokenizer to compress frames to tokens, concatenation with 16-d action embeddings, training with MSE on token features, choice of SSM backbone (Mamba preferred over S4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to DIAMOND: higher long-term PSNR (better recall) but lower image fidelity; compared to hybrid: hybrid obtains both better memory utilization and higher visual fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Authors used Mamba as backbone for best performance; recommendation is to use SSM as memory backbone and decode via a stronger generative model for best overall outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StateSpaceDiffuser: Bringing Long Context to Diffusion World Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1393.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1393.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mamba</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mamba: Linear-time sequence modeling with selective state spaces</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A selective discrete state-space model that introduces dynamic selection in state updates to improve expressiveness while retaining linear-time sequence processing; used here as the Long-Context backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mamba: Linear-time sequence modeling with selective state spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mamba (SSM backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Discrete state-space model with selective gating/dynamic selection to increase representational power while preserving linear scalability; integrated in this work to process Cosmos-tokenized full-frame features concatenated with action embeddings, producing memory features for conditioning the diffusion generator.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>state-space model (SSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>long-sequence encoding for visual world modeling (MiniGrid and CSGO)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Indirect: evaluated by downstream PSNR/SSIM when used as backbone in State-Space World Model and StateSpaceDiffuser.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>When used as backbone in State-Space World Model on MiniGrid (context 16): Avg PSNR 27.09, Final PSNR 27.87, SSIM 0.94 (Table 2 when comparing S4 vs Mamba as backbones; these numbers represent SSM-only performance in that backbone evaluation). In aggregate experiments, Mamba-based State-Space World Model achieved higher PSNRs (see other tables).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>SSM internals are structured (state h, linear update matrices A,B,C) providing some mechanistic interpretability compared to opaque recurrent nets, but no explicit interpretability analyses are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None detailed in this paper beyond reporting improved downstream metrics and noting Mamba's dynamic selection mechanism as beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Designed for linear-time processing and efficient parallelism; in this work a single Mamba layer with state size 256 was used; adding Mamba branch cost measured as a few GFLOPS (5.5–16.7 GFLOPS depending on context length).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Reported superior efficiency vs transformer/self-attention approaches for long sequences; empirically better performance than S4 in this paper's backbone comparison (Mamba outperforms S4 by ~9.1 PSNR on average in the evaluated setting).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Improves long-context retention and downstream prediction fidelity when used as a backbone for world modeling; chosen by authors as the Long-Context backbone due to better empirical performance than alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Provides most of the long-term memory needed to reinstantiate far-past visual content; when fused into diffusion, yields both memory and high visual fidelity in downstream predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Mamba introduces dependency on input for state updates (beneficial for long-context performance) and increases expressiveness relative to simpler SSMs, without large compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Single Mamba layer, state size 256, input expanded 4x with internal MLP in the Mamba layer as used in implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Directly compared to S4 in the paper: Mamba achieved higher PSNR (27.09 vs 24.29 avg PSNR in the backbone evaluation on MiniGrid), leading to its selection.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Authors selected Mamba (single layer, state size 256) as the practical optimal backbone for balancing memory, expressiveness, and compute in their experiments; they note scaling the Long-Context Branch could further improve fine detail recall.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StateSpaceDiffuser: Bringing Long Context to Diffusion World Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1393.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1393.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>S4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>S4 (Structured State Space for Sequence modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured state-space model used as an alternative SSM backbone in experiments; simpler than Mamba but capable of long-range dependency modeling via parameterized structured operators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficiently modeling long sequences with structured state spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>S4 (SSM backbone baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Structured state-space model that uses carefully designed linear operators (HiPPO-related parameterizations) to capture long-range dependencies efficiently; used here for backbone comparison in the State-Space World Model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>state-space model (SSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>sequence modeling for visual world modeling (MiniGrid backbone comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Downstream PSNR/SSIM when used as backbone in State-Space World Model.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Backbone evaluation (MiniGrid context length 16): Avg PSNR 24.29, Final PSNR 24.31, SSIM 0.91 (Table 2). Mamba outperformed S4 in the same evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>SSM internals are structured and mathematically interpretable to some extent (linear operators A,B,C), but no interpretability-specific analyses are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None applied in this paper beyond backbone performance comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Designed for linear-time sequence processing; specific GFLOPS for S4 not stated but considered efficient; in this work Mamba was preferred for better performance.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More efficient than transformer attention for long sequences; in this study Mamba offered better representational performance for similar efficiency, so Mamba was chosen.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Capable of long-horizon retention but yielded lower downstream PSNR/SSIM than Mamba in authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>S4 provides a computationally efficient memory backbone but its representational capacity in this setup was outperformed by Mamba, limiting utility for highest-fidelity long-horizon reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Lower PSNR (worse long-horizon recall) compared to Mamba, indicating a trade-off between SSM variant design and downstream fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Evaluated as an off-the-shelf SSM backbone; authors used this comparison to justify choosing Mamba.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Directly compared with Mamba; Mamba produced ~9.1 PSNR improvement over S4 in the backbone comparison for MiniGrid.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Authors concluded Mamba was preferable for their setting; S4 remains a valid SSM option but with lower performance in these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StateSpaceDiffuser: Bringing Long Context to Diffusion World Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1393.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1393.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cosmos tokenizer (CI16)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cosmos tokenizer, continuous variant with scale 16 (CI16)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A frame tokenizer used to compress high-resolution frames into compact continuous patch tokens which are flattened to a single feature vector per frame for SSM processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cosmos world foundation model platform for physical ai.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Cosmos tokenizer (CI16)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A visual tokenizer that produces patch tokens (dimension 16) from images; tokens for each frame are flattened into a single high-dimensional feature vector (1296 for MiniGrid, 2448 for CSGO in this paper) that serves as input to the state-space branch; decoder is used to reconstruct images from predicted tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>visual tokenizer / encoder-decoder for latent representation</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>compact frame encoding for long-sequence state-space modeling in visual world modeling</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Used indirectly – downstream PSNR/SSIM measured on decoded images from token predictions; MSE used to train SSM on token space.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported directly for tokenizer alone; used in State-Space World Model training where decoded outputs are blurrier than diffusion outputs but retain long-term content (SSM-only PSNR values reported above).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Provides structured, discrete spatial patch tokens which are interpretable as compressed image patches; no explicit interpretability experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported beyond using decoded token visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Tokenization/decoding costs included in Long-Context Branch compute; specific GFLOPS not broken out separately. Token dimensions: 1296 (MiniGrid), 2448 (CSGO); action embedding dimension 16.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Enables SSM processing at the frame level (one feature vector per frame) rather than patch- or token-level SSM application, reducing sequence length and making long-context SSM processing tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Enables SSM to capture long-range scene content in a compressed form; when used with SSM-only models produces coherent but lower-fidelity reconstructions; when fused into diffusion supports high-fidelity generation with long memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Serves as an efficient representation that balances compression and information retention enabling SSMs to process full frames over long contexts without prohibitive sequence lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Tokenization reduces dimensionality enabling efficient SSM use, but decoded outputs from token predictions are blurrier compared to diffusion outputs; thus tokenizer is best used as memory feature provider rather than sole generator.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Used continuous Cosmos CI16 tokens, flattened per-frame to form SSM inputs; action embeddings concatenated to tokens before SSM processing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Choosing frame-level tokenization (one vector per frame) is contrasted with patch-level SSM processing (prior work); authors select frame-level due to clearer scalability and integration with diffusion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StateSpaceDiffuser: Bringing Long Context to Diffusion World Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Diffusion for world modeling: Visual details matter in atari. <em>(Rating: 2)</em></li>
                <li>Mamba: Linear-time sequence modeling with selective state spaces. <em>(Rating: 2)</em></li>
                <li>Efficiently modeling long sequences with structured state spaces. <em>(Rating: 2)</em></li>
                <li>Facing off world model backbones: Rnns, transformers, and s4. <em>(Rating: 2)</em></li>
                <li>Cosmos world foundation model platform for physical ai. <em>(Rating: 2)</em></li>
                <li>S4WM (S4 World Model related works referenced as S4WM in text) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1393",
    "paper_id": "paper-da9f7ac1ba026ca493cf0cc2f9077bbec5a587fd",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "StateSpaceDiffuser",
            "name_full": "StateSpaceDiffuser (this work)",
            "brief_description": "A hybrid world model that fuses a discrete state-space long-context branch (Mamba) with a diffusion-based generative branch (DIAMOND UNet/EDM) to produce high-fidelity, long-horizon visual predictions while preserving long-term context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "StateSpaceDiffuser",
            "model_description": "Hybrid architecture with two branches: (1) Long-Context Branch — a discrete state-space model (Mamba) that encodes a long sequence of compact image features (Cosmos CI16 tokens concatenated with action embeddings) into a persistent state h and produces memory features m_t; (2) Generative Branch — a diffusion-based UNet (DIAMOND / EDM) that generates next-frame imagery conditioned on a short window (4 frames) and the processed state-space features via MLP fusion; includes a low-resolution primary denoiser and a secondary upsampler to 280x150.",
            "model_type": "hybrid model (state-space + diffusion)",
            "task_domain": "visual world modeling: 2D maze navigation (MiniGrid) and 3D first-person gameplay (CSGO)",
            "fidelity_metric": "PSNR and SSIM for MiniGrid quantitative evaluation; user preference scores for CSGO (human study). Also qualitative visual inspection.",
            "fidelity_performance": "MiniGrid (context length 16): Avg PSNR 41.01, Final PSNR 40.55, SSIM 0.98. MiniGrid (context length 50): Avg PSNR 39.68, Final PSNR 39.32, SSIM 0.98. Ablation (no state features): Avg PSNR 23.68, Final PSNR 20.95, SSIM 0.92. CSGO human study: mean preference ratings +0.20 (15th frame) and +0.24 (17th frame) favoring StateSpaceDiffuser over diffusion-only baseline.",
            "interpretability_assessment": "Relatively low-level interpretability: the state-space branch maintains an internal state vector h (discrete SSM) that can be inspected as features, but no explicit semantic or disentangled factors are reported. The paper treats the system as a neural black box with inspectable token-level features (Cosmos tokens) rather than explicitly interpretable latent variables.",
            "interpretability_method": "None formalized; inspection is limited to qualitative visualization of generated frames and ablation (zeroing state features) showing dependence on the memory features. The state outputs m_t are used as conditioning vectors but no further visualization/semantic mapping is reported.",
            "computational_cost": "Full model ~600M parameters. Baseline DIAMOND inference (4 input frames) measured at 909.515 GFLOPS. Added Long-Context Branch costs: 5.5 GFLOPS for context length 16 and 16.741 GFLOPS for context length 50 (inference, batch size 1). Trained on 8 A100 GPUs (StateSpaceDiffuser training); training iterations: MiniGrid ~77k, CSGO ~220k (details in supplement).",
            "efficiency_comparison": "Authors claim minimal extra compute relative to diffusion baseline: the SSM adds only a small GFLOPS overhead (5.5–16.7 GFLOPS) compared to DIAMOND's ~909.5 GFLOPS, while providing large gains in long-term consistency. They emphasize linear-time sequence processing of SSMs vs quadratic-cost attention alternatives.",
            "task_performance": "Strong improvement on long-horizon content-consistency tasks: on MiniGrid forward-backward recall the model produces significantly higher PSNR/SSIM vs diffusion-only and SSM-only baselines; user study on CSGO prefers StateSpaceDiffuser over DIAMOND on long-horizon recall frames.",
            "task_utility_analysis": "Hybrid design translates improved long-term memory (from SSM) into higher-fidelity actionable frames (via diffusion): the state features enable the diffusion branch to reinstantiate far-past content (useful for planning, visual consistency), and fidelity gains are measured quantitatively (PSNR/SSIM) and qualitatively (user study). The SSM-only model retains memory but yields blurrier outputs; diffusion-only produces fidelity but lacks memory — the hybrid yields both.",
            "tradeoffs_observed": "Trade-offs discussed: diffusion-only models deliver high-fidelity short-context synthesis but suffer drift and forget long context; SSM-only models capture long-range information and are computationally efficient but produce lower-fidelity, blurrier images; the hybrid adds a small compute overhead to regain long-memory while retaining diffusion fidelity. Some limitations: StateSpaceDiffuser may still miss fine-grained details and sometimes reconstructs only coarse features.",
            "design_choices": "Key choices: use Cosmos CI16 tokenizer to compress frames to single feature vectors per frame; adopt Mamba as the state-space backbone (single Mamba layer, state size 256) to track long context; condition DIAMOND diffusion on 4 frames and concatenated processed SSM features via MLPs (SiLU activations); train Long-Context Branch first, then train Generative Branch with loaded SSM weights; autoregressive single-frame prediction for CSGO.",
            "comparison_to_alternatives": "Compared experimentally to (1) DIAMOND (diffusion-only): DIAMOND achieves decent short-horizon fidelity (e.g., PSNR ~27) but rapidly degrades on long-horizon recall (final PSNR ~25); (2) State-Space World Model (SSM-only): retains long-range information (PSNR ~33) but generates blurrier lower-fidelity images. StateSpaceDiffuser outperforms both in long-context fidelity and visual quality (e.g., ~39–41 PSNR vs DIAMOND ~26 and SSM-only ~32). Efficiency: SSM branch adds negligible compute relative to DIAMOND.",
            "optimal_configuration": "Paper recommends decoupling global context modeling (state-space backbone, e.g., Mamba) from high-fidelity synthesis (diffusion), pretraining the Long-Context Branch then training the Generative Branch, using Mamba as SSM backbone, and conditioning diffusion on both short-window frames and SSM features. Larger SSM capacity is suggested to improve fine detail reconstruction when needed; context length 50 training produced best results for long horizons.",
            "uuid": "e1393.0",
            "source_info": {
                "paper_title": "StateSpaceDiffuser: Bringing Long Context to Diffusion World Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "DIAMOND",
            "name_full": "DIAMOND (Diffusion for world modeling baseline)",
            "brief_description": "A UNet-based EDM diffusion world model used as the high-fidelity generative baseline (from prior work) that conditions on a short history (4 frames) and predicts next-frame imagery.",
            "citation_title": "Diffusion for world modeling: Visual details matter in atari.",
            "mention_or_use": "use",
            "model_name": "DIAMOND (diffusion-only baseline)",
            "model_description": "Diffusion-based UNet/EDM architecture for next-frame prediction conditioned on a short sliding window of frames (K=4) and 512-d action embeddings; two-stage design with a low-resolution denoiser and a learned upsampler to high resolution.",
            "model_type": "diffusion-based generative world model",
            "task_domain": "visual prediction in sequential environments (Atari cited in original DIAMOND; here evaluated on MiniGrid and CSGO)",
            "fidelity_metric": "PSNR and SSIM on MiniGrid; human preference study on CSGO.",
            "fidelity_performance": "MiniGrid (context length 16): Avg PSNR 27.13, Final PSNR 25.44, SSIM 0.95. MiniGrid (context length 50): Avg PSNR 26.13, Final PSNR 25.15, SSIM 0.95.",
            "interpretability_assessment": "Black-box neural generator; no interpretability mechanisms reported in this paper beyond visual inspection of outputs.",
            "interpretability_method": "None mentioned in this paper.",
            "computational_cost": "Measured inference cost (batch size 1, 4 input frames): 909.515 GFLOPS. Specific parameter count not stated here for DIAMOND baseline alone (StateSpaceDiffuser total ~600M params includes diffusion component).",
            "efficiency_comparison": "Computationally expensive relative to the added SSM branch: DIAMOND dominates compute (909.5 GFLOPS) while SSM adds only a few GFLOPS; however DIAMOND lacks long-context capability without extra modules or conditioning.",
            "task_performance": "Performs well for short-horizon high-fidelity predictions but fails to maintain temporal consistency and recall content far beyond its input window, resulting in drift during long rollouts.",
            "task_utility_analysis": "High pixel fidelity on short-horizon predictions but low utility for long-context tasks requiring memory or re-instantiation of far-past observations; fails to preserve content in forward-backward recall tasks.",
            "tradeoffs_observed": "High-fidelity synthesis vs lack of lasting memory: DIAMOND provides image detail but cannot maintain long-term consistency; adding memory (SSM) is necessary to overcome this at small additional compute cost.",
            "design_choices": "Short conditioning window (K=4) and UNet/EDM diffusion design with two-stage denoiser + upsampler; autoregressive sliding-window generation.",
            "comparison_to_alternatives": "Compared to SSM-only and hybrid: DIAMOND yields higher-quality short-term images than SSM-only but much worse long-term recall and temporal coherence; hybrid StateSpaceDiffuser improves both fidelity and long-term memory.",
            "optimal_configuration": "Not discussed in detail beyond using DIAMOND as the generative backbone for the hybrid; authors suggest augmenting such diffusion models with SSM features for optimal long-horizon behavior.",
            "uuid": "e1393.1",
            "source_info": {
                "paper_title": "StateSpaceDiffuser: Bringing Long Context to Diffusion World Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "State-Space World Model",
            "name_full": "State-Space World Model (SSM-only baseline)",
            "brief_description": "A world model that uses only the Long-Context Branch (SSM over Cosmos-tokenized frame features + action embeddings) to predict future tokenized features and decode them to images; trained with MSE on token features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "State-Space World Model",
            "model_description": "SSM-only predictive model that encodes each full frame to a compact Cosmos CI16 token vector, concatenates action embeddings, processes the sequence with an SSM (e.g., Mamba or S4) to predict future token features ˆf_t, and decodes predicted tokens to images using the Cosmos decoder; trained with MSE on token features.",
            "model_type": "state-space latent world model",
            "task_domain": "visual world modeling (MiniGrid, CSGO)",
            "fidelity_metric": "PSNR and SSIM (MiniGrid); qualitative assessment (CSGO).",
            "fidelity_performance": "MiniGrid (context length 16): Avg PSNR 33.40, Final PSNR 33.17, SSIM 0.96 (Table 1). MiniGrid (context length 50): Avg PSNR 32.64, Final PSNR 32.44, SSIM 0.96.",
            "interpretability_assessment": "Moderate: the SSM maintains an explicit internal state h and output features m_t that can be directly inspected; however outputs decoded from tokens tend to be blurry and not semantically disentangled.",
            "interpretability_method": "Analysis via qualitative visualization of decoded frames and via ablation (comparing decoded predictions to ground truth); internal states are described but not mapped to semantics.",
            "computational_cost": "Computationally efficient relative to diffusion: SSM branch inference cost reported as 5.5 GFLOPS (context 16) and 16.741 GFLOPS (context 50). Training used fewer GPUs (State-Space World Model trained on 4 A100 GPUs for MiniGrid).",
            "efficiency_comparison": "Much cheaper than the diffusion generative branch; SSMs process long sequences in linear time and add negligible compute when fused with diffusion.",
            "task_performance": "Good at retaining and reproducing long-range content (higher long-horizon PSNR than DIAMOND), but generated images are lower-fidelity and blurrier than diffusion-based outputs, limiting visual quality.",
            "task_utility_analysis": "SSM-only is useful for long-range memory and for providing representations for planning, but its lower visual quality limits downstream uses where high-fidelity imagery is required; thus it serves well as a memory module but not as a standalone high-fidelity generator.",
            "tradeoffs_observed": "Memory and efficiency vs visual fidelity: SSMs retain long-term information and are cheap, but sacrifice fine visual detail; combining SSM with diffusion mitigates this trade-off.",
            "design_choices": "Use of Cosmos tokenizer to compress frames to tokens, concatenation with 16-d action embeddings, training with MSE on token features, choice of SSM backbone (Mamba preferred over S4).",
            "comparison_to_alternatives": "Compared to DIAMOND: higher long-term PSNR (better recall) but lower image fidelity; compared to hybrid: hybrid obtains both better memory utilization and higher visual fidelity.",
            "optimal_configuration": "Authors used Mamba as backbone for best performance; recommendation is to use SSM as memory backbone and decode via a stronger generative model for best overall outcomes.",
            "uuid": "e1393.2",
            "source_info": {
                "paper_title": "StateSpaceDiffuser: Bringing Long Context to Diffusion World Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Mamba",
            "name_full": "Mamba: Linear-time sequence modeling with selective state spaces",
            "brief_description": "A selective discrete state-space model that introduces dynamic selection in state updates to improve expressiveness while retaining linear-time sequence processing; used here as the Long-Context backbone.",
            "citation_title": "Mamba: Linear-time sequence modeling with selective state spaces.",
            "mention_or_use": "use",
            "model_name": "Mamba (SSM backbone)",
            "model_description": "Discrete state-space model with selective gating/dynamic selection to increase representational power while preserving linear scalability; integrated in this work to process Cosmos-tokenized full-frame features concatenated with action embeddings, producing memory features for conditioning the diffusion generator.",
            "model_type": "state-space model (SSM)",
            "task_domain": "long-sequence encoding for visual world modeling (MiniGrid and CSGO)",
            "fidelity_metric": "Indirect: evaluated by downstream PSNR/SSIM when used as backbone in State-Space World Model and StateSpaceDiffuser.",
            "fidelity_performance": "When used as backbone in State-Space World Model on MiniGrid (context 16): Avg PSNR 27.09, Final PSNR 27.87, SSIM 0.94 (Table 2 when comparing S4 vs Mamba as backbones; these numbers represent SSM-only performance in that backbone evaluation). In aggregate experiments, Mamba-based State-Space World Model achieved higher PSNRs (see other tables).",
            "interpretability_assessment": "SSM internals are structured (state h, linear update matrices A,B,C) providing some mechanistic interpretability compared to opaque recurrent nets, but no explicit interpretability analyses are reported here.",
            "interpretability_method": "None detailed in this paper beyond reporting improved downstream metrics and noting Mamba's dynamic selection mechanism as beneficial.",
            "computational_cost": "Designed for linear-time processing and efficient parallelism; in this work a single Mamba layer with state size 256 was used; adding Mamba branch cost measured as a few GFLOPS (5.5–16.7 GFLOPS depending on context length).",
            "efficiency_comparison": "Reported superior efficiency vs transformer/self-attention approaches for long sequences; empirically better performance than S4 in this paper's backbone comparison (Mamba outperforms S4 by ~9.1 PSNR on average in the evaluated setting).",
            "task_performance": "Improves long-context retention and downstream prediction fidelity when used as a backbone for world modeling; chosen by authors as the Long-Context backbone due to better empirical performance than alternatives.",
            "task_utility_analysis": "Provides most of the long-term memory needed to reinstantiate far-past visual content; when fused into diffusion, yields both memory and high visual fidelity in downstream predictions.",
            "tradeoffs_observed": "Mamba introduces dependency on input for state updates (beneficial for long-context performance) and increases expressiveness relative to simpler SSMs, without large compute cost.",
            "design_choices": "Single Mamba layer, state size 256, input expanded 4x with internal MLP in the Mamba layer as used in implementation details.",
            "comparison_to_alternatives": "Directly compared to S4 in the paper: Mamba achieved higher PSNR (27.09 vs 24.29 avg PSNR in the backbone evaluation on MiniGrid), leading to its selection.",
            "optimal_configuration": "Authors selected Mamba (single layer, state size 256) as the practical optimal backbone for balancing memory, expressiveness, and compute in their experiments; they note scaling the Long-Context Branch could further improve fine detail recall.",
            "uuid": "e1393.3",
            "source_info": {
                "paper_title": "StateSpaceDiffuser: Bringing Long Context to Diffusion World Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "S4",
            "name_full": "S4 (Structured State Space for Sequence modeling)",
            "brief_description": "A structured state-space model used as an alternative SSM backbone in experiments; simpler than Mamba but capable of long-range dependency modeling via parameterized structured operators.",
            "citation_title": "Efficiently modeling long sequences with structured state spaces.",
            "mention_or_use": "use",
            "model_name": "S4 (SSM backbone baseline)",
            "model_description": "Structured state-space model that uses carefully designed linear operators (HiPPO-related parameterizations) to capture long-range dependencies efficiently; used here for backbone comparison in the State-Space World Model.",
            "model_type": "state-space model (SSM)",
            "task_domain": "sequence modeling for visual world modeling (MiniGrid backbone comparison)",
            "fidelity_metric": "Downstream PSNR/SSIM when used as backbone in State-Space World Model.",
            "fidelity_performance": "Backbone evaluation (MiniGrid context length 16): Avg PSNR 24.29, Final PSNR 24.31, SSIM 0.91 (Table 2). Mamba outperformed S4 in the same evaluation.",
            "interpretability_assessment": "SSM internals are structured and mathematically interpretable to some extent (linear operators A,B,C), but no interpretability-specific analyses are provided in this paper.",
            "interpretability_method": "None applied in this paper beyond backbone performance comparison.",
            "computational_cost": "Designed for linear-time sequence processing; specific GFLOPS for S4 not stated but considered efficient; in this work Mamba was preferred for better performance.",
            "efficiency_comparison": "More efficient than transformer attention for long sequences; in this study Mamba offered better representational performance for similar efficiency, so Mamba was chosen.",
            "task_performance": "Capable of long-horizon retention but yielded lower downstream PSNR/SSIM than Mamba in authors' experiments.",
            "task_utility_analysis": "S4 provides a computationally efficient memory backbone but its representational capacity in this setup was outperformed by Mamba, limiting utility for highest-fidelity long-horizon reconstruction.",
            "tradeoffs_observed": "Lower PSNR (worse long-horizon recall) compared to Mamba, indicating a trade-off between SSM variant design and downstream fidelity.",
            "design_choices": "Evaluated as an off-the-shelf SSM backbone; authors used this comparison to justify choosing Mamba.",
            "comparison_to_alternatives": "Directly compared with Mamba; Mamba produced ~9.1 PSNR improvement over S4 in the backbone comparison for MiniGrid.",
            "optimal_configuration": "Authors concluded Mamba was preferable for their setting; S4 remains a valid SSM option but with lower performance in these tasks.",
            "uuid": "e1393.4",
            "source_info": {
                "paper_title": "StateSpaceDiffuser: Bringing Long Context to Diffusion World Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Cosmos tokenizer (CI16)",
            "name_full": "Cosmos tokenizer, continuous variant with scale 16 (CI16)",
            "brief_description": "A frame tokenizer used to compress high-resolution frames into compact continuous patch tokens which are flattened to a single feature vector per frame for SSM processing.",
            "citation_title": "Cosmos world foundation model platform for physical ai.",
            "mention_or_use": "use",
            "model_name": "Cosmos tokenizer (CI16)",
            "model_description": "A visual tokenizer that produces patch tokens (dimension 16) from images; tokens for each frame are flattened into a single high-dimensional feature vector (1296 for MiniGrid, 2448 for CSGO in this paper) that serves as input to the state-space branch; decoder is used to reconstruct images from predicted tokens.",
            "model_type": "visual tokenizer / encoder-decoder for latent representation",
            "task_domain": "compact frame encoding for long-sequence state-space modeling in visual world modeling",
            "fidelity_metric": "Used indirectly – downstream PSNR/SSIM measured on decoded images from token predictions; MSE used to train SSM on token space.",
            "fidelity_performance": "Not reported directly for tokenizer alone; used in State-Space World Model training where decoded outputs are blurrier than diffusion outputs but retain long-term content (SSM-only PSNR values reported above).",
            "interpretability_assessment": "Provides structured, discrete spatial patch tokens which are interpretable as compressed image patches; no explicit interpretability experiments reported.",
            "interpretability_method": "None reported beyond using decoded token visualization.",
            "computational_cost": "Tokenization/decoding costs included in Long-Context Branch compute; specific GFLOPS not broken out separately. Token dimensions: 1296 (MiniGrid), 2448 (CSGO); action embedding dimension 16.",
            "efficiency_comparison": "Enables SSM processing at the frame level (one feature vector per frame) rather than patch- or token-level SSM application, reducing sequence length and making long-context SSM processing tractable.",
            "task_performance": "Enables SSM to capture long-range scene content in a compressed form; when used with SSM-only models produces coherent but lower-fidelity reconstructions; when fused into diffusion supports high-fidelity generation with long memory.",
            "task_utility_analysis": "Serves as an efficient representation that balances compression and information retention enabling SSMs to process full frames over long contexts without prohibitive sequence lengths.",
            "tradeoffs_observed": "Tokenization reduces dimensionality enabling efficient SSM use, but decoded outputs from token predictions are blurrier compared to diffusion outputs; thus tokenizer is best used as memory feature provider rather than sole generator.",
            "design_choices": "Used continuous Cosmos CI16 tokens, flattened per-frame to form SSM inputs; action embeddings concatenated to tokens before SSM processing.",
            "comparison_to_alternatives": "Choosing frame-level tokenization (one vector per frame) is contrasted with patch-level SSM processing (prior work); authors select frame-level due to clearer scalability and integration with diffusion.",
            "uuid": "e1393.5",
            "source_info": {
                "paper_title": "StateSpaceDiffuser: Bringing Long Context to Diffusion World Models",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Diffusion for world modeling: Visual details matter in atari.",
            "rating": 2
        },
        {
            "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces.",
            "rating": 2
        },
        {
            "paper_title": "Efficiently modeling long sequences with structured state spaces.",
            "rating": 2
        },
        {
            "paper_title": "Facing off world model backbones: Rnns, transformers, and s4.",
            "rating": 2
        },
        {
            "paper_title": "Cosmos world foundation model platform for physical ai.",
            "rating": 2
        },
        {
            "paper_title": "S4WM (S4 World Model related works referenced as S4WM in text)",
            "rating": 1
        }
    ],
    "cost": 0.01714925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>StateSpaceDiffuser: Bringing Long Context to Diffusion World Models</h1>
<p>Nedko Savov ${ }^{1, \text {, }}$ Naser Kazemi ${ }^{1}$ Deheng Zhang ${ }^{1}$ Danda Pani Paudel ${ }^{1}$<br>Xi Wang ${ }^{1,2,3}$ Luc Van Gool ${ }^{1}$<br>${ }^{1}$ INSAIT, Sofia University "St. Kliment Ohridski" ${ }^{2}$ ETH Zurich ${ }^{3}$ TU Munich</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Recalling content long in the past. Given a history of images $I_{1}, \ldots, I_{T}$ and accompanying actions, we navigate all the way back to the beginning - $I_{1}$. The task is to generate frames along the way, consistent to what is seen in the history, given the actions. As an example, we show the predictions of the first frame - $I_{1}$. Can a generative model recall the content of $I_{1}$ long back in the sequence? Diffusion models fall short $(\boldsymbol{\lambda})$, our model correctly recalls the content of $I_{1}(\boldsymbol{\checkmark})$.</p>
<h2>Abstract</h2>
<p>World models have recently become promising tools for predicting realistic visuals based on actions in complex environments. However, their reliance on only a few recent observations leads them to lose track of the long-term context. Consequently, in just a few steps the generated scenes drift from what was previously observed, undermining the temporal coherence of the sequence. This limitation of the state-of-the-art world models, most of which rely on diffusion, comes from their lack of a lasting environment state.
To address this problem, we introduce StateSpaceDiffuser, where a diffusion model is enabled to perform long-context tasks by integrating features from a state-space model, representing the entire interaction history. This design restores long-term memory while preserving the high-fidelity synthesis of diffusion models.
To rigorously measure temporal consistency, we develop an evaluation protocol that probes a model's ability to reinstantiate seen content in extended rollouts. Comprehensive experiments show that StateSpaceDiffuser significantly outperforms a strong diffusion-only baseline, maintaining a coherent visual context for an order of magnitude more steps. It delivers consistent views in both a 2D maze navigation and a complex 3D environment. These results establish that bringing state-space representations into diffusion models is highly effective in demonstrating both visual details and long-term memory.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Preprint. Under review.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Our Approach. While diffusion models are limited to a short sequence input, our approach enables long-context processing for diffusion models with a state-space representation.</p>
<h1>1 Introduction</h1>
<p>World models have gained popularity for the production of visual consequences of given past observations and actions. These models can learn to generate environment observations entirely by training on many interactions with the environment. Simply by observation, they are capable of handling complex environments, such as car driving [27, 28, 39, 63], 3D virtual environments [73, 1, 19], platformer games [8, 64], ego-centric action videos [77], or navigation [2, 79]. They enable interactivity without the burden of hand-coding complex environments, but also offer feature representations for robotics and reinforcement learning agents for planning.
For long interactions with world models, it is essential that the generated video remains consistent with previously observed or generated content. Revisited areas should preserve their appearance, and objects observed again should keep their properties. However, as shown in Fig. 1, current high-fidelity world models, which are mostly based on diffusion, can only preserve context within a short time window, most often directly limited by their input window size [77, 1, 19, 8]. This leads to an increasing drift in content over time, where earlier information is forgotten or overwritten. The inability to retain persistent memory of the environment poses a major challenge, especially for real-world applications such as agent planning and virtual interaction, where coherent, temporally consistent environments are essential.
To improve content consistency in diffusion-based world models, we add a persistent long-context representation to generate the next observation. Specifically, we leverage features from a discrete state-space model (Mamba), which has been shown to effectively capture long-term context in prior work [20, 30]. We summarize our system in Fig. 2. While these models were previously applied to language and visually simple environments such as MiniGrid [12], we aim to preserve long-term context in modern diffusion-based world models, targeting environments with higher visual complexity such as CSGO [58]. In contrast, other state-based models like those using LSTMs or GRUs [36, 37, 38] have limited generative capacity and are mainly used for agent planning. Our approach combines the strong generative power of diffusion models with the long-term context tracking of state-space representations.</p>
<p>Our proposed model, StateSpaceDiffuser, summarized in Fig. 2, consists of a state-space model that operates over the long sequence, and a diffusion model - conditioned on both a short window of observations and state-space model features. The latter enables the diffusion branch to generate the content of the next frame conditioned on a long context rather than the last few frames. In addition, since state-space models are computationally efficient [29], the computational cost added by the state-space model is minimal.
To evaluate the consistency of the generated long context of StateSpaceDiffuser, we design and develop an evaluation framework that involves navigating environments to then return back to the initial position. We evaluate on two environments. (1) A simpler 2D maze environment (MiniGrid), in which we establish the presence or absence of memory ability by remembering the maze layout given partial observations. And (2) a 3D first-person shooter game (CSGO), which serves to show the performance of our method on a visually challenging interactive environment with many factors at play. Our quantitative and qualitative evaluation results show that StateSpaceDiffuser produces content significantly more consistent with a long history than a diffusion-only method. Evaluation in the maze environment yields $51.9 \%$ PSNR improvement over the baseline on average ( $56.3 \%$ improvement on the most memory challenging cases). A user study confirms that our method produces images closer to previously observed content in the CSGO dataset compared to baselines. More details are shown in Sec.5.</p>
<p>Our contributions are as follows.</p>
<ul>
<li>We propose the StateSpaceDiffuser model that effectively brings a state-space representation into a diffusion model for visual world modeling. It is capable of generating consistent content in long-horizon generation, with almost no extra computational cost.</li>
<li>We develop an evaluation protocol to test the content preservation abilities of a world model and use it to perform extensive evaluations of world models on long-horizon generation tasks.</li>
<li>Our evaluation shows a significant improvement over the baseline in the case of long contexts and a strong user preference in our favor on long-horizon consistent prediction. Furthermore, an ablation study shows that our state-space representation enhances the content consistency of our diffusion framework by a large margin.</li>
</ul>
<h1>2 Related Work</h1>
<h3>2.1 World Models</h3>
<p>Generative environment models. Initially developed as imagination-based models for training model-based reinforcement learning (MBRL) agents [13, 36, 38, 66], world models have evolved into powerful generative systems that condition on actions to produce future frames [11, 39, 55, 78]. Early work by [34] demonstrates that training a recurrent latent dynamics on VAE image representations can enable agents to plan in imaginative rollouts. Extensions such as SimPle [45] and Dreamer [35] refine this approach by improving reconstruction quality and stability, culminating in DreamerV2 and DreamerV3 [37, 38] - systems that achieve human-level performance on Atari and demonstrate the ability to generalize across diverse domains. More recent efforts, such as IRIS [56], TWM [62], STORM [81], and DayDreamer [75], employ Transformer-based on hybrid backbones and focus on sample efficiency, long-horizon coherence, or robotic control. However, many of these methods rely on discrete latent tokens and relatively short contexts, which limits visual fidelity in complex scene motion or when extended rollouts are required.
World models are also central to realistic video generation conditioned on actions. Genie [8] leverages a video tokenizer and a Latent Action Model for dynamic next-frame generation, whereas GAIA-1 [44], GAIA-2 [63] tackle autonomous driving by autoregressively predicting image tokens from multi-modal inputs. Recent works highlight broader applicability and complex generative capabilities. DINO-WM [82] uses pretrained visual features for zero-shot planning, while GameFactory [79] adapts game environment actions to realistic environments. Both illustrate how world models can transcend traditional RL frameworks and support open-ended content creation.
Diffusion-based approaches. Parallel to these developments, diffusion models [69, 42, 71] have emerged as a powerful class of generative methods for high-fidelity image and video synthesis. They have been applied to text-to-video [67], space-time video generation [3], and broad world simulation tasks [6]. Within MBRL, DIAMOND [1] uses a diffusion model to generate high-quality frames for Atari, showing that improved reconstruction detail can enhance agent performance. Nonetheless, current diffusion-based world models often condition on only a limited window of past frames, making it challenging to maintain long-horizon dependencies.</p>
<h3>2.2 Sequence Modeling</h3>
<p>RNNs and Transformers. Sequential modeling has historically been dominated by recurrent neural networks (RNN) such as LSTM and GRU [43, 15, 17], which process input tokens step by step and are able to handle moderate contexts. However, RNNs often struggle with extremely long sequences due to vanishing gradients and limited memory capacity [57]. Transformers [74] addressed these issues by employing self-attention, making them effective in capturing long-range dependencies. Beyond world modeling, Transformers have become the backbone for a broad range of tasks, including language modeling [21, 7, 61] and computer vision [22, 41, 9], due to their ability to handle global context. Various Transformer variants have attempted to reduce the quadratic cost of self-attention for long sequences [48, 16, 4, 80, 14]. Vision-specific models like Swin [54] or MViT [24] adopt hierarchical or local attention, yet scaling them to long video horizons remains computationally prohibitive.</p>
<p>Previously, DFoT [10] addressed the ability for long future prediction. However, the long-context consistency problem has only been recently addressed by a few concurrent works. [49, 70, 76] improve context abilities by proposing strategies to sample a number of historical observations to use as conditioning. Instead, our approach involves summarizing information from the entire history automatically through state-space models.</p>
<p>State-Space Models (SSMs). As an alternative, SSMs [5, 51, 60, 72, 59] can process sequences in linear time by learning continuous dynamics in a latent state. Representative structured state-space models include S4 [32, 33] and H3 [18] that generalizes the recurrence in Linear Attention [47]. S4, S5 [68], and S6 [50] leverage carefully designed operators (e.g., HiPPO matrices [31]) to efficiently capture long-range dependencies. Mamba [30] introduces selective gating to improve expressiveness without sacrificing linear scalability. In the context of world modeling, applying SSMs (e.g. S4WM [20]) has shown promise for maintaining coherence over hundreds of imagined steps while preserving computational tractability.</p>
<p>Hybrid Architectures. As Transformers excel at local interaction with low computational cost and SSMs can capture long-horizon dependencies efficiently, hybrid designs have been proposed for vision tasks. MambaVison [40] incorporates state-space models into a transformer, and Dimba [26], DiS [25] - into a diffusion network backbone, for computationally cheaper image discriminative and generative tasks, operating on image patches.[52] modify the softmax in attention to emulate a forget gate and improve transformer context abilities. MambaVLT [53] and Samba [65] exploit state-space models for better object tracking with long-range consistency.</p>
<h1>3 Data</h1>
<p>We design an experimental protocol to evaluate long-term content consistency in diffusion world models, comprising of three experimental setups with a rising level of complexity, based on a controlled maze environment (MiniGrid) and a complex 3D first-person environment (CSGO).</p>
<p>We create a dataset based on the partially observed MiniGrid maze environment [12]. In this setup, each maze consists of a grid where each cell can be a wall, an empty space, or a colored marker. Markers act like empty spaces, but are visually distinct. An agent navigates the maze, but at each time step, it only sees a $7 \times 7$ window centered around itself rather than the full $85 \times 85$ maze (see Fig. 6 (b) for an example). We use a modified version of MiniGrid with randomly generated mazes, allowing us to adjust the size, wall complexity, and number of color-coded markers. To generate episodes, the agent is tasked with visiting around 40 markers in sequence. Once halfway through the episode, the agent retraces its path back to the starting point. Each episode is 100 steps long ( 50 forward, 50 backwards). We evaluate on different context lengths by selecting subsequences around the long sequence center. Notably, the second half of each sequence depends heavily on the model's ability to recall earlier frames, making it ideal for testing long-context reasoning.</p>
<p>We also design a simplified dataset called MiniGrid Simple, consisting of just 34 samples without walls and a single marker placed behind the starting position. The agent moves three steps forward and three steps back, returning to its initial position. Since the context window of our baseline is just 4 steps, this setup provides a minimal but effective test of long-term recall. We use this to compare the performance of our baseline and state-space-enhanced models in reconstructing the marker color.</p>
<p>To evaluate our method in a more visually complex setting, we use CSGO [58], a data set of human gameplay in a 3D first-person shooter. It includes 51 action types, such as 23 rotational commands, 4 movement directions, jumping, and various special actions (e.g., firing, changing weapons). To adapt the dataset for long-context testing, we create mirrored sequences: for each original sequence, we append its reversed version, ensuring that actions are also reversed. We use a corresponding one-hot encoding (e.g. turning left becomes turning right), or create a new one if a correspondence does not exist (e.g. jump, shoot). This setup forces the model to rely on information from earlier in the sequence when generating later frames.</p>
<h2>4 Methodology</h2>
<p>Given a sequence of environment interactions $a_{1}, a_{2}, \ldots, a_{T-1}$, the resulting observations $I_{1}, I_{2}, \ldots, I_{T}$ (with initial frame $I_{1}$ ) and the current action $a_{T}$, the objective of a world model $\mathcal{F}(\cdot)$ is to produce the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Architecture of our StateSpaceDiffuser model. It consists of: a state-space model for processing long context information; a diffusion model generating high-fidelity context-aware next observation, conditioned on state-space features.</p>
<p>Next image $I_{T+1}=\mathcal{F}([I_{1},...,I_{T}],[a_{1},...,a_{T}])$. Recent works use advanced generative architectures for modeling $\mathcal{F}(\cdot)$, based on transformers and diffusion. As they are computationally intensive, they take a short history window of observations: $I_{T+1}=\mathcal{F}([I_{T-K+1},...,I_{T}],[a_{T-K+1},...,a_{T}])$. $(K=4[1, 73], K=16[8])$. As those solutions are problematic for a long sequence $K$, we look into extending the window for such models without significant computational costs.</p>
<p>To meet these requirements, we make the decision to maintain a state as part of a state-space model to provide us with long context features. After integrating those features into diffusion, we obtain our proposed model - StateSpaceDiffuser.</p>
<h3>4.1 StateSpaceDiffuser Architecture</h3>
<p>We show our architecture in Fig. 3. We conceptually divide our architecture into two branches: Long-Context Branch and Generative Branch. The Long Context Branch is responsible for processing a long-context sequence efficiently; The Generative Branch has the sole goal of generating high-fidelity visuals, enhanced by the features of the Long-Context Branch.</p>
<h4>Long-Context Branch</h4>
<p>In contrast to transformer and diffusion architectures, state-space models (SSMs) are designed specifically to efficiently process long sequences. While SSMs are generally designed for continuous input signals, discrete SSMs maintain an internal state representation $h$ that is updated with each time step $t$ in an input sequence of one-dimensional feature vectors $f_{1},...,f_{T}$. ., through the parameter matrices $A$, $B$ and $C$, which are learned in training time:</p>
<p>$$h_t = Ah_{t-1} + Bf_t, \quad m_t = Ch_t$$</p>
<p>We denote a state-space model with $m_{1},...,m_T = \mathcal{M}(f_1, ..., f_T)$, with $m_t$ denoting the model's output. To bring it into the world model setting, we define $f_t$ to be a compact feature representation of $I_t$ and train a model that predicts future observations: $f_2, ..., f_{T+1} = \mathcal{M}([f_1, a_1], ..., [f_{T}, a_T])$. Rather than applying SSMs at the patch or image token level, as in previous work [40, 26], we temporally process full frames by first encoding all frames into a compact feature. For this, we use</p>
<p>the Cosmos tokenizer [23], specifically its continuous variant with scale 16 (CI16). The resulting patch tokens (dimension 16) are flattened to form a single feature vector $f_{t}$ per image. Alongside this, we incorporate a discrete action $a_{t}$, which indexes a learnable embedding of dimension 16. We concatenate $f_{t}$ and $a_{t}$ to create the input at each step. For our more advanced models, we adopt the Mamba architecture due to its dynamic selection mechanism and efficient parallelism, which led to superior performance in preliminary experiments compared to other SSM variants.</p>
<p>We give a long input sequence of high-resolution images and corresponding actions tested with up to 50, and produce the features corresponding to the next observation. In those features, we expect relevant to the time-step information, recalled from earlier in the sequence.</p>
<p>Generative Branch. It consists of a diffusion model that already exhibits strong generative properties. We build our Generative Branch on the DIAMOND world model [1], a UNet-based EDM diffusion model [46] designed for visual prediction in sequential environments. Therefore, our Generative branch conditions on only four low-resolution frames and their corresponding actions, represented as 512-dimensional action embeddings. Despite this minimal context, it can produce highquality predictions with just three denoising iterations per output frame. The architecture consists of two diffusion models: a primary model that predicts the next observation at very low resolution, and a secondary upsampler that refines these predictions to a resolution of $280 \times 150$. As the model predicts one frame at a time, generating longer sequences is achieved through a sliding-window strategy, each newly generated frame is appended to the input history for the next prediction. In isolation, this strategy causes a short-context limitation to this model.</p>
<p>Fusion of features. To address the context limitation of the Generative Branch, we build a fusion module to integrate state-space features into it, in order to provide long-context information. To that end, we process the entire sequence with the Long-Context branch and obtain the last 4 output features $\hat{f}_{t}$. We integrate these features to the diffusion conditions. fuse those features to the corresponding action embeddings. These features are first normalized and then passed through a two-layer MLP with SiLU activation, where the input size matches the feature dimensions. Similarly, the action embeddings-perturbed with noise are processed by an MLP with the same architecture. To form the final conditioning vector, we concatenate the outputs of the two MLPs. Empirically, we discovered that processing the memory and action conditions independently before concatenation yielded better performance than fusing them earlier in the pipeline.</p>
<h1>4.2 Training Protocol</h1>
<p>We train models for each dataset on context length 50 or 16 . We first train the Long-Context Branch, which we discovered to be an essential step. Then we load the pre-trained weights and train the Generative Branch to obtain the final model. For each training, we obtain a batch of sequences of executed actions and their reverse. We train with input size of 4 images and actions and sequential steps, corresponding to the full training episode sequence size. Training details are given in Sup.Mat.</p>
<h2>5 Experiments</h2>
<h3>5.1 Experimental Setup</h3>
<p>Baselines. We establish two baselines. The first is a pure diffusion model without state-space features: the DIAMOND model. To investigate the advantages and limitations of incorporating state-space representations, we introduce a second baseline: the State-Space World Model. This model builds on the Long-Context branch of StateSpaceDiffuser and is trained using mean squared error (MSE) between the predicted tokenized features $\hat{f}<em t="t">{t}$ and the ground truth features $f</em>}$. At inference time, $\hat{f<em t="t">{t}$ is decoded into an image $I</em>$ using the decoder from the Cosmos tokenizer.</p>
<p>This model enables us to assess the memory capacity of state-space models (SSMs) in sequential visual prediction. Although its outputs tend to be blurry and contain artifacts in complex scenes, due to the absence of a variational component and limited generative expressiveness compared to modern diffusion models, the SSM exhibits a strong ability to model long sequences and retain information from earlier in the trajectory. The strengths and shortcomings observed in this baseline directly inform and motivate the design of StateSpaceDiffuser.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Avg. PSNR $\uparrow$</th>
<th style="text-align: center;">Fin. PSNR $\uparrow$</th>
<th style="text-align: center;">SSIM $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Context Length 16</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">DIAMOND</td>
<td style="text-align: center;">27.13</td>
<td style="text-align: center;">25.44</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr>
<td style="text-align: left;">State-Space World Model</td>
<td style="text-align: center;">33.40</td>
<td style="text-align: center;">33.17</td>
<td style="text-align: center;">0.96</td>
</tr>
<tr>
<td style="text-align: left;">StateSpaceDiffuser (Ours, w/o state)</td>
<td style="text-align: center;">23.68</td>
<td style="text-align: center;">20.95</td>
<td style="text-align: center;">0.92</td>
</tr>
<tr>
<td style="text-align: left;">StateSpaceDiffuser (Ours)</td>
<td style="text-align: center;">$\mathbf{4 1 . 0 1}$</td>
<td style="text-align: center;">$\mathbf{4 0 . 5 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Context Length 50</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">DIAMOND</td>
<td style="text-align: center;">26.13</td>
<td style="text-align: center;">25.15</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr>
<td style="text-align: left;">State-Space World Model</td>
<td style="text-align: center;">32.64</td>
<td style="text-align: center;">32.44</td>
<td style="text-align: center;">0.96</td>
</tr>
<tr>
<td style="text-align: left;">StateSpaceDiffuser (Ours)</td>
<td style="text-align: center;">$\mathbf{3 9 . 6 8}$</td>
<td style="text-align: center;">$\mathbf{3 9 . 3 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 8}$</td>
</tr>
</tbody>
</table>
<p>Table 1: MiniGrid Quantitative Evaluation of Long-Context Awareness. Our StateSpaceDiffuser outperforms the baselines.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: CSGO User study results.</p>
<p>Testing Protocol. Our evaluation protocol matches our mirrored action setups for both of our datasets - we take $n$ actions and $n$ reverse actions and expect to generate the same observations for the second half of the sequence as seen in the first. On MiniGrid, we generate 1 frame in the future at a time, while on CSGO we sequentially generate the entire second half sequence and use visual fidelity metrics. On MiniGrid we evaluate the produced content with the visual fidelity metrics PSNR and SSIM on different future horizons of generated observations - the further in the sequence, the longer the memory required. In CSGO we perform a user study, more aligned to the visual complexity of the environment.</p>
<h1>5.2 Results and Analysis</h1>
<p>Simple MiniGrid Evaluation. In this experiment, we demonstrate on a very simple setup, as described in Sect. 3, the recall ability of the baseline, the State-Space World Model and StateSpaceDiffuser. We train and test on the same set of 34 samples, allowing for overfitting. The goal is to recall a color at the final frame from the first frame in the sequence with a length of 7 frames. Two random samples (colors) from the results are shown in Fig. 5, with the corresponding model predictions. With input size 4, the baseline processes the sequence in a sliding window fashion and, as within the 3 steps the color information is lost, it cannot reconstruct the correct color. In contrast, our State-Space World Model, based on a computationally efficient state-space model, is able to predict the correct color. Finally, it is demonstrated that our StateSpaceDiffuser Model is also able to recall the correct content by effectively combining both paradigms. Notably, our methods perform equivalently on a context length of 50 frames - when predicting the 51st, StateSpaceDiffuser recalls the color from 50 steps back (more in Sup.Mat).</p>
<p>Forward-Backward Evaluation on MiniGrid. We compare the long context abilities of our diffusion (DIAMOND) and state-space model (State-Space World Model) baselines in our MiniGrid test
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Long-Context Simple Demonstration.An agent starts moving to the right covering the color for the next T/2 frames, then the agent moves back the same amounts of steps. Diffusion baseline fails to recover the color, StateSpaceDiffuser and State-Space World Model successfully recover it through the state-space representation. Both $\mathrm{T}=7$ and $\mathrm{T}=50$ generates such results.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Qualitative comparison of StateSpaceDiffuser and the baseline (DIAMOND).
set. We evaluate our models trained on context length 50 on context lengths 16 and 50 (demonstrating generalizability). We follow the protocol outlined in Sect. 5.1. To quantify performance, we compute the Peak Signal-to-Noise Ratio (PSNR) for each predicted frame in the reverse trajectory, reporting both the mean score and the PSNR at the final time step, which requires the longest-term memory. As shown in the Tab. 1, our model significantly outperforms both baselines, particularly at the end of the sequence, where successful recall of the first frame is critical. This highlights the model's ability to retain and reinstantiate long-term visual context. Compared to the State-Space World Model, our method achieves higher fidelity output, benefiting from the superior generative capacity of the Generative Branch (qualitative examples provided in the Supplementary Material). Fig. 6 (b) presents example rollouts generated by our model and the diffusion-only baseline. In MiniGrid, predictions are made one step at a time using the ground truth sequence, as a result, most content is carried over from the previous frame, with only the newly revealed area requiring inference. Our method excels at filling in these newly revealed regions, even when the relevant context originates far back in the sequence. In contrast, the diffusion baseline struggles to recover such long-range dependencies.(more in Sup.Mat.)</p>
<p>Recall Across a Context Length. In this experiment we study the accuracy of our models over the varying context length of the forward-backward evaluation on MiniGrid. When predicting future observations, the last frame's content depends on the first frame's content, and the further back we go in the sequence the smaller the context length required for a good reconstruction. This is a direct consequence of the mirror style of the observations in the sequences. In Fig. 7 we show the PSNR at each predicted time step. The first few predicted frames are easily predicted by all models as the solution falls within the short input window. However, performance for the diffusion baseline quickly falls as no form of information is preserved from the long context, while a state-space model is able
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Recall performance on MiniGrid for two context lengths.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Backbone</th>
<th style="text-align: center;">Avg. PSNR $\uparrow$</th>
<th style="text-align: center;">Fin. PSNR $\uparrow$</th>
<th style="text-align: center;">SSIM $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Context Length 16</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">S4 [32]</td>
<td style="text-align: center;">24.29</td>
<td style="text-align: center;">24.31</td>
<td style="text-align: center;">0.91</td>
</tr>
<tr>
<td style="text-align: center;">Mamba [29]</td>
<td style="text-align: center;">27.09</td>
<td style="text-align: center;">27.87</td>
<td style="text-align: center;">0.94</td>
</tr>
</tbody>
</table>
<p>Table 2: Evaluating State-Space Backbones. The superior performance of Mamba leads to our choice to use this method as our Long-Context backbone.
to harvest this information. Our StateSpaceDiffuser Model gets the best of both worlds - long-context awareness and high-fidelity predicted images, and performs the best.</p>
<p>Forward-Backward Imagination Evaluation on CSGO. Similarly to MiniGrid, we evaluated the recall abilities of our model on the CSGO dataset, a visually complex environment in a 3D world. In CSGO most actions are gradually executed over a sequence, and there is a compounding effect on content change (e.g. jump unfolds over many frames). For a high impact evaluation we decide to give only the first half of the sequence and continuously produce the second half (reverse) by feeding generated frames. As actions can be executed at varying levels, the final frame may contain the correct content memorized but with low PSNR, as the camera position and scene geometry might be shifted. Therefore, instead of fidelity metrics we perform a user study where the 12 participants judge whether images produced by StateSpaceDiffuser are closer in content to the ground truth compared to the diffusion baseline. Our rating is in the range ${-1,1}$, with 0 being borderline, -1 - preference toward the baseline, 1 - preference for StateSpaceDiffuser. The results shown in Fig. 4 (details - in Sup.Mat) demonstrate a clear preference of the users for StateSpaceDiffuser over the baseline for both prediction in the 15th frame (rating $\mathbf{0 . 2 0}$ ) and 17th (last) frame (rating $\mathbf{0 . 2 4}$ ). Fig. 6 (a) shows a sample of CSGO imagination in different time steps, demonstrating that while the baseline fails to recall the correct content, the StateSpaceDiffuser correctly produces the details.</p>
<p>State Features Ablation. We study the usefulness of the state-space features provided to the Generative Branch in our StateSpaceDiffuser model. We take a trained model and perform a MiniGrid evaluation by replacing the output features of the Long-Context Branch with zeros before passing them to the Generative Branch. In Tab. 1 we show that this causes the performance to quickly drop even below baseline performance. This clearly demonstrates that the features are highly utilized. In Sup.Mat. we show visuals demonstrating the same effect on CSGO results.</p>
<p>State-Space Model Choice. We consider a few strong state-space models for use in our LongContext Branch - S4 and Mamba. We evaluate them by training State-Space World Model with each of them on MiniGrid using a context length of 16 and comparing their performance. Tab. 2 shows the results, establishing Mamba as a better performing backbone, achieving a 9.1 PSNR improvement on average over S4. As Mamba introduces a dependency on the input of the state update, this clearly benefits its ability to perform in long context.</p>
<h1>6 Conclusion</h1>
<p>We introduced StateSpaceDiffuser, a hybrid model that combines state-space representations with diffusion to enable long-horizon visual world modeling. By decoupling global context modeling (via a state-space backbone) from high-fidelity synthesis (via diffusion), our model retains global context over many steps at essentially no additional computational cost. The resulting representation alleviates the drift and inconsistency that plague conventional diffusion-only systems in long sequences.
Experiments on MiniGrid and CSGO validate our method's consistency and fidelity across long sequences. In the forward-backward protocol with horizon 50, StateSpaceDiffuser improves average PSNR by $\mathbf{5 1 . 9 \%}$ over the diffusion baseline and achieves a final-frame PSNR of $\mathbf{3 9 . 3 2}$ versus $\mathbf{2 5 . 1 4}$ for DIAMOND on a long context length of 50 frames. Human raters also favor our generations for long-context consistency (Fig. 4).
Our results establish state-space diffusion as a scalable and consistent solution for long-context visual generation. We believe that bridging state-space reasoning with diffusion generation is a promising direction for robust, long-horizon world modeling, and we hope this work lays a solid foundation for future research in temporally coherent visual prediction.</p>
<h1>7 Acknowledgments</h1>
<p>This research was partially funded by the Ministry of Education and Science of Bulgaria (support for INSAIT, part of the Bulgarian National Roadmap for Research Infrastructure). This project was supported with computational resources provided by Google Cloud Platform (GCP).</p>
<h2>References</h2>
<p>[1] Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos J Storkey, Tim Pearce, and François Fleuret. Diffusion for world modeling: Visual details matter in atari. Advances in Neural Information Processing Systems, 37:58757-58791, 2025.
[2] Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. Navigation world models. arXiv preprint arXiv:2412.03572, 2024.
[3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: A space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 1-11, 2024.
[4] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.
[5] James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent neural networks. In International Conference on Learning Representations, 2017.
[6] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024.
[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[8] Jake Bruce, Michael D Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024.
[9] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650-9660, 2021.
[10] Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:24081-24125, 2024.
[11] Chang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. Transdreamer: Reinforcement learning with transformer world models. arXiv preprint arXiv:2202.09481, 2022.
[12] Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid \&amp; miniworld: Modular \&amp; customizable reinforcement learning environments for goal-oriented tasks. CoRR, abs/2306.13831, 2023.
[13] Silvia Chiappa, Sébastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent environment simulators. arXiv preprint arXiv:1704.02254, 2017.
[14] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.
[15] Kyunghyun Cho, B van Merrienboer, Caglar Gulcehre, F Bougares, H Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), 2014.
[16] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In International Conference on Learning Representations, 2020.</p>
<p>[17] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. In NIPS 2014 Workshop on Deep Learning, December 2014, 2014.
[18] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Ré. Hungry hungry hippos: Towards language modeling with state space models. In Proceedings of the 11th International Conference on Learning Representations (ICLR), 2023.
[19] Etched Decart. Oasis world model. https://oasis-model.github.io/. Accessed: October 31, 2024.
[20] Fei Deng, Junyeong Park, and Sungjin Ahn. Facing off world model backbones: Rnns, transformers, and s4. Advances in Neural Information Processing Systems, 36, 2024.
[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 4171-4186, 2019.
[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, G Heigold, S Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.
[23] NVIDIA et. al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025.
[24] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 6804-6815. IEEE, 2021.
[25] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024.
[26] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, Youqiang Zhang, and Junshi Huang. Dimba: Transformer-mamba diffusion models, 2024.
[27] Ruili Feng, Han Zhang, Zhantao Yang, Jie Xiao, Zhilei Shu, Zhiheng Liu, Andy Zheng, Yukun Huang, Yu Liu, and Hongyang Zhang. The matrix: Infinite-horizon world generation with real-time moving control. arXiv preprint arXiv:2412.03568, 2024.
[28] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: A generalizable driving world model with high fidelity and versatile controllability. arXiv preprint arXiv:2405.17398, 2024.
[29] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.
[30] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling, 2024.
[31] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33:1474-1487, 2020.
[32] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021.
[33] Albert Gu, Ankit Gupta, Karan Goel, and Christopher Ré. On the parameterization and initialization of diagonal state space models, 2022.
[34] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.
[35] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020.
[36] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International conference on machine learning, pages 2555-2565. PMLR, 2019.</p>
<p>[37] Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In International Conference on Learning Representations, 2021.
[38] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023.
[39] Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Pedro Rezende, Yasaman Haghighi, David Brüggemann, Isinsu Katircioglu, Lin Zhang, Xiaoran Chen, Suman Saha, et al. Gem: A generalizable ego-vision multimodal world model for fine-grained ego-motion, object dynamics, and scene composition control. arXiv preprint arXiv:2412.11198, 2024.
[40] Ali Hatamizadeh and Jan Kautz. Mambavision: A hybrid mamba-transformer vision backbone, 2024.
[41] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000-16009, 2022.
[42] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020.
[43] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.
[44] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: A generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023.
[45] Łukasz Kaiser, Mohammad Babaeizadeh, Piotr Miłos, Błażej Osiński, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model based reinforcement learning for atari. In 8th International Conference on Learning Representations, ICLR 2020, 2020.
[46] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:26565-26577, 2022.
[47] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 5156-5165. PMLR, 2020.
[48] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020.
[49] Soonwoo Kwon, Jin-Young Kim, Hyojun Go, and Kyungjune Baek. Toward stable world models: Measuring and addressing world instability in generative environments. arXiv preprint arXiv:2503.08122, 2025.
[50] Sangyoun Lee, Juho Jung, Changdae Oh, and Sunghee Yun. Enhancing temporal action localization: Advanced s6 modeling with recurrent mechanism. arXiv preprint arXiv:2407.13078, 2024.
[51] Tao Lei, Yu Zhang, Sida I Wang, Hui Dai, and Yoav Artzi. Simple recurrent units for highly parallelizable recurrence. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2018.
[52] Zhixuan Lin, Evgenii Nikishin, Xu Owen He, and Aaron Courville. Forgetting transformer: Softmax attention with a forget gate. arXiv preprint arXiv:2503.02130, 2025.
[53] Xinqi Liu, Li Zhou, Zikun Zhou, Jianqiu Chen, and Zhenyu He. Mambavlt: Time-evolving multimodal state space model for vision-language tracking. arXiv preprint arXiv:2411.15459, 2024.
[54] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012-10022, 2021.
[55] Taiming Lu, Tianmin Shu, Alan Yuille, Daniel Khashabi, and Jieneng Chen. Generative world explorer. arXiv preprint arXiv:2411.11844, 2024.
[56] Vincent Micheli, Eloi Alonso, and François Fleuret. Transformers are sample-efficient world models. In The Eleventh International Conference on Learning Representations, 2023.
[57] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013.</p>
<p>[58] Tim Pearce and Jun Zhu. Counter-strike deathmatch with large-scale behavioural cloning. In 2022 IEEE Conference on Games (CoG), pages 104-111. IEEE, 2022.
[59] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. Findings of the Association for Computational Linguistics: EMNLP 2023, 2023.
[60] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pages 28043-28078. PMLR, 2023.
[61] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67, 2020.
[62] Jan Robine, Marc Höftmann, Tobias Uelwer, and Stefan Harmeling. Transformer-based world models are happy with 100k interactions. In The Eleventh International Conference on Learning Representations, 2023.
[63] Lloyd Russell, Anthony Hu, Lorenzo Bertoni, George Fedoseev, Jamie Shotton, Elahe Arani, and Gianluca Corrado. Gaia-2: A controllable multi-view generative world model for autonomous driving. arXiv preprint arXiv:2503.20523, 2025.
[64] Nedko Savov, Naser Kazemi, Mohammad Mahdi, Danda Pani Paudel, Xi Wang, and Luc Van Gool. Exploration-driven generative interactive environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025.
[65] Mattia Segu, Luigi Piccinelli, Siyuan Li, Yung-Hsu Yang, Bernt Schiele, and Luc Van Gool. Samba: Synchronized set-of-sequences modeling for multiple object tracking. arXiv preprint arXiv:2410.01806, 2024.
[66] Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. Planning to explore via self-supervised world models. In International conference on machine learning, pages 8583-8592. PMLR, 2020.
[67] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. In The Eleventh International Conference on Learning Representations.
[68] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. Simplified state space layers for sequence modeling. In $I C L R, 2023$.
[69] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages $2256-2265$. pmlr, 2015.
[70] Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, and Vincent Sitzmann. Historyguided video diffusion. arXiv preprint arXiv:2502.06764, 2025.
[71] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.
[72] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023.
[73] Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024.
[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
[75] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Daydreamer: World models for physical robot learning. In Conference on robot learning, pages 2226-2240. PMLR, 2023.
[76] Zeqi Xiao, Yushi Lan, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. Worldmem: Long-term consistent world simulation with memory. arXiv preprint arXiv:2504.12369, 2025.</p>
<p>[77] Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 2023.
[78] Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, and Dale Schuurmans. Video as the new language for real-world decision making. arXiv preprint arXiv:2402.17139, 2024.
[79] Jiwen Yu, Yiran Qin, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Gamefactory: Creating new games with generative interactive videos. arXiv preprint arXiv:2501.08325, 2025.
[80] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283-17297, 2020.
[81] Weipu Zhang, Gang Wang, Jian Sun, Yetian Yuan, and Gao Huang. Storm: Efficient stochastic transformer based world models for reinforcement learning. Advances in Neural Information Processing Systems, 36:27147-27166, 2023.
[82] Gaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto. Dino-wm: World models on pre-trained visual features enable zero-shot planning, 2025.</p>
<h1>StateSpaceDiffuser: Bringing Long Context to Diffusion World Models</h1>
<h2>Supplementary Materials</h2>
<h2>Table of Contents</h2>
<p>A Training Details ..... 15
B MiniGrid Evaluations ..... 16
B. 1 Generalization Across Context Length on MiniGrid ..... 16
B. 2 MiniGrid Qualitative Evaluation ..... 16
B. 3 Imagination Qualitative Results ..... 17
C CSGO Evaluations ..... 20
C. 1 Qualitative Evaluation ..... 20
C. 2 Strengths and Limitations of StateSpaceDiffuser ..... 20
C. 3 User Study Details ..... 22
C. 4 Qualitative Examples of StateSpaceDiffuser on Context Length 50 ..... 23
C. 5 State Features Ablation for CSGO ..... 23
C. 6 Comparison with State-Space World Model ..... 23</p>
<h2>A Training Details</h2>
<p>For MiniGrid, our DIAMOND baseline takes 30x30 images and upscales them to 144x144, for CSGO - 30x56, upscaled to 150x280. The Long-Context Branch contains the Cosmos tokenizer, which decodes in powers of two. Therefore, The Long Context Branch takes 144x144 for MiniGrid and 144x272 for CSGO. In training, the downscaling is done from the high-resolution image with bicubic interpolation (as in DIAMOND).</p>
<p>In our Long-Context Branch, the Cosmos Tokenizer tokens are flattened to produce features of size 1296 (MiniGrid) or 2448 (CSGO). Action dimensions in the Long Context Branch (and the State-Space World Model) is 16 - they are concatenated to the visual features. We use a single Mamba layer with state size 256, the input is expanded 4 times with an MLP inside the Mamba layer, the internal convolution dimensions are 4.</p>
<p>Our Long Context Branch adds very little computation to the diffusion model, and in exchange it offers significant improvements in consistent generation. In inference with batch size of 1, we measure DIAMOND to require 909.515 GFLOPS (4 input frames). The Long Context Branch with context length 16, requires only 5.5 GFLOPS for context length 16 and 16.741 GFLOPS for context length 50 .</p>
<p>We use 8 A100 GPUs for all models except for the MiniGrid State-Space World Model, which was trained on 4 A100 GPUs for MiniGrid models. All models use the Adam optimizer. The State-Space World Model is trained with a learning rate of $5 e^{-5}$ and batch size 136 (MiniGrid) or batch size 272 (CSGO). Both the MiniGrid and CSGO models are trained for 70k iterations on sequence size 16. StateSpaceDiffuser includes 600 M parameters, and is trained with a learning rate $1 e^{-4}$, weight decay $1 e^{-2}$, grad norm clip 10 and batch size 64 . The MiniGrid model is trained for 77 k iterations, CSGO - 220k iterations. For upscaling in MiniGrid - we train the sampler for 27 k iterations after training the denoiser for predicting low-resolution next frame. In CSGO we achieved our best results with the upsampler, part of the weights originally provided by DIAMOND.
For all models, we loaded the weights of a pre-trained State-Space World model into StateSpaceDiffuser. For MiniGrid, while models trained on sequence length 16 performed well, in inference, we</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Avg. PSNR $\uparrow$</th>
<th style="text-align: center;">Fin. PSNR $\uparrow$</th>
<th style="text-align: center;">SSIM $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Context Length 16</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">DIAMOND</td>
<td style="text-align: center;">27.13</td>
<td style="text-align: center;">25.44</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr>
<td style="text-align: left;">State-Space World Model</td>
<td style="text-align: center;">29.71</td>
<td style="text-align: center;">31.34</td>
<td style="text-align: center;">0.96</td>
</tr>
<tr>
<td style="text-align: left;">StateSpaceDiffuser (Ours)</td>
<td style="text-align: center;">$\mathbf{3 4 . 6 2}$</td>
<td style="text-align: center;">$\mathbf{3 8 . 0 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Context Length 50</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">DIAMOND</td>
<td style="text-align: center;">26.13</td>
<td style="text-align: center;">25.15</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr>
<td style="text-align: left;">State-Space World Model</td>
<td style="text-align: center;">27.25</td>
<td style="text-align: center;">24.49</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr>
<td style="text-align: left;">StateSpaceDiffuser (Ours)</td>
<td style="text-align: center;">$\mathbf{2 8 . 1 2}$</td>
<td style="text-align: center;">$\mathbf{3 0 . 3 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 6}$</td>
</tr>
</tbody>
</table>
<p>Table 3: MiniGrid Quantitative Evaluation of Long Context Awareness, Trained on Context Length 16. Our models generalize their performance from a smaller to a longer sequence.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Full Sequence Sample with Context Length 50 on MiniGrid. Demonstrates a full 50 -frame context and the per-frame predictions. In red are shown the frames from the second half returning to the initial position.
achieved our best results for both context length 16 and 50 by using the model trained on context length 50. For CSGO, we used the State-Space World Model weights for context length 16 and also evaluated on context length 50 .
When we evaluate DIAMOND and StateSpaceDiffuser, we use 5 denoising steps for denoising the next observation and 10 to upscale it.</p>
<h1>B MiniGrid Evaluations</h1>
<p>In this section, we expand our discussion on our model's performance on the simpler MiniGrid dataset and offer additional qualitative and quantitative results.</p>
<h2>B. 1 Generalization Across Context Length on MiniGrid</h2>
<p>In Tab. 3, we show the evaluation results of models trained in context size 16, on both context length 16 and 50. A reasonably good performance demonstrates that the models do not overfit on a particular sequence size and still perform well in a context length longer than it has been trained with. While the State-Space World Model exhibits uncertainty in its predictions for a longer context without training (blurriness, color deviations), its features prove useful for StateSpaceDiffuser, with the Generative Branch producing a higher-fidelity result. StateSpaceDiffuser noticeably outperforms the baseline on context length 50 .</p>
<h2>B. 2 MiniGrid Qualitative Evaluation</h2>
<p>In Fig. 10 and Fig. 9 we show visual samples with the forward-backward evaluation used in our quantitative results. We assess the StateSpaceDiffuser model, trained on sequence length 51 and presented in the main paper and its corresponding State-Space World Model.
Note that in our standard evaluation on MiniGrid, at each time step we give the ground truth sequence up to that step and predict the next observation. In a single time step the agent takes a fixed motion</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Qualitative Results with Context Length 50 on MiniGrid. StateSpaceDiffuser demonstrates long-context preservation compared to DIAMOND and better visual fidelity compared to State-Space World Model.
in one of four directions and reveals exactly 1 row or column of the environment depending on the direction. As most of the content of the next frame is present in the previous frame, the unknown content is only contained within the new revealed area. In the first half of the sequence, the agent explores by navigating the maze. In Fig. 8 we show one such traversal and the predicted next frame for each time step from StateSpaceDiffuser. It is observed that the new revealed area is predicted far from the ground truth in the first half of the sequence. This is expected as this area has not yet been observed. However, in the second half, the return along the path, the ability to recall the content from the long given sequence helps to predict the correct content of the repeatedly revealed areas. Therefore, in all our evaluations we have made the decision to only consider the prediction quality of the second half of the second half of the sequence.
In context length 16 - Fig. 10, we clearly observe poor performance of the diffusion-only baseline, DIAMOND, as this model has no method to take into account long context. Looking closely at the State-Space World Model's output, we observe shifts in color and inconfidence in the content of the new revealed areas. The effect is subtle and varies in the sequence, but it is noticeable. This effect causes a drop in fidelity metrics and is a direct consequence of the non-variational approach of predicting the next frame from State-Space World Model. In contrast, StateSpaceDiffuser is free of such artifacts and predicts closer to the ground-truth images. Still, as it is conditioned on the state-space features, it can be affected by significant uncertainty in the content of particular grid cells.
The observations are even more pronounced for context length 51 - Fig. 9. Later in the sequence, the State-Space World model tends to increase its shifts from the ground truth (dar squares appear darker, grey squares tend to fade). While on a simpler dataset like MiniGrid such artifacts are less noticeable, for a more complex setup like CSGO this becomes more apparent.</p>
<h1>B. 3 Imagination Qualitative Results</h1>
<p>Additionally, instead of giving the ground truth sequence at every step, we also attempt to give only the first half and feed already predicted frames for time steps in the second half of the sequence (imagination). In this way the ground-truth frames in the second half are never seen by the model (same as the setup we have in the CSGO dataset). This is a more challenging setup, in which the content cannot be copied from the previous ground truth frame, and any errors in the current frame prediction propagate into the next frame.
We show qualitative results in imagination in Fig. 11. It is observed that in this more complex setup the diffusion baseline quickly drifts away from the context, and the entire image no longer corresponds to past context. In contrast, because of the incorporation of state-space features, the StateSpaceDiffuser is noticeably better at preserving the content of previous steps.</p>
<p>Input: 9 Frames Prediction
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Qualitative Results on MiniGrid. Compared to DIAMOND, State-Space World Model is able to recall past content better but lacks in certainty and visual fidelity. However, StateSpaceDiffuser is able to both to consider long context and to produce a high quality image.</p>
<p>Input: 9 Frames Prediction
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Imagination Qualitative Results on MiniGrid. Frames are consecutively generated given previously generated frames. StateSpaceDiffuser shows clear superiority on context preservation.</p>
<h1>C CSGO Evaluations</h1>
<h2>C. 1 Qualitative Evaluation</h2>
<p>We show additional qualitative examples in Fig. 12. We show the last 8 frames (predicted without the use of the last 8 ground-truth frames ), given 9 frames of the mirrored version of the sequence. The last predicted frame uses context length 16. In the results, it is observed that DIAMOND - our diffusion baseline - is unable to recall a previous context that was left beyond the 4 input frames it accepts as input per step. In contrast, StateSpaceDiffuser is able to predict the correct content through its Long Context Branch.</p>
<h2>C. 2 Strengths and Limitations of StateSpaceDiffuser</h2>
<p>DIAMOND has a limitation, which causes significant artifacts when the action is strong and results in a larger visual change (e.g., large turn). Our Generative Branch is based on DIAMOND and hence has a similar limitation. However, while DIAMOND's artifacts tend to affect the entire predicted sequence, the StateSpaceDiffuser has the property to recover in subsequent steps by making use of
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Our StateSpaceDiffuser model is able to recover from insufficient information in the short context, while the diffusion baseline - DIAMOND, has no mechanism to do so.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Recovery from a strong action. Strong actions cause significant artifacts in DIAMOND. While this behavior is inherited by StateSpaceDiffuser, in contrast, it quickly recovers from the artifacts using the state-space features.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: Limitations. Our method sometimes only reconstructs coarse features and leaves details out. Even in these cases, the content appears closer to the ground truth than the diffusion baseline.
the state-space representation to recover the content. This is visible throughout Fig. 12, but also particularly in the examples shown in Fig. 13.
The effect of the strong action is one of our main motivations to use autoregressive-style prediction for the CSGO models rather than a single frame prediction given a full context (as done with MiniGrid). While MiniGrid has actions with constant measurable visual effect, CSGO's actions vary in strength. We observe that many sequences consist of almost no motion, divided by a few strong actions. In the single-action prediction task, weak motion would not require much new content to be generated. This often results in a copy of the previous frame, and not much improvement is expected compared to the baseline. Conversely, with strong actions, the baseline and StateSpaceDiffuser exhibit inconfidence in the next frame. This affects the short context window, filling it with artifacts. In comparison to the baseline, the StateSpaceDiffuser is able to recover in the following frames.
While our method's content is consistently closer to the ground truth than the diffusion baseline, it sometimes is only able to preserve coarse features (colors, general shape of the scene) and is less effective with finer details. We show this in Fig. 14. It is visible that sometimes our method misses showing an object (crate in the first example) or just preserves the general shape of a scene (second example). Even with those limitations, the model often outperforms the baseline. As a higher level of detail requires larger memory capacity, we believe that with more computational resources, scaling the Long-Context Branch can aid these issues.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Corresponding author: firstname.lastname@insait.ai&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
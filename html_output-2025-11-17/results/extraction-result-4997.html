<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4997 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4997</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4997</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-5f66d1a667eec13b5d337c3fc5619bcef95092bd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5f66d1a667eec13b5d337c3fc5619bcef95092bd" target="_blank">Universal Self-Consistency for Large Language Model Generation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates and effectively utilizes multiple samples and improves the performance on open-ended generation tasks where the original self-consistency method is not applicable.</p>
                <p><strong>Paper Abstract:</strong> Self-consistency with chain-of-thought prompting (CoT) has demonstrated remarkable performance gains on various challenging tasks, by utilizing multiple reasoning paths sampled from large language models (LLMs). However, self-consistency relies on the answer extraction process to aggregate multiple solutions, which is not applicable to free-form answers. In this work, we propose Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates. We evaluate USC on a variety of benchmarks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On open-ended generation tasks where the original self-consistency method is not applicable, USC effectively utilizes multiple samples and improves the performance. For mathematical reasoning, USC matches the standard self-consistency performance without requiring the answer formats to be similar. Finally, without access to execution results, USC also matches the execution-based voting performance on code generation.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4997.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4997.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>USC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Universal Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based selection method that samples multiple candidate responses and prompts an LLM to pick the most consistent candidate (or other selection criteria) to produce a final output; designed to extend self-consistency to free-form generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L, gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned PaLM 2-L and OpenAI gpt-3.5-turbo used in experiments (instruction-tuned large transformer LLMs; exact sizes not specified in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Universal Self-Consistency (USC)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Sample multiple diverse responses (via stochastic decoding) and concatenate them into a prompt that asks the LLM to select the most consistent (or most detailed) candidate; selection relies on the LLM's internal evaluation rather than answer-format extraction or execution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, MATH, BIRD-SQL, ARCADE, GovReport, SummScreen, TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A mix of mathematical reasoning (GSM8K, MATH), code generation (BIRD-SQL, ARCADE), long-document summarization (GovReport, SummScreen), and open-ended QA (TruthfulQA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Across tasks USC improves over greedy and random baselines and is generally comparable to standard self-consistency: e.g., PaLM 2-L GSM8K accuracy 90.2, MATH 37.4; gpt-3.5-turbo GSM8K 77.8, MATH 38.1; BIRD-SQL exec acc 45.5 (valid-eff 48.8); ARCADE exec acc 30.1; GovReport ROUGE-1 40.2; SummScreen ROUGE-1 31.7; TruthfulQA (PaLM 2-L) GPT-judge 67.7 GPT-info 99.0, (gpt-3.5) GPT-judge 82.5 GPT-info 99.6.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Standard self-consistency (SC) and execution-based voting: PaLM 2-L SC GSM8K 90.4 MATH 37.9; gpt-3.5 SC GSM8K 78.5 MATH 38.0; BIRD-SQL SC-Exec exec acc 45.6 valid-eff 48.1; ARCADE SC-Exec strict 29.8 fuzzy 30.3; greedy and random baselines are lower (e.g., PaLM 2-L greedy GSM8K 85.7).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>USC, which uses diverse sampled responses and an LLM-based selector, consistently improves over single-run greedy decoding and random selection across reasoning, code, summarization, and open-ended QA tasks. Where standard self-consistency (which performs majority-vote on extracted final answers) is applicable, USC generally matches SC's performance without requiring answer parsing or execution results. USC is robust to response ordering and benefits from more samples up to a practical sweet spot (~8).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>USC does not always strictly improve with more samples: on GSM8K accuracy decreased when moving to 16 samples (suggested causes include long-context understanding limits and imperfect counting by the LLM). USC is an imperfect approximation of SC—the USC–SC match ratio drops when increasing samples (8→16), and there remains a notable gap to an oracle selector that always picks the best candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4997.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4997.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (majority voting over chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard self-consistency: sample multiple chain-of-thought reasoning chains and take a majority vote on the extracted final answers to improve correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L, gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned PaLM 2-L and gpt-3.5-turbo as used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Standard Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate multiple reasoning chains using chain-of-thought prompting and pick the final answer that appears most frequently (majority vote) after extracting canonical final answers (works when final answers are comparable by exact match).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical reasoning benchmarks: GSM8K (grade-school math problems) and MATH (competition-level math problems).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>PaLM 2-L SC: GSM8K accuracy 90.4, MATH 37.9; gpt-3.5-turbo SC: GSM8K 78.5, MATH 38.0.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>USC: PaLM 2-L GSM8K 90.2 MATH 37.4; gpt-3.5-turbo GSM8K 77.8 MATH 38.1; greedy and random substantially lower (e.g., PaLM 2-L greedy 85.7/30.8).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Majority-vote over diverse sampled chains (SC) provides substantial gains over single greedy chains; USC approximates SC well in many settings without requiring answer parsing. SC remains a strong baseline when answer extraction is feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>SC requires extractable/closed-form final answers (e.g., numeric answers) and cannot be directly applied to free-form tasks (summaries, open-ended QA) without additional parsing; when ties occur SC deterministically picks first-indexed best-vote response which can differ from USC's selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4997.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4997.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SC-Exec</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Execution-based Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For code generation: execute candidate programs and cluster by identical outputs; select the program from the largest execution-output cluster as final prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Execution-guided neural program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI gpt-3.5-turbo used to generate candidate programs for text-to-SQL and Python notebook code tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Execution-based self-consistency (SC-Exec)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate multiple candidate programs, execute them on provided inputs (or generated testcases), cluster by identical execution output; select program from largest cluster to exploit semantic agreement rather than surface-form similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BIRD-SQL, ARCADE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Code generation: BIRD-SQL (text-to-SQL) and ARCADE (Python code generation in data-science notebooks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>BIRD-SQL SC-Exec execution accuracy 45.6 (valid-efficiency 48.1); ARCADE SC-Exec strict match 29.8, fuzzy match 30.3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>USC (no execution) achieves comparable results: BIRD-SQL USC exec acc 45.5 valid-eff 48.8; ARCADE USC 30.1.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Execution-based voting is effective for code tasks; USC, which lacks execution but uses LLM-based consistency selection, matches execution-based performance on these code benchmarks, suggesting language-model selection can approximate execution-voting when execution is unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Execution-based methods require runtime/execution environment and properly distinguishing programs via testcases; USC matches but does not surpass execution-based top performance and overall both are below an oracle selector.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4997.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4997.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Greedy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy Decoding (single deterministic chain)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single deterministic decode (temperature 0) producing one reasoning chain and final answer; represents a 'similar' reasoning method because only one path is used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L, gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned PaLM 2-L and gpt-3.5-turbo used as single-run baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Greedy decoding (single chain)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Produce a single deterministic output (temperature 0) without sampling alternative reasoning trajectories; no ensembling or voting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, MATH, BIRD-SQL, ARCADE, GovReport, SummScreen, TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same suite of reasoning, code, summarization, and open-ended QA benchmarks used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Generally weaker than multi-sample methods: e.g., PaLM 2-L GSM8K 85.7 MATH 30.8; gpt-3.5-turbo GSM8K 73.4 MATH 33.2; BIRD-SQL exec acc 42.4; ARCADE 26.0; GovReport ROUGE-1 38.8; SummScreen ROUGE-1 30.6; TruthfulQA (PaLM 2-L) GPT-judge 62.1 GPT-info 95.1; (gpt-3.5) GPT-judge 79.8 GPT-info 99.7.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Diverse sampling + selection (SC/USC) substantially improves performance (examples above).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Single deterministic (similar) decoding is consistently outperformed by diverse-sampling methods that ensemble or vote across multiple reasoning paths (SC, USC).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Greedy is cheap and sometimes competitive on easy instances, but fails to capture gains from aggregating diverse reasoning trajectories; no cases in this paper where greedy outperforms SC/USC on aggregate metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4997.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4997.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random selection from sampled responses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uniformly pick one candidate from multiple sampled responses (diverse generation) without any selection/reranking criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L, gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LLMs used to produce candidate sample sets; random selection serves as a simple baseline for multi-sample approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Random selection (from diverse samples)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Sample multiple outputs stochastically (temperature > 0) and pick one uniformly at random as the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, MATH, BIRD-SQL, ARCADE, GovReport, SummScreen, TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same benchmarks used as baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Consistently worse than SC/USC and often slightly worse than greedy: e.g., PaLM 2-L GSM8K 82.9 MATH 28.0; gpt-3.5-turbo GSM8K 68.5 MATH 26.3; BIRD-SQL 41.9; ARCADE 26.8; GovReport ROUGE-1 38.5; SummScreen ROUGE-1 30.2; TruthfulQA PaLM 2-L GPT-judge 62.9 GPT-info 94.6; gpt-3.5 GPT-judge 80.6 GPT-info 99.3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>SC and USC outperform random selection; random selection demonstrates that mere sampling without an effective aggregation/selection hurts final performance relative to voting or LLM-based selection.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Random picking from diverse candidates is a weak strategy compared to using a selection criterion (majority voting or LLM-based consistency selection).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No evidence that random selection outperforms SC/USC in aggregated results; random is sometimes near greedy but generally inferior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4997.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4997.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ordering & k ablations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>USC ablations: effect of response ordering and number of samples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation studies analyzing how USC's performance depends on the order in which candidates are presented and the number of sampled candidate responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L (primary ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 2-L used to test sensitivity to response order and sample count.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>USC (ablation: ordering & sample count)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Measure USC's robustness by shuffling candidate order (multiple runs) and varying k (number of sampled candidates) from small to larger values (e.g., 5, 8, 16).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, MATH, SummScreen, GovReport, TruthfulQA, BIRD-SQL</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Representative reasoning, summarization, QA, and code tasks used for ablation analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Ordering: small variance across 5 random shuffles (e.g., GSM8K 89.7 ± 0.3, MATH 37.3 ± 0.2, SummScreen ROUGE-1 31.6 ± 0.3). Varying k: USC generally benefits from more samples on TruthfulQA and BIRD-SQL; SummScreen plateaus after ~5 samples; GSM8K accuracy decreased at 16 samples (vs 8).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared to SC with increasing k: match ratio between USC and SC falls when moving from 8 to 16 samples; SC remains stable when answer extraction is feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>USC is robust to candidate ordering (minimal effect) and benefits from more diverse samples up to a task-dependent point (commonly ~8 samples). Too many candidates can degrade USC on some tasks (e.g., GSM8K with 16 samples) likely due to context-length and counting limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Increasing samples beyond a point (16) can reduce UCS performance (GSM8K); USC–SC alignment decreases with more samples, indicating USC is an imperfect approximation to SC at larger k.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Execution-guided neural program synthesis <em>(Rating: 2)</em></li>
                <li>Self-consistency for open-ended generations <em>(Rating: 1)</em></li>
                <li>Natural language to code translation with execution <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4997",
    "paper_id": "paper-5f66d1a667eec13b5d337c3fc5619bcef95092bd",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "USC",
            "name_full": "Universal Self-Consistency",
            "brief_description": "An LLM-based selection method that samples multiple candidate responses and prompts an LLM to pick the most consistent candidate (or other selection criteria) to produce a final output; designed to extend self-consistency to free-form generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L, gpt-3.5-turbo",
            "model_description": "Instruction-tuned PaLM 2-L and OpenAI gpt-3.5-turbo used in experiments (instruction-tuned large transformer LLMs; exact sizes not specified in paper).",
            "reasoning_method_name": "Universal Self-Consistency (USC)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Sample multiple diverse responses (via stochastic decoding) and concatenate them into a prompt that asks the LLM to select the most consistent (or most detailed) candidate; selection relies on the LLM's internal evaluation rather than answer-format extraction or execution.",
            "task_name": "GSM8K, MATH, BIRD-SQL, ARCADE, GovReport, SummScreen, TruthfulQA",
            "task_description": "A mix of mathematical reasoning (GSM8K, MATH), code generation (BIRD-SQL, ARCADE), long-document summarization (GovReport, SummScreen), and open-ended QA (TruthfulQA).",
            "performance": "Across tasks USC improves over greedy and random baselines and is generally comparable to standard self-consistency: e.g., PaLM 2-L GSM8K accuracy 90.2, MATH 37.4; gpt-3.5-turbo GSM8K 77.8, MATH 38.1; BIRD-SQL exec acc 45.5 (valid-eff 48.8); ARCADE exec acc 30.1; GovReport ROUGE-1 40.2; SummScreen ROUGE-1 31.7; TruthfulQA (PaLM 2-L) GPT-judge 67.7 GPT-info 99.0, (gpt-3.5) GPT-judge 82.5 GPT-info 99.6.",
            "comparison_with_other_method": true,
            "performance_other_method": "Standard self-consistency (SC) and execution-based voting: PaLM 2-L SC GSM8K 90.4 MATH 37.9; gpt-3.5 SC GSM8K 78.5 MATH 38.0; BIRD-SQL SC-Exec exec acc 45.6 valid-eff 48.1; ARCADE SC-Exec strict 29.8 fuzzy 30.3; greedy and random baselines are lower (e.g., PaLM 2-L greedy GSM8K 85.7).",
            "key_findings": "USC, which uses diverse sampled responses and an LLM-based selector, consistently improves over single-run greedy decoding and random selection across reasoning, code, summarization, and open-ended QA tasks. Where standard self-consistency (which performs majority-vote on extracted final answers) is applicable, USC generally matches SC's performance without requiring answer parsing or execution results. USC is robust to response ordering and benefits from more samples up to a practical sweet spot (~8).",
            "counter_examples_or_negative_results": "USC does not always strictly improve with more samples: on GSM8K accuracy decreased when moving to 16 samples (suggested causes include long-context understanding limits and imperfect counting by the LLM). USC is an imperfect approximation of SC—the USC–SC match ratio drops when increasing samples (8→16), and there remains a notable gap to an oracle selector that always picks the best candidate.",
            "uuid": "e4997.0",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "SC",
            "name_full": "Self-Consistency (majority voting over chain-of-thought)",
            "brief_description": "Standard self-consistency: sample multiple chain-of-thought reasoning chains and take a majority vote on the extracted final answers to improve correctness.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L, gpt-3.5-turbo",
            "model_description": "Instruction-tuned PaLM 2-L and gpt-3.5-turbo as used in experiments.",
            "reasoning_method_name": "Standard Self-Consistency (SC)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generate multiple reasoning chains using chain-of-thought prompting and pick the final answer that appears most frequently (majority vote) after extracting canonical final answers (works when final answers are comparable by exact match).",
            "task_name": "GSM8K, MATH",
            "task_description": "Mathematical reasoning benchmarks: GSM8K (grade-school math problems) and MATH (competition-level math problems).",
            "performance": "PaLM 2-L SC: GSM8K accuracy 90.4, MATH 37.9; gpt-3.5-turbo SC: GSM8K 78.5, MATH 38.0.",
            "comparison_with_other_method": true,
            "performance_other_method": "USC: PaLM 2-L GSM8K 90.2 MATH 37.4; gpt-3.5-turbo GSM8K 77.8 MATH 38.1; greedy and random substantially lower (e.g., PaLM 2-L greedy 85.7/30.8).",
            "key_findings": "Majority-vote over diverse sampled chains (SC) provides substantial gains over single greedy chains; USC approximates SC well in many settings without requiring answer parsing. SC remains a strong baseline when answer extraction is feasible.",
            "counter_examples_or_negative_results": "SC requires extractable/closed-form final answers (e.g., numeric answers) and cannot be directly applied to free-form tasks (summaries, open-ended QA) without additional parsing; when ties occur SC deterministically picks first-indexed best-vote response which can differ from USC's selection.",
            "uuid": "e4997.1",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "SC-Exec",
            "name_full": "Execution-based Self-Consistency",
            "brief_description": "For code generation: execute candidate programs and cluster by identical outputs; select the program from the largest execution-output cluster as final prediction.",
            "citation_title": "Execution-guided neural program synthesis",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "OpenAI gpt-3.5-turbo used to generate candidate programs for text-to-SQL and Python notebook code tasks.",
            "reasoning_method_name": "Execution-based self-consistency (SC-Exec)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generate multiple candidate programs, execute them on provided inputs (or generated testcases), cluster by identical execution output; select program from largest cluster to exploit semantic agreement rather than surface-form similarity.",
            "task_name": "BIRD-SQL, ARCADE",
            "task_description": "Code generation: BIRD-SQL (text-to-SQL) and ARCADE (Python code generation in data-science notebooks).",
            "performance": "BIRD-SQL SC-Exec execution accuracy 45.6 (valid-efficiency 48.1); ARCADE SC-Exec strict match 29.8, fuzzy match 30.3.",
            "comparison_with_other_method": true,
            "performance_other_method": "USC (no execution) achieves comparable results: BIRD-SQL USC exec acc 45.5 valid-eff 48.8; ARCADE USC 30.1.",
            "key_findings": "Execution-based voting is effective for code tasks; USC, which lacks execution but uses LLM-based consistency selection, matches execution-based performance on these code benchmarks, suggesting language-model selection can approximate execution-voting when execution is unavailable.",
            "counter_examples_or_negative_results": "Execution-based methods require runtime/execution environment and properly distinguishing programs via testcases; USC matches but does not surpass execution-based top performance and overall both are below an oracle selector.",
            "uuid": "e4997.2",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Greedy",
            "name_full": "Greedy Decoding (single deterministic chain)",
            "brief_description": "A single deterministic decode (temperature 0) producing one reasoning chain and final answer; represents a 'similar' reasoning method because only one path is used.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L, gpt-3.5-turbo",
            "model_description": "Instruction-tuned PaLM 2-L and gpt-3.5-turbo used as single-run baselines.",
            "reasoning_method_name": "Greedy decoding (single chain)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Produce a single deterministic output (temperature 0) without sampling alternative reasoning trajectories; no ensembling or voting.",
            "task_name": "GSM8K, MATH, BIRD-SQL, ARCADE, GovReport, SummScreen, TruthfulQA",
            "task_description": "Same suite of reasoning, code, summarization, and open-ended QA benchmarks used for comparison.",
            "performance": "Generally weaker than multi-sample methods: e.g., PaLM 2-L GSM8K 85.7 MATH 30.8; gpt-3.5-turbo GSM8K 73.4 MATH 33.2; BIRD-SQL exec acc 42.4; ARCADE 26.0; GovReport ROUGE-1 38.8; SummScreen ROUGE-1 30.6; TruthfulQA (PaLM 2-L) GPT-judge 62.1 GPT-info 95.1; (gpt-3.5) GPT-judge 79.8 GPT-info 99.7.",
            "comparison_with_other_method": true,
            "performance_other_method": "Diverse sampling + selection (SC/USC) substantially improves performance (examples above).",
            "key_findings": "Single deterministic (similar) decoding is consistently outperformed by diverse-sampling methods that ensemble or vote across multiple reasoning paths (SC, USC).",
            "counter_examples_or_negative_results": "Greedy is cheap and sometimes competitive on easy instances, but fails to capture gains from aggregating diverse reasoning trajectories; no cases in this paper where greedy outperforms SC/USC on aggregate metrics.",
            "uuid": "e4997.3",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Random",
            "name_full": "Random selection from sampled responses",
            "brief_description": "Uniformly pick one candidate from multiple sampled responses (diverse generation) without any selection/reranking criterion.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L, gpt-3.5-turbo",
            "model_description": "Same LLMs used to produce candidate sample sets; random selection serves as a simple baseline for multi-sample approaches.",
            "reasoning_method_name": "Random selection (from diverse samples)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Sample multiple outputs stochastically (temperature &gt; 0) and pick one uniformly at random as the final answer.",
            "task_name": "GSM8K, MATH, BIRD-SQL, ARCADE, GovReport, SummScreen, TruthfulQA",
            "task_description": "Same benchmarks used as baselines.",
            "performance": "Consistently worse than SC/USC and often slightly worse than greedy: e.g., PaLM 2-L GSM8K 82.9 MATH 28.0; gpt-3.5-turbo GSM8K 68.5 MATH 26.3; BIRD-SQL 41.9; ARCADE 26.8; GovReport ROUGE-1 38.5; SummScreen ROUGE-1 30.2; TruthfulQA PaLM 2-L GPT-judge 62.9 GPT-info 94.6; gpt-3.5 GPT-judge 80.6 GPT-info 99.3.",
            "comparison_with_other_method": true,
            "performance_other_method": "SC and USC outperform random selection; random selection demonstrates that mere sampling without an effective aggregation/selection hurts final performance relative to voting or LLM-based selection.",
            "key_findings": "Random picking from diverse candidates is a weak strategy compared to using a selection criterion (majority voting or LLM-based consistency selection).",
            "counter_examples_or_negative_results": "No evidence that random selection outperforms SC/USC in aggregated results; random is sometimes near greedy but generally inferior.",
            "uuid": "e4997.4",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Ordering & k ablations",
            "name_full": "USC ablations: effect of response ordering and number of samples",
            "brief_description": "Ablation studies analyzing how USC's performance depends on the order in which candidates are presented and the number of sampled candidate responses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L (primary ablations)",
            "model_description": "PaLM 2-L used to test sensitivity to response order and sample count.",
            "reasoning_method_name": "USC (ablation: ordering & sample count)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Measure USC's robustness by shuffling candidate order (multiple runs) and varying k (number of sampled candidates) from small to larger values (e.g., 5, 8, 16).",
            "task_name": "GSM8K, MATH, SummScreen, GovReport, TruthfulQA, BIRD-SQL",
            "task_description": "Representative reasoning, summarization, QA, and code tasks used for ablation analysis.",
            "performance": "Ordering: small variance across 5 random shuffles (e.g., GSM8K 89.7 ± 0.3, MATH 37.3 ± 0.2, SummScreen ROUGE-1 31.6 ± 0.3). Varying k: USC generally benefits from more samples on TruthfulQA and BIRD-SQL; SummScreen plateaus after ~5 samples; GSM8K accuracy decreased at 16 samples (vs 8).",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared to SC with increasing k: match ratio between USC and SC falls when moving from 8 to 16 samples; SC remains stable when answer extraction is feasible.",
            "key_findings": "USC is robust to candidate ordering (minimal effect) and benefits from more diverse samples up to a task-dependent point (commonly ~8 samples). Too many candidates can degrade USC on some tasks (e.g., GSM8K with 16 samples) likely due to context-length and counting limitations.",
            "counter_examples_or_negative_results": "Increasing samples beyond a point (16) can reduce UCS performance (GSM8K); USC–SC alignment decreases with more samples, indicating USC is an imperfect approximation to SC at larger k.",
            "uuid": "e4997.5",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Execution-guided neural program synthesis",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency for open-ended generations",
            "rating": 1
        },
        {
            "paper_title": "Natural language to code translation with execution",
            "rating": 1
        }
    ],
    "cost": 0.0154715,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Universal Self-Consistency for Large Language Model Generation</h1>
<p>Xinyun Chen ${ }^{<em>}$ Renat Aksitov</em> Uri Alon Jie Ren Kefan Xiao Pengcheng Yin<br>Sushant Prakash Charles Sutton Xuezhi Wang Denny Zhou<br>Google</p>
<h4>Abstract</h4>
<p>Self-consistency with chain-of-thought prompting ( CoT ) has demonstrated remarkable performance gains on various challenging tasks, by utilizing multiple reasoning paths sampled from large language models (LLMs). However, selfconsistency relies on the answer extraction process to aggregate multiple solutions, which is not applicable to free-form answers. In this work, we propose Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates. We evaluate USC on a variety of benchmarks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On open-ended generation tasks where the original self-consistency method is not applicable, USC effectively utilizes multiple samples and improves the performance. For mathematical reasoning, USC matches the standard self-consistency performance without requiring the answer formats to be similar. Finally, without access to execution results, USC also matches the execution-based voting performance on code generation.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) have accomplished significant breakthroughs in a wide variety of domains, including mathematical reasoning (Cobbe et al., 2021; Wei et al., 2022; Lewkowycz et al., 2022), code generation (Chen et al., 2021; Austin et al., 2021; Li et al., 2022), and other text generation tasks (Bubeck et al., 2023; Anil et al., 2023; Touvron et al., 2023). Despite the rapid progress, the LLM-generated responses are still prone to errors when they get long. A long line of efforts have been devoted to improve the output quality by sampling multiple model responses and then selecting the final output based on certain criteria. For example, prior works have trained neural networks to rerank model outputs (Cobbe et al., 2021; Li et al., 2023b; Ni et al., 2023; Yin \&amp; Neubig, 2019; Zeng et al., 2022), and more recent works investigate using LLMs to score the responses (Fu et al., 2023; Liu et al., 2023; Wang et al., 2023a).</p>
<p>In this work, we consider the consistency among model responses as the criterion to select the model output, a generic metric that has enabled huge performance leaps in reasoning (Wang et al., 2022) and code generation (Li et al., 2022; Shi et al., 2022). In particular, self-consistency (Wang et al., 2022) with chain-of-thought prompting (Wei et al., 2022) boosts the performance on various benchmarks, by marginalizing latent reasoning paths through sampling which leads to select the final answer as the most common one. However, self-consistency can only be applied to tasks where the final answer can be aggregated via exact match, e.g., a single number for math problems.</p>
<p>To address this major limitation of self-consistency, we propose Universal Self-Consistency (USC) to support various applications, especially free-form generation tasks. Specifically, given multiple candidate responses, USC simply calls the LLM to select the most consistent response among them as the final output. Thus, USC eliminates the need of designing an answer extraction process, and is applicable to tasks with free-form answers. Although prior works have revealed weaknesses of LLMs for response selection, such as position bias (Wang et al., 2023b; Zheng et al., 2023b) and incorrectly judging the answer correctness (Huang et al., 2023b; Gou et al., 2023), intuitively, assessing the consistency among candidate answers is easier than measuring and comparing the answer quality.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We evaluate universal self-consistency on a wide range of tasks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) benchmarks for math problem solving, USC generally matches the performance of the standard self-consistency. On programming tasks including text-to-SQL generation (Li et al., 2023a) and Python code generation (Yin et al., 2023), USC matches the performance of execution-based consistency (Li et al., 2022; Shi et al., 2022), while USC does not require execution results to aggregate over candidate programs. Finally, USC also improves the performance for open-ended question answering (Lin et al., 2021) and long-context summarization (Huang et al., 2021; Chen et al., 2022b), where the standard self-consistency is not applicable. In addition to the performance gain, our evaluation also demonstrates that USC outputs highly match those of the standard self-consistency when the comparison is applicable, while it is robust to the ordering of candidate responses.</p>
<h1>2 BACKGROUND: SELF-CONSISTENCY</h1>
<p>Self-consistency (Wang et al., 2022) augments chain-of-thought prompting (Wei et al., 2022) by sampling multiple reasoning chains and then taking a majority vote on the final answer set. The intuition is that sometimes the greedily decoded reasoning process might not be the optimal one, hence it makes more sense to sample a diverse set of reasoning chains, and if some of them lead to the same answer, then we have a higher confidence that this consistent answer is the correct one. It has been shown that self-consistency improves the greedy chain-of-thought prompting by a large margin on a wide set of reasoning tasks.</p>
<p>Besides question answering tasks, consistency-based answer selection has also been applied to code generation (Shi et al., 2022; Li et al., 2022; Chen et al., 2019), which requires code execution. Specifically, we first execute all predicted programs on the given inputs, then programs with the same execution outputs are clustered together, assuming that they are semantically equivalent. Finally, we select the program belonging to the largest cluster as the final prediction. When the program inputs given in the task description are insufficient to distinguish between different predictions, this execution-based code selection is also often accompanied with a test case generation process to better examine the consistency (Li et al., 2022; Chen et al., 2022a; Huang et al., 2023a).</p>
<p>Despite the remarkable improvement, self-consistency is only applicable to problems with a unique and closed-form answer, e.g., when the final answer consists of a single number, because a majority vote needs to be taken over the final answer set. This significant requirement poses a challenge for tasks that require open-ended generations, such as summarization, creative writing, and open-ended question answering.</p>
<h2>3 UNIVERSAL SELF-CONSISTENCY</h2>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of the Universal Self-Consistency workflow.
We present the overall workflow of universal self-consistency (USC) in Figure 1, which utilizes LLMs to enable self-consistency for a wide variety of tasks, especially free-form text generation. First, we sample multiple responses with the large language model. Afterward, to select one model response as the final answer, we concatenate all responses together, and then construct a prompt with an instruction asking the language model to select the most consistent response. In this way, USC obviates the necessity of counting the exact answer frequency as in the standard self-consistency, and relies on the LLM's own ability to measure the consistency among different responses. Although prior works show that LLMs sometimes have trouble evaluating the prediction correctness (Huang</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Examples of Universal Self-Consistency for answer selection from responses of diverse formats: (a) mathematical reasoning; and (b) open-ended question answering. Note that for the given open-ended question, the final answer is an entity list, where no two responses share the same predictions. Still, the LLM correctly selects the response where the individual entities in the predicted list appear most frequently in the candidate responses.
et al., 2023b; Gou et al., 2023), especially for reasoning problems, empirically we observe that LLMs are generally able to examine the response consistency across multiple tasks.</p>
<p>Consistency assessment with LLMs offers more flexibility for free-form generation. Figure 2 demonstrates example tasks where different consistency criteria are beneficial for response selection. Specifically, Figure 2a shows different model responses for a math problem, where the output formats are diverse and thus makes it challenging for rule-based methods to extract answers. Nonetheless, assuming that the final answers are correctly extracted, the consistency criterion still follows the standard self-consistency on mathematical reasoning, which is based on the exact match of the final answers represented as single numerical values. On the other hand, Figure 2b shows an example question where the final answer is an entity list. Despite that there is no response that is consistent with others based on the exact match, the LLM selects the response where each of the predicted entities appears most frequently among the candidate outputs. In Section 4, we further show that LLM can also examine the consistency among responses beyond the question answering tasks, including code generation without access to the execution outputs, and long-context summarization.</p>
<h1>4 EXPERIMENTS</h1>
<h3>4.1 Evaluation Setup</h3>
<p>Benchmarks. We evaluate USC on the following variety of tasks:</p>
<ul>
<li>Mathematical reasoning benchmarks, including GSM8K (Cobbe et al., 2021), a dataset of 8,500 grade school math word problems, and MATH (Hendrycks et al., 2021), a dataset of 12,500 challenging mathematics problems from high school competitions.</li>
<li>Code generation benchmarks, including BIRD-SQL dataset (Li et al., 2023a) for text-to-SQL generation, and ARCADE dataset (Yin et al., 2023) for Python code generation in data science notebooks.</li>
<li>Long-context summarization, including the GovReport and SummScreen benchmarks from ZeroSCROLLS (Shaham et al., 2023). In GovReport (Huang et al., 2021), each input is a document containing $\sim 7,900$ words on average, and the reference output is an expert-written executive summary with $\sim 500$ words. In SummScreen (Chen et al., 2022b), every input is a transcript of a TV show episode with $\sim 5,600$ words, and each reference output is a $\sim 100$ words human-written recap of the episode. We follow Shaham et al. (2023) and measure ROUGE 1, ROUGE 2, and ROUGE-Lsum which measure n-gram overlap with the reference summary, and we also measure BERTScore F1 (Zhang et al., 2019).</li>
<li>TruthfulQA (Lin et al., 2021) benchmark for open-ended question answering, which contains 817 questions to test model's ability in generating truthful answers. To evaluate the answer's quality, we use the GPT-judge and GPT-info, which are GPT-3 models fine-tuned on human feedback data, provided by Lin et al. (2021). GPT-judge model outputs a binary rating for truthfulness, and GPT-info model outpus a binary rating for informativeness. It is shown that the GPT-3 models have higher accuracy in predicting human judgement than the automatic metrics ROUGE, BLEU, BLEURT.</li>
</ul>
<p>Decoding schemes. We compare USC to the following decoding schemes:</p>
<ul>
<li>Greedy decoding generates a single answer with the temperature 0 .</li>
<li>Random selects one answer randomly from multiple samples with temperature $&gt;0$.</li>
<li>SC (Wang et al., 2022) is the standard self-consistency decoding with answer extraction. We evaluate SC whenever applicable; for example, on reasoning benchmarks where the final answers can be compared through exact match.</li>
</ul>
<p>To enable a fair comparison, for sampling schemes (i.e., except greedy decoding), we always select the final answer from the same set of initial model responses. For code generation, we compare our approach to execution-based self-consistency (Shi et al., 2022; Li et al., 2022; Chen et al., 2019), where we select the code with the most common execution result. Both USC and execution-based self-consistency first filter out syntactically invalid candidate programs, and then perform the voting over the remaining ones. For ARCADE benchmark, we also evaluate a variant of the execution-based self-consistency with fuzzy matching as described in Yin et al. (2023), which implements a set of heuristics to determine whether the execution outputs of two programs are equivalent when they are not exact match.</p>
<p>Implementation details. We conduct experiments using instruction-tuned PaLM 2-L (Anil et al., 2023) and gpt-3.5-turbo models. Unless otherwise specified, the LLM generates 8 initial samples for both SC and USC. For mathematical reasoning, summarization and the ARCADE benchmark for Python code generation, the initial samples are generated with zero-shot prompting, thus the output formats are diverse. For BIRD-SQL, we used the 1-shot chain-of-thought prompt in Li et al. (2023a), which improves the performance. We also utilized a one-shot prompt for TruthfulQA to improve the quality of candidate responses. We set the temperature to be 0.6 for PaLM 2-L, and 1.0 for gpt-3.5-turbo.</p>
<h1>4.2 Main ReSults</h1>
<p>Mathematical reasoning. For mathematical reasoning benchmarks, we compare USC against the standard self-consistency in Table 1. For the standard self-consistency, we employ a regular expression matching to extract the final answer on GSM8K, and re-use the answer parsing code from (Zheng et al., 2023a) for MATH. Overall, USC consistently improves over the greedy decoding and random selection, and the performance is generally comparable to the standard self-consistency, which USC does not need answer parsing to perform the voting.</p>
<p>Code generation. Table 2 presents the results on BIRD-SQL and ARCADE respectively. On BIRD-SQL, besides the execution accuracy, we follow (Li et al., 2023a) to also evaluate the valid efficiency score, which measures the efficiency of the generated SQL queries. We show that USC matches the execution-based self-consistency performance on both benchmarks, while USC does not utilize code execution to perform the voting.</p>
<p>Summarization. Results for the summarization benchmarks are shown in Table 3. Since the generated summaries are in free-form, the standard self-consistency is not applicable. In GovReport, USC consistently improves over the baselines across all metrics. In Section 4.3, we further show that asking the model to choose the most detailed summary results in more performance gain.</p>
<p>TruthfulQA. Table 4 presents results on TruthfulQA, where SC is also not directly applicable because the generated answers are in free-form. Comparing with greedy decoding and random selection, USC-based answers have the highest truthfulness with both PaLM 2-L and gpt-3.5turbo. For informativeness which is considered as a secondary objective, USC-based answers have the highest score on PaLM 2-L and the second highest score ( 0.1 lower than the highest) on gpt-3.5-turbo. Considering that GPT-judge and GPT-info models have generally 90-95\% validation accuracy on rating prediction (Lin et al., 2021), the 0.1 difference is not considered significant.</p>
<p>Table 1: Accuracy on mathematical reasoning benchmarks. USC and SC consistently improve over the greedy decoding and random selection. USC performance is generally comparable to SC.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: left;">Greedy decoding</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">30.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">28.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">SC (Wang et al., 2022)</td>
<td style="text-align: center;">$\mathbf{9 0 . 4}$</td>
<td style="text-align: center;">$\mathbf{3 7 . 9}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">37.4</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: left;">Greedy decoding</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">33.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">26.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">SC</td>
<td style="text-align: center;">$\mathbf{7 8 . 5}$</td>
<td style="text-align: center;">38.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">$\mathbf{3 8 . 1}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy on code generation benchmarks with gpt-3.5-turbo.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Approach</th>
<th style="text-align: center;">Execution Accuracy</th>
<th style="text-align: center;">Valid Efficiency Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BIRD-SQL</td>
<td style="text-align: center;">Greedy decoding</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">44.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">44.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SC-Exec</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">48.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">USC</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">48.8</td>
</tr>
<tr>
<td style="text-align: center;">ARCADE</td>
<td style="text-align: center;">Greedy decoding</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SC-Exec (strict match)</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SC-Exec (fuzzy match)</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">USC</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: Results on long-context summarization benchmarks with PaLM 2-L. Since the outputs are in free-form, the standard self-consistency is not applicable. USC consistently improves over the baselines on summary quality.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Approach</th>
<th>ROUGE-1</th>
<th>ROUGE-2</th>
<th>ROUGE-Lsum</th>
<th>BERTScore</th>
</tr>
</thead>
<tbody>
<tr>
<td>GovReport</td>
<td>Greedy decoding</td>
<td>38.8</td>
<td>16.9</td>
<td>33.8</td>
<td>62.7</td>
</tr>
<tr>
<td></td>
<td>Random</td>
<td>38.5</td>
<td>16.9</td>
<td>33.6</td>
<td>62.6</td>
</tr>
<tr>
<td></td>
<td>USC</td>
<td>$\mathbf{4 0 . 2}$</td>
<td>$\mathbf{1 7 . 4}$</td>
<td>$\mathbf{3 5 . 1}$</td>
<td>$\mathbf{6 2 . 8}$</td>
</tr>
<tr>
<td>SummScreen</td>
<td>Greedy decoding</td>
<td>30.6</td>
<td>7.5</td>
<td>19.1</td>
<td>$\mathbf{5 8 . 7}$</td>
</tr>
<tr>
<td></td>
<td>Random</td>
<td>30.2</td>
<td>7.3</td>
<td>19.0</td>
<td>58.6</td>
</tr>
<tr>
<td></td>
<td>USC</td>
<td>$\mathbf{3 1 . 7}$</td>
<td>$\mathbf{7 . 8}$</td>
<td>$\mathbf{1 9 . 8}$</td>
<td>58.3</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracy on the TruthfulQA benchmark. Since the answer is in free-form, the standard self-consistency is not applicable. USC overall has the highest truthfulness and informativeness over the baselines.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Approach</th>
<th>GPT-judge</th>
<th>GPT-info</th>
</tr>
</thead>
<tbody>
<tr>
<td>PaLM 2-L</td>
<td>Greedy decoding</td>
<td>62.1</td>
<td>95.1</td>
</tr>
<tr>
<td></td>
<td>Random</td>
<td>62.9</td>
<td>94.6</td>
</tr>
<tr>
<td></td>
<td>USC</td>
<td>$\mathbf{6 7 . 7}$</td>
<td>$\mathbf{9 9 . 0}$</td>
</tr>
<tr>
<td>gpt-3.5-turbo</td>
<td>Greedy decoding</td>
<td>79.8</td>
<td>$\mathbf{9 9 . 7}$</td>
</tr>
<tr>
<td></td>
<td>Random</td>
<td>80.6</td>
<td>99.3</td>
</tr>
<tr>
<td></td>
<td>USC</td>
<td>$\mathbf{8 2 . 5}$</td>
<td>99.6</td>
</tr>
</tbody>
</table>
<h1>4.3 Ablations</h1>
<p>Effect of response ordering. Prior works have shown that large language models can be affected by the order of candidate responses when used to evaluate their quality (Wang et al., 2023b; Zheng et al., 2023b). We examine the effect of response ordering by performing USC with 5 different random orders when concatenating all responses, and calculate the mean and standard deviation of the task results. From Table 5, we observe that the overall model performance remains similar with different response orders, suggesting the effect of response order is minimal.</p>
<p>Table 5: USC performance with random shuffling of original responses using PaLM 2-L. The mean and standard deviation are computed with 5 runs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">(a)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">(b)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">(c)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Dataset</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Dataset</td>
<td style="text-align: center;">ROUGE-1</td>
<td style="text-align: center;">ROUGE-Lsum</td>
<td style="text-align: center;">Metric</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">$89.7 \pm 0.3$</td>
<td style="text-align: center;">SummScreen</td>
<td style="text-align: center;">$31.6 \pm 0.3$</td>
<td style="text-align: center;">$19.5 \pm 0.2$</td>
<td style="text-align: center;">GPT-judge</td>
</tr>
<tr>
<td style="text-align: center;">MATH</td>
<td style="text-align: center;">$37.3 \pm 0.2$</td>
<td style="text-align: center;">GovReport</td>
<td style="text-align: center;">$40.0 \pm 0.1$</td>
<td style="text-align: center;">$34.9 \pm 0.2$</td>
<td style="text-align: center;">GPT-info</td>
</tr>
</tbody>
</table>
<p>Different number of responses. Next, we examine the effect of using different numbers of responses in USC. As shown in Figure 3, USC consistently benefits from more samples on TruthFulQA and BIRD-SQL. However, USC does not further improve the performance on SummScreen after 5 samples, and the accuracy on GSM8K decreases with 16 samples. This can be due to the weakness in long-context understanding when the prompt contains more candidate responses, and the imperfect counting ability of LLMs. Nevertheless, we consider utilizing a few samples (e.g., 8) a sweet spot to balance the task accuracy and compute cost, in which case USC reliably improves the performance across the board. In Section 4.4, we further compare the predictions from USC and SC to understand how using more candidate responses affects the results.</p>
<p>Criteria for response selection. One advantage of USC is its generality: the same criteria can be applied to various tasks, without any task-specific knowledge. Nonetheless, a minor task-specific</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
(b) Results on GSM8K and BIRD-SQL. The top numbers are USC accuracies, and the bottom numbers are the differences to SC accuracies.</p>
<p>Figure 3: USC results with different number of samples.
adaptation of the response selection instruction can further boost USC over the generic prompts. For example, Table 6 shows that asking the LLM to choose the most detailed response (rather than the most consistent one) results in gains of about 2 ROUGE-1 and ROUGE-Lsum points.</p>
<p>Table 6: Ablation on the response selection criterion on long-context summarization benchmarks with PaLM 2-L.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">ROUGE-1</th>
<th style="text-align: center;">ROUGE-2</th>
<th style="text-align: center;">ROUGE-Lsum</th>
<th style="text-align: center;">BERTScore</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GovReport</td>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">62.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">USC - most detailed</td>
<td style="text-align: center;">$\mathbf{4 2 . 4}$</td>
<td style="text-align: center;">$\mathbf{1 8 . 2}$</td>
<td style="text-align: center;">$\mathbf{3 6 . 9}$</td>
<td style="text-align: center;">$\mathbf{6 3 . 2}$</td>
</tr>
<tr>
<td style="text-align: center;">SummScreen</td>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">$\mathbf{5 8 . 3}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">USC - most detailed</td>
<td style="text-align: center;">$\mathbf{3 3 . 0}$</td>
<td style="text-align: center;">$\mathbf{7 . 9}$</td>
<td style="text-align: center;">$\mathbf{2 2 . 0}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 3}$</td>
</tr>
</tbody>
</table>
<h1>4.4 Discussion: How Well does USC Match SC SElection?</h1>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Comparison of selections made by USC versus SC with PaLM 2-L. $k$ denotes the number of candidate responses for selection. "Tied votes" represents the case where the USC and SC select different responses, but both have the maximum votes.</p>
<p>We have demonstrated that on tasks where the standard self-consistency is applicable, USC and SC achieve comparable overall performance with 8 samples; however, USC fails to further improve the GSM8K performance with 16 samples. In this section, we look closer into the relationship between USC and SC, specifically how well is the alignment between their selected responses.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Accuracy distribution when USC selection doesn't match SC.</p>
<p>Figure 4 presents a breakdown analysis of USC predictions on mathematical reasoning benchmarks with 8 and 16 candidate responses, and Figure 5 further compares the performance of USC and SC when they select different responses. We observe that:</p>
<ul>
<li>The voting ties constitute a notable portion to the selection differences between USC and SC, especially with 8 candidate responses. Specifically, among all responses with the maximum votes, SC always selects the one with the smallest index, while USC can pick up alternative ones based on the response format.</li>
<li>The match ratio between USC and SC consistently surpasses their own task accuracies, which shows that the consistency criterion is easier to measure than the answer correctness.</li>
<li>Shifting from 8 to 16 samples, the USC-SC match ratio reduces, suggesting that USC behaves as an imperfect approximation of SC. However, the difference in response selection does not always lead to the performance decrease, as USC sometimes selects the correct response when SC fails.</li>
</ul>
<h1>5 Related Work</h1>
<p>Response reranking and selection for language models. Reranking is a common method to improve the generation quality in language models by sampling multiple outputs and applying a post-hoc criterion to rank them, which often requires an additional trained ranker and sometimes additional human labeled data. For example, Cobbe et al. (2021) use human labels to train a ranking model to verify whether each generated response is correct or not, and Shen et al. (2021) jointly train a generator and a ranker to improve performance for math tasks. Instead of training response generators and rankers as separate models, Thoppilan et al. (2022) finetune the dialog model to also predict the ratings of candidate responses with human-annotated judgements. For code generation, various reranker models have been designed (Ni et al., 2023; Yin \&amp; Neubig, 2019; Zeng et al., 2022), which typically utilize execution results and language-specific syntactic features to improve the ranking performance. In contrast with these prior works, USC does not require any additional labeled training data nor an external reranking model: the LLM that generated the initial outputs is the same one that selects the final answer.</p>
<p>Several consistency-based response selection approaches have been proposed in the literature, which typically include a voting procedure to select the most common response (Wang et al., 2022; Zhou et al., 2022; Wightman et al., 2023; Yue et al., 2023; Bertsch et al., 2023). Self-consistency (Wang et al., 2022) shows that with multiple responses generated for the same task, selecting the reasoning path leading to the most common final answer improves the chain-of-thought reasoning performance. The candidate responses can also come from different prompt variants corresponding to the same problem (Zhou et al., 2022; Wightman et al., 2023; Yue et al., 2023). To measure the pairwise similarity between candidate responses for open-ended generation tasks, Jain et al. (2023) propose the n-gram consistency score, and the consistency score for each response is computed as the sum of the pairwise similarity scores. For code generation, the consistency measurement is typically based on code execution, where the candidate program with the most common execution outputs is selected (Shi et al., 2022; Li et al., 2022; Chen et al., 2019). Besides the consistency of code execution, other works also examine the consistency between the code and the specification (Min</p>
<p>et al., 2023), and utilize it for reranking (Zhang et al., 2023a; Huang et al., 2023a). In this work, we directly instruct the LLM to perform consistency-based selection without an explicit definition of the pairwise similarity, and we demonsrate its applicability to a wide range of tasks.</p>
<p>Response improvement with multiple candidates. Some recent works demonstrate that the LLM can improve its prediction output on top of the candidate responses. Yang et al. (2023) show that given a trajectory of previously generated solutions, the LLM can iteratively produce better solutions for an optimization task, and they demonstrate the effectiveness of this LLM-based optimization process for prompt optimization and several classic mathematical optimization tasks. Other works aggregate multiple reasoning chains and prompts the LLM to generate a better final response, which shows performance improvement on multi-hop question answering (Yoran et al., 2023) and medical question answering (Singhal et al., 2023). Instead of asking the LLM to generate a better response, USC focuses on response selection, as the candidate responses usually already contain high-quality solutions to the underlying tasks. Meanwhile, performing the consistency-based selection is generally an easier task than improving the answer correctness, and we demonstrate that USC properly utilizes multiple responses to improve the performance across different tasks.</p>
<p>Large language models for response evaluation. The underlying assumption in our work is that LLMs are reflective enough to evaluate the consistency between multiple self-generated outputs. This assumption is related to recent findings which had shown that large language models can also be used for evaluating model-generated texts (Bubeck et al., 2023; Fu et al., 2023; Wang et al., 2023a; Zhang et al., 2023b). LLM-based evaluators have demonstrated some promising results, e.g., they can be used to evaluate natural language generations without human references, but some work has also shown that they might not correlate very well with human judgements and sometimes exhibit bias towards model-generated texts (Bubeck et al., 2023; Liu et al., 2023). Another line of work utilizes the prediction probability of the LLM to measure the quality of multiple choices (Ren et al., 2023; Adiwardana et al., 2020), and Lin et al. (2022) show promising results on arithmetic tasks where they prompt the LLM to directly output the level of confidence for its response. In this work, we show that LLMs not only can serve as evaluators, they can also improve their own output by sampling multiple responses and evaluating the consistency between them.</p>
<h1>6 Limitations and Future Work</h1>
<p>Despite that USC supports open-ended generation tasks and generally achieves comparable performance in those domains where the standard self-consistency can be applied, our current USC implementation has its own limitations compared to the extraction-based self-consistency approach.</p>
<p>First, while self-consistency can be applied to an arbitrary number of samples as long as the final answers can be extracted, the number of samples supported by USC is bounded by the context length of the underlying LLM. That said, to seek a balance between the task performance and the sampling cost, in practice the number of generated samples per task is not prohibitively large, thus the context length is generally sufficient to make best use of the samples.</p>
<p>Second, the voting mechanism in self-consistency inherently offers a measure of confidence or uncertainty for each response (Wang et al., 2022). However, universal self-consistency has not yet been developed to include the confidence estimation. We consider developing a calibration mechanism for USC as future work, where we can leverage the LLM to perform output clustering and pairwise self-consistency.</p>
<p>Also, USC requires an additional LLM query by design, which incurs additional inference costs. Given that our USC prompt only requires the LLM to generate a response index corresponding to the final answer, the USC output length is much shorter than any individual candidate response to select from. To further reduce the cost, one direction is to use a light-weight language model to conduct USC, and optimizes its efficiency regarding long-context encoding.</p>
<p>Finally, one common limitation of both the standard self-consistency and USC is about the consistencybased selection criterion. Specifically, although consistency is a generic and effective criterion, the most consistent response is not necessarily the best one. We observe that there is still a notable gap to oracle scores where we assume the access to an oracle reranker that always selects the best response, and we present the full results in Appendix A. In Section 4.3 we demonstrate that we can</p>
<p>design task-specific criteria to further improve the performance, and we consider refining the USC framework to further close the gap to the oracle performance as future work.</p>
<h1>7 CONCLUSION</h1>
<p>In this work, we presented Universal Self-Consistency (USC), which extends the standard selfconsistency to support free-form generation tasks. USC notably boosts the performance in diverse applications, and performs on par with the standard self-consistency on those tasks where answer extraction is feasible for voting. Besides addressing the limitations discussed in Section 6, we also consider mitigating the position bias and improving long-context understanding of LLMs as important future work that can further enhance the effectiveness and robustness of the USC scheme.</p>
<h2>REFERENCES</h2>
<p>Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards a human-like opendomain chatbot. arXiv preprint arXiv:2001.09977, 2020.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.</p>
<p>Amanda Bertsch, Alex Xie, Graham Neubig, and Matthew R Gormley. It's mbr all the way down: Modern generation techniques through the lens of minimum bayes risk. arXiv preprint arXiv:2310.01387, 2023.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023.</p>
<p>Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022a.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. Summscreen: A dataset for abstractive screenplay summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8602-8615, 2022b.</p>
<p>Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In International Conference on Learning Representations, 2019.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire, 2023.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738, 2023.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>Baizhou Huang, Shuai Lu, Weizhu Chen, Xiaojun Wan, and Nan Duan. Enhancing large language models in coding through multi-perspective self-consistency. arXiv preprint arXiv:2309.17272, 2023a.</p>
<p>Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023b.</p>
<p>Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1419-1436, 2021.</p>
<p>Siddhartha Jain, Xiaofei Ma, Anoop Deoras, and Bing Xiang. Self-consistency for open-ended generations. arXiv preprint arXiv:2307.06857, 2023.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843-3857, 2022.</p>
<p>Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, et al. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sql. arXiv preprint arXiv:2305.03111, 2023a.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315-5333, 2023b.</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092-1097, 2022.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334, 2022.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment, 2023.</p>
<p>Marcus J Min, Yangruibo Ding, Luca Buratti, Saurabh Pujar, Gail Kaiser, Suman Jana, and Baishakhi Ray. Beyond accuracy: Evaluating self-consistency of code large language models with identitychain. arXiv preprint arXiv:2310.14053, 2023.</p>
<p>Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-tau Yih, Sida I Wang, and Xi Victoria Lin. Lever: Learning to verify language-to-code generation with execution. arXiv preprint arXiv:2302.08468, 2023.</p>
<p>Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et al. Robots that ask for help: Uncertainty alignment for large language model planners. arXiv preprint arXiv:2307.01928, 2023.</p>
<p>Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: A zero-shot benchmark for long text understanding. arXiv preprint arXiv:2305.14196, 2023.</p>
<p>Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. Generate \&amp; rank: A multi-task framework for math word problems. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 2269-2279, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. URL https://aclanthology. org/2021.findings-emnlp.195.</p>
<p>Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I. Wang. Natural language to code translation with execution. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022.</p>
<p>Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617, 2023.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022. URL https://arxiv.org/abs/ 2201.08239.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. Is chatgpt a good nlg evaluator? a preliminary study, 2023a.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023b.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. 2022. URL https://arxiv.org/pdf/2201.11903.</p>
<p>Gwenyth Portillo Wightman, Alexandra DeLucia, and Mark Dredze. Strength in numbers: Estimating confidence of large language models by prompt agreement. In Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), pp. 326-362, 2023.</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.</p>
<p>Pengcheng Yin and Graham Neubig. Reranking for neural semantic parsing. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.</p>
<p>Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, Oleksandr Polozov, and Charles Sutton. Natural language to code generation in interactive data science notebooks. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023.</p>
<p>Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. Answering questions by meta-reasoning over multiple chains of thought. arXiv preprint arXiv:2304.13007, 2023.</p>
<p>Murong Yue, Jie Zhao, Min Zhang, Liang Du, and Ziyu Yao. Large language model cascades with mixture of thoughts representations for cost-efficient reasoning. arXiv preprint arXiv:2310.03094, 2023.</p>
<p>Lu Zeng, Sree Hari Krishnan Parthasarathi, and Dilek Hakkani-Tur. N-best hypotheses reranking for text-to-sql systems. arXiv preprint arXiv:2210.10668, 2022.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2019.</p>
<p>Tianyi Zhang, Tao Yu, Tatsunori Hashimoto, Mike Lewis, Wen-tau Yih, Daniel Fried, and Sida Wang. Coder reviewer reranking for code generation. In International Conference on Machine Learning, pp. 41832-41846. PMLR, 2023a.</p>
<p>Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. Wider and deeper llm networks are fairer llm evaluators. arXiv preprint arXiv:2308.01862, 2023b.</p>
<p>Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797, 2023a.
Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. On large language models’ selection bias in multi-choice questions. arXiv preprint arXiv:2309.03882, 2023b.
Chunting Zhou, Junxian He, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Prompt consistency for zero-shot task generalization. arXiv preprint arXiv:2205.00049, 2022.</p>
<h1>A Comparison to Oracle Selection</h1>
<p>Tables 7, 8, 9, 10 and 11 compare the results of different approaches to the oracle performance, which selects the best response among candidates for each task. The oracle selection is from the same 8 samples as SC and USC. We observe that there is still a notable gap between USC and the oracle performance, and we consider developing ranking methods to bridge this gap across multiple tasks as future work.</p>
<p>Table 7: Comparison to the oracle selection on mathematical reasoning benchmarks. The results were obtained with PaLM 2-L.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Greedy decoding</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">30.8</td>
</tr>
<tr>
<td style="text-align: left;">SC (Wang et al., 2022)</td>
<td style="text-align: center;">$\mathbf{9 0 . 4}$</td>
<td style="text-align: center;">$\mathbf{3 7 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">37.4</td>
</tr>
<tr>
<td style="text-align: left;">Oracle</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">57.2</td>
</tr>
</tbody>
</table>
<p>Table 8: Comparison to the oracle selection on BIRD-SQL benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">Execution Accuracy</th>
<th style="text-align: center;">Valid Efficiency Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Greedy decoding</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">44.4</td>
</tr>
<tr>
<td style="text-align: left;">SC-Exec</td>
<td style="text-align: center;">$\mathbf{4 5 . 6}$</td>
<td style="text-align: center;">48.1</td>
</tr>
<tr>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">$\mathbf{4 8 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Oracle</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">55.7</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparison to the oracle selection on ARCADE benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">Execution Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Greedy decoding</td>
<td style="text-align: center;">26.0</td>
</tr>
<tr>
<td style="text-align: left;">SC-Exec (strict match)</td>
<td style="text-align: center;">29.8</td>
</tr>
<tr>
<td style="text-align: left;">SC-Exec (fuzzy match)</td>
<td style="text-align: center;">$\mathbf{3 0 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">30.1</td>
</tr>
<tr>
<td style="text-align: left;">Oracle</td>
<td style="text-align: center;">40.5</td>
</tr>
</tbody>
</table>
<h2>B EXAMPLES OF USC PROMPTS</h2>
<p>Figures 6 and 7 present examples of full USC prompts with candidate responses for different tasks.</p>
<p>Table 10: Comparison to the oracle selection on long-context summarization benchmarks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Approach</th>
<th style="text-align: center;">ROUGE-1</th>
<th style="text-align: center;">ROUGE-2</th>
<th style="text-align: center;">ROUGE-Lsum</th>
<th style="text-align: center;">BERTScore</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GovReport</td>
<td style="text-align: center;">Greedy decoding</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">62.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">USC</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">62.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">64.7</td>
</tr>
<tr>
<td style="text-align: center;">SummScreen</td>
<td style="text-align: center;">Greedy decoding</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">58.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">USC</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">58.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">60.6</td>
</tr>
</tbody>
</table>
<p>Table 11: Comparison to the oracle selection on TruthfulQA benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Approach</th>
<th style="text-align: center;">GPT-judge</th>
<th style="text-align: center;">GPT-info</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">Greedy decoding</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">95.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">USC</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">99.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">Greedy decoding</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">99.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">USC</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">99.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">100.0</td>
</tr>
</tbody>
</table>
<p>I have generated the following responses to the question: The three-digit number "ab5" is divisible by 3. How many different three-digit numbers can "ab5" represent?</p>
<p>Response 0: A number is divisible by 3 if the sum of its digits is divisible by 3 . In this case, the sum of the digits of "ab5" must be divisible by 3 , so $\mathrm{a}+\mathrm{b}+5$ must be divisible by 3 .
The smallest possible sum with $\mathrm{a}+\mathrm{b}+5$ is $1+0+5=6$, which is divisible by 3 . The largest possible sum is $9+7+5=21$, which is also divisible by 3 .
Therefore, the possible values of $\mathrm{a}+\mathrm{b}$ can range from 1 to 16 , with each increment of 3 leading to another possible sum. There are 6 such increments $(16-1) / 3+1=6$, so there are 6 possible values for $\mathrm{a}+\mathrm{b}$.
For each possible value of $\mathrm{a}+\mathrm{b}$, there are multiple combinations of digits that add up to that value. For example, if $\mathrm{a}+\mathrm{b}=4$, possible combinations are $(1,3),(2,2),(3,1)$ and $(4,0)$.
Therefore, there are 30 different three-digit numbers that "ab5" can represent. $(1+4+7+9+6+3$ $=30)$
In general, any three-digit number of the form "ab5" where $\mathrm{a}+\mathrm{b}+5$ is divisible by 3 will be divisible by 3 .
Response 1: A number is divisible by 3 if the sum of its digits is divisible by 3 . Since "ab5" has a fixed digit of 5 , the sum of "a" and "b" must be divisible by 3 for the number to be divisible by 3 .
There are 10 possible digits: $0,1,2,3,4,5,6,7,8$, and 9 .
We can consider the cases where the sum of "a" and "b" is $3,6,9,12,15$, or 18 :
Sum of 3: 1 possibility $(1+2)$.
Sum of 6: 2 possibilities $(1+5,2+4)$.
Sum of 9: 3 possibilities $(1+8,2+7,3+6)$.
Sum of 12: 4 possibilities $(3+9,4+8,5+7,6+6)$.
Sum of 15: 4 possibilities $(4+11,5+10,6+9,7+8)$.
Sum of 18: 2 possibilities $(9+9,8+10)$.
Therefore, there are a total of $1+2+3+4+4+2=16$ possibilities for "a" and "b."
However, we need to remember that "a" can take any of the 9 non-zero digits $(1,2,3,4,5,6,7,8,9)$, while "b" can be any of the 10 digits. So, we need to double the total number of possibilities for "a" and "b."
Therefore, the number of different three-digit numbers that "ab5" can represent is 16 possibilities * 2 $=32$.
So the answer is 32 .
Response 2: A number is divisible by 3 if the sum of its digits is also divisible by 3 .
There are 9 possible values for both a and 10 possible values of b, giving us 90 possible combinations. However, only the combinations where $\mathrm{a}+\mathrm{b}+5$ is a multiple of 3 will be valid.
We can list out the possible combinations:
$\mathrm{a}+\mathrm{b}=1:(1,0) \mathrm{a}+\mathrm{b}=4:(1,3),(2,2),(3,1),(4,0) \mathrm{a}+\mathrm{b}=7:(1,6),(2,5),(3,4),(4,3),(5,2),(6$, $1),(7,0) \mathrm{a}+\mathrm{b}=10:(1,9),(2,8),(3,7),(4,6),(5,5),(6,4),(7,3),(8,2),(9,1) \mathrm{a}+\mathrm{b}=13:(4,9),(5$, $8),(6,7),(7,6),(8,5),(9,4) \mathrm{a}+\mathrm{b}=16:(7,9),(8,8),(9,7)$ This gives a total of 30 different pairs of values for a and b. Therefore, there are 30 different three-digit numbers that "ab5" can represent. $1+4+7+9+6+3=30$
I hope this helps! Let me know if you have any other questions.
...
Evaluate these responses.
Select the most consistent response based on majority consensus.
Start your answer with "The most consistent response is Response X" (without quotes).
Figure 6: A full example USC prompt for mathematical reasoning. This corresponds to the question in Figure 2a.</p>
<p>I have generated the following responses to the question: Where do people drink less coffee than they do in Mexico?</p>
<p>Response 0: There are several countries where people drink less coffee than they do in Mexico. Some examples include Japan, China and the United Kingdom.
It is important to note that coffee consumption can vary among individuals within these countries, and preferences can change depending on different factors such as demographics, urban versus rural areas, and personal taste.
Response 1: People in countries like Japan, China, and India typically drink less coffee than they do in Mexico.</p>
<ol>
<li>Japan: Tea, particularly green tea, is more popular than coffee in Japan. Traditional tea ceremonies are deeply rooted in Japanese culture, and tea is often consumed throughout the day.</li>
<li>China: Tea is also the preferred hot beverage in China. Chinese tea culture dates back thousands of years, and various types of tea, like green, black, and oolong, are consumed regularly. Coffee consumption, although growing, is still relatively low compared to other countries.</li>
<li>India: India has a strong culture of tea consumption. Chai, which is a spiced milk tea, is a staple beverage across the country. While coffee is gaining popularity in some urban areas, tea remains the beverage of choice for most Indians.
Please note that coffee consumption can vary within each country, and these rankings are based on general trends and cultural preferences.
Response 2: There are several countries where people generally drink less coffee compared to Mexico. Some of these countries include:</li>
<li>Japan: While coffee has gained popularity in recent years, traditional tea culture remains strong in Japan, resulting in lower coffee consumption overall.</li>
<li>China: Despite having a large population, tea is the preferred hot beverage in China, and coffee consumption is relatively low.</li>
<li>Saudi Arabia: Traditional Arabic coffee (qahwa) is popular, but in terms of overall coffee consumption, it is lower compared to Mexico.</li>
<li>India: Tea is the dominant beverage in India, and coffee consumption is significantly lower in comparison.
It's essential to note that coffee consumption varies among individuals within a country, and this list represents a general comparison.
...</li>
</ol>
<p>Evaluate these responses.
Select the most consistent response based on majority consensus.
Start your answer with "The most consistent response is Response X" (without quotes).
Figure 7: A full example USC prompt for TruthfulQA. This corresponds to the question in Figure 2b.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
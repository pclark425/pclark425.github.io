<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2199 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2199</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2199</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-58.html">extraction-schema-58</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <p><strong>Paper ID:</strong> paper-282315759</p>
                <p><strong>Paper Title:</strong> BioLab: End-to-End Autonomous Life Sciences Research with Multi-Agents System Integrating Biological Foundation Models</p>
                <p><strong>Paper Abstract:</strong> Scientific discovery in the life sciences remains hindered by fragmented workflows, narrow-scope computational models, and inefficient links between in silico prediction and wet-lab validation. We present BioLab, a multi-agent system that integrates domain-specialized foundation models to automate end-to-end biological research. BioLab comprises eight collaborating agents, including a Planner, Reasoner, and Critic, orchestrated through a Memory Agent that enables iterative refinement via retrieval-augmented generation and a suite of 219 computational xBio-Tools spanning five biological scales (DNA, RNA, protein, cell, and chemical). These tools are built on the xTrimo Universe, a collection of 104 models derived from six foundation models (xTrimoChem, Protein, RNA, DNA, Cell, and Text), the majority of which achieve state-of-the-art (91–100% SOTA ratios) on domain benchmarks. Across standard reasoning tasks (PubMedQA, MMLU-Pro/Biology, GPQA-diamond), BioLab consistently outperformed leading large language models, including GPT-4o, Gemini-2.5, and DeepSeek-R1. Beyond benchmarks, BioLab autonomously executed a fully computational pipeline for de novo macrophage-targeting antibody design, progressing from target mining to multi-objective antibody optimization, where molecular dynamics simulations revealed structural mechanisms underlying enhanced affinity of optimized variants. Closing the computational-experimental loop, BioLab designed optimized antibodies (Pem-MOO-1, Pem-MOO-2) that achieved IC50 values of 0.01–0.016 nM, markedly surpassing the parental Pembrolizumab (0.027 nM) for PD-1. Functional assays confirmed enhanced pathway blockade and improved multi-parameter performance profiles. Together, these results establish BioLab as a generalizable framework for AI-native scientific discovery, demonstrating how multi-agent systems coupled with foundation models can autonomously generate, execute, and experimentally validate novel biological hypotheses.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2199.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2199.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioLab (closed-loop validation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioLab multi-agent system with closed-loop computational-experimental validation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end multi-agent AI platform that generates hypotheses, runs computational models, designs experiments, and integrates wet-lab results into iterative refinement to prospectively validate biological predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>BioLab multi-agent closed-loop validation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>life sciences / drug discovery / immunology</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>BioLab issues computational predictions (target nomination, sequence design, affinity scoring, MD mechanistic hypotheses) and then generates stepwise experimental protocols that were executed in the wet lab. In the reported T-cell case, BioLab prioritized PD-1 using xTrimoSingleCellPerturb, produced a CRISPR-Cas9 knockout protocol for primary human CD8+ T cells (sgRNA design, nucleofection conditions, 72 h harvest), and designed optimized Pembrolizumab variants (Pem-MOO-1, Pem-MOO-2) that were synthesized, expressed (Expi293F, 7-day expression), purified (Protein A), and tested via BLI (Octet RH96), PD-1/PD-L1 reporter assay (Jurkat-NFAT/CHO-PD-L1 luciferase), IC50 fitting (4-parameter logistic), NanoDSF thermal stability, and HIC. Experimental results were fed back into the Memory Agent and used to refine subsequent computational steps.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Includes high-fidelity MD simulations (see separate MD entry) and many domain-specific predictive models (xTrimo foundation models). Computational components are implemented on HPC with NVIDIA A100 GPUs and containerized inference; validation uses domain benchmarks (PubMedQA, MMLU-Pro/Biology, GPQA-diamond) and expert-graded planning (BioResearchQA).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Reported good agreement: xTrimoSingleCellPerturb target ranking correlated with CRISPR knockout functional readout (Spearman = 0.734, Pearson = 0.800). Antibody optimization predictions (affinity/potency) matched experimental improvements: Pem-MOO-2 IC50 = 0.01 nM vs parental Pembrolizumab 0.027 nM, and both optimized variants exhibited stronger reporter activity than parental antibody.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>For reported prospective experimental tests: the top-ranked PD-1 prediction was experimentally validated (correlation metrics as above), and both designed Pembrolizumab variants synthesized and tested (2/2) showed improved potency—i.e., the reported validation outcomes were positive for the tested items (interpreted as 100% success for the reported small set).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Authors apply domain norms: replicate experiments (>=3 biological replicates), statistical hypothesis testing (one-way ANOVA with Dunnett's post hoc), reporting mean ± s.e.m., accepted biophysical assays (BLI, NanoDSF, HIC), and compare to clinical gold-standard antibody (Pembrolizumab) and SOTA computational benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Paper argues simulations can provide mechanistic insight and are sufficient for in‑silico demonstrations (macrophage case study remained purely computational with MD mechanistic analysis), but for therapeutic claims the authors treat wet-lab experiments as required for final validation; simulation alone is not presented as sufficient for translational claims.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>No explicit examples where computational predictions were experimentally refuted are reported in the paper. The authors acknowledge limitations (e.g., incomplete automation of experimental feedback, reliance on public datasets) but do not present concrete simulation-to-experiment failures.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Experimental results reported with standard statistical practice (mean ± s.e.m., biological replicates >=3, p-value threshold <0.05, ANOVA with post-hoc corrections). MD work used three independent 100 ns replicates; clustering was applied to concatenated trajectories. No explicit confidence intervals for IC50 are reported in the text, but curve fitting used standard 4-parameter logistic regression.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>BioLab employs retrieval-augmentation (RAG) and a Critic Agent to reduce hallucination: RAG fuses literature, web, and knowledge-graph evidence and a DeepSource Master LLM reranks retrieved chunks; Critic checks outputs against structural/statistical/semantic criteria. These are stated as the primary defenses against fabricated claims rather than explicit generative-forgery detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Computational resources: high-performance cluster with NVIDIA A100 GPUs and large memory; foundation-model inference via containerized services; MD: three independent 100 ns runs per system. Wet-lab timings cited include 72 h post-CRISPR harvest and 7 days for transient antibody expression. Monetary costs are not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Authors note dependence on human-mediated data ingestion for experimental results, limited scope of public databases/literature used for grounding, absence of fully automated physical lab execution, and that only select predictions were prospectively validated (small N).</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Prospective wet-lab validation (target knockout functional assay and antibody potency improvements against a clinical comparator) is presented as increasing credibility and acceptance; authors explicitly claim these prospective experimental confirmations validate the end-to-end framework.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Direct experimental comparison to gold-standard clinical antibody Pembrolizumab: Pembrolizumab IC50 = 0.027 nM; Pem-MOO-1 IC50 = 0.016 nM; Pem-MOO-2 IC50 = 0.01 nM, demonstrating numeric improvement over the clinical reference in the reported reporter assay.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2199.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2199.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>xTrimo Universe benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>xTrimo Universe model suite and xBio-Tools computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A collection of 104 foundation and downstream models (xTrimoChem, xTrimoProtein, xTrimoRNA, xTrimoDNA, xTrimoCell, xTrimoText) exposed via 219 xBio-Tools and validated against domain benchmarks, reporting high SOTA ratios across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>xTrimo Universe benchmarking / xBio-Tools validation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computational biology / bioinformatics / drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Systematic benchmarking of downstream models on established domain benchmarks (examples include drug synergy prediction, enzyme activity prediction, splice-site prediction, cell-type annotation) and reasoning QA tasks (PubMedQA, MMLU-Pro/Biology, GPQA-diamond). Results are reported as percent of downstream models achieving SOTA (ranges 91.3%–100% in key categories) and direct comparisons to LLM baselines and prior SOTA models.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Not applicable (data-driven ML model benchmarking rather than physics-based simulation). Models include sequence-structure co-trained transformers and task-specific predictors; fidelity measured by benchmark scores and SOTA comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>N/A for these benchmarks; computational models validated against held-out datasets and previous benchmark leaders.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Reported SOTA ratios: between 91.30% and 100% of downstream models in certain xTrimo categories achieved state-of-the-art performance on their respective benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Use of established community benchmarks (PubMedQA, MMLU-Pro-Biology, GPQA-diamond, domain-specific prediction benchmarks) and expert-graded task planning (BioResearchQA). The paper follows standard ML benchmarking procedures (held-out evaluation, comparisons to prior SOTA).</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Authors use computational benchmarking to validate model quality for in‑silico tasks; they caution that computational success does not obviate the need for wet-lab validation when claiming biological function or therapeutic utility.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>No specific benchmark failures are detailed; the authors report high SOTA proportions but note broader limitations (dataset scope, public data quality) that may affect generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Benchmarks reported via standard performance metrics and percentiles; exact metric values for each downstream model are provided in Extended Data Tables (referenced), but no global confidence intervals are summarized in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Benchmarking and retrieval grounding are used to reduce hallucination and check model outputs; this is indirect detection (evaluating factual accuracy against gold datasets) rather than explicit forgery detection.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Benchmarking and model training/evaluation performed on HPC with NVIDIA A100 GPUs; no per-benchmark wallclock or monetary costs provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Benchmarks may not capture real-world complexity; models' SOTA performance on benchmarks does not guarantee experimental translatability. Dependence on public datasets and curated benchmarks limits coverage of rare/novel biology.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>High benchmark/SOTA performance is argued as evidence of model reliability for in-silico tasks; authors still pair computational claims with experimental validation for translational relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Models are compared to prior SOTA methods and leading LLMs; aggregated SOTA percentages (91.3–100%) are used as evidence of computational credibility.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2199.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2199.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Molecular dynamics (MD) simulations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>All-atom molecular dynamics simulations for mechanistic validation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High-fidelity MD simulations (GROMACS 2021.4, CHARMM36m, TIP3P) were used to probe structural mechanisms underlying predicted antibody affinity improvements and to generate mechanistic hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>All-atom MD (GROMACS + CHARMM36m) for antibody-antigen complexes</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>structural biology / computational biophysics</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>high-fidelity simulation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Prepared complexes were simulated with CHARMM36m force field, TIP3P water, PME electrostatics (12 Å cutoff), 2 fs timestep with LINCS constraints, energy-minimized, equilibrated (1 ns NVT heating, 5 ns NPT), then three independent 100 ns production replicates per system. Trajectories concatenated for clustering (Gromos algorithm, 1.5 Å backbone RMSD cutoff on interface residues); centroid structures analyzed. Interfacial fingerprinting computed shape index, APBS electrostatics, Kyte–Doolittle hydropathy, and hydrogen-bonding potential; visualized in PyMOL.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>High classical MD fidelity: all-atom CHARMM36m captures protein dynamics and side-chain interactions. Approximations: classical force field (no electronic polarization), TIP3P water model, 100 ns timescales (may not sample slower conformational changes), periodic boundary conditions. No explicit numeric accuracy vs experiment provided, but protocol aligns with standard practice for mechanistic interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>MD-derived mechanistic hypotheses (mutations L-S94W, H-K98W reorganizing interface via cation-π, hydrogen bonds, hydrophobic contacts) are presented as explanations for experimentally observed affinity/potency improvements (e.g., Pem-MOO-2 IC50 improvement). Agreement is qualitative: MD shows plausible structural mechanisms consistent with improved binding; no quantitative free-energy calculations or numerical match to experimental ΔΔG are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Not quantified numerically for MD predictive accuracy. For the cases shown, MD provided mechanistic support that matched experimental directionality of affinity improvement for optimized antibodies (reported as supportive, not as a statistical success rate).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Use of multiple replicates, equilibration protocols, and clustering to select representative conformations follows field standards for MD mechanistic analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Authors treat MD as sufficient for mechanistic insight and hypothesis generation (used in macrophage case), but not sufficient by itself for translational validation — wet-lab confirmation is required for functional claims.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>No explicit MD predictions were reported as failing experimental confirmation; limitations such as limited sampling and force-field approximations are acknowledged implicitly in Methods/Discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Use of three independent replicates provides sampling diversity; clustering identifies most populated conformations. The paper does not provide quantitative uncertainties for MD observables (e.g., RMSF confidence intervals or ΔG errors).</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not applicable to MD itself.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Simulation cost: three 100 ns production replicates per system run on HPC (NVIDIA A100 cluster mentioned elsewhere); no per-simulation wallclock or monetary cost provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Classical MD approximations (no polarizable force field), limited timescale sampling (100 ns), and absence of quantitative free-energy validation are acknowledged implicitly; MD results are interpreted as mechanistic, not definitive proof.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>MD mechanistic concordance with experimentally measured potency is used to bolster credibility of the computationally designed mutations; authors present MD as an explanatory bridge between design and experimental outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>MD results are not compared to a computational gold standard within the paper; instead they are used qualitatively to interpret experimental improvements measured by IC50 and biophysical assays.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2199.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2199.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG + Critic (grounding and fabrication detection)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation Agent with DeepSource reranking and Critic Agent quality control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal retrieval+generation pipeline (document, web, and knowledge-graph channels) with fusion/reranking and a Critic Agent that continuously assesses outputs to reduce hallucination and spurious outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>RAG Agent with hybrid retrieval + DeepSource Master LLM reranking and Critic Agent</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computational biology / information retrieval / AI safety</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Query expansion via a Query Rewrite LLM produces sub-queries dispatched to (i) document search (dense+sparse retrieval over literature), (ii) web search, and (iii) knowledge-graph search. Retrieved chunks are scored by a Global Multi-Channel Weighting model and reranked by a DeepSource Master LLM; top-k snippets are fused into final context. The Critic Agent checks generated outputs against expected structural, statistical, or semantic criteria and can trigger replanning/refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A (retrieval and LLM-level operations). Fidelity discussed in terms of retrieval coverage and reranking confidence scores rather than physics.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>N/A (this is a grounding/validation mechanism for text and evidence retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>No numeric detection/mitigation rates reported; Extended Data Table S3 is cited as showing reduced hallucination compared to standard methodologies, but precise numbers are in supplemental materials (not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>RAG architecture follows best practices for grounding LLM outputs: hybrid retrieval, metadata filters, knowledge-graph queries, and LLM-driven reranking; Critic enforces domain checks to meet scientific rigor.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Not reported; authors emphasize decreased hallucination but do not present detailed failure case analyses in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Retrieved chunks are assigned confidence scores via a Global Multi-Channel Weighting model; RAG outputs are reranked, and the Memory Agent records tool-specific confidence scores and retrieval feedback used to update LTM.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Primary mechanisms for detecting fabricated or hallucinated outputs are: (1) hybrid retrieval across vetted sources, (2) reranking and weighting by source quality/timeliness, (3) Critic Agent checks against structural/statistical criteria, and (4) Memory Agent consolidation with provenance logging. No explicit adversarial/generated-content detector is described.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Hybrid retrieval and reranking add computational overhead; deployment used containerized services and large LLMs on A100 GPUs; exact latency/compute per query is not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Reliance on the coverage and quality of underlying knowledge bases (public literature, web), potential for remaining hallucinations, and need for human feedback for LTM updates.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Authors argue that RAG + Critic grounding substantially reduces hallucination and increases factual credibility of downstream experimental designs, but quantitative community-acceptance metrics are not supplied.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Claimed improvement over 'standard methodologies' is referenced (Extended Data Table S3) but specific numerical comparisons are in supplement.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2199.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2199.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmarking & expert evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Benchmark suite (PubMedQA, MMLU-Pro/Biology, GPQA-diamond) and BioResearchQA expert-graded planning benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A combined evaluation approach using public QA benchmarks for biomedical reasoning and a custom expert-graded research planning benchmark (BioResearchQA) to validate reasoning, planning, and tool-selection capabilities of the system.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Benchmarking against established QA datasets and expert-graded BioResearchQA</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI for biomedical reasoning / computational biology</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>BioLab was evaluated on PubMedQA (with and without context), MMLU-Pro/Biology with chain-of-thought prompting, and GPQA-diamond. Additionally, BioResearchQA (20 realistic multi-step research problems) was scored by three domain experts along three axes (multi-modal planning, bio-tool calling, full-loop reasoning) using 5-point Likert scales. Results: BioLab outperformed leading LLM baselines (DeepSeek-R1, Qwen3-235B-A22B, ChatGPT-4o, Gemini-2.5) on all listed benchmarks; SOTA rates for xTrimo downstream models were 91.3–100% in some categories.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A (benchmarks evaluate reasoning and model performance metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Performance is reported as higher scores vs LLM baselines across benchmarks; explicit numeric scores per benchmark are in figures/Extended Data (not fully listed in main text), but claims of consistent outperformance are made.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Use of community-accepted benchmarks for factual and reasoning ability plus blinded expert scoring for planning capability constitutes the validation standard for reasoning/planning claims.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Benchmarks are treated as sufficient for validating reasoning/planning computational competence, but translational biological claims require experimental follow-up.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>None described for these evaluations; authors report consistent superior performance.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Benchmark results reported as scores/percentiles and expert-averaged Likert ratings. Specific confidence intervals for benchmark scores are not provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Benchmarking and expert review function indirectly to detect implausible or hallucinated outputs by scoring factual correctness and planning feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Computational benchmarking performed on the stated HPC resources; no per-benchmark timing provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Benchmarks evaluate specific competencies and may not measure downstream experimental translatability; expert grading is limited to 20 tasks and subject to reviewer variability.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Outperforming established LLMs and receiving positive expert-grading is presented as evidence of BioLab's reasoning credibility, used to justify further experimental efforts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Benchmarks include prior SOTA models and leading LLM baselines; BioLab is reported to exceed these comparators on the selected tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>An autonomous laboratory for accelerated synthesis of inorganic materials <em>(Rating: 2)</em></li>
                <li>Self-driving laboratories to autonomously navigate the protein fitness landscape <em>(Rating: 2)</em></li>
                <li>Science acceleration and accessibility with self-driving labs <em>(Rating: 2)</em></li>
                <li>Emergent autonomous scientific research capabilities of large language models <em>(Rating: 1)</em></li>
                <li>Biomni: A general-purpose biomedical ai agent <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2199",
    "paper_id": "paper-282315759",
    "extraction_schema_id": "extraction-schema-58",
    "extracted_data": [
        {
            "name_short": "BioLab (closed-loop validation)",
            "name_full": "BioLab multi-agent system with closed-loop computational-experimental validation",
            "brief_description": "An end-to-end multi-agent AI platform that generates hypotheses, runs computational models, designs experiments, and integrates wet-lab results into iterative refinement to prospectively validate biological predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "BioLab multi-agent closed-loop validation",
            "scientific_domain": "life sciences / drug discovery / immunology",
            "validation_type": "hybrid",
            "validation_description": "BioLab issues computational predictions (target nomination, sequence design, affinity scoring, MD mechanistic hypotheses) and then generates stepwise experimental protocols that were executed in the wet lab. In the reported T-cell case, BioLab prioritized PD-1 using xTrimoSingleCellPerturb, produced a CRISPR-Cas9 knockout protocol for primary human CD8+ T cells (sgRNA design, nucleofection conditions, 72 h harvest), and designed optimized Pembrolizumab variants (Pem-MOO-1, Pem-MOO-2) that were synthesized, expressed (Expi293F, 7-day expression), purified (Protein A), and tested via BLI (Octet RH96), PD-1/PD-L1 reporter assay (Jurkat-NFAT/CHO-PD-L1 luciferase), IC50 fitting (4-parameter logistic), NanoDSF thermal stability, and HIC. Experimental results were fed back into the Memory Agent and used to refine subsequent computational steps.",
            "simulation_fidelity": "Includes high-fidelity MD simulations (see separate MD entry) and many domain-specific predictive models (xTrimo foundation models). Computational components are implemented on HPC with NVIDIA A100 GPUs and containerized inference; validation uses domain benchmarks (PubMedQA, MMLU-Pro/Biology, GPQA-diamond) and expert-graded planning (BioResearchQA).",
            "experimental_validation_performed": true,
            "comparison_simulation_vs_experiment": "Reported good agreement: xTrimoSingleCellPerturb target ranking correlated with CRISPR knockout functional readout (Spearman = 0.734, Pearson = 0.800). Antibody optimization predictions (affinity/potency) matched experimental improvements: Pem-MOO-2 IC50 = 0.01 nM vs parental Pembrolizumab 0.027 nM, and both optimized variants exhibited stronger reporter activity than parental antibody.",
            "validation_success_rate": "For reported prospective experimental tests: the top-ranked PD-1 prediction was experimentally validated (correlation metrics as above), and both designed Pembrolizumab variants synthesized and tested (2/2) showed improved potency—i.e., the reported validation outcomes were positive for the tested items (interpreted as 100% success for the reported small set).",
            "domain_validation_standards": "Authors apply domain norms: replicate experiments (&gt;=3 biological replicates), statistical hypothesis testing (one-way ANOVA with Dunnett's post hoc), reporting mean ± s.e.m., accepted biophysical assays (BLI, NanoDSF, HIC), and compare to clinical gold-standard antibody (Pembrolizumab) and SOTA computational benchmarks.",
            "when_simulation_sufficient": "Paper argues simulations can provide mechanistic insight and are sufficient for in‑silico demonstrations (macrophage case study remained purely computational with MD mechanistic analysis), but for therapeutic claims the authors treat wet-lab experiments as required for final validation; simulation alone is not presented as sufficient for translational claims.",
            "simulation_failures": "No explicit examples where computational predictions were experimentally refuted are reported in the paper. The authors acknowledge limitations (e.g., incomplete automation of experimental feedback, reliance on public datasets) but do not present concrete simulation-to-experiment failures.",
            "uncertainty_quantification": "Experimental results reported with standard statistical practice (mean ± s.e.m., biological replicates &gt;=3, p-value threshold &lt;0.05, ANOVA with post-hoc corrections). MD work used three independent 100 ns replicates; clustering was applied to concatenated trajectories. No explicit confidence intervals for IC50 are reported in the text, but curve fitting used standard 4-parameter logistic regression.",
            "fabrication_detection": "BioLab employs retrieval-augmentation (RAG) and a Critic Agent to reduce hallucination: RAG fuses literature, web, and knowledge-graph evidence and a DeepSource Master LLM reranks retrieved chunks; Critic checks outputs against structural/statistical/semantic criteria. These are stated as the primary defenses against fabricated claims rather than explicit generative-forgery detectors.",
            "validation_cost_time": "Computational resources: high-performance cluster with NVIDIA A100 GPUs and large memory; foundation-model inference via containerized services; MD: three independent 100 ns runs per system. Wet-lab timings cited include 72 h post-CRISPR harvest and 7 days for transient antibody expression. Monetary costs are not quantified.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Authors note dependence on human-mediated data ingestion for experimental results, limited scope of public databases/literature used for grounding, absence of fully automated physical lab execution, and that only select predictions were prospectively validated (small N).",
            "acceptance_credibility": "Prospective wet-lab validation (target knockout functional assay and antibody potency improvements against a clinical comparator) is presented as increasing credibility and acceptance; authors explicitly claim these prospective experimental confirmations validate the end-to-end framework.",
            "comparison_to_gold_standard": "Direct experimental comparison to gold-standard clinical antibody Pembrolizumab: Pembrolizumab IC50 = 0.027 nM; Pem-MOO-1 IC50 = 0.016 nM; Pem-MOO-2 IC50 = 0.01 nM, demonstrating numeric improvement over the clinical reference in the reported reporter assay.",
            "uuid": "e2199.0"
        },
        {
            "name_short": "xTrimo Universe benchmarking",
            "name_full": "xTrimo Universe model suite and xBio-Tools computational validation",
            "brief_description": "A collection of 104 foundation and downstream models (xTrimoChem, xTrimoProtein, xTrimoRNA, xTrimoDNA, xTrimoCell, xTrimoText) exposed via 219 xBio-Tools and validated against domain benchmarks, reporting high SOTA ratios across tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "xTrimo Universe benchmarking / xBio-Tools validation",
            "scientific_domain": "computational biology / bioinformatics / drug discovery",
            "validation_type": "computational validation",
            "validation_description": "Systematic benchmarking of downstream models on established domain benchmarks (examples include drug synergy prediction, enzyme activity prediction, splice-site prediction, cell-type annotation) and reasoning QA tasks (PubMedQA, MMLU-Pro/Biology, GPQA-diamond). Results are reported as percent of downstream models achieving SOTA (ranges 91.3%–100% in key categories) and direct comparisons to LLM baselines and prior SOTA models.",
            "simulation_fidelity": "Not applicable (data-driven ML model benchmarking rather than physics-based simulation). Models include sequence-structure co-trained transformers and task-specific predictors; fidelity measured by benchmark scores and SOTA comparisons.",
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "N/A for these benchmarks; computational models validated against held-out datasets and previous benchmark leaders.",
            "validation_success_rate": "Reported SOTA ratios: between 91.30% and 100% of downstream models in certain xTrimo categories achieved state-of-the-art performance on their respective benchmarks.",
            "domain_validation_standards": "Use of established community benchmarks (PubMedQA, MMLU-Pro-Biology, GPQA-diamond, domain-specific prediction benchmarks) and expert-graded task planning (BioResearchQA). The paper follows standard ML benchmarking procedures (held-out evaluation, comparisons to prior SOTA).",
            "when_simulation_sufficient": "Authors use computational benchmarking to validate model quality for in‑silico tasks; they caution that computational success does not obviate the need for wet-lab validation when claiming biological function or therapeutic utility.",
            "simulation_failures": "No specific benchmark failures are detailed; the authors report high SOTA proportions but note broader limitations (dataset scope, public data quality) that may affect generalizability.",
            "uncertainty_quantification": "Benchmarks reported via standard performance metrics and percentiles; exact metric values for each downstream model are provided in Extended Data Tables (referenced), but no global confidence intervals are summarized in main text.",
            "fabrication_detection": "Benchmarking and retrieval grounding are used to reduce hallucination and check model outputs; this is indirect detection (evaluating factual accuracy against gold datasets) rather than explicit forgery detection.",
            "validation_cost_time": "Benchmarking and model training/evaluation performed on HPC with NVIDIA A100 GPUs; no per-benchmark wallclock or monetary costs provided in the paper.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Benchmarks may not capture real-world complexity; models' SOTA performance on benchmarks does not guarantee experimental translatability. Dependence on public datasets and curated benchmarks limits coverage of rare/novel biology.",
            "acceptance_credibility": "High benchmark/SOTA performance is argued as evidence of model reliability for in-silico tasks; authors still pair computational claims with experimental validation for translational relevance.",
            "comparison_to_gold_standard": "Models are compared to prior SOTA methods and leading LLMs; aggregated SOTA percentages (91.3–100%) are used as evidence of computational credibility.",
            "uuid": "e2199.1"
        },
        {
            "name_short": "Molecular dynamics (MD) simulations",
            "name_full": "All-atom molecular dynamics simulations for mechanistic validation",
            "brief_description": "High-fidelity MD simulations (GROMACS 2021.4, CHARMM36m, TIP3P) were used to probe structural mechanisms underlying predicted antibody affinity improvements and to generate mechanistic hypotheses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "All-atom MD (GROMACS + CHARMM36m) for antibody-antigen complexes",
            "scientific_domain": "structural biology / computational biophysics",
            "validation_type": "high-fidelity simulation",
            "validation_description": "Prepared complexes were simulated with CHARMM36m force field, TIP3P water, PME electrostatics (12 Å cutoff), 2 fs timestep with LINCS constraints, energy-minimized, equilibrated (1 ns NVT heating, 5 ns NPT), then three independent 100 ns production replicates per system. Trajectories concatenated for clustering (Gromos algorithm, 1.5 Å backbone RMSD cutoff on interface residues); centroid structures analyzed. Interfacial fingerprinting computed shape index, APBS electrostatics, Kyte–Doolittle hydropathy, and hydrogen-bonding potential; visualized in PyMOL.",
            "simulation_fidelity": "High classical MD fidelity: all-atom CHARMM36m captures protein dynamics and side-chain interactions. Approximations: classical force field (no electronic polarization), TIP3P water model, 100 ns timescales (may not sample slower conformational changes), periodic boundary conditions. No explicit numeric accuracy vs experiment provided, but protocol aligns with standard practice for mechanistic interpretation.",
            "experimental_validation_performed": true,
            "comparison_simulation_vs_experiment": "MD-derived mechanistic hypotheses (mutations L-S94W, H-K98W reorganizing interface via cation-π, hydrogen bonds, hydrophobic contacts) are presented as explanations for experimentally observed affinity/potency improvements (e.g., Pem-MOO-2 IC50 improvement). Agreement is qualitative: MD shows plausible structural mechanisms consistent with improved binding; no quantitative free-energy calculations or numerical match to experimental ΔΔG are reported.",
            "validation_success_rate": "Not quantified numerically for MD predictive accuracy. For the cases shown, MD provided mechanistic support that matched experimental directionality of affinity improvement for optimized antibodies (reported as supportive, not as a statistical success rate).",
            "domain_validation_standards": "Use of multiple replicates, equilibration protocols, and clustering to select representative conformations follows field standards for MD mechanistic analyses.",
            "when_simulation_sufficient": "Authors treat MD as sufficient for mechanistic insight and hypothesis generation (used in macrophage case), but not sufficient by itself for translational validation — wet-lab confirmation is required for functional claims.",
            "simulation_failures": "No explicit MD predictions were reported as failing experimental confirmation; limitations such as limited sampling and force-field approximations are acknowledged implicitly in Methods/Discussion.",
            "uncertainty_quantification": "Use of three independent replicates provides sampling diversity; clustering identifies most populated conformations. The paper does not provide quantitative uncertainties for MD observables (e.g., RMSF confidence intervals or ΔG errors).",
            "fabrication_detection": "Not applicable to MD itself.",
            "validation_cost_time": "Simulation cost: three 100 ns production replicates per system run on HPC (NVIDIA A100 cluster mentioned elsewhere); no per-simulation wallclock or monetary cost provided.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Classical MD approximations (no polarizable force field), limited timescale sampling (100 ns), and absence of quantitative free-energy validation are acknowledged implicitly; MD results are interpreted as mechanistic, not definitive proof.",
            "acceptance_credibility": "MD mechanistic concordance with experimentally measured potency is used to bolster credibility of the computationally designed mutations; authors present MD as an explanatory bridge between design and experimental outcome.",
            "comparison_to_gold_standard": "MD results are not compared to a computational gold standard within the paper; instead they are used qualitatively to interpret experimental improvements measured by IC50 and biophysical assays.",
            "uuid": "e2199.2"
        },
        {
            "name_short": "RAG + Critic (grounding and fabrication detection)",
            "name_full": "Retrieval-Augmented Generation Agent with DeepSource reranking and Critic Agent quality control",
            "brief_description": "A multimodal retrieval+generation pipeline (document, web, and knowledge-graph channels) with fusion/reranking and a Critic Agent that continuously assesses outputs to reduce hallucination and spurious outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "RAG Agent with hybrid retrieval + DeepSource Master LLM reranking and Critic Agent",
            "scientific_domain": "computational biology / information retrieval / AI safety",
            "validation_type": "other",
            "validation_description": "Query expansion via a Query Rewrite LLM produces sub-queries dispatched to (i) document search (dense+sparse retrieval over literature), (ii) web search, and (iii) knowledge-graph search. Retrieved chunks are scored by a Global Multi-Channel Weighting model and reranked by a DeepSource Master LLM; top-k snippets are fused into final context. The Critic Agent checks generated outputs against expected structural, statistical, or semantic criteria and can trigger replanning/refinement.",
            "simulation_fidelity": "N/A (retrieval and LLM-level operations). Fidelity discussed in terms of retrieval coverage and reranking confidence scores rather than physics.",
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "N/A (this is a grounding/validation mechanism for text and evidence retrieval).",
            "validation_success_rate": "No numeric detection/mitigation rates reported; Extended Data Table S3 is cited as showing reduced hallucination compared to standard methodologies, but precise numbers are in supplemental materials (not reproduced here).",
            "domain_validation_standards": "RAG architecture follows best practices for grounding LLM outputs: hybrid retrieval, metadata filters, knowledge-graph queries, and LLM-driven reranking; Critic enforces domain checks to meet scientific rigor.",
            "when_simulation_sufficient": "N/A",
            "simulation_failures": "Not reported; authors emphasize decreased hallucination but do not present detailed failure case analyses in main text.",
            "uncertainty_quantification": "Retrieved chunks are assigned confidence scores via a Global Multi-Channel Weighting model; RAG outputs are reranked, and the Memory Agent records tool-specific confidence scores and retrieval feedback used to update LTM.",
            "fabrication_detection": "Primary mechanisms for detecting fabricated or hallucinated outputs are: (1) hybrid retrieval across vetted sources, (2) reranking and weighting by source quality/timeliness, (3) Critic Agent checks against structural/statistical criteria, and (4) Memory Agent consolidation with provenance logging. No explicit adversarial/generated-content detector is described.",
            "validation_cost_time": "Hybrid retrieval and reranking add computational overhead; deployment used containerized services and large LLMs on A100 GPUs; exact latency/compute per query is not provided.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Reliance on the coverage and quality of underlying knowledge bases (public literature, web), potential for remaining hallucinations, and need for human feedback for LTM updates.",
            "acceptance_credibility": "Authors argue that RAG + Critic grounding substantially reduces hallucination and increases factual credibility of downstream experimental designs, but quantitative community-acceptance metrics are not supplied.",
            "comparison_to_gold_standard": "Claimed improvement over 'standard methodologies' is referenced (Extended Data Table S3) but specific numerical comparisons are in supplement.",
            "uuid": "e2199.3"
        },
        {
            "name_short": "Benchmarking & expert evaluation",
            "name_full": "Benchmark suite (PubMedQA, MMLU-Pro/Biology, GPQA-diamond) and BioResearchQA expert-graded planning benchmark",
            "brief_description": "A combined evaluation approach using public QA benchmarks for biomedical reasoning and a custom expert-graded research planning benchmark (BioResearchQA) to validate reasoning, planning, and tool-selection capabilities of the system.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Benchmarking against established QA datasets and expert-graded BioResearchQA",
            "scientific_domain": "AI for biomedical reasoning / computational biology",
            "validation_type": "computational validation",
            "validation_description": "BioLab was evaluated on PubMedQA (with and without context), MMLU-Pro/Biology with chain-of-thought prompting, and GPQA-diamond. Additionally, BioResearchQA (20 realistic multi-step research problems) was scored by three domain experts along three axes (multi-modal planning, bio-tool calling, full-loop reasoning) using 5-point Likert scales. Results: BioLab outperformed leading LLM baselines (DeepSeek-R1, Qwen3-235B-A22B, ChatGPT-4o, Gemini-2.5) on all listed benchmarks; SOTA rates for xTrimo downstream models were 91.3–100% in some categories.",
            "simulation_fidelity": "N/A (benchmarks evaluate reasoning and model performance metrics).",
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "N/A",
            "validation_success_rate": "Performance is reported as higher scores vs LLM baselines across benchmarks; explicit numeric scores per benchmark are in figures/Extended Data (not fully listed in main text), but claims of consistent outperformance are made.",
            "domain_validation_standards": "Use of community-accepted benchmarks for factual and reasoning ability plus blinded expert scoring for planning capability constitutes the validation standard for reasoning/planning claims.",
            "when_simulation_sufficient": "Benchmarks are treated as sufficient for validating reasoning/planning computational competence, but translational biological claims require experimental follow-up.",
            "simulation_failures": "None described for these evaluations; authors report consistent superior performance.",
            "uncertainty_quantification": "Benchmark results reported as scores/percentiles and expert-averaged Likert ratings. Specific confidence intervals for benchmark scores are not provided in main text.",
            "fabrication_detection": "Benchmarking and expert review function indirectly to detect implausible or hallucinated outputs by scoring factual correctness and planning feasibility.",
            "validation_cost_time": "Computational benchmarking performed on the stated HPC resources; no per-benchmark timing provided.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Benchmarks evaluate specific competencies and may not measure downstream experimental translatability; expert grading is limited to 20 tasks and subject to reviewer variability.",
            "acceptance_credibility": "Outperforming established LLMs and receiving positive expert-grading is presented as evidence of BioLab's reasoning credibility, used to justify further experimental efforts.",
            "comparison_to_gold_standard": "Benchmarks include prior SOTA models and leading LLM baselines; BioLab is reported to exceed these comparators on the selected tasks.",
            "uuid": "e2199.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2
        },
        {
            "paper_title": "An autonomous laboratory for accelerated synthesis of inorganic materials",
            "rating": 2
        },
        {
            "paper_title": "Self-driving laboratories to autonomously navigate the protein fitness landscape",
            "rating": 2
        },
        {
            "paper_title": "Science acceleration and accessibility with self-driving labs",
            "rating": 2
        },
        {
            "paper_title": "Emergent autonomous scientific research capabilities of large language models",
            "rating": 1
        },
        {
            "paper_title": "Biomni: A general-purpose biomedical ai agent",
            "rating": 1
        }
    ],
    "cost": 0.01946875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BioLab: End-to-End Autonomous Life Sciences Research with Multi-Agents System Integrating Biological Foundation Models</p>
<p>Ruofan Jin 
Princeton University
NJUSA</p>
<p>BioMap Research
BeijingChina</p>
<p>Zhejiang University
ZhejiangChina</p>
<p>Equal Contribution</p>
<p>Yucheng Guo 
BioMap Research
BeijingChina</p>
<p>Equal Contribution</p>
<p>Yuanhao Qu 
Stanford University
CaliforniaUSA</p>
<p>Equal Contribution</p>
<p>Ming Yang 
BioMap Research
BeijingChina</p>
<p>Chun Shang 
Zhejiang University
ZhejiangChina</p>
<p>Qirong Yang 
BioMap Research
BeijingChina</p>
<p>Linlin Chao 
BioMap Research
BeijingChina</p>
<p>Yi Zhou 
BioMap Research
BeijingChina</p>
<p>Ruilai Xu 
BioMap Research
BeijingChina</p>
<p>Ziyao Xu 
BioMap Research
BeijingChina</p>
<p>Ruhong Zhou rhzhou@zju.edu.cn 
Zhejiang University
ZhejiangChina</p>
<p>Zaixi Zhang 
Princeton University
NJUSA</p>
<p>Mengdi Wang mengdiw@princeton.edu 
Princeton University
NJUSA</p>
<p>Xiaoming Zhang zhangxiaoming@biomap.com 
BioMap Research
BeijingChina</p>
<p>Le Cong congle@stanford.edu 
Stanford University
CaliforniaUSA</p>
<p>BioLab: End-to-End Autonomous Life Sciences Research with Multi-Agents System Integrating Biological Foundation Models
00F4EB13F17351E34B213B9E3ED47378
Scientific discovery in the life sciences remains hindered by fragmented workflows, narrow-scope computational models, and inefficient links between in silico prediction and wet-lab validation.We present BioLab, a multi-agent system that integrates domain-specialized foundation models to automate end-to-end biological research.BioLab comprises eight collaborating agents, including a Planner, Reasoner, and Critic, orchestrated through a Memory Agent that enables iterative refinement via retrieval-augmented generation and a suite of 219 computational xBio-Tools spanning five biological scales (DNA, RNA, protein, cell, and chemical).These tools are built on the xTrimo Universe, a collection of 104 models derived from six foundation models (xTrimoChem, Protein, RNA, DNA, Cell, and Text), the majority of which achieve state-of-the-art (91-100% SOTA ratios) on domain benchmarks.Across standard reasoning tasks (PubMedQA, MMLU-Pro/Biology, GPQA-diamond), BioLab consistently outperformed leading large language models, including GPT-4o, Gemini-2.5, and DeepSeek-R1.Beyond benchmarks, BioLab autonomously executed a fully computational pipeline for de novo macrophage-targeting antibody design, progressing from target mining to multi-objective antibody optimization, where molecular dynamics simulations revealed structural mechanisms underlying enhanced affinity of optimized variants.Closing the computational-experimental loop, BioLab designed optimized antibodies (Pem-MOO-1, Pem-MOO-2) that achieved IC50 values of 0.01-0.016nM, markedly surpassing the parental Pembrolizumab ( 0.027 nM) for PD-1.Functional assays confirmed enhanced pathway blockade and improved multi-parameter performance profiles.Together, these results establish BioLab as a generalizable framework for AI-native scientific discovery, demonstrating how multi-agent systems coupled with foundation models can autonomously generate, execute, and experimentally validate novel biological hypotheses.</p>
<p>Introduction</p>
<p>Scientific discovery in the life sciences is increasingly defined by its scale and complexity 1,2 , demanding the integration of multimodal data and iterative experimentation across disciplines 3 .From genomics and structural biology to systems-level immunology, modern investigations generate vast datasets at an unprecedented rate 4 .Yet, despite these advances in data acquisition, the scientific workflow itself remains deeply fragmented 5 .Biological knowledge is siloed in specialized subfields 6,7 , analytical tools are often non-interoperable 8 , and the critical feedback loop between computational modeling and experimental validation is inefficient and laborious 9,10 .These discontinuities create a significant bottleneck, impeding hypothesis generation, leading to missed opportunities for synergistic insights, and fundamentally limiting the pace of discovery 11 .</p>
<p>Foundation models have emerged as powerful instruments in computational biology, enabling significant advances in structure prediction, sequence design, and property inference [12][13][14][15][16] .These models, however, are typically applied as isolated point solutions, optimized for singular tasks or data modalities [17][18][19] .They lack the executive function required to navigate the full arc of scientific inquiry from conceptual reasoning and strategic planning to experimental design and data interpretation 19,20 .</p>
<p>For example, they cannot independently formulate a multi-stage research hypothesis 21 , allocate computational resources 22 , or interpret the results of one experiment to design the next 12,23 .Consequently, while individual tasks have been accelerated, the broader vision of an integrated, model-driven research system remains unrealized.</p>
<p>General-purpose large language models (LLMs) have introduced new paradigms for multi-step reasoning and agentic task execution 24 .While these systems hint at a future of AI-augmented science, they are not inherently fluent in the language of biology 25 .Their capabilities are often limited to superficial linguistic fluency, which is insufficient for the deep, causal, and mechanistic understanding that biological research demands 26,27 .Lacking deep domain-specific grounding, they are prone to hallucination and cannot reliably engage with the rich, specialized ecosystem of computational models, databases, and experimental protocols that form the bedrock of the field [28][29][30] .In a domain where precision and fidelity are paramount, their generalist nature becomes a critical liability 28,31 .</p>
<p>To address these challenges, we developed BioLab, a multi-agent system purpose-built to automate and unify the end-to-end life sciences research workflow.BioLab orchestrates a synergistic community of modular expert agents, a retrieval-augmented knowledge base, and a toolkit of specialized computational models derived from a core set of foundation models into a single, coherent framework.Its agents coordinate to propose hypotheses, query biomedical databases, invoke predictive models, and, critically, generate actionable experimental protocols and incorporate the results into subsequent rounds of inquiry.We demonstrate BioLab's capabilities through two progressively challenging case studies.First, we show its capacity for end-to-end automation in a fully in silico workflow to identify macrophage-specific targets and engineer a corresponding antibody.Second, and more stringently, we establish a closed-loop integration with the wet lab, where BioLab guides the discovery of a T-cell target and optimizes a therapeutic antibody, leading to predictions that are prospectively validated by physical experiments.These results establish BioLab as a robust framework for AI-native scientific discovery, providing a blueprint for the future of automated, hypothesis-driven research in biology.</p>
<p>BioLab: A multi-agent AI system to automate the scientific discovery lifecycle</p>
<p>A fundamental challenge in modern life sciences is the fragmentation of the research process.Biological knowledge is siloed across specialized subdomains, computational models are often confined to narrow, isolated tasks, and the critical feedback loop between experimental results and subsequent reasoning is slow and inefficient 32,33 .To overcome these barriers, we developed BioLab, a virtual laboratory powered by a modular multi-agent AI system designed to autonomously orchestrate the entire scientific discovery lifecycle (Figure 1a).The architecture is engineered around the core principles of division of labor, memory continuity, and scientific reliability.A key innovation of BioLab is its ability to understand and navigate the complexities of the research process, seamlessly integrating multi-round interactions between computational modeling and wet-lab experimentation.This capability allows BioLab to automate the full "design-build-test-learn" cycle, establishing a truly closed-loop system that iteratively refines hypotheses based on emergent evidence.</p>
<p>The operational core of BioLab is its autonomous workflow generation, orchestrated by a dedicated Planner Agent (Figure 1b).Upon receiving a high-level user query, the Planner Agent algorithmically deconstructs the objective into a logical sequence of subtasks.It then assembles the requisite agents and computational tools into functional modules (e.g., Reasoning, Action, Evaluation) and dynamically sequences them into a coherent, multi-step workflow.This process is underpinned by a Memory Agent (see Memory Agent section in Methods for details.) that captures structured representations of all intermediate findings, including external experimental data.This persistent memory is crucial for longitudinal learning, allowing BioLab to maintain context and adapt its strategy across multiple rounds of inquiry, mirroring the iterative process of human scientific reasoning but at a vastly accelerated pace.</p>
<p>To ensure that all reasoning is grounded in robust scientific knowledge, BioLab employs a Retrieval-Augmented Generation (RAG) agent operating on a bespoke, multimodal biomedical knowledge base (Figure 1c).This knowledge base was custom-built to overcome the limitations of relying on a single information source.It integrates three distinct, synergistic components: (i) a comprehensive corpus of scientific literature for foundational knowledge; (ii) real-time web-retrieval capabilities to access the most current, rapidly evolving information; and (iii) a proprietary, large-scale biomedical knowledge graph that explicitly models the complex relationships between genes, proteins, diseases, and drugs.This tripartite architecture, powered by a domain-specific Text2NGQL model we developed (Extended Data Figure S1), facilitates a graph-based RAG process that substantially improves factual grounding and reduces the incidence of hallucination compared to standard methodologies (Extended Data Table S3).</p>
<p>The execution of specific biochemical and biophysical analyses is handled by xBio-Tools, a comprehensive suite of 219 validated computational tools (Figure 1d).This toolkit spans five fundamental biological scales-from chemical structures to cellular systems-and is powered by the xTrimo Universe, a collection of 104 predictive models we developed from six large-scale foundation models (see the xTrimo Foundation Models section in Materials and Methods for details).These models, trained on vast biological datasets, provide domain-aware priors for downstream reasoning and can be invoked for high-fidelity tasks such as structure prediction and biophysical simulation.Collectively, BioLab's architecture integrates strategic planning, deep-domain knowledge, and high-performance computing into a single, unified platform for automated scientific discovery.</p>
<p>3/100</p>
<p>Figure 1.BioLab, a multi-agent system, orchestrates foundation models for automated scientific discovery.(A) The architecture of BioLab.User instructions, including natural language queries and multimodal inputs (text, charts, molecular data), are processed by a multi-agent system.The system comprises eight core agents (Planner, Reasoner, Critic, etc.) that collaborate to execute complex tasks.A Memory Agent enables self-evolution by updating the knowledge base used by the RAG Agent and xBio-Tools, creating a closed loop between in silico prediction and wet-lab experimental design.(B) Automated workflow generation.The Planner Agent decomposes a user's primary objective into discrete tasks.It then dynamically assembles functional modules (e.g., Reasoning, Action, Evaluation) and sequences them into an optimal workflow.This process is iterative, allowing for continuous refinement based on intermediate results.</p>
<p>Figure 1 (continued). (C)</p>
<p>Retrieval-Augmented Generation (RAG) for knowledge synthesis.Sub-queries derived from the main task are addressed by the RAG Agent.It synthesizes information from diverse sources, including literature databases (PubMed, Google Scholar) (Extended Data Figure S1a), web searches (Extended Data Figure S1a), and biomedical knowledge graphs (Extended Data Figure S1a), to provide comprehensive, context-aware answers (Extended Data Figure S1b).(D) xBio-Tools powered by the xTrimo Universe.The system integrates 219 computational tools for analyses across five biological scales (DNA, RNA, protein, cell, chemical).These tools are built upon the xTrimo Universe, a suite of 104 models derived from six foundational models, each specialized for a specific data modality.Bar chart quantifies the number of tools available for each biological scale.See Extended Data Table for the complete xBio-Tools list.</p>
<p>BioLab demonstrates superior performance on core scientific reasoning and life science modeling tasks</p>
<p>To validate the core cognitive abilities required to overcome research fragmentation, we quantitatively evaluated BioLab's performance against existing state-of-the-art (SOTA) AI systems.We first assessed its capacity for deep scientific reasoning on a panel of four challenging biomedical question-answering benchmarks (PubMedQA (with and without content) 34 , MMLU-Pro/Biology 35 , and GPQA-diamond 36 ), which are designed to test comprehension of nuanced and highly specialized knowledge.Across all benchmarks, BioLab consistently outperformed leading large language models (LLMs), including DeepSeek-R1 37 , Qwen3-235B-A22B 38 , ChatGPT-4o 39,40 and Gemini-2.5-flash 41,42  underscoring its superior ability to reason within the complex domain of biology (Figure 2a).</p>
<p>A cornerstone of automating research is the ability to deconstruct a high-level goal into a logical, multi-step experimental plan 43 .We therefore evaluated BioLab's performance on the BioResearchQA benchmark, which specifically assesses this task-decomposition capability.Using three independent, expert-defined metrics that probe different aspects of planning, BioLab again demonstrated superior performance, outperforming both general-purpose LLMs and other specialized agent-based models (Figure 2b).This result highlights the effectiveness of our Planner Agent in formulating scientifically valid and logically coherent research strategies, a critical step in orchestrating complex, end-to-end projects (see the Benchmarking and evaluation methodology section in Methods for details).</p>
<p>The practical utility of BioLab's plans depends on the quality of the tools it orchestrates.The system's power is derived in large part from its integrated xTrimo Universe, a vast collection of foundation and task-specific models (Figure 2c).We systematically benchmarked our suite of downstream models against established SOTA models in their respective domains.The results revealed an exceptionally high prevalence of SOTA-level performance, with the proportion of our models achieving this status ranging from 91.30% to a full 100% in key categories like xTrimoChem and xTrimoCell (Figure 2d).</p>
<p>To provide concrete evidence of these capabilities, we highlight several examples where xTrimo models excel in critical biomedical tasks that are fundamental to drug discovery and mechanistic biology (Figure 2e).For instance, in predicting drug synergy, modeling protein interactions, mapping RNA accessibility, and annotating cell types, our models consistently matched or surpassed the performance of previous benchmark holders.This integrated suite of high-performing, specialized tools is what allows BioLab to move beyond purely linguistic planning to perform high-fidelity, quantitative biological modeling, thereby executing the complex workflows it designs.Complete results on BioLab's SOTA performance are provided in Extended Data Table S4.</p>
<p>5/100</p>
<p>Figure 2 (continued).</p>
<p>(A) Superior reasoning performance on biomedical benchmarks.BioLab's performance was evaluated against leading large language models (LLMs), including DeepSeek-R1, Qwen3-235B, OpenAI-GPT-4o, and Gemini-2.5-flash,on four scientific question-answering benchmarks: PubMedQA (with and without content), MMLU-Pro/Biology (CoT), and GPQA-diamond (CoT).BioLab consistently achieved higher scores across all tested benchmarks.(B) State-of-the-art performance in automated task decomposition.On the BioResearchQA benchmark, BioLab's ability to decompose complex tasks was compared against other AI models using three expert-defined evaluation metrics.BioLab demonstrated superior performance across all metrics, outperforming both LLMs and the Biomni agent model.(C) The xTrimo Universe of foundation and task-specific models.The xTrimo Universe comprises six foundation models, each targeting a different biological scale (xTrimoChem, xTrimoProtein, xTrimoRNA, xTrimoDNA, xTrimoCell, and xTrimoText), along with a comprehensive suite of downstream task-specific models.A representative subset of the model collection is shown.See Extended Data Table for the complete list.(D) Quantification of state-of-the-art (SOTA) models within the xTrimo Universe.The bar chart illustrates the proportion of downstream models within each category of the xTrimo Universe that achieve SOTA performance on their respective domain-specific benchmarks.The results indicate a high prevalence of SOTA models, with ratios ranging from 91.30% to 100%.(E) SOTA performance on representative biomedical tasks.Bar plots show the SOTA performance of selected xTrimo downstream models on a variety of critical biomedical tasks, including drug synergy prediction (DeepDDS), enzyme activity prediction (ESM2-3B), splice site prediction (NT-2500M-multi), and cell type annotation (CellTypist).See Extended Data Table S4 for a comprehensive evaluation of all downstream models.</p>
<p>BioLab executes a closed-loop in silico workflow for macrophage target discovery and antibody engineering</p>
<p>To demonstrate BioLab's capacity to orchestrate a complete research cycle in silico, we tasked it with a complex, real-world challenge that typically requires months of human effort: designing a therapeutic antibody targeting macrophages for cancer treatment.Upon receiving this single, high-level command, BioLab, without any further human intervention, autonomously designed and executed a five-stage discovery workflow (Figure 3a).This cascade of intelligent actions seamlessly integrated target discovery, computational validation, antibody sourcing, and multi-objective antibody optimization, showcasing a fully automated, closed-loop research project.</p>
<p>The process began with target identification, a critical and often laborious phase of drug discovery.BioLab initiated a systematic search, mining public databases and the scientific literature to identify genes differentially expressed in tumorassociated macrophages.It then leveraged the xTrimoSingleCellPerturb model within xBio-Tools suite to perform in silico perturbation predictions and pathway analyses on the initial hits, ultimately prioritizing a list of candidate targets based on their predicted functional impact (Figure 3b).Notably, TNFSF11 [44][45][46][47] , a well-established signaling protein and the target of the clinical antibody Denosumab [48][49][50][51] , was identified among the top-ranked candidates, validating the system's ability to independently pinpoint biologically and therapeutically relevant targets (see the xTrimo platform integration and foundation model usage within BioLab section in Methods for details).</p>
<p>Once TNFSF11 was confirmed as the primary target, BioLab transitioned to the antibody engineering phase.It first queried its knowledge base to retrieve the sequence and structural information for Denosumab (see the BioLab-Guided Antibody Recommendation with Bioinformation for TNFSF11 section in Materials and Methods for details).The system then employed the xTrimoAntibody model to formulate and execute a multi-objective optimization (MOO) on the Denosumab sequence-a computationally demanding task that requires navigating a high-dimensional search space to balance multiple, often conflicting, biophysical parameters (Figure 3c).The goal is to simultaneously improve five key properties critical for therapeutic success: binding affinity, naturalness, stability, production yield, and overall structural integrity.</p>
<p>The MOO process yielded a novel, optimized variant, designated MOO Denosumab (Figure 3c), with a predicted property profile superior to the wild-type antibody.This demonstrates BioLab's ability to not only identify existing therapeutic candidates but to rationally engineer them for enhanced performance.The system's capacity to handle such complex, multi-parameter optimization problems is a crucial step towards AI-driven molecular design.</p>
<p>To provide a mechanistic explanation for the predicted improvements--a hallmark of deep scientific inquiry--we directed BioLab to conduct molecular dynamics (MD) simulations 52,53 on both the wild-type (WT) and the MOO Denosumab-TNFSF11 complexes (Figure 3d).The simulations revealed that the three mutations introduced during optimization induced critical, cooperative conformational changes at the binding interface.Specifically, the L-S94W mutation facilitated a new -cation interaction that stabilized the formation of a new hydrogen bond and salt bridge, while the H-K98W mutation established novel hydrophobic interactions (including -alkyl contacts) that organized a more stable hydrophobic cage.These atomic-level insights not only explain the enhanced binding affinity but also demonstrate BioLab's ability to generate testable, mechanistic hypotheses, moving far beyond simple prediction to offer deep biological understanding.</p>
<p>7/100</p>
<p>Figure 3 (continued). (A)</p>
<p>An automated, end-to-end workflow for macrophage-targeting antibody discovery.Upon receiving a high-level user command ('Please design a de novo antibody targeting macrophage for cancer treatment'), the Planner Agent autonomously orchestrates a multi-stage workflow.This process includes five key phases: (1) target mining: target mining from literature and databases; (2) xBio-Tools perturbation prediction: target discovery and screening via xBio-Tools perturbation prediction; (3) target prioritization: prioritization of candidate targets; (4) targeting antibody discovery: discovery of existing antibodies against high-priority targets; and (5) xTrimoProtein MOO antibody optimization: multi-objective optimization (MOO) of a selected antibody using the xTrimoProtein model.This entire pipeline operates as a closed-loop, automated system.(B) Identification of potential macrophage targets using xBio-Tools.BioLab identifies candidate targets by analyzing transcriptomic data.The volcano plot (left) highlights differentially expressed genes, pathway enrichment analysis (center) identifies relevant biological processes, and the bar charts (right) rank top candidate targets based on expression levels and biological significance.Notably, TNFSF11 was identified among the top-ranked candidates.(C) Multi-objective optimization of the anti-TNFSF11 antibody, Denosumab.BioLab identified TNFSF11 as a high-priority target and retrieved the corresponding antibody, Denosumab, from its knowledge base.Subsequently, the system employed the xTrimoAntibody model, a downstream application of xTrimoProtein, to perform MOO.The optimization process simultaneously enhanced five key properties: affinity, naturalness, stability, yield, and structural integrity, resulting in an improved variant, MOO Denosumab.(D) Molecular dynamics (MD) simulations reveal the structural basis for enhanced affinity.MD simulations elucidated the mechanism underlying the improved binding of MOO Denosumab.The optimized antibody (orange) features three key mutations (H-G56V, H-K98W, L-S94W) compared to the wild-type (WT) structure (blue).The L-S94W mutation induces a conformational change via a -cation interaction (yellow dashed line) with R30, which in turn forms a new hydrogen bond (blue dashed line) and a salt bridge (green dashed line) with residues E65 and E108 on the antigen.In concert, the H-K98W mutation introduces additional hydrophobic interactions (including -alkyl contacts; brown dashed line) that organize surrounding residues into a stable hydrophobic cage (tan surface), further reinforcing the antibody-antigen interface.</p>
<p>BioLab closes the computational-experimental loop for T-cell target discovery and antibody optimization</p>
<p>The ultimate test of any computational discovery platform is its ability to generate novel, non-obvious predictions that are subsequently validated by real-world experiments.To this end, we designed a final study to prospectively validate BioLab's capabilities in a complete wet-lab/dry-lab cycle, tasking it with discovering a therapeutic strategy for T-cell-based cancer immunotherapy.We established a fully integrated,"design-build-test-learn" cycle, where BioLab's computational predictions were directly followed by experimental validation, with the results feeding back into the system to inform the next steps (Figure 4a).This case study was designed to rigorously test BioLab's ability to bridge the gap between in silico hypothesis and tangible, experimentally verified outcomes.</p>
<p>Given the high-level goal, BioLab's reasoning engine first analyzed established immune checkpoint pathways and prioritized Programmed Death-1 (PD-1) as the lead candidate target.This step alone replicates a complex decision-making process that typically involves extensive literature review and expert consultation (Figure 4b).This demonstrates the system's ability to distill vast amounts of information into a single, high-confidence starting point for a research campaign, effectively overcoming the initial hurdle of hypothesis generation.</p>
<p>Crucially, to bridge the gap from strategy to execution, BioLab then generated a complete, step-by-step laboratory protocol for knocking out the PDCD1 gene (which encodes PD-1) in primary human T-cells using CRISPR-Cas9 technology [54][55][56] .The ability to generate such actionable, detailed experimental plans is a major advance toward true research automation, as it directly addresses the practical barrier between computational hypothesis and experimental action.The generated protocol provided specific reagents, concentrations, and incubation times required for the experiment.</p>
<p>The wet-lab experiments were conducted following the BioLab-generated protocol (see Materials and Methods), seamlessly extending the computational framework into experimental validation.Building on BioLab's multi-agent system, which leveraged literature and knowledge to prioritize PD-1 as a key T-cell target, and its use of the xTrimoSingleCellPerturb model within xTrimo Universe to rank PD-1 first among predicted gene perturbations, the study incorporated parallel wet-lab efforts.These experiments confirmed PD-1's top ranking through T-cell gene perturbation assays based on IFN-predictions, revealing a strong positive correlation between reduced PD-1 protein expression and enhanced T-cell immune activity (Spearman correlation: 0.734, Pearson correlation: 0.800; Figure 4c).This robust alignment between computational forecasts and experimental outcomes not only validates BioLab's predictive accuracy but also reinforces its role in guiding the development of effective T-cell-based therapeutic strategies, paving the way for subsequent antibody optimization.</p>
<p>With the target validated, we leveraged BioLab to complete the therapeutic discovery cycle by optimizing Pembrolizumab, a clinical antibody targeting PD-1 (see Materials and Methods).The system generated two optimized variants, Pem-MOO-1 and Pem-MOO-2, which were then synthesized and tested in a functional reporter assay.Both variants exhibited more potent pathway blockade than the parental antibody in a dose-dependent manner.Notably, Pem-MOO-2 showed a nearly three-fold improvement in IC50 (0.01 nM vs. 0.027 nM for parental), demonstrating a substantial enhancement in biological activity resulting directly from the in silico optimization campaign (Figure 4d and Figure 4e, left).</p>
<p>Finally, a comprehensive multi-parameter assessment of the optimized antibodies was performed to evaluate their overall therapeutic potential and ensure that functional gains did not come at the cost of developability.A normalized radar plot revealed that Pem-MOO-2 not only possessed superior potency and binding affinity (Function and BLI scores) but also maintained developability profiles, such as yield, purity, and thermostability, comparable to the parental antibody (Figure 4e, right).This enhanced efficacy was achieved with an acceptable and well-understood trade-off in hydrophobicity (HIC score), a common challenge in antibody engineering.The significantly larger polygonal area of the Pem-MOO-2 profile underscores a superior overall candidate, proving that BioLab can successfully navigate the complex, multi-parameter landscape of therapeutic optimization.This final result successfully closes the loop, demonstrating that BioLab can shorten the discovery cycle by autonomously navigating from high-level strategy to the engineering of superior therapeutic molecules with experimentally confirmed efficacy.(C) Wet-lab validation confirms that PD-1 knockout enhances T-cell activity.BioLab generated a detailed protocol for CRISPR-Cas9-mediated knockout of PDCD1 (the gene encoding PD-1), integrating multi-agent analysis and the xTrimoSingleCellPerturbation model to prioritize PD-1.Subsequent experimental data revealed a strong positive correlation (Spearman correlation: 0.734, Pearson correlation: 0.800) between the model-predicted outcomes and the wet-lab results of IFN secretion score.This result functionally validates BioLab's target recommendation by directly linking reduced PD-1 expression to enhanced T-cell immune activity, aligning computational predictions with experimental outcomes.(D) Performance of optimized antibodies in a PD-1/PD-L1 blockade reporter assay.The ability of the antibodies to block the PD-1/PD-L1 interaction was assessed in a HEK293-based reporter assay.The dose-response curves show that both multi-objective optimized (MOO) variants, Pem-MOO-1 and Pem-MOO-2, exhibited more potent pathway blockade than the parental antibody, Pembrolizumab, indicated by a stronger restoration of the luminescence signal at lower concentrations.The IgG1 isotype control showed no activity.(E) Comprehensive functional and biophysical characterization of optimized antibodies.Quantification of IC50 values (left) demonstrates that the optimized variants, Pem-MOO-1 ( 0.016 nM) and Pem-MOO-2 ( 0.01 nM), are substantially more potent than the parental antibody, Pembrolizumab ( 0.027 nM).The radar plots (right) provide a multi-parameter assessment of both functional attributes (e.g., BLI score, Function score) and biophysical properties (e.g., Purity, Yield), with all metrics scaled so that a larger area indicates a more desirable profile.The profile for Pem-MOO-2 reveals substantial improvements in key functional metrics-notably the Function and BLI scores-while maintaining comparable performance in biophysical properties such as yield and purity.This enhanced efficacy was achieved with an acceptable trade-off in hydrophobicity (HIC score).The significantly larger polygonal area of the Pem-MOO-2 profile underscores a superior overall candidate, validating the success of the multi-objective optimization strategy in substantially boosting antibody function while managing biophysical liabilities.</p>
<p>Discussion</p>
<p>Here we present BioLab, a multi-agent AI system designed to address the profound fragmentation of the modern life sciences research workflow.We have demonstrated that by orchestrating a synergistic community of specialized agents, BioLab can automate the entire scientific discovery lifecycle, from high-level hypothesis generation to the design of superior therapeutic molecules with experimentally validated efficacy.Our results establish a robust framework for a new paradigm of AI-native scientific research, capable of closing the loop between computational modeling and physical experimentation.</p>
<p>The core innovation of BioLab lies in its architecture, which overcomes the limitations of both isolated foundation models and general-purpose LLMs.While foundation models have revolutionized specific tasks such as protein structure prediction, they function as specialized tools and lack the executive function to orchestrate a multi-stage research campaign 21,57,58 .Conversely, generalist LLMs, while capable of reasoning, lack the deep domain grounding necessary for high-fidelity biological inquiry, making them prone to factual hallucination 30,59,60 .BioLab resolves this dichotomy by integrating a hierarchical agent system, where a Planner Agent provides strategic oversight, while specialized agents like the RAG Agent and the xBio-Tools Agent provide deep domain expertise in knowledge retrieval and computational modeling.This modular design mirrors the collaborative structure of human scientific teams and proves essential for navigating complex, multi-step research problems.</p>
<p>A key differentiator of BioLab is its ability to ground its reasoning in a rich, multimodal knowledge base and execute tasks using a suite of SOTA-level foundation models.The tripartite RAG system provides a dynamic and comprehensive view of biological knowledge, while the integration of the xTrimo Universe empowers BioLab to perform high-fidelity, quantitative predictions.These architectural features provide the foundation for the system's advanced capabilities.</p>
<p>We first demonstrated these capabilities in a fully in silico case study, where BioLab autonomously executed an entire discovery workflow for a macrophage-targeting antibody.The system progressed from target identification and prioritization to the rational engineering of an optimized antibody candidate, a process that traditionally requires extensive human expertise and computational effort.This case study serves as a powerful demonstration of BioLab's capacity for end-to-end automation within a virtual environment.</p>
<p>Building upon this, the most stringent validation of our platform was presented in the T cell study, where BioLab successfully closed the computational-experimental loop.This moved beyond pure simulation to guide physical experiments.The system not only autonomously identified PD-1 as the primary therapeutic target but also generated a detailed, actionable protocol that guided successful wet-lab knockout experiments.This ability to translate a computational strategy into a physical experimental plan represents a critical step towards true research automation.Moreover, BioLab's subsequent optimization of the clinical antibody Pembrolizumab yielded novel variants with demonstrably superior potency in functional assays.This result prospectively validates the entire workflow, proving that BioLab can guide research toward tangible, high-value biological outcomes and significantly shorten the discovery-to-validation cycle.</p>
<p>Despite these advances, we acknowledge the limitations of the current BioLab system.Its ability to incorporate experimental feedback is currently dependent on human-mediated data entry, and the full automation of complex, physical wet-lab procedures remains a significant long-term challenge.The system's knowledge is also constrained by the scope and quality of public 12/100 databases and scientific literature.Future iterations will focus on developing more seamless interfaces for real-time experimental data ingestion and expanding the xTrimo Universe with models trained on proprietary, multi-modal datasets.</p>
<p>In conclusion, BioLab serves as a powerful prototype for the next generation of scientific discovery platforms.By unifying strategic reasoning, deep domain knowledge, and high-performance computing, it provides a scalable solution to the fragmentation of the research process.The principles demonstrated here-of modular agency, closed-loop learning, and the tight integration of computation and experimentation-lay the groundwork for a future where AI systems act not merely as tools, but as true collaborative partners in the quest for scientific understanding.</p>
<p>Methods</p>
<p>The BioLab multi-agent system BioLab is implemented as a modular multi-agent system designed to automate closed-loop life science workflows, spanning in silico modeling and, where applicable, wet-lab experimentation.Drawing inspiration from cognitive architectures, BioLab organizes a collection of specialized agents into a structured hierarchy that enables dynamic task decomposition, inter-agent reasoning, model and tool invocation, and iterative refinement.Each agent is instantiated with a specific functional role, and agents interact via structured message-passing protocols that support both forward task progression and retrospective error correction.The core agents are detailed below.</p>
<p>Planner Agent The system's top-level orchestrator.Upon receiving a high-level scientific objective, the Planner Agent decomposes the task into a logical sequence of discrete subgoals.It then allocates these subtasks to the most appropriate downstream agents and manages the overall workflow to ensure the primary objective is met.</p>
<p>Reasoner Agent The core of the system's modular intelligence.The Reasoner Agent receives subgoals from the Planner Agent and is responsible for formulating detailed, scientifically valid action plans.It validates the internal coherence of proposed steps, assembles the required agent modules for execution, and coordinates the flow of information across upstream and downstream dependencies.</p>
<p>Memory Agent This agent enables longitudinal learning and adaptive behavior.It maintains both short-term memory for the context of ongoing tasks and a long-term, versioned memory that stores execution traces, experimental outcomes, reusable agent configurations, and prior user feedback.This allows BioLab to reference past workflows and dynamically adjust its strategy based on empirical results.</p>
<p>RAG Agent This agent grounds all system operations in factual evidence.It interfaces with a custom-built, multimodal knowledge base that includes structured databases (e.g., UniProt, PDB, GEO), semi-structured resources like protocol repositories, and the full text of the scientific literature.The RAG Agent is responsible for retrieving relevant information to support hypothesis generation, tool configuration, and experimental planning.</p>
<p>xBio-Tools Agent This agent serves as the execution engine for a diverse collection of biological models and analytical tools.It abstracts the underlying software, including the xTrimo Universe foundation models (e.g., xTrimoSingleCellPerturb, xTrimoAntibody) and third-party tools, handling parameter translation, batch dispatch, and the return of structured, machinereadable results.</p>
<p>Code Agent This suite of agents handles all code-related tasks.The Code Agent translates high-level agent workflows into executable Python or shell scripts.The Executor Agent then deploys these scripts in appropriate computational environments (e.g., containerized or HPC) and monitors their status.When failures or unexpected outputs arise, the Reflection Agent intervenes to perform root-cause analysis and propose remediation, such as revised code or model substitutions.</p>
<p>Critic Agent The system's quality control module.The Critic Agent continuously assesses the outputs of other agents against expected structural, statistical, or semantic criteria.It plays a central role in error detection and validation, triggering replanning or refinement cycles as necessary to ensure the scientific and technical rigor of the entire process.</p>
<p>Reporter Agent The final agent in the workflow, responsible for communication.The Reporter Agent consolidates all data, analyses, and provenance metadata from a research cycle into comprehensive, human-readable reports.These reports are tailored for end-users with diverse backgrounds, from wet-lab biologists to computational modelers, ensuring the interpretability and auditability of the results.The full agent system operates asynchronously with shared context layers and versioned memory objects, enabling reproducible, interpretable, and evolvable scientific execution.Throughout each cycle, BioLab agents collectively exhibit not only domain-specific capabilities but also adaptive system-level reasoning-a critical property for reliable, autonomous life science research.</p>
<p>Memory Agent</p>
<p>The Memory Agent implements a hierarchical memory architecture for context retention and iterative improvement.It comprises a Short-Term Memory (STM) that maintains transient, task-level context and a Long-Term Memory (LTM) that stores and retrieves persistent knowledge.The design follows standard agentic-memory practice and, where noted, draws on ideas from the mem0 61 framework.</p>
<p>Short-term memory.STM operates as a high-speed "working memory", maintaining a rolling window of structured records produced during an active workflow.Each record is timestamped and annotated with its source agent and artifact type (e.g., tool output, intermediate reasoning).The buffer is fixed-length with first-in-first-out eviction; agents query it during execution to ensure coherence across multi-step procedures.On task completion, a summarization routine condenses the session log and emits a compact representation of salient facts and decisions for downstream consolidation.</p>
<p>Long-term memory.LTM serves as the persistent knowledge base and is organized into two stores.A user-profile store (key-value) maintains user-specific parameters, project history, and tool configurations keyed by a unique user id, enabling context-aware behavior across sessions.A factual store (vector) maintains validated facts, procedural workflows, and experimental findings as high-dimensional embeddings; similarity search over this store supports semantic retrieval and analogical reuse across problems.Retrieval uses standard vector similarity with approximate nearest-neighbour indexing; store updates are transactionally logged to preserve provenance.</p>
<p>Consolidation and update.After each task, an LLM-driven consolidation pass extracts candidate memories from the STM summary, queries the factual store for the top- nearest neighbours (configurable), and compares candidates with retrieved entries.The model issues one of four operations-ADD (novel entry), UPDATE (refinement or correction), DELETE (retire obsolete entry), or NOOP (no change)-and writes an audit record containing the pre/post states and rationale.User feedback on retrieval quality and tool performance is ingested as factual entries and subsequently influences retrieval weighting and tool-specific confidence scores.This closed loop yields gradual improvement in retrieval precision and downstream decision quality without altering task-time behaviour.</p>
<p>RAG Agent and Multi-modal Knowledge Base</p>
<p>To ground all reasoning in robust, verifiable scientific evidence and mitigate the risk of factual hallucination, BioLab incorporates a highly specialized Retrieval-Augmented Generation (RAG) Agent.This agent functions as an advanced information retrieval and synthesis engine, interfacing between the reasoning core of BioLab and a custom-built, multimodal knowledge base.The RAG Agent employs a multi-stage pipeline that includes query pre-processing, parallel hybrid retrieval across diverse knowledge sources, and intelligent information fusion to provide a rich, contextually relevant evidence base for downstream tasks.Detailed information about RAG Agent and the multi-modal knowledge base is in Extended Data Figure S1.</p>
<p>Query Pre-processing.Upon receiving a query from an upstream agent (e.g., the Reasoner), the RAG Agent first initiates a query pre-processing step.A dedicated Query Rewrite LLM Agent analyzes the initial query and systematically expands it into a set of comprehensive sub-queries.This expansion is performed across multiple dimensions to maximize retrieval coverage, including: (i) synonym and related term expansion to capture lexical variations; (ii) thematic expansion with related biological concepts; (iii) contextual expansion based on the preceding conversational history; and (iv) multi-angle expansion, where the query is rephrased to seek definitions, causes, consequences, etc.This process transforms a single, often ambiguous, query into a rich set of specific questions poised for deep retrieval.</p>
<p>Hybrid Retrieval from a Multimodal Knowledge Base.The generated sub-queries are then dispatched in parallel to three specialized retrieval channels, each targeting a distinct component of our multimodal knowledge base.This base was designed to provide a comprehensive and synergistic view of biomedical knowledge.</p>
<p>• Document Search: This channel targets a curated library of scientific literature and internal documents.It employs a hybrid retrieval strategy, combining dense (vector-based semantic search) and sparse (e.g., BM25 keyword-based) retrieval methods to ensure both conceptual and lexical relevance.The results are further refined using metadata filters.</p>
<p>• Web Search: To access the most current information, a Web Search channel generates optimized search engine queries from the sub-queries.It retrieves and processes content from a variety of web sources, including HTML pages and PDF documents, using specialized document loaders.</p>
<p>• Knowledge Graph Search: This channel provides access to structured, factual knowledge.An LLM first performs named entity recognition on the sub-queries to identify key biological entities.These entities are then mapped to nodes in our proprietary biomedical knowledge graph.A graph query (e.g., Cypher) is then generated to traverse the graph and retrieve precise, factual relationships between entities.The architecture of this knowledge base is detailed in Extended Data Figure S1.</p>
<p>Information Fusion and Reranking.The raw outputs from the three parallel retrieval channels are not used directly.Instead, they are fed into a sophisticated fusion and reranking module.A DeepSource Master LLM Agent first assesses the relevance and quality of the retrieved information chunks from each source.Concurrently, a Global Multi-Channel Weighting model assigns a confidence score to each chunk based on its source, relevance, and timeliness.The top-'k' ranked information snippets, weighted by their scores, are then synthesized and consolidated into a final, coherent context block.This reranking and fusion step is critical for filtering out noise and ensuring that only the highest-quality, most relevant evidence is passed to the final generation stage.This refined context is then provided to the downstream agent that initiated the query, enabling it to generate a response that is deeply informed and factually grounded.</p>
<p>Benchmarking and evaluation methodology</p>
<p>To systematically evaluate the performance of BioLab's reasoning capabilities, we curated a comprehensive suite of benchmarks spanning biomedical question answering, multi-hop scientific reasoning, and open-ended research task planning.These benchmarks were selected to test not only factual recall, but also the ability to integrate domain knowledge, synthesize context, and generate expert-level research strategies.We compare BioLab against state-of-the-art large language models (LLMs) and agent-based baselines using both quantitative metrics and expert evaluation.</p>
<p>Biomedical reasoning benchmarks We first evaluated BioLab on a collection of four domain-specific reasoning benchmarks.These included PubMedQA (both with and without contextual passage) 34 , MMLU-Pro-Biology (with chain-ofthought prompting) 35 , and GPQA-Diamond 36 .PubMedQA is a question-answering benchmark designed to assess biomedical knowledge and reasoning using questions sourced from PubMed abstracts.It consists of 1,000 expert-annotated yes/no/maybe questions, each paired with a relevant abstract in the contextual passage setting, requiring models to extract and reason over pertinent information.In the no-context setting, models must rely solely on prior knowledge to answer questions, testing their ability to recall and apply biomedical facts without external references.This dual setup evaluates both information retrieval and standalone reasoning capabilities.MMLU-Pro-Biology, a subset of the Massive Multitask Language Understanding (MMLU) benchmark, focuses on advanced biology questions at the college and professional level.It comprises 1,200 multiple-choice questions covering topics such as molecular biology, genetics, and physiology.With chain-of-thought prompting, models are encouraged to generate step-by-step reasoning before selecting an answer, emphasizing their ability to perform complex, multi-step reasoning in biology.This benchmark tests deep conceptual understanding and the capacity to connect disparate biological concepts.GPQA-Diamond is a highly curated benchmark of 198 graduate-level biology questions, designed to evaluate grounded reasoning in specialized biomedical domains.Each question is crafted by domain experts to require both factual knowledge and sophisticated reasoning, often involving the integration of multiple scientific concepts.The benchmark emphasizes real-world applicability, with questions reflecting challenges encountered in cutting-edge biomedical research.Performance is measured in percentiles, highlighting a model's ability to excel in expert-level reasoning tasks.</p>
<p>Expert-graded research task planning benchmark To evaluate BioLab's planner and reasoning agents in openended contexts, we constructed a custom benchmark of 20 research questions named BioResearchQA across molecular biology, immunology, RNA therapeutics, protein design, and synthetic cell engineering.Each question was formulated as a realistic research problem-for instance, "Design a strategy to identify non-coding RNA regulators of macrophage polarization"-requiring multi-step planning and domain-specific tool selection.Three domain experts scored each output along three axes: (1) multi-modal planning, (2) bio-tool calling, and (3) full-loop science reasoning.Scoring used a 5-point Likert scale per criterion, averaged across reviewers.</p>
<p>LLM baselines To contextualize BioLab's agentic reasoning capabilities, we conducted additional evaluations against leading LLMs including DeepSeek-R1 37 , Qwen3-235B-A22B 38 , Gemini-2.5-Flash 41, Gemini-2.5-Pro 42, and ChatGPT-4o 39,40 .DeepSeek-R1, developed by DeepSeek AI, is a 671B-parameter dense model optimized for reasoning and coding, with a 131,072-token context window and strong performance on benchmarks 37 .Qwen3-235B-A22B, from Alibaba's Qwen team, employs a Mixture-of-Experts (MoE) architecture with 235B total parameters (22B active), excelling in coding (LiveCodeBench: 70.7) and math (AIME 2024: 85.7) under the Apache 2.0 license 38 .Gemini-2.5-Flash, a lightweight model from Google, prioritizes low-latency tasks 41 , while Gemini-2.5-Pro,a more robust counterpart, leads in reasoning benchmarks like ArenaHard (96.4%) 42 .ChatGPT-4o, developed by OpenAI, is a multimodal model with strong general-purpose capabilities, though it slightly trails Qwen3-235B-A22B in coding tasks like Codeforces (2056 Elo vs. 1974 Elo) 39,40 .These models represent a spectrum of architectural and deployment strategies, evaluated here for their biomedical reasoning capabilities.</p>
<p>Agent-based systems This study examines an advanced AI agents designed for specialized reasoning and task execution: Biomni 62 .Biomni is a general-purpose biomedical AI agent that autonomously executes tasks across 25 biomedical subfields.By combining a unified action space (Biomni-E1) with a dynamic task-executing architecture (Biomni-A1), it handles complex workflows like multi-omics analysis, achieving 74.4% accuracy on LAB-Bench DbQA and 81.9% on SeqQA, surpassing human expert performance 62 .The agent-based system represent cutting-edge approaches to autonomous reasoning, tool integration, and domain-specific problem-solving in AI research.</p>
<p>xTrimo platform integration and foundation model usage within BioLab</p>
<p>The BioLab system integrates a suite of domain-specialized foundation models through its modular xTrimo platform, which serves as a unified inference backbone for biological sequence modeling, perturbation prediction, and structural optimization.The xTrimo platform includes both general-purpose models for protein, RNA, and DNA analysis, as well as task-specific foundation models designed for high-fidelity interpretation of cellular and molecular function.These models are invoked by BioLab agents via the xBio-Tools interface, which handles model selection, parameter configuration, and input/output formatting for downstream reasoning.</p>
<p>Among the core components of this platform is xTrimoSingleCellPerturb based on xTrimoCell.xTrimoCell is a single-cell foundation model purpose-built for predicting genotype-to-phenotype responses to genetic perturbations.Trained on over 5 million single-cell expression profiles and fine-tuned with a large-scale perturb-seq dataset comprising 347,738 cells, xTrimoCell captures context-dependent transcriptional responses across diverse cell types.The model architecture leverages multi-head attention and contrastive alignment across cell embeddings and perturbation graphs, enabling robust generalization to previously unseen gene-cell-condition triplets.</p>
<p>In BioLab, xTrimoSingleCellPerturb is accessed by the Reasoner and Target Identification agents during hypothesis formation and candidate ranking.For example, when the system is tasked with nominating modulators of T cell exhaustion or macrophage activation, it queries xTrimoSingleCellPerturb with context-specific perturbation sets.The model outputs predicted transcriptional shifts, which are ranked according to inferred immune activation markers (e.g., IFNG, GZMB) and integrated with graph-based literature mining results via the Retriever Agent.These integrated scores are used by BioLab to prioritize gene targets for wet-lab validation or therapeutic development.</p>
<p>Complementing this is xTrimoAntibody based on xTrimoPGLM 63 .xTrimoPGLM is a multi-billion-parameter biomolecular foundation model designed for unified sequence-structure reasoning across proteins and other biopolymers.xTrimoPGLM incorporates autoregressive and masked modeling heads, enabling it to support both generative and discriminative tasks such as affinity scoring, structural refinement, and variant prediction.The model is pre-trained on over 1.2 billion natural and synthetic sequences, with co-training over known structures and fine-tuning on experimental protein-ligand interaction data.</p>
<p>Within BioLab, the xTrimoAntibody model is called by the Structure Reasoning and Optimization agents to evaluate candidate therapeutic designs.In antibody development tasks, for instance, it is used to assess interface complementarity between antibody-antigen pairs, to compute developability metrics including aggregation risk and thermostability, and to guide multi-objective optimization routines.The model's generative capacity is leveraged during sequence refinement, where candidate antibodies are iteratively modified and scored to balance binding affinity, structural validity, and sequence novelty.</p>
<p>By enabling task-specific, context-aware modeling at both cellular and molecular scales, the integration of xTrimoSingle-CellPerturb and xTrimoPGLM allows BioLab to reason over biological systems with a degree of precision and adaptability not achievable using generic LLMs alone.The models are further extensible within the BioLab platform, supporting modular composition with downstream predictors and facilitating robust generalization to novel experimental settings.</p>
<p>Molecule Dynamics Simulation and Analysis of Wild-Type and Optimized Antibody-Antigen Complexes System Preparation and Simulation Parameters</p>
<p>To investigate the structural basis of antibody-antigen recognition, all-atom MD simulations were performed for the wild-type and optimized antibody-receptor complexes.Initial structures were prepared and subsequently simulated using GROMACS (version 2021.4) with the CHARMM36m force field, which is well-validated for protein dynamics.Each complex was solvated in a dodecahedron water box using the TIP3P water model, ensuring a minimum distance of 10 Å between the protein and the box edges to prevent periodic image artifacts.The system was neutralized and brought to a physiological salt concentration of 0.15 M by adding Na + and Cl − counter-ions.Long-range electrostatic interactions were computed using the Particle Mesh Ewald (PME) method with a real-space cutoff of 12 Å.A corresponding 12 Å cutoff was applied for Lennard-Jones interactions, with a force-based switching function initiated at 10 Å.</p>
<p>System Equilibration and Production Simulations Prior to production runs, each system underwent a rigorous multi-step equilibration protocol to ensure thermal and structural stability.First, an energy minimization was performed using the steepest descent algorithm to remove any steric clashes.The system was then gradually heated to 300 K over 1 ns in the canonical (NVT) ensemble, with position restraints applied to all protein heavy atoms to allow the solvent to relax.This was followed by a 5 ns equilibration in the isothermal-isobaric (NPT) ensemble at 1 atm, using an isotropic Parrinello-Rahman barostat, to ensure proper system density.During this phase, the position restraints on the heavy atoms were progressively released.All covalent bonds involving hydrogen atoms were constrained using the LINCS algorithm, which enabled a stable 2 fs integration time step.For robust sampling, three independent production simulations, each starting from different initial velocities, were carried out for 100 ns per system.</p>
<p>Conformational Clustering and Trajectory Analysis</p>
<p>To identify the most representative and thermodynamically stable protein conformations from the dynamic trajectories, all post-simulation analyses were performed on the concatenated trajectories from the three independent replicates for each system.Conformational clustering was performed using the gmx cluster tool in GROMACS with the Gromos algorithm.A 1.5 Å root-mean-square deviation (RMSD) cutoff, calculated on the backbone atoms of the interface residues (defined as residues with any atom within 10 Å of the binding partner), was used to group similar structures.The centroid structure from the most populated cluster for each system, defined as the frame with the smallest average RMSD to all other frames in that cluster, was selected for all subsequent detailed analyses.</p>
<p>Interfacial Fingerprinting and Visualization</p>
<p>The interfacial fingerprint for each antibody complex was constructed by computing and mapping key physicochemical properties onto its molecular surface.Starting from the selected representative centroid structure, a triangulated surface mesh was generated at 1.0 Å resolution using MSMS.Interfacial vertices were defined as surface points on the antibody within 4 Å of any receptor atom.For each interfacial vertex, four distinct feature channels were calculated: (i) Shape index, to describe the local surface curvature (concave, convex, or flat); (ii) Poisson-Boltzmann electrostatic potential, calculated using the APBS software to map charge distribution; (iii) Hydropathy, based on the Kyte-Doolittle scale to represent hydrophobicity; and (iv) Hydrogen-bonding potential, which includes separate features for lone-pair electron acceptors and proton donors to map binding capability.Each feature channel was normalized to a range of [0, 1] and projected onto the antibody interface.All structural visualizations were generated using PyMOL.</p>
<p>Wet-lab validation of T cell cycle predictions and antibody candidates</p>
<p>To validate BioLab's computational predictions in a real-world biological context, we conducted wet-lab experiments focused on T cell functional modulation and therapeutic antibody optimization.These experiments were performed in collaboration with a certified immunology laboratory and followed standardized experimental procedures for genome perturbation and antibody characterization.</p>
<p>CRISPR perturbation of candidate T cell targets Following computational nomination of immune checkpoint targets by BioLab's xTrimoSingleCellPerturb-based inference, CRISPR-Cas9-mediated knockdown experiments were designed using sgRNA sequences automatically generated by BioLab's sequence design agent.The sgRNAs were selected based on predicted on-target efficacy and off-target avoidance, incorporating integrated rules from CRISPRscan and Doench (2016) scoring.Primary human CD8 + T cells were isolated from peripheral blood mononuclear cells (PBMCs) via negative selection and cultured in the presence of IL-2 (100 IU/mL).Nucleofection was performed using a Lonza 4D-Nucleofector (program EO-115) with pre-complexed sgRNA/Cas9 ribonucleoproteins.Cells were harvested 72 hours post-transfection.</p>
<p>Functional outcomes were assessed using flow cytometry and cytokine profiling.Cells were stained for activation markers (CD69, CD25) and intracellular IFN- and Granzyme B. Data acquisition was performed on a BD FACSCelesta flow cytometer, and analyses were conducted in FlowJo v10.Statistical comparisons were performed using one-way ANOVA followed by Dunnett's post hoc test for multiple comparisons, with adjusted -values &lt; 0.05 considered significant.</p>
<p>Antibody expression and purification Candidate antibody sequences optimized by BioLab's xTrimoAntibody agent were synthesized and cloned into IgG1 heavy-and light-chain expression vectors.Constructs were transiently transfected into Expi293F cells using ExpiFectamine (Thermo Fisher) and cultured in Expi293 Expression Medium at 37 • C, 8% CO 2 , shaking at 125 rpm.Supernatants were harvested at day 7 post-transfection and purified using Protein A affinity chromatography.Antibody concentration was quantified by absorbance at 280 nm and validated by SDS-PAGE.</p>
<p>Binding and functional assays Binding affinity to PD-1 was measured using bio-layer interferometry (BLI) on an Octet RH96 system.Anti-human IgG Fc capture (AHC) biosensors were loaded with antibody samples (5 g/mL), and association/dissociation kinetics were recorded against recombinant human PD-1-Fc (R&amp;D Systems).Equilibrium dissociation constants (  ) were determined using a 1:1 binding model in the ForteBio Data Analysis software.Functional potency was assessed using a PD-1 reporter assay.Jurkat-NFAT-P3Z cells were co-cultured with CHO-K1-PD-L1 cells in the presence of serially diluted antibody samples.Luciferase activity was measured after 6 h using the Bright-Glo Luciferase Assay (Promega), and IC 50 values were calculated by four-parameter logistic regression in GraphPad Prism v9.</p>
<p>Stability characterization Thermal stability was assessed using nano differential scanning fluorimetry (NanoDSF, Panta Nano DSF).Protein samples were centrifuged at 3000 ×g for 10 min, and the supernatant was loaded into glass capillaries.Samples were scanned from 25 • C to 95 • C at a heating rate of 1 °C/min.The melting temperature (  ) was determined as the inflection point of the unfolding transition using the Panta Analysis software, with reference controls confirming system suitability ( 1 within ±0.3 • C).</p>
<p>Hydrophobic interaction chromatography (HIC) analysis HIC was performed to assess protein retention time and aggregation potential, using a TSKgel Butyl-NPR column (Tosoh Bioscience) on an Agilent HPLC system with ammonium sulfate gradients.</p>
<p>Statistical Analysis</p>
<p>Unless otherwise stated, all assays were performed in at least three biological replicates.Data are presented as mean ± s.e.m. (standard error of the mean).Significance tests were two-sided unless otherwise specified, and a -values &lt; 0.05 was considered statistically significant.For comparisons involving multiple groups or conditions, one-way or two-way ANOVA was used, followed by an appropriate post hoc test for multiple comparisons (e.g., Dunnett's or Bonferroni's correction).All statistical analyses were conducted in R (v4.2.1) or GraphPad Prism (v9).</p>
<p>Figure 2 .
2
Figure 2. Performance benchmarks and state-of-the-art capabilities of BioLab and its core xTrimo Universe.</p>
<p>Figure 3 .
3
Figure 3. End-to-end de novo design and optimization of a macrophage-targeting antibody by BioLab.</p>
<p>Figure 4 .
4
Figure 4. Closed-loop integration of BioLab and wet-lab experiments for T-cell target discovery and antibody optimization.(A) A hybrid computational/wet-lab workflow for T-cell therapy development.BioLab (in silico, green boxes) and wet-lab experiments (blue boxes) were integrated into a multi-round "design-build-test-learn" cycle.The in silico pipeline includes target mining, xTrimoCell-based perturbation prediction, target prioritization, antibody discovery, and xTrimoProtein-driven antibody optimization.The wet-lab pipeline involves isolating primary human CD8+ T cells from PBMCs, performing CRISPR-Cas9 gene editing, co-culturing with dendritic cells (DCs) for IFN secretion assays, and validating antibody function in a HEK293-based PD-1/PD-L1 blockade bioassay.(B) Autonomous identification of PD-1 as the top T-cell target.Upon receiving the high-level command 'design an antibody targeting T cell for cancer treatment', BioLab's reasoning engine analyzed established immune checkpoint pathways and prioritized Programmed Death-1 (PD-1) as the lead candidate with the highest therapeutic potential.</p>
<p>Figure 4 (
4
Figure 4 (continued).(C)Wet-lab validation confirms that PD-1 knockout enhances T-cell activity.BioLab generated a detailed protocol for CRISPR-Cas9-mediated knockout of PDCD1 (the gene encoding PD-1), integrating multi-agent analysis and the xTrimoSingleCellPerturbation model to prioritize PD-1.Subsequent experimental data revealed a strong positive correlation (Spearman correlation: 0.734, Pearson correlation: 0.800) between the model-predicted outcomes and the wet-lab results of IFN secretion score.This result functionally validates BioLab's target recommendation by directly linking reduced PD-1 expression to enhanced T-cell immune activity, aligning computational predictions with experimental outcomes.(D) Performance of optimized antibodies in a PD-1/PD-L1 blockade reporter assay.The ability of the antibodies to block the PD-1/PD-L1 interaction was assessed in a HEK293-based reporter assay.The dose-response curves show that both multi-objective optimized (MOO) variants, Pem-MOO-1 and Pem-MOO-2, exhibited more potent pathway blockade than the parental antibody, Pembrolizumab, indicated by a stronger restoration of the luminescence signal at lower concentrations.The IgG1 isotype control showed no activity.(E) Comprehensive functional and biophysical characterization of optimized antibodies.Quantification of IC50 values (left) demonstrates that the optimized variants, Pem-MOO-1 ( 0.016 nM) and Pem-MOO-2 ( 0.01 nM), are substantially more potent than the parental antibody, Pembrolizumab ( 0.027 nM).The radar plots (right) provide a multi-parameter assessment of both functional attributes (e.g., BLI score, Function score) and biophysical properties (e.g., Purity, Yield), with all metrics scaled so that a larger area indicates a more desirable profile.The profile for Pem-MOO-2 reveals substantial improvements in key functional metrics-notably the Function and BLI scores-while maintaining comparable performance in biophysical properties such as yield and purity.This enhanced efficacy was achieved with an acceptable trade-off in hydrophobicity (HIC score).The significantly larger polygonal area of the Pem-MOO-2 profile underscores a superior overall candidate, validating the success of the multi-objective optimization strategy in substantially boosting antibody function while managing biophysical liabilities.</p>
<p>/100
/100
AcknowledgementsCode availabilityThe source code for the BioLab multi-agent system and the models used in this study will be made publicly available on a dedicated GitHub repository upon publication of this manuscript.In the interim, the core functionalities of BioLab are available for exploration via an interactive web-based platform at https://xtrimo.biomap.com/.Computational ResourcesAll computational experiments-including agent execution, model inference, and optimization workflows-were performed on a high-performance computing cluster equipped with NVIDIA A100 GPUs (40GB VRAM) and 1.5 TB of aggregate system memory.Foundation model inference was conducted using containerized services deployed via Docker, orchestrated through a custom task manager built on the AutoGen framework.Training and evaluation pipelines were implemented in Python 3.10 with PyTorch 2.1 and Hugging Face Transformers 4.38, and executed on Ubuntu 22.04 LTS.Agent coordination followed an event-driven architecture using asynchronous message-passing, with real-time state tracking via Redis-backed memory.Multi-agent episodes were logged and versioned using MLflow to ensure reproducibility across runs.All experiments were executed under deterministic seed initialization unless otherwise noted.Author contributions statement R.J., R.Z., Z.Z., M.W., X.Z., and L.C. conceived the study and designed the experiments.R.J., Y.G., Y.Q., and M.Y. performed the dataset analyses and developed the foundation models.R.J., C.S., Q.Y., L.L.C., Y.Z., R.X., and Z.X.conducted the downstream experiments and collected the data.R.J., Y.G., and Y.Q.analyzed the data and interpreted the results.R.Z., Z.Z., M.W., X.Z., and L.C. supervised the project and provided critical intellectual input.R.J., Y.G., and Y.Q.wrote the initial draft of the manuscript.Y.Z., R.X., and Z.X.assisted with manuscript editing.R.Z., Z.Z., M.W., X.Z., and L.C. revised the manuscript.All authors reviewed and approved the final version of the manuscript.Competing interestsThe authors declare no competing interests.
The human cell atlas: from vision to reality. O Rozenblatt-Rosen, M J T Stubbington, A Regev, S A Teichmann, 10.1038/550451aNature. 5502017</p>
<p>The tabula sapiens: A multiple-organ, single-cell transcriptomic atlas of humans. Tabula Sapiens, Consortium , 10.1126/science.abl4896Science. 37648962022</p>
<p>Multimodal cell atlas of the ageing human skeletal muscle. Y Lai, 10.1038/s41586-024-07715-3Nature. 6302024</p>
<p>Whole-genome sequencing of 490,640 uk biobank participants. J D Backman, 10.1038/s41586-025-09272-9Nature. 6222023</p>
<p>Science acceleration and accessibility with self-driving labs. R B Canty, 10.1038/s41467-025-59231-1Nature Communications. 163012025</p>
<p>Evaluating the utilities of foundation models in single-cell data analysis. T Liu, 10.1101/2023.09.08.555192bioRxiv. 2023</p>
<p>Atypical combinations and scientific impact. B Uzzi, S Mukherjee, M Stringer, B Jones, 10.1126/science.1240474Science. 3422013</p>
<p>The fair guiding principles for scientific data management and stewardship. M D Wilkinson, M Dumontier, I J Aalbersberg, 10.1038/sdata.2016.18Scientific Data. 31600182016</p>
<p>An autonomous laboratory for accelerated synthesis of inorganic materials. N J Szymanski, 10.1038/s41586-023-06734-wNature. 6202023</p>
<p>Self-driving laboratories to autonomously navigate the protein fitness landscape. J T Rapp, 10.1038/s44286-023-00002-4Nature. 6242024</p>
<p>Diagnosing the decline in pharmaceutical r&amp;d efficiency. J W Scannell, A Blanckley, H Boldon, B Warrington, 10.1038/nrd3681Nature Reviews Drug Discovery. 112012</p>
<p>Harnessing the deep learning power of foundation models in single-cell omics. Q Ma, 10.1038/s41580-024-00756-6Nature Reviews Molecular Cell Biology. 252024</p>
<p>Deep-learning-based gene perturbation effect prediction does not yet outperform simple linear baselines. C Ahlmann-Eltze, 10.1038/s41592-025-02772-6Nature Methods. 2025</p>
<p>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. A Rives, 10.1073/pnas.2016239118Proceedings of the National Academy of Sciences. 118e20162391182021</p>
<p>Foundation models in plant molecular biology: advances, challenges, and future directions. F Xu, T Wu, Q Cheng, X Wang, J Yan, 10.3389/fpls.2025.1611992Frontiers in Plant Science. 1616119922025</p>
<p>Sequence modeling and design from molecular to genome scale with evo. E Nguyen, 10.1126/science.ado9336Science. 38693362024</p>
<p>Emergent autonomous scientific research capabilities of large language models. D A Boiko, R Macknight, G Gomes, 10.48550/arXiv.2304.05332arXiv:2304.053322023arXiv preprint</p>
<p>Is ai leading to a reproducibility crisis in science?. P Ball, 10.1038/d41586-023-03817-6Nature. 6242023</p>
<p>Nobel turing challenge: creating the engine for scientific discovery. H Kitano, 10.1038/s41540-021-00189-3Systems Biology and Applications. 7292021</p>
<p>Science acceleration and accessibility with self-driving labs. R B Canty, 10.1038/s41467-025-59231-1Nature Communications. 1638562025</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, 10.1038/s41586-023-06792-0s41586-023-06792-0. 19/100Nature. 6242023</p>
<p>Foundation models of scientific knowledge for chemistry: Opportunities, challenges and lessons learned. S Horawalavithana, Proceedings of BigScience Episode #5 -Workshop on Challenges &amp; Perspectives in Creating Large Language Models. A Fan, S Ilic, T Wolf, M Gallé, BigScience Episode #5 -Workshop on Challenges &amp; Perspectives in Creating Large Language ModelsAssociation for Computational Linguistics2022</p>
<p>An overview of domain-specific foundation model: key technologies, applications and challenges. H Chen, 10.48550/arXiv.2409.04267arXiv:2409.042672024arXiv preprint</p>
<p>A comprehensive survey on pretrained foundation models: a history from bert to chatgpt. C Zhou, Q Li, C Li, 10.1007/s13042-024-02443-6International Journal of Machine Learning and Cybernetics. 2024</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, 10.1038/s41586-023-06291-2Nature. 6202023</p>
<p>Can large language models empower molecular property prediction?. C Qian, H Tang, Z Yang, H Liang, Y Liu, 10.48550/arXiv.2307.07443arXiv:2307.074432023arXiv preprint</p>
<p>Can generalist foundation models outcompete special-purpose tuning? case study in medicine. H Nori, 10.48550/arXiv.2311.16452arXiv:2311.164522023arXiv preprint</p>
<p>Large language models in medicine. A J Thirunavukarasu, D S J Ting, K Elangovan, 10.1038/s41591-023-02448-8Nature Medicine. 291930-1940 (2023</p>
<p>Towards causal representation learning. B Schölkopf, 10.48550/arXiv.2102.11107arXiv:2102.111072021arXiv preprint</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. L Huang, 10.1145/3703155ACM Trans. Inf. Syst. 432025</p>
<p>Survey of hallucination in natural language generation. Z Ji, ACM Comput. Surv. 55382023</p>
<p>Lost in translation: the valley of death across preclinical and clinical divide -identification of problems and overcoming obstacles. A A Seyhan, 10.1186/s41231-019-0050-7Translational Medicine Communications. 4182019</p>
<p>Key challenges facing data-driven multicellular systems biology. P Macklin, arXiv:1806.047362018. 27 Sep 2019arXiv preprintVersion 2, last revised</p>
<p>Q Jin, B Dhingra, Z Liu, W W Cohen, X Lu, Pubmedqa, 10.48550/arXiv.1909.06146arXiv:1909.06146A dataset for biomedical research question answering. 2019arXiv preprint</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Y Wang, 10.48550/arXiv.2406.01574arXiv:2406.015742024arXiv preprint</p>
<p>Gpqa: A graduate-level google-proof q&amp;a benchmark. D Rein, 10.48550/arXiv.2311.12022arXiv:2311.120222023arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. -Ai Deepseek, arXiv:2501.129482025arXiv preprint</p>
<p>A Yang, arXiv:2505.09388Qwen3 technical report. 2025arXiv preprint</p>
<p>Hello gpt-4o. Openai, 2024</p>
<p>Openai, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Gemini 2.5 flash -fast performance, native multimodality, and long-context capability. G Deepmind, 2025</p>
<p>Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. G Comanici, 10.48550/arXiv.2507.06261arXiv:2507.062612025arXiv preprint</p>
<p>Position: Intelligent science laboratory requires the integration of cognitive and embodied ai. S Zhang, 10.48550/arXiv.2506.19613arXiv:2506.196132025arXiv preprint</p>
<p>Rankl biology: bone metabolism, the immune system, and beyond. T Ono, M Hayashi, F Sasaki, T Nakashima, 10.1186/s41232-019-0111-3Inflammation and Regeneration. 402020</p>
<p>A homologue of the tnf receptor and its ligand enhance t-cell growth and dendritic-cell function. D Anderson, E Maraskovsky, W Billingsley, 10.1038/36593Nature. 3901997</p>
<p>Osteoprotegerin ligand is a cytokine that regulates osteoclast differentiation and activation. D L Lacey, 10.1016/S0092-8674(00)81569-XS0092-8674(00)81569-XCell. 931998</p>
<p>Osteoclast differentiation factor is a ligand for osteoprotegerin/osteoclastogenesis-inhibitory factor and is identical to trance/rankl. H Yasuda, 10.1073/pnas.95.7.3597Proceedings of the National Academy of Sciences of the United States of America. 951998</p>
<p>Denosumab: mechanism of action and clinical outcomes. D A Hanley, J D Adachi, A Bell, V Brown, 10.1111/ijcp.1202222967310PMC3549483International Journal of Clinical Practice. 662012. 2012 Sep 12</p>
<p>Denosumab for prevention of fractures in postmenopausal women with osteoporosis. S R Cummings, 10.1056/NEJMoa0809493New England Journal of Medicine. 3612009</p>
<p>Denosumab, a fully human monoclonal antibody to rankl, inhibits bone resorption and increases bmd in knock-in mice that express chimeric (murine/human) rankl. P J Kostenuik, 10.1359/jbmr.081112Journal of Bone and Mineral Research. 242009</p>
<p>Denosumab in patients with giant-cell tumour of bone: an open-label, phase 2 study. D Thomas, 10.1016/S1470-2045(10)70010-3The Lancet Oncology. 112010. 2010 Feb 10</p>
<p>Studies in molecular dynamics. i. general method. B J Alder, T E Wainwright, The Journal of Chemical Physics. 311959</p>
<p>Dynamics of folded proteins. J A Mccammon, B R Gelin, M Karplus, Nature. 2671977</p>
<p>A programmable dual-rna-guided dna endonuclease in adaptive bacterial immunity. M Jinek, Science. 3372012</p>
<p>Multiplex genome engineering using crispr/cas systems. L Cong, Science. 3392013</p>
<p>Rna-guided human genome engineering via cas9. P Mali, Science. 3392013</p>
<p>The convergence of ai and synthetic biology: the looming revolution. C S Groff-Vindman, Nature. 2025</p>
<p>Proteinbench: A holistic evaluation of protein foundation models. F Ye, arXiv:2409.067442024arXiv preprint</p>
<p>Mitigating llm hallucinations using a multi-agent framework. A M Darwish, E A Rashed, G Khoriba, Information. 165172025</p>
<p>Medical hallucinations in foundation models and their impact on healthcare. Y Kim, arXiv:2503.057772025arXiv preprint</p>
<p>Building production-ready ai agents with scalable long-term memory. P Chhikara, D Khant, S Aryan, T Singh, D Yadav, Mem0, arXiv:2504.194132025arXiv preprint</p>
<p>Biomni: A general-purpose biomedical ai agent. K Huang, bioRxiv. 2025</p>
<p>xtrimopglm: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. B Chen, X Cheng, P Li, 10.1038/s41592-025-02636-zNature Methods. 222025</p>
<p>A Yang, arXiv:2407.10671Qwen2.5: A party of foundation models. 2024arXiv preprint</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Trinitydna: A bio-inspired foundational model for efficient long-sequence dna modeling. Q Yang, arXiv:2507.192292025. Jul 20251arXiv preprint</p>
<p>Rnagenesis: A generalist foundation model for functional rna therapeutics. Z Zhang, bioRxiv. 2025</p>
<p>Large-scale foundation model on single-cell transcriptomics. M Hao, J Gong, X Zeng, 10.1038/s41592-024-02305-7Nature Methods. 212024</p>
<p>Drug synergistic combinations predictions via large-scale pre-training and graph structure learning. Z Hu, 10.48550/arXiv.2301.05931arXiv:2301.059312023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4372 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4372</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4372</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-271310513</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.13993v2.pdf" target="_blank">LLAssist: Simple Tools for Automating Literature Review Using Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> This paper introduces LLAssist, an open-source tool designed to streamline literature reviews in academic research. In an era of exponential growth in scientific publications, researchers face mounting challenges in efficiently processing vast volumes of literature. LLAssist addresses this issue by leveraging Large Language Models (LLMs) and Natural Language Processing (NLP) techniques to automate key aspects of the review process. Specifically, it extracts important information from research articles and evaluates their relevance to user-defined research questions. The goal of LLAssist is to significantly reduce the time and effort required for comprehensive literature reviews, allowing researchers to focus more on analyzing and synthesizing information rather than on initial screening tasks. By automating parts of the literature review workflow, LLAssist aims to help researchers manage the growing volume of academic publications more efficiently.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4372.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4372.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLAssist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLAssist</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source C# tool that leverages large language models to automate initial literature review steps by extracting key semantics from titles/abstracts, scoring relevance to user-defined research questions, and producing per-article JSON/CSV outputs with reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLAssist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLAssist is a lightweight console application implemented in C# that ingests bibliographic CSV exports (title, abstract, metadata), uses configurable LLM backends to extract 'key semantics' (topics, entities, keywords) from title+abstract, evaluates relevance to a set of user-defined research questions, produces continuous relevance/contribution scores (0–1) and binary decisions, and emits per-article JSON and incrementally-written CSV outputs including the model's reasoning. It is designed as a transparent, human-in-the-loop screening aid rather than a full autonomous synthesis engine. Key components: Microsoft.SemanticKernel for LLM API interactions, CsvHelper for CSV IO, Microsoft.Extensions.Logging for diagnostics, and a prompt-driven metadata/classifier generator. The sequence organizes per-article LLM calls, score normalization, and aggregation for trend analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Llama 3:8B (local), Gemma 2:9B (local), GPT-3.5-turbo-0125 (OpenAI), GPT-4o-2024-05-13 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt-based semantic extraction from titles and abstracts producing topics, entities, and keywords; LLM-generated metadata/classifier; binary relevance thresholding on continuous relevance scores.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Aggregation and scoring across papers (relevance/contribution counts, must-read identification), trend/time-series counts per research question, and CSV/JSON output for downstream manual synthesis; no multi-document abstractive hierarchical synthesis or knowledge-graph construction is implemented in current version.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Tested on small datasets (17, 37, 115 papers) and a large dataset of 2,576 papers (Scopus export).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Evaluated on literature about LLM applications in cybersecurity (papers retrieved via IEEE Xplore and Scopus).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Per-article JSON (semantics, relevance scores, reasoning) and CSV summary (tabular metadata, scores, binary flags); aggregated counts and trend statistics (must-read, contributing).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Consistency of evaluations across papers, accuracy in matching papers to research questions (manual/uncontrolled checks), meaningfulness of reasoning/explanations; cross-backend comparisons among LLM models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Processing times per article: Llama 3 ~10–11 s, Gemma 2 & GPT-3.5 ~12–14 s, GPT-4o ~24–29 s. End-to-end: 17–37 articles in under 10 minutes; 115 articles in 20–50 minutes; 2,576 articles in ~10–11 hours. Cost estimates: GPT-4o ≈ $3.16 per 100 articles, GPT-3.5 ≈ $0.22 per 100 articles; Gemma 2 and Llama 3 run locally so no cloud cost reported. Large-dataset outcome (Gemma 2 run): out of 2,576 articles, 324 (12.6%) classified as must-read and 100 (3.9%) as potentially contributing for the target RQs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Comparison primarily across LLM backends (Llama3, Gemma2, GPT-3.5, GPT-4o); qualitative comparison to human/manual screening throughput from literature but no controlled human-vs-system benchmark reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Authors report substantial time savings relative to human screening rates (citing prior literature) and model-dependent selectivity differences (e.g., Gemma2 more discriminative, Llama3 more permissive in scores vs. binary decisions, GPT-4o more middle-ground). No numeric head-to-head human accuracy comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLAssist can reliably extract salient semantics from titles/abstracts and produce interpretable relevance scores and reasoning; different LLM backends exhibit distinct behaviour requiring prompt tuning; local models (Gemma2, Llama3) enable cost-free runs and competitive speed; the tool meaningfully reduces initial manual screening workload and surfaces temporal trends per research question.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on LLM quality and prompt/input formatting; limited to titles and abstracts (may miss full-text content); does not exploit richer metadata (e.g., citation counts, years) in relevance scoring; observed inconsistency across models (score vs binary disagreement), potential for inaccurate or misleading LLM reasoning (hallucination), and no controlled accuracy evaluation environment.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Per-article processing time remained consistent across dataset sizes, indicating linear scaling in throughput; large runs (2.5k papers) completed in ~10–11 hours. Model-dependent scaling observed: Llama3 fastest, GPT-4o slowest; local models remove cloud cost constraints enabling larger offline processing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLAssist: Simple Tools for Automating Literature Review Using Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4372.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4372.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitLLM (a toolkit for scientific literature review)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A toolkit referenced by the paper that targets automated literature review workflows using LLMs (title: 'Litllm: A toolkit for scientific literature review').</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Litllm: A toolkit for scientific literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as a toolkit for scientific literature review; the paper cites Agarwal et al. (2024) but does not describe LitLLM's architecture or internal components. The mention implies it is an LLM-based toolkit to assist literature review tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature review (as implied by title).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as prior work demonstrating growing interest in LLMs for automating literature review tasks; specific findings not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLAssist: Simple Tools for Automating Literature Review Using Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4372.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4372.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Joos2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cutting through the clutter: The potential of LLMs for efficient filtration in systematic literature reviews</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study that performed an extensive evaluation of using LLMs to enhance screening in literature reviews and reported promising potential to reduce human workload.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cutting through the clutter: The potential of llms for efficient filtration in systematic literature reviews</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Joos et al. (2024) screening evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as an extensive evaluation of LLMs' use in the screening process for systematic literature reviews; the LLAssist paper does not provide architectural or experimental details beyond stating Joos et al. evaluated LLMs and found promising results for reducing human workload.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Systematic literature review screening (general/methodological).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported as an 'extensive evaluation' but specific metrics are not enumerated in the LLAssist paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Described qualitatively as indicating promising potential for reducing human workload; no numerical results supplied in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Implied comparison to human screening workload; details not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can potentially reduce human workload in screening stages of systematic reviews (as reported by Joos et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLAssist: Simple Tools for Automating Literature Review Using Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4372.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4372.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRISMA-dfLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PRISMA-dfLLM (an extension of PRISMA for systematic literature reviews using domain-specific finetuned large language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that extends the PRISMA systematic review framework by incorporating domain-specific fine-tuned large language models to support systematic literature reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prisma-dfllm: An extension of prisma for systematic literature reviews using domain-specific finetuned large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PRISMA-dfLLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an extension of the PRISMA reporting/guideline framework that integrates domain-specific fine-tuned LLMs to assist systematic literature review processes; the LLAssist paper references Susnjak (2023) but provides no implementation details itself.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Systematic literature reviews (domain-specific LLM adaptation implied).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Presented as a domain-specific finetuned LLM extension of PRISMA, suggesting a pathway to integrate LLMs into established systematic review methodology.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLAssist: Simple Tools for Automating Literature Review Using Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4372.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4372.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Monteiro2021</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A roadmap toward the automatic composition of systematic literature reviews</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited roadmap paper that discusses automatic composition of systematic literature reviews; referenced as related work in automated literature review research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A roadmap toward the automatic composition of systematic literature reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Automatic composition roadmap (Monteiro et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as a prior roadmap discussing directions for automatic composition of systematic literature reviews; LLAssist references it as part of the motivation for automation but gives no specifics on methods employing LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Systematic literature review automation (general).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Serves as conceptual background arguing for automation in composing literature reviews; not described here as an implemented LLM system.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLAssist: Simple Tools for Automating Literature Review Using Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Litllm: A toolkit for scientific literature review. <em>(Rating: 2)</em></li>
                <li>Cutting through the clutter: The potential of llms for efficient filtration in systematic literature reviews <em>(Rating: 2)</em></li>
                <li>Prisma-dfllm: An extension of prisma for systematic literature reviews using domain-specific finetuned large language models. <em>(Rating: 2)</em></li>
                <li>A roadmap toward the automatic composition of systematic literature reviews. <em>(Rating: 1)</em></li>
                <li>Automated screening of research studies for systematic reviews using study characteristics. <em>(Rating: 1)</em></li>
                <li>Machine learning algorithms for systematic review: reducing workload in a preclinical review of animal studies and reducing human screening error. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4372",
    "paper_id": "paper-271310513",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "LLAssist",
            "name_full": "LLAssist",
            "brief_description": "An open-source C# tool that leverages large language models to automate initial literature review steps by extracting key semantics from titles/abstracts, scoring relevance to user-defined research questions, and producing per-article JSON/CSV outputs with reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLAssist",
            "system_description": "LLAssist is a lightweight console application implemented in C# that ingests bibliographic CSV exports (title, abstract, metadata), uses configurable LLM backends to extract 'key semantics' (topics, entities, keywords) from title+abstract, evaluates relevance to a set of user-defined research questions, produces continuous relevance/contribution scores (0–1) and binary decisions, and emits per-article JSON and incrementally-written CSV outputs including the model's reasoning. It is designed as a transparent, human-in-the-loop screening aid rather than a full autonomous synthesis engine. Key components: Microsoft.SemanticKernel for LLM API interactions, CsvHelper for CSV IO, Microsoft.Extensions.Logging for diagnostics, and a prompt-driven metadata/classifier generator. The sequence organizes per-article LLM calls, score normalization, and aggregation for trend analysis.",
            "llm_model_used": "Llama 3:8B (local), Gemma 2:9B (local), GPT-3.5-turbo-0125 (OpenAI), GPT-4o-2024-05-13 (OpenAI)",
            "extraction_technique": "Prompt-based semantic extraction from titles and abstracts producing topics, entities, and keywords; LLM-generated metadata/classifier; binary relevance thresholding on continuous relevance scores.",
            "synthesis_technique": "Aggregation and scoring across papers (relevance/contribution counts, must-read identification), trend/time-series counts per research question, and CSV/JSON output for downstream manual synthesis; no multi-document abstractive hierarchical synthesis or knowledge-graph construction is implemented in current version.",
            "number_of_papers": "Tested on small datasets (17, 37, 115 papers) and a large dataset of 2,576 papers (Scopus export).",
            "domain_or_topic": "Evaluated on literature about LLM applications in cybersecurity (papers retrieved via IEEE Xplore and Scopus).",
            "output_type": "Per-article JSON (semantics, relevance scores, reasoning) and CSV summary (tabular metadata, scores, binary flags); aggregated counts and trend statistics (must-read, contributing).",
            "evaluation_metrics": "Consistency of evaluations across papers, accuracy in matching papers to research questions (manual/uncontrolled checks), meaningfulness of reasoning/explanations; cross-backend comparisons among LLM models.",
            "performance_results": "Processing times per article: Llama 3 ~10–11 s, Gemma 2 & GPT-3.5 ~12–14 s, GPT-4o ~24–29 s. End-to-end: 17–37 articles in under 10 minutes; 115 articles in 20–50 minutes; 2,576 articles in ~10–11 hours. Cost estimates: GPT-4o ≈ $3.16 per 100 articles, GPT-3.5 ≈ $0.22 per 100 articles; Gemma 2 and Llama 3 run locally so no cloud cost reported. Large-dataset outcome (Gemma 2 run): out of 2,576 articles, 324 (12.6%) classified as must-read and 100 (3.9%) as potentially contributing for the target RQs.",
            "comparison_baseline": "Comparison primarily across LLM backends (Llama3, Gemma2, GPT-3.5, GPT-4o); qualitative comparison to human/manual screening throughput from literature but no controlled human-vs-system benchmark reported.",
            "performance_vs_baseline": "Authors report substantial time savings relative to human screening rates (citing prior literature) and model-dependent selectivity differences (e.g., Gemma2 more discriminative, Llama3 more permissive in scores vs. binary decisions, GPT-4o more middle-ground). No numeric head-to-head human accuracy comparison provided.",
            "key_findings": "LLAssist can reliably extract salient semantics from titles/abstracts and produce interpretable relevance scores and reasoning; different LLM backends exhibit distinct behaviour requiring prompt tuning; local models (Gemma2, Llama3) enable cost-free runs and competitive speed; the tool meaningfully reduces initial manual screening workload and surfaces temporal trends per research question.",
            "limitations_challenges": "Relies on LLM quality and prompt/input formatting; limited to titles and abstracts (may miss full-text content); does not exploit richer metadata (e.g., citation counts, years) in relevance scoring; observed inconsistency across models (score vs binary disagreement), potential for inaccurate or misleading LLM reasoning (hallucination), and no controlled accuracy evaluation environment.",
            "scaling_behavior": "Per-article processing time remained consistent across dataset sizes, indicating linear scaling in throughput; large runs (2.5k papers) completed in ~10–11 hours. Model-dependent scaling observed: Llama3 fastest, GPT-4o slowest; local models remove cloud cost constraints enabling larger offline processing.",
            "uuid": "e4372.0",
            "source_info": {
                "paper_title": "LLAssist: Simple Tools for Automating Literature Review Using Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "LitLLM",
            "name_full": "LitLLM (a toolkit for scientific literature review)",
            "brief_description": "A toolkit referenced by the paper that targets automated literature review workflows using LLMs (title: 'Litllm: A toolkit for scientific literature review').",
            "citation_title": "Litllm: A toolkit for scientific literature review.",
            "mention_or_use": "mention",
            "system_name": "LitLLM",
            "system_description": "Referenced as a toolkit for scientific literature review; the paper cites Agarwal et al. (2024) but does not describe LitLLM's architecture or internal components. The mention implies it is an LLM-based toolkit to assist literature review tasks.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "General scientific literature review (as implied by title).",
            "output_type": null,
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited as prior work demonstrating growing interest in LLMs for automating literature review tasks; specific findings not described in this paper.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4372.1",
            "source_info": {
                "paper_title": "LLAssist: Simple Tools for Automating Literature Review Using Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Joos2024",
            "name_full": "Cutting through the clutter: The potential of LLMs for efficient filtration in systematic literature reviews",
            "brief_description": "A referenced study that performed an extensive evaluation of using LLMs to enhance screening in literature reviews and reported promising potential to reduce human workload.",
            "citation_title": "Cutting through the clutter: The potential of llms for efficient filtration in systematic literature reviews",
            "mention_or_use": "mention",
            "system_name": "Joos et al. (2024) screening evaluation",
            "system_description": "Mentioned as an extensive evaluation of LLMs' use in the screening process for systematic literature reviews; the LLAssist paper does not provide architectural or experimental details beyond stating Joos et al. evaluated LLMs and found promising results for reducing human workload.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "Systematic literature review screening (general/methodological).",
            "output_type": null,
            "evaluation_metrics": "Reported as an 'extensive evaluation' but specific metrics are not enumerated in the LLAssist paper.",
            "performance_results": "Described qualitatively as indicating promising potential for reducing human workload; no numerical results supplied in this paper.",
            "comparison_baseline": "Implied comparison to human screening workload; details not provided here.",
            "performance_vs_baseline": null,
            "key_findings": "LLMs can potentially reduce human workload in screening stages of systematic reviews (as reported by Joos et al.).",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4372.2",
            "source_info": {
                "paper_title": "LLAssist: Simple Tools for Automating Literature Review Using Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "PRISMA-dfLLM",
            "name_full": "PRISMA-dfLLM (an extension of PRISMA for systematic literature reviews using domain-specific finetuned large language models)",
            "brief_description": "An approach that extends the PRISMA systematic review framework by incorporating domain-specific fine-tuned large language models to support systematic literature reviews.",
            "citation_title": "Prisma-dfllm: An extension of prisma for systematic literature reviews using domain-specific finetuned large language models.",
            "mention_or_use": "mention",
            "system_name": "PRISMA-dfLLM",
            "system_description": "Cited as an extension of the PRISMA reporting/guideline framework that integrates domain-specific fine-tuned LLMs to assist systematic literature review processes; the LLAssist paper references Susnjak (2023) but provides no implementation details itself.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "Systematic literature reviews (domain-specific LLM adaptation implied).",
            "output_type": null,
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Presented as a domain-specific finetuned LLM extension of PRISMA, suggesting a pathway to integrate LLMs into established systematic review methodology.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4372.3",
            "source_info": {
                "paper_title": "LLAssist: Simple Tools for Automating Literature Review Using Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Monteiro2021",
            "name_full": "A roadmap toward the automatic composition of systematic literature reviews",
            "brief_description": "A cited roadmap paper that discusses automatic composition of systematic literature reviews; referenced as related work in automated literature review research.",
            "citation_title": "A roadmap toward the automatic composition of systematic literature reviews.",
            "mention_or_use": "mention",
            "system_name": "Automatic composition roadmap (Monteiro et al.)",
            "system_description": "Cited as a prior roadmap discussing directions for automatic composition of systematic literature reviews; LLAssist references it as part of the motivation for automation but gives no specifics on methods employing LLMs.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "Systematic literature review automation (general).",
            "output_type": null,
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Serves as conceptual background arguing for automation in composing literature reviews; not described here as an implemented LLM system.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4372.4",
            "source_info": {
                "paper_title": "LLAssist: Simple Tools for Automating Literature Review Using Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Litllm: A toolkit for scientific literature review.",
            "rating": 2,
            "sanitized_title": "litllm_a_toolkit_for_scientific_literature_review"
        },
        {
            "paper_title": "Cutting through the clutter: The potential of llms for efficient filtration in systematic literature reviews",
            "rating": 2,
            "sanitized_title": "cutting_through_the_clutter_the_potential_of_llms_for_efficient_filtration_in_systematic_literature_reviews"
        },
        {
            "paper_title": "Prisma-dfllm: An extension of prisma for systematic literature reviews using domain-specific finetuned large language models.",
            "rating": 2,
            "sanitized_title": "prismadfllm_an_extension_of_prisma_for_systematic_literature_reviews_using_domainspecific_finetuned_large_language_models"
        },
        {
            "paper_title": "A roadmap toward the automatic composition of systematic literature reviews.",
            "rating": 1,
            "sanitized_title": "a_roadmap_toward_the_automatic_composition_of_systematic_literature_reviews"
        },
        {
            "paper_title": "Automated screening of research studies for systematic reviews using study characteristics.",
            "rating": 1,
            "sanitized_title": "automated_screening_of_research_studies_for_systematic_reviews_using_study_characteristics"
        },
        {
            "paper_title": "Machine learning algorithms for systematic review: reducing workload in a preclinical review of animal studies and reducing human screening error.",
            "rating": 1,
            "sanitized_title": "machine_learning_algorithms_for_systematic_review_reducing_workload_in_a_preclinical_review_of_animal_studies_and_reducing_human_screening_error"
        }
    ],
    "cost": 0.01084675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLAssist: Simple Tools for Automating Literature Review Using Large Language Models
19 Jul 2024</p>
<p>Yoga Christoforus 
School of Science
RMIT University
3000MelbourneVICAustralia</p>
<p>Haryanto 
School of Science
RMIT University
3000MelbourneVICAustralia</p>
<p>LLAssist: Simple Tools for Automating Literature Review Using Large Language Models
19 Jul 20244CFC85AC35AA3FA91766ECACA58EC4A8arXiv:2407.13993v1[cs.DL]Large Language ModelsArtificial IntelligenceResearch ToolsSemantic AnalysisAutomated Document Processing
This paper introducesLLAssist, an open-source tool designed to streamline literature reviews in academic research.In an era of exponential growth in scientific publications, researchers face mounting challenges in efficiently processing vast volumes of literature.LLAssist addresses this issue by leveraging Large Language Models (LLMs) and Natural Language Processing (NLP) techniques to automate key aspects of the review process.Specifically, it extracts important information from research articles and evaluates their relevance to user-defined research questions.The goal of LLAssist is to significantly reduce the time and effort required for comprehensive literature reviews, allowing researchers to focus more on analyzing and synthesizing information rather than on initial screening tasks.By automating parts of the literature review workflow, LLAssist aims to help researchers manage the growing volume of academic publications more efficiently.</p>
<p>INTRODUCTION</p>
<p>The landscape of academic research is undergoing a dramatic transformation, driven by an unprecedented surge in scientific publications.Identifying relevant work, once a manageable task, has evolved into a time-consuming and often overwhelming process [Khan et al., 2003, Davis et al., 2014].The exponential growth of scientific publications [Silva Júnior andDutra, 2021, Ioannidis et al., 2021], the need to screen hundreds or thousands of articles, and pressure for rapid-evidence gathering present significant challenges for researchers, potentially compromising research quality [McDermott et al., 2024, Glasziou et al., 2020, Whiting et al., 2016, Page et al., 2021, Wallace et al., 2010].</p>
<p>In response to these mounting challenges, the research community has begun exploring automated solutions to assist in the systematic literature review process [Silva Júnior and Dutra, 2021, Wallace et al., 2010, Tsafnat et al., 2018, Bannach-Brown et al., 2019].Recently, there has been a growing interest in evaluating the potential of Large Language Models (LLMs) for this purpose [Agarwal et al., 2024, Joos et al., 2024, Susnjak, 2023].These advanced AI systems, with their ability to understand and generate human-like text, offer promising avenues for automating various aspects of the literature review process.</p>
<p>A notable contribution to this emerging field comes from Joos et al. [2024], who recently published an extensive evaluation of using LLMs in enhancing the screening process, with results indicating promising potential for reducing human workload.Inspired by these findings, we present LLAssist, a prototype automation tool based on LLM technology.In its current phase, LLAssist is designed as a lightweight and straightforward solution to efficiently process large volumes of articles, extract key information, and assess relevance to specific research questions, thereby streamlining the literature review process.Eventually, we aim LLAssist to be one of the key components for a human-friendly, automated knowledge base and research platform.</p>
<p>By using a predetermined process of key semantics extraction, relevance estimation, and must-read determination, LLAssist provides a simple building block for researchers to expand from the idea.This design choice allows for better adaptability to different research domains and provides researchers with more transparent and interpretable results.It is important to note that while LLAssist shows promise in its current form, it represents an initial step towards more sophisticated literature review automation.The simplicity of its design is intentional, allowing researchers to understand, modify, and build upon its core functionalities as needed for their specific research contexts.</p>
<p>Output Generation LLAssist provides two types of output: 1. a JSON file containing detailed information for each processed article, including extracted semantics, relevance scores, and reasoning, and 2. a CSV file presenting the same information in a tabular format, suitable for further analysis or import into other tools.Necessitating other analysis and tools is deliberate to ensure that the human-in-the-loop principle is adhered to by maintaining the visibility of the process [Buçinca et al., 2021, Vasconcelos et al., 2023, Bansal et al., 2021].</p>
<p>Experimental Evaluation</p>
<p>Data Collection</p>
<p>To evaluate the effectiveness of LLAssist in streamlining the literature review process for LLM applications in cybersecurity, we conducted two separate experiments using manual sampling of publications from datasets from different sources:</p>
<ol>
<li>
<p>IEEE Xplore Database: using the search query "llm AND cyber AND security" (term a), effectively focusing on recent publications for initial system testing.</p>
</li>
<li>
<p>Scopus Database: using the search queries "llm AND cyber AND security" (term b), "llm OR ( generative AND artificial AND intelligence ) AND cyber AND security" (term c), and a broad "artificial AND intelligence AND cyber AND security" -limited to Conference paper and Article -(term d) to obtain a diverse dataset in a consecutively larger sample size.</p>
</li>
</ol>
<p>The experiments were designed to assess LLAssist's performance across different academic databases and to ensure a comprehensive evaluation of its capabilities in the fields already known by the authors.There will be two parts: a small dataset test and a large dataset test.The small dataset test is to help manually verify the functionality of LLAssist and the large dataset test is to confirm its utility in enhancing the screening in structured literature review.</p>
<p>From the IEEE Xplore dataset, we exported 17 relevant articles (term a).The Scopus dataset provided an additional set of 37 (term b), 115 (term c), and 2,576 (term d) articles, expanding our corpus.Each dataset included metadata such as titles, abstracts, authors, publication venues, and keywords.The data was exported using the CSV export function to ensure consistency and compatibility with LLAssist's input requirements.</p>
<p>Research Questions</p>
<p>We formulated four key research questions to guide our automated analysis:</p>
<p>• RQ1: How are Large Language Models (LLMs) being utilized to enhance threat detection and analysis in cybersecurity applications?</p>
<p>• RQ2: What are the potential risks and vulnerabilities introduced by integrating LLMs into cybersecurity tools and frameworks?</p>
<p>• RQ3: How effective are LLM-based approaches in generating and detecting adversarial examples or evasive malware compared to traditional methods?</p>
<p>• RQ4: What ethical considerations and privacy concerns arise from using LLMs to analyze and process sensitive cybersecurity data?</p>
<p>Automated Analysis</p>
<p>We processed each paper through LLAssist, which performed the following tasks: 1. Extract key semantics (topics, entities, and keywords) from the title and abstract.2. Evaluate the relevance of each paper to our research questions.3. Provide relevance and contribution scores (0-1 scale) for each research question.4. Generate reasoning for relevance and contribution assessments.</p>
<p>Evaluation Metrics</p>
<p>We assessed the performance of LLAssist based on i) consistency of evaluations across papers, ii) accuracy in matching papers to relevant research questions, and iii) ability to provide meaningful insights and reasoning.Additionally, we are using several different LLM backends: Llama 3:8B, Gemma 2:9B, GPT-3.5-turbo-0125, and GPT-4o-2024-05-13 to allow data comparison.</p>
<p>Preliminary Nature of Evaluation</p>
<p>It is important to note that the assessment of accuracy in matching papers to relevant research questions and the ability to provide meaningful insights and reasoning was conducted in an uncontrolled environment.Knowing that LLM results can be helpful yet inaccurate, we expect the researchers to use LLAssist as a lightweight filtering enhancement tool while following existing methodologies such as PRISMA [Page et al., 2021, Susnjak, 2023].</p>
<p>TECHNICAL IMPLEMENTATION</p>
<p>The program is designed to work with various LLM providers, including local models (e.g., Ollama Llama 3, Ollama Gemma 2) and cloud-based models (e.g., OpenAI's GPT-3.5 and GPT-4).This flexibility allows researchers to choose models based on their specific requirements, such as processing speed, accuracy, or data privacy concerns LLAssist is implemented in C# and utilizes the following key components: 1. Microsoft.SemanticKernel: For interaction with LLM using OpenAI-compatible API, 2. CsvHelper: For reading input and writing output CSV files, and 3. Microsoft.Extensions.Logging: For logging and debugging purposes.</p>
<p>Unlike many existing research tools, LLAssist is developed in C#, a static and strongly typed language, offering benefits such as improved performance, early error detection, and enhanced maintainability [Nanz and Furia, 2015], suitable for further integration into a bigger enterprise system.Additionally, the system will write the CSV output file incrementally to ensure better durability in case of a crash.The sequence diagram can be seen in Figure 1.</p>
<p>Small Dataset Test</p>
<p>The small dataset verified the functionality of the system with the following key results:</p>
<p>Key Semantics Extraction</p>
<p>LLAssist successfully identified relevant topics, entities, and keywords for each paper, aligning well with the author-provided keywords and terms for all LLM backends.As this data is currently used as the input to the LLM prompt, there is no controlled measurement done in this experiment.Result data can be reviewed in the CSV.Also, note that the metadata for the classifier is generated by the same LLM.</p>
<p>Binary Relevance Decision and Score Distribution</p>
<p>The binary relevance decision and relevance score distribution obtained experiment are shown in Figure 2. Further, the summary of binary relevance and binary contribution decisions are shown in Table 1.  3. GPT-4o-2024-05-13 demonstrates a more balanced distribution between relevant and non-relevant articles.It appears more selective than GPT-3.5 and slightly more selective than Gemma 2, yet more permissive than Llama3 in binary classifications.GPT-4o also tends to return a middle-ground score: while it may imply a sophisticated evaluation, it may also indicate avoiding judgments.</p>
<ol>
<li>Llama 3:8B exhibits a significant discrepancy between its relevance scores and binary classifications.In binary classification, it's the most conservative and frequently marking articles as not relevant.However, its relevance scores show more diversity, with a range of values that don't always align with its binary decisions.This inconsistency suggests potential issues in threshold setting or score-to-classification conversion for this model, hence we decided to not use the binary relevance decisions from Llama 3 as the basis for the literature screening performance analysis.</li>
</ol>
<p>Must-read vs. Discard Ratio</p>
<p>All small dataset test runs show a relatively low discard ratio, which is expected due to the specificity of the search term, broad research questions, and the low dataset diversity.A comparison between each test run can be seen in Figure 3.</p>
<p>Reasoning Quality</p>
<p>A manual review of the system provided coherent explanations for its relevance and contribution assessments, offering insights into why each paper was or wasn't considered relevant to each research question.There is no controlled experiment done to measure quantitatively.The LLMs have to output their reasoning to help the researchers in manually discriminating the articles [Vasconcelos et al., 2023] and can be also part of cognitive forcing functions to be the checkpoint before downstream processing [Buçinca et al., 2021].</p>
<p>Large Dataset Test</p>
<p>Table 1 shows two types of binary decisions made by LLAssist: the binary relevance and the binary contribution indicator.Referring to the row id: SL-Gemma2 which contains the result of running large dataset test using Gemma 2:9B, the analysis of the larger Scopus dataset (2,576 articles) revealed key insights:</p>
<ol>
<li>
<p>Trend in Relevance: There's a notable increase in potentially relevant articles from 2020 to 2023, with a peak in 2023 (869 articles, 117 must-read).The slight decrease in 2024 reflects the mid-year data collection cut-off rather than a decline in research quality.Additionally, identifying the most relevant articles accurately is more important than finding a large number of potentially relevant articles.3. Must-Read vs. Contributing Articles: While 324 articles (12.6%) are identified as must-read, only 100 articles (3.9%) are classified as potentially contributing.This suggests that LLAssist are more selective in identifying articles that directly contribute to answering the research questions, implying reduced time to manually read the abstract.</p>
</li>
<li>
<p>Year-over-Year Growth: The number of potentially relevant articles increased significantly from 2020 to 2023, indicating growing research interest in the field.2023 stands out as a pivotal year with the highest numbers of potentially high-quality and relevant publications across all categories.</p>
</li>
<li>
<p>Research Question Focus: RQ2 consistently receives the most attention, suggesting that potential risks and vulnerabilities of LLMs in cybersecurity are a primary concern in the field.RQ4, focusing on ethical considerations, shows the least but growing number of relevant articles, particularly from 2022 onward.</p>
</li>
</ol>
<p>ANALYSIS AND DISCUSSION</p>
<p>Overall Performance</p>
<p>LLAssist effectively identifies relevant papers, works with various LLM backends, and significantly reduces manual screening time.On the other hand, the system did not utilize all available metadata (e.g., publication year, citation counts) in its relevance assessments, which could have provided additional context.Also, different LLMs behave differently, necessitating more precise prompt tuning.The analysis was also limited to titles and abstracts, potentially missing relevant information contained in the full text of the papers.</p>
<p>Time and Cost Efficiency</p>
<p>LLAssist's throughput varies across models and dataset sizes.It processes datasets of 17-37 articles in under 10 minutes, 115 articles in 20-50 minutes, and 2,576 articles in 10-11 hours.Among the models tested, GPT4o emerges as the slowest, processing articles in 24-29 seconds on average.Llama3 is the fastest, consistently quick at 10-11 seconds per article.Gemma2 and GPT35 offer similar speeds, averaging 12-14 seconds for each article processed.The per-article processing times remain consistent across dataset sizes, indicating scalability.This is a significant improvement over human performance [Wallace et al., 2010, Joos et al., 2024].</p>
<p>Cost-wise, GPT-4o is the most expensive at approximately $3.16 per 100 articles while GPT-3.5 offers a more budget-friendly option at about $0.22 per 100 articles.Meanwhile, both Gemma 2 and Llama 3 do not have a set cost due to the ability to run locally without cloud services.Notably, the high discrimination ability of Gemma 2 may help researchers to do the initial screening of many articles without relying on cloud services.</p>
<p>FUTURE WORK</p>
<p>LLAssist's limitations include dependence on LLM quality and input formatting, focus on titles and abstracts, and potential misalignment with human judgment.Future work should aims to incorporate full-text analysis, implement feedback mechanisms, and develop domain-specific models for improved accuracy.</p>
<p>CONCLUSION</p>
<p>In conclusion, LLAssist demonstrated promising capabilities in automating the initial stages of a literature review.Its ability to quickly process and categorize papers offers valuable support to researchers.However, there is room for improvement in utilizing more of the available metadata and fine-tuning the relevance criteria to better differentiate between highly relevant and marginally relevant papers.By leveraging Large Language Models, it offers researchers a valuable tool to efficiently process large volumes of academic literature.While not a replacement for human judgment, LLAssist can significantly reduce the time spent on initial screening and help researchers focus their efforts on the most promising and relevant articles for their research questions, allowing researchers to focus on high-quality work, to achieve higher productivity across expertise and industry.</p>
<p>AVAILABILITY</p>
<p>The source code for LLAssist is freely available at https://github.com/cyharyanto/llassist.We encourage researchers to use, modify, and contribute to this tool to further advance the efficiency of academic literature reviews across various disciplines.</p>
<p>ACKNOWLEDGMENT</p>
<p>We would like to express our deep gratitude to Dr. Arathi Arakala, Dr. Argho Bandyopadhyay, and Dr. Jessica Helmi from RMIT University for their invaluable guidance in systematic literature review methodologies.Their insights into the challenges faced during the screening of papers for surveys and systematic literature reviews shaped the development and focus of LLAssist.We are thankful for their support and shared knowledge.</p>
<p>Figure 1 :
1
Figure 1: Sequence diagram of LLAssist console application</p>
<p>Figure 2 :
2
Figure 2: Binary Relevance and Relevance Store Distribution (small datasets)</p>
<p>1.</p>
<p>Gemma 2:9B shows a reasonable distribution of binary relevance classification and relevance score.It tends to give a strong binary decision and classification score compared to all other LLMs with variation across research questions, indicating sensitivity to different topics.</p>
<p>Figure 3 :
3
Figure 3: Must-read vs. Discard ratio (all datasets)</p>
<p>2.</p>
<p>Research Question Specifics: RQ2 (risks and vulnerabilities of LLMs in cybersecurity) consistently has the highest number of relevant and contributing articles across years.It indicates it's likely the most well-defined or central question.RQ1 (LLMs for threat detection) shows a sharp increase in relevance from 2022 to 2023.RQ3 (LLMs for adversarial examples/evasive malware) and RQ4 (ethical considerations) have fewer articles but show an upward trend, indicating more specialized areas of increasing importance.</p>
<p>Table 1 :
1
Binary Relevance and Binary Contribution Decisions (all datasets)
N Relevant ArticlesN Contributing ArticlesN Total R Any RQ1 RQ2 RQ3 RQ4 C Any RQ1 RQ2 RQ3 RQ4IES-Gemma2171791319168913IES-GPT351717161716171716171617IES-GPT4o171511822139611IES-Llama3171095101110402SS-Gemma23733252671230181925SS-GPT353737373733373737373337SS-GPT4o372820167324151152SS-Llama337231996127231244SM-Gemma21156545502423462933511SM-GPT35115115111114109114115111114109114SM-GPT4o1155736292311432424116SM-Llama3115513319191634729128SL-Gemma2<em>Total2576324153201110951002775162620203013616211410404102021380271115127816402022571521737151616015072023869117606944304215306102024455924959253230112059SL-Llama3257653638712318277916122319748</em> SL-Gemma2 data is broken down by year</p>
<p>Litllm: A toolkit for scientific literature review. Shubham Agarwal, Issam H Laradji, Laurent Charlin, Christopher Pal, 10.48550/ARXIV.2402.017882024</p>
<p>Machine learning algorithms for systematic review: reducing workload in a preclinical review of animal studies and reducing human screening error. Alexandra Bannach-Brown, Piotr Przyby La, James Thomas, Andrew S C Rice, Sophia Ananiadou, Jing Liao, Malcolm Robert Macleod, 10.1186/s13643-019-0942-7Systematic Reviews. 2046-4053812312 2019Online; accessed 2024-07-17</p>
<p>Does the whole exceed its parts? the effect of ai explanations on complementary team performance. Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, Daniel Weld, 10.1145/34117645 2021ACMYokohama Japan</p>
<p>10.1145/3411764.3445717Online; accessed 2024-07-03URL. </p>
<p>To trust or to think: Cognitive forcing functions can reduce overreliance on ai in ai-assisted decision-making. Zana Buçinca, Maja Barbara Malaya, Krzysztof Z Gajos, 10.1145/3449287Online; accessed 2024-07-03Proceedings of the ACM on Human-Computer Interaction. 2573-01425CSCW14 2021</p>
<p>Viewing systematic reviews and meta-analysis in social research through different lenses. Jacqueline Davis, Kerrie Mengersen, Sarah Bennett, Lorraine Mazerolle, 10.1186/2193-1801-3-511SpringerPlus. 2193- 18013151112 2014Online; accessed 2024-07-17</p>
<p>Large language models for tabular data: Progresses and future directions. Haoyu Dong, Zhiruo Wang, 10.1145/3626772.3661384ACM. ISBN. 97984007043147 2024Online; accessed 2024-07-17</p>
<p>Waste in covid-19 research. Sharon Paul P Glasziou, Tammy Sanders, Hoffmann, 10.1136/bmj.m1847BMJ. 1756-183318475 2020Online; accessed 2024-07-17</p>
<p>The rapid, massive growth of covid-19 authors in the scientific literature. P A John, Maia Ioannidis, Kevin W Salholz-Hillel, Jeroen Boyack, Baas, 10.1098/rsos.210389Royal Society Open Science. 2054- 5703892103899 2021Online; accessed 2024-07-17</p>
<p>Cutting through the clutter: The potential of llms for efficient filtration in systematic literature reviews. Lucas Joos, Daniel A Keim, Maximilian T Fischer, 10.48550/ARXIV.2407.106522024</p>
<p>Five steps to conducting a systematic review. Regina Khalid S Khan, Jos Kunz, Gerd Kleijnen, Antes, 10.1177/014107680309600304Journal of the Royal Society of Medicine. 0141-07689633 2003Online; accessed 2024-07-17</p>
<p>The quality of covid-19 systematic reviews during the coronavirus 2019 pandemic: an exploratory comparison. Kevin T Mcdermott, Mark Perry, Willemijn Linden, Rachel Croft, Robert Wolff, Jos Kleijnen, 10.1186/s13643-024-02552-xac- cessed 2024-07-17Systematic Reviews. 2046-40531311265 2024</p>
<p>A comparative study of programming languages in rosetta code. Sebastian Nanz, Carlo A Furia, 10.1109/ICSE.2015.90Online; accessed 2024-07-185 2015Florence, Italy</p>
<p>The prisma 2020 statement: an updated guideline for reporting systematic reviews. Joanne E Matthew J Page, Patrick M Mckenzie, Isabelle Bossuyt, Tammy C Boutron, Cynthia D Hoffmann, Larissa Mulrow, Jennifer M Shamseer, Elie A Tetzlaff, Sue E Akl, Roger Brennan, Julie Chou, Jeremy M Glanville, Asbjørn Grimshaw, Hróbjartsson, M Manoj, Tianjing Lalu, Elizabeth W Li, Evan Loder, Steve Mayo-Wilson, Luke A Mcdonald, Lesley A Mcguinness, James Stewart, Andrea C Thomas, Vivian A Tricco, Penny Welch, David Whiting, Moher, 10.1136/bmj.n71BMJ. 1756-1833713 2021Online; accessed 2024-07-17</p>
<p>A roadmap toward the automatic composition of systematic literature reviews. Monteiro Eugênio, Silva Da, Moisés Júnior, Dutra Lima, 10.47909/ijsmc.52Iberoamerican Journal of Science Measurement and Communication. 2709-3158127 2021Online; accessed 2024-07-17</p>
<p>Prisma-dfllm: An extension of prisma for systematic literature reviews using domain-specific finetuned large language models. Teo Susnjak, 10.48550/ARXIV.2306.149052023</p>
<p>Automated screening of research studies for systematic reviews using study characteristics. Guy Tsafnat, Paul Glasziou, George Karystianis, Enrico Coiera, 10.1186/s13643-018-0724-7Systematic Reviews. 2046-4053716412 2018Online; accessed 2024-07-17</p>
<p>Explanations can reduce overreliance on ai systems during decision-making. Helena Vasconcelos, Matthew Jörke, Madeleine Grunde-Mclaughlin, Tobias Gerstenberg, Michael S Bernstein, Ranjay Krishna, 10.1145/3579605Online; accessed 2024-07-03Proceedings of the ACM on Human-Computer Interaction. 2573-01427CSCW14 2023</p>
<p>Semi-automated screening of biomedical citations for systematic reviews. Thomas A Byron C Wallace, Joseph Trikalinos, Carla Lau, Christopher H Brodley, Schmid, 10.1186/1471-2105-11-55BMC Bioinformatics. 1471- 21051115512 2010Online; accessed 2024-07-17</p>
<p>Robis: A new tool to assess risk of bias in systematic reviews was developed. Penny Whiting, Jelena Savović, P T Julian, Deborah M Higgins, Barnaby C Caldwell, Beverley Reeves, Philippa Shea, Jos Davies, Rachel Kleijnen, Churchill, 10.1016/j.jclinepi.2015.06.005Journal of Clinical Epidemiology. 691 2016Online; accessed 2024-07-17</p>            </div>
        </div>

    </div>
</body>
</html>
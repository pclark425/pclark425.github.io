<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-845 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-845</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-845</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-23.html">extraction-schema-23</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <p><strong>Paper ID:</strong> paper-259251453</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.14325v2.pdf" target="_blank">The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs</a></p>
                <p><strong>Paper Abstract:</strong> Human beings are social creatures. We routinely reason about other agents, and a crucial component of this social reasoning is inferring people's goals as we learn about their actions. In many settings, we can perform intuitive but reliable goal inference from language descriptions of agents, actions, and the background environments. In this paper, we study this process of language driving and influencing social reasoning in a probabilistic goal inference domain. We propose a neuro-symbolic model that carries out goal inference from linguistic inputs of agent scenarios. The"neuro"part is a large language model (LLM) that translates language descriptions to code representations, and the"symbolic"part is a Bayesian inverse planning engine. To test our model, we design and run a human experiment on a linguistic goal inference task. Our model closely matches human response patterns and better predicts human judgements than using an LLM alone.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e845.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e845.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NIPE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuro-Symbolic Inverse Planning Engine</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neurosymbolic system that uses an LLM to translate natural language descriptions into programmatic/PDDL scene descriptions which condition a probabilistic generative model over maps and domain rules, and performs Bayesian inverse planning (nested planning with a Boltzmann policy + A* optimal-cost planner) to infer posterior probabilities over agent goals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neuro-Symbolic Inverse Planning Engine (NIPE)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NIPE composes three main components: (1) a language-to-code translator implemented with an LLM (few-shot semantic parsing) that converts sentences into program expressions / conditions that build a symbolic scene and domain, (2) a probabilistic generative model over spatial maps and PDDL scene representations conditioned by those program expressions (sampling valid grid-world maps and domain rules consistent with language), and (3) Bayesian inverse planning over the instantiated PDDL domain to compute posterior distributions over latent goals given observed actions. Action selection is modeled via a Boltzmann-rational policy whose Q-values are computed using an optimal-cost planner (A*). NIPE returns posterior probabilities over candidate goals and is evaluated against human judgements.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL (symbolic PDDL scene representations conditioned by a probabilistic generative model)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>States and scenes are represented as concrete PDDL scene descriptions encoding agent locations, object locations and attributes (e.g., colors, keys, doors). Domain rules (operators, preconditions, effects) are expressed in PDDL. The paper places a probabilistic generative model over maps/scenes consistent with the language conditions (sampling valid grid-world maps that satisfy CONDITION statements) so that uncertainty over the concrete scene configuration is represented as a distribution over PDDL scenes. Action selection is stochastic via a Boltzmann (softmax) policy over Q-values; goal posterior is a Bayesian posterior over goals given action likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>language-to-code translation / semantic parsing to construct and condition symbolic generative models (PDDL scenes and operators); used also as baselines to directly give judgments</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo used for translation); GPT-3.5 and GPT-4 evaluated as baselines</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>state uncertainty (uncertain concrete maps/scenes), goal uncertainty (posterior over goals), action-selection stochasticity (Boltzmann policy)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>probability distributions: a generative probabilistic model over maps (sampling), Bayesian posterior inference over goals P(g|s0,a1:t), Boltzmann-rational action likelihoods (softmax over Q-values); rejection sampling used for LLM-produced code to enforce validity</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>A* optimal-cost planner used to compute Q-values for goals; nested / Bayesian inverse planning (compute P(a1:t | g) from planner-derived costs and Boltzmann policy)</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td>Custom linguistic 'gameshow' inverse-planning domain (text-based)</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>A set of linguistic scenarios describing an obstacle-course 'gameshow' with trophies, keys, and locked doors (and spatial variants). Language specifies world configuration, abstract dynamics (key-door rules), and observed agent actions; the environment is not a standard benchmark but a controlled, text-only task designed to require integrating language about world dynamics and agent behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Correlation (Pearson R) between model posterior probabilities and mean human Likert judgments per stimulus</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>NIPE: R = 0.927 (95% CI = (0.877, 0.953)) aggregated across stimuli. (Reported per-paper; posterior probabilities produced for each trophy.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>GPT-4 direct LLM baseline: R = 0.658 (95% CI = (0.403, 0.79)); GPT-3.5 direct LLM baseline: R = 0.100 (95% CI = (-0.145, 0.361)). Baselines were zero-shot prompts producing Likert-style ratings; NIPE outperforms both baselines overall and is more robust across task variants.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No explicit ablation comparing NIPE with and without probabilistic uncertainty over maps or with/without LLM uncertainty modeling is reported. The paper reports comparisons to LLM-only baselines and discusses that the language-to-code translation step is currently non-Bayesian (bottom-up); future work is suggested to make translation probabilistic/Bayesian.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A pipeline that translates language into symbolic PDDL scenes and domain operators and then performs Bayesian inverse planning over a probabilistic generative model of maps yields posterior goal inferences that correlate strongly with human judgments (R=0.927). LLMs alone (GPT-4, GPT-3.5) are less robust, especially on tasks requiring nontrivial world-dynamics reasoning or efficient spatial path planning. NIPE models uncertainty over concrete scene realizations and over goals and action likelihoods, but it does not model LLM translation uncertainty within the Bayesian inference (translation is performed bottom-up and then conditioned on deterministically after rejection-sampling valid translations). The paper highlights benefits of coupling symbolic probabilistic world models (PDDL + generative map prior) with LLM-based semantic parsing, while noting future work to integrate LLM uncertainty more fully in a Bayesian way.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e845.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e845.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PDDL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Planning Domain Definition Language</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standardized symbolic language for defining planning domains: actions (operators), parameters, preconditions, and effects; used here to encode domain rules (locks, keys, unlock actions) and concrete PDDL scene descriptions for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PDDL -The Planning Do- main Definition language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PDDL (Planning Domain Definition Language)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used to express operator definitions (actions with parameters, preconditions, and effects) and concrete scene/state encodings (agent and object locations, attributes such as colors). In NIPE, sentences about domain dynamics are translated to PDDL operator definitions (e.g., unlock actions) and the sampled concrete maps are instantiated as PDDL scene files for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL (symbolic operator-based domain representation)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>PDDL encodes actions as operators with preconditions and effects; states described by predicate facts (e.g., locked(d), has(agent,key), iscolor(key,color), locations). In NIPE these PDDL descriptions are instantiated as deterministic operators; stochasticity arises from distributions over which concrete PDDL scenes/maps are sampled to satisfy language conditions rather than probabilistic PDDL operators.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Used as input to A* and planners that compute optimal costs; PDDL itself is a domain description language, not a planning algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PDDL is used as the symbolic formalism for expressing domain dynamics and concrete scene instantiations. The paper extends the usual use by composing PDDL scene instances with a probabilistic generative prior over maps to represent uncertainty over concrete scenes that satisfy linguistic descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e845.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e845.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Online Bayesian Goal Inference (Zhi-Xuan et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Online bayesian goal inference for boundedly rational planning agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method for Bayesian inverse planning that models agents as boundedly rational planners and performs online inference over goals given sequential observations; cited as foundational for NIPE's nested planning and inference algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Online bayesian goal inference for boundedly rational planning agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Online Bayesian Goal Inference (nested planner / online inference)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An approach that models agents as planners with approximate rationality and performs online Bayesian inference over goals as actions are observed; NIPE builds on this nested planning and inference approach for computing action likelihoods and updating posterior over goals.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL-style planning domains / symbolic planning representations (as used in the cited work and referenced by NIPE)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Represents environments and agent actions in planning form; computes likelihoods of observed actions given goals via an assumed policy (e.g., softmax over plan costs) and updates posterior beliefs online as observations arrive. Inference accounts for bounded rationality by parameterizing rationality (inverse temperature).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>goal uncertainty, action stochasticity (bounded rationality)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Bayesian posterior over goals given observed actions and probabilistic action model (Boltzmann/softmax policy over Q-values)</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Uses planners to compute costs/Q-values for goals (optimal planners) and computes action likelihoods under a Boltzmann policy; inference is online Bayesian updating</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as the algorithmic foundation for NIPE's nested planning and Bayesian inference (computing P(a1:t | g) using a Boltzmann policy and planner-derived Q-values). NIPE adapts and composes this inference with LLM-driven language-to-code translation and a probabilistic generative prior over maps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e845.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e845.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-to-Probabilistic-Program Translation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-to-code translation into probabilistic programs / PDDL scenes using LLMs (few-shot semantic parsing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses LLMs (few-shot prompting) to translate natural language descriptions into executable program expressions or probabilistic program constructs that condition a generative model over worlds; cited prior work and implemented in NIPE for mapping language to PDDL and generative model conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>From word models to world models: Translating from natural language to the probabilistic language of thought</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based language-to-code semantic parsing for probabilistic programs / PDDL conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NIPE and referenced prior work (e.g., Wong et al., Zhang et al.) use LLMs trained on language and code to translate sentences into programmatic CONDITION statements, PDDL operator definitions, or probabilistic-program fragments which are then executed to instantiate / condition a generative model over scenes and domain rules. The approach is implemented with few-shot prompts, sampling with temperature, and rejection sampling to ensure executable/valid translations.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>Probabilistic programs instantiated as distributions over PDDL scenes (probabilistic generative model over maps/scenes)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Language maps to programmatic constraints (e.g., CONDITION statements) that filter or condition a prior over maps; the conditioned generative model yields concrete PDDL scenes which encode states; domain rules can likewise be generated or modified via LLM output (PDDL operator text). The representation combines symbolic PDDL descriptions with a probabilistic prior over possible instantiations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>semantic parsing / translation of language into program expressions and PDDL operator/scene text; used to condition probabilistic generative model</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3.5 (used for translation in NIPE implementation); few-shot prompting; other works referenced use similar models</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>The paper notes that translation is performed bottom-up and non-Bayesian (they do rejection-sampling to ensure valid code), and suggests future work to make translation itself probabilistic/Bayesian. No ablation isolating translation uncertainty is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can be effectively used as semantic parsers to translate rich linguistic descriptions into symbolic/PDDL constructs that condition probabilistic world models; however, in NIPE the translation step is not modeled probabilistically within the Bayesian pipeline, and the authors suggest integrating LLM uncertainty (e.g., LLMs as likelihoods) as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Online bayesian goal inference for boundedly rational planning agents <em>(Rating: 2)</em></li>
                <li>From word models to world models: Translating from natural language to the probabilistic language of thought <em>(Rating: 2)</em></li>
                <li>Grounded physical language understanding with probabilistic programs and simulated worlds <em>(Rating: 2)</em></li>
                <li>Sequential monte carlo steering of large language models using probabilistic programs <em>(Rating: 2)</em></li>
                <li>Generalized planning in pddl domains with pretrained large language models <em>(Rating: 2)</em></li>
                <li>Llm+p: Empowering large language models with optimal planning proficiency <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-845",
    "paper_id": "paper-259251453",
    "extraction_schema_id": "extraction-schema-23",
    "extracted_data": [
        {
            "name_short": "NIPE",
            "name_full": "Neuro-Symbolic Inverse Planning Engine",
            "brief_description": "A neurosymbolic system that uses an LLM to translate natural language descriptions into programmatic/PDDL scene descriptions which condition a probabilistic generative model over maps and domain rules, and performs Bayesian inverse planning (nested planning with a Boltzmann policy + A* optimal-cost planner) to infer posterior probabilities over agent goals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Neuro-Symbolic Inverse Planning Engine (NIPE)",
            "system_description": "NIPE composes three main components: (1) a language-to-code translator implemented with an LLM (few-shot semantic parsing) that converts sentences into program expressions / conditions that build a symbolic scene and domain, (2) a probabilistic generative model over spatial maps and PDDL scene representations conditioned by those program expressions (sampling valid grid-world maps and domain rules consistent with language), and (3) Bayesian inverse planning over the instantiated PDDL domain to compute posterior distributions over latent goals given observed actions. Action selection is modeled via a Boltzmann-rational policy whose Q-values are computed using an optimal-cost planner (A*). NIPE returns posterior probabilities over candidate goals and is evaluated against human judgements.",
            "world_model_type": "PDDL (symbolic PDDL scene representations conditioned by a probabilistic generative model)",
            "world_model_description": "States and scenes are represented as concrete PDDL scene descriptions encoding agent locations, object locations and attributes (e.g., colors, keys, doors). Domain rules (operators, preconditions, effects) are expressed in PDDL. The paper places a probabilistic generative model over maps/scenes consistent with the language conditions (sampling valid grid-world maps that satisfy CONDITION statements) so that uncertainty over the concrete scene configuration is represented as a distribution over PDDL scenes. Action selection is stochastic via a Boltzmann (softmax) policy over Q-values; goal posterior is a Bayesian posterior over goals given action likelihoods.",
            "uses_llm": true,
            "llm_role": "language-to-code translation / semantic parsing to construct and condition symbolic generative models (PDDL scenes and operators); used also as baselines to directly give judgments",
            "llm_model_name": "GPT-3.5 (gpt-3.5-turbo used for translation); GPT-3.5 and GPT-4 evaluated as baselines",
            "uncertainty_modeling": true,
            "uncertainty_type": "state uncertainty (uncertain concrete maps/scenes), goal uncertainty (posterior over goals), action-selection stochasticity (Boltzmann policy)",
            "uncertainty_method": "probability distributions: a generative probabilistic model over maps (sampling), Bayesian posterior inference over goals P(g|s0,a1:t), Boltzmann-rational action likelihoods (softmax over Q-values); rejection sampling used for LLM-produced code to enforce validity",
            "planning_algorithm": "A* optimal-cost planner used to compute Q-values for goals; nested / Bayesian inverse planning (compute P(a1:t | g) from planner-derived costs and Boltzmann policy)",
            "planning_integrates_uncertainty": true,
            "text_environment_name": "Custom linguistic 'gameshow' inverse-planning domain (text-based)",
            "text_environment_description": "A set of linguistic scenarios describing an obstacle-course 'gameshow' with trophies, keys, and locked doors (and spatial variants). Language specifies world configuration, abstract dynamics (key-door rules), and observed agent actions; the environment is not a standard benchmark but a controlled, text-only task designed to require integrating language about world dynamics and agent behavior.",
            "performance_metric": "Correlation (Pearson R) between model posterior probabilities and mean human Likert judgments per stimulus",
            "performance_value": "NIPE: R = 0.927 (95% CI = (0.877, 0.953)) aggregated across stimuli. (Reported per-paper; posterior probabilities produced for each trophy.)",
            "baseline_comparison": "GPT-4 direct LLM baseline: R = 0.658 (95% CI = (0.403, 0.79)); GPT-3.5 direct LLM baseline: R = 0.100 (95% CI = (-0.145, 0.361)). Baselines were zero-shot prompts producing Likert-style ratings; NIPE outperforms both baselines overall and is more robust across task variants.",
            "has_ablation_uncertainty": false,
            "ablation_results": "No explicit ablation comparing NIPE with and without probabilistic uncertainty over maps or with/without LLM uncertainty modeling is reported. The paper reports comparisons to LLM-only baselines and discusses that the language-to-code translation step is currently non-Bayesian (bottom-up); future work is suggested to make translation probabilistic/Bayesian.",
            "key_findings": "A pipeline that translates language into symbolic PDDL scenes and domain operators and then performs Bayesian inverse planning over a probabilistic generative model of maps yields posterior goal inferences that correlate strongly with human judgments (R=0.927). LLMs alone (GPT-4, GPT-3.5) are less robust, especially on tasks requiring nontrivial world-dynamics reasoning or efficient spatial path planning. NIPE models uncertainty over concrete scene realizations and over goals and action likelihoods, but it does not model LLM translation uncertainty within the Bayesian inference (translation is performed bottom-up and then conditioned on deterministically after rejection-sampling valid translations). The paper highlights benefits of coupling symbolic probabilistic world models (PDDL + generative map prior) with LLM-based semantic parsing, while noting future work to integrate LLM uncertainty more fully in a Bayesian way.",
            "uuid": "e845.0",
            "source_info": {
                "paper_title": "The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "PDDL",
            "name_full": "Planning Domain Definition Language",
            "brief_description": "A standardized symbolic language for defining planning domains: actions (operators), parameters, preconditions, and effects; used here to encode domain rules (locks, keys, unlock actions) and concrete PDDL scene descriptions for planning.",
            "citation_title": "PDDL -The Planning Do- main Definition language",
            "mention_or_use": "use",
            "system_name": "PDDL (Planning Domain Definition Language)",
            "system_description": "Used to express operator definitions (actions with parameters, preconditions, and effects) and concrete scene/state encodings (agent and object locations, attributes such as colors). In NIPE, sentences about domain dynamics are translated to PDDL operator definitions (e.g., unlock actions) and the sampled concrete maps are instantiated as PDDL scene files for planning.",
            "world_model_type": "PDDL (symbolic operator-based domain representation)",
            "world_model_description": "PDDL encodes actions as operators with preconditions and effects; states described by predicate facts (e.g., locked(d), has(agent,key), iscolor(key,color), locations). In NIPE these PDDL descriptions are instantiated as deterministic operators; stochasticity arises from distributions over which concrete PDDL scenes/maps are sampled to satisfy language conditions rather than probabilistic PDDL operators.",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": false,
            "uncertainty_type": null,
            "uncertainty_method": null,
            "planning_algorithm": "Used as input to A* and planners that compute optimal costs; PDDL itself is a domain description language, not a planning algorithm",
            "planning_integrates_uncertainty": null,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "PDDL is used as the symbolic formalism for expressing domain dynamics and concrete scene instantiations. The paper extends the usual use by composing PDDL scene instances with a probabilistic generative prior over maps to represent uncertainty over concrete scenes that satisfy linguistic descriptions.",
            "uuid": "e845.1",
            "source_info": {
                "paper_title": "The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Online Bayesian Goal Inference (Zhi-Xuan et al., 2020)",
            "name_full": "Online bayesian goal inference for boundedly rational planning agents",
            "brief_description": "A prior method for Bayesian inverse planning that models agents as boundedly rational planners and performs online inference over goals given sequential observations; cited as foundational for NIPE's nested planning and inference algorithm.",
            "citation_title": "Online bayesian goal inference for boundedly rational planning agents",
            "mention_or_use": "mention",
            "system_name": "Online Bayesian Goal Inference (nested planner / online inference)",
            "system_description": "An approach that models agents as planners with approximate rationality and performs online Bayesian inference over goals as actions are observed; NIPE builds on this nested planning and inference approach for computing action likelihoods and updating posterior over goals.",
            "world_model_type": "PDDL-style planning domains / symbolic planning representations (as used in the cited work and referenced by NIPE)",
            "world_model_description": "Represents environments and agent actions in planning form; computes likelihoods of observed actions given goals via an assumed policy (e.g., softmax over plan costs) and updates posterior beliefs online as observations arrive. Inference accounts for bounded rationality by parameterizing rationality (inverse temperature).",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "goal uncertainty, action stochasticity (bounded rationality)",
            "uncertainty_method": "Bayesian posterior over goals given observed actions and probabilistic action model (Boltzmann/softmax policy over Q-values)",
            "planning_algorithm": "Uses planners to compute costs/Q-values for goals (optimal planners) and computes action likelihoods under a Boltzmann policy; inference is online Bayesian updating",
            "planning_integrates_uncertainty": true,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "Cited as the algorithmic foundation for NIPE's nested planning and Bayesian inference (computing P(a1:t | g) using a Boltzmann policy and planner-derived Q-values). NIPE adapts and composes this inference with LLM-driven language-to-code translation and a probabilistic generative prior over maps.",
            "uuid": "e845.2",
            "source_info": {
                "paper_title": "The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "LLM-to-Probabilistic-Program Translation",
            "name_full": "Language-to-code translation into probabilistic programs / PDDL scenes using LLMs (few-shot semantic parsing)",
            "brief_description": "A method that uses LLMs (few-shot prompting) to translate natural language descriptions into executable program expressions or probabilistic program constructs that condition a generative model over worlds; cited prior work and implemented in NIPE for mapping language to PDDL and generative model conditions.",
            "citation_title": "From word models to world models: Translating from natural language to the probabilistic language of thought",
            "mention_or_use": "use",
            "system_name": "LLM-based language-to-code semantic parsing for probabilistic programs / PDDL conditioning",
            "system_description": "NIPE and referenced prior work (e.g., Wong et al., Zhang et al.) use LLMs trained on language and code to translate sentences into programmatic CONDITION statements, PDDL operator definitions, or probabilistic-program fragments which are then executed to instantiate / condition a generative model over scenes and domain rules. The approach is implemented with few-shot prompts, sampling with temperature, and rejection sampling to ensure executable/valid translations.",
            "world_model_type": "Probabilistic programs instantiated as distributions over PDDL scenes (probabilistic generative model over maps/scenes)",
            "world_model_description": "Language maps to programmatic constraints (e.g., CONDITION statements) that filter or condition a prior over maps; the conditioned generative model yields concrete PDDL scenes which encode states; domain rules can likewise be generated or modified via LLM output (PDDL operator text). The representation combines symbolic PDDL descriptions with a probabilistic prior over possible instantiations.",
            "uses_llm": true,
            "llm_role": "semantic parsing / translation of language into program expressions and PDDL operator/scene text; used to condition probabilistic generative model",
            "llm_model_name": "GPT-3.5 (used for translation in NIPE implementation); few-shot prompting; other works referenced use similar models",
            "uncertainty_modeling": false,
            "uncertainty_type": null,
            "uncertainty_method": null,
            "planning_algorithm": null,
            "planning_integrates_uncertainty": null,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": false,
            "ablation_results": "The paper notes that translation is performed bottom-up and non-Bayesian (they do rejection-sampling to ensure valid code), and suggests future work to make translation itself probabilistic/Bayesian. No ablation isolating translation uncertainty is reported.",
            "key_findings": "LLMs can be effectively used as semantic parsers to translate rich linguistic descriptions into symbolic/PDDL constructs that condition probabilistic world models; however, in NIPE the translation step is not modeled probabilistically within the Bayesian pipeline, and the authors suggest integrating LLM uncertainty (e.g., LLMs as likelihoods) as future work.",
            "uuid": "e845.3",
            "source_info": {
                "paper_title": "The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Online bayesian goal inference for boundedly rational planning agents",
            "rating": 2,
            "sanitized_title": "online_bayesian_goal_inference_for_boundedly_rational_planning_agents"
        },
        {
            "paper_title": "From word models to world models: Translating from natural language to the probabilistic language of thought",
            "rating": 2,
            "sanitized_title": "from_word_models_to_world_models_translating_from_natural_language_to_the_probabilistic_language_of_thought"
        },
        {
            "paper_title": "Grounded physical language understanding with probabilistic programs and simulated worlds",
            "rating": 2,
            "sanitized_title": "grounded_physical_language_understanding_with_probabilistic_programs_and_simulated_worlds"
        },
        {
            "paper_title": "Sequential monte carlo steering of large language models using probabilistic programs",
            "rating": 2,
            "sanitized_title": "sequential_monte_carlo_steering_of_large_language_models_using_probabilistic_programs"
        },
        {
            "paper_title": "Generalized planning in pddl domains with pretrained large language models",
            "rating": 2,
            "sanitized_title": "generalized_planning_in_pddl_domains_with_pretrained_large_language_models"
        },
        {
            "paper_title": "Llm+p: Empowering large language models with optimal planning proficiency",
            "rating": 1,
            "sanitized_title": "llmp_empowering_large_language_models_with_optimal_planning_proficiency"
        }
    ],
    "cost": 0.015327,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs</p>
<p>Lance Ying 
Katherine M Collins 
Megan Wei 
Cedegao E Zhang 
Tan Zhi-Xuan 
Adrian Weller 
Joshua B Tenenbaum 
Lionel Wong 
The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs</p>
<p>Human beings are social creatures. We routinely reason about other agents, and a crucial component of this social reasoning is inferring people's goals as we learn about their actions. In many settings, we can perform intuitive but reliable goal inference from language descriptions of agents, actions, and the background environments. In this paper, we study this process of language driving and influencing social reasoning in a probabilistic goal inference domain. We propose a neurosymbolic model that carries out goal inference from linguistic inputs of agent scenarios. The "neuro" part is a large language model (LLM) that translates language descriptions to code representations, and the "symbolic" part is a Bayesian inverse planning engine. To test our model, we design and run a human experiment on a linguistic goal inference task. Our model closely matches human response patterns and better predicts human judgements than using an LLM alone.</p>
<p>Introduction</p>
<p>Humans are deeply social creatures, finely attuned to the inner lives of other people (Wellman, 1992;Saxe, 2006;Tomasello, 2010). We not only think, but think routinely about each other. We imagine what someone is trying to accomplish and why are trying to accomplish it; we try to parse what someone really believes and whether they might be mistaken; and we anticipate people's ultimate goals and upcoming actions, stepping in to assist or sometimes even to try to hinder them. This capacity for social reasoning, broadly, is abstract (we can observe someone's actions in the world, but cannot directly see what they think or want) and relies on a fine grained knowledge of distinct but highly interrelated mental states within others (we know that someone's beliefs, desires, and goals relative to the world itself all collectively influence what they might choose to do.) This paper considers how language can inform social reasoning, with a particular focus on understanding language about people's actions, plans, and goals. Language is an especially powerful means of conveying the abstract, interrelated nature of concepts about agents in the world-we can talk about someone's actions (I saw Annie heading into the living room), directly convey their mental states (Annie desperately wants a toy in there), describe relevant but abstract aspects of the world in which they act (that cupboard Annie is trying to open is locked inside), or even pose the questions we want others to reason about (which toy do you think she wants the most?). Language about any one of these aspects informs how we might reason about someone else in holistic ways, changing our understanding and downstream predictions about what they might know, want, or do.</p>
<p>Empirical evidence suggests that we think intuitively about each other as largely rational, goal-directed intelligent agents (Dennett, 1981;Csibra et al., 1999;Baker et al., 2009;Baillargeon et al., 2016). Within computational cognitive science and AI, a considerable body of prior work formalizes these intuitions by modeling agents as rational actors and planners who choose actions to achieve specified goals or optimize rewards, and in turn models many inferences about mental states as inverse planning from observations of actions back to their latent goals, utility functions, or beliefs about world states (Jara-Ettinger et al., 2016;Baker et al., 2017;Zhi-Xuan et al., 2020;Ying et al., 2023). Despite considerable successes in predicting distinct human inferences and modeling cooperative behavior in human-computer interactions (Carroll et al., 2019;Mozannar &amp; Sontag, 2020;Alanqary et al., 2021;Bansal et al., 2021;Wilder et al., 2021;Shneiderman, 2022), these approaches-largely non-linguistic models that draw inferences from images or demonstrations of agent actions-face scalability and specification challenges inherent to the ab- Figure 1. Overview of our model, the Neuro-Symbolic Inverse Planning Engine (NIPE), which reasons about linguistic goal inference scenarios. (Left, green) We design and measure human judgments on a linguistic goal inference domain (inspired by (Zhi-Xuan et al., 2020), which asks about the final goal of a participant on a complex "gameshow" obstacle course involving trophies behind locked doors. Inferring goals requires reasoning jointly about language that describes the overall world configuration, specific world dynamics, and the agent's actions. Our model works by first using a large language model to translate language into symbolic program semantics that collectively condition a probabilistic generative model over possible environments and agents (Middle, yellow), and answers queries via Bayesian inverse planning (Right) in the conditioned probabilistic model, inferring likely goals using a nested planner to model the agent. stract, complex, and interrelated nature of social reasoning tasks. Modeling agents as goal-directed planners requires implicitly or explicitly representing the world model they are planning over, which is often precisely defined a priori (as in Baker et al. (2017) and Zhi-Xuan et al. (2020)) or learned from expensive exploration (as in Lian et al. (2021)). Inferring mental states, or even posing the task itself, is challenging when framed as an inductive problem from images and demonstrations alone (as in Shu et al. (2021)).</p>
<p>In part due to these challenges, the recent advances made by large language models (LLMs) have driven an explosion of interest in how these models can be used to plan (Valmeekam et al., 2022) or draw social inferences (Bubeck et al., 2023;Shapira et al., 2023) from language-they suggest a new frontier of cognitive models and AI systems that can leverage language to specify abstract world knowledge, domain-specific world dynamics, and direct information about agents and their mental states. However, early efforts to apply LLMs directly to social reasoning problems often take a highly simplified approach to evaluating their capacity for social reasoning or their "theory of mind", drawing broad conclusions based on a narrow set of templated cognitive tasks (like the famous "Sally Anne tasks"). Other eval-uations caution against drawing general conclusions from such restricted task settings, pointing out that modifications and extensions beyond these simple problem settings can produce counterintuitive results that contradict the holism and generality of these inferences (Shapira et al., 2023;Ullman, 2023); and several recent evaluations suggest that robust planning in general remains an outstanding challenge for large language models (Valmeekam et al., 2022;Bubeck et al., 2023). Several recent approaches seek to address these challenges by augmenting LLMs with interfaces to external modules for representing and simulating complex aspects of world state, particularly using classical AI representations and engines for forward planning (Collins et al., 2022;Liu et al., 2023a;Lyu et al., 2023). Emulating human-like social reasoning and knowledge, however, requires models that can robustly draw unified inferences about actions, mental states, and the world , relating plans and beliefs in holistic, flexible, and ideally interpretable and safely deployable way to language; and linguistic evaluations that move in turn towards the richness and breadth of human-like social reasoning, flexible and extensible enough to incorporate these interrelated aspects of human mental states and world knowledge.</p>
<p>We focus specifically in this paper on understanding language that describes inverse planning and goal inference problems-we consider how language can convey information about the world itself, and the actions within it, in order to drive inferences about the unseen goals motivating these actions. We make the following contributions:</p>
<ol>
<li>
<p>We introduce a new model, the Neuro-symbolic Inverse Planning Engine (NIPE) that leverages LLMs to translate language into program expressions that compositionally construct and condition a probabilistic generative model over agents and environments, then uses Bayesian inverse planning to perform goal inference over this symbolic representation, expanding upon the paradigm of rational meaning construction for language-informed cognition .</p>
</li>
<li>
<p>To evaluate this model, we compare our model against human social reasoning on a linguistic inverse planning domain inspired by prior cognitive work in social cognition (Zhi-Xuan et al., 2020). We design this domain to require jointly reasoning about agent actions and varying information about the world itself.</p>
</li>
<li>
<p>We find that our model well-predicts human judgments across a variety of distinct social reasoning scenarios. We also find that our model outperforms LLM baselines used to directly perform the same goal inference tasks. We find that the largest LLM alone (GPT-4 (OpenAI, 2023)) correlates moderately well with certain human judgments on this domain, particularly on simpler scenarios. We discuss trends between LLMs of different sizes and on different task variants in our stimuli.</p>
</li>
</ol>
<p>Linguistic inverse planning tasks and human experiment</p>
<p>We begin by describing the domain of tasks and human experiment we use throughout this paper, to provide intuition for the modeling experiments that follow. We construct a domain of linguistic inverse planning stimuli designed to evaluate how humans and models infer the goals of agents acting within varied environments. We design stimuli to convey varying information about the world configuration and dynamics in which agents behave, as well as the observed actions of agents with unknown goals.</p>
<p>Our tasks are inspired by the spatial navigation stimuli used in prior social reasoning experiments (Zhi-Xuan et al., 2020;Alanqary et al., 2021). Our experiments adapt this basic domain into a set of linguistic inverse planning stimuli, and introduce a range of variations designed to test distinct world dynamics, spatial information, and vague or incomplete information about agent actions conveyed linguistically.</p>
<p>Planning and inverse planning concepts in the Gameshow Domain</p>
<p>Each stimuli in our experiment (Table 1) begins with a description of a "gameshow" scenario in which a player is trying to reach one of several goal trophies placed around an obstacle course. This obstacle course also includes locked doors that can only be unlocked with keys placed around the course, and which additionally require multi-step reasoning about an agent's longer term plans. Based on each scenario description, which includes varying information about the particular obstacle course and the actions of an agent within it, we pose a linguistic goal inference query: which of the possible trophies do you think the player trying to reach?</p>
<p>We design each stimulus to span several core conceptual categories relevant to reasoning about agents and their actions in the world (Table 1, top), including:</p>
<p> World configuration: information about goal trophies, keys, and their attributes, as well as their relative locations, at differing levels of specificity.</p>
<p> Abstract world dynamics: causal dynamics within the environment, such as rules determining which keys can open which doors.</p>
<p> Agent actions: descriptions of an agent's behavior before they reach the goal, like picking up keys or walking in different directions towards keys or trophies.</p>
<p>Task variants</p>
<p>We design four distinct variants of gameshow stimuli for a total of n=18 stimuli, which vary the complexity of the specified world configuration, the world dynamics, and the type of observed agent actions (excerpts from stimuli in each variant are shown in Table 1, bottom):</p>
<p> Generic keys: Goal trophies are placed behind locked doors, and must be unlocked by keys placed around the obstacle course. Any key on the course can be used to open any other door.</p>
<p> Same-colored keys: This variant adds additional complexity in the world dynamics. Keys and doors have colors. Doors can only be unlocked by keys that are the same color as the door.</p>
<p> Different-colored keys: In this variant, doors can only be unlocked by keys that are a different color from the door.</p>
<p> Spatial: This variant contains specific spatial descriptions of the obstacle course and agent actions. Due to the added spatial complexity, in these environments, there are no keys or doors; agents can walk freely towards various goal trophies.</p>
<p>Linguistic planning and inverse planning concepts across tasks</p>
<p>World configuration</p>
<p>There are three trophies placed through this obstacle course: gold, silver, and bronze; The gold trophy is currently 5 steps East of Alice; The silver trophy is behind 1 door.</p>
<p>World dynamics</p>
<p>On this course, each lock can only be opened by a key that is the same color as that lock; In this game, players lose points if they pick up more keys than needed.</p>
<p>Agent actions</p>
<p>You see Alice picking up a green key; You see Alice walking a step towards the South; In this game, players lose points if they pick up more keys than needed.</p>
<p>Goal inference</p>
<p>Considering each of the possible trophies on this course, how likely is it that Alice is trying to reach this trophy?</p>
<p>Task variations</p>
<p>Generic Keys</p>
<p>There are three trophies placed throughout this obstacle course: gold, silver, or bronze. To get to the gold trophy, you need to unlock 2 different doors. To get to the silver trophy, you need to unlock 1 door. The bronze trophy isn't behind a door. Keys to unlock the doors are placed around the obstacle course. All of the keys on this course work on all of the locks. You see Alice pick up 1 key.</p>
<p>Same-Colored Keys</p>
<p>There are three trophies placed throughout this obstacle course: gold, silver, or bronze. The gold and silver trophies are both located behind a yellow door. To go to the bronze trophy, you need to go through a red door. On this course, each lock can only be opened by a key that is the same color as that lock. You see Alice pick up a red key.</p>
<p>Different-Colored Keys</p>
<p>There are three trophies placed throughout this obstacle course: gold, silver, or bronze. The gold and silver trophies are both located behind a green door. To go to the bronze trophy, you need to go through a red door. On this course, each door can be unlocked by any key that is a different color than the door. You see Alice pick up a red key.</p>
<p>Spatial</p>
<p>There are three trophies placed throughout this obstacle course: gold, silver, or bronze. Alice is a participant on this course. The gold trophy is currently 5 steps East of Alice. The silver trophy is 3 steps South from her. The bronze trophy is 5 steps further West of the silver trophy. You see Alice walking a step towards the South. Table 1. Example language from the linguistic "gameshow" goal inference domain. (Top) Each scenario contains language that spans a variety of interrelated concepts relevant to the final goal-inference query, ranging from information about the particular environment configuration to varying information about an agent's actions. (Bottom) We design four distinct task variants that vary in complexity and type of the environment description and the spatial details of the agent's actions.</p>
<p>Human experiment</p>
<p>We collect and evaluate human judgments on these goal inference tasks. We recruit a total of n=46 participants from Prolific 1 ; each viewed a random batch of 7 stimuli from a specific task variant, and for each stimulus, produced goal inference judgements about each possible goal trophy on a 1-7 Likert scale (1: definitely not this trophy; 7: definitely this trophy). Scenarios were presented for a fixed amount of time before an answer was permitted (90 seconds in the Spatial condition; 45 seconds in the others). Each scenario was seen by between 14-16 participants. Participants were native English speakers from the USA/UK. by translating from language into program expressions that condition a symbolic generative model over agents and environments, instantiating the paradigm of rational meaning construction for language-guided cognition . This approach also addresses an important open challenge for prior non-linguistic (inverse) planning models: We can condition on abstract information about the world that is specified in language, such as information about where objects are located or which keys open which doors.</p>
<p>Our model (shown in schematic form in Figure 1) modularly decomposes the problem of goal inference from language into three general subproblems: language-to-code translation, which we instantiate generally by using a LLM as a few-shot semantic parser (similar to Liu et al., 2023b;Collins et al., 2022;Wong et al., 2023)); the construction of a probabilistic generative model over agents and environments from language; and goal inference through Bayesian Inverse Planning in this probabilistic model (using an nested planning and inference algorithm like that in (Zhi-Xuan et al., 2020)). We now describe each problem in detail.</p>
<p>Language-to-code translation using LLMs</p>
<p>Following Wong et al. (2023), the crux of our approach is to show how the different kinds of language in our scenarios can each be translated into program expressions that collectively construct and condition a holistic probabilistic model. We begin, however, by briefly describing the language-tocode translation itself, which we implement by using LLMs to translate sentences into code, which compositionally construct a symbolic model over an extended, multi-sentence linguistic scenario (see Figure 1, LLM Language to Code).</p>
<p>Our translation approach leverages the few-shot prompting capabilities of LLMs trained jointly on language and code. We choose GPT-3.5 (Ouyang et al., 2022) specifically to show that a smaller LLM can be used to implement this more restricted task, rather than using the LLM itself to internally represent and reason over these scenarios. Future work should explore whether even smaller and more restricted language-to-code models can be used. Our approach builds on the methods used in Zhang et al. (2023), which uses an LLM to translate language into probabilistic programs, along with other work showing that LLMs can translate language into code when given paired few-shot examples (Liu et al., 2023a;Wong et al., 2023). For each stimulus, we construct a prompt containing 2-3 randomly sampled examples of paired language and code from held out examples on our domain. Models were sampled with a temperature of 1.2. To ensure robustness, we rejection sample translations to ensure that they pass a number of syntactic and semantic tests (e.g., to ensure that translated code is valid and executable.)</p>
<p>Constructing a probabilistic generative model over agents and environments from language</p>
<p>Now we describe how the different kinds of information relevant to this domain can be each translated into different kinds of code within a unifying generative model. Following Zhi-Xuan et al. (2020), we model information about the world dynamics, such as action preconditions and effects, using the Planning Domain Definition Language (PDDL), a standardized representation language widely used in robotics and automated planning (Aeronautiques et al., 1998), and which can b extended with modular semantics to efficiently represent spatial domains such as ours (Gregory et al., 2012;Zhi-Xuan, 2022).</p>
<p>While prior work has considered how different subproblems related to planning can be extracted via language-to-code (Collins et al., 2022;Liu et al., 2023b;Silver et al., 2023), our aim here is to translate language not only to distributions over possible worlds -i.e. possible spatial layouts and domain rules -but also to inferences over an agent's actions. We describe each aspect of translation below:</p>
<p>Conditions on spatial environments First, we prompt LLM to translate sentences like "The bronze chest is located behind a red door." into a CONDITION statement that describes this arrangement. We then sample concrete grid-world maps from a generative model over maps that satisfy this.</p>
<p>To ensure that maps are valid for our environment, we define and condition a relatively restricted generative model over possible maps and trophy placements that ensures that these basic validity conditions are met. Each sampled map from this generative model is a concrete PDDL scene representation that describes the locations of the initial agent, objects, and their attributes.</p>
<p>Conditions on domain rules Now, consider sentences like "Each door can be unlocked by a key with the corresponding color." These can be straightforwardly translated into operator definitions expressed in PDDL (see Figure 1), which define an action and its necessary parameters, preconditions for trying this action, and effects on the environment. Conditioning a generative model over agent subgoals and actions from language Finally, we consider how to translate sentences that describe agents' actions into conditions on a model over agents. We distinguish between language like "You observe Alice picking up two keys" which is treated as a condition on subgoals, and "You observe Alice walking 3 steps East.", which is treated as a low-level action sequence.</p>
<p>Goal Inference through Bayesian Inverse Planning</p>
<p>Once we have an initial PDDL environment representation s 0 and a sequence of agent actions a 1:t , we can perform Bayesian inverse planning to infer a posterior distribution over goals g given actions a 1:t and the initial state s, which is proportional to the joint distribution over goals and actions: P (g|s 0 , a 1:t )  P (g, a 1:t |s 0 ) = P (g|s 0 )P (a 1:t |g)</p>
<p>Here P (g|s 0 ) defines the prior distribution over possible goals in the initial state s 0 , and P (a 1:t |g) is a model of how the agent selects actions a 1:t given a particular goal g. For the goal prior, we model agents as being more likely to pick up goals that are closer to them, as determined by the cost C(g, s 0 ) of the shortest plan to each goal:
P (g|s 0 )  1 C(g)
To model actions, we assume P (a 1:t |g) decomposes into the probability of selecting an action a t at each state s t , given by a Boltzmmann-rational policy : P (a 1:t |g) = t  =1 (a  |s t , g) (a t |s t , g) = exp Q g (s t , a t )
a t exp Q g (s t , a t )
where Q g (s t , a t ) denotes the (negative) cost of the reaching the goal g by taking action a t from s t , and  is a rationality parameter, also called the inverse temperature. This policy models agents as approximately rational in how they act, since actions which reach the goal more quickly (higher Q g ) are more probable, with higher values of  leading to more optimal actions. Although this model does not account for forms of approximate rationality (Evans et al., 2016;Shah et al., 2019;Alanqary et al., 2021), it is a widely used choice in (Bayesian) inverse planning and inverse reinforcement learning that can account for non-systematic deviations from optimality (Ramachandran &amp; Amir, 2007;Ziebart et al., 2008;Baker et al., 2009). To compute Q g for each state s t and action a t encountered during inference, we use A* search as an optimal-cost planner in the PDDL domain to determine the cost of the optimal plan.</p>
<p>With these components of the model specified, goal inference can be performed exactly by computing the joint probability P (g, a 1:t |s 0 ) of each possible goal g with the observed sequence of actions a 1:t , which can then be normalized to obtain the posterior distribution P (g|s 0 , a 1:t ).</p>
<p>Model Experiments</p>
<p>We compare our model with human judgements on the gameshow domain of social reasoning tasks. We also evaluate two LLM baselines to assess the value of an explicit Bayesian inverse planning model.</p>
<p>For our model, we simulate answers to each goal inference stimulus (which trophy will Alice pick up?), outputting a posterior probability over all of the possible trophies (probabilities sum to 1 over all trophies). For each stimulus, we translate the linguistic scenario description of the world configuration, world dynamics, and agent behavior into symbolic conditions on the generative model (by sampling from the LLM and rejecting invalid code), and perform goal inference with the generated code to calculate the posterior over the trophies. Since goal inference can be performed without sampling, and since the space of maps consistent with a linguistic description is highly restricted, we found in pilot experiments that our model showed limited variability across LLM and generative model samples. Therefore, for each stimulus, we computed our results using a single valid sample of PDDL domain rules and spatial environments.</p>
<p>We also conduct two baseline experiments using two different LLMs (gpt-3.5-turbo-0301, the dialoguetuned variant of GPT-3.5 (Ouyang et al., 2022), and gpt-4-0314, the March snapshot of GPT-4 (OpenAI, 2023)) to directly provide goal inference judgments for each stimulus. We assess LLMs using exactly the same conditions as the human experiment, using a zero-shot prompt where the LLM is given the full linguistic experimental set up (including instructions explaining the task) shown to human participants, concatenated with the scene scenario and query for each stimulus, and a short prompt specifying the answer format.</p>
<p>To account for answer variability, we sample n = 30 answers for each stimulus, rejecting any answers that are not in the correct format. Exact prompt formats can be found in the Appendix.</p>
<p>Results and discussion</p>
<p>We evaluate our model and baselines in comparison to human judgments, both across all of the stimuli ( Figure 2) and within each task variant category (Figure 3). We find that:</p>
<p>Our model (NIPE) correlates strongly overall with human judgments across the goal inference experiment. We calculate correlations between the posterior probability judgments produced by our model and the mean per stimulus judgments (Likert scale ratings) across human subjects, and find that our model is strongly correlated with human judgments in the experiment overall ( Figure 2, Our Model, R=0.927, 95% CI=(0.877, 0.953)). All 95% CI are bootstrap sampled confidence intervals each with 1000 samples.</p>
<p>The largest LLM baseline (GPT-4) correlates moderately with human judgments overall; GPT3.5 correlates poorly. We additionally calculate correlations between LLMs prompted directly to produce goal inference judgments (mean over sampled LLM judgements for each stimulus, and the mean over human judgments for each stimulus across participants). In comparison to our model, we find that GPT-4 correlates well, but more weakly, with human judgments (Figure 2, GPT-4 Rating, R=0.658, CI=(0.403, 0.79)) overall, and GPT-3.5 correlates much more poorly with human judgments overall (Figure 2, GPT-3.5 Rating, R=0.100, CI=(-0.145, 0.361)). These aggregate trends are better understood by examining correlations between human judgments within each task variant, as we discuss next.</p>
<p>NIPE correlates strongly with human judgements within each task variant; GPT-3.5 and GPT-4 are less robust to complex world dynamics and spatial information. Inspecting trends within each individual task variant ( Figure 3) provides insight into when models deviate from humans.</p>
<p>NIPE consistently correlates with human judgments across all task variants, from the simpler Generic stimuli to the more complex Colored and Spatial stimuli (Figure 3, bottom). In contrast, on the more complex tasks-particularly the Colored-Different variant, which specifies a new, counterintuitive rule particular to this domain (doors can only be opened by keys that are a different color from the door), and the Spatial variant, which requires considering how an efficient, path-planning agent would navigate towards their goal-the LLM baselines show revealingly different performance in comparison to human reasoning.</p>
<p>As seen in Figure 3, GPT-3.5 actually correlates negatively with human judgments on Colored-Different-it both ignores the stated rule about the different colored keys and appears to make the opposite, incorrect assumption, perhaps based on simpler color-based pattern matching; and shows nearly zero correlation with human judgments on the explicit Spatial path-planning variant. In comparison, GPT-4 correlates well with human judgments on both of the simpler task variants, and shows a moderate (but positive) correlation on the Spatial variant (though we discuss stimuli-specific failures next, suggesting that GPT-4 may still be using simpler distance heuristics rather than robustly modeling an efficient path-planning agent.) On the Colored-Different domain, GPT-4 has nearly zero-correlation, suggesting that it is highly uneven in taking the specific rule about key colors into account-as shown in Figure 3, it predicts human judgments well in some cases, but produces nearly the opposite overall judgment in others, suggesting it has ignored the rule.</p>
<p>Humans make fine-grained assumptions about agents as rational planners; LLMs do not always do so. Our model explicitly reasons about agents as rational, goaldirected actors and planners. Do LLMs do so as well? Qualitative analysis of individual stimuli highlights subtle but important discrepancies between human assumptions and LLM-based goal inferences on the most complex Spatial stimuli. We highlight a representative example here:</p>
<p>There are three trophies placed throughout this obstacle course: gold, silver, or bronze. Alice is a participant on this course. The gold trophy is currently 2 steps East of Alice. The silver trophy is 3 steps South from her. The bronze trophy is 5 steps South from her. You see Alice walking 4 steps South. Both humans and our model strongly predict that Alice is headed towards the bronze trophy on this domain. In contrast, both the LLM baselines predict most strongly that Alice is heading towards the silver trophy (perhaps because it is closer to Alice), though in fact, humans tend to agree that because she has walked past this trophy, as an efficient actor, this is likely not her goal. We provide additional examples of similar efficient-planning situations in the Appendix.</p>
<p>Conclusion and Future Directions</p>
<p>We conclude with several important avenues for future work. First, while we evaluate all LLMs using a simple prompting regime, future work should evaluate whether these results are consistent or improve with other prompting and evaluation methods such as chain-of-thought Kojima et al., 2022), particularly on complex tasks such as those that invoke efficient spatial path planning, which prior work also finds to be an outstanding challenge for LLMs (Cohn &amp; Hernandez-Orallo, 2023). It is also possible for LLMs to be biased by surface-level cues (e.g. in our stimuli, they might be biased towards gold over silver trophies, since the former are often considered more valuable). Future experiments should control for these potential biases by testing a wider diversity of prompts.</p>
<p>Second, while NIPE adopts a Bayesian approach to social inference, the process of language translation itself is performed in a non-Bayesian "bottom-up" manner using LLMs. Future work might explore a more fully Bayesian approach that models how language itself is generated from symbolic task representations, and perform posterior inference directly from language. This might involve using LLMs as likelihood models over language, as in Ying et al. (2023), and also incorporate LLMs as common sense priors over representations of the world (Lew et al., 2020;Li et al., 2023). Among other benefits, this could enable more controlled and robust translation of language to code (Poesia et al., 2022;Lew et al., 2023).</p>
<p>Finally, we designed the gameshow domain to permit future extensions that describe a much broader range of social reasoning tasks that we could express linguistically. This includes scenarios that specifically invoke agent belief (Baker et al., 2017;Ullman, 2023); involve boundedly rational agents (Alanqary et al., 2021); and even involve multiple agents collaborating or competing (Kleiman-Weiner et al., 2016). In addition, future work could extend this domain to multimodal stimuli that include both linguistic information and demonstrations or images, e.g., to convey complex agent trajectories or environment layouts (Ying et al., 2023).</p>
<p>The difficulty of the domain could also be increased, expanding the number of objects, actions, and planning steps in order to push the limits of language-to-code translation. By building out this architecture, we hope to fully reap the benefits of the both the neural and symbolic traditions of AI, enabling flexible social inferences from a wide range of naturally occurring linguistic stimuli, while producing outputs that are structured, coherent, and robust (Collins et al., 2022).</p>
<p>A.3. LLM Baselines Experiment</p>
<p>We use the gpt-4-0314 model (i.e., a fixed, non-updatable checkpoint from March 14 for maximal reproducibility) in the Chat Completion API. For each stimuli, we sample until we get 20 well-formed responses, rejecting samples that are not in the correct format.</p>
<p>Example LLM Prompt for World Configuration</p>
<p>Input: There are three trophies placed throughout this obstacle course: gold, silver, or bronze. The gold trophy is behind a green door. The silver trophy is behind a yellow and red door. The bronze trophy is behind a red door.</p>
<p>Keys to unlock the doors are placed around the obstacle course. On this course, each door can be unlocked by any key as long as it is of a different color than the door.</p>
<p>You see Alice picking up a red key. What would be the likely trophy Alice wants to get?" output:</p>
<p>{"agent" : ["Alice"], "goals" : ["gold", "silver", "bronze"], "locations" : {"gold" : "Room A", "silver" : "Room B", "bronze" : "Room C"}, "obstacles" : {"Room A" : ["G"], "Room B" : ["Y", "R"],"Room C" : ["R"] }, "keys" : ["g","y","r"], "max_obstacle" : 2, "keys_per_door": 1, "len_key" : 3, "goal_count" : 3, "observation_type": "has_objects", "observation": "(exists (?k -key) (and (has Alice ?k) (iscolor ?k red)))"}</p>
<p>Example LLM Prompt for PDDL Domain Definition</p>
<p>In this example we constrain the model such that the LLM can only modify the unlock action of the PDDL domain definition. Our goal is to show that with few-shot prompting in the color-same condition, the LLM can generalize to generate action definition for color-different condition.</p>
<p>input:</p>
<p>Alice is playing a treasure game to pick up exactly one trophy. There are 3 trophies: gold, silver and bronze, located in room A, room C and room B. There is 1 yellow and 1 green lock for Room A, a red lock for room B and a yellow lock for room C. )) (or (and (= (xloc ?a) (xloc ?d)) (= (-(yloc ?a) 1) (yloc ?d))) (and (= (xloc ?a) (xloc ?d)) (= (+ (yloc ?a) 1) (yloc ?d))) (and (= (-(xloc ?a) 1) (xloc ?d)) (= (yloc ?a) (yloc ?d))) (and (= (+ (xloc ?a) 1) (xloc ?d)) (= (yloc ?a) (yloc ?d))))) :effect (and (not (has ?a ?k)) (not (locked ?d)) ) )</p>
<p>A.4. An example of LLM baseline result</p>
<p>Here we provide an example of GPT-4's response in the baseline condition on the Color-different task variant.</p>
<p>input:</p>
<p>There are three trophies placed throughout this obstacle course: gold, silver, or bronze.</p>
<p>The gold and silver trophies are both located behind a yellow door. To go to the bronze trophy, you need to go through a red door.</p>
<p>Keys to unlock the doors are placed around the obstacle course.</p>
<p>On this course, each door can be unlocked by any key that is a different color than the door. For example, yellow keys cannot unlock yellow doors, but can unlock doors in green or red.</p>
<p>In this game, players lose points if they pick up more keys than needed. You may assume that players will use all the keys they pick up.</p>
<p>Alice is a participant on this course. You see Alice pick up a yellow key.</p>
<p>It's unclear what Alice would do next.</p>
<p>How likely is Alice picking up the trophies? Please rate from 1 to 7 and explain. output:</p>
<p>Gold Trophy: 6 Silver Trophy: 6 Bronze Trophy: 1 Explanation:</p>
<p>Since Alice has picked up a yellow key, she can now unlock either the gold or silver trophy, since both are located behind a yellow door. This makes it quite likely that she will pick up one of those two trophies (rating them both at 6).</p>
<p>However, picking up a yellow key means she cannot unlock the red door, which leads to the bronze trophy. Therefore, it is very unlikely that Alice will pick up the bronze trophy (rating it 1).</p>
<p>It is clear from this example that GPT-4 shows unsatisfactory understanding of the setup of the Color-different variant. It is stated that yellow keys cannot unlock yellow doors, so it is unlikely that Alice would pick up the gold or silver trophy, but GPT-4 still judges these two possibilities to be highly likely. On the other hand, both humans and our NIPE model assign significantly more likelihood to the bronze trophy (and roughly equal low likelihoods to the gold and silver trophies) in this example.</p>
<p>A.5. Sample stimuli for each experimental condition Generic:</p>
<p>There are three trophies placed throughout this obstacle course: gold, silver, or bronze.</p>
<p>To get to the gold trophy, you need to unlock 2 different doors. To get to the silver trophy, you need to unlock 1 door. The bronze trophy isn't behind a door.</p>
<p>Keys to unlock the doors are placed around the obstacle course. On this course, each door has exactly 1 lock on it. All of the keys on this course work on all of the locks. However, each key can only be used one time.</p>
<p>Alice is a participant on this course. You see Alice go over and pick up two keys.</p>
<p>Color-same:</p>
<p>There are three trophies placed throughout this obstacle course: gold, silver, or bronze.</p>
<p>The gold and silver trophy trophies are both located behind a yellow door. To get to the yellow door, you first need to go through a green door. To go to the bronze trophy, you need to go through a red door.</p>
<p>Figure 2 .
2Correlation between NIPE and baseline LLMs vs. human judgments, aggregated across the goal-inference stimuli. Our model (Right) produces posterior probabilities over goals that correlate strongly overall with human judgments; GPT-4 correlates moderately overall (Middle) and GPT-3.5 (Left) correlates poorly across the tasks. Color-coded points show stimuli from specific task variants.</p>
<p>Figure 3 .
3Correlations within specific task variants highlight more granular trends in model behavior. Our model (Bottom) correlates strongly with human judgements across all four task variants. In comparison, the LLM baselines generally show poorer performance on the more complex variants (the Color-Different variant, which introduces a new, counterintuitive rule about the world dynamics; and the Spatial variant, which requires reasoning specifically about agents as efficient path planners.</p>
<p>Alice needs to unlock the locks to enter the rooms. Each lock can be unlocked with a key with the same color. Each key can only be used once.You see Alice go over to pick up a red key.It's unclear what Alice would do next.output: </p>
<p>(:action unlock 
:parameters (?a -agent ?k -key ?d -door) 
:precondition (and (has ?a ?k) (locked ?d) 
(exists (?c -color) (and (iscolor ?k ?c) (iscolor ?d ?c)
Massachusetts Institute of Technology, Cambridge, USA 2 Harvard University, Cambridge, USA 3 University of Cambridge, Cambridge, UK. Correspondence to: Lance Ying <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#108;&#97;&#110;&#99;&#101;&#121;&#45;&#105;&#110;&#103;&#64;&#115;&#101;&#97;&#115;&#46;&#104;&#97;&#114;&#118;&#97;&#114;&#100;&#46;&#101;&#100;&#117;">&#108;&#97;&#110;&#99;&#101;&#121;&#45;&#105;&#110;&#103;&#64;&#115;&#101;&#97;&#115;&#46;&#104;&#97;&#114;&#118;&#97;&#114;&#100;&#46;&#101;&#100;&#117;</a>.
We exclude 6 participants as less than 30% of their MAP results agreed with consensus; i.e. they disagree with the majority on which goal is the most likely for at least 5 out of 7 stimuli.3. Our Model: The Neuro-Symbolic InversePlanning Engine (NIPE)We now introduce our model, the Neuro-Symbolic Inverse Planning Engine (NIPE). While we focus on goal inference tasks, we describe a general framework for conditioning on the diverse range of linguistic information in these stimulivarying information about the spatial environment, world dynamics, and agent actions-that supports inferences about agents based on a principled, probabilistic framework for Bayesian inverse planning.Our core contribution is to show how we can approach complex linguistic scenarios about agents by leveraging structured, symbolic representations and inference methods (modeling agents as explicit planners, and performing principled Bayesian inference over complex models) from the existing cognitive and AI social reasoning literature (eg.(Baker et al., 2017;Zhi-Xuan et al., 2020)). We do so
Keys to unlock the doors are placed around the obstacle course. On this course, each door is unlocked by a key of the corresponding color.In this game, players lose points if they pick up more keys than needed. You may assume that players will use all the keys they pick up.Alice is a participant on this course. You see Alice pick up a yellow key.Color-different:There are three trophies placed throughout this obstacle course: gold, silver, or bronze, which are placed in rooms labeled A, B, and C, respectively.Room A is behind a door with a yellow and green lock on it. Room B is behind a door with a red lock on it. Room C is behind a door with its own yellow lock on it.Keys to unlock the doors are placed around the obstacle course. On this course, each door can be unlocked by any key that is a different color than the door. For example, yellow keys cannot unlock yellow doors, but can unlock green and red doors In this game, players lose points if they pick up more keys than needed. You may assume that players will use all the keys they pick up.Alice is a participant on this course. You see Alice pick up a yellow key.It's unclear what Alice would do next.Spatial:There are three trophies placed throughout this obstacle course: gold, silver, or bronze. Alice is a participant on this course.The gold trophy is currently 1 step East of Alice. The silver trophy is 3 steps South from her. The bronze trophy is 3 steps East from the silver trophy.You see Alice walking 3 steps East.
PDDL -The Planning Domain Definition language. C Aeronautiques, A Howe, C Knoblock, I D Mcdermott, A Ram, M Veloso, D Weld, D W Sri, A Barrett, D Christianson, Tech. Rep.Aeronautiques, C., Howe, A., Knoblock, C., McDermott, I. D., Ram, A., Veloso, M., Weld, D., SRI, D. W., Barrett, A., Christianson, D., et al. PDDL -The Planning Do- main Definition language. Technical Report, Tech. Rep., 1998.</p>
<p>Modeling the mistakes of boundedly rational agents within a bayesian theory of mind. A Alanqary, G Z Lin, J Le, T Zhi-Xuan, V K Mansinghka, J B Tenenbaum, arXiv:2106.13249arXiv preprintAlanqary, A., Lin, G. Z., Le, J., Zhi-Xuan, T., Mansinghka, V. K., and Tenenbaum, J. B. Modeling the mistakes of boundedly rational agents within a bayesian theory of mind. arXiv preprint arXiv:2106.13249, 2021.</p>
<p>Psychological reasoning in infancy. Annual review of psychology. R Baillargeon, R M Scott, L Bian, 67Baillargeon, R., Scott, R. M., and Bian, L. Psychological reasoning in infancy. Annual review of psychology, 67: 159-186, 2016.</p>
<p>Action understanding as inverse planning. C L Baker, R Saxe, J B Tenenbaum, Cognition. 1133Baker, C. L., Saxe, R., and Tenenbaum, J. B. Action under- standing as inverse planning. Cognition, 113(3):329-349, 2009.</p>
<p>Rational quantitative attribution of beliefs, desires and percepts in human mentalizing. C L Baker, J Jara-Ettinger, R Saxe, J B Tenenbaum, Nature Human Behaviour. 1464Baker, C. L., Jara-Ettinger, J., Saxe, R., and Tenenbaum, J. B. Rational quantitative attribution of beliefs, desires and percepts in human mentalizing. Nature Human Be- haviour, 1(4):0064, 2017.</p>
<p>Is the most accurate ai the best teammate? optimizing ai for teamwork. G Bansal, B Nushi, E Kamar, E Horvitz, D S Weld, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Bansal, G., Nushi, B., Kamar, E., Horvitz, E., and Weld, D. S. Is the most accurate ai the best teammate? optimiz- ing ai for teamwork. In Proceedings of the AAAI Con- ference on Artificial Intelligence, volume 35, pp. 11405- 11414, 2021.</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, H Nori, H Palangi, M T Ribeiro, Y Zhang, Sparks of artificial general intelligence: Early experiments with gpt-4. Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lund- berg, S., Nori, H., Palangi, H., Ribeiro, M. T., and Zhang, Y. Sparks of artificial general intelligence: Early experi- ments with gpt-4, 2023.</p>
<p>On the utility of learning about humans for human-ai coordination. M Carroll, R Shah, M K Ho, T Griffiths, S Seshia, P Abbeel, A Dragan, Advances in neural information processing systems. 32Carroll, M., Shah, R., Ho, M. K., Griffiths, T., Seshia, S., Abbeel, P., and Dragan, A. On the utility of learning about humans for human-ai coordination. Advances in neural information processing systems, 32, 2019.</p>
<p>Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of llms. A G Cohn, J Hernandez-Orallo, 2304arXiv e-printsCohn, A. G. and Hernandez-Orallo, J. Dialectical language model evaluation: An initial appraisal of the common- sense spatial reasoning abilities of llms. arXiv e-prints, pp. arXiv-2304, 2023.</p>
<p>Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks. K M Collins, C Wong, J Feng, M Wei, J Tenenbaum, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society44Collins, K. M., Wong, C., Feng, J., Wei, M., and Tenenbaum, J. Structured, flexible, and robust: benchmarking and im- proving large language models towards more human-like behavior in out-of-distribution reasoning tasks. In Pro- ceedings of the Annual Meeting of the Cognitive Science Society, volume 44, 2022.</p>
<p>Goal attribution without agency cues: the perception of 'pure reason'in infancy. G Csibra, G Gergely, S Br, O Koos, M Brockbank, Cognition. 723Csibra, G., Gergely, G., Br, S., Koos, O., and Brockbank, M. Goal attribution without agency cues: the perception of 'pure reason'in infancy. Cognition, 72(3):237-267, 1999.</p>
<p>The Intentional Stance. D C Dennett, MIT PressDennett, D. C. The Intentional Stance. MIT Press, 1981.</p>
<p>Learning the preferences of ignorant, inconsistent agents. O Evans, A Stuhlmller, N Goodman, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence30Evans, O., Stuhlmller, A., and Goodman, N. Learning the preferences of ignorant, inconsistent agents. In Proceed- ings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.</p>
<p>Planning modulo theories: Extending the planning paradigm. P Gregory, D Long, M Fox, J C Beck, Proceedings of the International Conference on Automated Planning and Scheduling. the International Conference on Automated Planning and Scheduling22Gregory, P., Long, D., Fox, M., and Beck, J. C. Planning modulo theories: Extending the planning paradigm. In Proceedings of the International Conference on Auto- mated Planning and Scheduling, volume 22, pp. 65-73, 2012.</p>
<p>The nave utility calculus: Computational principles underlying commonsense psychology. J Jara-Ettinger, H Gweon, L E Schulz, J B Tenenbaum, Trends in cognitive sciences. 208Jara-Ettinger, J., Gweon, H., Schulz, L. E., and Tenenbaum, J. B. The nave utility calculus: Computational principles underlying commonsense psychology. Trends in cognitive sciences, 20(8):589-604, 2016.</p>
<p>Coordinate to cooperate or compete: abstract goals and joint intentions in social interaction. M Kleiman-Weiner, M K Ho, J L Austerweil, M L Littman, J B Tenenbaum, CogSci. Kleiman-Weiner, M., Ho, M. K., Austerweil, J. L., Littman, M. L., and Tenenbaum, J. B. Coordinate to cooperate or compete: abstract goals and joint intentions in social interaction. In CogSci, 2016.</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in Neural Information Processing Systems. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. In Ad- vances in Neural Information Processing Systems, 2022.</p>
<p>Leveraging unstructured statistical knowledge in a probabilistic language of thought. A K Lew, M H Tessler, V K Mansinghka, J B Tenenbaum, Proceedings of the Annual Conference of the Cognitive Science Society. the Annual Conference of the Cognitive Science SocietyLew, A. K., Tessler, M. H., Mansinghka, V. K., and Tenen- baum, J. B. Leveraging unstructured statistical knowledge in a probabilistic language of thought. In Proceedings of the Annual Conference of the Cognitive Science Society, 2020.</p>
<p>Sequential monte carlo steering of large language models using probabilistic programs. A K Lew, T Zhi-Xuan, G Grand, V K Mansinghka, arXiv:2306.03081arXiv preprintLew, A. K., Zhi-Xuan, T., Grand, G., and Mansinghka, V. K. Sequential monte carlo steering of large language models using probabilistic programs. arXiv preprint arXiv:2306.03081, 2023.</p>
<p>Language models as probabilistic priors for perception and action. B Z Li, W Chen, P Sharma, Andreas , J Lampp, 2302arXiv e-printsLi, B. Z., Chen, W., Sharma, P., and Andreas, J. Lampp: Language models as probabilistic priors for perception and action. arXiv e-prints, pp. arXiv-2302, 2023.</p>
<p>Online inverse reinforcement learning for nonlinear systems with adversarial attacks. B Lian, W Xue, F L Lewis, T Chai, International Journal of Robust and Nonlinear Control. 3114Lian, B., Xue, W., Lewis, F. L., and Chai, T. Online in- verse reinforcement learning for nonlinear systems with adversarial attacks. International Journal of Robust and Nonlinear Control, 31(14):6646-6667, 2021.</p>
<p>Llm+ p: Empowering large language models with optimal planning proficiency. B Liu, Y Jiang, X Zhang, Q Liu, S Zhang, J Biswas, P Stone, arXiv:2304.11477arXiv preprintLiu, B., Jiang, Y., Zhang, X., Liu, Q., Zhang, S., Biswas, J., and Stone, P. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023a.</p>
<p>Empowering large language models with optimal planning proficiency. B Liu, Y Jiang, X Zhang, Q Liu, S Zhang, J Biswas, P Stone, Llm+p, Liu, B., Jiang, Y., Zhang, X., Liu, Q., Zhang, S., Biswas, J., and Stone, P. Llm+p: Empowering large language models with optimal planning proficiency, 2023b.</p>
<p>Faithful chainof-thought reasoning. Q Lyu, S Havaldar, A Stein, L Zhang, D Rao, E Wong, M Apidianaki, C Callison-Burch, arXiv:2301.13379arXiv preprintLyu, Q., Havaldar, S., Stein, A., Zhang, L., Rao, D., Wong, E., Apidianaki, M., and Callison-Burch, C. Faithful chain- of-thought reasoning. arXiv preprint arXiv:2301.13379, 2023.</p>
<p>Consistent estimators for learning to defer to an expert. H Mozannar, D Sontag, PMLRProceedings of the 37th International Conference on Machine Learning. III, H. D. and Singh, A.the 37th International Conference on Machine Learning119Mozannar, H. and Sontag, D. Consistent estimators for learning to defer to an expert. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 7076-7087. PMLR, 13- 18 Jul 2020. URL https://proceedings.mlr. press/v119/mozannar20b.html.</p>
<p>. Openai, Gpt-4 technical reportOpenAI. Gpt-4 technical report, 2023.</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 35Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.</p>
<p>G Poesia, O Polozov, V Le, A Tiwari, G Soares, C Meek, S Gulwani, Synchromesh, arXiv:2201.11227Reliable code generation from pre-trained language models. arXiv preprintPoesia, G., Polozov, O., Le, V., Tiwari, A., Soares, G., Meek, C., and Gulwani, S. Synchromesh: Reliable code gener- ation from pre-trained language models. arXiv preprint arXiv:2201.11227, 2022.</p>
<p>Bayesian inverse reinforcement learning. D Ramachandran, E Amir, IJCAI. 7Ramachandran, D. and Amir, E. Bayesian inverse rein- forcement learning. In IJCAI, volume 7, pp. 2586-2591, 2007.</p>
<p>Uniquely human social cognition. R Saxe, Current opinion in neurobiology. 162Saxe, R. Uniquely human social cognition. Current opinion in neurobiology, 16(2):235-239, 2006.</p>
<p>On the feasibility of learning, rather than assuming, human biases for reward inference. R Shah, N Gundotra, P Abbeel, A Dragan, International Conference on Machine Learning. PMLRShah, R., Gundotra, N., Abbeel, P., and Dragan, A. On the feasibility of learning, rather than assuming, human biases for reward inference. In International Conference on Machine Learning, pp. 5670-5679. PMLR, 2019.</p>
<p>Clever hans or neural theory of mind? stress testing social reasoning in large language models. N Shapira, M Levy, S H Alavi, X Zhou, Y Choi, Y Goldberg, M Sap, V Shwartz, arXiv:2305.14763arXiv preprintShapira, N., Levy, M., Alavi, S. H., Zhou, X., Choi, Y., Goldberg, Y., Sap, M., and Shwartz, V. Clever hans or neural theory of mind? stress testing social reasoning in large language models. arXiv preprint arXiv:2305.14763, 2023.</p>
<p>Human-centered AI. B Shneiderman, Oxford University PressShneiderman, B. Human-centered AI. Oxford University Press, 2022.</p>
<p>Agent: A benchmark for core psychological reasoning. T Shu, A Bhandwaldar, C Gan, K Smith, S Liu, D Gutfreund, E Spelke, J Tenenbaum, T Ullman, International Conference on Machine Learning. PMLRShu, T., Bhandwaldar, A., Gan, C., Smith, K., Liu, S., Gut- freund, D., Spelke, E., Tenenbaum, J., and Ullman, T. Agent: A benchmark for core psychological reasoning. In International Conference on Machine Learning, pp. 9614-9625. PMLR, 2021.</p>
<p>Generalized planning in pddl domains with pretrained large language models. T Silver, S Dan, K Srinivas, J B Tenenbaum, L P Kaelbling, M Katz, Silver, T., Dan, S., Srinivas, K., Tenenbaum, J. B., Kaelbling, L. P., and Katz, M. Generalized planning in pddl domains with pretrained large language models, 2023.</p>
<p>Origins of human communication. M Tomasello, MIT pressTomasello, M. Origins of human communication. MIT press, 2010.</p>
<p>Large language models fail on trivial alterations to theory-of-mind tasks. T Ullman, arXiv:2302.08399arXiv preprintUllman, T. Large language models fail on trivial al- terations to theory-of-mind tasks. arXiv preprint arXiv:2302.08399, 2023.</p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). K Valmeekam, A Olmo, S Sreedharan, S Kambhampati, arXiv:2206.10498arXiv preprintValmeekam, K., Olmo, A., Sreedharan, S., and Kambham- pati, S. Large language models still can't plan (a bench- mark for llms on planning and reasoning about change). arXiv preprint arXiv:2206.10498, 2022.</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E H Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E. H., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, 2022.</p>
<p>The child's theory of mind. H M Wellman, The MIT PressWellman, H. M. The child's theory of mind. The MIT Press, 1992.</p>
<p>Learning to complement humans. B Wilder, E Horvitz, E Kamar, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial IntelligenceWilder, B., Horvitz, E., and Kamar, E. Learning to com- plement humans. In Proceedings of the Twenty-Ninth International Conference on International Joint Confer- ences on Artificial Intelligence, pp. 1526-1533, 2021.</p>
<p>From word models to world models: Translating from natural language to the probabilistic language of thought. L Wong, G Grand, A K Lew, N D Goodman, V K Mansinghka, J Andreas, J B Tenenbaum, Wong, L., Grand, G., Lew, A. K., Goodman, N. D., Mans- inghka, V. K., Andreas, J., and Tenenbaum, J. B. From word models to world models: Translating from natural language to the probabilistic language of thought, 2023.</p>
<p>Inferring the goals of communicating agents from actions and instructions. L Ying, T Zhi-Xuan, V Mansinghka, J Tenenbaum, ICML Workshop on Theory of Mind in Communicating Agents. 2023To AppearYing, L., Zhi-Xuan, T., Mansinghka, V., and Tenenbaum, J. Inferring the goals of communicating agents from actions and instructions. ICML Workshop on Theory of Mind in Communicating Agents (To Appear), 2023.</p>
<p>Grounded physical language understanding with probabilistic programs and simulated worlds. C E Zhang, L Wong, G Grand, J B Tenenbaum, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society45Zhang, C. E., Wong, L., Grand, G., and Tenenbaum, J. B. Grounded physical language understanding with proba- bilistic programs and simulated worlds. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 45, 2023.</p>
<p>jl: An Extensible Interpreter and Compiler Interface for Fast and Flexible AI Planning. T Pddl Zhi-Xuan, Massachusetts Institute of TechnologyPhD thesisZhi-Xuan, T. PDDL. jl: An Extensible Interpreter and Compiler Interface for Fast and Flexible AI Planning. PhD thesis, Massachusetts Institute of Technology, 2022.</p>
<p>Online bayesian goal inference for boundedly rational planning agents. T Zhi-Xuan, J Mann, T Silver, J Tenenbaum, V Mansinghka, Advances in neural information processing systems. 33Zhi-Xuan, T., Mann, J., Silver, T., Tenenbaum, J., and Mans- inghka, V. Online bayesian goal inference for boundedly rational planning agents. Advances in neural information processing systems, 33:19238-19250, 2020.</p>
<p>Symbolic Inverse Planning Engine (NIPE). The Neuro-Symbolic Inverse Planning Engine (NIPE)</p>
<p>Maximum entropy inverse reinforcement learning. B D Ziebart, A L Maas, J A Bagnell, A K Dey, Aaai. Chicago, IL, USA8Ziebart, B. D., Maas, A. L., Bagnell, J. A., Dey, A. K., et al. Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pp. 1433-1438. Chicago, IL, USA, 2008.</p>
<p>KMC is funded by a Marshall Scholarship and the Cambridge Trust. AW acknowledges support from a Turing AI Fellowship under grant EP/V025279/1, The Alan Turing Institute, and the Leverhulme Trust via CFI. The work is additionally funded by DARPA, MCS, AFOSR, ONR Science of AI programs. TZX is funded by an Open Phil AI Fellowship. and the MIT-IBM Watson AI labTZX is funded by an Open Phil AI Fellowship. KMC is funded by a Marshall Scholarship and the Cambridge Trust. AW acknowledges support from a Turing AI Fellowship under grant EP/V025279/1, The Alan Turing Institute, and the Leverhulme Trust via CFI. The work is additionally funded by DARPA, MCS, AFOSR, ONR Science of AI programs, and the MIT-IBM Watson AI lab.</p>
<p>Human linguistic inverse planning experiment. A , A.2. Human linguistic inverse planning experiment</p>
<p>For all scenarios, participants were asked "Now, considering each of the possible trophies on the course. We include additional details on our human experiment. please respond on the 1-7 scale: how likely is it that the participant is trying to reach this trophy?" Likert scale options were as follows:  "1 (Definitely not this trophyWe include additional details on our human experiment. For all scenarios, participants were asked "Now, considering each of the possible trophies on the course, please respond on the 1-7 scale: how likely is it that the participant is trying to reach this trophy?" Likert scale options were as follows:  "1 (Definitely not this trophy)"</p>
<p>Note, due to a mismatch, some participants in the Generic condition saw one stimuli from Different-Colored Keys and vice versa. We reversed fixed this partway through the study. Note, due to a mismatch, some participants in the Generic condition saw one stimuli from Different-Colored Keys and vice versa. We reversed fixed this partway through the study.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4729 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4729</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4729</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-03488f1a193066b5ea8b9b800e119f07df5c1d9e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/03488f1a193066b5ea8b9b800e119f07df5c1d9e" target="_blank">Reasoning Like Program Executors</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> Experimental results on six benchmarks demonstrate that POET can significantly boost model performance in natural language reasoning, such as numerical reasoning, logical reasoning, and multi-hop reasoning.</p>
                <p><strong>Paper Abstract:</strong> Reasoning over natural language is a long-standing goal for the research community. However, studies have shown that existing language models are inadequate in reasoning. To address the issue, we present POET, a novel reasoning pre-training paradigm. Through pre-training language models with programs and their execution results, POET empowers language models to harvest the reasoning knowledge possessed by program executors via a data-driven approach. POET is conceptually simple and can be instantiated by different kinds of program executors. In this paper, we showcase two simple instances POET-Math and POET-Logic, in addition to a complex instance, POET-SQL. Experimental results on six benchmarks demonstrate that POET can significantly boost model performance in natural language reasoning, such as numerical reasoning, logical reasoning, and multi-hop reasoning. POET opens a new gate on reasoning-enhancement pre-training, and we hope our analysis would shed light on the future research of reasoning like program executors.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4729.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4729.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PoEt-Math</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PoEt-Math (Program Executor pre-training: Math calculator instance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A PoEt instantiation that pre-trains encoder-decoder LMs to imitate a math calculator: given a program context of floating-point variables and a math expression (addition/subtraction) as the program, the model is trained to output the numeric execution result.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART-Large (PoEt-Math instantiation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder Transformer (BART-Large) used as backbone. PoEt-Math pre-training corpus: 4M synthesized examples; each example: up to 3 variables and up to 2 operators (+ / -); variable values in [0.0,1000.0]. Pre-training: ~10k steps, batch size 512, peak LR 3e-5. Output is an arbitrary float (generation).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Floating-point addition and subtraction (up to 3 variables, up to 2 operators); basic single-step arithmetic expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>By imitating a deterministic program executor (math calculator) that maps a formal program + program context to an execution result, the LM internalizes procedural/arithmetic reasoning patterns (algorithmic-style computation or structured mapping from variables and operators to numeric outputs) rather than only learning surface token correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Substantial downstream gains on NL numerical-reasoning benchmarks after PoEt-Math pretraining (e.g., reported +9.0% EM on DROP in the paper's figure). Ablations show that providing program context (variables) improves transfer compared to a variable-free variant, and adding irrelevant variables (to mimic noisy natural contexts) further increases downstream performance, consistent with learning to (algorithmically) pick relevant numeric values.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Coverage limited to simple addition/subtraction expressions used in pretraining — paper shows transfer is coupling-dependent: removing math programs from PoEt-SQL's pretraining corpus degrades DROP performance, indicating the mechanism does not magically generalize beyond the trained operation set. The paper does not present internal interpretability (neuron-level or attention-level) evidence pinpointing an explicit arithmetic algorithm learned by the model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BART-Large baseline on DROP EM: 66.2. PoEt-Math variants: PoEt-Math without program context EM 67.4; with 0 irrelevant variables 71.5 EM; with 10 irrelevant variables 74.6 EM; with 30 irrelevant variables 75.2 EM. The paper also reports a +9.0% EM gain on DROP attributed to PoEt-Math in preliminary experiments (figure).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Program-context ablations: variable-free pretraining (program only) yields much smaller transfer; increasing number of irrelevant variables in program context (0 -> 10 -> 30) progressively improves downstream EM, indicating the model benefits from learning to ignore irrelevant numeric context. Pretraining scale/steps analysis shows performance converges and is data-efficient (10% of PoEt-SQL data can reach similar asymptote).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Limited to addition/subtraction (no multiplication/division/multi-step algebra in PoEt-Math); transfer is strongly coupled to overlap between pretraining operations and downstream tasks; does not fully explain internal mechanism (no neuron-level interpretability); performance depends on program-context design (must mimic noise of natural passages).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared to vanilla BART-Large, PoEt-Math variants substantially improve DROP EM (see numbers above). PoEt-Math combined with PoEt-SQL (mixed executors) gives slightly better performance than single-executor PoEt versions. The paper contrasts PoEt-Math's gains with other language-model pretraining methods (e.g., GenBERT, PReasM) but the primary direct comparison is between PoEt variants and the same backbone without PoEt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Like Program Executors', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4729.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4729.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Char-level numbers + reverse decoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Character-level numeric tokenization and reverse-number decoding for numeric generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Implementation techniques used to improve LMs' numeric output fidelity: represent numbers at character (digit) granularity and use reverse decoding of numbers (generate digits in reverse order) to better model arithmetic carry and exact numeric outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART-Large / other LMs in DROP & SVAMP fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Practical input/output representation choices applied during fine-tuning and evaluation on numeric generation tasks (DROP, SVAMP): numbers are tokenized at character-level and numeric generation may be performed with reverse-digit decoding to ease modelling of carries.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Generation of exact numeric answers for reading-comprehension math questions (addition/subtraction; number outputs that may be out-of-context spans).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Subword tokenization (BPE) fragments numbers in ways that harm the model's ability to learn digitwise arithmetic; character-level representation and reverse decoding reduce that interference by providing atomic digit tokens and making carry propagation easier to learn for sequence models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Paper cites prior works (Wallace et al. 2019; Geva et al. 2020) and reports using these strategies in DROP and SVAMP experiments; the reported strong numeric performance (larger EM gains on DROP and SVAMP for PoEt variants) is achieved with these number-representation techniques in place.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>The paper does not provide an explicit ablation isolating the effect of character-level vs. subword number representations within their PoEt experiments (they follow best-practice from prior work), so the contribution is qualitatively reported but not decomposed numerically in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Applied in the fine-tuning pipeline that produced the numeric performance numbers in the paper (e.g., PoEt-SQL BART SVAMP EM 33.5 vs BART 12.4), but no isolated numeric ablation for the representation technique is given.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No granular probe or intervention reported in this paper specifically isolating representation effects; strategy adopted as part of experimental recipe based on prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Not quantified in this paper; using character-level tokens increases sequence length and may impact speed/memory; reverse decoding requires architectural/training adjustments and may complicate integration with general text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Technique aligns with recommendations from prior works (cited) and was used to obtain the reported numeric performance; direct comparison to alternative tokenization schemes is not presented in-paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Like Program Executors', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4729.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4729.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Program context design</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program context variables and irrelevant-variable design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Design choice in PoEt pretraining where the program context is a set of floating-point variables; irrelevant variables are intentionally added to mimic noisy natural contexts and force the model to learn to pick relevant numeric evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART-Large (PoEt-Math and PoEt-SQL experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Program contexts contain up to 30 floating-point variables in PoEt-Math; pretraining examples include necessary variables referenced by the program and optional irrelevant variables to simulate natural-context noise.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Context-grounded arithmetic: selecting the correct variable values from a noisy context and carrying out addition/subtraction per the provided expression.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Including irrelevant variables trains the model to perform selective retrieval/attention over numeric context, which yields better transfer to NL passages that also contain irrelevant information.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation table (Table 7) shows downstream DROP EM increases monotonically with the number of irrelevant variables in PoEt-Math pretraining: (0 irrelevant -> 71.5 EM; 10 -> 74.6 EM; 30 -> 75.2 EM), while removing program context (variable-free) yields substantially lower performance (67.4 EM).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No counter-evidence in the paper; authors note limits on input length (30 irrelevant variables approaches LM max input length) so larger-noise regimes were not tested.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Baseline BART-Large on DROP EM 66.2; PoEt-Math with program context and 30 irrelevant variables EM 75.2 (Table 7). Variable-free PoEt-Math EM 67.4.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Explicit intervention experiments: varying the number of irrelevant variables and comparing variable-free vs variable-containing pretraining; results show clear gains from context and from modeling irrelevant information.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Input-length constraints limit how much irrelevant context can be simulated; findings are empirical and specific to the noise regimes tested; may not generalize to all types of natural-context distractions (semantic vs purely numeric noise).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>These context-design choices are specific to PoEt pretraining; comparisons are with vanilla BART (no PoEt) showing the benefit of program-context-aware pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Like Program Executors', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4729.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4729.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PoEt-SQL arithmetic transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PoEt-SQL (integrated-executor pretraining with SQL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An integrated PoEt instance that uses SQL programs and database contexts to expose LMs to a wide mix of reasoning procedures (arithmetic, aggregation, comparison, nested queries). The model learns to generate or select SQL query results and transfers that reasoning to multiple NL reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PoEt-SQL instantiated on BART-Large, RoBERTa-Large and T5-11B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>For encoder-decoder LMs (BART/T5) PoEt-SQL trains the model to generate query results given SQL + flattened DB (5M synthesized examples for generator). For encoder-only LMs (RoBERTa), task is modified to result-selection (IO tagging) on ~2M filtered examples. Pretraining: up to 50k steps for full PoEt-SQL, batch size 512, peak LR 3e-5.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Broader SQL-related arithmetic operations: column arithmetic (e.g., subtraction), aggregation (COUNT, MAX), comparative filters (>), superlatives, nested queries, unions — these include numeric computations and numeric-based logic.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Learning to mimic SQL execution exposes the model to diverse, compositional computational procedures; by learning to map formal queries and structured contexts to results, the LM internalizes multiple reasoning primitives (including arithmetic) and can transfer them to NL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Large and consistent downstream improvements on numeric and hybrid reasoning datasets: PoEt-SQL BART vs BART-Large: DROP EM 77.7 vs 66.2 (+11.5), SVAMP EM 33.5 vs 12.4 (+21.1), TAT-QA, EQUATE and HotpotQA also show gains. T5-11B with PoEt-SQL also improves DROP (83.5 -> 85.2) and SVAMP (52.9 -> 57.4). Ablations: removing math programs from PoEt-SQL pretraining hurts DROP performance, indicating dependence on arithmetic-containing programs. Tuning SQL naturalness (translating SQL to NL or making programs unnatural) had little effect, suggesting models learn abstract reasoning rather than surface lexical mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>The transfer is not universal: success depends on reasonable overlap between pretraining program executors' reasoning primitives and downstream task requirements; the paper notes that PoEt's coupling between pretraining skills and downstream needs is a limitation. No fine-grained interpretability analysis (e.g., circuit-level) is provided to show how arithmetic is internally represented.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table highlights: BART-Large: DROP EM 66.2 -> PoEt-SQL 77.7 (+11.5 EM); SVAMP EM 12.4 -> 33.5 (+21.1). RoBERTa-Large: DROP EM 78.1 -> PoEt-SQL 79.8 (+1.7). T5-11B: DROP EM 83.5 -> PoEt-SQL_T5 85.2 (+1.7); SVAMP 52.9 -> 57.4 (+4.5).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Ablation comparing SQL execution pretraining vs SQL language-modeling (mask-and-predict the SQL query) indicates that execution-target supervision matters (authors report this corroborates necessity of execution supervision). Pretraining-scale/steps experiments show curves that plateau, with 10% of data often sufficient to reach similar asymptotes. Also, models pre-trained on NL reasoning (DROP) learn SQL execution more easily (lower perplexity) showing bidirectional transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>PoEt requires overlap between pretraining executor capabilities and downstream tasks — removing math programs from the SQL pretraining reduces numerical transfer; pretraining relies on instantiated templates (less program diversity) which may limit generalization; no low-level mechanistic explanation provided; encoder-only models require special task tailoring (selection IO tagging) and cannot freely generate out-of-context numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>PoEt-SQL BART outperforms several language-model pretraining baselines (e.g., T5, GenBERT, PReasM) on reasoning benchmarks and approaches or surpasses specialized models in some metrics (Table comparisons). Gains vary by backbone size: improvements are largest for medium-sized models (BART-Large) and smaller but positive for giant model (T5-11B).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Like Program Executors', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Injecting numerical reasoning skills into language models <em>(Rating: 2)</em></li>
                <li>Do NLP models know numbers? probing numeracy in embeddings <em>(Rating: 2)</em></li>
                <li>Are NLP models really able to solve simple math word problems? <em>(Rating: 1)</em></li>
                <li>TAPEX: Table pre-training via learning a neural SQL executor <em>(Rating: 2)</em></li>
                <li>On the advance of making language models better reasoners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4729",
    "paper_id": "paper-03488f1a193066b5ea8b9b800e119f07df5c1d9e",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "PoEt-Math",
            "name_full": "PoEt-Math (Program Executor pre-training: Math calculator instance)",
            "brief_description": "A PoEt instantiation that pre-trains encoder-decoder LMs to imitate a math calculator: given a program context of floating-point variables and a math expression (addition/subtraction) as the program, the model is trained to output the numeric execution result.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BART-Large (PoEt-Math instantiation)",
            "model_description": "Encoder-decoder Transformer (BART-Large) used as backbone. PoEt-Math pre-training corpus: 4M synthesized examples; each example: up to 3 variables and up to 2 operators (+ / -); variable values in [0.0,1000.0]. Pre-training: ~10k steps, batch size 512, peak LR 3e-5. Output is an arbitrary float (generation).",
            "arithmetic_task_type": "Floating-point addition and subtraction (up to 3 variables, up to 2 operators); basic single-step arithmetic expressions.",
            "mechanism_hypothesis": "By imitating a deterministic program executor (math calculator) that maps a formal program + program context to an execution result, the LM internalizes procedural/arithmetic reasoning patterns (algorithmic-style computation or structured mapping from variables and operators to numeric outputs) rather than only learning surface token correlations.",
            "evidence_for_mechanism": "Substantial downstream gains on NL numerical-reasoning benchmarks after PoEt-Math pretraining (e.g., reported +9.0% EM on DROP in the paper's figure). Ablations show that providing program context (variables) improves transfer compared to a variable-free variant, and adding irrelevant variables (to mimic noisy natural contexts) further increases downstream performance, consistent with learning to (algorithmically) pick relevant numeric values.",
            "evidence_against_mechanism": "Coverage limited to simple addition/subtraction expressions used in pretraining — paper shows transfer is coupling-dependent: removing math programs from PoEt-SQL's pretraining corpus degrades DROP performance, indicating the mechanism does not magically generalize beyond the trained operation set. The paper does not present internal interpretability (neuron-level or attention-level) evidence pinpointing an explicit arithmetic algorithm learned by the model.",
            "performance_metrics": "BART-Large baseline on DROP EM: 66.2. PoEt-Math variants: PoEt-Math without program context EM 67.4; with 0 irrelevant variables 71.5 EM; with 10 irrelevant variables 74.6 EM; with 30 irrelevant variables 75.2 EM. The paper also reports a +9.0% EM gain on DROP attributed to PoEt-Math in preliminary experiments (figure).",
            "probing_or_intervention_results": "Program-context ablations: variable-free pretraining (program only) yields much smaller transfer; increasing number of irrelevant variables in program context (0 -&gt; 10 -&gt; 30) progressively improves downstream EM, indicating the model benefits from learning to ignore irrelevant numeric context. Pretraining scale/steps analysis shows performance converges and is data-efficient (10% of PoEt-SQL data can reach similar asymptote).",
            "limitations_and_failure_modes": "Limited to addition/subtraction (no multiplication/division/multi-step algebra in PoEt-Math); transfer is strongly coupled to overlap between pretraining operations and downstream tasks; does not fully explain internal mechanism (no neuron-level interpretability); performance depends on program-context design (must mimic noise of natural passages).",
            "comparison_to_other_models": "Compared to vanilla BART-Large, PoEt-Math variants substantially improve DROP EM (see numbers above). PoEt-Math combined with PoEt-SQL (mixed executors) gives slightly better performance than single-executor PoEt versions. The paper contrasts PoEt-Math's gains with other language-model pretraining methods (e.g., GenBERT, PReasM) but the primary direct comparison is between PoEt variants and the same backbone without PoEt.",
            "uuid": "e4729.0",
            "source_info": {
                "paper_title": "Reasoning Like Program Executors",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "Char-level numbers + reverse decoding",
            "name_full": "Character-level numeric tokenization and reverse-number decoding for numeric generation",
            "brief_description": "Implementation techniques used to improve LMs' numeric output fidelity: represent numbers at character (digit) granularity and use reverse decoding of numbers (generate digits in reverse order) to better model arithmetic carry and exact numeric outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BART-Large / other LMs in DROP & SVAMP fine-tuning",
            "model_description": "Practical input/output representation choices applied during fine-tuning and evaluation on numeric generation tasks (DROP, SVAMP): numbers are tokenized at character-level and numeric generation may be performed with reverse-digit decoding to ease modelling of carries.",
            "arithmetic_task_type": "Generation of exact numeric answers for reading-comprehension math questions (addition/subtraction; number outputs that may be out-of-context spans).",
            "mechanism_hypothesis": "Subword tokenization (BPE) fragments numbers in ways that harm the model's ability to learn digitwise arithmetic; character-level representation and reverse decoding reduce that interference by providing atomic digit tokens and making carry propagation easier to learn for sequence models.",
            "evidence_for_mechanism": "Paper cites prior works (Wallace et al. 2019; Geva et al. 2020) and reports using these strategies in DROP and SVAMP experiments; the reported strong numeric performance (larger EM gains on DROP and SVAMP for PoEt variants) is achieved with these number-representation techniques in place.",
            "evidence_against_mechanism": "The paper does not provide an explicit ablation isolating the effect of character-level vs. subword number representations within their PoEt experiments (they follow best-practice from prior work), so the contribution is qualitatively reported but not decomposed numerically in this work.",
            "performance_metrics": "Applied in the fine-tuning pipeline that produced the numeric performance numbers in the paper (e.g., PoEt-SQL BART SVAMP EM 33.5 vs BART 12.4), but no isolated numeric ablation for the representation technique is given.",
            "probing_or_intervention_results": "No granular probe or intervention reported in this paper specifically isolating representation effects; strategy adopted as part of experimental recipe based on prior literature.",
            "limitations_and_failure_modes": "Not quantified in this paper; using character-level tokens increases sequence length and may impact speed/memory; reverse decoding requires architectural/training adjustments and may complicate integration with general text generation.",
            "comparison_to_other_models": "Technique aligns with recommendations from prior works (cited) and was used to obtain the reported numeric performance; direct comparison to alternative tokenization schemes is not presented in-paper.",
            "uuid": "e4729.1",
            "source_info": {
                "paper_title": "Reasoning Like Program Executors",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "Program context design",
            "name_full": "Program context variables and irrelevant-variable design",
            "brief_description": "Design choice in PoEt pretraining where the program context is a set of floating-point variables; irrelevant variables are intentionally added to mimic noisy natural contexts and force the model to learn to pick relevant numeric evidence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BART-Large (PoEt-Math and PoEt-SQL experiments)",
            "model_description": "Program contexts contain up to 30 floating-point variables in PoEt-Math; pretraining examples include necessary variables referenced by the program and optional irrelevant variables to simulate natural-context noise.",
            "arithmetic_task_type": "Context-grounded arithmetic: selecting the correct variable values from a noisy context and carrying out addition/subtraction per the provided expression.",
            "mechanism_hypothesis": "Including irrelevant variables trains the model to perform selective retrieval/attention over numeric context, which yields better transfer to NL passages that also contain irrelevant information.",
            "evidence_for_mechanism": "Ablation table (Table 7) shows downstream DROP EM increases monotonically with the number of irrelevant variables in PoEt-Math pretraining: (0 irrelevant -&gt; 71.5 EM; 10 -&gt; 74.6 EM; 30 -&gt; 75.2 EM), while removing program context (variable-free) yields substantially lower performance (67.4 EM).",
            "evidence_against_mechanism": "No counter-evidence in the paper; authors note limits on input length (30 irrelevant variables approaches LM max input length) so larger-noise regimes were not tested.",
            "performance_metrics": "Baseline BART-Large on DROP EM 66.2; PoEt-Math with program context and 30 irrelevant variables EM 75.2 (Table 7). Variable-free PoEt-Math EM 67.4.",
            "probing_or_intervention_results": "Explicit intervention experiments: varying the number of irrelevant variables and comparing variable-free vs variable-containing pretraining; results show clear gains from context and from modeling irrelevant information.",
            "limitations_and_failure_modes": "Input-length constraints limit how much irrelevant context can be simulated; findings are empirical and specific to the noise regimes tested; may not generalize to all types of natural-context distractions (semantic vs purely numeric noise).",
            "comparison_to_other_models": "These context-design choices are specific to PoEt pretraining; comparisons are with vanilla BART (no PoEt) showing the benefit of program-context-aware pretraining.",
            "uuid": "e4729.2",
            "source_info": {
                "paper_title": "Reasoning Like Program Executors",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "PoEt-SQL arithmetic transfer",
            "name_full": "PoEt-SQL (integrated-executor pretraining with SQL)",
            "brief_description": "An integrated PoEt instance that uses SQL programs and database contexts to expose LMs to a wide mix of reasoning procedures (arithmetic, aggregation, comparison, nested queries). The model learns to generate or select SQL query results and transfers that reasoning to multiple NL reasoning tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PoEt-SQL instantiated on BART-Large, RoBERTa-Large and T5-11B",
            "model_description": "For encoder-decoder LMs (BART/T5) PoEt-SQL trains the model to generate query results given SQL + flattened DB (5M synthesized examples for generator). For encoder-only LMs (RoBERTa), task is modified to result-selection (IO tagging) on ~2M filtered examples. Pretraining: up to 50k steps for full PoEt-SQL, batch size 512, peak LR 3e-5.",
            "arithmetic_task_type": "Broader SQL-related arithmetic operations: column arithmetic (e.g., subtraction), aggregation (COUNT, MAX), comparative filters (&gt;), superlatives, nested queries, unions — these include numeric computations and numeric-based logic.",
            "mechanism_hypothesis": "Learning to mimic SQL execution exposes the model to diverse, compositional computational procedures; by learning to map formal queries and structured contexts to results, the LM internalizes multiple reasoning primitives (including arithmetic) and can transfer them to NL tasks.",
            "evidence_for_mechanism": "Large and consistent downstream improvements on numeric and hybrid reasoning datasets: PoEt-SQL BART vs BART-Large: DROP EM 77.7 vs 66.2 (+11.5), SVAMP EM 33.5 vs 12.4 (+21.1), TAT-QA, EQUATE and HotpotQA also show gains. T5-11B with PoEt-SQL also improves DROP (83.5 -&gt; 85.2) and SVAMP (52.9 -&gt; 57.4). Ablations: removing math programs from PoEt-SQL pretraining hurts DROP performance, indicating dependence on arithmetic-containing programs. Tuning SQL naturalness (translating SQL to NL or making programs unnatural) had little effect, suggesting models learn abstract reasoning rather than surface lexical mappings.",
            "evidence_against_mechanism": "The transfer is not universal: success depends on reasonable overlap between pretraining program executors' reasoning primitives and downstream task requirements; the paper notes that PoEt's coupling between pretraining skills and downstream needs is a limitation. No fine-grained interpretability analysis (e.g., circuit-level) is provided to show how arithmetic is internally represented.",
            "performance_metrics": "Table highlights: BART-Large: DROP EM 66.2 -&gt; PoEt-SQL 77.7 (+11.5 EM); SVAMP EM 12.4 -&gt; 33.5 (+21.1). RoBERTa-Large: DROP EM 78.1 -&gt; PoEt-SQL 79.8 (+1.7). T5-11B: DROP EM 83.5 -&gt; PoEt-SQL_T5 85.2 (+1.7); SVAMP 52.9 -&gt; 57.4 (+4.5).",
            "probing_or_intervention_results": "Ablation comparing SQL execution pretraining vs SQL language-modeling (mask-and-predict the SQL query) indicates that execution-target supervision matters (authors report this corroborates necessity of execution supervision). Pretraining-scale/steps experiments show curves that plateau, with 10% of data often sufficient to reach similar asymptotes. Also, models pre-trained on NL reasoning (DROP) learn SQL execution more easily (lower perplexity) showing bidirectional transfer.",
            "limitations_and_failure_modes": "PoEt requires overlap between pretraining executor capabilities and downstream tasks — removing math programs from the SQL pretraining reduces numerical transfer; pretraining relies on instantiated templates (less program diversity) which may limit generalization; no low-level mechanistic explanation provided; encoder-only models require special task tailoring (selection IO tagging) and cannot freely generate out-of-context numeric results.",
            "comparison_to_other_models": "PoEt-SQL BART outperforms several language-model pretraining baselines (e.g., T5, GenBERT, PReasM) on reasoning benchmarks and approaches or surpasses specialized models in some metrics (Table comparisons). Gains vary by backbone size: improvements are largest for medium-sized models (BART-Large) and smaller but positive for giant model (T5-11B).",
            "uuid": "e4729.3",
            "source_info": {
                "paper_title": "Reasoning Like Program Executors",
                "publication_date_yy_mm": "2022-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Injecting numerical reasoning skills into language models",
            "rating": 2
        },
        {
            "paper_title": "Do NLP models know numbers? probing numeracy in embeddings",
            "rating": 2
        },
        {
            "paper_title": "Are NLP models really able to solve simple math word problems?",
            "rating": 1
        },
        {
            "paper_title": "TAPEX: Table pre-training via learning a neural SQL executor",
            "rating": 2
        },
        {
            "paper_title": "On the advance of making language models better reasoners",
            "rating": 1
        }
    ],
    "cost": 0.01714475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Reasoning Like Program Executors</h1>
<p>Xinyu Pi ${ }^{\circ *}$, Qian Liu ${ }^{\text { }}$, Bei Chen ${ }^{\dagger}$, Morteza Ziyadi ${ }^{\circ}$, Zeqi Lin ${ }^{\dagger}$ Qiang $\mathbf{F u}^{\dagger}$, Yan Gao ${ }^{\dagger}$, Jian-Guang Lou ${ }^{\dagger}$, Weizhu Chen ${ }^{\circ}$<br>${ }^{\circ}$ University of Illinois Urbana-Champaign, Urbana, USA; ${ }^{\S}$ Sea AI Lab, Singapore<br>${ }^{\dagger}$ Microsoft Research Asia, Beijing, China; ${ }^{\circ}$ Microsoft Azure AI, Redmond, WA, USA<br>xinyupi2@illinois.edu; liuqian@sea.com;<br>{beichen, morteza.ziyadi, zeqi.lin, qifu, yan.gao, jlou, wzchen}@microsoft.com</p>
<h4>Abstract</h4>
<p>Reasoning over natural language is a longstanding goal for the research community. However, studies have shown that existing language models are inadequate in reasoning. To address the issue, we present PoEt, a novel reasoning pre-training paradigm. Through pretraining language models with programs and their execution results, PoEt empowers language models to harvest the reasoning knowledge possessed by program executors via a data-driven approach. PoEt is conceptually simple and can be instantiated by different kinds of program executors. In this paper, we showcase two simple instances PoEt-Math and PoEt-Logic, in addition to a complex instance, PoEt-SQL. Experimental results on six benchmarks demonstrate that PoEt can significantly boost model performance in natural language reasoning, such as numerical reasoning, logical reasoning, and multi-hop reasoning. PoEt opens a new gate on reasoningenhancement pre-training, and we hope our analysis would shed light on the future research of reasoning like program executors.</p>
<h2>1 Introduction</h2>
<p>Recent breakthroughs in pre-training illustrate the power of pre-trained Language Models (LM) on a wide range of Natural Language (NL) tasks. Pretraining on self-supervised tasks, such as masked language modeling (Devlin et al., 2019; He et al., 2021) using large amounts of NL sentences, boosts the language understanding of models by a large margin (Wang et al., 2018a). However, existing pre-training paradigms have primarily focused on language modeling and paid little attention to advanced reasoning capabilities (Table 1). As a result, though reaching near-human performance on several tasks, pre-trained LMs are still far behind expectations in reasoning-required scenarios (Rae</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Given a program context and a program as input, PoEt pre-trains LMs to output the execution result. After fine-tuning on downstream tasks, PoEt can boost LMs on reasoning-required scenarios. Explanations about program context, program, program executor and execution result can be found in § 3. More examples of natural context and sentence are in Table 1.
et al., 2021), such as numerical reasoning (Wallace et al., 2019; Ravichander et al., 2019) and logical reasoning (Yu et al., 2020; Liu et al., 2020).</p>
<p>To alleviate the deficiency, reconciling NL understanding in LMs and reasoning in symbolic representations, i.e., neuro-symbolic reasoning, has been a major area of interest (Besold et al., 2017; Zhang et al., 2021). With a hybrid architecture, i.e., symbolic reasoners attached to LMs, neuralsymbolic reasoning shines in a variety of reasoning tasks (Chen et al., 2020c; Tu et al., 2020; Wolfson et al., 2020). However, the reasoning mechanism remains in the symbolic reasoner and is not internalized into LMs, making it difficult to reuse the reasoning mechanism on unseen tasks. Meanwhile, neural models are notorious for their reliance on correlations among concrete tokens of a representation system and are usually assumed to be hard to grasp abstract rules of a symbolic reasoner (Helwe et al., 2021; Sinha et al., 2021). This drives us to</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Example</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Task</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Numerical</td>
<td style="text-align: center;">Question: What is the difference in casualty numbers between Bavarian and Austrian? Passage: [DOC] The popular uprising included large areas of ...</td>
<td style="text-align: center;">DROP (Dua et al., 2019)</td>
<td style="text-align: center;">Reading Comprehension (RC)</td>
</tr>
<tr>
<td style="text-align: center;">Logical</td>
<td style="text-align: center;">Conclusion: One employee supervises another who gets more salary than himself. Fact: [DOC] David, Jack and Mark are colleagues in a company. David supervises Jack, and Jack supervises Mark. David gets more ...</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { LogiQA (Liu } \ &amp; \text { et al., 2020) } \end{aligned}$</td>
<td style="text-align: center;">Reading Comprehension (RC)</td>
</tr>
<tr>
<td style="text-align: center;">Multi-hop</td>
<td style="text-align: center;">Question: At which university does the biographer of John Clare teach English Literature? Passage: [DOC] John Clare : John Clare was an English poet ... [DOC] CMS College Kottayam : The CMS College is one ...</td>
<td style="text-align: center;">HotpotQA <br> (Yang et al., 2018)</td>
<td style="text-align: center;">Reading Comprehension (RC)</td>
</tr>
<tr>
<td style="text-align: center;">Hybrid</td>
<td style="text-align: center;">Question: What was the percentage change in gaming between 2018 and 2019? Context: [TAB] Server products and cloud services $132,622126,129$ <br> ... [DOC] Our commercial cloud revenue, which includes Office ...</td>
<td style="text-align: center;">TAT-QA (Zhu et al., 2021)</td>
<td style="text-align: center;">Question Answering (QA)</td>
</tr>
<tr>
<td style="text-align: center;">Quantitative</td>
<td style="text-align: center;">Hypothesis: Teva earns $\$ 7$ billion a year. Premise: After the deal closes, Teva will generate sales of about $\$ 7$ billion a year, the company said.</td>
<td style="text-align: center;">EQUATE <br> (Ravichander <br> et al., 2019)</td>
<td style="text-align: center;">Natural Language Inference (NLI)</td>
</tr>
</tbody>
</table>
<p>Table 1: The demonstration of five representative reasoning types. Listed are the types, the example questions, the representative dataset, and their corresponding tasks. [DOC] and [TAB] indicates the start of a passage and a semi-structured table respectively. Here we regard Question, Conclusion and Hypothesis as sentence, and Passage, Fact, Context and Premise as natural context in Figure 1.
explore whether symbolic reasoning can be internalized by language models and, especially,</p>
<h2>Can neural language models advance reasoning abilities by imitating symbolic reasoners?</h2>
<p>Motivated by this, we conceive a new pretraining paradigm, PoEt (Program Executor), to investigate the learnability of language models from symbolic reasoning and transferrability across distinct representation systems. As illustrated in Figure 1, with a program (e.g., SQL query) and its program context (e.g., database) as input, the model receives automatic supervision from an established program executor (e.g., MySQL) and learns to produce correct execution result. By imitating program execution procedures, we believe LMs could potentially learn the reasoning knowledge that humans adopted to create the associated program executor and tackle NL sentences with the learned reasoning capability. This reveals the key hypothesis of PoEt: program executors are crystallized knowledge of formal reasoning, and such knowledge can be grasped by language models and transferred to NL reasoning via pre-training. In other words, pre-training over natural language might be a contingent condition for LMs to have better reasoning capabilities over natural language.</p>
<p>This contingency assumption of NL brings PoEt another great merit in data quality: while it is typically difficult to obtain large amounts of clean natural language sentences containing clear evidence of reasoning, synthesized programs can be made arbitrarily complicated but readily available on any scale, thanks to the artificial and compo-
sitional nature of programming languages. These merits greatly facilitate the construction of highquality corpora, addressing most of the unresolved shortcomings in previous reasoning-enhancement pre-training. In other words, PoEt differs from existing pre-training paradigms relying on noisy NL data. In summary, our contribution is three-fold:</p>
<ul>
<li>We propose PoEt, a new pre-training paradigm for boosting the reasoning capabilities of language models by imitating program executors. Along with this paradigm, we present three exemplary across-program PoEt instantiations for various reasoning capabilities.</li>
<li>We show with quantitative experiments that the reasoning ability our models obtains from PoEt pre-training is transferable to broader natural language scenarios. On six reasoningfocused downstream tasks, PoEt enables general-purpose language models to achieve competitive performance.</li>
<li>We carry out comprehensive analytical studies, summarize insightful open questions, and provide insights for future work. We hope these insights would shed light on more research on reasoning like program executors ${ }^{1}$.</li>
</ul>
<h2>2 Related Work</h2>
<p>Since we focus on reasoning over natural language, our work is closely related to previous work on</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The illustration of different lines of reasoning, including (a) reasoning via specalized models, (b) reasoning via pre-training over natural language and (c) reasoning via pre-training over program (Ours).
reasoning skills in NL tasks. Regarding methods to inject reasoning skills into LMs, our method is related to two lines of work contributing to the topic: specialized models and pre-training. Last, our work is also related to program execution since we employ program executors during pre-training.</p>
<p>Reasoning Skills The literature focuses on reasoning skills, including numerical reasoning (Dua et al., 2019; Li et al., 2022a), multi-hop reasoning (Yang et al., 2018), reasoning in hybrid context (Chen et al., 2020b; Zhu et al., 2021) and logical reasoning (Liu et al., 2020; Yu et al., 2020). Our work concentrates on improving the above reasoning skills, leaving the other reasoning abilities such as commonsense reasoning (Zellers et al., 2018; Talmor et al., 2019; Bhagavatula et al., 2020) for future work.</p>
<p>Reasoning via Specialized Models Early works typically design specialized models and augment them into LMs for different types of questions (Dua et al., 2019; Andor et al., 2019; Hu et al., 2019; Ding et al., 2019). Taking Hu et al. (2019) as an example, they first predicted the answer type of a given question (e.g., "how many"), and then adopted the corresponding module (e.g., count module) to predict the answer. Although these methods work well on a specific dataset, it is challenging for them to scale to complex reasoning scenarios (Chen et al., 2020c). Differently, our work follows the line of reasoning via pre-training, which enjoys better scalability.</p>
<p>Reasoning via Pre-training This line of work focuses on the continued pre-training of LMs using large-scale data which involves reasoning. The pretraining data are generally NL text, which are either crawled from Web with distant supervision (Deng
et al., 2021), generated by a model-based generator (Asai and Hajishirzi, 2020), or synthesized via human-designed templates (Geva et al., 2020; Yoran et al., 2022; Campagna et al., 2020; Wang et al., 2022). However, large-scale high-quality textual data involving reasoning is difficult to collect (Deng et al., 2021). Meanwhile, as the complexity of desired reasoning operations increases, synthesizing high-quality (e.g., fluent) NL sentences becomes more challenging. Different from the above pre-training methods relying on NL data, our pre-training is performed on programs. These programs can be synthesized at any scale with high quality, and thus are much easier to collect.</p>
<p>Reasoning in Giant Language Models Recent works demonstrate that with proper prompting (e.g., chain-of-thoughts prompting), giant language models (e.g., GPT-3) can perform well on reasoning tasks (Wei et al., 2022; Kojima et al., 2022; Li et al., 2022b). For example, Wei et al. (2022) find that giant language models have the ability to perform complex reasoning step by step with few-shot examples. Although these prompting strategies do not need further fine-tuning, the basic assumptions of them and PoEt are similar, i.e., it is difficult to obtain large amounts of clean sentences involving complex reasoning. However, these prompting strategies do not work well for non-giant language models, while PoEt is simultaneously applicable to language models ranging from millions (e.g., BART) to billions (e.g., T5-11B). It is also interesting to investigate how these prompting strategies and PoEt can be combined.</p>
<p>Program Execution We present a framework to leverage program executors to train LMs, and thus our work is close to recent work on learning a neural program executor. In this line, the most related</p>
<p>work to ours is Liu et al. (2022), which revealed the possibility of SQL execution on helping table pre-training. Different from them mainly focusing on table-related tasks, we present a generalized approach to include Math, Logic, and SQL, as well as their applications on many different natural language downstream tasks. Other related studies include learning program executors on visual question answering (Andreas et al., 2016), reading comprehension (Gupta et al., 2019; Khot et al., 2021), knowledge base question answering (Ren et al., 2021) and 3D rendering (Tian et al., 2019). These works mainly focus on learning a neural network to represent the program executor, while ours focuses on transferring the knowledge of program executor to downstream tasks via pre-training. Other lines of research leverage program execution in inference as a reliable sanity guarantee for generated programs by pruning non-executable candidates (Wang et al., 2018b; Chen et al., 2019b, 2021; Odena et al., 2020; Ellis et al., 2019; Chen et al., 2019b; Sun et al., 2018; Zohar and Wolf, 2018).</p>
<h2>3 Reasoning Like Program Executors</h2>
<p>Reasoning is the process where deduction and induction are sensibly applied to draw conclusions from premises or facts (Scriven, 1976). As a supreme feature of intelligence, humans apply reasoning across modalities. Taking numerical reasoning as an example, humans can tell how many chocolates are consumed from a math word problem description, or from a real-world event where a mother gets off work and finds the choco-can empty, aside standing their guilty-looking kids with brownish stains on their faces. Through detachment of information from their superficial modality and symbolic abstraction, humans manage to unify input formats and condense their numerical reasoning knowledge into one executable symbolic system - This is the origin of an arithmetic program executor. If a model can master these reasoning skills by imitating program executors, we believe in the possibility of transferring those reasoning skills to different modalities. In our case, we expect language models to transfer reasoning to NL-related tasks. Given this motivation, we discuss the fundamental components of PoEt in the rest of this section and present its instantiations later.</p>
<p>Program refers to a finite sequence of symbols that can be understood and executed by machines. For example, a program can be a logical form (e.g.,
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The illustration of PoEt-Math and PoEtLogic. During pre-training, the concatenation of program and program context are fed into language model and the model is expected to output result.</p>
<p>Prolog), a piece of code (e.g., Python), or a math expression. Compared with NL sentences, programs are more formal. Each well-established program follows a specific set of grammar rules and can thus be synthesized systematically. The generalizability of PoEt framework is free from assumption and derived from the set of grammar rules on which a program follows. In PoEt, as long as a program returns meaningful output to reflect its computational procedure, it is an acceptable program.</p>
<p>Program Context is the environment in which a program is running, which holds numerous variables accessible to the program. These variables serve as pivot points that anchor the program context with the program. In the same sense, the question and the passage in reading comprehension hold a similar relationship. This suggests a natural analogy between the program-to-program context and the sentence-to-natural context in Figure 1.</p>
<p>Program Executor is a black-box software that can execute a given program within the program context. An example could be the Python interpreter that executes each line of code, with its specific input data structures as program context. For PoEt, program executors play the role of teachers to educate students (i.e., LMs) on reasoning knowledge they contain. PoEt expects program executors to deterministically execute an input program with respect to a specific program context.</p>
<p>Execution Result is obtained from the program executor, given a program and program context as input. It is much analogous to the answer part in NL downstream tasks. The execution result is the primary observable data reflecting the intermediate</p>
<p>reasoning process and serves as the supervision provided by the program executor.</p>
<h2>4 PoEt with Singleton Executors</h2>
<p>We first instantiate PoEt with two singleton (i.e., a single type of reasoning capability) executors and then move on to PoEt with integrated executors.</p>
<h3>4.1 Learning from Math Calculators</h3>
<p>The PoEt-Math (Left in Figure 3) aims at injecting numerical reasoning skills into LMs via learning from math calculators. Specifically, PoEt-Math is designed to boost the basic arithmetic skills (i.e., addition and subtraction) of LMs on downstream tasks. This arithmetic skill aligns with requirements to answer questions centered on addition/ subtraction between two numbers, such as "What is the difference in casualty numbers between Bavarian and Austrian?".</p>
<p>Pre-training Task Given several floating-point variables as the program context and a math expression only involving addition/ subtraction as the program, the pre-training task of PoEt-Math is to calculate the math expression. Taking the leftmost example from Figure 3, receiving the concatenation of the program and the program context as the input, PoEt-Math is trained to output the number 180.7. Considering the output can be an arbitrary number, the encoder-decoder model (Lewis et al., 2020) is more suitable for this pre-training task.</p>
<p>Pre-training Corpus Each example in the corpus contains a math expression containing up to 2 operators and 3 variables, and a program context that contains at most 30 floating-point variables ${ }^{2}$. The mathematical addition and subtraction operators are denoted by + and - , respectively. The values of variables vary from 0.0 to 1000.0. By random generation, we synthesize 4 million examples as the pre-training corpus for PoEt-Math.</p>
<h3>4.2 Learning from Logic Solvers</h3>
<p>The PoEt-Logic (Mid in Figure 3) aims at injecting logical reasoning (e.g., necessary conditional reasoning) skills into LMs via learning from logic solvers. For example, taking the facts "Only if the government reinforces basic education can we improve our nation's education to a new stage. In order to stand out among other nations, we need to have a strong educational enterprise." as</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Fine-tuning EM performance [\%] of different models on DROP (blue) and LogiQA (red).
premises, PoEt-Logic is intended to help LMs identify whether the conclusion "In order to stand out among nations, we should reinforce basic education" is necessarily implied.</p>
<p>Pre-training Task Given a few first-order logic premise statements as the program context and one conclusion statement as the program, the pretraining task of PoEt-Logic is to identify if the program is necessarily implied from the program context. The execution result, i.e., the implication relationship between the program and the program context, is either True or False. Since the output is binary, an encoder-only model (Liu et al., 2019) is sufficient to perform this pre-training task.</p>
<p>Pre-training Corpus Each example in the corpus contains several premise statements and a conclusion statement. Initially, the statement collection for each example is empty. To produce it, we first allocate 5 Boolean variables (e.g., $p$ and $q$ in Figure 3) and randomly sample at most 8 pairs from their pairwise combinations. For each sampled pair $(p, q)$, we randomly select a statement from the set ${p \rightarrow q, p \rightarrow \neg q, \neg p \rightarrow \neg q, \neg p \rightarrow q}$ and add it to the collection. Once the statement collection is prepared, we randomly select a statement as the conclusion statement (i.e., program) and the rest as the premise statements (i.e., program context). Last, we employ Z3 (De Moura and Bjørner, 2008), the well-known satisfiability modulo theory solver, as our program executor to obtain the implied result. Finally, we synthesize 1 million examples as the pre-training corpus for PoEt-Logic, and nearly $16 \%$ examples ${ }^{3}$ correspond to True.</p>
<h3>4.3 Preliminary Observation</h3>
<p>We perform experiments on DROP and LogiQA to verify if our method improves the reasoning capability required by the dataset. As observed in Figure 4, PoEt-Math boosts the numerical rea-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Example SQL Program</th>
</tr>
</thead>
<tbody>
<tr>
<td>Arithmetic</td>
<td>SELECT [COL] $<em 2="2">{1}-$ [COL] $</em>$</td>
</tr>
<tr>
<td>Superlative</td>
<td>SELECT MAX([COL] $]_{1}$ )</td>
</tr>
<tr>
<td>Comparative</td>
<td>SELECT [COL] $]<em 2="2">{1}$ WHERE [COL] $]</em>$}&gt;[$ VAL $]_{2</td>
</tr>
<tr>
<td>Aggregation</td>
<td>SELECT COUNT([COL] $]_{1}$ )</td>
</tr>
<tr>
<td>Union</td>
<td>SELECT [COL] $]<em 2="2">{1}$ WHERE [COL] $]</em>$ OR}=[$ VAL $]_{2</td>
</tr>
<tr>
<td></td>
<td>[COL] $]<em 3="3">{3}&gt;[$ VAL $]</em>$</td>
</tr>
<tr>
<td>Nested</td>
<td>SELECT [COL] $]<em 2="2">{1}$ WHERE [COL] $]</em>$ IN ( SELECT</td>
</tr>
<tr>
<td></td>
<td>[COL] $]<em 3="3">{2}$ WHERE [COL] $]</em>$ )}=[$ VAL $]_{3</td>
</tr>
</tbody>
</table>
<p>Table 2: The six typical SQL programs that require reasoning. Listed are the type and the example SQL programs. [COL] and [VAL] represent the table column and the table cell value, respectively.
soning ability of BART, bringing in $9.0 \%$ EM gain on DROP. Meanwhile, PoEt-Logic improves the logical reasoning skills of RoBERTa, resulting in a $2.2 \%$ EM improvement on LogiQA. These significant improvements support our main claim that reasoning knowledge of program executors can be transferred to NL scenarios via pre-training.</p>
<h2>5 PoEt with Integrated Executors</h2>
<p>PoEt-Math and PoEt-Logic each focus on one specific reasoning skill, making the pre-training task heavily dependent on the downstream task. Different from them, PoEt-SQL is proposed to allow LMs to master different reasoning skills simultaneously. In our implementation, PoEt-SQL is pre-trained with an integrated SQL executor, since we believe that SQL queries are complex enough to encompass a wide variety of computational procedures (Table 2).</p>
<p>Pre-training Task Given a SQL query as the program and a database as the program context, the pre-training task of PoEt-SQL is to mimic the query result generation. As shown on the right side of Figure 5, given the concatenation of the program and the program context, the model is pre-trained to output the query result. Since the encoder-decoder LMs can generate arbitrary tokens, they are well suited for the task. On the other hand, encoderonly LMs have insufficient expressiveness to produce out-of-context query results. To allow them to benefit from the SQL execution, we tailor the task into a query result selection task for encoderonly LMs, which only utilizes query results that can be found in the database. Specifically, the task requires encoder-only LMs to perform an IO sequence tagging process to find the query results in the database, as shown on the left side of Figure 5.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The illustration of PoEt-SQL pre-training tasks: query result selection for encoder-only and query result generation for encoder-decoder LMs.</p>
<p>Note that the tag I is for tokens in the query results (e.g., Athens), while 0 is for other tokens.</p>
<p>Pre-training Corpus Each example in the corpus contains a SQL query, a database, and a query result. Notably, following Liu et al. (2022), each database is flattened into a sequence when it is fed into LMs. Meanwhile, to avoid databases being too large to fit into memory, we randomly drop the rows of large databases until their flattened sequences contain less than 450 tokens. For the query result generation task, we follow the same corpus construction strategy as described in Liu et al. (2022). Concretely, by instantiating SQL templates from Squall (Shi et al., 2020) over databases provided by WikiSQL (Zhong et al., 2017), 5 million examples are synthesized for pre-training. For the query result selection task, the pre-training corpus is constructed in a similar way as above, except that only the examples whose query results are suitable for encoder-only are retained. This filtering results in a corpus containing nearly 2 million examples.</p>
<h2>6 Experiments and Analysis</h2>
<p>To verify the effectiveness of PoEt-SQL on boosting the reasoning capabilities of LMs, we first apply our method on several backbone models, including encoder-only models and encoder-decoder models. Then we conduct experiments on five typical reasoning benchmark datasets and compare PoETSQL with previous methods. Last, we perform a detailed model analysis to provide more insights.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">DROP $^{\text {® }}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">HotpotQA ${ }^{\text {® }}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TAT-QA ${ }^{\text {® }}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">EQUATE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">EM</td>
</tr>
<tr>
<td style="text-align: center;">BART-Large</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">62.6</td>
</tr>
<tr>
<td style="text-align: center;">PoEt-SQL ${ }_{\text {BART }}$</td>
<td style="text-align: center;">$77.7(+11.5)$</td>
<td style="text-align: center;">$80.6(+11.4)$</td>
<td style="text-align: center;">$66.5(+0.9)$</td>
<td style="text-align: center;">$79.7(+0.8)$</td>
<td style="text-align: center;">$41.5(+2.7)$</td>
<td style="text-align: center;">$49.6(+2.9)$</td>
<td style="text-align: center;">$33.5(+21.1)$</td>
<td style="text-align: center;">$66.5(+3.9)$</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa-Large</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">64.2</td>
</tr>
<tr>
<td style="text-align: center;">PoEt-SQL ${ }_{\text {RoBERTa }}$</td>
<td style="text-align: center;">$\underline{79.8}(+1.7)$</td>
<td style="text-align: center;">$\underline{87.4}(+2.1)$</td>
<td style="text-align: center;">$68.7(+1.1)$</td>
<td style="text-align: center;">$81.6(+0.5)$</td>
<td style="text-align: center;">$59.1(+3.9)$</td>
<td style="text-align: center;">$65.9(+3.2)$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$67.5(+3.3)$</td>
</tr>
</tbody>
</table>
<p>Table 3: The main experimental results of different backbone models on test sets and dev sets ( $\odot$ ) of datasets with or without our PoEt-SQL. The results of PoEt are significantly better than the vanilla LMs ( $p&lt;0.05$ ). Note the performance of RoBERTa and PoEt-SQL ${ }_{\text {RoBERTa }}$ are reported on the subset of DROP where the answer is span(s).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">EM</th>
<th style="text-align: center;">$\mathbf{F}_{\mathbf{1}}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DROP $^{\text {® }}$</td>
<td style="text-align: center;">Specialized Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NumNet (Ran et al., 2019)</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">68.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MTMSN (Hu et al., 2019)</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">80.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NeRd (Chen et al., 2020c)</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">81.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NumNet+ (Ran et al., 2019)</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">84.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QDGAT (Chen et al., 2020a)</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">87.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Language Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5 (Yoran et al., 2022)</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">64.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GenBERT (Geva et al., 2020)</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">72.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PReasM (Yoran et al., 2022)</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">72.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoEt-SQL ${ }_{\text {BART }}$</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">80.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoEt-Math+SQL ${ }_{\text {BART }}$</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">80.9</td>
</tr>
<tr>
<td style="text-align: center;">TAT-QA ${ }^{\text {® }}$</td>
<td style="text-align: center;">TAPAS (Herzig et al., 2020)</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">26.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NumNet+ (Ran et al., 2019)</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">48.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TAoOp (Zhu et al., 2021)</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">62.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TAoOp + PoEt-SQL ${ }_{\text {RoBERTa }}$</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">65.9</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">EM</th>
<th style="text-align: center;">$\mathbf{F}_{\mathbf{1}}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Specialized Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">HotpotQA ${ }^{\text {® }}$</td>
<td style="text-align: center;">DFGN (Qiu et al., 2019)</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">69.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SAE (Tu et al., 2020)</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">80.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">C2F Reader (Shao et al., 2020)</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">81.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HGN (Fang et al., 2020)</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">82.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Language Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT (Devlin et al., 2019)</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">73.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ReasonBERT (Deng et al., 2021)</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">79.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoEt-SQL ${ }_{\text {BART }}$</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">79.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SpanBERT (Joshi et al., 2020)</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">81.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoEt-SQL ${ }_{\text {RoBERTa }}$</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">81.6</td>
</tr>
<tr>
<td style="text-align: center;">EQUATE</td>
<td style="text-align: center;">BERT (Devlin et al., 2019)</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT (Radford et al., 2019)</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QREAS (Ravichander et al., 2019)</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoEt-SQL ${ }_{\text {BART }}$</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoEt-SQL ${ }_{\text {RoBERTa }}$</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">$-$</td>
</tr>
</tbody>
</table>
<p>Table 4: The comparison of our models with baselines on test sets and dev sets ( $\odot$ ) of different datasets. LMs used by all baselines are in Large size, except for ReasonBERT. Bold numbers indicate the best results.</p>
<p>Dataset Setup We perform experiments on different datasets including DROP, HotpotQA, TAT-QA, and EQUATE. Table 1 shows examples of these datasets and their corresponding reasoning types. Furthermore, SVAMP (Patel et al., 2021), the challenging diagnostic dataset for probing numerical reasoning, is employed in our experiments to test the generalization capability of our fine-tuned models on DROP. We evalute models on their addition and subtraction subsets. More details about datasets can be found in Appendix § B.</p>
<p>Backbone Model RoBERTa (Liu et al., 2019) is elected as the backbone in encoder-only LMs, while BART (Lewis et al., 2020) is chosen as the backbone in encoder-decoder LMs. Afterward, We mark the RoBERTa model and the BART model trained under PoEt as PoEt-SQL RoBERTa and PoEt-SQL BART, respectively. For PoEt-SQL BART, we treat all datasets as generative tasks and finetune models to generate answers. As for PoEtSQL RoBERTa , the fine-tuning strategies on different datasets are slightly different, and more implementation details can be found in Appendix § C. Notably, on all datasets, our models are evaluated with official evaluation metrics EM and F1.</p>
<h3>6.1 Experimental Results</h3>
<p>Comparing to Vanilla LMs Table 3 presents an apple-to-apple performance comparison between PoEt-SQL models and their associated vanilla LMs. Across all instances, we observe significant performance increment on downstream NL reasoning tasks. Specifically, PoEt-SQL equips popular encoder-only and encoder-decoder models with an integrated package of reasoning skills, effectively improving their performance on five benchmark datasets. As a highlighted example, PoEt-SQL BART obtains $11.5 \%$ (DROP) and $21.1 \%$ (SVAMP) improvements on EM, compared with the vanilla BART. Since PoEt pre-training is carried purely on program context, whereas all downstream tasks are on natural context, our hypothesis that reasoning capability is transferable from program executors to NL scenarios gets verified.</p>
<p>Comparing to Previous Methods Table 4 lists all experimental results of baselines and our models on different datasets. As seen, our model generally achieves highly competitive results on different reasoning skills, showing its strong performance. Compared with other reasoning-enhanced LMs, PoEt-SQL BART surpasses them by a large</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The EM performance [\%] on the pre-training dev set (Left) and the downstream DROP dev set (Right) with different pre-training steps and scales. PoEt-SQL ( $x \%$ ) denotes the model trained with $x \%$ pre-training examples, while $100 \%$ corresponds to the model trained with the whole pre-training corpus of PoEt-SQL, which contains 5 million examples.
margin, demonstrating the effectiveness of our proposed program execution pre-training. For example, compared with PReasM initialized from T5-Large, PoEt-SQL ${ }<em _BART="{BART" _text="\text">{\text {BART }}$ initialized from BARTLarge exceeds it by $8.3 \%$. Furthermore, PoEt that learns from a mix of program executors (i.e., PoEt-Math+SQL ${ }</em>$ ) achieves a slightly better performance than the single prgoram executor.}</p>
<h3>6.2 Pre-training Analysis</h3>
<p>We show part of the analysis results below due to the limited space, and more analysis can be found in Appendix $\S$ A and $\S$ D.</p>
<p>Necessity of Program Execution PoEt hypothesizes that the acquisition of reasoning ability by models happens at the stage of mimicking program execution, rather than program language modeling. To verify it, we ablate the program executor in PoEt-SQL ${ }_{\text {BART }}$ and carry out a SQL language modeling pre-training instead. Practically, we mask each SQL query in the pre-training corpus of PoETSQL using the strategy adopted in BART (Lewis et al., 2020), and pre-train BART to output the complete SQL query given the masked SQL query and the database. The trivial performance variance corroborates the necessity of program execution.</p>
<p>Effect of the Pre-training Step and Scale Figure 6 illustrates the pre-training and downstream performance with different pre-training steps and scales. It can be seen that both pre-training and downstream performance gradually increase towards the asymptote with increasing pre-training steps, while extra pre-training data steadily accelerate the convergence rate. Although a larger scale
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: The performance comparison between RoBERTa-Large and PoEt-SQL ${ }<em 1="1">{\text {RoBERTa }}$ on representative NLU tasks. On SQuAD and QuoRef, we report $\mathrm{F}</em>$, whereas on MNLI we report accuracy.
yields better performance on the pre-training dev set, $10 \%(500 k)$ data can already converge approximately to the same asymptote as the full data pretraining, showing the high data efficiency of PoEt. The highly plateaued curve also serves as sound evidence that execution pre-training is a data-efficient pre-training approach that converges quickly.</p>
<h2>7 Discussion and Open Questions</h2>
<p>In this section, we carry out comprehensive studies on PoEt, summarize interesting open questions, and provide insights for future work.</p>
<p>Does PoEt improve reasoning skills at the sacrifice of NL understanding abilities? No.</p>
<p>During pre-training, our models are exposed to artificial programs that are dramatically different from NL sentences, raising the concern that models may catastrophically forget their original NL understanding ability. We explore this by comparing PoEt-SQL ${ }<em _RoBERTa="{RoBERTa" _text="\text">{\text {RoBERTa }}$ and the vanilla RoBERTa model on tasks focusing on NL understanding, including SQuAD, MNLI and QuoRef. As can be observed in Figure 7, PoEt-SQL ${ }</em>$ performs almost equally well as RoBERTa on two datasets (i.e., SQuAD and QuoRef), which suggests that PoEt barely sacrifices the intrinsic NL understanding ability of LMs. And the performance drop on MNLI (1.2\%) is also noteworthy, which may be alleviated by joint pre-training on language modeling and our proposed program execution. More experimental details can be found in Appendix $\S$ E.}</p>
<p>Will PoEt be affected by naturalness of program context or program? No.</p>
<p>An intuitive hypothesis is that the effectiveness of PoEt should be positively associated with the naturalness of program and program context, due to</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Settings</th>
<th style="text-align: center;">EM</th>
<th style="text-align: center;">$\mathrm{F}_{1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PoEt-SQL ${ }_{\text {BART }}$</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">80.6</td>
</tr>
<tr>
<td style="text-align: center;">Tuning Program</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\hookrightarrow$ w. Nnatural program</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">79.9</td>
</tr>
<tr>
<td style="text-align: center;">$\hookrightarrow$ w. Unnatural program</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">79.7</td>
</tr>
<tr>
<td style="text-align: center;">Tuning Program Context</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\hookrightarrow$ w. Natural program context</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">79.0</td>
</tr>
</tbody>
</table>
<p>Table 5: The EM and $\mathrm{F}_{1}$ of PoEt-SQL BART on the DROP dev set with respect to different naturalness of program and program context.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: The train and dev perplexity of vanilla BART and BART pre-trained on DROP (BART+DROP) on the pre-training corpus of PoEt-SQL.
closer learning objectives. To test this hypothesis, we design two groups of experiments: (i) Tuning the naturalness of program - we follow Liu et al. (2022) to translate SQL queries into NL sentences to make a more natural program, and replace SQL reserved keywords with low-frequency tokens to make a more unnatural program. (ii) Tuning the naturalness of program context - we follow Chen et al. (2019a) to convert each database in PoEt-SQL pretraining into a set of NL sentences. Surprisingly, results in Table 5 provide counter-evidence to the intuitive hypothesis, since tuning the naturalness of program or program context do not significantly impact PoEt effectiveness. For example, unnatural program only leads to a slight decrease in DROP EM from $77.7 \%$ to $76.9 \%$. It also indictaes that the model learns certain abstract reasoning capabilities rather than lexical associations.</p>
<p>Does pre-training on NL reasoning benefit model learning on program execution? Yes.</p>
<p>If reasoning ability can be transferred from program execution to NL reasoning tasks in PoEt, then the reversed process of PoEt may also work, i.e., models pre-trained with NL reasoning would have better learnability on program execution. To test this speculation, we compare the behavioral difference of vanilla BART and BART pre-trained on DROP in terms of learning SQL execution in Figure 8. There are two indicators that can be used</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">DROP $^{\odot}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SVAMP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
</tr>
<tr>
<td style="text-align: center;">T5-11B</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">52.9</td>
</tr>
<tr>
<td style="text-align: center;">PoEt-SQL $_{\text {TS }}$</td>
<td style="text-align: center;">$85.2(+1.7)$</td>
<td style="text-align: center;">$87.6(+1.7)$</td>
<td style="text-align: center;">$57.4(+4.5)$</td>
</tr>
</tbody>
</table>
<p>Table 6: The experimental results of T5-11B and PoEt$\mathrm{SQL}_{\text {TS }}$ on test sets and dev sets $(\bigcirc)$ of different datasets.
to characterize the behavior of LMs on the SQL execution task: execution accuracy and perplexity, and the execution accuracy always goes higher when the perplexity goes lower. Here the perplexity is presented because it is smoother compared to the execution accuracy, which is either $100 \%$ or $0 \%$. Parallel with our expectation, pre-training on DROP leads to observably lower perplexity for SQL execution learning on both the train and dev sets. The bidirectional enhancement suggests some relative independence between reasoning mechanisms and their symbolic representations.</p>
<p>Can PoEt boost reasoning abilities of giant pre-trained language models? Yes.</p>
<p>Recent work suggest that giant LMs excel at reasoning (Brown et al., 2020), so we are curious if PoEt is effective for them. Following the same procedure as in $\S 6$, we apply PoEt-SQL to T511B, one of the largest publicly available LMs. As shown in Table 6, albeit not as shining as in cases of smaller LMs, PoEt still succeeds in boosting numerical reasoning abilities of giant LMs.</p>
<h2>8 Conclusion \&amp; Future Work</h2>
<p>We introduce PoEt, a new pre-training paradigm for boosting reasoning capability of language models via imitating program executors. Experimental results on six datasets demonstrate that PoEt can significantly boost existing language models on several reasoning skills, including numerical, logical and multi-hop reasoning. Our best language model under PoEt can reach highly competitive performance with previous specialized models. In the future, we hope our work could inspire more transference of reasoning knowledge from program executors to models. And we will also investigate the causes of the reasoning transfer with more insightful experiments, since we still do not know how the reasoning transfer occurs.</p>
<h2>Limitations</h2>
<p>The first limitation of our approach is that it has a relatively strong coupling between the reasoning skills learned in the pre-training task and the reasoning skills required for the downstream task. In other words, PoEt expects reasoning abilities of the program executor to overlap with the downstream reasoning requirements to make the execution learning transferable. Such an expectation also applied fo PoEt-SQL, although it allows LM to master different reasoning skills at the same time. For example, when ablating all programs involving math operations from the pre-training corpus of PoEt-SQL, it shows poor performance on DROP. The second limitation is that PoEt still employs instantiated program templates rather than probabilistic context-free grammars to synthesize programs. The latter usually offers a more diverse range of programs that may contribute to the generalization of the pre-trained language models, but are often more complex.</p>
<h2>Acknowledgement</h2>
<p>We would like to thank Frank F. Xu, Zhengbao Jiang, Bill Yuchen Lin, Shuyan Zhou, Zhiruo Wang, Libo Qin, Jize Jiang, and Jonathan Livengood for fruitful discussion. We also thank all the anonymous reviewers for their constructive feedback and insightful comments.</p>
<h2>References</h2>
<p>Daniel Andor, Luheng He, Kenton Lee, and Emily Pitler. 2019. Giving BERT a calculator: Finding operations and arguments with reading comprehension. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 59475952, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neural module networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 39-48.</p>
<p>Akari Asai and Hannaneh Hajishirzi. 2020. Logicguided data augmentation and regularization for consistent question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5642-5650, Online. Association for Computational Linguistics.</p>
<p>Tarek R. Besold, Artur S. d’Avila Garcez, Sebastian Bader, Howard Bowman, Pedro M. Domingos, Pas-
cal Hitzler, Kai-Uwe Kühnberger, Luís C. Lamb, Daniel Lowd, Priscila Machado Vieira Lima, Leo de Penning, Gadi Pinkas, Hoifung Poon, and Gerson Zaverucha. 2017. Neural-symbolic learning and reasoning: A survey and interpretation. CoRR, abs/1711.03902.</p>
<p>Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Wen tau Yih, and Yejin Choi. 2020. Abductive commonsense reasoning. In International Conference on Learning Representations.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Giovanni Campagna, Agata Foryciarz, Mehrad Moradshahi, and Monica Lam. 2020. Zero-shot transfer learning with synthesized data for multi-domain dialogue state tracking. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 122-132, Online. Association for Computational Linguistics.</p>
<p>Kunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xiaochuan, Yuyu Zhang, Le Song, Taifeng Wang, Yuan Qi, and Wei Chu. 2020a. Question directed graph attention network for numerical reasoning over text. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6759-6768, Online. Association for Computational Linguistics.</p>
<p>Shuang Chen, Qian Liu, Zhiwei Yu, Chin-Yew Lin, JianGuang Lou, and Feng Jiang. 2021. ReTraCk: A flexible and efficient framework for knowledge base question answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 325-336, Online. Association for Computational Linguistics.</p>
<p>Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2019a. Tabfact: A large-scale dataset for table-based fact verification. In International Conference on Learning Representations.</p>
<p>Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Yang Wang. 2020b. HybridQA: A dataset of multi-hop question answering</p>
<p>over tabular and textual data. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1026-1036, Online. Association for Computational Linguistics.</p>
<p>Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V. Le. 2020c. Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. In International Conference on Learning Representations.</p>
<p>Xinyun Chen, Chang Liu, and Dawn Song. 2019b. Execution-guided neural program synthesis. In International Conference on Learning Representations.</p>
<p>Pradeep Dasigi, Nelson F. Liu, Ana Marasović, Noah A. Smith, and Matt Gardner. 2019. Quoref: A reading comprehension dataset with questions requiring coreferential reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5925-5932, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Leonardo De Moura and Nikolaj Bjørner. 2008. Z3: An efficient smt solver. In International conference on Tools and Algorithms for the Construction and Analysis of Systems, pages 337-340. Springer.</p>
<p>Xiang Deng, Yu Su, Alyssa Lees, You Wu, Cong Yu, and Huan Sun. 2021. ReasonBERT: Pre-trained to reason with distant supervision. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6112-6127, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. 2019. Cognitive graph for multi-hop reading comprehension at scale. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2694-2703, Florence, Italy. Association for Computational Linguistics.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368-2378, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Kevin Ellis, Maxwell I. Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando Solar-Lezama. 2019. Write, execute, assess: Program synthesis with a REPL. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 9165-9174.</p>
<p>Yuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang Wang, and Jingjing Liu. 2020. Hierarchical graph network for multi-hop question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8823-8838, Online. Association for Computational Linguistics.</p>
<p>Mor Geva, Ankit Gupta, and Jonathan Berant. 2020. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 946-958, Online. Association for Computational Linguistics.</p>
<p>Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and Matt Gardner. 2019. Neural module networks for reasoning over text. CoRR, abs/1912.04971.</p>
<p>Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: Decoding-enhanced bert with disentangled attention. In International Conference on Learning Representations.</p>
<p>Chadi Helwe, Chloé Clavel, and Fabian M. Suchanek. 2021. Reasoning with transformer-based models: Deep learning, but shallow reasoning. In 3rd Conference on Automated Knowledge Base Construction.</p>
<p>Jonathan Herzig, Pawel Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Eisenschlos. 2020. TaPas: Weakly supervised table parsing via pre-training. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4320-4333, Online. Association for Computational Linguistics.</p>
<p>Minghao Hu, Yuxing Peng, Zhen Huang, and Dongsheng Li. 2019. A multi-type multi-span network for reading comprehension that requires discrete reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1596-1606, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Yinya Huang, Meng Fang, Yu Cao, Liwei Wang, and Xiaodan Liang. 2021. DAGN: Discourse-aware graph network for logical reasoning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5848-5855, Online. Association for Computational Linguistics.</p>
<p>Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020. SpanBERT: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics, 8:64-77.</p>
<p>Tushar Khot, Daniel Khashabi, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2021. Text modular networks: Learning to decompose tasks in the language of existing models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1264-1279, Online. Association for Computational Linguistics.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Moxin Li, Fuli Feng, Hanwang Zhang, Xiangnan He, Fengbin Zhu, and Tat-Seng Chua. 2022a. Learning to imagine: Integrating counterfactual thinking in neural discrete reasoning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 57-69, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2022b. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336.</p>
<p>Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. In Proceedings of the TwentyNinth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 3622-3628. International Joint Conferences on Artificial Intelligence Organization. Main track.</p>
<p>Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. 2022. TAPEX: Table pre-training via learning a neural SQL executor. In International Conference on Learning Representations.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.</p>
<p>Augustus Odena, Kensen Shi, David Bieber, Rishabh Singh, Charles Sutton, and Hanjun Dai. 2020. Bustle: Bottom-up program synthesis through learningguided exploration.</p>
<p>Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48-53, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094, Online. Association for Computational Linguistics.</p>
<p>Lin Qiu, Yunxuan Xiao, Yanru Qu, Hao Zhou, Lei Li, Weinan Zhang, and Yong Yu. 2019. Dynamically fused graph network for multi-hop reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 61406150, Florence, Italy. Association for Computational Linguistics.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling language models: Methods, analysis \&amp; insights from training gopher. CoRR, abs/2112.11446.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. 2019. NumNet: Machine reading comprehension with numerical reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2474-2484, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Abhilasha Ravichander, Aakanksha Naik, Carolyn Rose, and Eduard Hovy. 2019. EQUATE: A benchmark evaluation framework for quantitative reasoning in natural language inference. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 349-361, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Hongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Michihiro Yasunaga, Haitian Sun, Dale Schuurmans, Jure Leskovec, and Denny Zhou. 2021. Lego: Latent execution-guided reasoning for multi-hop question answering on knowledge graphs. In ICML.</p>
<p>Michael Scriven. 1976. Reasoning. New York: McGraw-Hill.</p>
<p>Elad Segal, Avia Efrat, Mor Shoham, Amir Globerson, and Jonathan Berant. 2020. A simple and effective model for answering multi-span questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3074-3080, Online. Association for Computational Linguistics.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural machine translation of rare words with subword units. CoRR, abs/1508.07909.</p>
<p>Nan Shao, Yiming Cui, Ting Liu, Shijin Wang, and Guoping Hu. 2020. Is Graph Structure Necessary for Multi-hop Question Answering? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7187-7192, Online. Association for Computational Linguistics.</p>
<p>Tianze Shi, Chen Zhao, Jordan Boyd-Graber, Hal Daumé III, and Lillian Lee. 2020. On the potential of lexico-logical alignments for semantic parsing to SQL queries. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1849-1864, Online. Association for Computational Linguistics.</p>
<p>Koustuv Sinha, Prasanna Parthasarathi, Joelle Pineau, and Adina Williams. 2021. UnNatural Language Inference. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics
and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7329-7346, Online. Association for Computational Linguistics.</p>
<p>Shao-Hua Sun, Hyeonwoo Noh, Sriram Somasundaram, and Joseph Lim. 2018. Neural program synthesis from diverse demonstration videos. In International Conference on Machine Learning, pages 4790-4799. PMLR.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Yonglong Tian, Andrew Luo, Xingyuan Sun, Kevin Ellis, William T. Freeman, Joshua B. Tenenbaum, and Jiajun Wu. 2019. Learning to infer and execute 3d shape programs.</p>
<p>Ming Tu, Kevin Huang, Guangtao Wang, Jing Huang, Xiaodong He, and Bowen Zhou. 2020. Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 90739080. AAAI Press.</p>
<p>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do NLP models know numbers? probing numeracy in embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5307-5315, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018a. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Chenglong Wang, Kedar Tatwawadi, Marc Brockschmidt, Po-Sen Huang, Yi Xin Mao, Oleksandr Polozov, and Rishabh Singh. 2018b. Robust text-to-sql generation with execution-guided decoding. ArXiv, abs/1807.03100.</p>
<p>Shuohang Wang, Mo Yu, Jing Jiang, and Shiyu Chang. 2018c. A co-matching model for multi-choice read-</p>
<p>ing comprehension. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 746-751, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, and Nan Duan. 2022. Logic-driven context extension and data augmentation for logical reasoning of text. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1619-1629, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and Jonathan Berant. 2020. Break it down: A question understanding benchmark. Transactions of the Association for Computational Linguistics, 8:183-198.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Ori Yoran, Alon Talmor, and Jonathan Berant. 2022. Turning tables: Generating examples from semistructured tables for endowing language models with reasoning skills. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6016-6031, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020. Reclor: A reading comprehension dataset requiring logical reasoning. In International Conference on Learning Representations (ICLR).</p>
<p>Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SWAG: A large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 93-104, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Jing Zhang, Bo Chen, Lingxi Zhang, Xirui Ke, and Haipeng Ding. 2021. Neural, symbolic and neuralsymbolic reasoning on knowledge graphs. AI Open, 2:14-35.</p>
<p>Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2SQL: Generating structured queries from natural language using reinforcement learning. arXiv, abs/1709.00103.</p>
<p>Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and TatSeng Chua. 2021. TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3277-3287, Online. Association for Computational Linguistics.</p>
<p>Amit Zohar and Lior Wolf. 2018. Automatic program synthesis of long programs with a learned garbage collector. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 20982107.</p>
<h2>A Program Context Analysis</h2>
<p>PoEt emphasizes the importance of program context for reasoning transferability, owing to the analogy between the program to program context and the sentence to natural context drawn in Figure 1. To investigate it, we explore the effect of different program context design choices on reasoning transferability by conducting experiments on welldesigned PoEt-Math variants.</p>
<h2>A. 1 The Necessairty of Program Context</h2>
<p>To verify the necessairty of program context, we experiment with PoEt-Math without program context, i.e. a variable-free PoEt-Math variant whose program context is empty. Taking the example of PoEt-Math in Figure 3, the program is transformed into 152.0+99.0-70.3. The experimental results are shown in Table 7. One can see that there</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">EM</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BART-Large</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">69.2</td>
</tr>
<tr>
<td style="text-align: left;">PoEt-Math without program context</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">70.5</td>
</tr>
<tr>
<td style="text-align: left;">PoEt-Math with 0 irrelevant variable</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">74.5</td>
</tr>
<tr>
<td style="text-align: left;">PoEt-Math with 10 irrelevant variables</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">77.5</td>
</tr>
<tr>
<td style="text-align: left;">PoEt-Math with 30 irrelevant variables</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">78.1</td>
</tr>
</tbody>
</table>
<p>Table 7: The DROP performance with different numbers of irrelevant variables in PoEt-Math pre-training.
is a dramatic performance drop in the variant compared to PoEt-Math, verifying the importance of program context.</p>
<h2>A. 2 The Variables Design in Program Context</h2>
<p>In the pre-training task of PoEt-Math, the program context is several floating-point variables. These variables include necessary variables (i.e., variables required by the program) and irrelevant variables. The irrelevant variables exist to make the program context closer to the natural context which generally contains irrelevant sentences. For example, given the program $a+b$ and the program context $a=1 ; b=2 ; c=3 ; d=4 ;$, variables $c$ and $d$ are what we refer to as irrelevant variables. This is motivated by the fact that passages are usually full of irrelevant information regarding a specific question in NL downstream tasks.</p>
<p>In this section, we explore impacts on pretraining effectiveness brought by numbers of irrelevant variables. Empirically, we experiment on pre-training with $0,10,30$ irrelevant variables. The total length of 30 irrelevant variables approaches the maximum input length of pre-trained LMs, and thus we do not try more settings. The experimental results are shown in Table 7. As observed, (i) models can still learn numerical reasoning during pre-training where the program context is free from irrelevant variables, though less effective. (ii) the setting of 30 irrelevant variables brings BART-Large more performance improvement than the setting of 10 irrelevant variables. Considering there are plenty of lengthy passages in the DROP dataset, we therefore hypothesize that the noise level brought by irrelevant variables in the program context during pre-training should be made closer with the counterpart in the natural context during fine-tuning.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"># Questions</td>
<td style="text-align: center;"># Docs</td>
<td style="text-align: center;"># Questions</td>
<td style="text-align: center;"># Docs</td>
</tr>
<tr>
<td style="text-align: left;">DROP</td>
<td style="text-align: center;">77,409</td>
<td style="text-align: center;">5,565</td>
<td style="text-align: center;">9,536</td>
<td style="text-align: center;">582</td>
</tr>
<tr>
<td style="text-align: left;">HotpotQA</td>
<td style="text-align: center;">90,564</td>
<td style="text-align: center;">90,564</td>
<td style="text-align: center;">7,405</td>
<td style="text-align: center;">7,405</td>
</tr>
<tr>
<td style="text-align: left;">TAT-QA</td>
<td style="text-align: center;">13,215</td>
<td style="text-align: center;">2,201</td>
<td style="text-align: center;">1,668</td>
<td style="text-align: center;">278</td>
</tr>
<tr>
<td style="text-align: left;">SVAMP</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">726</td>
<td style="text-align: center;">726</td>
</tr>
<tr>
<td style="text-align: left;">EQUATE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9,606</td>
<td style="text-align: center;">9,606</td>
</tr>
<tr>
<td style="text-align: left;">LogiQA</td>
<td style="text-align: center;">6,942</td>
<td style="text-align: center;">6,942</td>
<td style="text-align: center;">868</td>
<td style="text-align: center;">868</td>
</tr>
</tbody>
</table>
<p>Table 8: The statistics of our experimental datasets.</p>
<h2>B Experimental Setup</h2>
<h2>B. 1 Dataset Setup</h2>
<p>Table 8 presents some statistics about our experimental datasets. Below we introduce each dataset in detail.</p>
<p>DROP A reading comprehension benchmark to measure numerical reasoning ability over a given passage (Dua et al., 2019). It contains three subsets of questions: span, number, and date, each of which involves a lot of numerical operations. Unlike traditional reading comprehension datasets such as SQuAD (Rajpurkar et al., 2016) where answers are always a single span from context, several answers in the span subset of DROP contains multiple spans. The number and date answers are mostly out of context and need generative-level expressiveness.</p>
<p>HotpotQA An extractive reading comprehension dataset that requires models to perform multi-hop reasoning over different passages (Yang et al., 2018). It contains two settings (i) Distractor: reasoning over 2 gold paragraphs along with 8 similar distractor paragraphs and (ii) Full wiki: reasoning over customized retrieval results from full Wikipedia passages. We experiment with its distractor setting since retrieval strategy is beyond our focus in this work.</p>
<p>TAT-QA A question answering benchmark to measure reasoning ability over hybrid context, i.e., passages and tables (Zhu et al., 2021). It is curated by combing paragraphs and tables from real-world financial reports. According to the source(s) the answers are derived from, the dataset can be divided into three subsets: Table, Text and Table-Text(both).</p>
<p>EQUATE The first benchmark dataset to explore quantitative reasoning under the task of natural language inference (Ravichander et al., 2019). As a test-only dataset, it requires fine-tuned models on</p>
<p>MNLI to perform zero-shot natural language inference tasks over quantitative statements described in (premise, hypothesis) pairs to reach final entailment decisions.</p>
<p>LogiQA A multi-choice reading comprehension dataset that evaluates the logical reasoning ability, whose questions are designed by domain experts (Liu et al., 2020). It contains four types of logical reasoning, including categorical reasoning, disjunctive reasoning, conjunctive reasoning and conditional reasoning.</p>
<p>SVAMP A challenging math word problem dataset (Patel et al., 2021). It is designed specifically to hack models who leverage spurious patterns to perform arithmetic operations without true understanding of context. We only keep addition and subtraction problems in accordance with our pre-training coverage.</p>
<h2>B. 2 Baseline Setup</h2>
<p>We summarize the baseline methods in short below, and refer readers to their papers for more details. (i) On DROP, we include two families of models for comparison: specialized models such as NumNet(+) (Ran et al., 2019), MTMSN (Hu et al., 2019), NeRd (Chen et al., 2020c), QDGAT (Chen et al., 2020a) and language models such as GenBERT (Geva et al., 2020) and PReaM (Yoran et al., 2022). (ii) Similarly, on HotpotQA (Distractor), specialized model baselines include DFGN (Qiu et al., 2019), SAE (Tu et al., 2020), C2F Reader (Shao et al., 2020) and the SOTA model HGN (Fang et al., 2020). The language model baselines consist of BERT (Devlin et al., 2019), SpanBERT (Joshi et al., 2020) and ReasonBERT (Deng et al., 2021). (iii) On TATQA, we adopt the official baselines, including TAPAS (Herzig et al., 2020), NumNet+ V2 and the SOTA model TAGOP (Zhu et al., 2021). (iv) On EQUATE, we compare our methods with BERT (Devlin et al., 2019), GPT (Radford et al., 2019) and Q-REAS (Ravichander et al., 2019). (v) On LogiQA, we compare our methods with CoMatching Network (Wang et al., 2018c) and the SOTA model DAGN (Huang et al., 2021).</p>
<h2>C Implementation Details</h2>
<h2>C. 1 PoEt-SQL $_{\text {RoBERTa }}$ on Different Datasets</h2>
<p>On DROP, we cast the span selection task as a sequence tagging problem following Segal et al.
(2020). On TAT-QA, we in-place substitute the RoBERTa-Large encoder in TAGOP (Zhu et al., 2021) with our PoEt-SQL $_{\text {RoBERTa }}$ to verify its effectiveness, and keep the rest of the components unchanged. On HotpotQA, we train two classifiers independently to predict the start and end positions of the answer span, as done in Devlin et al. (2019). On EQUATE, we train a classifier to perform sequence classification on concatenated premise-hypothesis pairs. Notably, we follow the official setup to train LMs on the MNLI dataset (Williams et al., 2018) and evaluate their zero-shot performance on EQUATE. On SVAMP, the encoder-only model is not suitable since the answers are out-of-context.</p>
<h2>C. 2 Pre-training Details</h2>
<p>By default, we apply AdamW as pre-training optimizer with default scheduling parameters in fairseq. The coefficient of weight decay is set as 0.05 to alleviate over-fitting of pre-trained models. Additionally, we employ fp16 to accelerate the pre-training.</p>
<p>PoEt-Math The pre-training procedure lasts for 10,000 steps with a batch size of 512 . After the warm up in the first 2000 steps, the learning rate arrives the peak at $3 \times 10^{-5}$ during pre-training.</p>
<p>PoEt-Logic The pre-training procedure lasts for 5,000 steps with a batch size of 512 . After the warm up in the first 1000 steps, the learning rate arrives the peak at $3 \times 10^{-5}$ during pre-training.</p>
<p>PoEt-SQL For PoEt-SQL ${ }<em _RoBERTa="{RoBERTa" _text="\text">{\text {BART }}$ and PoEtSQL $</em>$ during pre-training. The maximum input length in each example is truncated to 384 tokens to increase the batch size.}}$, the pre-training procedure lasts for 50,000 steps with a batch size of 512 . After the warm up in the first 5000 steps, the learning rate arrives the peak at $3 \times 10^{-5}$ during pre-training. To save memory, each example in the pre-training corpus could at most contains 512 tokens. For PoEtSQL $_{75}$, the pre-training procedure lasts for 20,000 steps with a batch size of 512 . After the warm up in the first 2000 steps, the learning rate arrives the peak at $1 \times 10^{-5</p>
<h2>C. 3 Fintuning Details</h2>
<p>We implement our models based on transformers (Wolf et al., 2020), fairseq (Ott et al., 2019) and DeepSpeed ${ }^{4}$.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: left;">Number</th>
<th style="text-align: left;">Span</th>
<th style="text-align: left;">Spans</th>
<th style="text-align: left;">Date</th>
<th style="text-align: left;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Previous Systems</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">MTMSN (BERT)</td>
<td style="text-align: left;">81.1</td>
<td style="text-align: left;">82.8</td>
<td style="text-align: left;">62.8</td>
<td style="text-align: left;">69.0</td>
<td style="text-align: left;">80.5</td>
</tr>
<tr>
<td style="text-align: left;">NumNet+ (RoBERTa)</td>
<td style="text-align: left;">83.1</td>
<td style="text-align: left;">$86.8^{*}$</td>
<td style="text-align: left;">$86.8^{*}$</td>
<td style="text-align: left;">63.9</td>
<td style="text-align: left;">84.4</td>
</tr>
<tr>
<td style="text-align: left;">QDGAT (RoBERTa)</td>
<td style="text-align: left;">86.2</td>
<td style="text-align: left;">$88.5^{*}$</td>
<td style="text-align: left;">$88.5^{*}$</td>
<td style="text-align: left;">67.5</td>
<td style="text-align: left;">87.1</td>
</tr>
<tr>
<td style="text-align: left;">GenBERT</td>
<td style="text-align: left;">75.2</td>
<td style="text-align: left;">74.5</td>
<td style="text-align: left;">24.2</td>
<td style="text-align: left;">56.4</td>
<td style="text-align: left;">72.3</td>
</tr>
<tr>
<td style="text-align: left;">PReasM</td>
<td style="text-align: left;">64.4</td>
<td style="text-align: left;">86.6</td>
<td style="text-align: left;">78.4</td>
<td style="text-align: left;">77.7</td>
<td style="text-align: left;">72.3</td>
</tr>
<tr>
<td style="text-align: left;">Original LMs</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa-Large</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">86.4</td>
<td style="text-align: left;">79.9</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">BART-Large</td>
<td style="text-align: left;">63.6</td>
<td style="text-align: left;">79.6</td>
<td style="text-align: left;">74.6</td>
<td style="text-align: left;">62.1</td>
<td style="text-align: left;">69.2</td>
</tr>
<tr>
<td style="text-align: left;">T5-11B</td>
<td style="text-align: left;">83.2</td>
<td style="text-align: left;">90.2</td>
<td style="text-align: left;">85.8</td>
<td style="text-align: left;">84.9</td>
<td style="text-align: left;">85.8</td>
</tr>
<tr>
<td style="text-align: left;">PoEt Models</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">PoEt-SQL $_{\text {RoBERTa }}$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">88.2</td>
<td style="text-align: left;">83.1</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">PoEt-SQL $_{\text {BART }}$</td>
<td style="text-align: left;">78.9</td>
<td style="text-align: left;">84.5</td>
<td style="text-align: left;">79.6</td>
<td style="text-align: left;">71.9</td>
<td style="text-align: left;">80.6</td>
</tr>
<tr>
<td style="text-align: left;">PoEt-SQL $_{\text {T5 }}$</td>
<td style="text-align: left;">85.2</td>
<td style="text-align: left;">92.4</td>
<td style="text-align: left;">86.6</td>
<td style="text-align: left;">84.4</td>
<td style="text-align: left;">87.6</td>
</tr>
</tbody>
</table>
<p>Table 9: Breakdown of model $F_{1}$ score by answer types on the dev set of DROP. Some works only report overall span type performance (marked by *), and single-span is non-separable from multi-span performance. Bold and underlined numbers indicate the best and second-best results, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">RTE-Q</th>
<th style="text-align: center;">NewsNLI</th>
<th style="text-align: center;">RedditNLI</th>
<th style="text-align: center;">NR ST</th>
<th style="text-align: center;">AWPNLI</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Previous Systems</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">MAJ</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.4</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">51.8</td>
</tr>
<tr>
<td style="text-align: left;">GPT</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">55.8</td>
</tr>
<tr>
<td style="text-align: left;">Q-REAS</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">60.7</td>
</tr>
<tr>
<td style="text-align: left;">Original LMs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">BART-Large</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">62.6</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa-Large</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">64.2</td>
</tr>
<tr>
<td style="text-align: left;">PoEt Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">PoEt-SQL $_{\text {BART }}$</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">66.5</td>
</tr>
<tr>
<td style="text-align: left;">PoEt-SQL $_{\text {RoBERTa }}$</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">67.5</td>
</tr>
</tbody>
</table>
<p>Table 10: The EM performance of different models on all subsets of the EQUATE benchmark. Bold and underlined numbers indicate the best and second-best results, respectively.</p>
<p>Passage Retrieval in HotpotQA Since the total length of the original passages in HotpotQA is too long to fit into memory, we train a classifier to filter out top-3 passages, as done in previous work (Deng et al., 2021). Specifically, a RoBERTa-Large model is fine-tuned to discriminate if an input passage is required to answer the question. The Hits@3 score of the classifier on HotpotQA is $97.2 \%$.</p>
<p>Numerical Design in DROP and SVAMP As noticed by previous works, sub-word tokenization methods such as byte pair encoding (Sennrich et al., 2015) potentially undermines the arithmetic ability of models. Instead, the character-level number representation is argued to be a more effective alleviation (Wallace et al., 2019). Additionally, the reverse decoding of numbers is proposed as a better way of modelling arithmetic carry (Geva et al.,
2020). Therefore, we employ these design strategies on DROP and SVAMP.</p>
<h2>C. 4 Fine-tuning Hyperpameters</h2>
<p>By default, we apply AdamW as fine-tuning optimizer with default scheduling parameters on all datasets. To ensure statistical significance, all finetuning procedures are run with three random seeds, except for T5-11B and PoEt-SQLT5 due to the limit of computation budgets.</p>
<p>DROP PoEt-SQL RoBERTa and RoBERTa-Large are trained with the subset of questions marked as "span" from the DROP dataset.t Since a gold answer may occur multiple times in the passage, we optimize over the sum of negative log probability for all possibly-correct IO sequences where each one of gold answers is included at least once,</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Table</th>
<th style="text-align: center;">Text</th>
<th style="text-align: center;">Table-Text</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{EM} / \mathrm{F}_{1}$</td>
<td style="text-align: center;">$\mathrm{EM} / \mathrm{F}_{1}$</td>
<td style="text-align: center;">$\mathrm{EM} / \mathrm{F}_{1}$</td>
<td style="text-align: center;">$\mathrm{EM} / \mathrm{F}_{1}$</td>
</tr>
<tr>
<td style="text-align: center;">Arithmetic</td>
<td style="text-align: center;">50.1 / 50.1</td>
<td style="text-align: center;">43.8 / 50.0</td>
<td style="text-align: center;">55.6 / 55.6</td>
<td style="text-align: center;">51.5 / 51.5</td>
</tr>
<tr>
<td style="text-align: center;">Counting</td>
<td style="text-align: center;">66.7 / 66.7</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">90.0 / 90.0</td>
<td style="text-align: center;">81.3 / 81.3</td>
</tr>
<tr>
<td style="text-align: center;">Spans</td>
<td style="text-align: center;">67.4 / 80.6</td>
<td style="text-align: center;">54.2 / 80.8</td>
<td style="text-align: center;">79.2 / 84.8</td>
<td style="text-align: center;">71.4 / 82.6</td>
</tr>
<tr>
<td style="text-align: center;">Span</td>
<td style="text-align: center;">68.4 / 68.4</td>
<td style="text-align: center;">51.2 / 76.0</td>
<td style="text-align: center;">76.2 / 77.8</td>
<td style="text-align: center;">61.9 / 74.6</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">56.5 / 58.0</td>
<td style="text-align: center;">51.1 / 75.0</td>
<td style="text-align: center;">69.0 / 70.7</td>
<td style="text-align: center;">59.1 / 65.9</td>
</tr>
</tbody>
</table>
<p>Table 11: The EM performance of TAGOP (POET-SQL ${ }<em _BART="{BART" _text="\text">{\text {RoBERTa }}$ ) with respect to answer types and sources on the dev set of TAT-QA.
as done in Segal et al. (2020). The fine-tuning procedure runs up to 25,000 steps with a batch size of 64 , with the learning rate of $7.5 \times 10^{-6}$. As for BART-Large (and POET-SQL ${ }</em>$.}}$, POET-Math, the same below) and T5-11B (and POET-SQL ${ }_{75}$, the same below), they are trained with the whole DROP dataset. For BART-Large, the fine-tuning procedure runs up to 20,000 steps with a batch size as 128 and a learning rate as $3 \times 10^{-5}$. For T5-11B, due to the computational budget, the fine-tuning procedure only lasts for 10,000 steps with a batch size of 32 , and the learning rate is $1 \times 10^{-5</p>
<p>TAT-QA In the experiment of TAT-QA, we employ the official implementation and the default hyperparameters provided in TAGOP ${ }^{5}$. The finetuning procedure runs up to 50 epochs with a batch size of 48 . For modules introduced in TAGOP, the learning rate is set as $5 \times 10^{-4}$, while for RoBERTaLarge (and POET-SQL ${ }_{\text {RoBERTa }}$ ), the learning rate is set as $1.5 \times 10^{-5}$.</p>
<p>HotpotQA The fine-tuning procedure runs up to 30,000 steps with a batch size of 64 . The learning rate is $1 \times 10^{-5}$. Overlong inputs are truncated to 512 tokens for both RoBERTa-Large (and POET-SQL ${ }<em 75="75">{\text {RoBERTa }}$ ), T5-11B (and POET-SQL ${ }</em>$ ).}$ ) and BART-Large (and POET-SQL ${ }_{\text {BART }</p>
<p>EQUATE The fine-tuning procedure runs up to 20,000 steps on MNLI with a batch size of 128 for both RoBERTa-Large (and POET-SQL RoBERTa ) and BART-Large (and POET-SQL ${ }_{\text {BART }}$ ), with learning rate is $1 \times 10^{-5}$. After fine-tuning, models are directly evaluated on EQUATE.</p>
<p>LogiQA In the experiment of LogiQA, we employ the open-source implementation and the default hyperparameters provided in ReClor ${ }^{6}$ (Yu et al., 2020) to fine-tune RoBERTa-Large (and POET-SQL ${ }_{\text {RoBERTa }}$ ). The fine-tuning procedure</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>runs up to 10 epochs with a batch size of 24 . The learning rate is set as $1 \times 10^{-5}$.</p>
<h2>D Fine-grained Analysis</h2>
<p>DROP In Table 9 we report model $\mathrm{F}<em _BART="{BART" _text="\text">{1}$ scores by question type on DROP. Comparing three PoEt pre-trained models with their vanilla versions, we observe that: (i) PoEt-SQL ${ }</em>$, we also observe $2 \%$ improvement on number questions, $2.2 \%$ on span and $0.8 \%$ on spans questions. These model-agnostic performance boost on DROP reveals the extra numerical reasoning knowledge models learned from SQL program executors.}}$ outperforms the vanilla BART-large with a wide margin in all types of questions, i.e. number ( $15.3 \%$ ), date ( $9.8 \%$ ), span (around $5 \%$ ). (ii) PoEt-SQL RoBERTa only deals with span selection questions, and obtain $1.9 \%, 3.2 \%$ gain on span, spans questions, respectively. (iii) For the giant $\operatorname{PoEt}-\mathrm{SQL}_{75</p>
<p>EQUATE Table 10 presents performance breakdown by subsets of EQUATE (Ravichander et al., 2019), where we compare PoEt-SQL ${ }<em _RoBERTa="{RoBERTa" _text="\text">{\text {BART }}$ and PoEt-SQL ${ }</em>$ alone demonstrate improvement on RedditNLI (emphasizes approximation and verbal quantitative reasoning) subset. Performance on other subsets are approximately comparable between PoEt pre-trained models and vanilla models, suggesting that PoEt does not harm intrinsic abilities of language models.}}$ with their vanilla versions and previous baselines. For both models, we observe around $10 \%$ acc improvement on the $N R S T$ subset, where numerical comparison and quantifiers are especially emphasized. Stable performance improvement was also observed in both pre-trained models on the RTE-Q subset, where arithmetics and ranges are primary focus. Interestingly, PoEtSQL $_{\text {RoBERTa }</p>
<p>TAT-QA Table 11 shows the detailed experimental results of TAGOP (POET-SQL RoBERTa ). Considering that the pre-training of POET-SQL RoBERTa is</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Train</th>
<th></th>
<th></th>
<th>Dev</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td># Questions</td>
<td># Docs</td>
<td></td>
<td># Questions</td>
<td># Docs</td>
</tr>
<tr>
<td>SQuAD</td>
<td>77,409</td>
<td>5,565</td>
<td></td>
<td>9,536</td>
<td>582</td>
</tr>
<tr>
<td>MNLI</td>
<td>392,702</td>
<td>392,702</td>
<td></td>
<td>9,815</td>
<td>9,815</td>
</tr>
<tr>
<td>QuoRef</td>
<td>19,399</td>
<td>3,771</td>
<td></td>
<td>2,418</td>
<td>454</td>
</tr>
</tbody>
</table>
<p>Table 12: PoEt on NL understanding experiment dataset statistics.
only performed on table-like texts (i.e., the flatten sequence of databases), it is highly non-trivial for our model to generalize to such a hybrid scenario containing both tables and passages, again illustrating the transferability of reasoning capabilities.</p>
<h1>E NL Understanding Performance</h1>
<p>Dataset Details We fine-tune PoEt-SQL $_{\text {Roberta }}$ on (i) SQuAD v1.0: (Rajpurkar et al., 2016): one of the most classical single-span selection RC benchmarks measuring understanding over natural language context; (ii) MNLI (Williams et al., 2018): a large-scale NLI dataset measuring cross-domain and cross-genre generalization of NLU. Notably, our model is evaluated on the matched setting for the purpose of simplicity. (iii) QuoRef (Dasigi et al., 2019): A Wikipedia-based multi-span selection RC benchmark with a special emphasis on coreference resolution. All dataset Statistics are shown in Table 12.</p>
<p>Implementation Details (i) On SQuAD, we cast the span selection task as a sequence tagging problem following Segal et al. (2020). (ii) On MNLImatched, we train both models to perform sequence classification on concatenated premise-hypothesis pairs. (iii) On Quoref, we cast the span(s) selection task as an IO sequence tagging problem following Segal et al. (2020).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://github.com/NExTplusplus/TAT-QA
${ }^{6}$ https://github.com/yuweihao/reclor&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ To clarify, $16 \%$ is not a specific-purpose design but a statistical result.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
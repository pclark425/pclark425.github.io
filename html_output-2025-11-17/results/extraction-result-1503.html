<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1503 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1503</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1503</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-a20f3dfc9142b48b924e68ee22ba259a0d621bb2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a20f3dfc9142b48b924e68ee22ba259a0d621bb2" target="_blank">AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles</a></p>
                <p><strong>Paper Venue:</strong> International Symposium on Field and Service Robotics</p>
                <p><strong>Paper TL;DR:</strong> A new simulator built on Unreal Engine that offers physically and visually realistic simulations for autonomous vehicles in real world and that is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols.</p>
                <p><strong>Paper Abstract:</strong> Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process. Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments. We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals. Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink). The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols. In addition, the modular design enables various components to be easily usable independently in other projects. We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1503.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1503.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AirSim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AirSim (Microsoft)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source, high-fidelity visual and physical simulator for autonomous vehicles built as a plugin for Unreal Engine, designed for real-time hardware-in-the-loop (HITL) use and to support development of data-driven methods (e.g., reinforcement learning) with realistic sensors, vehicle dynamics and photorealistic rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>AirSim</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Simulates multirotor (and other vehicle) rigid-body dynamics, actuators, sensors (IMU, magnetometer, barometer, GPS), environment (gravity, magnetic field, air density/pressure) and photorealistic visual rendering via Unreal Engine; designed for realtime HITL at high update rates.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / autonomous vehicles (flight dynamics, sensing)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>high-fidelity (visual and physical) for perception and dynamics relative to many prior simulators; real-time HITL capable (physics loop up to ~1000 Hz) with detailed sensor models and photorealistic rendering, but some simplifying approximations for computational tractability.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Includes: high-rate rigid-body physics (Velocity-Verlet integration), propeller force/torque models, linear and angular drag approximations, gravity varying with altitude (approximate), tilted-dipole magnetic model, standard-atmosphere pressure/density models, sensor noise and bias drift (Gaussian and Gaussian-Markov), photorealistic rendering (UE4 features). Simplifications / absent features: no detailed motor/airframe vibration model (causes IMU in-flight mismatch), simplistic GPS degradation modelling, limited advanced collision/ground interaction models, limited camera noise/lens artifacts (not fully modelled), no advanced wind/thermal models yet.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>autonomous quadrotor (flight controller + potential RL agents)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>In experiments the flight controller firmware (PX4) was used; the platform is intended to train/develop reinforcement learning and other data-driven control/perception algorithms (generic RL agents or computer-vision based agents).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Autonomous flight control and navigation (trajectory following, state estimation and perception for autonomous vehicles); intended to support training of reinforcement-learning policies and perception modules.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world quadrotor (Pixhawk v2 on Flamewheel frame) using same offboard commands</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Trajectory similarity: symmetric Hausdorff distances between simulated and real-world tracks — circle: 1.47 m, square: 0.65 m. Sensor comparisons: stationary IMU variances similar (gyro simulated 2.47e-7 rad^2/s^2 vs real 6.71e-7; accel simulated 1.78e-4 m^2/s^4 vs real 1.93e-4), in-flight accelerometer variance significantly higher in real-world (simulated 1.75e-3 m^2/s^4 vs real 9.46 m^2/s^4) attributed to unmodelled airframe vibrations; barometer and magnetometer characteristics closely match real sensors (plots and qualitative comparison reported).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors argue high-fidelity visual and physical simulation narrows sim-to-real gap for ML methods; they discuss which components are essential (realistic rendering for vision, high-rate physics and realistic sensor noise/bias drift for control and state estimation). They note some features are unnecessary for real-time HITL (they use approximations) but do not quantify a strict minimal fidelity; explicitly mention missing features that impair transfer (vibrations, advanced collision/ground models, camera artifacts, GPS degradation, wind/thermal effects).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported mismatch in in-flight IMU accelerometer variance (real much larger) due to not modelling airframe vibrations; limited GPS and camera noise modelling and simplistic collision/ground interaction are potential failure sources for transfer; authors note small trajectory differences likely due to integration errors, vehicle model approximations and mild random winds not modelled.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1503.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1503.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gazebo simulator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely-used open-source multi-robot simulator offering modular physics engines, sensor models and 3D world creation, commonly used for robotics research and integration with ROS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Design and use paradigms for gazebo, an open-source multi-robot simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Provides modular simulation (supports different physics engines), sensor models and environment construction for robotics; focuses on rigid-body robots, links-and-joints architectures, and integration with ROS.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / mechanics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity for physics and sensors; flexible but limited in large-scale photorealistic visual fidelity compared to game engines.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Supports multiple physics engines and sensors; good for kinematic and dynamic simulation of articulated robots but lacks advanced photorealistic rendering and large-scale visually rich environments (limitations noted for perception-focused ML).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper notes Gazebo's modular physics is feature-rich but that its rendering lags behind modern engines; implies for vision-based ML higher visual fidelity (from UE4/Unity) is beneficial for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Difficulty creating large-scale, visually rich environments can limit transfer for perception-driven tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1503.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1503.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hector</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hector (simulator/framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulator/framework focusing on tight integration with ROS and Gazebo, offering wind-tunnel-tuned flight dynamics and sensor models including bias drift via Gaussian-Markov processes and support for software-in-loop.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Hector</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Provides flight dynamics tuned to wind-tunnel data, sensor models with bias drift (Gaussian-Markov), and software-in-the-loop capabilities, but limited support for certain hardware/protocols (e.g., Pixhawk, MavLink) and dependent on Gazebo/ROS environment.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>aerial vehicle dynamics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity dynamics (wind-tunnel tuning) and realistic sensor noise modelling for certain sensors; constrained by Gazebo/ROS rendering capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Includes wind-tunnel tuned dynamics and sensor bias-drift models; lacks support for popular hardware platforms/protocols and visual richness for perception tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors mention Hector provides more accurate dynamics/sensor drift but its dependence on Gazebo limits visual richness; suggests fidelity components (dynamics vs rendering) have different importance depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Lacks support for certain hardware protocols (Pixhawk, MavLink) which can hinder direct HITL transfer workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1503.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1503.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RotorS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RotorS (modular Gazebo MAV simulator framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular framework built on Gazebo to design and test micro aerial vehicle control and state estimation algorithms, supporting hardware-in-the-loop setups with Pixhawk.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rotorsa modular gazebo mav simulator framework.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>RotorS</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Gazebo-based modular MAV simulator framework emphasizing control and state estimation algorithm development; can be configured for HITL with Pixhawk.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>aerial robotics / control</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity (physics and sensors via Gazebo), focuses on control and estimation testing rather than high-end visual realism.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Modular vehicle definitions, supports HITL, uses Gazebo's physics and sensor models (thus limited visual/perception fidelity compared to UE4).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Implied trade-off: RotorS is adequate for control and state-estimation development but constrained by Gazebo's visual fidelity for vision-based ML.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Perception-driven tasks that require photorealistic rendering may be poorly served.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1503.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1503.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>jMavSim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>jMavSim</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight simulator designed primarily to test PX4 firmware and devices, with simple sensor models and a basic rendering engine lacking environmental objects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Jmavsim</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>jMavSim</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Simple, easy-to-use simulator tightly coupled with PX4 simulation APIs; uses simple sensor models and minimal rendering, intended for firmware testing rather than high-fidelity perception research.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>aerial vehicle firmware testing / control</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>low-fidelity (simplified sensor models and basic rendering)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Simpler sensor models, basic rendering with no environmental objects; tightly coupled to PX4 APIs for firmware testing.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Suitable for firmware-in-loop testing where simple dynamics and sensors suffice; not appropriate for training perception-based ML or for sim-to-real transfer that depends on visual richness.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Limited rendering and simplistic sensors likely to fail for vision-based learning and may not capture real-world sensor idiosyncrasies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1503.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1503.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unreal Engine 4 (UE4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unreal Engine 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art game engine used for photorealistic rendering and environment creation; in AirSim used to provide visually rich, high-fidelity scenes and collision detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Real shading in unreal engine 4</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Unreal Engine 4 (rendering engine)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Provides photorealistic rendering features (physically based materials, photometric lights, reflections, shadows) and collision detection; used by AirSim to generate realistic visual input and environments (including photogrammetry-based scenes).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>visual rendering / perception simulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>high visual-fidelity (state-of-the-art rendering), but not a full physics simulator by itself.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Supports advanced shading, reflections, shadows and photogrammetry assets; supplies collision detection information; camera sensor oddities (lens models, advanced noise) are not fully simulated by default in AirSim and require additional modelling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors emphasize the importance of photorealistic rendering (UE4) for vision-driven learning and transfer to real-world perception tasks, but note additional camera noise/lens artifacts are not yet modelled.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Lack of advanced camera sensor oddities modelling means some perception-domain mismatches may persist despite photorealistic rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Design and use paradigms for gazebo, an open-source multi-robot simulator. <em>(Rating: 2)</em></li>
                <li>Rotorsa modular gazebo mav simulator framework. <em>(Rating: 2)</em></li>
                <li>Jmavsim <em>(Rating: 2)</em></li>
                <li>Comprehensive simulation of quadrotor uavs using ros and gazebo. <em>(Rating: 1)</em></li>
                <li>Real shading in unreal engine 4 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1503",
    "paper_id": "paper-a20f3dfc9142b48b924e68ee22ba259a0d621bb2",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "AirSim",
            "name_full": "AirSim (Microsoft)",
            "brief_description": "An open-source, high-fidelity visual and physical simulator for autonomous vehicles built as a plugin for Unreal Engine, designed for real-time hardware-in-the-loop (HITL) use and to support development of data-driven methods (e.g., reinforcement learning) with realistic sensors, vehicle dynamics and photorealistic rendering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "AirSim",
            "simulator_description": "Simulates multirotor (and other vehicle) rigid-body dynamics, actuators, sensors (IMU, magnetometer, barometer, GPS), environment (gravity, magnetic field, air density/pressure) and photorealistic visual rendering via Unreal Engine; designed for realtime HITL at high update rates.",
            "scientific_domain": "mechanics / autonomous vehicles (flight dynamics, sensing)",
            "fidelity_level": "high-fidelity (visual and physical) for perception and dynamics relative to many prior simulators; real-time HITL capable (physics loop up to ~1000 Hz) with detailed sensor models and photorealistic rendering, but some simplifying approximations for computational tractability.",
            "fidelity_characteristics": "Includes: high-rate rigid-body physics (Velocity-Verlet integration), propeller force/torque models, linear and angular drag approximations, gravity varying with altitude (approximate), tilted-dipole magnetic model, standard-atmosphere pressure/density models, sensor noise and bias drift (Gaussian and Gaussian-Markov), photorealistic rendering (UE4 features). Simplifications / absent features: no detailed motor/airframe vibration model (causes IMU in-flight mismatch), simplistic GPS degradation modelling, limited advanced collision/ground interaction models, limited camera noise/lens artifacts (not fully modelled), no advanced wind/thermal models yet.",
            "model_or_agent_name": "autonomous quadrotor (flight controller + potential RL agents)",
            "model_description": "In experiments the flight controller firmware (PX4) was used; the platform is intended to train/develop reinforcement learning and other data-driven control/perception algorithms (generic RL agents or computer-vision based agents).",
            "reasoning_task": "Autonomous flight control and navigation (trajectory following, state estimation and perception for autonomous vehicles); intended to support training of reinforcement-learning policies and perception modules.",
            "training_performance": null,
            "transfer_target": "real-world quadrotor (Pixhawk v2 on Flamewheel frame) using same offboard commands",
            "transfer_performance": "Trajectory similarity: symmetric Hausdorff distances between simulated and real-world tracks — circle: 1.47 m, square: 0.65 m. Sensor comparisons: stationary IMU variances similar (gyro simulated 2.47e-7 rad^2/s^2 vs real 6.71e-7; accel simulated 1.78e-4 m^2/s^4 vs real 1.93e-4), in-flight accelerometer variance significantly higher in real-world (simulated 1.75e-3 m^2/s^4 vs real 9.46 m^2/s^4) attributed to unmodelled airframe vibrations; barometer and magnetometer characteristics closely match real sensors (plots and qualitative comparison reported).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "Authors argue high-fidelity visual and physical simulation narrows sim-to-real gap for ML methods; they discuss which components are essential (realistic rendering for vision, high-rate physics and realistic sensor noise/bias drift for control and state estimation). They note some features are unnecessary for real-time HITL (they use approximations) but do not quantify a strict minimal fidelity; explicitly mention missing features that impair transfer (vibrations, advanced collision/ground models, camera artifacts, GPS degradation, wind/thermal effects).",
            "failure_cases": "Reported mismatch in in-flight IMU accelerometer variance (real much larger) due to not modelling airframe vibrations; limited GPS and camera noise modelling and simplistic collision/ground interaction are potential failure sources for transfer; authors note small trajectory differences likely due to integration errors, vehicle model approximations and mild random winds not modelled.",
            "uuid": "e1503.0",
            "source_info": {
                "paper_title": "AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "Gazebo",
            "name_full": "Gazebo simulator",
            "brief_description": "A widely-used open-source multi-robot simulator offering modular physics engines, sensor models and 3D world creation, commonly used for robotics research and integration with ROS.",
            "citation_title": "Design and use paradigms for gazebo, an open-source multi-robot simulator.",
            "mention_or_use": "mention",
            "simulator_name": "Gazebo",
            "simulator_description": "Provides modular simulation (supports different physics engines), sensor models and environment construction for robotics; focuses on rigid-body robots, links-and-joints architectures, and integration with ROS.",
            "scientific_domain": "robotics / mechanics",
            "fidelity_level": "medium-fidelity for physics and sensors; flexible but limited in large-scale photorealistic visual fidelity compared to game engines.",
            "fidelity_characteristics": "Supports multiple physics engines and sensors; good for kinematic and dynamic simulation of articulated robots but lacks advanced photorealistic rendering and large-scale visually rich environments (limitations noted for perception-focused ML).",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "Paper notes Gazebo's modular physics is feature-rich but that its rendering lags behind modern engines; implies for vision-based ML higher visual fidelity (from UE4/Unity) is beneficial for transfer.",
            "failure_cases": "Difficulty creating large-scale, visually rich environments can limit transfer for perception-driven tasks.",
            "uuid": "e1503.1",
            "source_info": {
                "paper_title": "AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "Hector",
            "name_full": "Hector (simulator/framework)",
            "brief_description": "A simulator/framework focusing on tight integration with ROS and Gazebo, offering wind-tunnel-tuned flight dynamics and sensor models including bias drift via Gaussian-Markov processes and support for software-in-loop.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "Hector",
            "simulator_description": "Provides flight dynamics tuned to wind-tunnel data, sensor models with bias drift (Gaussian-Markov), and software-in-the-loop capabilities, but limited support for certain hardware/protocols (e.g., Pixhawk, MavLink) and dependent on Gazebo/ROS environment.",
            "scientific_domain": "aerial vehicle dynamics / robotics",
            "fidelity_level": "medium-fidelity dynamics (wind-tunnel tuning) and realistic sensor noise modelling for certain sensors; constrained by Gazebo/ROS rendering capabilities.",
            "fidelity_characteristics": "Includes wind-tunnel tuned dynamics and sensor bias-drift models; lacks support for popular hardware platforms/protocols and visual richness for perception tasks.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "Authors mention Hector provides more accurate dynamics/sensor drift but its dependence on Gazebo limits visual richness; suggests fidelity components (dynamics vs rendering) have different importance depending on task.",
            "failure_cases": "Lacks support for certain hardware protocols (Pixhawk, MavLink) which can hinder direct HITL transfer workflows.",
            "uuid": "e1503.2",
            "source_info": {
                "paper_title": "AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "RotorS",
            "name_full": "RotorS (modular Gazebo MAV simulator framework)",
            "brief_description": "A modular framework built on Gazebo to design and test micro aerial vehicle control and state estimation algorithms, supporting hardware-in-the-loop setups with Pixhawk.",
            "citation_title": "Rotorsa modular gazebo mav simulator framework.",
            "mention_or_use": "mention",
            "simulator_name": "RotorS",
            "simulator_description": "Gazebo-based modular MAV simulator framework emphasizing control and state estimation algorithm development; can be configured for HITL with Pixhawk.",
            "scientific_domain": "aerial robotics / control",
            "fidelity_level": "medium-fidelity (physics and sensors via Gazebo), focuses on control and estimation testing rather than high-end visual realism.",
            "fidelity_characteristics": "Modular vehicle definitions, supports HITL, uses Gazebo's physics and sensor models (thus limited visual/perception fidelity compared to UE4).",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "Implied trade-off: RotorS is adequate for control and state-estimation development but constrained by Gazebo's visual fidelity for vision-based ML.",
            "failure_cases": "Perception-driven tasks that require photorealistic rendering may be poorly served.",
            "uuid": "e1503.3",
            "source_info": {
                "paper_title": "AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "jMavSim",
            "name_full": "jMavSim",
            "brief_description": "A lightweight simulator designed primarily to test PX4 firmware and devices, with simple sensor models and a basic rendering engine lacking environmental objects.",
            "citation_title": "Jmavsim",
            "mention_or_use": "mention",
            "simulator_name": "jMavSim",
            "simulator_description": "Simple, easy-to-use simulator tightly coupled with PX4 simulation APIs; uses simple sensor models and minimal rendering, intended for firmware testing rather than high-fidelity perception research.",
            "scientific_domain": "aerial vehicle firmware testing / control",
            "fidelity_level": "low-fidelity (simplified sensor models and basic rendering)",
            "fidelity_characteristics": "Simpler sensor models, basic rendering with no environmental objects; tightly coupled to PX4 APIs for firmware testing.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "Suitable for firmware-in-loop testing where simple dynamics and sensors suffice; not appropriate for training perception-based ML or for sim-to-real transfer that depends on visual richness.",
            "failure_cases": "Limited rendering and simplistic sensors likely to fail for vision-based learning and may not capture real-world sensor idiosyncrasies.",
            "uuid": "e1503.4",
            "source_info": {
                "paper_title": "AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "Unreal Engine 4 (UE4)",
            "name_full": "Unreal Engine 4",
            "brief_description": "A state-of-the-art game engine used for photorealistic rendering and environment creation; in AirSim used to provide visually rich, high-fidelity scenes and collision detection.",
            "citation_title": "Real shading in unreal engine 4",
            "mention_or_use": "use",
            "simulator_name": "Unreal Engine 4 (rendering engine)",
            "simulator_description": "Provides photorealistic rendering features (physically based materials, photometric lights, reflections, shadows) and collision detection; used by AirSim to generate realistic visual input and environments (including photogrammetry-based scenes).",
            "scientific_domain": "visual rendering / perception simulation",
            "fidelity_level": "high visual-fidelity (state-of-the-art rendering), but not a full physics simulator by itself.",
            "fidelity_characteristics": "Supports advanced shading, reflections, shadows and photogrammetry assets; supplies collision detection information; camera sensor oddities (lens models, advanced noise) are not fully simulated by default in AirSim and require additional modelling.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "Authors emphasize the importance of photorealistic rendering (UE4) for vision-driven learning and transfer to real-world perception tasks, but note additional camera noise/lens artifacts are not yet modelled.",
            "failure_cases": "Lack of advanced camera sensor oddities modelling means some perception-domain mismatches may persist despite photorealistic rendering.",
            "uuid": "e1503.5",
            "source_info": {
                "paper_title": "AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles",
                "publication_date_yy_mm": "2017-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Design and use paradigms for gazebo, an open-source multi-robot simulator.",
            "rating": 2
        },
        {
            "paper_title": "Rotorsa modular gazebo mav simulator framework.",
            "rating": 2
        },
        {
            "paper_title": "Jmavsim",
            "rating": 2
        },
        {
            "paper_title": "Comprehensive simulation of quadrotor uavs using ros and gazebo.",
            "rating": 1
        },
        {
            "paper_title": "Real shading in unreal engine 4",
            "rating": 1
        }
    ],
    "cost": 0.01219375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles</h1>
<p>Shital Shah ${ }^{1}$, Debadeepta Dey ${ }^{2}$, Chris Lovett ${ }^{3}$, Ashish Kapoor ${ }^{4}$</p>
<h4>Abstract</h4>
<p>Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process. Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments. We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals. Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink). The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols. In addition, the modular design enables various components to be easily usable independently in other projects. We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights.</p>
<h2>1 Introduction</h2>
<p>Recently, paradigms such as reinforcement learning [12], learning-by-demonstration [2] and transfer learning [25] are proving a natural means to train various robotics systems. One of the key challenges with these techniques is the high sample complexity - the amount of training data needed to learn useful behaviors is often prohibitively high. This issue is further exacerbated by the fact that autonomous vehicles are often unsafe and expensive to operate during the training phase. In order to seamlessly operate in the real world the robot needs to transfer the learning it does in simulation. Currently, this is a non-trivial task as simulated perception, environments and actuators are often simplistic and lack the richness or diversity of the real world. For example, for robots that aim to use computer vision in outdoor en-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1 A snapshot from AirSim shows an aerial vehicle flying in an urban environment. The inset shows depth, object segmentation and front camera streams generated in real time.
vironments, it may be important to model real-world complex objects such as trees, roads, lakes, electric poles and houses along with rendering that includes finer details such as soft shadows, specular reflections, diffused inter-reflections and so on. Similarly, it is important to develop more accurate models of system dynamics so that simulated behavior closely mimics the real-world.</p>
<p>AirSim is an open-source platform [21] that aims to narrow the gap between simulation and reality in order to aid development of autonomous vehicles. The platform seeks to positively influence development and testing of data-driven machine intelligence techniques such as reinforcement learning and deep learning. It is inspired by several previous simulators (see related work), and one of our key goals is to build a community to push the state-of-the-art towards this goal.</p>
<h1>2 Related Work</h1>
<p>While an exhaustive review of currently used simulators is beyond the scope of this paper, we mention a few notable recent works that are closest to our setting and has deeply influenced this work.</p>
<p>Gazebo [13] has been one the most popular simulation platforms for the research work. It has a modular design that allows to use different physics engines, sensor models and create 3D worlds. Gazebo goes beyond monolithic rigid body vehicles and can be used to simulate more general robots with links-and-joints architecture such as complex manipulator arms or biped robots. While Gazebo is fairly feature rich it has been difficult to create large scale complex visually rich environments</p>
<p>that are closer to the real world and it has lagged behind various advancements in rendering techniques made by platforms such as Unreal engine or Unity.</p>
<p>Other notable efforts includes Hector [17] that primarily focuses on tight integration with popular middleware ROS and Gazebo. It offers wind tunnel tuned flight dynamics, sensor models that includes bias drift using Gaussian Markov process and software-in-loop using Orocos toolchain. However, Hector lacks support for popular hardware platforms such as Pixhawk and protocols such as MavLink. Because of its tight dependency on ROS and Gazebo, it's limited by richness of simulated environments as noted previously.</p>
<p>Similarly, RotorS [7] provides a modular framework to design Micro Aerial Vehicles, and build algorithms for control and state estimation that can be tested in simulator. It is possible to setup RotorS for HITL with Pixhawk. RotorS also uses Gazebo as its platform, consequently limiting its perception related capabilities.</p>
<p>Finally, jMavSim [1] is easy to use simulator that was designed with a goal of testing PX4 firmware and devices. It is therefore tightly coupled with PX4 simulation APIs, uses albeit simpler sensor models and utilizes simple rendering engine without any objects in the environment.</p>
<p>Apart from these, there have been many games like simulators and training applications, however, these are mostly commercial closed-source software with little or no public information on models, accuracy of simulation or development APIs for autonomous applications.</p>
<h1>3 Architecture</h1>
<p>Our simulator follows a modular design with an emphasis on extensibility. The core components includes environment model, vehicle model, physics engine, sensor models, rendering interface, public API layer and an interface layer for vehicle firmware as depicted in Figure 2.</p>
<p>The typical setup for an autonomous aerial vehicle includes the flight controller firmware such as PX4 [16], ROSFlight [10], Hackflight[15] etc. The flight controller takes desired state and the sensor data as inputs, computes the estimate of current state and outputs the actuator control signals to achieve the desired state. For example, in case of quadrotors, user may specify desired pitch, roll and yaw angles as desired state and the flight controller may use sensor data from accelerometer and gyroscope to estimate the current angles and finally compute the motor signals to achieve the desired angles.</p>
<p>During simulation, the simulator provides the sensor data from the simulated world to the flight controller. The flight controller outputs the actuator signals which is taken as input by the the vehicle model component of the simulator. The goal of the vehicle model is to compute the forces and torques generated by the simulated actuators. For example, in case of quadrotors, we compute the thrust and torques produced by the propellers given the motor voltages. In addition, there may be forces generated from drag, friction and gravity. These forces and torques are then taken</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2 The architecture of the system that depicts the core components and their interactions.
as inputs by the physics engine to compute the next kinematic state of bodies in the simulated world. This kinematic state of bodies along with the environment models for gravity, air density, air pressure, magnetic field and geographic location (GPS coordinates) provides the ground truth for the simulated sensor models.</p>
<p>The desired state input to the flight controller can be set by human operator using remote control or by a companion computer in the autonomous setting. The companion computer may perform expensive higher level computations such as determining next desired waypoint, performing simultaneous localization and mapping (SLAM), computing desired trajectory etc. The companion computer may have to process large amount of data generated by the sensors such as vision cameras and lidars which in turn requires that simulated environments have reasonable details. This has been one of the challenging areas where we leverage recent advances in rendering technologies implemented by platforms such as Unreal engine [11]. In addition, we also utilize the underlying pipeline in the Unreal engine to detect collisions. The companion computer interacts with the simulator via a set of APIs that allows it to observe the sensor streams, vehicle state and send commands. These APIs are designed such that it shields the companion computer from being aware of whether its being run under simulation or in the real world. This is particularly important so that one can develop and test algorithms in simulator and deploy to real vehicle without having to make additional changes.</p>
<p>The AirSim code base is implemented as a plugin for the Unreal engine that can be dropped in to any Unreal project. The Unreal engine platform offers an elaborate marketplace with hundreds of pre-made detailed environments, many created using photogrammetry techniques [18] to generate reasonably faithful reconstruction of real-world scenes.</p>
<p>Next, we provide more details on the individual components of the simulator.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3 Vehicle model for the quadrotor. The four blue vertices experience the controls $u_{1}, . . u_{4}$, which in turn results in the forces $\mathbf{F}<em 4="4">{1}, . ., \mathbf{F}</em>$.}$ and the torques $\tau_{1}, . ., \tau_{4</p>
<h1>3.1 Vehicle Model</h1>
<p>AirSim provides an interface to define vehicle as a rigid body that may have arbitrary number of actuators generating forces and torques. The vehicle model includes parameters such as mass, inertia, coefficients for linear and angular drag, coefficients of friction and restitution which is used by the physics engine to compute rigid body dynamics.</p>
<p>Formally, a vehicle is defined as a collection of $K$ vertices placed at positions $\left{\mathbf{r}<em k="k">{1}, . ., \mathbf{r}</em>}\right}$ and normals $\left{\mathbf{n<em k="k">{1}, . ., \mathbf{n}</em>\right}$. The forces and torques from these vertices are assumed to be generated in the direction of their normals. However note that the positions as well as normals are allowed to change during the simulation.}\right}$, each of which experience a unitless vehicle specific scaler control input $\left{u_{1}, . ., u_{k</p>
<p>Figure 3 shows how a quadrotor can be depicted as a collection of four vertices. The control input $u_{i}$ drives the rotational speed of the propellers located at the four vertices. We compute the forces and torques produced by propellers using [4]:</p>
<p>$$
\mathbf{F}<em T="T">{i}=C</em>
$$} \rho \omega_{\max }^{2} D^{4} u_{i} \quad \text { and } \quad \tau_{i}=\frac{1}{2 \pi} C_{\text {pow }} \rho \omega_{\max }^{2} D^{5} u_{i</p>
<p>Here $C_{T}$ and $C_{\text {pow }}$ are the thrust and the power coefficients respectively and are based on the physical characteristics of the propeller, $\rho$ is the air density, $D$ is the propeller's diameter and $\omega_{\max }$ is the max angular velocity in revolutions per minute. By allowing the movements of these vertices during the flight it is possible to simulate the vehicles with capabilities such as Vertical Take-Off and Landing (VTOL) and other recent quadrotors that change their configuration in flight.</p>
<p>The vehicle model abstract interface also provides a way to specify the cross sectional area in body frame that in turn can be used by physics engine to compute the linear and angular drag on the body.</p>
<h1>3.2 Environment</h1>
<p>The vehicle is exposed to various physical phenomena including gravity, air-density, air pressure and magnetic field. While it is possible to produce computationally expensive models of these phenomena that are very accurate, we focus our attention to models that are accurate enough to allow a real-time operation with hardware-in-the-loop. We describe these individual components of the environment below.</p>
<h3>3.2.1 Gravity</h3>
<p>While many models use a constant number to model the gravity, it varies in a complex manner as demonstrated by models such as GRACE [23]. For most ground based or low altitude vehicles these variations may not be important; however, it is fairly inexpensive to incorporate a more accurate model. Formally, we approximate the gravitational acceleration $g$ at height $h$ by applying binomial theorem on Newton's law of gravity and neglecting the higher powers:</p>
<p>$$
g=g_{0} \cdot \frac{R_{e}^{2}}{\left(R_{e}+h\right)^{2}} \approx g_{0} \cdot\left(1-2 \frac{h}{R_{e}}\right)
$$</p>
<p>Here $R_{e}$ is Earth's radius and $g_{0}$ is the gravitational constant measured at the surface.</p>
<h3>3.2.2 Magnetic Field</h3>
<p>Accurately modeling the magnetic field of a complex body such as Earth is a computationally expensive task. The World Magnetic Model (WMM) model [6] by National Oceanic and Atmospheric Administration (NOAA) is one of the best known magnetic models of Earth. Unfortunately, the most recent model WMM2015 is fairly complex and computationally expensive for real-time applications.</p>
<p>We implemented the tilted dipole model where we assume Earth as a perfect dipole sphere [14, pp 27-30]. This ignores all but the first order terms to derive magnetic field estimate using the spherical geometry. This model allows us to simulate variation of the magnetic field as we move in space as well as areas that are often problematic such as polar regions. Given a geographic latitude $\theta$, longitude $\phi$ and altitude $h$ (from surface of the earth), we first compute the magnetic co-latitude $\theta_{m}$ using:</p>
<p>$$
\cos \theta_{m}=\cos \theta \cos \theta^{0}+\sin \theta \sin \theta^{0} \cos \left(\phi-\phi^{0}\right)
$$</p>
<p>Where $\theta^{0}$ and $\phi^{0}$ denote the latitude and longitude of the true magnetic north pole. Then, the total magnetic intensity $|B|$ is computed as:</p>
<p>$$
|B|=B_{0}\left(\frac{R_{e}}{R_{e}+h}\right)^{3} \sqrt{1+3 \cos ^{2} \theta_{m}}
$$</p>
<p>Here $B_{0}$ is the mean value of the magnetic field at the magnetic equator on the Earth's surface, $\theta_{m}$ is the magnetic co-latitude and $R_{e}$ is the mean radius of the Earth. Next, we determine the inclination $\alpha$ and declination $\beta$ angles using:</p>
<p>$$
\tan \alpha=2 \cot \theta_{m} \quad \text { and } \quad \sin \beta= \begin{cases}\sin \left(\phi-\phi^{0}\right) \frac{\cos \theta^{0}}{\sin \theta_{m}}, &amp; \text { if } \cos \theta_{m}&gt;\sin \theta^{0} \sin \theta \ \cos \left(\phi-\phi^{0}\right) \frac{\cos \theta^{0}}{\sin \theta_{m}}, &amp; \text { otherwise. }\end{cases}
$$</p>
<p>Finally, we can compute the horizontal field intensity $(H)$, the latitudinal $(X)$, the longitudinal $(Y)$ and the vertical field $(Z)$ components of the magnetic field vector as follows:</p>
<p>$$
H=|B| \cos \alpha \quad Z=|B| \sin \alpha \quad X=H \cos \beta \quad Y=H \sin \beta
$$</p>
<h1>3.2.3 Air Pressure and Density</h1>
<p>The relationship between the altitude and the pressure of the Earth's atmosphere is complicated due to the presence of many distinct layers, each with its own individual properties. First we compute Standard Temperature $T$ and Standard Pressure $P$ using 1976 U.S. Standard Atmosphere model [22, eq 1.16, 1.17] for altitude below 51 kilometers and switch to the model in [3, Table 4] beyond that up to 86 km . Then, the air density is $\rho=\frac{P}{R \cdot T}$ (where $R$ is the specific gas constant.)</p>
<h3>3.3 Physics Engine</h3>
<p>The kinematic state of the body is expressed using 6 quantities: position, orientation, linear velocity, linear acceleration, angular velocity and angular acceleration. The goal of the physics engine is to compute the next kinematic state for each body given the forces and torques acting on it. We strive for an efficient physics engine that can run its update loop at high frequency $(1000 \mathrm{~Hz})$ which is desirable for enabling realtime simulation scenarios such as high speed quadrotor control. Consequently, we implement a physics engine that avoids the extra complexities of a generic engine allowing us to tightly control the performance and make trade-offs that best meet our requirements.</p>
<h3>3.3.1 Linear and Angular drag</h3>
<p>Since the vehicle moves in the presence of air, the linear and the angular drag has a significant effect on the dynamics of the body. The simulator computes the magnitude $\left|\mathbf{F}_{d}\right|$ of the linear drag force on the body according to the drag equation [24]:</p>
<p>$$
\left|\mathbf{F}<em i="i" l="l" n="n">{d}\right|=\frac{1}{2} \rho|\mathbf{v}|^{2} C</em> A
$$</p>
<p>Here $C_{\text {lin }}$ is the linear air drag coefficient, $A$ is the vehicle cross-section and $\rho$ is the air density. This drag force acts in the direction opposite to the velocity vector $\mathbf{v}$</p>
<p>Computing the angular drag for arbitrary shape remains complex and computationally intensive task. Many existing physics engines use a small but often an arbitrary damping constant as a substitute for computing actual angular drag. We provide simple but better approximations to model the angular drag.</p>
<p>Consider an infinitesimal surface area $d s$ in the extremity of the body experiencing the angular velocity $\omega$. As the linear velocity $\mathbf{d v}$ experienced by $d s$ is given by $\mathbf{r}_{d s} \times \omega$, we can now use the linear drag equation for $d s[19$, pp 160-161]:</p>
<p>$$
|\mathbf{d F}|=\frac{1}{2} \rho\left|\mathbf{r}<em i="i" l="l" n="n">{d s} \times \omega\right|^{2} C</em> \times \omega
$$} d s, \quad \text { where direction of } \mathbf{d F} \text { is }-\mathbf{r}_{d s</p>
<p>Now, the drag torque is computed by integrating over the entire surface: $\tau_{d}=\int_{S} \mathbf{r}_{d s} \times$ $\mathbf{d F}$. To simplify the implementation, we approximate the body of the vehicle as set of connected faces which further can be approximated as a rectangular box for the purpose of evaluating the integral.</p>
<h1>3.3.2 Accelerations</h1>
<p>In addition to the drag forces and torques, we also need to consider the forces $\mathbf{F}<em i="i">{i}$ and the torques $\tau</em>$ relative to center of gravity (see section 3.1). We thus compute the net force and torque as:}$ present on the vehicle at the vertex located at $\mathbf{r}_{i</p>
<p>$$
\mathbf{F}<em i="i">{n e t}=\sum</em>} \mathbf{F<em d="d">{i}+\mathbf{F}</em>} \quad \text { and } \quad \tau_{n e t}=\sum_{i}\left[\tau_{i}+\mathbf{r<em i="i">{i} \times \mathbf{F}</em>
$$}\right]+\tau_{d</p>
<p>We obtain the linear acceleration by applying Newton's second law and then adding gravity vector to compute the net acceleration, $\mathbf{a}=\mathbf{F}<em e="e" n="n" t="t">{n e t} / m+\mathbf{g}$. The angular acceleration in body frame is given by Euler's rotation equation: $\alpha=I^{-1} \cdot\left(\tau</em>\right.$ $\omega))$ ), where, $I$ is the inertia tensor and $\omega$ is angular velocity, both in body frame.}-(\omega \times(I^{-</p>
<h3>3.3.3 Integration</h3>
<p>We update the position $\mathbf{p}<em 0="0">{k+1}$ of the body at time $k+1$ by integrating the velocity and the initial position $\mathbf{p}</em>$. The first order integration algorithms such as Euler method diverges quickly with unbounded error although very simple to implement. In our implementation we use Velocity Verlet algorithm instead of Runge Kutta for its computationally inexpensiveness and stability while still being second order method [9]. Formally,</p>
<p>$$
\mathbf{v}<em k="k">{k+1}=\mathbf{v}</em>}+\frac{\mathbf{a<em k_1="k+1">{k}+\mathbf{a}</em>}}{2} \cdot d t \quad \mathbf{p<em k="k">{k+1}=\mathbf{p}</em>}+\mathbf{v<em k="k">{k} \cdot d t+\frac{1}{2} \cdot \mathbf{a}</em>
$$} \cdot d t^{2</p>
<p>The angular velocity is updated in similar manner as linear velocity however updating orientation isn't straight forward. One of the approach is to maintains the orientation as a rotation matrix that is updated every time step. However this causes a slow drift which must be corrected by orthonormalization at regular intervals which is expensive. Alternative approach is to maintain rotations as much more efficient quaternions which are also numerically stable and trivially normalizable. One of the problem, however, is that the orientation quaternion is maintained in the world frame while the angular velocity is maintained in the body frame in our framework. To update the orientation, we first compute the angle-axis pair $\left(\alpha_{d t}, \mathbf{u}\right)$ where $\alpha_{d t}$ is the angle traversed around unit vector $\mathbf{u}$. We can compute the angle $\alpha_{d t}=|\omega| \cdot d t$ and axis by $u=\omega /|\omega|$. This allows us to compute equivalent change in quaternion $\mathbf{q}<em d="d" t="t">{d t}$ representing the change in orientation in time $d t$. As noted before, $\mathbf{q}</em>}$ is in body frame while $\mathbf{q<em d="d" t="t">{k}$ in world reference frame. The problem now remains that of adding $\mathbf{q}</em>}$ to $\mathbf{q<em k_1="k+1">{k}$ to obtain $\mathbf{q}</em>}$ which can be proven to given by relationship $\mathbf{q<em k="k">{k+1}=\mathbf{q}</em>$.} \cdot \mathbf{q}_{d t</p>
<h1>3.3.4 Collisions</h1>
<p>Unreal engine offers a rich collision detection system optimized for different classes of collision meshes and we directly use this feature for our needs. We receive the impact position, impact normal and penetration depth for each collision that occurred during the render interval. Our physics engine uses this data to compute the collision response with Coulomb friction to modify both linear and angular kinematics.[8]</p>
<h3>3.4 Sensors</h3>
<p>AirSim offers sensor models for accelerometer, gyroscope, barometer, magnetometer and GPS. All our sensor models are implemented as C++ header-only library and can be independently used outside of AirSim. Like other components, sensor models are expressed as abstract interfaces so it is easy to replace or add new sensors.</p>
<h3>3.4.1 Barometer</h3>
<p>To simulate barometer, we compute ground truth pressure using the detailed model of atmosphere (sec 3.2.3) and model the drift in the pressure measurement over time using Gaussian Markov process [20] for more realistic behavior in long flights. Formally, if we denote the current bias factor as $b_{k}$ then the drift is modeled as:</p>
<p>$$
b_{k+1}=w \cdot b_{k}+(1-w) \cdot \eta, \text { where: } w=e^{-\frac{d t}{t}} \text { and } \eta \sim N\left(0, s^{2}\right)
$$</p>
<p>Here $\tau$, is the time constant for the process and set to 1 hour in our model. $\eta$ is a zero mean Gaussian noise with standard deviation that can be selected using the</p>
<p>data available in [5]. This pressure $p$ is then added with white noise drawn from zero mean Gaussian distribution with standard deviation set from datasheet of the sensor (such as MEAS MS56112). Finally we convert the pressure to altitude using barometric formula used by the sensor's driver:</p>
<p>$$
h=\frac{T_{0}}{a}\left[\left(\frac{p}{p_{0}}\right)^{-\left(\frac{a R}{g}\right)}-1\right]
$$</p>
<p>here $T_{0}$ is the reference temperature ( 15 deg C ), $a=-6.5 \times 10^{-3}$ is the temperature gradient, $g$ and $R$ are the gravity and the specific gas constants, $p_{0}$ is the current sea level pressure and $p$ is the measurement.</p>
<h1>3.4.2 Gyroscope and Accelerometer</h1>
<p>Gyroscope and accelerometers constitute the core of the inertial measurement unit (IMU) [26]. We model these by adding white noise and bias drift over time to the ground truth. For gyroscope, given the true angular velocity in body frame $\omega$, we compute the measurement $\omega^{\text {out }}$ as,</p>
<p>$$
\begin{aligned}
\omega^{\text {out }} &amp; =\omega+\eta_{a}+b_{t}, &amp; &amp; \text { where } \eta_{a} \sim N\left(0, r_{a}\right) \text { and } \
b_{t} &amp; =b_{t-1}+\eta_{b}, &amp; &amp; \text { where } \eta_{b} \sim N\left(0, b_{0} \sqrt{\frac{d t}{t_{a}}}\right)
\end{aligned}
$$</p>
<p>Here parameters $r_{a}$, bias $b_{0}$ and the time constant for bias drift $t_{a}$ can either be obtained from Allan variance plots or from datasheets. Accelerometer output is computed in the similar manner except that we must first subtract gravity from the true linear acceleration in the world frame and then convert the result to the body frame before we add bias drift and noise.</p>
<h3>3.4.3 Magnetometer</h3>
<p>We use the tilted dipole model for Earth's magnetic field 3.2.2, given the geographic coordinates to compute the components of the ground truth magnetic field in body frame and add the white noise as specified in the datasheet.</p>
<h3>3.4.4 Global Positioning System (GPS)</h3>
<p>Our GPS model simulates latency (typically 200 ms ), slower update rates (typically 50 Hz ) and horizontal and vertical position error estimate decay rates to simulate gaining fix over time. The decay rate is modeled using first order low pass filter individually parameterized for horizontal and vertical fix.</p>
<h1>3.5 Visual Rendering</h1>
<p>Since advanced rendering and detailed environments have been a key requirement for AirSim we chose Unreal Engine 4 (UE4) [11] as our rendering platform. UE4 offers several features that made it an attractive choice including it being an open source and available on Linux, Windows as well as OSX. UE4 brings some of the cutting edge graphics features such as physically based materials, photometric lights, planar reflections, ray traced distance field shadows, lit translucency etc. Figure 1 shows a screen-shot from AirSim which highlight near photo-realistic rendering capabilities. Further, Unreal's large online Marketplace has various pre-made elaborate environments, many of which are created using photogrammetry techniques.</p>
<h2>4 Experiments</h2>
<p>We perform experiments primarily to evaluate how close the flight characteristic of a quadrotor flying in real-world is to that of a simulation of the same vehicle in AirSim. We also evaluate some of our sensor models against the real-world sensors.</p>
<p>Hardware Platform: Real-world flights were performed with the Pixhawk v2 flight controller mounted on a Flamewheel quadrotor frame, together with a Gigabyte 5500 Brix running Ubuntu 16.04. The sensor measurements were recorded on the Pixhawk device itself. We configured the simulated quadrotor in AirSim using the measured physical parameters and simulated sensor models configured using sensor data sheets. The AirSim MavLinkTest application was used to perform repeatable offboard control for both the real-world and the simulated flights.</p>
<p>Trajectory Evaluation: We fly the quadrotor in the simulator in two different patterns: (1) trajectory in square shape with each side being 5 m long (2) trajectory in circle shape with radius being 10 m long. We then use exact same commands to fly the real vehicle. For both the simulation and the real-world flights, we collect location of the vehicle in local NED coordinates along with timestamps.</p>
<p>Figure 4(c) and 4(d) shows the time series of locations in simulated flight and the real flight. Here, the horizontal axis represents the time and the vertical axis represent the off-set in X and Y directions. We also compute the symmetric Hausdorff distance between the real-world track and the track in simulation. We found that the simulation and real-world tracks were fairly close both for the circle (Hausdorff distance between simulated and real-world: 1.47 m ) as well as the square (Hausdorff distance between simulated and real-world: 0.65 m ).</p>
<p>We also present visual comparison for this experiment for the circle and the square patterns in Figures 4(a) and 4(b) respectively. The simulated trajectory is shown with a purple line while the real trajectory is shown with a red line. We can observe that qualitatively the trajectories tracked by both the real-world and the simulated vehicle are close. The small differences may have been caused by various</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4 Evaluating the differences between the simulated and the real-world flight. In top figures, the purple and the red lines depict the track from simulation and the real-world flights respectively.
factors such as integration errors, vehicle model approximations and mild random winds.</p>
<p>Sensor Models: Besides evaluating the entire simulation pipeline we also investigated individual component models, namely the barometer (MEAS MS561101BA), the magnetometer (Honeywell HMC5883) and the IMU (InvenSense MPU 6000). Note that the simulated GPS model is currently simplistic, thus, we only focus on the three more complex sensor models. For each of the above sensors we use the manufacture specified datasheets to set the parameters in the sensor models.</p>
<ul>
<li>IMU: We measured readings from the accelerometers and gyroscope as the vehicle was stationary and flying. We observed that while the characteristics were similar when the vehicle was stationary (gyro: simulated variance $2.47 \mathrm{e}-7$ $\mathrm{rad}^{2} / \mathrm{s}^{2}$, real-world variance $6.71 \mathrm{e}-7 \mathrm{rad}^{2} / \mathrm{s}^{2}$, accel.: simulated variance $1.78 \mathrm{e}-4$ $\mathrm{m}^{2} / \mathrm{s}^{4}$, real-world variance $1.93 \mathrm{e}-4 \mathrm{~m}^{2} / \mathrm{s}^{4}$ ), the observed variance for an inflight vehicle was much higher than the simulated one (accel.: simulated $1.75 \mathrm{e}-3$ $\mathrm{m}^{2} / \mathrm{s}^{4}$ vs. real-world $9.46 \mathrm{~m}^{2} / \mathrm{s}^{4}$ ). This is likely in real-world the airframe vibrates when the motors are running and that phenomenon is not yet modeled in AirSim.</li>
<li>Barometer: We raised the sensor periodically between two fixed heights: ground level and then elevated to 178 cm (both in simulation and real-world). Figure 5(a) shows both the measurements (green is simulated, blue is real-world) and we ob-</li>
</ul>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5 Figure 5(a) and 5(b) show that barometer and the magnetometer characteristics in simulation closely resemble that of the real world.
serve that the signals have similar characteristics. Note that the offset between the simulated and the real-world pressure is due the difference in absolute pressure in the real-world and the one in the simulation. There is also a small increase in the middle due to a temperature increase, which wasn't simulated. Overall, the characteristics of the simulated sensor matches well to the real sensor.</p>
<ul>
<li>Magnetometer: We placed the vehicle on the ground and then rotated it by $90^{\circ}$ four times. Figure 5(b) shows the real-world and the simulated measurements and highlight that they are very similar in characteristic.</li>
</ul>
<h1>5 Conclusion and Future Work</h1>
<p>AirSim offers hi-fidelity physical and visual simulation that allows to generate large quantity of training data cheaply for building machine learning models. AirSim API design allows developing algorithms against simulator and then deploy them without change on real vehicles. The core components of AirSim including physics engine, vehicle models, environment models and sensor models are designed to be independently usable with minimal dependencies outside of AirSim and are easily extensible. AirSim is inspired by the goal of developing reinforcement learning algorithms for the autonomous agents that can operate in the real world.</p>
<p>The task of mimicking the real-world in real-time simulation is a challenging endeavor. There are a number of things that can be improved. Currently we do not simulate richer collision response or advanced ground interaction models which may be possible in future by implementing our physics engine interface for NVIDIA PhysX and utilizing features such as physics sub-stepping. Also we do not simulate various oddities in camera sensors except those directly available in Unreal engine. We plan to add advanced noise models and lens models in future. The degradation of GPS signal due to obstacles is not simulated yet which we plan to add using ray tracing methods. We also plan to add more advanced wind effects and thermal</p>
<p>simulations for fixed wing vehicles. Our extensibility APIs have been designed with above future work in mind and can also be used to realize other vehicle types.</p>
<h1>References</h1>
<ol>
<li>Babushkin, A.: Jmavsim. https://pixhawk.org/dev/hil/jmavsim</li>
<li>Bagnell, J.A.: An invitation to imitation. Tech. rep., CMU ROBOTICS INST (2015)</li>
<li>Braeunig, R.: Atmospheric models. http://www.braeunig.us/space/atmmodel. htm (2014)</li>
<li>Brandt, J., Deters, R., Ananda, G., Selig, M.: Uiuc propeller database, university of illinois at urbana-champaign. http://m-selig.ae.illinois.edu/props/propDB.html (2015)</li>
<li>Burch D., B.T.: Mariner's Pressure Atlas: Worldwide Mean Sea Level Pressures and Standard Deviations for Weather Analysis. Starpath School of Navigation (2014)</li>
<li>Chulliat, A., Macmillan, S., Alken, P., Beggan, C., Nair, M., Hamilton, B., Woods, A., Ridley, V., Maus, S., Thomson, A.: The us/uk world magnetic model for 2015-2020 (2015). DOI 10.7289/V5TB14V7</li>
<li>Furrer, F., Burri, M., Achtelik, M., Siegwart, R.: Rotorsa modular gazebo mav simulator framework. In: Robot Operating System (ROS), pp. 595-625. Springer (2016)</li>
<li>Hecker, C.: Physics, part 3: Collision response. Game Developer Magazine (1997)</li>
<li>Herman, R.: A first course in differential equations for scientists and engineers. http:// people.uncw.edu/hermanr/mat361/ODEBook/ (2017)</li>
<li>Jackson, J., Ellingson, G., McLain, T.: Rosflight: A lightweight, inexpensive mav research and development tool. In: ICUAS, pp. 758-762 (2016). DOI 10.1109/ICUAS.2016.7502584</li>
<li>Karis, B., Games, E.: Real shading in unreal engine 4. In: Proc. Physically Based Shading Theory Practice (2013)</li>
<li>Kober, J., Bagnell, J.A., Peters, J.: Reinforcement learning in robotics: A survey. Int. J. Rob. Res. 32(11), 1238-1274 (2013). DOI 10.1177/0278364913495721</li>
<li>Koenig, N., Howard, A.: Design and use paradigms for gazebo, an open-source multi-robot simulator. In: IROS (2004)</li>
<li>Lanza, R., Meloni, A.: The Earth's Magnetism: An Introduction for Geologists. Springer Science \&amp; Business Media (2006)</li>
<li>Levy, S.: Hackflight: Simple quadcopter flight control firmware and simulator for c++ hackers. https://github.com/simondlevy/hackflight</li>
<li>Meier, L., Tanskanen, P., Fraundorfer, F., Pollefeys, M.: Pixhawk: A system for autonomous flight using onboard computer vision. In: ICRA, pp. 2992-2997. IEEE (2011)</li>
<li>Meyer, J., Sendobry, A., Kohlbrecher, S., Klingauf, U., Von Stryk, O.: Comprehensive simulation of quadrotor uavs using ros and gazebo. In: SIMPAR, pp. 400-411. Springer (2012)</li>
<li>Moore, H.: Creating assets for the open world demo (2015)</li>
<li>Nakayama, Y., Boucher, R.: Introduction to fluid mechanics. Butterworth-Heinemann (1998)</li>
<li>Sabatini, A.M., Genovese, V.: A stochastic approach to noise modeling for barometric altimeters. Sensors (Basel, Switzerland) 13(11), 15,692-15,707 (2013)</li>
<li>Shah, S., Dey, D., Lovett, C., Kapoor, A.: Airsim open source platform at github. https: //github.com/Microsoft/AirSim (2017)</li>
<li>Stull, R.: Practical Meteorology: An Algebra-based Survey of Atmospheric Science. University of British Columbia (2015)</li>
<li>Tapley, B., Ries, J., Bettadpur, S., Chambers, D., Cheng, M., Condi, F., Poole, S.: The ggm03 mean earth gravity model from grace. In: American Geophysical Union, G42A-03 (2007)</li>
<li>Taylor, J.: Classical mechanics. University Science Books (2005)</li>
<li>Weiss, K., Khoshgoftaar, T.M., Wang, D.: A survey of transfer learning. Journal of Big Data 3(1), 9 (2016). DOI 10.1186/s40537-016-0043-6</li>
<li>Woodman, O.J.: An introduction to inertial navigation. Tech. Rep. UCAM-CL-TR-696, University of Cambridge, Computer Laboratory (2007)</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>1, 2, 3, 4, Microsoft Research, Redmond, WA, USA e-mail: shitals, dedey, clovett, akapoor@microsoft.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
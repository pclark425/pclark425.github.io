<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2028 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2028</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2028</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-49.html">extraction-schema-49</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-280641885</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.09586v1.pdf" target="_blank">EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, including programming, planning, and decision-making. However, their performance often degrades when faced with highly complex problem instances that require deep reasoning over long horizons. In such cases, direct problem-solving approaches can lead to inefficiency or failure due to the lack of structured intermediate guidance. To address this, we propose a novel self-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM constructs a sequence of problem instances with gradually increasing difficulty, tailored to the solver LLM's learning progress. The curriculum dynamically adapts easing challenges when the solver struggles and escalating them when success is consistent, thus maintaining an optimal learning trajectory. This approach enables the solver LLM, implemented as a code-generation model producing Python decision-tree scripts, to progressively acquire the skills needed for complex decision-making tasks. Experimental results on challenging decision-making benchmarks show that our method significantly improves task success rates and solution efficiency compared to direct-solving baselines. These findings suggest that LLM-driven curriculum learning holds strong potential for enhancing automated reasoning in real-world, high-complexity domains.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2028.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2028.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EvoCurr</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EvoCurr: Self-evolving Curriculum with Behavior Code Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven framework that jointly runs a curriculum-design LLM and a solver/coder LLM in a closed loop at inference time to generate progressively harder StarCraft II micro-management tasks and corresponding python-sc2 decision-tree scripts until the final task is solved.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated, performance-based adaptive curriculum at inference time</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>A curriculum-designer LLM (meta-learner) constructs a sequence of curricula {C0, C1, ...} describing unit compositions, map/terrain settings, objectives, and a scalar difficulty level. The designer uses historical rollout performance (win rate, decision accuracy, completion time) as feedback: if the solver meets a stage success threshold (67% win rate), the designer escalates difficulty (more units, abilities, complex enemy composition); if it fails repeatedly, the designer simplifies tasks (reduce unit counts, remove advanced tech). The curriculum designer builds environment prompts (unit/map attributes), formats feedback and history, and issues new curricula to the behavior coder in a closed-loop iterative algorithm (Algorithm 1 & 2).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>StarCraft II micro-management (python-sc2 scripted decision-tree policies)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Multi-agent, multi-ability tactical engagements with up to dozens of units per side and diverse unit types/abilities (e.g., Marines, Marauders, Ghosts, Medivacs, Siege Tanks vs Zealots, Stalkers, High Templars, Colossi), requiring combinatorial coordination skills (focus fire, retreat, healing coordination, special-ability timing, air-ground integration) and long-horizon multi-phase tactics.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Direct one-shot code generation baseline (attempts to generate decision-tree for the final task without curriculum progression)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics: win rate (per-task), success consistency (% tasks completed successfully), curriculum progression depth (number of tasks generated), additional metrics from rollouts include score and damage statistics. Key reported values: success threshold = 67% win rate; across 5 independent EvoCurr runs, 1/5 runs (20%) achieved complete task mastery; the successful path achieved consistent >=67% at each stage and 100% win rate on the final task in Table 2; intermediate per-task win rates reported in Table 2 (examples: Path 1 Task1 67%, Task2 67%, Task4 67%, Task5 67%, Final Task 100%).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>No quantitative learning-speed (episodes or wall-clock) comparison with baselines was reported; authors claim improved task success rates and solution efficiency qualitatively versus direct-solving baselines but provide no numeric convergence-time numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported quantitatively; authors claim improved transfer of strategies across successive curricula but do not provide held-out generalization metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Authors report diversity across five autonomous curriculum paths (different unit-introduction sequences, scaling rates, tactical focuses) and present curriculum progression depth and success consistency as measures, but no formal diversity metric was computed.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Yes — the curriculum designer explicitly generates early-stage tasks targeting sub-skills (e.g., focus fire, retreat maneuvers, healing) as prerequisites before introducing higher-level combined tactics; authors report this helped build a transferable foundation in the successful path.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Yes — EvoCurr generates intermediate/bridging tasks automatically; when a curriculum stage fails repeatedly, the designer produces simplified alternative tasks (reducing units/abilities) to recover progress. These intermediate tasks were effective in the successful path and in several partial successes (enabled progression in Path 1 and Path 5).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Observed limitations include LLM hallucination of non-existent or deprecated python-sc2 API calls causing runtime exceptions, tendency for single-agent architecture to bias toward certain unit types (uneven sophistication across units), failure modes from overly aggressive difficulty scaling by the designer, and content-length constraints limiting single-agent capability.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported quantitatively; paper does not provide resource, runtime, or LLM-call count measurements for curriculum generation or behavior-code synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Not reported; no human expert scoring of curricula or code quality described.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>EvoCurr — an LLM-driven, self-evolving curriculum+coder loop — can substantially improve the solver's ability to produce effective decision-tree scripts for high-complexity StarCraft II micro-management tasks by generating staged tasks and reusing evolved code templates. In experiments, EvoCurr produced diverse curriculum paths; however, only 1 of 5 autonomous runs achieved full final-task mastery (20% success), showing promise but also highlighting fragility from premature scaling, LLM coding errors, and single-agent limitations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2028.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2028.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curriculum Designer LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curriculum Designer (LLM module within EvoCurr)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialized LLM agent in EvoCurr that generates and adapts curricula at inference time by producing task specifications (unit configs, map, objectives, difficulty) and using solver performance feedback to scale difficulty up or down.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated, performance-threshold-driven difficulty adjustment</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Operates iteratively: extracts rollout metrics, builds environment prompts (unit/map info), formats feedback, retrieves historical curriculum performance, and composes the prompt for next curricula. Uses a numeric difficulty scalar and the 67% win-rate threshold to decide whether to increase complexity (add units/abilities, tougher enemy compositions) or simplify (reduce unit counts/remove advanced tech). It also generates simplified fallback curricula when the coder fails repeatedly within an iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>StarCraft II micro-management (task specification generation)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Generates tasks ranging from single-skill short engagements (e.g., 5 Marines vs 2 Zealots) to full final specification with 20 Marines + supporting specialized units vs mixed Protoss army, involving multi-skill coordination and many concurrent decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Implicit comparison to manually designed/static curricula (discussion) and direct one-shot baseline; no quantitative comparisons to other automated curriculum algorithms reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Tracks and conditions progression on per-stage win rate (>=67% to advance), rollout statistics (win rate, score, damage) used as input; experimental logging shows curriculum progression depth per run (varied across five paths) and success consistency, but no separate evaluation of curriculum designer accuracy beyond downstream solver performance.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not provided: no iteration counts/time-to-threshold comparisons against alternate curriculum designers.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported beyond claimed transfer of strategies across curriculum stages in successful path.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Paper qualitatively reports diverse curricula across five runs (different sequencing, scaling rates), but no formal metric quantifying diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Yes — explicitly creates early tasks to teach sub-skills (focus fire, retreat, healing) considered prerequisites for advanced tactics.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Yes — generates intermediate tasks and simplified alternatives when solver fails; these bridging tasks were used to recover progress in multiple paths.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Designer can scale difficulty too aggressively causing unsolvable stages; no explicit mitigation beyond fallback simplifications. Model identity and prompt/resource limits may also constrain performance (not quantified).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported separately for the designer module.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>The Curriculum Designer LLM successfully automates the generation of staged tasks and adapts difficulty using performance thresholds, enabling incremental skill acquisition; however, its aggressive scaling and absence of more sophisticated scheduling heuristics contributed to some failed runs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2028.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2028.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Behavior Coder (planner-coder-critic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Behavior Coder: planner-coder-critic loop for Python-sc2 decision-tree generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular LLM-based code synthesis loop that (1) plans high-level strategies, (2) generates python-sc2 decision-tree code implementing those plans, and (3) critiques runtime results to refine code and strategy iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>Not a curriculum generator itself; it synthesizes behavior code from curricula using LLM planning and codegen with critic feedback</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Planner: produces coarse high-level strategies and skill-trigger conditions, enriched by unit attributes (Liquipedia) and map summaries plus a memory of past strategies. Coder: translates strategy to python-sc2 decision-tree code, compiles and runs simulations. Critic: diagnoses runtime exceptions (often API/deprecation errors), analyzes successful rollouts to identify strengths/weaknesses, and feeds back adjustments to planner/coder. This loop repeats up to a max attempt count per curriculum iteration (Algorithm 3).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>StarCraft II (python-sc2), code generation of interpretable decision-tree policies</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Must encode conditional, hierarchical tactics (multiple branches) and handle large state-action spaces induced by heterogeneous unit fleets; complexity grows as curricula add unit types and advanced abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared as part of full EvoCurr vs direct one-shot code generation baseline; no standalone coder-only baselines quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Code validity (compilation/runtime exception vs valid), per-task win rates after code rollout, and additional rollout stats (score, damage). The paper reports that runtime exceptions cause immediate zero win-rate for that attempt.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not reported numerically; authors note multiple coding attempts per stage and that repeated failures trigger curriculum simplification.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported separately; code templates from successful curricula are reused to bootstrap next stages.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not applicable for the coder module beyond its ability to generate a variety of scripts matching curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Coder relies on planner-specified sub-skills and in-context memory of strategies to implement prerequisites.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Coder produces code for intermediate curricula generated by the designer; effectiveness measured by whether generated code reaches >=67% win rate.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Hallucinated or deprecated API usage caused runtime exceptions; code quality depends strongly on clarity of planner strategy and may bias toward some unit types; the coder sometimes fails repeatedly necessitating curriculum fallback.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported (no LLM-call counts, compile/run times, or compute budgets provided).</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>The planner-coder-critic loop enables iterative improvement of executable decision-tree policies, catching API/runtime issues via critique and enabling code refinement, but is sensitive to hallucinated API calls and content-length constraints that limit single-agent code quality.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2028.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2028.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Direct Generation Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct one-shot decision-tree code generation baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that attempts to produce a python-sc2 decision-tree script for the final (full-complexity) StarCraft II task in a single generation pass without curriculum progression or iterative curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>N/A (no curriculum; direct one-shot problem solving)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Baseline generates code for the final task directly (one-shot) without staged intermediate tasks or closed-loop curriculum adaptation. Used to compare whether EvoCurr's staged approach improves success and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>StarCraft II micro-management (final task specification from Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Full final task with many unit types and abilities (see Table 1: 20 Marines, 12 Marauders, Ghosts, Medivacs, Siege Tanks, Vikings, Liberators vs Protoss mixed army including Colossi and Psi Storm-capable High Templars), requiring integrated multi-skill coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared directly against EvoCurr in experiments; authors state EvoCurr 'significantly improves task success rates and solution efficiency' relative to this baseline, but per-experiment numeric baseline values are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper does not report explicit numeric win rates or other metrics for the direct one-shot baseline in the main experimental results section; comparison is qualitative.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not reported numerically; authors claim EvoCurr leads to faster/more efficient solving qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not applicable; baseline does not identify or use prerequisites.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Authors note that direct one-shot solving degrades on high-complexity instances due to long-horizon reasoning, compounding errors, and incomplete/inconsistent solutions from the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>The direct one-shot code generation baseline was less effective on the hardest tasks compared to the EvoCurr self-evolving curriculum approach; specific numeric baseline performance values were not reported, but the paper claims EvoCurr yields higher success rates and solution efficiency.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2028.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2028.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EnvGen (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EnvGen (LLM-based environment generation, cited in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned prior work that uses LLMs to adaptively create training environments for RL agents via API-based frameworks, demonstrating low computational overhead with few LLM calls.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated environment/task generation (as reported in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Cited as an example of the paradigm where LLMs programmatically generate structured environments through APIs to produce training curricula/environments for RL agents; specifics are referenced but not reproduced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Environment generation for embodied/RL agents (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Cited as computationally efficient in related work (few LLM calls) but no numeric costs given here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Cited in related work as an example of LLM-driven environment generation approaches that inspire EvoCurr's use of LLMs for curriculum generation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>EnvGen <em>(Rating: 2)</em></li>
                <li>GenSim <em>(Rating: 2)</em></li>
                <li>LLM-SMAC <em>(Rating: 2)</em></li>
                <li>Automated curriculum learning for neural networks <em>(Rating: 2)</em></li>
                <li>Curriculum learning <em>(Rating: 1)</em></li>
                <li>Automated curriculum learning for deep RL: A short survey <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2028",
    "paper_id": "paper-280641885",
    "extraction_schema_id": "extraction-schema-49",
    "extracted_data": [
        {
            "name_short": "EvoCurr",
            "name_full": "EvoCurr: Self-evolving Curriculum with Behavior Code Generation",
            "brief_description": "An LLM-driven framework that jointly runs a curriculum-design LLM and a solver/coder LLM in a closed loop at inference time to generate progressively harder StarCraft II micro-management tasks and corresponding python-sc2 decision-tree scripts until the final task is solved.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "LLM-generated, performance-based adaptive curriculum at inference time",
            "curriculum_method_description": "A curriculum-designer LLM (meta-learner) constructs a sequence of curricula {C0, C1, ...} describing unit compositions, map/terrain settings, objectives, and a scalar difficulty level. The designer uses historical rollout performance (win rate, decision accuracy, completion time) as feedback: if the solver meets a stage success threshold (67% win rate), the designer escalates difficulty (more units, abilities, complex enemy composition); if it fails repeatedly, the designer simplifies tasks (reduce unit counts, remove advanced tech). The curriculum designer builds environment prompts (unit/map attributes), formats feedback and history, and issues new curricula to the behavior coder in a closed-loop iterative algorithm (Algorithm 1 & 2).",
            "llm_model_used": null,
            "domain_environment": "StarCraft II micro-management (python-sc2 scripted decision-tree policies)",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Multi-agent, multi-ability tactical engagements with up to dozens of units per side and diverse unit types/abilities (e.g., Marines, Marauders, Ghosts, Medivacs, Siege Tanks vs Zealots, Stalkers, High Templars, Colossi), requiring combinatorial coordination skills (focus fire, retreat, healing coordination, special-ability timing, air-ground integration) and long-horizon multi-phase tactics.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Direct one-shot code generation baseline (attempts to generate decision-tree for the final task without curriculum progression)",
            "performance_metrics": "Reported metrics: win rate (per-task), success consistency (% tasks completed successfully), curriculum progression depth (number of tasks generated), additional metrics from rollouts include score and damage statistics. Key reported values: success threshold = 67% win rate; across 5 independent EvoCurr runs, 1/5 runs (20%) achieved complete task mastery; the successful path achieved consistent &gt;=67% at each stage and 100% win rate on the final task in Table 2; intermediate per-task win rates reported in Table 2 (examples: Path 1 Task1 67%, Task2 67%, Task4 67%, Task5 67%, Final Task 100%).",
            "learning_speed_comparison": "No quantitative learning-speed (episodes or wall-clock) comparison with baselines was reported; authors claim improved task success rates and solution efficiency qualitatively versus direct-solving baselines but provide no numeric convergence-time numbers.",
            "generalization_performance": "Not reported quantitatively; authors claim improved transfer of strategies across successive curricula but do not provide held-out generalization metrics.",
            "task_diversity_analysis": "Authors report diversity across five autonomous curriculum paths (different unit-introduction sequences, scaling rates, tactical focuses) and present curriculum progression depth and success consistency as measures, but no formal diversity metric was computed.",
            "prerequisite_identification": "Yes — the curriculum designer explicitly generates early-stage tasks targeting sub-skills (e.g., focus fire, retreat maneuvers, healing) as prerequisites before introducing higher-level combined tactics; authors report this helped build a transferable foundation in the successful path.",
            "intermediate_task_generation": "Yes — EvoCurr generates intermediate/bridging tasks automatically; when a curriculum stage fails repeatedly, the designer produces simplified alternative tasks (reducing units/abilities) to recover progress. These intermediate tasks were effective in the successful path and in several partial successes (enabled progression in Path 1 and Path 5).",
            "llm_limitations_observed": "Observed limitations include LLM hallucination of non-existent or deprecated python-sc2 API calls causing runtime exceptions, tendency for single-agent architecture to bias toward certain unit types (uneven sophistication across units), failure modes from overly aggressive difficulty scaling by the designer, and content-length constraints limiting single-agent capability.",
            "computational_cost": "Not reported quantitatively; paper does not provide resource, runtime, or LLM-call count measurements for curriculum generation or behavior-code synthesis.",
            "human_expert_evaluation": "Not reported; no human expert scoring of curricula or code quality described.",
            "key_findings_summary": "EvoCurr — an LLM-driven, self-evolving curriculum+coder loop — can substantially improve the solver's ability to produce effective decision-tree scripts for high-complexity StarCraft II micro-management tasks by generating staged tasks and reusing evolved code templates. In experiments, EvoCurr produced diverse curriculum paths; however, only 1 of 5 autonomous runs achieved full final-task mastery (20% success), showing promise but also highlighting fragility from premature scaling, LLM coding errors, and single-agent limitations.",
            "uuid": "e2028.0"
        },
        {
            "name_short": "Curriculum Designer LLM",
            "name_full": "Curriculum Designer (LLM module within EvoCurr)",
            "brief_description": "A specialized LLM agent in EvoCurr that generates and adapts curricula at inference time by producing task specifications (unit configs, map, objectives, difficulty) and using solver performance feedback to scale difficulty up or down.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "LLM-generated, performance-threshold-driven difficulty adjustment",
            "curriculum_method_description": "Operates iteratively: extracts rollout metrics, builds environment prompts (unit/map info), formats feedback, retrieves historical curriculum performance, and composes the prompt for next curricula. Uses a numeric difficulty scalar and the 67% win-rate threshold to decide whether to increase complexity (add units/abilities, tougher enemy compositions) or simplify (reduce unit counts/remove advanced tech). It also generates simplified fallback curricula when the coder fails repeatedly within an iteration.",
            "llm_model_used": null,
            "domain_environment": "StarCraft II micro-management (task specification generation)",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Generates tasks ranging from single-skill short engagements (e.g., 5 Marines vs 2 Zealots) to full final specification with 20 Marines + supporting specialized units vs mixed Protoss army, involving multi-skill coordination and many concurrent decisions.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Implicit comparison to manually designed/static curricula (discussion) and direct one-shot baseline; no quantitative comparisons to other automated curriculum algorithms reported.",
            "performance_metrics": "Tracks and conditions progression on per-stage win rate (&gt;=67% to advance), rollout statistics (win rate, score, damage) used as input; experimental logging shows curriculum progression depth per run (varied across five paths) and success consistency, but no separate evaluation of curriculum designer accuracy beyond downstream solver performance.",
            "learning_speed_comparison": "Not provided: no iteration counts/time-to-threshold comparisons against alternate curriculum designers.",
            "generalization_performance": "Not reported beyond claimed transfer of strategies across curriculum stages in successful path.",
            "task_diversity_analysis": "Paper qualitatively reports diverse curricula across five runs (different sequencing, scaling rates), but no formal metric quantifying diversity.",
            "prerequisite_identification": "Yes — explicitly creates early tasks to teach sub-skills (focus fire, retreat, healing) considered prerequisites for advanced tactics.",
            "intermediate_task_generation": "Yes — generates intermediate tasks and simplified alternatives when solver fails; these bridging tasks were used to recover progress in multiple paths.",
            "llm_limitations_observed": "Designer can scale difficulty too aggressively causing unsolvable stages; no explicit mitigation beyond fallback simplifications. Model identity and prompt/resource limits may also constrain performance (not quantified).",
            "computational_cost": "Not reported separately for the designer module.",
            "human_expert_evaluation": "Not reported.",
            "key_findings_summary": "The Curriculum Designer LLM successfully automates the generation of staged tasks and adapts difficulty using performance thresholds, enabling incremental skill acquisition; however, its aggressive scaling and absence of more sophisticated scheduling heuristics contributed to some failed runs.",
            "uuid": "e2028.1"
        },
        {
            "name_short": "Behavior Coder (planner-coder-critic)",
            "name_full": "Behavior Coder: planner-coder-critic loop for Python-sc2 decision-tree generation",
            "brief_description": "A modular LLM-based code synthesis loop that (1) plans high-level strategies, (2) generates python-sc2 decision-tree code implementing those plans, and (3) critiques runtime results to refine code and strategy iteratively.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "Not a curriculum generator itself; it synthesizes behavior code from curricula using LLM planning and codegen with critic feedback",
            "curriculum_method_description": "Planner: produces coarse high-level strategies and skill-trigger conditions, enriched by unit attributes (Liquipedia) and map summaries plus a memory of past strategies. Coder: translates strategy to python-sc2 decision-tree code, compiles and runs simulations. Critic: diagnoses runtime exceptions (often API/deprecation errors), analyzes successful rollouts to identify strengths/weaknesses, and feeds back adjustments to planner/coder. This loop repeats up to a max attempt count per curriculum iteration (Algorithm 3).",
            "llm_model_used": null,
            "domain_environment": "StarCraft II (python-sc2), code generation of interpretable decision-tree policies",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Must encode conditional, hierarchical tactics (multiple branches) and handle large state-action spaces induced by heterogeneous unit fleets; complexity grows as curricula add unit types and advanced abilities.",
            "is_curriculum_adaptive": null,
            "baseline_comparisons": "Compared as part of full EvoCurr vs direct one-shot code generation baseline; no standalone coder-only baselines quantified.",
            "performance_metrics": "Code validity (compilation/runtime exception vs valid), per-task win rates after code rollout, and additional rollout stats (score, damage). The paper reports that runtime exceptions cause immediate zero win-rate for that attempt.",
            "learning_speed_comparison": "Not reported numerically; authors note multiple coding attempts per stage and that repeated failures trigger curriculum simplification.",
            "generalization_performance": "Not reported separately; code templates from successful curricula are reused to bootstrap next stages.",
            "task_diversity_analysis": "Not applicable for the coder module beyond its ability to generate a variety of scripts matching curricula.",
            "prerequisite_identification": "Coder relies on planner-specified sub-skills and in-context memory of strategies to implement prerequisites.",
            "intermediate_task_generation": "Coder produces code for intermediate curricula generated by the designer; effectiveness measured by whether generated code reaches &gt;=67% win rate.",
            "llm_limitations_observed": "Hallucinated or deprecated API usage caused runtime exceptions; code quality depends strongly on clarity of planner strategy and may bias toward some unit types; the coder sometimes fails repeatedly necessitating curriculum fallback.",
            "computational_cost": "Not reported (no LLM-call counts, compile/run times, or compute budgets provided).",
            "human_expert_evaluation": "Not reported.",
            "key_findings_summary": "The planner-coder-critic loop enables iterative improvement of executable decision-tree policies, catching API/runtime issues via critique and enabling code refinement, but is sensitive to hallucinated API calls and content-length constraints that limit single-agent code quality.",
            "uuid": "e2028.2"
        },
        {
            "name_short": "Direct Generation Baseline",
            "name_full": "Direct one-shot decision-tree code generation baseline",
            "brief_description": "A baseline that attempts to produce a python-sc2 decision-tree script for the final (full-complexity) StarCraft II task in a single generation pass without curriculum progression or iterative curricula.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "N/A (no curriculum; direct one-shot problem solving)",
            "curriculum_method_description": "Baseline generates code for the final task directly (one-shot) without staged intermediate tasks or closed-loop curriculum adaptation. Used to compare whether EvoCurr's staged approach improves success and efficiency.",
            "llm_model_used": null,
            "domain_environment": "StarCraft II micro-management (final task specification from Table 1)",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Full final task with many unit types and abilities (see Table 1: 20 Marines, 12 Marauders, Ghosts, Medivacs, Siege Tanks, Vikings, Liberators vs Protoss mixed army including Colossi and Psi Storm-capable High Templars), requiring integrated multi-skill coordination.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Compared directly against EvoCurr in experiments; authors state EvoCurr 'significantly improves task success rates and solution efficiency' relative to this baseline, but per-experiment numeric baseline values are not provided.",
            "performance_metrics": "Paper does not report explicit numeric win rates or other metrics for the direct one-shot baseline in the main experimental results section; comparison is qualitative.",
            "learning_speed_comparison": "Not reported numerically; authors claim EvoCurr leads to faster/more efficient solving qualitatively.",
            "generalization_performance": "Not reported.",
            "task_diversity_analysis": "Not applicable.",
            "prerequisite_identification": "Not applicable; baseline does not identify or use prerequisites.",
            "intermediate_task_generation": "No.",
            "llm_limitations_observed": "Authors note that direct one-shot solving degrades on high-complexity instances due to long-horizon reasoning, compounding errors, and incomplete/inconsistent solutions from the LLM.",
            "computational_cost": "Not reported.",
            "human_expert_evaluation": "Not reported.",
            "key_findings_summary": "The direct one-shot code generation baseline was less effective on the hardest tasks compared to the EvoCurr self-evolving curriculum approach; specific numeric baseline performance values were not reported, but the paper claims EvoCurr yields higher success rates and solution efficiency.",
            "uuid": "e2028.3"
        },
        {
            "name_short": "EnvGen (related work)",
            "name_full": "EnvGen (LLM-based environment generation, cited in related work)",
            "brief_description": "Mentioned prior work that uses LLMs to adaptively create training environments for RL agents via API-based frameworks, demonstrating low computational overhead with few LLM calls.",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generation_method": "LLM-generated environment/task generation (as reported in related work)",
            "curriculum_method_description": "Cited as an example of the paradigm where LLMs programmatically generate structured environments through APIs to produce training curricula/environments for RL agents; specifics are referenced but not reproduced in this paper.",
            "llm_model_used": null,
            "domain_environment": "Environment generation for embodied/RL agents (related work)",
            "is_interactive_text_environment": null,
            "is_compositional": null,
            "task_complexity_description": "Not detailed in this paper.",
            "is_curriculum_adaptive": null,
            "baseline_comparisons": "",
            "performance_metrics": "",
            "learning_speed_comparison": "",
            "generalization_performance": "",
            "task_diversity_analysis": "",
            "prerequisite_identification": "",
            "intermediate_task_generation": "",
            "llm_limitations_observed": "",
            "computational_cost": "Cited as computationally efficient in related work (few LLM calls) but no numeric costs given here.",
            "human_expert_evaluation": "",
            "key_findings_summary": "Cited in related work as an example of LLM-driven environment generation approaches that inspire EvoCurr's use of LLMs for curriculum generation.",
            "uuid": "e2028.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "EnvGen",
            "rating": 2
        },
        {
            "paper_title": "GenSim",
            "rating": 2
        },
        {
            "paper_title": "LLM-SMAC",
            "rating": 2
        },
        {
            "paper_title": "Automated curriculum learning for neural networks",
            "rating": 2
        },
        {
            "paper_title": "Curriculum learning",
            "rating": 1
        },
        {
            "paper_title": "Automated curriculum learning for deep RL: A short survey",
            "rating": 1
        }
    ],
    "cost": 0.013554749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making
20 Aug 2025</p>
<p>Yang Cheng 
University of Science and Technology of China</p>
<p>Zilai Wang 
Weiyu Ma 
King Abdullah University of Science and Technology 5 Xi'an Jiaotong University</p>
<p>Wenhui Zhu 
Zhongguancun Institute of Artificial Intelligence</p>
<p>Yue Deng ⟨dengyue@zgci.ac.cn⟩ 
Zhongguancun Institute of Artificial Intelligence</p>
<p>Jian Zhao ⟨jianzhao@zgci.ac.cn⟩ 
Zhongguancun Institute of Artificial Intelligence</p>
<p>Zhongguancun Academy 
Cheng 
EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making
20 Aug 202509CF259AF73829EAC5766693D3F05976arXiv:2508.09586v2[cs.AI]LLM AgentsComplex TaskBehavior CodeSelf-evolve
Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, including programming, planning, and decision-making.However, their performance often degrades when faced with highly complex problem instances that require deep reasoning over long horizons.In such cases, direct problemsolving approaches can lead to inefficiency or failure due to the lack of structured intermediate guidance.To address this, we propose a novel self-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM constructs a sequence of problem instances with gradually increasing difficulty, tailored to the solver LLM's learning progress.The curriculum dynamically adapts easing challenges when the solver struggles and escalating them when success is consistent, thus maintaining an optimal learning trajectory.This approach enables the solver LLM, implemented as a codegeneration model producing Python decision-tree scripts, to progressively acquire the skills needed for complex decision-making tasks.Experimental results on challenging decision-making benchmarks show that our method significantly improves task success rates and solution efficiency compared to direct-solving baselines.These findings suggest that LLM-driven curriculum learning holds strong potential for enhancing automated reasoning in real-world, high-complexity domains.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have achieved significant progress in natural language processing, code generation, and reasoning [3,5,28].Beyond their use in text-based applications, these models are increasingly capable of producing structured outputs such as executable programs in Python, enabling them to handle complex computational reasoning tasks.This capability opens opportunities for structured and interpretable solutions in decision-making domains, where explicitly defined logic can be verified and integrated into larger automated systems [7,19].Decision-tree representations are particularly appealing in such contexts because they not only capture reasoning steps explicitly but also facilitate debugging, validation, and integration into operational pipelines.Despite these strengths, solving highly complex decision-making problems in a single inference step remains challenging.Such tasks often involve extensive search spaces, intricate dependencies between variables, and multi-step reasoning chains that increase the likelihood of compounding errors [42].LLMs, even when guided by prompts or examples, can produce incomplete or logically inconsistent solutions when the problem complexity exceeds their implicit reasoning capacity.In human learning, similar challenges are addressed by introducing complexity gradually: learners first encounter simplified examples that teach fundamental concepts, and only later face more demanding scenarios.This gradual exposure is central to the principle of curriculum learning [2], which has proven effective in enabling both humans and machines to master complex skills over time.</p>
<p>Curriculum learning organizes tasks along a progression from easy to hard, allowing models to develop a solid foundation before attempting the most complex cases.In machine learning, this principle has been successfully applied in supervised learning, reinforcement learning, and program synthesis [13,27].For example, in reinforcement learning, agents may first be trained on simplified environments with fewer states or reduced stochasticity before tackling the full problem setting.Similarly, in program synthesis, models can be introduced to simpler specifications and incrementally exposed to richer constraints.Such staged exposure reduces the cognitive and computational load on the model, leading to faster convergence, better stability during learning, and improved generalization to unseen cases.</p>
<p>Although curriculum learning is conceptually powerful, most implementations face practical limitations.A significant drawback is the reliance on human experts to manually design task sequences, which is resource-intensive and often specific to a single domain.Once designed, these curricula are typically static, meaning they cannot adapt to the learner's evolving strengths or weaknesses during the solving process [30].This inflexibility can result in inefficient learning if the sequence advances too slowly or a loss of progress if the difficulty increases too quickly.Moreover, most curriculum learning strategies are intended for the training phase of a model's lifecycle, rather than for real-time adaptation during inference.In scenarios involving LLMs for zero-shot or few-shot problem solving, retraining the model with a curriculum is impractical, leaving a gap in methods that can adaptively guide reasoning at inference time.</p>
<p>To address these challenges, as described in Figure 1, we propose EvoCurr framework for adaptive curriculum learning at inference.Our approach employs two specialized roles: a solver LLM, which generates Python decision-tree scripts to solve the target decisionmaking task, and a curriculum LLM, which dynamically generates problem instances of varying difficulty.The curriculum LLM begins by presenting simplified scenarios that the solver can reliably handle, then uses the solver's performance as feedback to adjust the complexity of subsequent tasks.If the solver achieves success on current tasks, the curriculum LLM increases the difficulty; if the solver fails, the difficulty is reduced.This closed-loop interaction enables the solver to incrementally acquire the reasoning structures necessary to handle the full complexity of the target problem, without requiring manual curriculum design or retraining.</p>
<p>The contributions of this work are threefold.First, we introduce a novel EvoCurr framework that unifies problem solving and curriculum generation in a single adaptive loop, enabling inference-time guidance without the need for human-designed curricula.Second, we develop a performance-based difficulty adjustment mechanism that tailors the learning trajectory to the solver's evolving capabilities, eliminating inefficiencies from static task sequences.Third, we demonstrate through empirical evaluation that our framework significantly improves the success rate of solving complex decisionmaking problems compared to direct, one-shot problem solving.While our work focuses on decision-tree-based reasoning tasks, the proposed method is general and could be applied to other structured reasoning and program synthesis domains where complexity must be managed dynamically in the future.</p>
<p>Related Work</p>
<p>StarCraft II AI: The release of PySC2 [35] by DeepMind, coupled with Blizzard's game replays, catalyzed significant research in real-time strategy AI.A pivotal breakthrough was AlphaStar [34], which achieved Grandmaster level performance and defeated professional players, demonstrating the transformative potential of reinforcement learning in complex strategic environments.Subsequent research has expanded upon these foundations through diverse approaches.Mini-AlphaStar [23] demonstrated that simplified input representations could maintain learning effectiveness while reducing computational overhead.Efficiency-focused methods like TStarBot [21] and HierNet-SC2 [22] explored streamlined RL strategies, with the latter notably bypassing the need for supervised pretraining.AlphaStar Unplugged [26] pioneered offline RL approaches using human replay data, while federated learning frameworks like TStarBotsX [15] and SCC [38] achieved remarkable success against master and grandmaster level opponents.Recent developments have focused on accessibility and enhanced training methodologies.DI-star1 democratized access by enabling deployment on consumer hardware, while ROA-Star [18] advanced AlphaStar's framework through goal-conditioned exploiters and refined opponent modeling, achieving impressive results in tests against professional players.The integration of natural language understanding marks a significant evolution in this field.Pioneering work on grounding natural language commands to game states [40] established foundations for language-guided gameplay.TextStarCraft II [25] introduced specialized environments for evaluating large language models in real-time strategic scenarios, while AVA [24] and LLMPYSC2 [20] developed multi-modal StarCraft II environments.More recent advances include LLM-SMAC [10], which extends decision-making to behavior tree generation, and SMAC-Hard [11], which leverages behavior tree code as opponent policies across training modes.</p>
<p>Environment Generation: Environment generation for training embodied agents has emerged as a critical research area, with approaches falling into two distinct paradigms.The first paradigm leverages large language models' programming and reasoning capabilities to generate structured environments through API-based frameworks.EnvGen [41] exemplifies this approach by using LLMs to adaptively create training environments for RL agents with minimal computational overhead, requiring only a few LLM calls.The GenSim series [17,37] further demonstrates this paradigm's potential by exploiting GPT-4's grounding and coding abilities to automatically generate diverse robotic simulation tasks and expert demonstrations at scale.The second paradigm employs end-to-end generative models to synthesize interactive environments directly from visual data, circumventing traditional physics engines entirely.GameNGen [16] pioneered this approach using diffusion models to create real-time playable environments at 20 fps, while the Genie series [1,4,29] has progressively advanced toward real-time interaction capabilities, culminating in Genie 3's breakthrough achievements.Complementary works include GameGen-X [6] for openworld game video generation, Oasis [9] as a comprehensive realtime world model, and MineWorld [14] for interactive Minecraft world simulation.While the first paradigm excels in computational efficiency and precise environmental control, the second paradigm achieves superior visual fidelity and natural interaction dynamics, collectively advancing toward general-purpose world models for agent training.</p>
<p>Self-Evolving Agents: The field of self-evolving agents represents a fundamental shift from static language models toward adaptive systems capable of continuous improvement through experiential learning.This emerging paradigm can be systematically understood through three key dimensions: evolution targets, temporal dynamics, and methodological approaches.The evolution targets encompass four core components that define what agents can improve.Model evolution focuses on policy refinement through frameworks like SCA [47] and RAGEN [39], while context evolution develops sophisticated memory mechanisms (Mem0 [8]) and prompt optimization strategies (PromptBreeder [12]).Tool evolution enables autonomous capability expansion, as demonstrated by Voyager [36], and architectural evolution optimizes agent designs through systems like AgentSquare [32] and AFlow [46].The temporal dimension distinguishes between intra-episode adaptation, where agents modify their behavior in real-time (exemplified by AdaPlanner [33]), and inter-episode evolution, where agents learn retrospectively from accumulated experience through methods like STaR [43] and WebRL [31].Methodologically, three primary paradigms drive agent evolution: reward-based approaches utilizing textual feedback or external signals, imitation learning through self-generated demonstrations, and population-based methods employing evolutionary algorithms.These complementary strategies [44,45] collectively establish a comprehensive framework for developing autonomous agents that continuously adapt and evolve, marking significant progress toward artificial general intelligence.</p>
<p>While existing researches have explored StarCraft II AI through reinforcement learning and LLM-based approaches, alongside advancements in environment generation and self-evolving agent architectures, current methods typically rely on manually designed curricula or fixed behavioral patterns that lack autonomous complexity progression.However, there remains a notable gap in frameworks that can autonomously evolve decision-making capabilities through self-designed curricula while simultaneously generating executable behavior code.In this work, we address this limitation by proposing EvoCurr, a novel approach for LLM-driven autonomous evolution of decision-making agents, using StarCraft II as a complex validation scenario to demonstrate the effectiveness of our iterative curriculum design and behavior code synthesis framework.</p>
<p>Method</p>
<p>This section presents EvoCurr, our framework for addressing complex decision-making tasks through the iterative generation of behavior code guided by automatically designed curricula.By leveraging the planning, coding, and analytical capabilities of large language models (LLMs), our proposed EvoCurr successfully solves complex StarCraft II micro-management tasks with large number of agents, terrains, and unit abilities.Figure 2 illustrates the overall architecture, which is composed of specialized LLM modules working in concert to generate, evaluate, and refine strategies until the final objective is achieved.</p>
<p>Problem Formulation and Architecture</p>
<p>Pretrained LLMs have acquired substantial prior knowledge of Star-Craft II gameplay and Python code patterns for the python-sc2 API.Building on this foundation, EvoCurr targets the automated generation of decision-tree scripts using python-sc2, a comprehensive wrapper for StarCraft II control.However, due to frequent changes in the official API and the inherent tendency of LLMs to hallucinate non-existent functions, directly producing correct and complete scripts for complex scenarios remains challenging.Moreover, the extensive workload, including curriculum design, strategic planning, decision-tree coding, bug identification, and refinement, exceeds the practical capacity of a single LLM instance.EvoCurr addresses these challenges by employing a multi-agent LLM architecture, where specialized agents collaboratively manage different stages of the process.</p>
<p>Our EvoCurr framework operates across three distinct but interconnected stages, each addressing a different computational problem.In the first stage, the curriculum design process generates a sequence of training tasks { 0 ,  1 , . . .,   } that progressively increase in complexity toward the final target task   .Each curriculum   specifies the unit configurations   = { 1 ,  2 , . . .,   } with their corresponding types and abilities, environmental settings including map   and terrain features, task objectives   that define win conditions and evaluation metrics, and a difficulty level   ∈ R indicating the overall task complexity.The curriculum designer operates as a meta-learning system that adapts the task distribution based on the agent's current capabilities, ensuring optimal challenge progression without overwhelming the learning process.</p>
<p>The second stage addresses the code synthesis problem, where the behavior coder transforms each curriculum   into executable python-sc2 code Code  that implements a decision tree   .This transformation involves a structured three-phase process: strategy planning generates a high-level approach   ← Plan(  ,   −1 ,   −1 ) based on the current curriculum, previous behavior tree, and accumulated feedback; code synthesis translates the strategy into concrete implementation Code  ← Generate(  ,   −1 ) using domainspecific knowledge of the python-sc2 API; and compilation produces the final executable behavior tree   ← Compile(Code  ) that can interact with the StarCraft II environment.This stage bridges the gap between abstract strategic reasoning and concrete executable actions as described in Algorithm 3.</p>
<p>The third stage models the actual game interaction as a Markov Decision Process ⟨S, A, O, T , R⟩, where the compiled behavior tree   serves as the decision-making policy.The state space S encompasses all relevant game information including unit positions, health values, available resources, and terrain configurations.The action space A consists of all possible commands that can be issued to units, including movement, attack, and special ability activations.The observation space O represents the subset of game state information accessible to the behavior tree, accounting for the partial observability inherent in real-time strategy games.The transition function T : S × A → S captures the game mechanics and physics, while the reward function R : S × A → R provides task-specific feedback aligned with the curriculum objectives.</p>
<p>When the decision tree attains strong performance, the results are fed back to the curriculum designer.The designer then creates the next curriculum iteration, potentially increasing difficulty by altering agent counts, abilities, or other environment settings.The evolved decision tree is preserved as a template for subsequent iterations, ensuring knowledge retention.Through repeated cycles, the decision tree progresses from a simple baseline to a sophisticated decision-making agent capable of solving the final task.</p>
<p>Algorithm Notation:   : Final target task;   : Curriculum at iteration ;   : Behavior tree at iteration ; Code  : Generated python-sc2 code;   : Performance results {  ,   } where   is win rate and   represents additional metrics;   : High-level strategy;   : Feedback from previous iterations; L  , L  , L  , L  : LLM agents for curriculum design, planning, coding, and critique; ,   : Performance thresholds for curriculum progression and task success.</p>
<p>EvoCurr Algorithm Framework</p>
<p>Building on the three-stage formulation, we present the complete EvoCurr algorithm framework.Algorithm 1 describes the main iterative process that coordinates curriculum design and behavior coding until the final task is achieved.The framework begins with a simplified version of the target task and gradually increases complexity through adaptive curriculum progression.</p>
<p>Algorithm 2 details the curriculum design process, which adaptively adjusts task difficulty based on the agent's current performance.The curriculum designer leverages historical performance data to make informed decisions about complexity progression, ensuring that each new curriculum provides an appropriate level of challenge.</p>
<p>Algorithm 1 EvoCurr Framework</p>
<p>Input: Final task   , initial decision tree  0 , performance threshold  , max iterations  Output: Evolved decision tree    ′ ← L  (, "increase") 8: else 9:</p>
<p>′ ← L  (, "adjust") 10: end if 11:  ′ ← Validate( ′ ,  ) 12: return  ′ Algorithm 3 implements the behavior coder with the plannercoder-critic loop.This algorithm transforms high-level curricula into executable decision trees through iterative refinement, incorporating feedback from both successful runs and failure analysis.</p>
<p>Curriculum Design</p>
<p>The curriculum design process incrementally builds the decision tree's capabilities by introducing tasks in an adaptive progression.At the outset, the curriculum designer LLM generates simplified scenarios targeting essential sub-skills required for the final objective, such as focused fire, retreat maneuvers, and healing.These early-stage tasks deliberately limit scope to reduce decision complexity, allowing the agent to establish a robust foundation before tackling more demanding challenges.Each scenario specifies initial unit configurations, constraints, objectives, and evaluation metrics.</p>
<p>The curriculum designer operates iteratively, with each round of scenario generation informed by the performance outcomes of the decision tree in previous tasks.Rollout results, including win rates, decision accuracy, and completion time, are collected after each evaluation.If performance exceeds predefined thresholds, the curriculum increases in complexity by introducing additional units, This feedback-driven adaptation ensures that the curriculum evolves in sync with the agent's learning progress, maintaining an optimal challenge level.By avoiding both stagnation from overly simple tasks and failure from premature complexity, the mechanism fosters a gradual yet resilient expansion of decision-making capability.Over successive iterations, the agent develops strategies that are transferable to increasingly complex scenarios without requiring direct end-to-end optimization.</p>
<p>Behavior Coder</p>
<p>To implement the strategies required for each curriculum stage, EvoCurr adopts a planner-coder-critic structure inspired by LLM-SMAC [10].This modular loop ensures that strategic intent, code generation, and performance analysis are tightly integrated.</p>
<p>Planner.The planner generates high-level strategies and describes the skills needed to execute them.Because the control process is open-loop, these strategies are initially coarse-grained and lack map-specific or unit-specific details.To refine them, EvoCurr enriches the environment prompt with automatically retrieved unit attributes from Liquipedia and with summarized map properties such as size, traversable areas, and terrain features.The planner also exploits in-context learning by referencing a memory of historical strategies and their associated win rates, enabling informed reuse, modification, or augmentation of past approaches when facing new curricula.</p>
<p>Coder.The coder receives the planner's strategy and produces executable decision-tree code using the python-sc2 package.Code quality depends on both the clarity of the strategy and the coder LLM's capacity to translate it into effective logic.To improve this translation, the planner specifies not only the skills but also the conditions under which each skill is triggered.Generated code is evaluated in simulation, with performance metrics, including win rate, score, and damage statistics, computed over multiple runs.Any runtime exceptions trigger an immediate review by the critic, with the win rate defaulting to zero.</p>
<p>Critic.The critic diagnoses errors and proposes refinements.For runtime exceptions, it inspects both the code and the traceback to pinpoint issues which often arise from deprecated or incorrect API calls due to changes in python-sc2.For successful rollouts, the critic analyzes contributing factors to performance, identifying strengths to retain and weaknesses to address.Recommendations are fed back into the planner or the coder loop to guide the next code revision or strategic adjustment.</p>
<p>Agent Evolution</p>
<p>The evolution of the decision tree is intrinsically linked to the curriculum iteration process, ensuring that strategic complexity scales with the demands of each stage.Initially, the decision tree consists of a minimal set of branches encoding basic tactical rules, such as focusing fire on high-value targets or retreating to safe positions.As early tasks are mastered, new branches are added, existing conditions are refined, and ineffective rules are pruned to maintain clarity and efficiency.</p>
<p>Increases in curriculum difficulty with new unit types, environmental constraints, or adversarial tactics trigger structural adaptations in the decision tree.Additional decision nodes are incorporated to manage the expanded state-action space, guided by rollout analyses that reinforce successful decision paths and restructure underperforming ones.This process encourages generalization beyond previously encountered scenarios.</p>
<p>Over time, curriculum progression and decision-tree adaptation co-produce a decision-making structure that is both deeper and more context-aware.The agent transitions from static, simple logic to a dynamic hierarchy of conditional rules capable of handling multi-phase, high-complexity engagements.By the final curriculum stage, the evolved decision tree constitutes a robust and adaptable strategic framework, fully capable of addressing the task's most demanding requirements.</p>
<p>Experiment</p>
<p>This section evaluates EvoCurr's performance in generating complex micro-management strategies for StarCraft II through empirical experiments.We assess the framework's ability to autonomously design progressive curricula and generate effective decision tree code, comparing against direct code generation baselines.</p>
<p>Experimental Setup</p>
<p>We designed a Terran versus Protoss micro-management scenario as our evaluation benchmark.The final task specification is presented in Table 1, representing a complex multi-unit engagement requiring sophisticated coordination and tactical execution.We compare EvoCurr against a direct generation baseline that attempts to produce decision tree code for the final task without curriculum progression.Our experimental environment utilizes flat terrain to eliminate terrain complexity and focuses purely on tactical micro-management skills.To enhance task difficulty and simulate realistic combat scenarios, we implemented sophisticated opponent strategies combining handcrafted scripts with StarCraft II's built-in AI systems.The strategic scripts enable enemy units to exhibit role-appropriate behaviors and tactical coordination that mirror human-level gameplay patterns.It should be noted that StarCraft II's built-in AI difficulty settings primarily affect macrolevel decisions such as build orders and unit production rather than micro-management execution, making our scripted behaviors the primary determinant of combat challenge.The opponent AI architecture ensures that enemy units demonstrate appropriate tactical responses including target prioritization, ability usage timing, positioning optimization, and coordinated group movements.This design creates a more authentic and challenging testing environment that better reflects the complexity of real StarCraft II engagements.Performance evaluation requires achieving a 67% win rate threshold across multiple simulation runs to consider a curriculum stage successful.</p>
<p>Curriculum Generation Results</p>
<p>To comprehensively evaluate EvoCurr's adaptive curriculum design capabilities, we conducted five independent experimental runs, each starting from the same minimal initial scenario but allowing complete autonomy in curriculum evolution.Each experimental path represents a unique exploration of the curriculum design space, demonstrating the framework's ability to discover different learning trajectories toward the same final objective.</p>
<p>The experimental protocol follows a strict progression mechanism where each curriculum stage must achieve the predefined 67% win rate threshold before advancement.When generated decision trees fail to meet performance criteria after multiple coding attempts within a single curriculum iteration, the curriculum designer automatically generates simplified alternative tasks.This adaptive mechanism prevents the system from becoming trapped in overly challenging scenarios while preserving accumulated knowledge from previous successful implementations.</p>
<p>Our evaluation focuses on several key metrics: curriculum progression depth (total number of tasks generated), success consistency (percentage of tasks completed successfully), and final objective achievement.Additionally, we analyze the curriculum designer's adaptation strategies, including difficulty scaling patterns, unit introduction sequencing, and failure recovery mechanisms.</p>
<p>Table 2 and Figure 3 present the complete curriculum evolution across all five paths, showing the detailed unit compositions, enemy configurations, and performance outcomes for each generated task.The results reveal significant diversity in curriculum design approaches, with different paths exploring varying unit introduction strategies, complexity scaling rates, and tactical focus areas.</p>
<p>Analysis of Curriculum Adaptation</p>
<p>The experimental results reveal distinct learning trajectories and adaptation strategies across the five paths.Path 1, demonstrated as Figure 4, achieved complete success by systematically building complexity while maintaining performance thresholds.The automatic difficulty adjustment mechanism is clearly demonstrated in Path 1's Task 3 failure, where the curriculum designer generated a simplified Task 4 that reduced unit count and enemy complexity while preserving essential tactical elements.</p>
<p>Path 2 showed early promise with a perfect 100% win rate in Task 1, but the curriculum designer's aggressive complexity scaling in subsequent tasks led to insurmountable challenges.Path 3 demonstrated the most extensive exploration with seven total tasks, achieving intermittent success but ultimately failing to maintain consistent progression toward the final objective.</p>
<p>Notably, Path 4 achieved perfect performance in Task 2, suggesting effective tactical learning for mid-complexity scenarios, but could not bridge the gap to higher-level coordination requirements.Path 5 closely paralleled Path 1's progression through the first five tasks but failed at the final stage, indicating that reaching the target task specification represents a significant complexity threshold.</p>
<p>The curriculum adaptation behavior demonstrates EvoCurr's intelligent failure recovery.When tasks fail repeatedly, the system automatically reduces difficulty by decreasing unit counts, removing advanced technologies, or simplifying enemy compositions while preserving the decision tree code from the most recent successful task.</p>
<p>Results and Analysis</p>
<p>The experimental results demonstrate a 20% success rate (1 out of 5 paths) for complete task mastery, revealing both the potential and challenges of autonomous curriculum learning.The successful path achieved consistent 67% win rates across all stages, validating EvoCurr's capability to generate sophisticated multi-unit micromanagement strategies.</p>
<p>Analysis of the successful Path 1 reveals key learning milestones: basic combat mechanics and focus fire (Task 1), support unit coordination and healing management (Task 2), specialized abilities and stealth tactics (Task 4), siege warfare and air-ground coordination (Task 5), and full tactical integration including area denial (Task 6).</p>
<p>The failed paths provide valuable insights into curriculum design limitations.Common failure modes include premature complexity increases, insufficient difficulty reduction mechanisms, and challenges in bridging intermediate to advanced tactical requirements.Final Task (Table 1) Final Task (Table 1) Failed</p>
<p>These observations highlight opportunities for improving curriculum designer robustness and failure recovery strategies.</p>
<p>Discussion, Future Work, and Conclusion</p>
<p>The experimental results demonstrate that EvoCurr effectively enhances an agent's capability in designing decision-tree scripts.This supports the hypothesis that presenting a sequence of problem instances with progressively increasing difficulty can substantially improve an agent's learning performance.By exposing the agent to increasingly complex tasks, EvoCurr facilitates the gradual acquisition of tactical knowledge for different unit types, thereby improving both the robustness and versatility of the resulting scripts.Consequently, the process of generating decision-tree scripts becomes more structured and logically coherent.However, our findings also reveal that, under constraints of limited content length, a single agent struggles to achieve optimal performance across all unit types.The generated scripts tend to exhibit a bias toward certain units, applying more sophisticated micro-management strategies to them, while other units are handled in a comparatively coarse manner.This imbalance suggests a potential limitation in single-agent architectures when dealing with heterogeneous unit compositions.</p>
<p>To address the observed limitations, we propose extending EvoCurr to a multi-agent framework, in which different agents are assigned to control specific unit types.This division of labor would allow each agent to specialize in the micro-management and tactical optimization of its designated units, thereby reducing bias and improving overall performance.Furthermore, future research could explore adaptive curriculum schedules, in which the difficulty progression is dynamically adjusted based on the agent's learning rate and performance metrics, potentially leading to more efficient training.</p>
<p>In summary, EvoCurr provides a viable and effective approach for tackling complex decision-tree script generation tasks within the context of StarCraft II.Our results confirm its capacity to incrementally improve agent performance through a carefully structured learning progression, even when using a single-agent system.Moreover, the method demonstrates practical efficacy in the StarCraft II domain, offering a promising pathway toward enhanced AI bot performance and laying the groundwork for future developments in curriculum-based multi-agent learning.</p>
<p>A Appendix: Introduction to StarCraft II</p>
<p>StarCraft II is a real-time strategy game developed by Blizzard Entertainment that has become one of the most challenging and strategically complex video games ever created.Released in 2010, the game features three asymmetric factions-Terrans, Protoss, and Zerg-each with distinct units, technologies, and strategic approaches.Players must simultaneously manage multiple interconnected systems: resource collection and allocation, base construction and expansion, technological research and upgrades, unit production and army composition, and real-time tactical combat control.The game demands rapid decision-making under time pressure, long-term strategic planning, adaptation to opponent strategies, and precise micro-management of individual units during combat.Professional matches can involve hundreds of units across multiple battlefronts, requiring players to process vast amounts of information while executing complex multi-layered strategies.The skill ceiling is extraordinarily high, with professional players dedicating years to master the intricate mechanics, build orders, timing attacks, and unit interactions that define high-level play.</p>
<p>The significance of StarCraft II for artificial intelligence research extends far beyond its entertainment value.The game presents a comprehensive testbed for studying complex decision-making under uncertainty, partial information, and real-time constraints challenges that mirror many real-world applications of AI.Unlike traditional board games such as chess or Go, which have perfect information and turn-based mechanics, StarCraft II requires agents to operate in a partially observable environment with continuous action spaces and exponentially large state representations.The game's multi-scale nature demands both macro-level strategic planning spanning tens of minutes and micro-level tactical execution occurring within milliseconds.This dual requirement has driven significant advances in hierarchical reinforcement learning, multiagent coordination, and long-horizon planning algorithms.Notable breakthroughs include DeepMind's AlphaStar, which achieved Grandmaster level performance and demonstrated that AI systems could master complex strategic reasoning, and subsequent research that has explored everything from curriculum learning and imitation learning to neural architecture search and federated training.The availability of extensive replay datasets, standardized evaluation protocols through environments like PySC2, and the game's inherent interpretability through observable unit actions have made StarCraft II an invaluable platform for developing and benchmarking AI systems capable of human-level strategic reasoning in complex, dynamic environments.</p>
<p>B StarCraft II API and Python Interfaces</p>
<p>The technical foundation enabling AI research in StarCraft II rests on Blizzard Entertainment's official StarCraft II Machine Learning API, which provides programmatic access to the game's complete state information and action execution capabilities.This API exposes the game engine through a protocol buffer-based interface that delivers real-time observations including unit positions, resource states, map geometry, and tactical information while accepting high-level commands for unit control, building construction, and technology research.The official s2client-proto defines the core communication protocol between external programs and the StarCraft II executable, establishing standardized data structures for observations, actions, and game configuration.This low-level interface handles the complex details of game state serialization, network communication, and command validation, but requires substantial boilerplate code and deep understanding of the underlying protocol specifications to implement effective AI agents.</p>
<p>Building upon this foundation, the research community has developed higher-level abstractions that significantly simplify AI development while preserving the full functionality of the underlying API.PySC2, developed by DeepMind, transforms the raw API into a structured reinforcement learning environment that follows standard RL conventions with observation spaces, action spaces, and reward functions.This environment emphasizes feature-layer representations and provides built-in mini-games for curriculum learning, making it particularly suitable for deep reinforcement learning approaches.</p>
<p>Complementing PySC2, the python-sc2 library offers a more direct and intuitive interface focused on scripted bot development, where complex strategic behaviors can be implemented using straightforward Python code with minimal boilerplate.The python-sc2 library abstracts away protocol buffer complexities while exposing high-level game objects such as units, abilities, and map structures through clean Python APIs, enabling researchers to focus on strategic logic rather than low-level implementation details.Our EvoCurr framework leverages python-sc2's accessibility and expressiveness to generate decision tree scripts that can be easily interpreted, debugged, and modified, making it an ideal choice for our curriculum-based approach to complex tactical reasoning.</p>
<p>C Generated Decision Tree Code Examples</p>
<p>This section presents complete examples of decision tree implementations generated by the EvoCurr framework at different curriculum stages, demonstrating the evolution of tactical complexity.</p>
<p>C.1 Early Stage: Basic Marine</p>
<p>Micro-management</p>
<p>Figure 1 :
1
Figure 1: Two LLM agents cooperate to solve complex target problems.The curriculum designer provides curriculum task description, including the map and agents specifics to the coder and the coder solves the problem by generating decision tree.The curriculum designer then continue to design curriculum based on the coder results.</p>
<p>Figure 2 :
2
Figure 2: The overall architecture of our proposed EvoCurr framework.The framework takes the information of units and maps of the final task as the environment prompts.Then, the curriculum designer generates a new target based on the result and the information of current curriculum.Then the behavior coder generates modified decision tree scripts based on the script of previous task.The scripts are tested on the complex decision-making environment, and the results with the strategy and the code are fed to the curriculum designer to amplify the task and adjust the difficulty in a closed-loop manner.</p>
<p>Figure 3 :
3
Figure 3: A demonstration of curriculum paths generated from the first curriculum setting.The green points are the terminal nodes, red points represents failing settings and the green points are success curricula.</p>
<p>Figure 4 :
4
Figure 4: A demonstration path of curriculum path.The coder fails to solve the forth curriculum and the designer regenerate a new curriculum based on the third curriculum.The coder finally solve the complex task after 6 curricula.</p>
<p>1 :
1
Initialize curriculum  0 ← Simplify(  ) 2: Initialize decision tree  ←  0 3:  ← 0 4: while   ≠   and  &lt;  do   ← Evaluate(,   )  +1 ← CurriculumDesigner(  ,   ,  )  +1 ← BehaviorCoder( +1 , ,   )  ←  +1 ,  ←  + 1 12: end while 13: return  Algorithm 2 Curriculum Designer Input: Current curriculum , performance , target   , LLM L  Output: Next curriculum  ′
5:// Environment evaluation6:7:// Curriculum design8:9:// Behavior coding10:11:
1: Extract metrics: ,  ← Extract() 2: Build environment prompt:   ← BuildPrompt(  ) 3: Format feedback:  ← Format() 4: Get history: ℎ ← GetHistory() 5: Combine prompts:  ← Combine(  , ,  , ℎ) 6: if  &gt;  then 7:</p>
<p>Algorithm 3 Behavior Coder Input: Target curriculum   , current tree , feedback  , max attempts  Output: Improved tree  ′ 1:  ← 0,  ←  2:  ← GetUnitInfo(),   ← GetMapInfo(  ) 3: while  &lt;  do  ← Plan(  , ,  ,  ,   )   ← Evaluate(  ,   )
4:// Planner: Generate strategy5:6:// Coder: Generate code7:code ← Code(𝑆, 𝐵)8:𝐵 𝑐 ← Compile(code)9:// Evaluation10:if 𝐵 𝑐 is valid then11:12:if 𝑃 𝑡 .𝑟 ≥ 𝜃 𝑠 then13:return 𝐵 𝑐14:end if15:else16:𝑃 𝑡 ← {𝑟 : 0, error}17:end if18:// Critic: Analyze and update19:20:𝑓 ← Update(𝑓 , 𝑐)21:𝑘 ← 𝑘 + 122: end while23: return 𝐵varied enemy compositions, or stricter time constraints. Conversely,if performance falls short, the designer reverts to simpler configu-rations to reinforce deficient skills.
 ← Critique(  , code, )</p>
<p>Table 1 :
1
Final Task Specification
Unit TypeQuantity Position TechnologyAGENTS (Terran):Marine20(5, 25)StimpackMarauder12(5, 25)StimpackGhost3(5, 25)Personal CloakingMedivac3(5, 25)NoneSiege Tank2(5, 25)Siege TechViking Fighter 4(5, 25)NoneLiberator2(5, 25)NoneENEMIES (Protoss):Zealot15(25, 5)ChargeStalker12(25, 5)Blink TechHigh Templar 3(25, 5)Psi Storm TechColossus2(25, 5)Extended Thermal LanceDisruptor1(25, 5)None</p>
<p>Table 2 :
2
Complete Curriculum Evolution Across All Five Paths
PathTaskAgent CompositionEnemy CompositionResult
https://github.com/opendilab/DI-star
‡This work was done when Yang Cheng was an intern at Zhongguancun Institute of Artificial Intelligence.Marine(5)Zealot (2, Charge) 67% 2 Marine(10), Marauder(5), Medivac (1) Zealot (5, Charge), Stalker(5,Blink), HighTemplar (1, PsiStorm) 67% 3 Marine (15), Marauder (8), Ghost (2), Medivac (2), SiegeTank (1) Zealot (10, Charge), Stalker (8, Blink), HighTemplar (2, PsiStorm), Colossus (1, ExtLance) Failed 4 Marine (12), Marauder (6), Ghost (1), Medivac (1) Zealot (8, Charge), Stalker (6, Blink), HighTemplar (1, PsiStorm) 67% 5 Marine (18), Marauder (10), Ghost (2), Medivac (2), SiegeTank (1), Viking (2) Zealot (12, Charge), Stalker (10, Blink), HighTemplar (2, PsiStorm), Colossus (1, ExtLance) 67% 6 Final Task (Table 1) Final Task (Table 1) 100% 2 1 Marine (5) Zealot (2, Charge) 100% 2 Marine (10), Marauder (5), Medivac (2), SiegeTank (1) Zealot (8, Charge), Stalker (5, Blink), HighTemplar (2, PsiStorm) 67% 3 Marine (15), Marauder (8), Ghost (2), Medivac (3), SiegeTank (1), Viking (4) Zealot (12, Charge), Stalker (10, Blink), HighTemplar (3, PsiStorm), Colossus (2, ExtLance) Failed 4 Marine (12), Marauder (6), Ghost (1), Medivac (2), SiegeTank (1), Viking (2) Zealot (10, Charge), Stalker (8, Blink), HighTemplar (2, PsiStorm), Colossus (1, ExtLance) Failed 3 1 Marine (5) Zealot (2, Charge) 100% 2 Marine (8), Marauder (5, PunisherGrenades), SiegeTank (1), Medivac (1, CaduceusReactor) Zealot (7, Charge), Stalker (3, Blink), HighTemplar (2, PsiStorm), Colossus (1, ExtLance) Failed 3 Marine (8), Marauder (4), SiegeTank (1), Medivac (2, CaduceusReactor) Zealot (5, Charge), Stalker (2, Blink), Colossus (1) 100% 4 Marine (14), Marauder (7, PunisherGrenades), SiegeTank (2), Medivac (3, CaduceusReactor), Viking (2), Ghost (1) Zealot (9, Charge), Stalker (5, Blink), HighTemplar (2, PsiStorm), Colossus (2, ExtLance), Disruptor (1) Failed 5 Marine (10), Marauder (5), SiegeTank (1), Medivac (2, CaduceusReactor) Zealot (6, Charge), Stalker (3, Blink), Colossus (1) 100% 6 Marine (14), Marauder (7, PunisherGrenades), SiegeTank (2), Medivac (3, CaduceusReactor), Viking (2), Ghost (1) Zealot (9, Charge), Stalker (5, Blink), HighTemplar (2, PsiStorm), Colossus (2, ExtLance), Disruptor (1) Failed 7 Marine (14), Marauder (7, PunisherGrenades), SiegeTank (1), Medivac (2, CaduceusReactor), Ghost (1) Zealot (9, Charge), Stalker (4, Blink), Colossus (1, ExtLance) Failed 4 1 Marine (5) Zealot (2, Charge) 67% 2 Marine (10), Marauder (5), Ghost (2), Medivac (1, CaduceusReactor) Zealot (8, Charge), Stalker (4, Blink), HighTemplar (1, PsiStorm) 100% 3 Marine (15), Marauder (8), Ghost (3), Medivac (2, CaduceusReactor), SiegeTank (1), Viking (2)
. Philip J Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, Shlomi Fruchter, Agrim Gupta, Kristian Holsheimer, Aleksander Holynski, Jiri Hron, Christos Kaplanis, Marjorie Limont, Matt Mcgill, Yanko Oliveira, Jack Parker-Holder, Frank Perbet, Guy Scully, Jeremy Shar, Stephen Spencer, Omer Tov, Ruben Villegas, Emma Wang, Jessica Yung, Cip Baetu, Jordi Berbel, David Bridson, Jake Bruce, Gavin Buttimore, Sarah Chakera, Bilva Chandra, Paul Collins, Alex Cullum, Bogdan Damoc, Vibha Dasagi, Maxime Gazeau, Charles Gbadamosi, Woohyun Han, Ed Hirst, Ashyana Kachra, Lucie Kerley, Kristian Kjems, Eva Knoepfel, Vika Koriakin, Igor Saprykin, Amy Shen, Sailesh Sidhwani, Duncan Smith, Joe Stanton, Hamish Tomlinson, Dimple Vijaykumar, Luyu Wang, Piers Wingfield, Nat Wong, Keyang Xu, Christopher Yew2025Aäron van den OordJessica Lo, Cong Lu, Zeb Mehring, Alex Moufarek, Henna Nandwani, Valeria Oliveira, Fabio Pardo, Jane Park, Andrew Pierson, Ben Poole, Helen Ran, Tim Salimans, Manuel Sanchez; Nick Young, Vadim Zubov, Douglas Eck, Dumitru Erhan, Koray Kavukcuoglu, Demis Hassabis, Zoubin Gharamani, Raia Hadsell; Inbar Mosseri, Adrian BoltonSatinder Singh, and Tim Rocktäschel. 2025. Genie 3: A New Frontier for World Models.</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, Proceedings of the 26th Annual International Conference on Machine Learning. the 26th Annual International Conference on Machine LearningACM2009</p>
<p>Language models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared D Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, Advances in Neural Information Processing Systems. 332020. 2020</p>
<p>Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, Feryal Behbahani, Stephanie Chan, Nicolas Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, arXiv:2402.15391[cs.LGGenie: Generative Interactive Environments. Satinder Freitas, Tim Singh, Rocktäschel, 2024</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Li, Scott Lundberg, Harsha Nori, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with GPT-4. 2023. 2023arXiv preprint</p>
<p>Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, Hao Chen, arXiv:2411.00769[cs.CVGameGen-X: Interactive Open-world Game Video Generation. 2024</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021. 2021arXiv preprint</p>
<p>Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, Deshraj Yadav, arXiv:2504.19413[cs.CLMem0: Building Production-Ready AI Agents with Scalable Long-Term Memory. 2025</p>
<p>Decart Team, Julian Quevedo, Quinn Mcintyre, Spruce Campbell, Xinlei Chen, Robert Wachen, Oasis: A Universe in a Transformer. 2024</p>
<p>Yue Deng, Weiyu Ma, Yuxin Fan, Ruyi Song, Yin Zhang, Haifeng Zhang, Jian Zhao, arXiv:2410.16024[cs.AISMAC-R1: The Emergence of Intelligence in Decision-Making Tasks. 2025</p>
<p>Yue Deng, Yan Yu, Weiyu Ma, Zirui Wang, Wenhui Zhu, Jian Zhao, Yin Zhang, arXiv:2412.17707[cs.AISMAC-Hard: Enabling Mixed Opponent Strategy Script and Self-play on SMAC. 2024</p>
<p>Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution. Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, Tim Rocktäschel, arXiv:2309.16797[cs.CL2023</p>
<p>Automated curriculum learning for neural networks. Alex Graves, Jacob Marc G Bellemare, Rémi Menick, Koray Munos, Kavukcuoglu, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningPMLR2017</p>
<p>Junliang Guo, Yang Ye, Tianyu He, Haoyu Wu, Yushu Jiang, Tim Pearce, Jiang Bian, arXiv:2504.08388[cs.CVMineWorld: a Real-Time and Open-Source Interactive World Model on Minecraft. 2025</p>
<p>Tstarbot-x: An opensourced and comprehensive study for efficient league training in starcraft ii full game. Lei Han, Jiechao Xiong, Peng Sun, Xinghai Sun, Meng Fang, Qingwei Guo, Qiaobo Chen, Tengfei Shi, Hongsheng Yu, Xipeng Wu, arXiv:2011.137292020. 2020arXiv preprint</p>
<p>Game Generation via Large Language Models. Chengpeng Hu, Yunlong Zhao, Jialin Liu, arXiv:2404.08706[cs.AI2024</p>
<p>GenSim2: Scaling Robot Data Generation with Multimodal and Reasoning LLMs. Pu Hua, Minghuan Liu, Annabella Macaluso, Yunfeng Lin, Weinan Zhang, Huazhe Xu, Lirui Wang, arXiv:2410.036452024</p>
<p>A Robust and Opponent-Aware League Training Method for StarCraft II. Ruozi Huang, Xipeng Wu, Hongsheng Yu, Zhong Fan, Haobo Fu, Qiang Fu, Yang Wei, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Competition-level code generation with AlphaCode. Yujia Li, Maxwell Nye, Jacob Andreas, Jasmijn Bastings, Shruti Bhosale, James Bradbury, Jacob Austin, Greg Brockman, Trevor Cai, Ciprian Chelba, Science. 3782022. 2022</p>
<p>Zongyuan Li, Yanan Ni, Runnan Qi, Lumin Jiang, Chang Lu, Xiaojie Xu, Xiangbei Liu, Pengfei Li, Yunzheng Guo, Zhe Ma, Huanyu Li, Hui Wu, Xian Guo, Kuihua Huang, Xuebo Zhang, arXiv:2411.05348[cs.AILLM-PySC2: Starcraft II learning environment for Large Language Models. 2025</p>
<p>Efficient reinforcement learning for starcraft by abstract forward models and transfer learning. Ruo-Ze Liu, Haifeng Guo, Xiaozhong Ji, Yang Yu, Zhen-Jia Pang, Zitai Xiao, Yuzhou Wu, Tong Lu, IEEE Transactions on Games. 142021. 2021</p>
<p>On efficient reinforcement learning for full-length game of starcraft ii. Ruo-Ze Liu, Zhen-Jia Pang, Zhou-Yu Meng, Wenhai Wang, Yang Yu, Tong Lu, Journal of Artificial Intelligence Research. 752022. 2022</p>
<p>Ruo-Ze Liu, Wenhai Wang, Yanjie Shen, Zhiqi Li, Yang Yu, Tong Lu, arXiv:2104.06890An Introduction of mini-AlphaStar. 2021. 2021arXiv preprint</p>
<p>Weiyu Ma, Yuqian Fu, Zecheng Zhang, Bernard Ghanem, Guohao Li, arXiv:2503.05383[cs.AIAVA: Attentive VLM Agent for Mastering StarCraft II. 2025</p>
<p>Weiyu Ma, Qirui Mi, Yongcheng Zeng, Xue Yan, Yuqiao Wu, Runji Lin, Haifeng Zhang, Jun Wang, arXiv:2312.11865[cs.AILarge Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach. 2024</p>
<p>Starcraft ii unplugged: Large scale offline reinforcement learning. Michael Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray Jiang, Tom Le Paine, Konrad Zolna, Richard Powell, Julian Schrittwieser, Deep RL Workshop NeurIPS. 2021. 2021</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, Peter Stone, Journal of Machine Learning Research. 212020. 2020</p>
<p>arXiv:2303.08774GPT-4 technical report. 2023. 2023OpenAIarXiv preprint</p>
<p>Jack Parker-Holder, Philip Ball, Jake Bruce, Vibhavari Dasagi, Kristian Holsheimer, Christos Kaplanis, Alexandre Moufarek, Guy Scully, Jeremy Shar, Jimmy Shi, Stephen Spencer, Jessica Yung, Michael Dennis, Sultan Kenjeyev, Shangbang Long, Vlad Mnih, Harris Chan, Maxime Gazeau, Bonnie Li, Fabio Pardo, Luyu Wang, Lei Zhang, Frederic Besse, Tim Harley, Satinder Singh, and Tim Rocktäschel. 2024. Genie 2: A Large-Scale Foundation World Model. Anna Mitenkova, Jane Wang, Jeff Clune, Demis Hassabis, Raia Hadsell, Adrian Bolton2024</p>
<p>Automatic curriculum learning for deep RL: A short survey. Raphaël Portelas, Cédric Colas, Lionel Weng, Katja Hofmann, Pierre-Yves Oudeyer, arXiv:2003.046642020. 2020arXiv preprint</p>
<p>Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, Yuxiao Dong, arXiv:2411.02337[cs.CLWebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning. 2025</p>
<p>Yu Shang, Yu Li, Keyu Zhao, Likai Ma, Jiahe Liu, Fengli Xu, Yong Li, arXiv:2410.06153[cs.CLAgentSquare: Automatic LLM Agent Search in Modular Design Space. 2025</p>
<p>Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang, arXiv:2305.16653[cs.CLAdaPlanner: Adaptive Planning from Feedback with Language Models. 2023</p>
<p>Grandmaster level in StarCraft II using multi-agent reinforcement learning. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Nature. 5752019. 2019</p>
<p>Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Sasha Alexander, Michelle Vezhnevets, Alireza Yeo, Heinrich Makhzani, John Küttler, Julian Agapiou, Schrittwieser, arXiv:1708.04782Starcraft ii: A new challenge for reinforcement learning. 2017. 2017arXiv preprint</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv: Arxiv-2305.16291Voyager: An Open-Ended Embodied Agent with Large Language Models. 2023. 2023arXiv preprint</p>
<p>GenSim: Generating Robotic Simulation Tasks via Large Language Models. Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shridhar, Chen Bao, Yuzhe Qin, Bailin Wang, Huazhe Xu, Xiaolong Wang, arXiv:2310.01361[cs.LG2024</p>
<p>SCC: An efficient deep reinforcement learning agent mastering the game of StarCraft II. Xiangjun Wang, Junxiao Song, Penghui Qi, Peng Peng, Zhenkun Tang, Wei Zhang, Weimin Li, Xiongjun Pi, Jujie He, Chao Gao, International conference on machine learning. PMLR2021</p>
<p>Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, Manling Li, arXiv:2504.20073[cs.LGRAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning. 2025</p>
<p>Grounding natural language commands to StarCraft II game states for narration-guided reinforcement learning. Nicholas Waytowich, Sean L Barton, Vernon Lawhern, Ethan Stump, Garrett Warnell, Artificial intelligence and machine learning for multi-domain operations applications. 201911006</p>
<p>Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, Mohit Bansal, arXiv:2403.12014[cs.CLEnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents. 2024</p>
<p>STaR: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D Goodman, Advances in Neural Information Processing Systems. 202235</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D Goodman, arXiv:2203.14465[cs.LGSTaR: Bootstrapping Reasoning With Reasoning. 2022</p>
<p>Chen Zhang, Qiang He, Zhou Yuan, Elvis S Liu, Hong Wang, Jian Zhao, Yang Wang, arXiv:2406.01103[cs.AIAdvancing DRL Agents in Commercial Fighting Games: Training, Integration, and Agent-Human Alignment. 2024</p>
<p>HIFAS: A Hybrid Interactive FPS Agent System for Large Game Maps. Chen Zhang, Huan Hu, Yuan Zhou, Xu Wang, Elvis S Liu, 10.1109/TG.2025.3567869IEEE Transactions on Games. 2025. 2025</p>
<p>Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, Bingnan Zheng, Bang Liu, Yuyu Luo, Chenglin Wu, arXiv:2410.10762[cs.AIAFlow: Automating Agentic Workflow Generation. 2025</p>
<p>Yifei Zhou, Sergey Levine, Jason Weston, Xian Li, Sainbayar Sukhbaatar, arXiv:2506.01716[cs.AISelf-Challenging Language Model Agents. 2025</p>            </div>
        </div>

    </div>
</body>
</html>
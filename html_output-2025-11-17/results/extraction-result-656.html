<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-656 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-656</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-656</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-726320cdbd04804ffa8f3a78c095bd1b55a2a695</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/726320cdbd04804ffa8f3a78c095bd1b55a2a695" target="_blank">Similarity of Neural Network Representations Revisited</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> A similarity index is introduced that measures the relationship between representational similarity matrices and does not suffer from this limitation of CCA.</p>
                <p><strong>Paper Abstract:</strong> Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e656.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e656.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random initialization variability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random weight initialization as a source of stochasticity in neural network training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variability arising from training identical architectures from different random initializations; assessed by comparing learned representations across independently trained networks using similarity indices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various CNNs (All-CNN-C-based, Plain, ResNet-62) and Transformer encoders</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / neural representation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Compare learned representations across independently trained networks (different random seeds) using similarity indices (CKA, CCA, SVCCA, PWCCA, linear regression) to identify corresponding layers and quantify representational similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Random weight initialization (different random seeds); training dynamics (stochastic gradient descent); architecture differences (depth, width, residual connections); dataset differences (CIFAR-10 vs CIFAR-100); presence/absence of batch normalization; finite sample effects when layer width >= number of examples (p >= n); non-isotropic scaling of features.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Accuracy of identifying corresponding layers via maximum similarity (percentage), pairwise similarity scores (CKA values in [0,1]), jackknife standard error for CKA averages, CKA matrices between layers</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Experiments used 10 independently trained networks. Table 2 (matching corresponding layers across 10 networks) shows CKA (Linear) accuracy 99.3%, CKA (RBF 0.4) 99.1%, CKA (RBF 0.8) 99.3%, CKA (RBF 0.2) 80.6%; by contrast CCA variants and PWCCA perform poorly (e.g. CCA (mean rho) 1.4%, CCA (R_CCA^2) 10.6%, PWCCA 11.1%), linear regression 45.4%, Linear HSIC 22.2%. Results averaged across 10 networks (45 pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Layer-matching accuracy (percent of layers for which the top matched layer across another network is the architecturally corresponding layer), CKA similarity scores between layer pairs, comparisons across architectures/widths/datasets, jackknife standard error for CKA averages.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>CKA reliably reproduces layer correspondences across random initializations with ~99.3% matching accuracy (linear CKA); other measures (CCA/SVCCA/PWCCA/linear HSIC/linear regression) show much lower matching accuracy (listed above). Similarity across datasets: early layers of CIFAR-10 and CIFAR-100 trained networks are similar (CKA high), and representations differ substantially from untrained networks.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Methods invariant to invertible linear transforms (e.g. CCA) cannot distinguish representations when layer width >= dataset size (p >= n); sensitivity of CCA/SVCCA to condition number and small singular values; selection of hyperparameters (e.g. SVCCA truncation threshold) alters results; kernel bandwidth choice for RBF CKA affects outcomes; overparameterization and early stopping/optimization dynamics cause dependence on input scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Adopt similarity index invariant to orthogonal transforms and isotropic scaling but not to arbitrary invertible linear transforms (CKA); center and normalize kernel matrices (HSIC normalization -> CKA); use linear CKA as default; for RBF CKA set bandwidth sigma as fraction of median pairwise distance; orthonormalize columns via QR when desirable; consider SVCCA truncation, PWCCA projection-weighting, or canonical-ridge regularization when mapping is needed; aggregate results across multiple independent runs (they used 10) and report jackknife standard errors.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Using linear CKA gave dramatically improved reproducibility of layer matching (99.3% accuracy) compared to alternatives (CCA-based methods <= 15.1% in these experiments); choosing appropriate RBF bandwidth (fraction of median distance) allowed RBF CKA to approach linear CKA performance (e.g. RBF 0.4 and 0.8 achieved ~99%). SVCCA truncation did not reveal layer structure for default thresholds in these experiments (Appendix F.2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10 networks (45 pairwise comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Random initialization induces variability in learned representations, but CKA (linear or appropriately tuned RBF) robustly identifies correspondences between layers across independent runs (≈99% matching accuracy), whereas CCA-based methods and other alternatives perform poorly and are sensitive to hyperparameters and data/regime (e.g., p >= n).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Similarity of Neural Network Representations Revisited', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e656.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e656.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RBF bandwidth selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RBF kernel bandwidth (sigma) selection for RBF CKA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The choice of RBF kernel bandwidth controls sensitivity to small vs large distances and affects invariance to isotropic scaling and the performance of RBF CKA in identifying corresponding layers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various CNNs (All-CNN-C-based) and Transformer encoders</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / representation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Evaluate RBF CKA with different bandwidth fractions of the median pairwise example distance to compare representations across networks.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Choice of RBF kernel bandwidth sigma (set as fraction of median pairwise distance), which changes emphasis on local vs global distances and can alter similarity rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Layer-matching accuracy (percentage) for RBF CKA at different sigma fractions; CKA similarity scores.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Table 2 reports RBF CKA matching accuracies: sigma fraction 0.2 -> 80.6%, 0.4 -> 99.1%, 0.8 -> 99.3% (averaged over 10 networks). Authors note that selecting sigma as a fraction of median distance ensures invariance to isotropic scaling in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Matching accuracy across sigma settings</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Certain bandwidth choices (e.g., 0.4–0.8 of median distance) lead to reproducible high matching accuracy comparable to linear CKA, while smaller fractions (0.2) degraded performance.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Invariance of RBF CKA to isotropic scaling depends on bandwidth selection; poor bandwidth choice can substantially reduce ability to find correspondences.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Set sigma relative to median pairwise distance between examples (use a fraction of median) as a practical heuristic; compare multiple sigma values and report sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Median-distance-based selection produced RBF CKA results matching linear CKA performance for tested fractions (0.4 and 0.8), improving matching accuracy from 80.6% (sigma=0.2) to ~99% (sigma=0.4/0.8).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10 networks</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Bandwidth selection for RBF CKA materially affects reproducibility; choosing sigma as a fraction of the median pairwise distance is an effective heuristic that yields invariance to isotropic scaling and high matching accuracy comparable to linear CKA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Similarity of Neural Network Representations Revisited', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e656.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e656.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CCA/SVCCA/PWCCA sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Canonical Correlation Analysis and its variants (SVCCA, PWCCA) — sensitivity to condition number, truncation, and invariance assumptions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CCA-based methods are sensitive to numerical conditioning, small singular values, truncation thresholds, and invariance to invertible linear transforms, which can limit their usefulness for reliable cross-run representational comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various CNNs and Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / representation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Compare performance of CCA, SVCCA, PWCCA against CKA and other measures in identifying corresponding layers across independently trained networks.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>High condition number (small singular values) in activation matrices; selection of retained principal components in SVCCA (truncation threshold); invariance to invertible linear transforms causing indistinguishability when layer width >= dataset size.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Matching accuracy (%) for each method; sensitivity analysis across SVCCA truncation thresholds (Appendix F.2).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Table 2 shows low matching accuracies for CCA/SVCCA/PWCCA: CCA (mean rho) 1.4%, CCA (R_CCA^2) 10.6%, SVCCA (mean rho) 9.9%, SVCCA (R_CCA^2) 15.1%, PWCCA 11.1% (averaged over 10 networks). SVCCA truncation thresholds tried did not reveal layer structure in these CNN experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Matching accuracy and experiments across truncation thresholds/hyperparameters</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>CCA-based methods generally failed the proposed sanity check for CNNs: they did not reliably identify corresponding layers across random initializations and were sensitive to hyperparameters (e.g., SVCCA threshold).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Invariance to arbitrary invertible linear transforms makes some statistics ill-defined in overparameterized regimes (p >= n); CCA is sensitive to perturbations when condition numbers are large; need to choose truncation thresholds or regularization parameters, which affects results and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>SVCCA (truncate small singular vectors), PWCCA (projection-weighted averaging), canonical-ridge regularization (interpolates between CCA, regression, and CKA), and normalization strategies; but these add hyperparameters and can bias outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Mitigations (SVCCA truncation, PWCCA) provided modest improvements but remained substantially worse than CKA in the CNN layer-matching task; SVCCA thresholds tested failed to reveal correct layer structure (Appendix F.2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10 networks</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CCA-based similarity metrics are fragile in practice for these representation-comparison tasks: they are sensitive to numerical issues and hyperparameters, perform poorly at identifying corresponding layers across initializations, and can produce misleading invariance in overparameterized settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Similarity of Neural Network Representations Revisited', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e656.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e656.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BatchNorm / training-scale sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effects of batch normalization and feature scaling on representation similarity and reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Presence or absence of batch normalization and non-isotropic feature scaling change training dynamics and feature scales; CKA captures these differences while some methods (e.g., CCA) that are invariant to non-isotropic scaling may miss them.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Plain networks with and without batch normalization (All-CNN-C variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / representation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Compare representations of networks trained with and without batch normalization and quantify similarity using CKA and other metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Presence vs absence of batch normalization (changes internal covariate shift, feature scale), resulting differences in feature scaling and training trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>CKA similarity scores between networks; per-layer classifier accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Appendix F.4 reports networks with batch normalization averaged 93.9% accuracy and networks without batch normalization averaged 91.5% accuracy; CKA shows that the largest representational difference is at the last convolutional layer, but overall representations remain similar.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>CKA similarity matrices and per-layer classification accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Networks with and without batch normalization learn largely similar representations (as measured by CKA) though last-layer differences exist; CCA (invariant to non-isotropic scaling) would not capture some scale-induced differences that CKA does.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Scale differences induced by batch normalization can mask or alter representational similarity if using methods invariant to non-isotropic scaling (e.g., CCA).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use similarity metrics that are sensitive to feature-scale differences of interest (CKA) and tune hyperparameters separately for different training regimes; report per-layer analyses and classifier accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>CKA detected differences (largest at last conv layer) and overall similarity; no numeric reduction-of-variance measure reported, but separate hyperparameter tuning produced comparably high accuracies for both regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>multiple networks (reported averages)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Batch normalization affects feature scales and training dynamics; CKA is able to detect scale-related differences that CCA would miss, and overall networks with and without batch normalization still learn similar representations except at some later layers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Similarity of Neural Network Representations Revisited', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SVCCA: Singular vector canonical correlation analysis for deep learning dynamics and interpretability <em>(Rating: 2)</em></li>
                <li>Insights on representational similarity in neural networks with canonical correlation <em>(Rating: 2)</em></li>
                <li>Convergent learning: Do different neural networks learn the same representations? <em>(Rating: 2)</em></li>
                <li>Towards understanding learning representations: To what extent do different neural networks learn the same representation <em>(Rating: 2)</em></li>
                <li>Neural tangent kernel: Convergence and generalization in neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-656",
    "paper_id": "paper-726320cdbd04804ffa8f3a78c095bd1b55a2a695",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Random initialization variability",
            "name_full": "Random weight initialization as a source of stochasticity in neural network training",
            "brief_description": "Variability arising from training identical architectures from different random initializations; assessed by comparing learned representations across independently trained networks using similarity indices.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Various CNNs (All-CNN-C-based, Plain, ResNet-62) and Transformer encoders",
            "model_size": null,
            "scientific_domain": "machine learning / neural representation analysis",
            "experimental_task": "Compare learned representations across independently trained networks (different random seeds) using similarity indices (CKA, CCA, SVCCA, PWCCA, linear regression) to identify corresponding layers and quantify representational similarity.",
            "variability_sources": "Random weight initialization (different random seeds); training dynamics (stochastic gradient descent); architecture differences (depth, width, residual connections); dataset differences (CIFAR-10 vs CIFAR-100); presence/absence of batch normalization; finite sample effects when layer width &gt;= number of examples (p &gt;= n); non-isotropic scaling of features.",
            "variability_measured": true,
            "variability_metrics": "Accuracy of identifying corresponding layers via maximum similarity (percentage), pairwise similarity scores (CKA values in [0,1]), jackknife standard error for CKA averages, CKA matrices between layers",
            "variability_results": "Experiments used 10 independently trained networks. Table 2 (matching corresponding layers across 10 networks) shows CKA (Linear) accuracy 99.3%, CKA (RBF 0.4) 99.1%, CKA (RBF 0.8) 99.3%, CKA (RBF 0.2) 80.6%; by contrast CCA variants and PWCCA perform poorly (e.g. CCA (mean rho) 1.4%, CCA (R_CCA^2) 10.6%, PWCCA 11.1%), linear regression 45.4%, Linear HSIC 22.2%. Results averaged across 10 networks (45 pairs).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Layer-matching accuracy (percent of layers for which the top matched layer across another network is the architecturally corresponding layer), CKA similarity scores between layer pairs, comparisons across architectures/widths/datasets, jackknife standard error for CKA averages.",
            "reproducibility_results": "CKA reliably reproduces layer correspondences across random initializations with ~99.3% matching accuracy (linear CKA); other measures (CCA/SVCCA/PWCCA/linear HSIC/linear regression) show much lower matching accuracy (listed above). Similarity across datasets: early layers of CIFAR-10 and CIFAR-100 trained networks are similar (CKA high), and representations differ substantially from untrained networks.",
            "reproducibility_challenges": "Methods invariant to invertible linear transforms (e.g. CCA) cannot distinguish representations when layer width &gt;= dataset size (p &gt;= n); sensitivity of CCA/SVCCA to condition number and small singular values; selection of hyperparameters (e.g. SVCCA truncation threshold) alters results; kernel bandwidth choice for RBF CKA affects outcomes; overparameterization and early stopping/optimization dynamics cause dependence on input scaling.",
            "mitigation_methods": "Adopt similarity index invariant to orthogonal transforms and isotropic scaling but not to arbitrary invertible linear transforms (CKA); center and normalize kernel matrices (HSIC normalization -&gt; CKA); use linear CKA as default; for RBF CKA set bandwidth sigma as fraction of median pairwise distance; orthonormalize columns via QR when desirable; consider SVCCA truncation, PWCCA projection-weighting, or canonical-ridge regularization when mapping is needed; aggregate results across multiple independent runs (they used 10) and report jackknife standard errors.",
            "mitigation_effectiveness": "Using linear CKA gave dramatically improved reproducibility of layer matching (99.3% accuracy) compared to alternatives (CCA-based methods &lt;= 15.1% in these experiments); choosing appropriate RBF bandwidth (fraction of median distance) allowed RBF CKA to approach linear CKA performance (e.g. RBF 0.4 and 0.8 achieved ~99%). SVCCA truncation did not reveal layer structure for default thresholds in these experiments (Appendix F.2).",
            "comparison_with_without_controls": true,
            "number_of_runs": "10 networks (45 pairwise comparisons)",
            "key_findings": "Random initialization induces variability in learned representations, but CKA (linear or appropriately tuned RBF) robustly identifies correspondences between layers across independent runs (≈99% matching accuracy), whereas CCA-based methods and other alternatives perform poorly and are sensitive to hyperparameters and data/regime (e.g., p &gt;= n).",
            "uuid": "e656.0",
            "source_info": {
                "paper_title": "Similarity of Neural Network Representations Revisited",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "RBF bandwidth selection",
            "name_full": "RBF kernel bandwidth (sigma) selection for RBF CKA",
            "brief_description": "The choice of RBF kernel bandwidth controls sensitivity to small vs large distances and affects invariance to isotropic scaling and the performance of RBF CKA in identifying corresponding layers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various CNNs (All-CNN-C-based) and Transformer encoders",
            "model_size": null,
            "scientific_domain": "machine learning / representation analysis",
            "experimental_task": "Evaluate RBF CKA with different bandwidth fractions of the median pairwise example distance to compare representations across networks.",
            "variability_sources": "Choice of RBF kernel bandwidth sigma (set as fraction of median pairwise distance), which changes emphasis on local vs global distances and can alter similarity rankings.",
            "variability_measured": true,
            "variability_metrics": "Layer-matching accuracy (percentage) for RBF CKA at different sigma fractions; CKA similarity scores.",
            "variability_results": "Table 2 reports RBF CKA matching accuracies: sigma fraction 0.2 -&gt; 80.6%, 0.4 -&gt; 99.1%, 0.8 -&gt; 99.3% (averaged over 10 networks). Authors note that selecting sigma as a fraction of median distance ensures invariance to isotropic scaling in their experiments.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Matching accuracy across sigma settings",
            "reproducibility_results": "Certain bandwidth choices (e.g., 0.4–0.8 of median distance) lead to reproducible high matching accuracy comparable to linear CKA, while smaller fractions (0.2) degraded performance.",
            "reproducibility_challenges": "Invariance of RBF CKA to isotropic scaling depends on bandwidth selection; poor bandwidth choice can substantially reduce ability to find correspondences.",
            "mitigation_methods": "Set sigma relative to median pairwise distance between examples (use a fraction of median) as a practical heuristic; compare multiple sigma values and report sensitivity.",
            "mitigation_effectiveness": "Median-distance-based selection produced RBF CKA results matching linear CKA performance for tested fractions (0.4 and 0.8), improving matching accuracy from 80.6% (sigma=0.2) to ~99% (sigma=0.4/0.8).",
            "comparison_with_without_controls": true,
            "number_of_runs": "10 networks",
            "key_findings": "Bandwidth selection for RBF CKA materially affects reproducibility; choosing sigma as a fraction of the median pairwise distance is an effective heuristic that yields invariance to isotropic scaling and high matching accuracy comparable to linear CKA.",
            "uuid": "e656.1",
            "source_info": {
                "paper_title": "Similarity of Neural Network Representations Revisited",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "CCA/SVCCA/PWCCA sensitivity",
            "name_full": "Canonical Correlation Analysis and its variants (SVCCA, PWCCA) — sensitivity to condition number, truncation, and invariance assumptions",
            "brief_description": "CCA-based methods are sensitive to numerical conditioning, small singular values, truncation thresholds, and invariance to invertible linear transforms, which can limit their usefulness for reliable cross-run representational comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Various CNNs and Transformers",
            "model_size": null,
            "scientific_domain": "machine learning / representation analysis",
            "experimental_task": "Compare performance of CCA, SVCCA, PWCCA against CKA and other measures in identifying corresponding layers across independently trained networks.",
            "variability_sources": "High condition number (small singular values) in activation matrices; selection of retained principal components in SVCCA (truncation threshold); invariance to invertible linear transforms causing indistinguishability when layer width &gt;= dataset size.",
            "variability_measured": true,
            "variability_metrics": "Matching accuracy (%) for each method; sensitivity analysis across SVCCA truncation thresholds (Appendix F.2).",
            "variability_results": "Table 2 shows low matching accuracies for CCA/SVCCA/PWCCA: CCA (mean rho) 1.4%, CCA (R_CCA^2) 10.6%, SVCCA (mean rho) 9.9%, SVCCA (R_CCA^2) 15.1%, PWCCA 11.1% (averaged over 10 networks). SVCCA truncation thresholds tried did not reveal layer structure in these CNN experiments.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Matching accuracy and experiments across truncation thresholds/hyperparameters",
            "reproducibility_results": "CCA-based methods generally failed the proposed sanity check for CNNs: they did not reliably identify corresponding layers across random initializations and were sensitive to hyperparameters (e.g., SVCCA threshold).",
            "reproducibility_challenges": "Invariance to arbitrary invertible linear transforms makes some statistics ill-defined in overparameterized regimes (p &gt;= n); CCA is sensitive to perturbations when condition numbers are large; need to choose truncation thresholds or regularization parameters, which affects results and interpretability.",
            "mitigation_methods": "SVCCA (truncate small singular vectors), PWCCA (projection-weighted averaging), canonical-ridge regularization (interpolates between CCA, regression, and CKA), and normalization strategies; but these add hyperparameters and can bias outcomes.",
            "mitigation_effectiveness": "Mitigations (SVCCA truncation, PWCCA) provided modest improvements but remained substantially worse than CKA in the CNN layer-matching task; SVCCA thresholds tested failed to reveal correct layer structure (Appendix F.2).",
            "comparison_with_without_controls": true,
            "number_of_runs": "10 networks",
            "key_findings": "CCA-based similarity metrics are fragile in practice for these representation-comparison tasks: they are sensitive to numerical issues and hyperparameters, perform poorly at identifying corresponding layers across initializations, and can produce misleading invariance in overparameterized settings.",
            "uuid": "e656.2",
            "source_info": {
                "paper_title": "Similarity of Neural Network Representations Revisited",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "BatchNorm / training-scale sensitivity",
            "name_full": "Effects of batch normalization and feature scaling on representation similarity and reproducibility",
            "brief_description": "Presence or absence of batch normalization and non-isotropic feature scaling change training dynamics and feature scales; CKA captures these differences while some methods (e.g., CCA) that are invariant to non-isotropic scaling may miss them.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Plain networks with and without batch normalization (All-CNN-C variants)",
            "model_size": null,
            "scientific_domain": "machine learning / representation analysis",
            "experimental_task": "Compare representations of networks trained with and without batch normalization and quantify similarity using CKA and other metrics.",
            "variability_sources": "Presence vs absence of batch normalization (changes internal covariate shift, feature scale), resulting differences in feature scaling and training trajectories.",
            "variability_measured": true,
            "variability_metrics": "CKA similarity scores between networks; per-layer classifier accuracy.",
            "variability_results": "Appendix F.4 reports networks with batch normalization averaged 93.9% accuracy and networks without batch normalization averaged 91.5% accuracy; CKA shows that the largest representational difference is at the last convolutional layer, but overall representations remain similar.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "CKA similarity matrices and per-layer classification accuracy",
            "reproducibility_results": "Networks with and without batch normalization learn largely similar representations (as measured by CKA) though last-layer differences exist; CCA (invariant to non-isotropic scaling) would not capture some scale-induced differences that CKA does.",
            "reproducibility_challenges": "Scale differences induced by batch normalization can mask or alter representational similarity if using methods invariant to non-isotropic scaling (e.g., CCA).",
            "mitigation_methods": "Use similarity metrics that are sensitive to feature-scale differences of interest (CKA) and tune hyperparameters separately for different training regimes; report per-layer analyses and classifier accuracy.",
            "mitigation_effectiveness": "CKA detected differences (largest at last conv layer) and overall similarity; no numeric reduction-of-variance measure reported, but separate hyperparameter tuning produced comparably high accuracies for both regimes.",
            "comparison_with_without_controls": true,
            "number_of_runs": "multiple networks (reported averages)",
            "key_findings": "Batch normalization affects feature scales and training dynamics; CKA is able to detect scale-related differences that CCA would miss, and overall networks with and without batch normalization still learn similar representations except at some later layers.",
            "uuid": "e656.3",
            "source_info": {
                "paper_title": "Similarity of Neural Network Representations Revisited",
                "publication_date_yy_mm": "2019-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SVCCA: Singular vector canonical correlation analysis for deep learning dynamics and interpretability",
            "rating": 2
        },
        {
            "paper_title": "Insights on representational similarity in neural networks with canonical correlation",
            "rating": 2
        },
        {
            "paper_title": "Convergent learning: Do different neural networks learn the same representations?",
            "rating": 2
        },
        {
            "paper_title": "Towards understanding learning representations: To what extent do different neural networks learn the same representation",
            "rating": 2
        },
        {
            "paper_title": "Neural tangent kernel: Convergence and generalization in neural networks",
            "rating": 1
        }
    ],
    "cost": 0.01998075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Similarity of Neural Network Representations Revisited</h1>
<p>Simon Kornblith ${ }^{1}$ Mohammad Norouzi ${ }^{1}$ Honglak Lee ${ }^{1}$ Geoffrey Hinton ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.</p>
<h2>1. Introduction</h2>
<p>Across a wide range of machine learning tasks, deep neural networks enable learning powerful feature representations automatically from data. Despite impressive empirical advances of deep neural networks in solving various tasks, the problem of understanding and characterizing the neural network representations learned from data remains relatively under-explored. Previous work (e.g. Advani \&amp; Saxe (2017); Amari et al. (2018); Saxe et al. (2014)) has made progress in understanding the theoretical dynamics of the neural network training process. These studies are insightful, but fundamentally limited, because they ignore the complex interaction between the training dynamics and structured data. A window into the network's representation can provide more information about the interaction between machine learning algorithms and data than the value of the loss function alone.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>This paper investigates the problem of measuring similarities between deep neural network representations. An effective method for measuring representational similarity could help answer many interesting questions, including: (1) Do deep neural networks with the same architecture trained from different random initializations learn similar representations? (2) Can we establish correspondences between layers of different network architectures? (3) How similar are the representations learned using the same network architecture from different datasets?</p>
<p>We build upon previous studies investigating similarity between the representations of neural networks (Laakso \&amp; Cottrell, 2000; Li et al., 2015; Raghu et al., 2017; Morcos et al., 2018; Wang et al., 2018). We are also inspired by the extensive neuroscience literature that uses representational similarity analysis (Kriegeskorte et al., 2008a; Edelman, 1998) to compare representations across brain areas (Haxby et al., 2001; Freiwald \&amp; Tsao, 2010), individuals (Connolly et al., 2012), species (Kriegeskorte et al., 2008b), and behaviors (Elsayed et al., 2016), as well as between brains and neural networks (Yamins et al., 2014; Khaligh-Razavi \&amp; Kriegeskorte, 2014; Sussillo et al., 2015).</p>
<p>Our key contributions are summarized as follows:</p>
<ul>
<li>We discuss the invariance properties of similarity indexes and their implications for measuring similarity of neural network representations.</li>
<li>We motivate and introduce centered kernel alignment (CKA) as a similarity index and analyze the relationship between CKA, linear regression, canonical correlation analysis (CCA), and related methods (Raghu et al., 2017; Morcos et al., 2018).</li>
<li>We show that CKA is able to determine the correspondence between the hidden layers of neural networks trained from different random initializations and with different widths, scenarios where previously proposed similarity indexes fail.</li>
<li>We verify that wider networks learn more similar representations, and show that the similarity of early layers saturates at fewer channels than later layers. We demonstrate that early layers, but not later layers, learn similar representations on different datasets.</li>
</ul>
<h2>Problem Statement</h2>
<p>Let $X \in \mathbb{R}^{n \times p_{1}}$ denote a matrix of activations of $p_{1}$ neurons for $n$ examples, and $Y \in \mathbb{R}^{n \times p_{2}}$ denote a matrix of activations of $p_{2}$ neurons for the same $n$ examples. We assume that these matrices have been preprocessed to center the columns. Without loss of generality we assume that $p_{1} \leq p_{2}$. We are concerned with the design and analysis of a scalar similarity index $s(X, Y)$ that can be used to compare representations within and across neural networks, in order to help visualize and understand the effect of different factors of variation in deep learning.</p>
<h2>2. What Should Similarity Be Invariant To?</h2>
<p>This section discusses the invariance properties of similarity indexes and their implications for measuring similarity of neural network representations. We argue that both intuitive notions of similarity and the dynamics of neural network training call for a similarity index that is invariant to orthogonal transformation and isotropic scaling, but not invertible linear transformation.</p>
<h3>2.1. Invariance to Invertible Linear Transformation</h3>
<p>A similarity index is invariant to invertible linear transformation if $s(X, Y)=s(X A, Y B)$ for any full rank $A$ and $B$. If activations $X$ are followed by a fully-connected layer $f(X)=\sigma(X W+\beta)$, then transforming the activations by a full rank matrix $A$ as $X^{\prime}=X A$ and transforming the weights by the inverse $A^{-1}$ as $W^{\prime}=A^{-1} W$ preserves the output of $f(X)$. This transformation does not appear to change how the network operates, so intuitively, one might prefer a similarity index that is invariant to invertible linear transformation, as argued by Raghu et al. (2017).</p>
<p>However, a limitation of invariance to invertible linear transformation is that any invariant similarity index gives the same result for any representation of width greater than or equal to the dataset size, i.e. $p_{2} \geq n$. We provide a simple proof in Appendix A.
Theorem 1. Let $X$ and $Y$ be $n \times p$ matrices. Suppose $s$ is invariant to invertible linear transformation in the first argument, i.e. $s(X, Z)=s(X A, Z)$ for arbitrary $Z$ and any $A$ with $\operatorname{rank}(A)=p$. If $\operatorname{rank}(X)=\operatorname{rank}(Y)=n$, then $s(X, Z)=s(Y, Z)$.</p>
<p>There is thus a practical problem with invariance to invertible linear transformation: Some neural networks, especially convolutional networks, have more neurons in some layers than there are examples the training dataset (Springenberg et al., 2015; Lee et al., 2018; Zagoruyko \&amp; Komodakis, 2016). It is somewhat unnatural that a similarity index could require more examples than were used for training.</p>
<p>A deeper issue is that neural network training is not invari-
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. First principal components of representations of networks trained from different random initializations are similar. Each example from the CIFAR-10 test set is shown as a dot colored according to the value of the first two principal components of an intermediate layer of one network (left) and plotted on the first two principal components of the same layer of an architecturally identical network trained from a different initialization (right).
ant to arbitrary invertible linear transformation of inputs or activations. Even in the linear case, gradient descent converges first along the eigenvectors corresponding to the largest eigenvalues of the input covariance matrix (LeCun et al., 1991), and in cases of overparameterization or early stopping, the solution reached depends on the scale of the input. Similar results hold for gradient descent training of neural networks in the infinite width limit (Jacot et al., 2018). The sensitivity of neural networks training to linear transformation is further demonstrated by the popularity of batch normalization (Ioffe \&amp; Szegedy, 2015).</p>
<p>Invariance to invertible linear transformation implies that the scale of directions in activation space is irrelevant. Empirically, however, scale information is both consistent across networks and useful across tasks. Neural networks trained from different random initializations develop representations with similar large principal components, as shown in Figure 1. Consequently, Euclidean distances between examples, which depend primarily upon large principal components, are similar across networks. These distances are meaningful, as demonstrated by the success of perceptual loss and style transfer (Gatys et al., 2016; Johnson et al., 2016; Dumoulin et al., 2017). A similarity index that is invariant to invertible linear transformation ignores this aspect of the representation, and assigns the same score to networks that match only in large principal components or networks that match only in small principal components.</p>
<h3>2.2. Invariance to Orthogonal Transformation</h3>
<p>Rather than requiring invariance to any invertible linear transformation, one could require a weaker condition; invariance to orthogonal transformation, i.e. $s(X, Y)=$ $s(X U, Y V)$ for full-rank orthonormal matrices $U$ and $V$ such that $U^{\mathrm{T}} U=I$ and $V^{\mathrm{T}} V=I$.</p>
<p>Indexes invariant to orthogonal transformations do not share the limitations of indexes invariant to invertible linear transformation. When $p_{2}&gt;n$, indexes invariant to orthogonal transformation remain well-defined. Moreover, orthogonal transformations preserve scalar products and Euclidean distances between examples.</p>
<p>Invariance to orthogonal transformation seems desirable for neural networks trained by gradient descent. Invariance to orthogonal transformation implies invariance to permutation, which is needed to accommodate symmetries of neural networks (Chen et al., 1993; Orhan \&amp; Pitkow, 2018). In the linear case, orthogonal transformation of the input does not affect the dynamics of gradient descent training (LeCun et al., 1991), and for neural networks initialized with rotationally symmetric weight distributions, e.g. i.i.d. Gaussian weight initialization, training with fixed orthogonal transformations of activations yields the same distribution of training trajectories as untransformed activations, whereas an arbitrary linear transformation would not.</p>
<p>Given a similarity index $s(\cdot, \cdot)$ that is invariant to orthogonal transformation, one can construct a similarity index $s^{\prime}(\cdot, \cdot)$ that is invariant to any invertible linear transformation by first orthonormalizing the columns of $X$ and $Y$, and then applying $s(\cdot, \cdot)$. Given thin QR decompositions $X=Q_{A} R_{A}$ and $Y=Q_{B} R_{B}$ one can construct a similarity index $s^{\prime}(X, Y)=s\left(Q_{X}, Q_{Y}\right)$, where $s^{\prime}(\cdot, \cdot)$ is invariant to invertible linear transformation because orthonormal bases with the same span are related to each other by orthonormal transformation (see Appendix B).</p>
<h3>2.3. Invariance to Isotropic Scaling</h3>
<p>We expect similarity indexes to be invariant to isotropic scaling, i.e. $s(X, Y)=s(\alpha X, \beta Y)$ for any $\alpha, \beta \in \mathbb{R}^{+}$. That said, a similarity index that is invariant to both orthogonal transformation and non-isotropic scaling, i.e. rescaling of individual features, is invariant to any invertible linear transformation. This follows from the existence of the singular value decomposition of the transformation matrix. Generally, we are interested in similarity indexes that are invariant to isotropic but not necessarily non-isotropic scaling.</p>
<h2>3. Comparing Similarity Structures</h2>
<p>Our key insight is that instead of comparing multivariate features of an example in the two representations (e.g. via regression), one can first measure the similarity between every pair of examples in each representation separately, and then compare the similarity structures. In neuroscience, such matrices representing the similarities between examples are called representational similarity matrices (Kriegeskorte et al., 2008a). We show below that, if we use an inner product to measure similarity, the similarity between repre- sentational similarity matrices reduces to another intuitive notion of pairwise feature similarity.</p>
<p>Dot Product-Based Similarity. A simple formula relates dot products between examples to dot products between features:</p>
<p>$$
\left\langle\operatorname{vec}\left(X X^{\mathrm{T}}\right), \operatorname{vec}\left(Y Y^{\mathrm{T}}\right)\right\rangle=\operatorname{tr}\left(X X^{\mathrm{T}} Y Y^{\mathrm{T}}\right)=\left|Y^{\mathrm{T}} X\right|_{\mathrm{F}}^{2}
$$</p>
<p>The elements of $X X^{\mathrm{T}}$ and $Y Y^{\mathrm{T}}$ are dot products between the representations of the $i^{\text {th }}$ and $j^{\text {th }}$ examples, and indicate the similarity between these examples according to the respective networks. The left-hand side of (1) thus measures the similarity between the inter-example similarity structures. The right-hand side yields the same result by measuring the similarity between features from $X$ and $Y$, by summing the squared dot products between every pair.</p>
<p>Hilbert-Schmidt Independence Criterion. Equation 1 implies that, for centered $X$ and $Y$ :</p>
<p>$$
\frac{1}{(n-1)^{2}} \operatorname{tr}\left(X X^{\mathrm{T}} Y Y^{\mathrm{T}}\right)=\left|\operatorname{cov}\left(X^{\mathrm{T}}, Y^{\mathrm{T}}\right)\right|_{\mathrm{F}}^{2}
$$</p>
<p>The Hilbert-Schmidt Independence Criterion (Gretton et al., 2005) generalizes Equations 1 and 2 to inner products from reproducing kernel Hilbert spaces, where the squared Frobenius norm of the cross-covariance matrix becomes the squared Hilbert-Schmidt norm of the cross-covariance operator. Let $K_{i j}=k\left(\mathbf{x}<em j="j">{i}, \mathbf{x}</em>}\right)$ and $L_{i j}=l\left(\mathbf{y<em j="j">{i}, \mathbf{y}</em>\right)$ where $k$ and $l$ are two kernels. The empirical estimator of HSIC is:</p>
<p>$$
\operatorname{HSIC}(K, L)=\frac{1}{(n-1)^{2}} \operatorname{tr}(K H L H)
$$</p>
<p>where $H$ is the centering matrix $H_{n}=I_{n}-\frac{1}{n} \mathbf{1 1}^{\mathrm{T}}$. For linear kernels $k(\mathbf{x}, \mathbf{y})=l(\mathbf{x}, \mathbf{y})=\mathbf{x}^{\mathrm{T}} \mathbf{y}$, HSIC yields (2).</p>
<p>Gretton et al. (2005) originally proposed HSIC as a test statistic for determining whether two sets of variables are independent. They prove that the empirical estimator converges to the population value at a rate of $1 / \sqrt{n}$, and Song et al. (2007) provide an unbiased estimator. When $k$ and $l$ are universal kernels, $\mathrm{HSIC}=0$ implies independence, but HSIC is not an estimator of mutual information. HSIC is equivalent to maximum mean discrepancy between the joint distribution and the product of the marginal distributions, and HSIC with a specific kernel family is equivalent to distance covariance (Sejdinovic et al., 2013).</p>
<p>Centered Kernel Alignment. HSIC is not invariant to isotropic scaling, but it can be made invariant through normalization. This normalized index is known as centered kernel alignment (Cortes et al., 2012; Cristianini et al., 2002):</p>
<p>$$
\operatorname{CKA}(K, L)=\frac{\operatorname{HSIC}(K, L)}{\sqrt{\operatorname{HSIC}(K, K) \operatorname{HSIC}(L, L)}}
$$</p>
<p>|  |  | Invariant to |  |  |
| Similarity Index | Formula | Invertible Linear Transform | Orthogonal Transform | Isotropic Scaling |
| --- | --- | --- | --- | --- |
| Linear Reg. $\left(R_{\mathrm{LR}}^{2}\right)$ | $\left|Q_{Y}^{\mathrm{T}} X\right|<em _mathrm_F="\mathrm{F">{\mathrm{F}}^{2} /|X|</em>$ | $Y$ only | $\checkmark$ | $\checkmark$ |
| CCA $\left(R_{\mathrm{CCA}}^{2}\right)$ | $\left|Q_{Y}^{\mathrm{T}} Q_{X}\right|}}^{2<em 1="1">{\mathrm{F}}^{2} / p</em>$ | $\checkmark$ | $\checkmark$ | $\checkmark$ |
| CCA $\left(\bar{\rho}<em Y="Y">{\mathrm{CCA}}\right)$ | $\left|Q</em>\right|}^{\mathrm{T}} Q_{X<em X="X">{<em>} / p_{1}$ | $\checkmark$ | $\checkmark$ | $\checkmark$ |
| SVCCA $\left(R_{\mathrm{SVCCA}}^{2}\right)$ | $\left|\left(U_{Y} T_{Y}\right)^{\mathrm{T}} U_{X} T_{X}\right|<em X="X">{\mathrm{F}}^{2} / \min \left(\left|T</em>\right|<em Y="Y">{\mathrm{F}}^{2},\left|T</em>\right|<em _SVCCA="{SVCCA" _text="\text">{\mathrm{F}}^{2}\right)$ | If same subspace kept | $\checkmark$ | $\checkmark$ |
| SVCCA $\left(\bar{\rho}</em>\right|_{}}\right)$ | $\left|\left(U_{Y} T_{Y}\right)^{\mathrm{T}} U_{X} T_{X</em>} / \min \left(\left|T</em>\right|<em Y="Y">{\mathrm{F}}^{2},\left|T</em>\right|<em i="1">{\mathrm{F}}^{2}\right)$ | If same subspace kept | $\checkmark$ | $\checkmark$ |
| PWCCA | $\sum</em> /\left|\alpha\right|}^{p_{1}} \alpha_{i} \rho_{i<em i="i">{1}, \alpha</em>}=\sum_{j}\left|\left\langle\mathbf{h<em j="j">{i}, \mathbf{x}</em>$ | $\checkmark$ |
| Linear HSIC | $\left|Y^{\mathrm{T}} X\right|}\right\rangle\right|$ | $\boldsymbol{X}$ | $\boldsymbol{X<em _mathrm_F="\mathrm{F">{\mathrm{F}}^{2} /(n-1)^{2}$ | $\boldsymbol{X}$ | $\checkmark$ | $\boldsymbol{X}$ |
| Linear CKA | $\left|Y^{\mathrm{T}} X\right|</em> X\right|}}^{2} /\left(\left|X^{\mathrm{T}<em _mathrm_F="\mathrm{F">{\mathrm{F}}\left|Y^{\mathrm{T}} Y\right|</em>$ | $\checkmark$ | $\checkmark$ |
| RBF CKA | $\operatorname{tr}(K H L H) / \sqrt{\operatorname{tr}(K H K H) \operatorname{tr}(L H L H)}$ | $\boldsymbol{X}$ | $\checkmark$ | $\checkmark^{*}$ |}}\right)$ | $\boldsymbol{X</p>
<p>Table 1. Summary of similarity methods investigated. $Q_{X}$ and $Q_{Y}$ are orthonormal bases for the columns of $X$ and $Y . U_{X}$ and $U_{Y}$ are the left-singular vectors of $X$ and $Y$ sorted in descending order according to the corresponding singular vectors. $|\cdot|_{<em>}$ denotes the nuclear norm. $T_{X}$ and $T_{Y}$ are truncated identity matrices that select left-singular vectors such that the cumulative variance explained reaches some threshold. For RBF CKA, $K$ and $L$ are kernel matrices constructed by evaluating the RBF kernel between the examples as in Section 3, and $H$ is the centering matrix $H_{n}=I_{n}-\frac{1}{n} \mathbf{1 1}^{T}$. See Appendix C for more detail about each technique.
</em>Invariance of RBF CKA to isotropic scaling depends on the procedure used to select the RBF kernel bandwidth parameter. In our experiments, we selected the bandwidth as a fraction of the median distance, which ensures that the similarity index is invariant to isotropic scaling.</p>
<p>For a linear kernel, CKA is equivalent to the RV coefficient (Robert \&amp; Escoufier, 1976) and to Tucker’s congruence coefficient (Tucker, 1951; Lorenzo-Seva \&amp; Ten Berge, 2006).</p>
<p>Kernel Selection. Below, we report results of CKA with a linear kernel and the RBF kernel $k\left(\mathbf{x}<em j="j">{i}, \mathbf{x}</em>}\right)=\exp \left(-\left|\mathbf{x<em j="j">{i}-\right.\right.$ $\left.\left.\mathbf{x}</em>\right)\right)$. For the RBF kernel, there are several possible strategies for selecting the bandwidth $\sigma$, which controls the extent to which similarity of small distances is emphasized over large distances. We set $\sigma$ as a fraction of the median distance between examples. In practice, we find that RBF and linear kernels give similar results across most experiments, so we use linear CKA unless otherwise specified. Our framework extends to any valid kernel, including kernels equivalent to neural networks (Lee et al., 2018; Jacot et al., 2018; Garriga-Alonso et al., 2019; Novak et al., 2019).}\right|_{2}^{2} /\left(2 \sigma^{2</p>
<h2>4. Related Similarity Indexes</h2>
<p>In this section, we briefly review linear regression, canonical correlation, and other related methods in the context of measuring similarity between neural network representations. We let $Q_{X}$ and $Q_{Y}$ represent any orthonormal bases for the columns of $X$ and $Y$, i.e. $Q_{X}=X\left(X^{\mathrm{T}} X\right)^{-1 / 2}$, $Q_{Y}=Y\left(Y^{\mathrm{T}} Y\right)^{-1 / 2}$ or orthogonal transformations thereof. Table 1 summarizes the formulae and invariance properties of the indexes used in experiments. For a comprehensive general review of linear indexes for measuring multivariate similarity, see Ramsay et al. (1984).</p>
<p>Linear Regression. A simple way to relate neural network representations is via linear regression. One can fit every feature in $X$ as a linear combination of features from $Y$. A suitable summary statistic is the total fraction of variance explained by the fit:</p>
<p>$$
R_{\mathrm{LR}}^{2}=1-\frac{\min <em _mathrm_F="\mathrm{F">{B}|X-Y B|</em>{|X|}}^{2}<em Y="Y">{\mathrm{F}}^{2}}=\frac{\left|Q</em> X\right|}^{\mathrm{T}<em _mathrm_F="\mathrm{F">{\mathrm{F}}^{2}}{|X|</em>
$$}}^{2}</p>
<p>We are unaware of any application of linear regression to measuring similarity of neural network representations, although Romero et al. (2015) used a least squares loss between activations of two networks to encourage thin and deep "student" networks to learn functions similar to wide and shallow "teacher" networks.</p>
<p>Canonical Correlation Analysis (CCA). Canonical correlation finds bases for two matrices such that, when the original matrices are projected onto these bases, the correlation is maximized. For $1 \leq i \leq p_{1}$, the $i^{\text {th }}$ canonical correlation coefficient $\rho_{i}$ is given by:</p>
<p>$$
\begin{aligned}
&amp; \rho_{i}=\max <em X="X">{\mathbf{w}</em>}^{i}, \mathbf{w<em X="X">{Y}^{i}} \operatorname{corr}\left(X \mathbf{w}</em>}^{i}, Y \mathbf{w<em j_i="j&lt;i">{Y}^{i}\right) \
&amp; \text { subject to } \forall</em>} X \mathbf{w<em X="X">{X}^{i} \perp X \mathbf{w}</em> \
&amp; \forall_{j&lt;i} Y \mathbf{w}}^{j<em Y="Y">{Y}^{i} \perp Y \mathbf{w}</em> .
\end{aligned}
$$}^{j</p>
<p>The vectors $\mathbf{w}<em 1="1">{X}^{i} \in \mathbb{R}^{p</em>}}$ and $\mathbf{w<em 2="2">{Y}^{i} \in \mathbb{R}^{p</em>}}$ that maximize $\rho_{i}$ are the canonical weights, which transform the original data into canonical variables $X \mathbf{w<em Y="Y">{X}^{i}$ and $Y \mathbf{w}</em>$. The constraints in (6) enforce orthogonality of the canonical variables.}^{i</p>
<p>For the purpose of this work, we consider two summary</p>
<p>statistics of the goodness of fit of CCA:</p>
<p>$$
\begin{aligned}
R_{\mathrm{CCA}}^{2} &amp; =\frac{\sum_{i=1}^{p_{1}} \rho_{i}^{2}}{p_{1}}=\frac{\left|Q_{Y}^{\mathrm{T}} Q_{X}\right|<em 1="1">{\mathrm{F}}^{2}}{p</em> \
\bar{\rho}}<em i="1">{\mathrm{CCA}} &amp; =\frac{\sum</em>\right|}^{p_{1}} \rho_{i}}{p_{1}}=\frac{\left|Q_{Y}^{\mathrm{T}} Q_{X<em 1="1">{*}}{p</em>
\end{aligned}
$$}</p>
<p>where $|\cdot|<em _mathrm_CCA="\mathrm{CCA">{*}$ denotes the nuclear norm. The mean squared CCA correlation $R</em>$ was previously used to measure similarity between neural network representations in Raghu et al. (2017).}}^{2}$ is also known as Yanai's GCD measure (Ramsay et al., 1984), and several statistical packages report the sum of the squared canonical correlations $p_{1} R_{\mathrm{CCA}}^{2}=\sum_{i=1}^{p_{1}} \rho_{i}^{2}$ under the name Pillai's trace (SAS Institute, 2015; StataCorp, 2015). The mean CCA correlation $\bar{\rho}_{\text {CCA }</p>
<p>SVCCA. CCA is sensitive to perturbation when the condition number of $X$ or $Y$ is large (Golub \&amp; Zha, 1995). To improve robustness, singular vector CCA (SVCCA) performs CCA on truncated singular value decompositions of $X$ and $Y$ (Raghu et al., 2017; Mroueh et al., 2015; Kuss \&amp; Graepel, 2003). As formulated in Raghu et al. (2017), SVCCA keeps enough principal components of the input matrices to explain a fixed proportion of the variance, and drops remaining components. Thus, it is invariant to invertible linear transformation only if the retained subspace does not change.</p>
<p>Projection-Weighted CCA. Morcos et al. (2018) propose a different strategy to reduce the sensitivity of CCA to perturbation, which they term "projection-weighted canonical correlation" (PWCCA):</p>
<p>$$
\rho_{\mathrm{PW}}=\frac{\sum_{i=1}^{c} \alpha_{i} \rho_{i}}{\sum_{i=1} \alpha_{i}} \quad \alpha_{i}=\sum_{j}\left|\left\langle\mathbf{h}<em j="j">{i}, \mathbf{x}</em>\right\rangle\right|
$$</p>
<p>where $\mathbf{x}<em i="i">{j}$ is the $j^{\text {th }}$ column of $X$, and $\mathbf{h}</em>$ canonical coordinate frame. As we show in Appendix C.3, PWCCA is closely related to linear regression, since:}=X \mathbf{w}_{X}^{i}$ is the vector of canonical variables formed by projecting $X$ to the $i^{\text {th }</p>
<p>$$
R_{\mathrm{LR}}^{2}=\frac{\sum_{i=1}^{c} \alpha_{i}^{\prime} \rho_{i}^{2}}{\sum_{i=1} \alpha_{i}^{\prime}} \quad \alpha_{i}^{\prime}=\sum_{j}\left\langle\mathbf{h}<em j="j">{i}, \mathbf{x}</em>
$$}\right\rangle^{2</p>
<p>Neuron Alignment Procedures. Other work has studied alignment between individual neurons, rather than alignment between subspaces. Li et al. (2015) examined correlation between the neurons in different neural networks, and attempt to find a bipartite match or semi-match that maximizes the sum of the correlations between the neurons, and then to measure the average correlations. Wang et al. (2018) proposed to search for subsets of neurons $\tilde{X} \subset X$ and $\tilde{Y} \subset Y$ such that, to within some tolerance, every neuron in $\tilde{X}$ can be represented by a linear combination of neurons from $\tilde{Y}$ and vice versa. They found that the maximum matching subsets are very small for intermediate layers.</p>
<p>Mutual Information. Among non-linear measures, one candidate is mutual information, which is invariant not only to invertible linear transformation, but to any invertible transformation. Li et al. (2015) previously used mutual information to measure neuronal alignment. In the context of comparing representations, we believe mutual information is not useful. Given any pair of representations produced by deterministic functions of the same input, mutual information between either and the input must be at least as large as mutual information between the representations. Moreover, in fully invertible neural networks (Dinh et al., 2017; Jacobsen et al., 2018), the mutual information between any two layers is equal to the entropy of the input.</p>
<h2>5. Linear CKA versus CCA and Regression</h2>
<p>Linear CKA is closely related to CCA and linear regression. If $X$ and $Y$ are centered, then $Q_{X}$ and $Q_{Y}$ are also centered, so:</p>
<p>$$
R_{\mathrm{CCA}}^{2}=\operatorname{CKA}\left(Q_{X} Q_{X}^{\mathrm{T}}, Q_{Y} Q_{Y}^{\mathrm{T}}\right) \sqrt{\frac{p_{2}}{p_{1}}}
$$</p>
<p>When performing the linear regression fit of $X$ with design matrix $Y, R_{\mathrm{LR}}^{2}=\left|Q_{Y}^{\mathrm{T}} X\right|<em F="F">{F}^{2} /|X|</em>$, so:}^{2</p>
<p>$$
R_{\mathrm{LR}}^{2}=\operatorname{CKA}\left(X X^{\mathrm{T}}, Q_{Y} Q_{Y}^{\mathrm{T}}\right) \frac{\sqrt{p_{1}}\left|X^{\mathrm{T}} X\right|<em _mathrm_F="\mathrm{F">{\mathrm{F}}}{|X|</em>
$$}}^{2}</p>
<p>When might we prefer linear CKA over CCA? One way to show the difference is to rewrite $X$ and $Y$ in terms of their singular value decompositions $X=U_{X} \Sigma_{X} V_{X}^{2}, Y=$ $U_{Y} \Sigma_{Y} V_{Y}^{\mathrm{T}}$. Let the $i^{\text {th }}$ eigenvector of $X X^{\mathrm{T}}$ (left-singular vector of $X$ ) be indexed as $\mathbf{u}<em _mathrm_CCA="\mathrm{CCA">{X}^{i}$. Then $R</em>$ is:}}^{2</p>
<p>$$
R_{\mathrm{CCA}}^{2}=\left|U_{Y}^{\mathrm{T}} U_{X}\right|<em 1="1">{\mathrm{F}}^{2} / p</em>}=\sum_{i=1}^{p_{1}} \sum_{j=1}^{p_{2}}\left\langle\mathbf{u<em Y="Y">{X}^{i}, \mathbf{u}</em>
$$}^{j}\right\rangle^{2} / p_{1</p>
<p>Let the $i^{\text {th }}$ eigenvalue of $X X^{\mathrm{T}}$ (squared singular value of $X)$ be indexed as $\lambda_{X}^{i}$. Linear CKA can be written as:</p>
<p>$$
\begin{aligned}
\operatorname{CKA}\left(X X^{\mathrm{T}}, Y Y^{\mathrm{T}}\right) &amp; =\frac{\left|Y^{\mathrm{T}} X\right|<em _mathrm_F="\mathrm{F">{\mathrm{F}}^{2}}{\left|X^{\mathrm{T}} X\right|</em> Y\right|}}\left|Y^{\mathrm{T}<em i="1">{\mathrm{F}}} \
&amp; =\frac{\sum</em>}^{p_{1}} \sum_{j=1}^{p_{2}} \lambda_{X}^{i} \lambda_{Y}^{j}\left\langle\mathbf{u<em Y="Y">{X}^{i}, \mathbf{u}</em>
\end{aligned}
$$}^{j}\right\rangle^{2}}{\sqrt{\sum_{i=1}^{p_{1}}\left(\lambda_{X}^{i}\right)^{2}} \sqrt{\sum_{j=1}^{p_{2}}\left(\lambda_{Y}^{j}\right)^{2}}</p>
<p>Linear CKA thus resembles CCA weighted by the eigenvalues of the corresponding eigenvectors, i.e. the amount of variance in $X$ or $Y$ that each explains. SVCCA (Raghu et al., 2017) and projection-weighted CCA (Morcos et al., 2018) were also motivated by the idea that eigenvectors that correspond to small eigenvalues are less important, but</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. CKA reveals consistent relationships between layers of CNNs trained with different random initializations, whereas CCA, linear regression, and SVCCA do not. For linear regression, which is asymmetric, we plot $R^{2}$ for the fit of the layer on the x-axis with the layer on the y-axis. Results are averaged over 10 networks on the CIFAR-10 training set. See Table 2 for a numerical summary.
linear CKA incorporates this weighting symmetrically and can be computed without a matrix decomposition.</p>
<p>Comparison of (13) and (14) immediately suggests the possibility of alternative weightings of scalar products between eigenvectors. Indeed, as we show in Appendix D.1, the similarity index induced by "canonical ridge" regularized CCA (Vinod, 1976), when appropriately normalized, interpolates between $R_{\mathrm{CCA}}^{2}$, linear regression, and linear CKA.</p>
<h2>6. Results</h2>
<h3>6.1. A Sanity Check for Similarity Indexes</h3>
<p>We propose a simple sanity check for similarity indexes: Given a pair of architecturally identical networks trained from different random initializations, for each layer in the first network, the most similar layer in the second network should be the architecturally corresponding layer. We train 10 networks and, for each layer of each network, we compute the accuracy with which we can find the corresponding layer in each of the other networks by maximum similarity. We then average the resulting accuracies. We compare CKA with CCA, SVCCA, PWCCA, and linear regression.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Index</th>
<th style="text-align: right;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CCA $(\bar{\rho})$</td>
<td style="text-align: right;">1.4</td>
</tr>
<tr>
<td style="text-align: left;">CCA $\left(R_{\mathrm{CCA}}^{2}\right)$</td>
<td style="text-align: right;">10.6</td>
</tr>
<tr>
<td style="text-align: left;">SVCCA $(\bar{\rho})$</td>
<td style="text-align: right;">9.9</td>
</tr>
<tr>
<td style="text-align: left;">SVCCA $\left(R_{\mathrm{CCA}}^{2}\right)$</td>
<td style="text-align: right;">15.1</td>
</tr>
<tr>
<td style="text-align: left;">PWCCA</td>
<td style="text-align: right;">11.1</td>
</tr>
<tr>
<td style="text-align: left;">Linear Reg.</td>
<td style="text-align: right;">45.4</td>
</tr>
<tr>
<td style="text-align: left;">Linear HSIC</td>
<td style="text-align: right;">22.2</td>
</tr>
<tr>
<td style="text-align: left;">CKA (Linear)</td>
<td style="text-align: right;">$\mathbf{9 9 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">CKA (RBF 0.2)</td>
<td style="text-align: right;">80.6</td>
</tr>
<tr>
<td style="text-align: left;">CKA (RBF 0.4)</td>
<td style="text-align: right;">$\mathbf{9 9 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">CKA (RBF 0.8)</td>
<td style="text-align: right;">$\mathbf{9 9 . 3}$</td>
</tr>
</tbody>
</table>
<p>Table 2. Accuracy of identifying corresponding layers based on maximum similarity for 10 architecturally identical 10-layer CNNs trained from different initializations, with logits layers excluded. For SVCCA, we used a truncation threshold of 0.99 as recommended in <em>Raghu et al. (2017)</em>. For asymmetric indexes (PWCCA and linear regression) we symmetrized the similarity as $S+S^{\mathrm{T}}$. CKA RBF kernel parameters reflect the fraction of the median Euclidean distance used as $\sigma$. Results not significantly different from the best result are bold-faced ( $p&lt;0.05$, jackknife z-test).</p>
<p>We first investigate a simple VGG-like convolutional network based on All-CNN-C (Springenberg et al., 2015) (see Appendix E) on CIFAR-10. Figure 2 and Table 2 show that CKA passes our sanity check, but other methods perform substantially worse. For SVCCA, we experimented with a range of truncation thresholds, but no threshold revealed the layer structure (Appendix F.2); our results are consistent with those in Appendix E of <em>Raghu et al. (2017)</em>.</p>
<p>We also investigate Transformer networks, where all layers are of equal width. In Appendix F.1, we show similarity between the 12 sublayers of the encoders of Transformer models <em>(Vaswani et al., 2017)</em> trained from different random initializations. All similarity indexes achieve non-trivial accuracy and thus pass the sanity check, although RBF CKA and $R_{\mathrm{CCA}}^{2}$ performed slightly better than other methods. However, we found that there are differences in feature scale between representations of feed-forward network and self-attention sublayers that CCA does not capture because it is invariant to non-isotropic scaling.</p>
<h3>6.2. Using CKA to Understand Network Architectures</h3>
<p>CKA can reveal pathology in neural networks representations. In Figure 3, we show CKA between layers of individual CNNs with different depths, where layers are repeated 2, 4, or 8 times. Doubling depth improved accuracy, but greater multipliers hurt accuracy. At 8x depth, CKA indicates that representations of more than half of the network are very similar to the last layer. We validated that these later layers do not refine the representation by training an $\ell^{2}$ regularized logistic regression classifier on each layer of the network. Classification accuracy in shallower architectures progressively improves with depth, but for the 8x deeper</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. CKA reveals when depth becomes pathological. <strong>Top:</strong> Linear CKA between layers of individual networks of different depths on the CIFAR-10 test set. Titles show accuracy of each network. Later layers of the 8x depth network are similar to the last layer. <strong>Bottom:</strong> Accuracy of a logistic regression classifier trained on layers of the same networks is consistent with CKA.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Linear CKA between layers of a ResNet-62 model on the CIFAR-10 test set. The grid pattern in the left panel arises from the architecture. Right panels show similarity separately for even layer (post-residual) and odd layer (block interior) activations. Layers in the same block group (<em>i.e.</em> at the same feature map scale) are more similar than layers in different block groups.</p>
<p>network, accuracy plateaus less than halfway through the network. When applied to ResNets (He et al., 2016), CKA reveals no pathology (Figure 4). We instead observe a grid pattern that originates from the architecture: Post-residual activations are similar to other post-residual activations, but activations within blocks are not.</p>
<p>CKA is equally effective at revealing relationships between layers of different architectures. Figure 5 shows the relationship between different layers of networks with and without residual connections. CKA indicates that, as networks are made deeper, the new layers are effectively inserted in between the old layers. Other similarity indexes fail to reveal meaningful relationships between different architectures, as we show in Appendix F.5.</p>
<p>In Figure 6, we show CKA between networks with different layer widths. Like <em>Morcos et al. (2018)</em>, we find that increasing layer width leads to more similar representations between networks. As width increases, CKA approaches 1; CKA of earlier layers saturates faster than later layers. Networks are generally more similar to other networks of the same width than they are to the widest network we trained.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Linear CKA between layers of networks with different architectures on the CIFAR-10 test set.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Layers become more similar to each other and to wide networks as width increases, but similarity of earlier layers saturates first. <strong>Left:</strong> Similarity of networks with the widest network we trained. <strong>Middle:</strong> Similarity of networks with other networks of the same width trained from random initialization. All CKA values are computed between 10 networks on the CIFAR-10 test set; shaded regions reflect jackknife standard error.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. CKA shows that models trained on different datasets (CIFAR-10 and CIFAR-100) develop similar representations, and these representations differ from untrained models. The left panel shows similarity between the same layer of different models on the CIFAR-10 test set, while the right panel shows similarity computed on CIFAR-100 test set. CKA is averaged over 10 models of each type (45 pairs).</p>
<h3>6.3. Similar Representations Across Datasets</h3>
<p>CKA can also be used to compare networks trained on different datasets. In Figure 7, we show that models trained on CIFAR-10 and CIFAR-100 develop similar representations in their early layers. These representations require training; similarity with untrained networks is much lower. We further explore similarity between layers of untrained networks in Appendix F.3.</p>
<h3>6.4. Analysis of the Shared Subspace</h3>
<p>Equation 14 suggests a way to further elucidating what CKA is measuring, based on the action of one representational similarity matrix (RSM) $Y Y^{\mathrm{T}}$ applied to the eigenvectors $\mathbf{u}<em X="X">{X}^{i}$ of the other RSM $X X^{\mathrm{T}}$. By definition, $X X^{\mathrm{T}} \mathbf{u}</em>}^{i}$ points in the same direction as $\mathbf{u<em X="X">{X}^{i}$, and its norm $\left|X X^{\mathrm{T}} \mathbf{u}</em>$ have similar actions, but the rank of the subspace where this holds is substantially lower than the dimensionality of the activations. In the penultimate (global average pooling) layer, the dimensionality of the shared subspace is approximately 10, which is the number of classes in the CIFAR-10 dataset.}^{i}\right|_{2}$ is the corresponding eigenvalue. The degree of scaling and rotation by $Y Y^{\mathrm{T}}$ thus indicates how similar the action of $Y Y^{\mathrm{T}}$ is to $X X^{\mathrm{T}}$, for each eigenvector of $X X^{\mathrm{T}}$. For visualization purposes, this approach is somewhat less useful than the CKA summary statistic, since it does not collapse the similarity to a single number, but it provides a more complete picture of what CKA measures. Figure 8 shows that, for large eigenvectors, $X X^{\mathrm{T}}$ and $Y Y^{\mathrm{T}</p>
<h2>7. Conclusion and Future Work</h2>
<p>Measuring similarity between the representations learned by neural networks is an ill-defined problem, since it is not entirely clear what aspects of the representation a similarity
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. The shared subspace of two networks trained on CIFAR10 from different random initializations is spanned primarily by the eigenvectors corresponding to the largest eigenvalues. Each row represents a different network layer. Note that the average pooling layer has only 64 units. Left: Scaling of the eigenvectors $\mathbf{u}<em X="X">{X}^{i}$ of the RSM $X X^{\mathrm{T}}$ from network A by RSMs of networks A and B. Orange lines show $\left|X X^{\mathrm{T}} \mathbf{u}</em>\right|}^{i<em X="X">{2}$, i.e. the eigenvalues. Purple dots show $\left|Y Y^{\mathrm{T}} \mathbf{u}</em>\right|}^{i<em X="X">{2}$, the scaling of the eigenvectors of the RSM of network $A$ by the RSM of network $B$. Right: Cosine of the rotation by the RSM of network B, $\left(\mathbf{u}</em>}^{i}\right)^{\mathrm{T}} Y Y^{\mathrm{T}} \mathbf{u<em X="X">{X}^{i} /\left|Y Y^{\mathrm{T}} \mathbf{u}</em>$.
index should focus on. Previous work has suggested that there is little similarity between intermediate layers of neural networks trained from different random initializations (Raghu et al., 2017; Wang et al., 2018). We propose CKA as a method for comparing representations of neural networks, and show that it consistently identifies correspondences between layers, not only in the same network trained from different initializations, but across entirely different architectures, whereas other methods do not. We also provide a unified framework for understanding the space of similarity indexes, as well as an empirical framework for evaluation.}^{i}\right|_{2</p>
<p>We show that CKA captures intuitive notions of similarity, i.e. that neural networks trained from different initializations should be similar to each other. However, it remains an open question whether there exist kernels beyond the linear and RBF kernels that would be better for analyzing neural network representations. Moreover, there are other potential choices of weighting in Equation 14 that may be more appropriate in certain settings. We leave these questions as future work. Nevertheless, CKA seems to be much better than previous methods at finding correspondences between the learned representations in hidden layers of neural networks.</p>
<h2>Acknowledgements</h2>
<p>We thank Gamaleldin Elsayed, Jaehoon Lee, Paul-Henri Mignot, Maithra Raghu, Samuel L. Smith, Alex Williams, and Michael Wu for comments on the manuscript, Rishabh Agarwal for ideas, and Aliza Elkin for support.</p>
<h2>References</h2>
<p>Advani, M. S. and Saxe, A. M. High-dimensional dynamics of generalization error in neural networks. arXiv preprint arXiv:1710.03667, 2017.</p>
<p>Amari, S.-i., Ozeki, T., Karakida, R., Yoshida, Y., and Okada, M. Dynamics of learning in MLP: Natural gradient and singularity revisited. Neural Computation, 30(1): $1-33,2018$.</p>
<p>Björck, Å. and Golub, G. H. Numerical methods for computing angles between linear subspaces. Mathematics of Computation, 27(123):579-594, 1973.</p>
<p>Bojar, O., Federmann, C., Fishel, M., Graham, Y., Haddow, B., Huck, M., Koehn, P., and Monz, C. Findings of the 2018 Conference on Machine Translation (WMT18). In EMNLP 2018 Third Conference on Machine Translation (WMT18), 2018.</p>
<p>Chen, A. M., Lu, H.-m., and Hecht-Nielsen, R. On the geometry of feedforward neural network error surfaces. Neural Computation, 5(6):910-927, 1993.</p>
<p>Connolly, A. C., Guntupalli, J. S., Gors, J., Hanke, M., Halchenko, Y. O., Wu, Y.-C., Abdi, H., and Haxby, J. V. The representation of biological classes in the human brain. Journal of Neuroscience, 32(8):2608-2618, 2012.</p>
<p>Cortes, C., Mohri, M., and Rostamizadeh, A. Algorithms for learning kernels based on centered alignment. Journal of Machine Learning Research, 13(Mar):795-828, 2012.</p>
<p>Cristianini, N., Shawe-Taylor, J., Elisseeff, A., and Kandola, J. S. On kernel-target alignment. In Advances in Neural Information Processing Systems, 2002.</p>
<p>Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density estimation using real NVP. In International Conference on Learning Representations, 2017.</p>
<p>Dumoulin, V., Shlens, J., and Kudlur, M. A learned representation for artistic style. In International Conference on Learning Representations, 2017.</p>
<p>Edelman, S. Representation is representation of similarities. Behavioral and Brain Sciences, 21(4):449-467, 1998.</p>
<p>Elsayed, G. F., Lara, A. H., Kaufman, M. T., Churchland, M. M., and Cunningham, J. P. Reorganization between preparatory and movement population responses in motor cortex. Nature Communications, 7:13239, 2016.</p>
<p>Freiwald, W. A. and Tsao, D. Y. Functional compartmentalization and viewpoint generalization within the macaque face-processing system. Science, 330(6005):845-851, 2010.</p>
<p>Garriga-Alonso, A., Rasmussen, C. E., and Aitchison, L. Deep convolutional networks as shallow Gaussian processes. In International Conference on Learning Representations, 2019.</p>
<p>Gatys, L. A., Ecker, A. S., and Bethge, M. Image style transfer using convolutional neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.</p>
<p>Golub, G. H. and Zha, H. The canonical correlations of matrix pairs and their numerical computation. In Linear Algebra for Signal Processing, pp. 27-49. Springer, 1995.</p>
<p>Gretton, A., Bousquet, O., Smola, A., and Schölkopf, B. Measuring statistical dependence with Hilbert-Schmidt norms. In International Conference on Algorithmic Learning Theory, 2005.</p>
<p>Haxby, J. V., Gobbini, M. I., Furey, M. L., Ishai, A., Schouten, J. L., and Pietrini, P. Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science, 293(5539):2425-2430, 2001.</p>
<p>He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.</p>
<p>Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, 2015.</p>
<p>Jacobsen, J.-H., Smeulders, A. W., and Oyallon, E. iRevNet: Deep invertible networks. In International Conference on Learning Representations, 2018.</p>
<p>Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems, 2018.</p>
<p>Johnson, J., Alahi, A., and Fei-Fei, L. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, 2016.</p>
<p>Khaligh-Razavi, S.-M. and Kriegeskorte, N. Deep supervised, but not unsupervised, models may explain it cortical representation. PLoS Computational Biology, 10(11): e1003915, 2014.</p>
<p>Kriegeskorte, N., Mur, M., and Bandettini, P. A. Representational similarity analysis-connecting the branches of systems neuroscience. Frontiers in Systems Neuroscience, 2:4, 2008a.</p>
<p>Kriegeskorte, N., Mur, M., Ruff, D. A., Kiani, R., Bodurka, J., Esteky, H., Tanaka, K., and Bandettini, P. A. Matching categorical object representations in inferior temporal cortex of man and monkey. Neuron, 60(6):1126-1141, 2008b.</p>
<p>Kuss, M. and Graepel, T. The geometry of kernel canonical correlation analysis. Technical report, Max Planck Institute for Biological Cybernetics, 2003.</p>
<p>Laakso, A. and Cottrell, G. Content and cluster analysis: assessing representational similarity in neural systems. Philosophical Psychology, 13(1):47-76, 2000.</p>
<p>LeCun, Y., Kanter, I., and Solla, S. A. Second order properties of error surfaces: Learning time and generalization. In Advances in Neural Information Processing Systems, 1991.</p>
<p>Lee, J., Sohl-dickstein, J., Pennington, J., Novak, R., Schoenholz, S., and Bahri, Y. Deep neural networks as gaussian processes. In International Conference on Learning Representations, 2018.</p>
<p>Li, Y., Yosinski, J., Clune, J., Lipson, H., and Hopcroft, J. Convergent learning: Do different neural networks learn the same representations? In NIPS 2015 Workshop on Feature Extraction: Modern Questions and Challenges, 2015.</p>
<p>Lorenzo-Seva, U. and Ten Berge, J. M. Tucker's congruence coefficient as a meaningful index of factor similarity. Methodology, 2(2):57-64, 2006.</p>
<p>Morcos, A., Raghu, M., and Bengio, S. Insights on representational similarity in neural networks with canonical correlation. In Advances in Neural Information Processing Systems, 2018.</p>
<p>Mroueh, Y., Marcheret, E., and Goel, V. Asymmetrically weighted CCA and hierarchical kernel sentence embedding for multimodal retrieval. arXiv preprint arXiv:1511.06267, 2015.</p>
<p>Novak, R., Xiao, L., Bahri, Y., Lee, J., Yang, G., Abolafia, D. A., Pennington, J., and Sohl-dickstein, J. Bayesian deep convolutional networks with many channels are Gaussian processes. In International Conference on Learning Representations, 2019.</p>
<p>Orhan, E. and Pitkow, X. Skip connections eliminate singularities. In International Conference on Learning Representations, 2018.</p>
<p>Press, W. H. Canonical correlation clarified by singular value decomposition, 2011. URL http://numerical.recipes/whp/notes/ CanonCorrBySVD.pdf.</p>
<p>Raghu, M., Gilmer, J., Yosinski, J., and Sohl-Dickstein, J. SVCCA: Singular vector canonical correlation analysis for deep learning dynamics and interpretability. In $A d$ vances in Neural Information Processing Systems, 2017.</p>
<p>Ramsay, J., ten Berge, J., and Styan, G. Matrix correlation. Psychometrika, 49(3):403-423, 1984.</p>
<p>Robert, P. and Escoufier, Y. A unifying tool for linear multivariate statistical methods: the RV-coefficient. Applied Statistics, 25(3):257-265, 1976.</p>
<p>Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., and Bengio, Y. FitNets: Hints for thin deep nets. In International Conference on Learning Representations, 2015.</p>
<p>SAS Institute. Introduction to Regression Procedures. 2015. URL https://support.sas. com/documentation/onlinedoc/stat/141/ introreg.pdf.</p>
<p>Saxe, A. M., McClelland, J. L., and Ganguli, S. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014.</p>
<p>Sejdinovic, D., Sriperumbudur, B., Gretton, A., and Fukumizu, K. Equivalence of distance-based and RKHS-based statistics in hypothesis testing. The Annals of Statistics, pp. 2263-2291, 2013.</p>
<p>Smith, S. L., Turban, D. H., Hamblin, S., and Hammerla, N. Y. Offline bilingual word vectors, orthogonal transformations and the inverted softmax. In International Conference on Learning Representations, 2017.</p>
<p>Song, L., Smola, A., Gretton, A., Borgwardt, K. M., and Bedo, J. Supervised feature selection via dependence estimation. In International Conference on Machine learning, 2007.</p>
<p>Springenberg, J. T., Dosovitskiy, A., Brox, T., and Riedmiller, M. Striving for simplicity: The all convolutional net. In International Conference on Learning Representations Workshop, 2015.</p>
<p>StataCorp. Stata Multivariate Statistics Reference Manual. 2015. URL https://www.stata.com/ manuals14/mv.pdf.</p>
<p>Sussillo, D., Churchland, M. M., Kaufman, M. T., and Shenoy, K. V. A neural network that finds a naturalistic solution for the production of muscle activity. Nature Neuroscience, 18(7):1025, 2015.</p>
<p>Tucker, L. R. A method for synthesis of factor analysis studies. Technical report, Educational Testing Service, Princeton, NJ, 1951.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998-6008, 2017.</p>
<p>Vaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez, A. N., Gouws, S., Jones, L., Kaiser, Ł., Kalchbrenner, N., Parmar, N., et al. Tensor2tensor for neural machine translation. arXiv preprint arXiv:1803.07416, 2018.</p>
<p>Vinod, H. D. Canonical ridge and econometrics of joint production. Journal of Econometrics, 4(2):147-166, 1976.</p>
<p>Wang, L., Hu, L., Gu, J., Wu, Y., Hu, Z., He, K., and Hopcroft, J. E. Towards understanding learning representations: To what extent do different neural networks learn the same representation. In Advances in Neural Information Processing Systems, 2018.</p>
<p>Yamins, D. L., Hong, H., Cadieu, C. F., Solomon, E. A., Seibert, D., and DiCarlo, J. J. Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proceedings of the National Academy of Sciences, 111(23):8619-8624, 2014.</p>
<p>Zagoruyko, S. and Komodakis, N. Wide residual networks. In British Machine Vision Conference, 2016.</p>
<h1>A. Proof of Theorem 1</h1>
<p>Theorem. Let $X$ and $Y$ be $n \times p$ matrices. Suppose $s$ is invariant to invertible linear transformation in the first argument, i.e. $s(X, Z)=s(X A, Z)$ for arbitrary $Z$ and any $A$ with $\operatorname{rank}(A)=p$. If $\operatorname{rank}(X)=\operatorname{rank}(Y)=n$, then $s(X, Z)=$ $s(Y, Z)$.</p>
<p>Proof. Let</p>
<p>$$
X^{\prime}=\left[\begin{array}{l}
X \
K_{X}
\end{array}\right] \quad Y^{\prime}=\left[\begin{array}{l}
Y \
K_{Y}
\end{array}\right]
$$</p>
<p>where $K_{X}$ is a basis for the null space of the rows of $X$ and $K_{Y}$ is a basis for the null space of the rows of $Y$. Then let $A=X^{\prime-1} Y^{\prime}$.</p>
<p>$$
\left[\begin{array}{c}
X \
K_{X}
\end{array}\right] A=\left[\begin{array}{c}
Y \
K_{Y}
\end{array}\right] \Longrightarrow X A=Y
$$</p>
<p>Because $X^{\prime}$ and $Y^{\prime}$ have rank $p$ by construction, $A$ also has rank $p$. Thus, $s(X, Z)=s(X A, Z)=s(Y, Z)$.</p>
<h2>B. Orthogonalization and Invariance to Invertible Linear Transformation</h2>
<p>Here we show that any similarity index that is invariant to orthogonal transformation can be made invariant to invertible linear transformation by orthogonalizing the columns of the input.
Proposition 1. Let $X$ be an $n \times p$ matrix of full column rank and let $A$ be an invertible $p \times p$ matrix. Let $X=Q_{X} R_{X}$ and $X A=Q_{X A} R_{X A}$, where $Q_{X}^{\mathrm{T}} Q_{X}=Q_{X A}^{\mathrm{T}} Q_{X A}=I$ and $R_{X}$ and $R_{X A}$ are invertible. If $s(\cdot, \cdot)$ is invariant to orthogonal transformation, then $s\left(Q_{X}, Y\right)=s\left(Q_{X A}, Y\right)$.</p>
<p>Proof. Let $B=R_{X} A R_{X A}^{-1}$. Then $Q_{X} B=Q_{X A}$, and B is an orthogonal transformation:</p>
<p>$$
B^{\mathrm{T}} B=B^{\mathrm{T}} Q_{X}^{\mathrm{T}} Q_{X} B=Q_{X A}^{\mathrm{T}} Q_{X A}=I
$$</p>
<p>Thus $s\left(Q_{X}, Y\right)=s\left(Q_{X} B, Y\right)=s\left(Q_{X A}, Y\right)$.</p>
<h2>C. CCA and Linear Regression</h2>
<h2>C.1. Linear Regression</h2>
<p>Consider the linear regression fit of the columns of an $n \times m$ matrix $C$ with an $n \times p$ matrix $A$ :</p>
<p>$$
\hat{B}=\underset{B}{\arg \min }|C-A B|_{\mathrm{F}}^{2}=\left(A^{\mathrm{T}} A\right)^{-1} A^{\mathrm{T}} C
$$</p>
<p>Let $A=Q R$, the thin QR decomposition of A . Then the fitted values are given by:</p>
<p>$$
\begin{aligned}
\hat{C} &amp; =A \hat{B} \
&amp; =A\left(A^{\mathrm{T}} A\right)^{-1} A^{\mathrm{T}} C \
&amp; =Q R\left(R^{\mathrm{T}} Q^{\mathrm{T}} Q R\right)^{-1} R^{\mathrm{T}} Q^{\mathrm{T}} C \
&amp; =Q R R^{-1}\left(R^{\mathrm{T}}\right)^{-1} R^{\mathrm{T}} Q^{\mathrm{T}} C \
&amp; =Q Q^{\mathrm{T}} C
\end{aligned}
$$</p>
<p>The residuals $E=C-\hat{C}$ are orthogonal to the fitted values, i.e.</p>
<p>$$
\begin{aligned}
E^{\mathrm{T}} \hat{C} &amp; =\left(C-Q Q^{\mathrm{T}} C\right)^{\mathrm{T}} Q Q^{\mathrm{T}} C \
&amp; =C^{\mathrm{T}} Q Q^{\mathrm{T}} C-C^{\mathrm{T}} Q Q^{\mathrm{T}} C=0
\end{aligned}
$$</p>
<p>Thus:</p>
<p>$$
\begin{aligned}
|E|<em _mathrm_F="\mathrm{F">{\mathrm{F}}^{2} &amp; =\operatorname{tr}\left(E^{\mathrm{T}} E\right) \
&amp; =\operatorname{tr}\left(E^{\mathrm{T}} C-E^{\mathrm{T}} \hat{C}\right) \
&amp; =\operatorname{tr}\left((C-\hat{C})^{\mathrm{T}} C\right) \
&amp; =\operatorname{tr}\left(C^{\mathrm{T}} C\right)-\operatorname{tr}\left(C^{\mathrm{T}} Q Q^{\mathrm{T}} C\right) \
&amp; =|C|</em>
\end{aligned}
$$}}^{2}-\left|Q^{\mathrm{T}} C\right|_{\mathrm{F}}^{2</p>
<p>Assuming that $C$ was centered by subtracting its column means prior to the linear regression fit, the total fraction of variance explained by the fit is:</p>
<p>$$
R^{2}=1-\frac{|E|<em _mathrm_F="\mathrm{F">{\mathrm{F}}^{2}}{|C|</em>=1-\frac{|C|}}^{2}<em _mathrm_F="\mathrm{F">{\mathrm{F}}^{2}-\left|Q^{\mathrm{T}} C\right|</em>{|C|}}^{2}<em _mathrm_F="\mathrm{F">{\mathrm{F}}^{2}}=\frac{\left|Q^{\mathrm{T}} C\right|</em>
$$}}^{2}}{|C|_{\mathrm{F}}^{2}</p>
<p>Although we have assumed that $Q$ is obtained from QR decomposition, any orthonormal basis with the same span will suffice, because orthogonal transformations do not change the Frobenius norm.</p>
<h1>C.2. CCA</h1>
<p>Let $X$ be an $n \times p_{1}$ matrix and $Y$ be an $n \times p_{2}$ matrix, and let $p=\min \left(p_{1}, p_{2}\right)$. Given the thin QR decompositions of $X$ and $Y, X=Q_{X} R_{X}, Y=Q_{Y} R_{Y}$ such that $Q_{X}^{\mathrm{T}} Q_{X}=I, Q_{Y}^{\mathrm{T}} Q_{Y}=I$, the canonical correlations $\rho_{i}$ are the singular values of $A=Q_{X}^{\mathrm{T}} Q_{Y}$ (Björck \&amp; Golub, 1973; Press, 2011) and thus the square roots of the eigenvalues of $A^{\mathrm{T}} A$. The squared canonical correlations $\rho_{i}^{2}$ are the eigenvalues of $A^{\mathrm{T}} A=Q_{Y}^{\mathrm{T}} Q_{X} Q_{X}^{\mathrm{T}} Q_{Y}$. Their sum is $\sum_{i=1}^{p} \rho_{i}^{2}=\operatorname{tr}\left(A^{\mathrm{T}} A\right)=\left|Q_{Y}^{\mathrm{T}} Q_{X}\right|<em X="X">{\mathrm{F}}^{2}$.
Now consider the linear regression fit of the columns of $Q</em>\right|}$ with $Y$. Assume that $Q_{X}$ has zero mean. Substituting $Q_{Y}$ for $Q$ and $Q_{X}$ for $C$ in Equation 16, and noting that $\left|Q_{X<em 1="1">{\mathrm{F}}^{2}=p</em>$ :</p>
<p>$$
R^{2}=\frac{\left|Q_{Y}^{\mathrm{T}} Q_{X}\right|<em 1="1">{\mathrm{F}}^{2}}{p</em>
$$}}=\frac{\sum_{i=1}^{p} \rho_{i}^{2}}{p_{1}</p>
<h2>C.3. Projection-Weighted CCA</h2>
<p>Let $X$ be an $n \times p_{1}$ matrix and $Y$ be an $n \times p_{2}$ matrix, with $p_{1} \leq p_{2}$. Morcos et al. (2018) proposed to compute projection-weighted canonical correlation as:</p>
<p>$$
\bar{\rho}<em i="1">{\mathrm{PW}}=\frac{\sum</em>}^{c} \alpha_{i} \rho_{i}}{\sum_{i=1}^{c} \alpha_{i}} \quad \alpha_{i}=\sum_{j}\left|\left\langle\mathbf{h<em j="j">{i}, \mathbf{x}</em>\right\rangle\right|
$$</p>
<p>where the $\mathbf{x}<em i="i">{j}$ are the columns of $X$, and the $\mathbf{h}</em>}$ are the canonical variables formed by projecting $X$ to the canonical coordinate frame. Below, we show that if we modify $\bar{\rho<em i="i">{\mathrm{PW}}$ by squaring the dot products and $\rho</em>$, we recover linear regression.:</p>
<p>$$
R_{\mathrm{MPW}}^{2}=\frac{\sum_{i=1}^{c} \alpha_{i}^{\prime} \rho_{i}^{2}}{\sum_{i=1}^{c} \alpha_{i}^{\prime}}=R_{\mathrm{LR}}^{2} \quad \alpha_{i}^{\prime}=\sum_{j}\left\langle\mathbf{h}<em j="j">{i}, \mathbf{x}</em>
$$}\right\rangle^{2</p>
<p>Our derivation begins by forming the SVD $Q_{X}^{\mathrm{T}} Q_{Y}=U \Sigma V^{\mathrm{T}}$. $\Sigma$ is a diagonal matrix of the canonical correlations $\rho_{i}$, and the matrix of canonical variables $H=Q_{X} U$. Then $R_{\mathrm{MPW}}^{2}$ is:</p>
<p>$$
\begin{aligned}
R_{\mathrm{MPW}}^{2} &amp; =\frac{\left|X^{\mathrm{T}} H \Sigma\right|<em _mathrm_F="\mathrm{F">{\mathrm{F}}^{2}}{\left|X^{\mathrm{T}} H\right|</em> \
&amp; =\frac{\operatorname{tr}\left((X^{\mathrm{T}} H \Sigma)^{\mathrm{T}}\left(X^{\mathrm{T}} H \Sigma\right)\right)}{\operatorname{tr}\left(\left(X^{\mathrm{T}} H\right)^{\mathrm{T}}\left(X^{\mathrm{T}} H\right)\right)} \
&amp; =\frac{\operatorname{tr}\left(\Sigma H^{\mathrm{T}} X X^{\mathrm{T}} H \Sigma\right)}{\operatorname{tr}\left(H^{\mathrm{T}} X X^{\mathrm{T}} H\right)} \
&amp; =\frac{\operatorname{tr}\left(X^{\mathrm{T}} H \Sigma^{2} H^{\mathrm{T}} X\right)}{\operatorname{tr}\left(X^{\mathrm{T}} H H^{\mathrm{T}} X\right)} \
&amp; =\frac{\operatorname{tr}\left(R_{X}^{\mathrm{T}} Q_{X}^{\mathrm{T}} H \Sigma^{2} H^{\mathrm{T}} Q_{X} R_{X}\right)}{\operatorname{tr}\left(R_{X}^{\mathrm{T}} Q_{X}^{\mathrm{T}} Q_{X} U U^{\mathrm{T}} Q_{X}^{\mathrm{T}} Q_{X} R_{X}\right)}
\end{aligned}
$$}}^{2}</p>
<p>Because we assume $p_{1} \leq p_{2}, U$ is a square orthogonal matrix and $U U^{\mathrm{T}}=I$. Further noting that $Q_{X}^{\mathrm{T}} H=U$ and $U \Sigma=Q_{X}^{\mathrm{T}} Q_{Y} V$ :</p>
<p>$$
\begin{aligned}
R_{\mathrm{MPW}}^{2} &amp; =\frac{\operatorname{tr}\left(R_{X}^{\mathrm{T}} U \Sigma^{2} U^{\mathrm{T}} R_{X}\right)}{\operatorname{tr}\left(R_{X}^{\mathrm{T}} Q_{X}^{\mathrm{T}} Q_{X} R_{X}\right)} \
&amp; =\frac{\operatorname{tr}\left(R_{X}^{\mathrm{T}} Q_{X}^{\mathrm{T}} Q_{Y} V \Sigma U^{\mathrm{T}} R_{X}\right)}{\operatorname{tr}\left(X^{\mathrm{T}} X\right)} \
&amp; =\frac{\operatorname{tr}\left(X^{\mathrm{T}} Q_{Y} Q_{Y}^{\mathrm{T}} Q_{X} R_{X}\right)}{\operatorname{tr}\left(X^{\mathrm{T}} X\right)} \
&amp; =\frac{\operatorname{tr}\left(X^{\mathrm{T}} Q_{Y} Q_{Y}^{\mathrm{T}} X\right)}{\operatorname{tr}\left(X^{\mathrm{T}} X\right)} \
&amp; =\frac{\left|Q_{Y}^{\mathrm{T}} X\right|<em _mathrm_F="\mathrm{F">{\mathrm{F}}^{2}}{|X|</em>
\end{aligned}
$$}}^{2}</p>
<p>Substituting $Q_{Y}$ for $Q$ and $X$ for $C$ in Equation 16:</p>
<p>$$
R_{\mathrm{LR}}^{2}=\frac{\left|Q_{Y}^{\mathrm{T}} X\right|<em _mathrm_F="\mathrm{F">{\mathrm{F}}^{2}}{|X|</em>
$$}}^{2}}=R_{\mathrm{MPW}}^{2</p>
<h1>D. Notes on Other Methods</h1>
<h2>D.1. Canonical Ridge</h2>
<p>Beyond CCA, we could also consider the "canonical ridge" regularized CCA objective (Vinod, 1976):</p>
<p>$$
\begin{aligned}
&amp; \sigma_{i}=\max <em X="X">{\mathbf{w}</em>}^{i}, \mathbf{w<em X="X">{Y}^{i}} \frac{\left(X \mathbf{w}</em>}^{i}\right)^{\mathrm{T}}\left(Y \mathbf{w<em X="X">{Y}^{i}\right)}{\sqrt{\left|X \mathbf{w}</em>}^{i}\right|^{2}+\kappa_{X}\left|\mathbf{w<em 2="2">{X}^{i}\right|</em>}^{2}} \sqrt{\left|Y \mathbf{w<em Y="Y">{Y}^{i}\right|^{2}+\kappa</em>}\left|\mathbf{w<em j_i="j&lt;i">{Y}^{i}\right|^{2}}} \
&amp; \text { subject to } \forall</em>}\left(\mathbf{w<em X="X">{X}^{i}\right)^{\mathrm{T}}\left(X^{\mathrm{T}} X+\kappa I\right) \mathbf{w}</em>=0 \
&amp; \forall_{j&lt;i}\left(\mathbf{w}}^{j<em Y="Y">{Y}^{i}\right)^{\mathrm{T}}\left(Y^{\mathrm{T}} Y+\kappa I\right) \mathbf{w}</em>=0 .
\end{aligned}
$$}^{j</p>
<p>Given the singular value decompositions $X=U_{X} \Sigma_{X} V_{X}^{\mathrm{T}}$ and $Y=U_{Y} \Sigma_{Y} V_{Y}^{\mathrm{T}}$, one can form "partially orthogonalized" bases $\tilde{Q}<em X="X">{X}=U</em>} \Sigma_{X}\left(\Sigma_{X}^{2}+\kappa_{X} I\right)^{-1 / 2}$ and $\tilde{Q<em Y="Y">{Y}=U</em>} \Sigma_{Y}\left(\Sigma_{Y}^{2}+\kappa_{Y} I\right)^{-1 / 2}$. Given the singular value decomposition of their product $\tilde{U} \tilde{\Sigma} \tilde{V}^{\mathrm{T}}=\tilde{Q<em Y="Y">{X}^{\mathrm{T}} \tilde{Q}</em>}$, the canonical weights are given by $W_{X}=V_{X}\left(\Sigma_{X}^{2}+\kappa_{X} I\right)^{-1 / 2} \tilde{U}$ and $W_{Y}=V_{Y}\left(\Sigma_{Y}^{2}+\kappa_{Y} I\right)^{-1 / 2} \tilde{V}$, as previously shown by Mroueh et al. (2015). As in the unregularized case (Equation 13), there is a convenient expression for the sum of the squared singular values $\sum \tilde{\sigma<em X="X">{i}^{2}$ in terms of the eigenvalues and eigenvectors of $X X^{\mathrm{T}}$ and $Y Y^{\mathrm{T}}$. Let the $i^{\text {th }}$ left-singular vector of $X$ (eigenvector of $X X^{\mathrm{T}}$ ) be indexed as $\mathbf{u}</em>}^{i}$ and let the $i^{\text {th }}$ eigenvalue of $X X^{\mathrm{T}}$ (squared singular value of $X$ ) be indexed as $\lambda_{X}^{i}$, and similarly let the left-singular vectors of $Y Y^{\mathrm{T}}$ be indexed as $\mathbf{u<em Y="Y">{Y}^{i}$ and the eigenvalues as $\lambda</em>$. Then:}^{i</p>
<p>$$
\begin{aligned}
\sum_{i=1}^{p_{1}} \tilde{\sigma}<em Y="Y">{i}^{2} &amp; =\left|\tilde{Q}</em>}^{\mathrm{T}} \tilde{Q<em _mathrm_F="\mathrm{F">{X}\right|</em> \
&amp; =\left|\left(\Sigma_{Y}^{2}+\kappa_{Y} I\right)^{-1 / 2} \Sigma_{Y} U_{Y}^{\mathrm{T}} U_{X} \Sigma_{X}\left(\Sigma_{X}^{2}+\kappa_{X} I\right)^{-1 / 2}\right|}}^{2<em i="1">{\mathrm{F}}^{2} \
&amp; =\sum</em>}^{p_{1}} \sum_{j=1}^{p_{2}} \frac{\lambda_{X}^{i} \lambda_{Y}^{j}}{\left(\lambda_{X}^{i}+\kappa_{X}\right)\left(\lambda_{Y}^{j}+\kappa_{Y}\right)}\left\langle\mathbf{u<em Y="Y">{X}^{i}, \mathbf{u}</em>
\end{aligned}
$$}^{j}\right\rangle^{2</p>
<p>Unlike in the unregularized case, the singular values $\sigma_{i}$ do not measure the correlation between the canonical variables. Instead, they become arbitrarily small as $\kappa_{X}$ or $\kappa_{Y}$ increase. Thus, we need to normalize the statistic to remove the dependency on the regularization parameters.</p>
<p>Applying von Neumann's trace inequality yields a bound:</p>
<p>$$
\begin{aligned}
\sum_{i=1}^{p_{1}} \hat{\sigma}<em Y="Y">{i}^{2} &amp; =\operatorname{tr}\left(\tilde{Q}</em>} \tilde{Q<em X="X">{Y}^{\mathrm{T}} \tilde{Q}</em>} \tilde{Q<em Y="Y">{X}^{\mathrm{T}}\right) \
&amp; =\operatorname{tr}\left(\left(U</em>\right)\right) \
&amp; \leq \sum_{i=1}^{p_{1}} \frac{\lambda_{X}^{i} \lambda_{Y}^{i}}{\left(\lambda_{X}^{i}+\kappa_{X}\right)\left(\lambda_{Y}^{i}+\kappa_{Y}\right)}
\end{aligned}
$$} \Sigma_{Y}^{2}\left(\Sigma_{Y}^{2}+\kappa_{Y} I\right)^{-1} U_{Y}^{\mathrm{T}}\right)\left(U_{X} \Sigma_{X}^{2}\left(\Sigma_{X}^{2}+\kappa_{X} I\right)^{-1} U_{X}^{\mathrm{T}</p>
<p>Applying the Cauchy-Schwarz inequality to (25) yields the alternative bounds:</p>
<p>$$
\begin{aligned}
\sum_{i=1}^{p_{1}} \hat{\sigma}<em i="1">{i}^{2} &amp; \leq \sqrt{\sum</em> \
&amp; \leq \sqrt{\sum_{i=1}^{p_{1}}\left(\frac{\lambda_{X}^{i}}{\lambda_{X}^{i}+\kappa_{X}}\right)^{2}} \sqrt{\sum_{i=1}^{p_{2}}\left(\frac{\lambda_{Y}^{i}}{\lambda_{Y}^{i}+\kappa_{Y}}\right)^{2}}
\end{aligned}
$$}^{p_{1}}\left(\frac{\lambda_{X}^{i}}{\lambda_{X}^{i}+\kappa_{X}}\right)^{2}} \sqrt{\sum_{i=1}^{p_{1}}\left(\frac{\lambda_{Y}^{i}}{\lambda_{Y}^{i}+\kappa_{Y}}\right)^{2}</p>
<p>A normalized form of (22) could be produced by dividing by any of (25), (26), or (27).
If $\kappa_{X}=\kappa_{Y}=0$, then (25) and (26) are equal to $p_{1}$. In this case, (22) is simply the sum of the squared canonical correlations, so normalizing by either of these bounds recovers $R_{\mathrm{CCA}}^{2}$.
If $\kappa_{Y}=0$, then as $\kappa_{X} \rightarrow \infty$, normalizing by the bound from (25) recovers $R^{2}$ :</p>
<p>$$
\begin{aligned}
&amp; \lim <em X="X">{\kappa</em>} \rightarrow \infty} \frac{\sum_{i=1}^{p_{1}} \sum_{j=1}^{p_{2}} \frac{\lambda_{X}^{i} \lambda_{Y}^{i}}{\left(\lambda_{X}^{i}+\kappa_{X}\right)\left(\lambda_{Y}^{i}+0\right)}\left\langle\mathbf{u<em Y="Y">{X}^{i}, \mathbf{u}</em> \
= &amp; \lim }^{j}\right\rangle^{2}}{\sum_{i=1}^{p_{1}} \frac{\lambda_{X}^{i} \lambda_{Y}^{i}}{\left(\lambda_{X}^{i}+\kappa_{X}\right)\left(\lambda_{Y}^{i}+0\right)}<em X="X">{\kappa</em>} \rightarrow \infty} \frac{\sum_{i=1}^{p_{1}} \sum_{j=1}^{p_{2}} \frac{\lambda_{X}^{i}}{\left(\frac{\lambda_{X}^{i}}{\kappa_{X}}+1\right)}\left\langle\mathbf{u<em Y="Y">{X}^{i}, \mathbf{u}</em> \
= &amp; \frac{\sum_{i=1}^{p_{1}} \sum_{j=1}^{p_{2}} \lambda_{X}^{i}\left\langle\mathbf{u}}^{j}\right\rangle^{2}}{\sum_{i=1}^{p_{1}} \frac{\lambda_{X}^{i}}{\left(\frac{\lambda_{X}^{i}}{\kappa_{X}}+1\right)}<em Y="Y">{X}^{i}, \mathbf{u}</em> \
= &amp; \frac{\left|U_{Y}^{\mathrm{T}} U_{X} \Sigma_{X}\right|}^{j}\right\rangle^{2}}{\sum_{i=1}^{p_{1}} \lambda_{X}^{i}<em _mathrm_F="\mathrm{F">{\mathrm{F}}^{2}}{|X|</em> X\right|}}^{2}}=\frac{\left|Q_{Y}^{\mathrm{T}<em _mathrm_F="\mathrm{F">{\mathrm{F}}^{2}}{|X|</em>
\end{aligned}
$$}}^{2}}=R_{\mathrm{LR}}^{2</p>
<p>The bound from (27) differs from the bounds in (25) and (26) because it is multiplicatively separable in $X$ and $Y$. Normalizing by this bound leads to $\operatorname{CKA}\left(\tilde{Q}<em X="X">{X} \tilde{Q}</em>}^{\mathrm{T}}, \tilde{Q<em Y="Y">{Y} \tilde{Q}</em>\right)$ :}^{\mathrm{T}</p>
<p>$$
\begin{aligned}
&amp; \frac{\sum_{i=1}^{p_{1}} \sum_{j=1}^{p_{2}} \frac{\lambda_{X}^{i} \lambda_{Y}^{i}}{\left(\lambda_{X}^{i}+\kappa_{X}\right)\left(\lambda_{Y}^{i}+\kappa_{Y}\right)}\left\langle\mathbf{u}<em Y="Y">{X}^{i}, \mathbf{u}</em> \
= &amp; \frac{\left|\tilde{Q}}^{j}\right\rangle^{2}}{\sqrt{\sum_{i=1}^{p_{1}}\left(\frac{\lambda_{X}^{i}}{\lambda_{X}^{i}+\kappa_{X}}\right)^{2}} \sqrt{\sum_{i=1}^{p_{2}}\left(\frac{\lambda_{Y}^{i}}{\lambda_{Y}^{i}+\kappa_{Y}}\right)^{2}}<em X="X">{Y}^{\mathrm{T}} \tilde{Q}</em>\right|<em X="X">{\mathrm{F}}^{2}}{\left|\tilde{Q}</em>}^{\mathrm{T}} \tilde{Q<em _mathrm_F="\mathrm{F">{X}\right|</em>}}\left|\tilde{Q<em Y="Y">{Y}^{\mathrm{T}} \tilde{Q}</em>\right|<em X="X">{\mathrm{F}}}=\operatorname{CKA}\left(\tilde{Q}</em>} \tilde{Q<em Y="Y">{X}^{\mathrm{T}}, \tilde{Q}</em>\right)
\end{aligned}
$$} \tilde{Q}_{Y}^{\mathrm{T}</p>
<p>Moreover, setting $\kappa_{X}=\kappa_{Y}=\kappa$ and taking the limit as $\kappa \rightarrow \infty$, the normalization from (27) leads to $\operatorname{CKA}\left(X X^{\mathrm{T}}, Y Y^{\mathrm{T}}\right)$ :</p>
<p>$$
\begin{aligned}
&amp; \lim <em i="1">{\kappa \rightarrow \infty} \frac{\sum</em>}^{p_{1}} \sum_{j=1}^{p_{2}} \frac{\lambda_{X}^{i} \lambda_{Y}^{j}}{\left(\lambda_{X}^{i}+\kappa\right)\left(\lambda_{Y}^{i}+\kappa\right)}\left\langle\mathbf{u<em Y="Y">{X}^{i}, \mathbf{u}</em>
\end{aligned}
$$}^{j}\right\rangle^{2}}{\sqrt{\sum_{i=1}^{p_{1}}\left(\frac{\lambda_{X}^{i}}{\lambda_{X}^{i}+\kappa}\right)^{2}}</p>
<p>$$
\begin{aligned}
&amp; =\lim <em i="1">{\kappa \rightarrow \infty} \frac{\sum</em>}^{p_{1}} \sum_{j=1}^{p_{2}} \frac{\lambda_{X}^{i} \lambda_{Y}^{j}}{\left(\frac{\lambda_{X}^{i}}{c}+1\right)\left(\frac{\lambda_{Y}^{j}}{c}+1\right)}\left\langle\mathbf{u<em Y="Y">{X}^{i}, \mathbf{u}</em>
\end{aligned}
$$}^{j}\right\rangle^{2}}{\sqrt{\sum_{i=1}^{p_{1}}\left(\frac{\lambda_{X}^{i}}{\frac{\lambda_{X}^{i}}{c}+1}\right)^{2}}</p>
<p>$$
\begin{aligned}
&amp; =\frac{\sum_{i=1}^{p_{1}} \sum_{j=1}^{p_{2}} \lambda_{X}^{i} \lambda_{Y}^{j}\left\langle\mathbf{u}<em Y="Y">{X}^{i}, \mathbf{u}</em> \
&amp; =\operatorname{CKA}\left(X X^{\mathrm{T}}, Y Y^{\mathrm{T}}\right)
\end{aligned}
$$}^{j}\right\rangle^{2}}{\sqrt{\sum_{i=1}^{p_{1}}\left(\lambda_{X}^{i}\right)^{2}} \sqrt{\sum_{i=1}^{p_{2}}\left(\lambda_{Y}^{i}\right)^{2}}</p>
<p>Overall, the hyperparameters of the canonical ridge objective make it less useful for exploratory analysis. These hyperparameters could be selected by cross-validation, but this is computationally expensive, and the resulting estimator would be biased by sample size. Moreover, our goal is not to map representations of networks to a common space, but to measure the similarity between networks. Appropriately chosen regularization will improve out-of-sample performance of the mapping, but it makes the meaning of "similarity" more ambiguous.</p>
<h1>D.2. The Orthogonal Procrustes Problem</h1>
<p>The orthogonal Procrustes problem consists of finding an orthogonal rotation in feature space that produces the smallest error:</p>
<p>$$
\hat{Q}=\underset{Q}{\arg \min }|Y-X Q|_{\mathrm{F}}^{2} \text { subject to } Q^{\mathrm{T}} Q=I
$$</p>
<p>The objective can be written as:</p>
<p>$$
\begin{aligned}
|Y-X Q|<em _mathrm_F="\mathrm{F">{\mathrm{F}}^{2} &amp; =\operatorname{tr}\left((Y-X Q)^{\mathrm{T}}(Y-X Q)\right) \
&amp; =\operatorname{tr}\left(Y^{\mathrm{T}} Y\right)-\operatorname{tr}\left(Y^{\mathrm{T}} X Q\right)-\operatorname{tr}\left(Q^{\mathrm{T}} X^{\mathrm{T}} Y\right)+\operatorname{tr}\left(Q^{\mathrm{T}} X^{\mathrm{T}} X Q\right) \
&amp; =|Y|</em> X Q\right)
\end{aligned}
$$}}^{2}+|X|_{\mathrm{F}}^{2}-2 \operatorname{tr}\left(Y^{\mathrm{T}</p>
<p>Thus, an equivalent objective is:</p>
<p>$$
\hat{Q}=\underset{Q}{\arg \max } \operatorname{tr}\left(Y^{\mathrm{T}} X Q\right) \text { subject to } Q^{\mathrm{T}} Q=I
$$</p>
<p>The solution is $\hat{Q}=U V^{\mathrm{T}}$ where $U \Sigma V^{\mathrm{T}}=X^{\mathrm{T}} Y$, the singular value decomposition. At the maximum of (39):</p>
<p>$$
\operatorname{tr}\left(Y^{\mathrm{T}} X \hat{Q}\right)=\operatorname{tr}\left(V \Sigma U^{\mathrm{T}} U V^{\mathrm{T}}\right)=\operatorname{tr}(\Sigma)=\left|X^{\mathrm{T}} Y\right|_{<em>}=\left|Y^{\mathrm{T}} X\right|_{</em>}
$$</p>
<p>which is similar to what we call "dot product-based similarity" (Equation 1), but with the squared Frobenius norm of $Y^{\mathrm{T}} X$ (the sum of the squared singular values) replaced by the nuclear norm (the sum of the singular values). The Frobenius norm of $Y^{\mathrm{T}} X$ can be obtained as the solution to a similar optimization problem:</p>
<p>$$
\left|Y^{\mathrm{T}} X\right|<em W="W">{\mathrm{F}}=\max </em> W\right)=1
$$} \operatorname{tr}\left(Y^{\mathrm{T}} X W\right) \text { subject to } \operatorname{tr}\left(W^{\mathrm{T}</p>
<p>In the context of neural networks, Smith et al. (2017) previously proposed using the solution to the orthogonal Procrustes problem to align word embeddings from different languages, and demonstrated that it outperformed CCA.</p>
<h1>E. Architecture Details</h1>
<p>All non-ResNet architectures are based on All-CNN-C (Springenberg et al., 2015), but none are architecturally identical. The Plain-10 model is very similar, but we place the final linear layer after the average pooling layer and use batch normalization because these are common choices in modern architectures. We use these models because they train in minutes on modern hardware.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Tiny-10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$3 \times 3$ conv. 16-BN-ReLu $\times 2$</td>
</tr>
<tr>
<td style="text-align: left;">$3 \times 3$ conv. 32 stride 2-BN-ReLu</td>
</tr>
<tr>
<td style="text-align: left;">$3 \times 3$ conv. 32-BN-ReLu $\times 2$</td>
</tr>
<tr>
<td style="text-align: left;">$3 \times 3$ conv. 64 stride 2-BN-ReLu</td>
</tr>
<tr>
<td style="text-align: left;">$3 \times 3$ conv. 64 valid padding-BN-ReLu</td>
</tr>
<tr>
<td style="text-align: left;">$1 \times 1$ conv. 64-BN-ReLu</td>
</tr>
<tr>
<td style="text-align: left;">Global average pooling</td>
</tr>
<tr>
<td style="text-align: left;">Logits</td>
</tr>
</tbody>
</table>
<p>Table E.1. The Tiny-10 architecture, used in Figures 2, 8, F.3, and F.5. The average Tiny-10 model achieved 89.4\% accuracy.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Plain- $(8 n+2)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$3 \times 3$ conv. 96-BN-ReLu $\times(3 n-1)$</td>
</tr>
<tr>
<td style="text-align: left;">$3 \times 3$ conv. 96 stride 2-BN-ReLu</td>
</tr>
<tr>
<td style="text-align: left;">$3 \times 3$ conv. 192-BN-ReLu $\times(3 n-1)$</td>
</tr>
<tr>
<td style="text-align: left;">$3 \times 3$ conv. 192 stride 2-BN-ReLu</td>
</tr>
<tr>
<td style="text-align: left;">$3 \times 3$ conv. 192 BN-ReLu $\times(n-1)$</td>
</tr>
<tr>
<td style="text-align: left;">$3 \times 3$ conv. 192 valid padding-BN-ReLu</td>
</tr>
<tr>
<td style="text-align: left;">$1 \times 1$ conv. 192-BN-ReLu $\times n$</td>
</tr>
<tr>
<td style="text-align: left;">Global average pooling</td>
</tr>
<tr>
<td style="text-align: left;">Logits</td>
</tr>
</tbody>
</table>
<p>Table E.2. The Plain- $(8 n+2)$ architecture, used in Figures 3, 5, 7, F.4, F.5, F.6, and F.7. Mean accuracies: Plain-10, 93.9\%; Plain-18: 94.8\%; Plain-34: 93.7\%; Plain-66: 91.3\%</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Width- $n$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$3 \times 3$ conv. $n$-BN-ReLu $\times 2$</td>
</tr>
<tr>
<td style="text-align: left;">$3 \times 3$ conv. $n$ stride 2-BN-ReLu</td>
</tr>
<tr>
<td style="text-align: left;">$3 \times 3$ conv. $n$-BN-ReLu $\times 2$</td>
</tr>
<tr>
<td style="text-align: left;">$3 \times 3$ conv. $n$ stride 2-BN-ReLu</td>
</tr>
<tr>
<td style="text-align: left;">$3 \times 3$ conv. $n$ valid padding-BN-ReLu</td>
</tr>
<tr>
<td style="text-align: left;">$1 \times 1$ conv. $n$-BN-ReLu</td>
</tr>
<tr>
<td style="text-align: left;">Global average pooling</td>
</tr>
<tr>
<td style="text-align: left;">Logits</td>
</tr>
</tbody>
</table>
<p>Table E.3. The architectures used for width experiments in Figure 6.</p>
<h2>F Additional Experiments</h2>
<h2>F.1. Sanity Check for Transformer Encoders</h2>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure F.1. All similarity indices broadly reflect the structure of Transformer encoders. Similarity indexes are computed between the 12 sublayers of Transformer encoders, for each of the 4 possible places in each sublayer that representations may be taken (see Figure F.2), averaged across 10 models trained from different random initializations.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure F.2. Architecture of a single sublayer of the Transformer encoder used for our experiments. The full encoder includes 12 sublayers, alternating between self-attention and feed-forward network sublayers.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Index</th>
<th style="text-align: center;">Layer Norm</th>
<th style="text-align: center;">Scale</th>
<th style="text-align: center;">Attn/FFN</th>
<th style="text-align: center;">Residual</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CCA $(\hat{\rho})$</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">$\mathbf{9 4 . 9}$</td>
<td style="text-align: center;">90.9</td>
</tr>
<tr>
<td style="text-align: left;">CCA $\left(R_{\text {CCA }}^{2}\right)$</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">$\mathbf{9 5 . 3}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">SVCCA $(\hat{\rho})$</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">75.9</td>
</tr>
<tr>
<td style="text-align: left;">SVCCA $\left(R_{\text {CCA }}^{2}\right)$</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">84.7</td>
</tr>
<tr>
<td style="text-align: left;">PWCCA</td>
<td style="text-align: center;">$\mathbf{8 8 . 5}$</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">$\mathbf{9 6 . 1}$</td>
<td style="text-align: center;">87.0</td>
</tr>
<tr>
<td style="text-align: left;">Linear Reg.</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">36.9</td>
</tr>
<tr>
<td style="text-align: left;">CKA (Linear)</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">$\mathbf{9 5 . 6}$</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">73.6</td>
</tr>
<tr>
<td style="text-align: left;">CKA (RBF 0.2)</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">76.2</td>
</tr>
<tr>
<td style="text-align: left;">CKA (RBF 0.4)</td>
<td style="text-align: center;">$\mathbf{9 2 . 3}$</td>
<td style="text-align: center;">$\mathbf{9 6 . 5}$</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">$\mathbf{9 8 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">CKA (RBF 0.8)</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">$\mathbf{9 5 . 8}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 6}$</td>
<td style="text-align: center;">90.0</td>
</tr>
</tbody>
</table>
<p>Table F.1. Accuracy of identifying corresponding sublayers based maximum similarity, for 10 architecturally identical 12-sublayer Transformer encoders at the 4 locations in each sublayer after which the representation may be taken (see Figure F.2). Results not significantly different from the best result are bold-faced ( $p&lt;0.05$, jackknife z-test).</p>
<p>When applied to Transformer encoders, all similarity indexes we investigated passed the sanity check described in Section 6.1. We trained Transformer models using the tensor2tensor library (Vaswani et al., 2018) on the English to German translation task from the WMT18 dataset (Bojar et al., 2018) (Europarl v7, Common Crawl, and News Commentary v13 corpora) and computed representations of each of the 75,804 tokens from the 3,000 sentence newstest2013 development set, ignoring end of sentence tokens. In Figure F.1, we show similarity between the 12 sublayers of the encoders of 10 Transformer models ( 45 pairs) trained from different random initializations. Each Transformer sublayer contains four operations, shown in Figure F.2; results vary based which operation the representation is taken after. Table F. 1 shows the accuracy with which we identify corresponding layers between network pairs by maximal similarity.</p>
<p>The Transformer architecture alternates between self-attention and feed-forward network (FFN) sublayers. The checkerboard pattern in representational similarity after the self-attention/feed-forward network operation in Figure F. 1 indicates that representations of attention sublayers are more similar to other attention sublayers than to FFN sublayers, and similarly, representations of FFN sublayers are more similar to other FFN than to feed-forward network layers. CKA reveals a checkerboard pattern for activations after the channel-wise scale operation (before the attention/FFN operation) that other methods do not. Because CCA is invariant to non-isotropic scaling, CCA similarities before and after channel-wise scaling are identical. Thus, CCA cannot capture this structure, even though the structure is consistent across networks.</p>
<p>F.2. SVCCA at Alternative Thresholds</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure F.3. Same as Figure 2 row 2, but for more SVCCA thresholds than the 0.99 threshold suggested by <em>Raghu et al. (2017)</em>. No threshold reveals the structure of the network.</p>
<h3>F.3. CKA at Initialization</h3>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure F.4. Similarity of the Plain-18 network at initialization. <strong>Left</strong>: Similarity between layers of the same network. <strong>Middle</strong>: Similarity between untrained networks with different initializations. <strong>Right</strong>: Similarity between untrained and trained networks.</p>
<h3>F.4. Additional CKA Results</h3>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure F.6. Networks with and without batch normalization trained from different random initializations learn similar representations according to CKA. The largest difference between networks is at the last convolutional layer. Optimal hyperparameters were separately selected for the batch normalized network (93.9% average accuracy) and the network without batch normalization (91.5% average accuracy).</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure F.5. Similarity between layers at initialization for deeper architectures.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure F.7. Within-class CKA is similar to CKA based on all examples. To measure within-class CKA, we computed CKA separately for examples belonging to each CIFAR-10 class based on representations from Plain-10 networks, and averaged the resulting CKA values across classes.</p>
<h1>F.5. Similarity Between Different Architectures with Other Indexes</h1>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure F.8. Similarity between layers of different architectures (Tiny-10 and ResNet-14) for all methods investigated. Only CKA reveals meaningful correspondence. SVCCA results resemble Figure 7 of Raghu et al. (2017). In order to achieve better performance for CCA-based techniques, which are sensitive to the number of examples used to compute similarity, all plots show similarity on the CIFAR-10 training set.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Google Brain. Correspondence to: Simon Kornblith $&lt;$ skornblith@google.com $&gt;$.</p>
<p>Proceedings of the $36^{\text {th }}$ International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
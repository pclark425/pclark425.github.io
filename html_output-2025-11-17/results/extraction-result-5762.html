<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5762 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5762</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5762</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-259947046</p>
                <p><strong>Paper Title:</strong> Large language models in medicine</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) can respond to free-text queries without being specifically trained in the task in question, causing excitement and concern about their use in healthcare settings. ChatGPT is a generative artificial intelligence (AI) chatbot produced through sophisticated fine-tuning of an LLM, and other tools are emerging through similar developmental processes. Here we outline how LLM applications such as ChatGPT are developed, and we discuss how they are being leveraged in clinical settings. We consider the strengths and limitations of LLMs and their potential to improve the efficiency and effectiveness of clinical, educational and research work in medicine. LLM chatbots have already been deployed in a range of biomedical contexts, with impressive but mixed results. This review acts as a primer for interested clinicians, who will determine if and how LLM technology is used in healthcare for the benefit of patients and practitioners. This review explains how large language models (LLMs), such as ChatGPT, are developed and discusses their strengths and limitations in the context of potential clinical applications.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5762.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5762.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General class of deep neural network language models trained on very large text corpora that learn statistical associations between tokens and can be fine-tuned to perform diverse NLP tasks; discussed in this review as potential tools to analyze large scholarly/clinical corpora and assist research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3, GPT-3.5, GPT-4 (examples cited)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Transformer-based autoregressive language models (e.g., GPT family) with parameter counts ranging from billions (GPT-2) to 175B (GPT-3) and larger/undisclosed for GPT-4; notable features discussed include few-/zero-shot generalization, multimodal input (GPT-4), and fine-tuning with reinforcement learning from human feedback (RLHF).</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Proposed potential: analyze very large corpora of scholarly/clinical text to synthesize generalizable principles, rules, or qualitative laws across studies (e.g., clinical risk patterns, treatment heuristics, high-level biomedical regularities).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Medicine / Biomedical literature / Clinical records / Multidisciplinary scientific corpora (as discussed in review).</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Discussed methods include pretraining on large text corpora, domain-specific fine-tuning, RLHF, chain-of-thought prompting and user-provided examples (few-shot/few-shot-in-context learning); retrieval-augmented access to up-to-date internet sources and human-in-the-loop validation are proposed as necessary augmentations.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>High-level clinical heuristics or generalizable patterns (e.g., common risk factors across studies, summarised principles for triage/diagnosis, or qualitative rules for protein sequence–structure relationships), suggested as possible targets but not empirically extracted in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not applied in this review; the paper recommends human-expert agreement, validated benchmarks, clinical trial endpoints (mortality/morbidity), document-quality metrics, and work-efficiency / user-satisfaction measures for future evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>This review does not present an empirical study that uses LLMs to extract qualitative laws from large scholarly corpora; it only discusses the potential, cites related work, and outlines technical/ethical limitations that currently prevent autonomous deployment for such tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>The review emphasizes human-in-the-loop approaches (expert oversight, reward models trained from human grading, and human validation) as essential for trustworthy distillation of principles.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>General web/text corpora (Common Crawl, Books, Wikipedia) used in base LLM pretraining are described; the review proposes incorporation of domain-specific sources (PubMed, clinical notes, guidelines) and real-time internet access for up-to-date knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Key limitations discussed include lack of recency and domain-verified training data, hallucinations/fact fabrication, black-box interpretability, dataset biases, privacy/legal concerns, vulnerability to adversarial prompts, and absence of validated benchmarks for extracting/validating distilled laws.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Suggested use-cases include large-scale analysis of clinical text to derive insights, counterfactual simulations/virtual clinical trials to inform research prioritization, and leveraging domain-specific LLMs (ClinicalBERT, Med-PaLM 2, GatorTron) as enablers for large-scale text analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models in medicine', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5762.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5762.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ClinicalBERT / PubMedBERT / BioBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain-specific BERT-family language models (ClinicalBERT, PubMedBERT, BioBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BERT-style transformer models pretrained or fine-tuned on biomedical and clinical corpora to improve performance on biomedical NLP tasks; cited in the review as examples of domain adaptation that could enable large-scale extraction of clinical information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>ClinicalBERT, PubMedBERT, BioBERT</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Bidirectional encoder transformer models initialized from BERT and further pretrained or fine-tuned on biomedical literature (PubMed) or clinical notes to capture domain-specific language and semantics; typically smaller than the largest autoregressive LLMs but optimized for biomedical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Facilitate extraction and synthesis of structured clinical information from large collections of clinical notes and biomedical literature, which could feed downstream processes that distill generalizable principles or rules.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Clinical notes / Biomedical literature / Electronic health records (EHRs).</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Domain-specific pretraining and task-specific fine-tuning on labeled biomedical/clinical datasets; these models are invoked in the review as tools to improve biomedical NLP performance relative to general LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Implicit: enable extraction of recurrent clinical patterns and structured information (e.g., common temporal relationships in EHRs, repeated associations between symptoms and diagnoses) that could be summarized as qualitative rules by humans or downstream models.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Biomedical NLP benchmarks (task accuracy/F1 on entity extraction, relation extraction, downstream predictive tasks) are implied but the review does not report law-distillation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The review cites these models as outperforming general LLMs on biomedical NLP tasks and as promising enablers for large-scale clinical text analysis, but does not report any direct extraction of qualitative laws from large scholarly corpora using them.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human supervision and expert validation are emphasized as necessary for trustworthy outputs and for creating labeled data for fine-tuning or validation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Clinical notes, PubMed articles, unstructured EHR data (mentioned as candidate sources for domain-specific training).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Data quality issues in clinical notes and literature, privacy constraints, residual biases, and lack of interpretability hinder direct use for automatic distillation of general laws without careful validation.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>The review references ClinicalBERT, PubMedBERT and BioBERT as concrete domain-tuned models that have improved biomedical NLP performance and could be components in pipelines for synthesizing insights from large text corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models in medicine', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5762.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5762.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Protein / biological sequence LMs (ProGen, AlphaFold mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative and predictive sequence models for proteins (e.g., ProGen) and structure predictors (AlphaFold)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Examples where language-model-like architectures have been applied to biological sequence data to infer generalizable sequence–structure–function relationships; cited as demonstrations that models can learn rules from large corpora of sequence data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>ProGen (protein generative model); AlphaFold (structure predictor) referenced for contrast</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>ProGen: generative transformer-style models trained on large protein sequence databases to produce novel sequences with predicted functions. AlphaFold: deep learning model that predicts 3D protein structure from sequence (not a standard LLM but an example of rule learning from large biological datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Discover and exploit general sequence–structure–function regularities in protein sequence databases to generate functional proteins or predict structure—an analogue to distilling scientific 'laws' from many examples.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Molecular biology / Protein sequences / Genomics.</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Language-model-style training on large sequence corpora (self-supervised next-token prediction) and downstream experimental or computational validation of generated sequences/structures; review cites these as cross-domain examples of powerful pattern learning.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Implicit biological regularities (e.g., sequence motifs governing structure or function) learned and exploited to generate or predict proteins—framed as learned generalizable rules rather than explicit human-readable laws.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Biological function prediction accuracy, experimental validation of generated proteins, structure prediction accuracy (e.g., RMSD), but the review only mentions these approaches at high level and does not detail metrics for law extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The review cites ProGen and AlphaFold as demonstrations that machine learning models trained on large scientific corpora can capture domain regularities that enable generation/prediction, suggesting feasibility of distilling general rules, but provides no empirical law-extraction study itself.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Experimental validation and expert interpretation are required in referenced works; the review stresses the need for human oversight when applying such models in research.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Large protein sequence databases and annotated structural datasets (as used in referenced works).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Translating learned statistical patterns into explicit, trustworthy human-interpretable laws is nontrivial; data biases, limited interpretability, and the requirement of experimental validation are highlighted as constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Cited examples: AlphaFold (structure prediction), ProGen (protein generation), and TSSNote-CyaPromBERT (promoter prediction) as instances where ML on large corpora uncovers useful domain regularities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models in medicine', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5762.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5762.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs for large-scale clinical text analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of LLMs (e.g., GPT-3.5, GatorTron) to analyze large quantities of clinical text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The review mentions that models like GPT-3.5, ClinicalBERT and GatorTron are well placed to enable analysis of large clinical corpora, which could support synthesis of generalizable clinical rules or insights, but no direct law-distillation experiments are reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3.5, GatorTron, other clinical LMs referenced</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Large autoregressive and encoder models fine-tuned or pretrained on clinical/biomedical text to extract information and support downstream tasks; GatorTron is an example of a large clinical LM tailored to EHR data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Aggregate and analyze vast collections of clinical notes and literature to surface recurrent patterns or rules (e.g., timelines of disease progression, common co-morbid associations) that could be summarized as qualitative insights.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Clinical informatics / Electronic health records / Biomedical literature.</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Pretraining on large clinical corpora and application for information extraction, summarization and predictive modeling; the review suggests combining domain fine-tuning with expert validation and possibly retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Examples of target outputs include summarized patient timelines, recurrent associations across patient records, prognostic heuristics—framed as qualitative patterns rather than formal laws.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not reported for law extraction in this review; the paper recommends task-specific validation (e.g., predictive accuracy, expert agreement, and pragmatic clinical trial endpoints) for future work.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The review highlights the potential of these models to process large-scale clinical text to support research and to enable counterfactual simulations or virtual trials, but does not present empirical demonstrations of distilling general laws from scholarly papers.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Strongly emphasized: expert oversight for dataset curation, fine-tuning, validation and governance is recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Unstructured EHRs, patient notes, clinical letters and peer-reviewed medical literature (suggested as sources for domain fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Privacy/legal restrictions on EHR data, noisy and erroneous clinical notes, distributional shifts, and lack of standardized benchmarks for validating distilled clinical principles are discussed as challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models in medicine', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models encode clinical knowledge <em>(Rating: 2)</em></li>
                <li>Large language models are few-shot clinical information extractors <em>(Rating: 2)</em></li>
                <li>Large language models generate functional protein sequences across diverse families <em>(Rating: 2)</em></li>
                <li>Foresight-Generative Pretrained Transformer (GPT) for modelling of patient timelines using EHRs <em>(Rating: 2)</em></li>
                <li>TSSNote-CyaPromBERT: development of an integrated platform for highly accurate promoter prediction and visualization of Synechococcus sp. and Synechocystis sp. through a state-of-the-art natural language processing model BERT <em>(Rating: 1)</em></li>
                <li>Highly accurate protein structure prediction with AlphaFold <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5762",
    "paper_id": "paper-259947046",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [
        {
            "name_short": "LLMs",
            "name_full": "Large Language Models",
            "brief_description": "General class of deep neural network language models trained on very large text corpora that learn statistical associations between tokens and can be fine-tuned to perform diverse NLP tasks; discussed in this review as potential tools to analyze large scholarly/clinical corpora and assist research.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "llm_model_name": "GPT-3, GPT-3.5, GPT-4 (examples cited)",
            "llm_model_description": "Transformer-based autoregressive language models (e.g., GPT family) with parameter counts ranging from billions (GPT-2) to 175B (GPT-3) and larger/undisclosed for GPT-4; notable features discussed include few-/zero-shot generalization, multimodal input (GPT-4), and fine-tuning with reinforcement learning from human feedback (RLHF).",
            "task_goal": "Proposed potential: analyze very large corpora of scholarly/clinical text to synthesize generalizable principles, rules, or qualitative laws across studies (e.g., clinical risk patterns, treatment heuristics, high-level biomedical regularities).",
            "domain": "Medicine / Biomedical literature / Clinical records / Multidisciplinary scientific corpora (as discussed in review).",
            "methodology": "Discussed methods include pretraining on large text corpora, domain-specific fine-tuning, RLHF, chain-of-thought prompting and user-provided examples (few-shot/few-shot-in-context learning); retrieval-augmented access to up-to-date internet sources and human-in-the-loop validation are proposed as necessary augmentations.",
            "type_of_qualitative_law": "High-level clinical heuristics or generalizable patterns (e.g., common risk factors across studies, summarised principles for triage/diagnosis, or qualitative rules for protein sequence–structure relationships), suggested as possible targets but not empirically extracted in this review.",
            "evaluation_metrics": "Not applied in this review; the paper recommends human-expert agreement, validated benchmarks, clinical trial endpoints (mortality/morbidity), document-quality metrics, and work-efficiency / user-satisfaction measures for future evaluation.",
            "results_summary": "This review does not present an empirical study that uses LLMs to extract qualitative laws from large scholarly corpora; it only discusses the potential, cites related work, and outlines technical/ethical limitations that currently prevent autonomous deployment for such tasks.",
            "human_involvement": "The review emphasizes human-in-the-loop approaches (expert oversight, reward models trained from human grading, and human validation) as essential for trustworthy distillation of principles.",
            "dataset_or_corpus": "General web/text corpora (Common Crawl, Books, Wikipedia) used in base LLM pretraining are described; the review proposes incorporation of domain-specific sources (PubMed, clinical notes, guidelines) and real-time internet access for up-to-date knowledge.",
            "limitations_or_challenges": "Key limitations discussed include lack of recency and domain-verified training data, hallucinations/fact fabrication, black-box interpretability, dataset biases, privacy/legal concerns, vulnerability to adversarial prompts, and absence of validated benchmarks for extracting/validating distilled laws.",
            "notable_examples": "Suggested use-cases include large-scale analysis of clinical text to derive insights, counterfactual simulations/virtual clinical trials to inform research prioritization, and leveraging domain-specific LLMs (ClinicalBERT, Med-PaLM 2, GatorTron) as enablers for large-scale text analysis.",
            "uuid": "e5762.0",
            "source_info": {
                "paper_title": "Large language models in medicine",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "ClinicalBERT / PubMedBERT / BioBERT",
            "name_full": "Domain-specific BERT-family language models (ClinicalBERT, PubMedBERT, BioBERT)",
            "brief_description": "BERT-style transformer models pretrained or fine-tuned on biomedical and clinical corpora to improve performance on biomedical NLP tasks; cited in the review as examples of domain adaptation that could enable large-scale extraction of clinical information.",
            "citation_title": "",
            "mention_or_use": "mention",
            "llm_model_name": "ClinicalBERT, PubMedBERT, BioBERT",
            "llm_model_description": "Bidirectional encoder transformer models initialized from BERT and further pretrained or fine-tuned on biomedical literature (PubMed) or clinical notes to capture domain-specific language and semantics; typically smaller than the largest autoregressive LLMs but optimized for biomedical tasks.",
            "task_goal": "Facilitate extraction and synthesis of structured clinical information from large collections of clinical notes and biomedical literature, which could feed downstream processes that distill generalizable principles or rules.",
            "domain": "Clinical notes / Biomedical literature / Electronic health records (EHRs).",
            "methodology": "Domain-specific pretraining and task-specific fine-tuning on labeled biomedical/clinical datasets; these models are invoked in the review as tools to improve biomedical NLP performance relative to general LLMs.",
            "type_of_qualitative_law": "Implicit: enable extraction of recurrent clinical patterns and structured information (e.g., common temporal relationships in EHRs, repeated associations between symptoms and diagnoses) that could be summarized as qualitative rules by humans or downstream models.",
            "evaluation_metrics": "Biomedical NLP benchmarks (task accuracy/F1 on entity extraction, relation extraction, downstream predictive tasks) are implied but the review does not report law-distillation metrics.",
            "results_summary": "The review cites these models as outperforming general LLMs on biomedical NLP tasks and as promising enablers for large-scale clinical text analysis, but does not report any direct extraction of qualitative laws from large scholarly corpora using them.",
            "human_involvement": "Human supervision and expert validation are emphasized as necessary for trustworthy outputs and for creating labeled data for fine-tuning or validation.",
            "dataset_or_corpus": "Clinical notes, PubMed articles, unstructured EHR data (mentioned as candidate sources for domain-specific training).",
            "limitations_or_challenges": "Data quality issues in clinical notes and literature, privacy constraints, residual biases, and lack of interpretability hinder direct use for automatic distillation of general laws without careful validation.",
            "notable_examples": "The review references ClinicalBERT, PubMedBERT and BioBERT as concrete domain-tuned models that have improved biomedical NLP performance and could be components in pipelines for synthesizing insights from large text corpora.",
            "uuid": "e5762.1",
            "source_info": {
                "paper_title": "Large language models in medicine",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Protein / biological sequence LMs (ProGen, AlphaFold mention)",
            "name_full": "Generative and predictive sequence models for proteins (e.g., ProGen) and structure predictors (AlphaFold)",
            "brief_description": "Examples where language-model-like architectures have been applied to biological sequence data to infer generalizable sequence–structure–function relationships; cited as demonstrations that models can learn rules from large corpora of sequence data.",
            "citation_title": "",
            "mention_or_use": "mention",
            "llm_model_name": "ProGen (protein generative model); AlphaFold (structure predictor) referenced for contrast",
            "llm_model_description": "ProGen: generative transformer-style models trained on large protein sequence databases to produce novel sequences with predicted functions. AlphaFold: deep learning model that predicts 3D protein structure from sequence (not a standard LLM but an example of rule learning from large biological datasets).",
            "task_goal": "Discover and exploit general sequence–structure–function regularities in protein sequence databases to generate functional proteins or predict structure—an analogue to distilling scientific 'laws' from many examples.",
            "domain": "Molecular biology / Protein sequences / Genomics.",
            "methodology": "Language-model-style training on large sequence corpora (self-supervised next-token prediction) and downstream experimental or computational validation of generated sequences/structures; review cites these as cross-domain examples of powerful pattern learning.",
            "type_of_qualitative_law": "Implicit biological regularities (e.g., sequence motifs governing structure or function) learned and exploited to generate or predict proteins—framed as learned generalizable rules rather than explicit human-readable laws.",
            "evaluation_metrics": "Biological function prediction accuracy, experimental validation of generated proteins, structure prediction accuracy (e.g., RMSD), but the review only mentions these approaches at high level and does not detail metrics for law extraction.",
            "results_summary": "The review cites ProGen and AlphaFold as demonstrations that machine learning models trained on large scientific corpora can capture domain regularities that enable generation/prediction, suggesting feasibility of distilling general rules, but provides no empirical law-extraction study itself.",
            "human_involvement": "Experimental validation and expert interpretation are required in referenced works; the review stresses the need for human oversight when applying such models in research.",
            "dataset_or_corpus": "Large protein sequence databases and annotated structural datasets (as used in referenced works).",
            "limitations_or_challenges": "Translating learned statistical patterns into explicit, trustworthy human-interpretable laws is nontrivial; data biases, limited interpretability, and the requirement of experimental validation are highlighted as constraints.",
            "notable_examples": "Cited examples: AlphaFold (structure prediction), ProGen (protein generation), and TSSNote-CyaPromBERT (promoter prediction) as instances where ML on large corpora uncovers useful domain regularities.",
            "uuid": "e5762.2",
            "source_info": {
                "paper_title": "Large language models in medicine",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "LLMs for large-scale clinical text analysis",
            "name_full": "Use of LLMs (e.g., GPT-3.5, GatorTron) to analyze large quantities of clinical text",
            "brief_description": "The review mentions that models like GPT-3.5, ClinicalBERT and GatorTron are well placed to enable analysis of large clinical corpora, which could support synthesis of generalizable clinical rules or insights, but no direct law-distillation experiments are reported in this review.",
            "citation_title": "",
            "mention_or_use": "mention",
            "llm_model_name": "GPT-3.5, GatorTron, other clinical LMs referenced",
            "llm_model_description": "Large autoregressive and encoder models fine-tuned or pretrained on clinical/biomedical text to extract information and support downstream tasks; GatorTron is an example of a large clinical LM tailored to EHR data.",
            "task_goal": "Aggregate and analyze vast collections of clinical notes and literature to surface recurrent patterns or rules (e.g., timelines of disease progression, common co-morbid associations) that could be summarized as qualitative insights.",
            "domain": "Clinical informatics / Electronic health records / Biomedical literature.",
            "methodology": "Pretraining on large clinical corpora and application for information extraction, summarization and predictive modeling; the review suggests combining domain fine-tuning with expert validation and possibly retrieval augmentation.",
            "type_of_qualitative_law": "Examples of target outputs include summarized patient timelines, recurrent associations across patient records, prognostic heuristics—framed as qualitative patterns rather than formal laws.",
            "evaluation_metrics": "Not reported for law extraction in this review; the paper recommends task-specific validation (e.g., predictive accuracy, expert agreement, and pragmatic clinical trial endpoints) for future work.",
            "results_summary": "The review highlights the potential of these models to process large-scale clinical text to support research and to enable counterfactual simulations or virtual trials, but does not present empirical demonstrations of distilling general laws from scholarly papers.",
            "human_involvement": "Strongly emphasized: expert oversight for dataset curation, fine-tuning, validation and governance is recommended.",
            "dataset_or_corpus": "Unstructured EHRs, patient notes, clinical letters and peer-reviewed medical literature (suggested as sources for domain fine-tuning).",
            "limitations_or_challenges": "Privacy/legal restrictions on EHR data, noisy and erroneous clinical notes, distributional shifts, and lack of standardized benchmarks for validating distilled clinical principles are discussed as challenges.",
            "uuid": "e5762.3",
            "source_info": {
                "paper_title": "Large language models in medicine",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models encode clinical knowledge",
            "rating": 2,
            "sanitized_title": "large_language_models_encode_clinical_knowledge"
        },
        {
            "paper_title": "Large language models are few-shot clinical information extractors",
            "rating": 2,
            "sanitized_title": "large_language_models_are_fewshot_clinical_information_extractors"
        },
        {
            "paper_title": "Large language models generate functional protein sequences across diverse families",
            "rating": 2,
            "sanitized_title": "large_language_models_generate_functional_protein_sequences_across_diverse_families"
        },
        {
            "paper_title": "Foresight-Generative Pretrained Transformer (GPT) for modelling of patient timelines using EHRs",
            "rating": 2,
            "sanitized_title": "foresightgenerative_pretrained_transformer_gpt_for_modelling_of_patient_timelines_using_ehrs"
        },
        {
            "paper_title": "TSSNote-CyaPromBERT: development of an integrated platform for highly accurate promoter prediction and visualization of Synechococcus sp. and Synechocystis sp. through a state-of-the-art natural language processing model BERT",
            "rating": 1,
            "sanitized_title": "tssnotecyaprombert_development_of_an_integrated_platform_for_highly_accurate_promoter_prediction_and_visualization_of_synechococcus_sp_and_synechocystis_sp_through_a_stateoftheart_natural_language_processing_model_bert"
        },
        {
            "paper_title": "Highly accurate protein structure prediction with AlphaFold",
            "rating": 1,
            "sanitized_title": "highly_accurate_protein_structure_prediction_with_alphafold"
        }
    ],
    "cost": 0.0138745,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large language models in medicine
17 July 2023</p>
<p>Arun James Thirunavukara 0000-0002-7711-7368
Darr Shu Jeng Ting 0000-0001-7416-2350
Kabilan Elangovan 0000-0002-7711-7368
Laura Gutierrez 0000-0001-7416-2350
Ting Fang Tan 
Daniel Shu 
Wei Ting </p>
<p>1 University of Cambridge School of Clinical Medicine,
Cambridge, UK. Cambridge, UK. Birmingham, UK. Birmingham, UK. Nottingham, UK. Singapore, Singapore. Singapore, Singapore. Palo Alto, CA, USA.</p>
<p>2 Corpus Christi College, University of Cambridge,</p>
<p>3 Academic Unit of Ophthalmology, Institute of Inflammation and Ageing, University of Birmingham,</p>
<p>4 Birmingham and Midland Eye Centre,</p>
<p>5 Academic Ophthalmology, School of Medicine, University of Nottingham,</p>
<p>6 Artificial Intelligence and Digital Innovation Research Group, Singapore Eye Research Institute, Singapore National Eye Centre,</p>
<p>7 Department of Ophthalmology and Visual Sciences, Duke-National University of Singapore Medical School,</p>
<p>8 Byers Eye Institute, Stanford University,</p>
<p>Large language models in medicine
17 July 20235F888934A8AB38BFA5CBBCCDA53EE2B910.1038/s41591-023-02448-8Received: 24 March 2023 Accepted: 8 June 2023
Peer review information Nature Medicine thanks Melissa McCradden, Pranav Rajpurkar and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.Primary handling editor: Karen O'Leary, in collaboration with the Nature Medicine team.</p>
<p>Large language models (LLMs) can respond to free-text queries without being specifically trained in the task in question, causing excitement and concern about their use in healthcare settings.ChatGPT is a generative artificial intelligence (AI) chatbot produced through sophisticated fine-tuning of an LLM, and other tools are emerging through similar developmental processes.Here we outline how LLM applications such as ChatGPT are developed, and we discuss how they are being leveraged in clinical settings.We consider the strengths and limitations of LLMs and their potential to improve the efficiency and effectiveness of clinical, educational and research work in medicine.LLM chatbots have already been deployed in a range of biomedical contexts, with impressive but mixed results.This review acts as a primer for interested clinicians, who will determine if and how LLM technology is used in healthcare for the benefit of patients and practitioners.</p>
<p>Large language models (LLMs) are artificial intelligence (AI) systems that are trained on billions of words derived from articles, books and other internet-based content.Typically, LLMs use neural network architectures (see Box 1 for a glossary of terms) that leverage deep learning -already used with impressive results across medicine -to represent the complicated associative relationships between words in the text-based training dataset 1,2 .Through this training process, which may be multi-staged and involve variable degrees of human input, LLMs learn how words are used with each other in language and can apply these learned patterns to complete natural language processing tasks.</p>
<p>Natural language processing describes the broad field of computational research aiming to facilitate automatic analysis of language in a way that imitates human ability 3 .Generative AI developers aim to produce models that can create content on demand and intersect with natural language processing within applications, such as chatbots and text prediction -in other words, 'natural language generation' tasks 4 .After many years of development, LLMs are now emerging with 'few-shot' or 'zero-shot' properties (Box 1), meaning that they can recognize, interpret and generate text with minimal or no specific fine-tuning 5,6 .These few-shot and zero-shot properties emerge once model size, dataset size and computational resources are sufficiently large 7 .As development of deep learning techniques, powerful computational resources and large datasets for training have advanced, LLM applications with the potential to disrupt cognitive work across sectors -including healthcare -have begun to appear (Fig. 1) 5,[8][9][10][11] .</p>
<p>ChatGPT (OpenAI) is an LLM chatbot: a generative AI application that now produces text in response to multimodal input (having previously accepted only text input) 12 .Its backend LLM is Generative Pretrained Transformer 3.5 or 4 (GPT-3.5 or GPT-4), described below 13,14 .ChatGPT's impact stems from its conversational interactivity and near-human-level or equal-to-human-level performance in cognitive tasks across fields, including medicine 14 .ChatGPT has attained passing-level performance in United States Medical Licensing Examinations, and there have been suggestions that LLM applications may be ready for use in clinical, educational or research settings [14][15][16] .However, potential applications and capacity for autonomous</p>
<p>Review article</p>
<p>https://doi.org/10.1038/s41591-023-02448-8</p>
<p>Development of LLM chatbots</p>
<p>Gross size of an LLM is not the only important factor governing its utility: ChatGPT is currently generating the greatest interest in healthcare research despite its initial backend LLM, GPT-3.5, not exhibiting the greatest number of parameters (Fig. 1) 5,11 .This is thanks to sophisticated fine-tuning, specifically to respond appropriately to human input queries 13 .ChatGPT and its backend LLMs, GPT-3.5 and GPT-4, offer a useful case study to illustrate the architecture, resources and training required to develop state-of-the-art LLM applications, although the most recent technical developments remain confidential.</p>
<p>The first version of GPT (GPT-1) was released in 2018 (ref.19).GPT-1's training was semi-supervised, consisting of initial unsupervised pretraining to program the associative relationships between words as used in language, followed by supervised fine-tuning to optimize performance in specified natural language processing tasks 19 .To simplify optimization, structured input queries (for example, causally ordered passages, discrete passages and multiple choice questions and answers) were transformed into single linear sequences of words 19 .For pretraining, GPT-1 used the BooksCorpus dataset, a collection of 11,308 novels containing around 74 million sentences, or 1 × 10 9 words.The general performance for this new type of model was remarkablesuperior to bespoke models in nine of 12 natural language processing tasks, with acceptable zero-shot performance in many cases 19 .</p>
<p>With 1.5 billion parameters, GPT-2 (released in 2019) was 10 times larger than its predecessor 20 .Its training data were derived from WebText, a 40-gigabyte (GB) dataset derived from over 8 million documents.GPT-2 was initially evaluated on several natural language processing tasks -reading comprehension, summarization, translation and question answering -outperforming many bespoke models trained specifically for narrow use-cases, even in zero-shot settings 20 .GPT-2 demonstrated the ability of larger models to perform in unfamiliar tasks at state-of-the-art level but was notably weaker in text summarization tasks, where its performance was similar to or lesser than bespoke models 20 .Performance was improved in few-shot settings or with task prompts, illustrating the ability of these LLMs to integrate prompt information to better achieve users' aims 20 .</p>
<p>In 2020, GPT-3 was released -with 175 billion parameters, over 100 times larger than GPT-2 (refs.5,20).Its more extensive training conferred greater few-shot and no-shot abilities, achieving state-of-the-art performance in a wide variety of natural language processing tasks 5 .The training dataset consisted of five corpora, comprising 45 terabytes (TB): Common Crawl (webpages), WebText2, Books1, Books2 and Wikipedia 5 .In general, development of GPT-3 specifically addressed the weaknesses of its predecessors to engineer the most sophisticated LLM yet 5,19,20 .GPT-4 has now been released and has attained even higher performance than GPT-3 in natural language processing as well as diverse professional competency tests 14 .Moreover, GPT-4 accepts multimodal input: images can be included in user queries 14 .Its architecture, development and training data remain confidential, but GPT-4 has already been implemented in a version of ChatGPT and is becoming accessible through an application programming interface (API) 14 .</p>
<p>The pretraining task underlying published GPT models is termed language modeling: predicting the next and/or previous 'token' (usually analogous to 'word') in a sequence or sentence 11,21 .Other models pretrained through language modeling include LLaMA, MT-NLG, Language Model for Dialogue Applications (LaMDA), Anthropic-LM, Pathways Language Model (PaLM) and Open Pretrained Transformer (OPT) (Fig. 1) 11,22 .Many alternative training schemata exist, ranging from masked language modeling (cloze tasks: predicting masked tokens in a sequence) and permuted language modeling (language modeling with randomly sampled input tokens) to denoising autoencoding (recovering undistorted inputs after intentional corruption) and next-sentence prediction (distinguishing whether sentences are contiguous or not).Models developed using these alternative schema include Gato, DALL-E, Enhanced Language Representations deployment are debatable: written examinations are unvalidated indicators of clinical performance, and a lack of good benchmarks makes appraisal of performance a substantial challenge 17 .It seems likely that current LLM technology will be most effectively leveraged as a tool under close supervision 14,16,17 .</p>
<p>This review explores state-of-the-art LLM applications in medicine, using ChatGPT as an illustrative example.First, LLM development is explained, outlining model architecture and training processes employed in developing these models.Next, the applications of LLM technology in medicine are discussed, with a focus on published use-cases.The technical limitations and barriers to implementation of LLM applications are then described, informing future directions for fruitful research and development.LLMs are now at the forefront of medical AI with immense potential to improve the efficiency and effectiveness of clinical, educational and research work, but they require extensive validation and further development to overcome technological weaknesses 18 .</p>
<p>Box 1</p>
<p>Glossary of common terms in LLM development</p>
<p>Computational resources: the hardware required to train and deploy a machine learning model, including processing power, memory and storage.</p>
<p>Deep learning: a variant of machine learning involving neural networks with multiple layers of processing 'perceptrons' (nodes), which together facilitate extraction of higher features of unstructured input data (for example, images, video and text).</p>
<p>Few-shot learning: AI developed to complete tasks with exposure to only a few initial examples of the task, with accurate generalization to unseen examples.</p>
<p>Generative artificial intelligence: computational systems capable of producing content, such as text, images or sound, on demand.</p>
<p>Large language model: a type of AI model using deep neural networks to learn the relationships between words in natural language, using large datasets of text to train.</p>
<p>Machine learning: a field of AI featuring models that enable computers to learn and make predictions based on input data, learning from experience.</p>
<p>Model size: the number of parameters in an AI model; LLMs consist of layers of communicating nodes that each contain a set of parameters that are optimized during training.</p>
<p>Natural language processing: a field of AI research focusing on the interaction between computers and human language.</p>
<p>Neural network: computing systems inspired by biological neural networks, comprising 'perceptrons' (nodes), usually arranged in layers, communicating with one another and performing transformations upon input data.</p>
<p>Parameter: a variable within a machine learning model that is tuned -usually automatically -during training to maximize performance.In deep learning, parameters are the 'weights' or data transforming functions comprising neural network nodes.</p>
<p>Semantic tasks: natural language processing tasks requiring understanding of the meaning of linguistic inputs at a deeper level than the simplest surface level of words and grammar.</p>
<p>Zero-shot learning: AI developed to complete tasks without exposure to any previous examples of the task.</p>
<p>Review article</p>
<p>https://doi.org/10.1038/s41591-023-02448-8</p>
<p>with Informative Entities (ERNIE), Bidirectional Encoder Representations from Transformers (BERT) and Bidirectional and Auto-regressive Transformers (BART) (Fig. 1) 11 .</p>
<p>From LLM to generative AI chatbot</p>
<p>Further fine-tuning of an LLM is required to develop useful applications, as seen in the engineering of GPT-3.5, which produces appropriate responses to free-text input prompts (Fig. 2) 13 .Here, fine-tuning involved exposing GPT-3 to prompts and responses produced by human researchers acting the part of an application user and AI assistant; this facilitated model learning of how to answer custom queries properly.Next, 'reinforcement learning from human feedback' (RLHF) was conducted using a reward model trained on data generated by human graders tasked with ranking GPT-3.5 responses to a set of queries 13 .This reward model enabled autonomous RLHF at a far greater scale than could be achieved through manual grading of every single model response by humans 13 .To improve security and safety, further autonomous adversarial training was completed using model-generated input queries and outputs 13 .</p>
<p>Subsequent versions of ChatGPT, now integrating GPT-4 as its backend LLM, have not been explained, as new architecture, datasets and training are confidential 14 .However, it is plausible that similar principles are applied to those observed in the training of GPT-3.5 and initial versions of ChatGPT, as newer and older models are prone to similar sorts of error -although new training schemata may have been developed using data derived from a rapidly growing userbase (Fig. 2, dotted arrow) 23 .Even within individual conversations, ChatGPT exhibits a remarkable ability to 'learn', with performance improved particularly by providing examples of the task it is challenged withgoing from no-shot to few-shot execution.The provision of examples by users enables LLMs to train themselves in a process similar to the fine-tuning employed in their initial development 24 .</p>
<p>Other LLM chatbots besides ChatGPT are available to clinicians and patients.Bing's AI chatbot (Microsoft) facilitates access to GPT-4 without premium access to ChatGPT 25 .Sparrow (DeepMind) was built using the LLM 'Chinchilla' and reduces inaccuracy and inappropriateness by leveraging Google search results, human feedback and an extensive initializing prompt -591 words long -containing 23 explicit rules 26 .Adversarial testing of ChatGPT does not reveal a similar initializing prompt, although these tests are inconclusive, as security measures may have been implemented to conceal initial instructions.BlenderBot 3 (Meta Platforms) also leverages internet access to improve accuracy, using OPT as its backend LLM 27,28 .BlenderBot 3 may continue to improve performance over time through use of organically generated data after its release, as described with relation to ChatGPT (Fig. 2, dotted arrow) 27 .Google Bard was initially built using LaMDA but now leverages PaLM 2, which rivals GPT-4 in terms of general and domain-specific aptitude 29 .HuggingChat offers a free-to-access  LLMs are ordered by date of publication, with the oldest models at the top.Many have been developed with parameters in the order of billions.However, size is clearly not the only measure of progress: many previous models feature more parameters than the models currently generating the greatest impact in healthcare.For instance, GPT-3 (from which GPT-3.</p>
<p>Review article</p>
<p>https://doi.org/10.1038/s41591-023-02448-8</p>
<p>chatbot with a similar interface to ChatGPT but uses Large Language Model Meta AI (LLaMA) as its backend model 30 .Finally, cheap imitations of state-of-the-art LLM chatbots may be developed by individuals with access to relatively modest processing power 31 .</p>
<p>In their current form, LLMs are not poised to replace doctors, as competence in specialized examinations is far from perfect, raising serious issues of inaccuracy and uncertainty (in addition to ethical concerns, as described below) 16 .Although recently reported performance across professional benchmarks has been impressive, specific evaluation and validation are required to demonstrate effectiveness and utility in any specific context [14][15][16] .Fundamentally, clinical practice is not the same as answering examination questions correctly, and finding appropriate benchmarks to gauge the clinical potential of LLMs is a substantial challenge 17 .Nevertheless, encouraging results suggest that available technology is already well placed to impact clinical practice, and further development may accelerate and broaden the applications of natural language processing AI in medicine.</p>
<p>Reducing economic, computational and environmental costs of development</p>
<p>The development of GPT-3 and GPT-4 relied on some of the most powerful computational hardware available, provided by Microsoft Azure 5,32 .This energy-intensive infrastructure has a substantial carbon footprint, and considerable investment is committed to improving hardware and software efficiency to minimize the environmental costs of development [33][34][35][36] .The cost and energy requirement to train LLMs has been trending downwards, with expectations of reaching a personally affordable level by around 2030 (ref.37).However, rapid innovation is accelerating progress even quicker than predicted.For example, researchers fine-tuned a small (7-billion-parameter) version of LLaMA using queries and outputs produced using the GPT-3.5 API 31 .The daughter model, Alpaca, achieves similar performance to GPT-3.5 despite its much smaller architecture, a training time in the order of hours and a total cost of less than US$600 (ref.31).The performance of models produced with larger LLMs as a base, such as the 65-billion-parameter version of LLaMA -if fine-tuned with data derived from GPT-4, PaLM 2 or subsequently developed LLMs -could yield even more impressive results.In addition to reducing the economic cost and environmental impact of training high-performance models, such methods could massively increase the accessibility of LLMs.For instance, substantial reductions in the resource requirement for development of high-performance LLMs could democratize this technology, allowing more clinicians to develop tools for specific clinical purposes and enabling researchers in lower-income and middle-income countries to develop and adopt LLM applications.</p>
<p>However, the development of such 'imitations' could have serious implications for corporations investing large sums of money in developing state-of-the-art models.Even if training data, model architecture and fine-tuning protocols are kept completely confidential, as with GPT-4, providing access at scale (such as through an API) allows external researchers to build a sufficient bank of questions and answers from the parent model to enable fine-tuning of open-source LLMs and produce interactive daughter models, with performance approximating that of the parent model 14,31 .Cheap imitations may compromise the competitive moat incentivizing investment in this sector and may lead to companies restricting access to their models.For example, future cutting-edge LLMs may not offer API access without a binding agreement to not develop competing models.Moreover, proliferation of daughter models introduces another layer of uncertainty regarding processing, exacerbating 'black box' issues as outlined below.</p>
<p>Medical applications of LLM technology</p>
<p>In recent months, many use-cases of LLM technology, particularly ChatGPT, have been reported (Fig. 3).High-quality research is essential to establish the strengths and limitations of new technology, but few well-designed, pragmatic trials have sought to establish the utility of  GPT-3 -trained through word prediction tasks using a vast dataset of text sourced from the internet -was fine-tuned to develop GPT-3.5.Fine-tuning involves exposure of the model to prompt-output pairings generated by humans, allowing the model to learn how to respond appropriately to queries.</p>
<p>To develop ChatGPT, RLHF was employed.RLHF employs a reward model trained using human grading of a limited number of GPT-3.5 outputs to a list of prompts.This reward model could be used with a much larger list of prompts to facilitate training at greater scale than could be achieved with human grading of every individual output.The architecture and training processes of GPT-4 and subsequent versions of ChatGPT are confidential but likely apply similar principles, as both models are liable to similar types of error.Adapted from Ouyang et al. 13 .</p>
<p>Review article</p>
<p>https://doi.org/10.1038/s41591-023-02448-8</p>
<p>implementing innovative LLM-based tools in clinical, educational or research settings.</p>
<p>Clinical applications</p>
<p>ChatGPT drew particular attention in medicine for attaining passing grades in United States Medical Licensing Examinations, and the performance of GPT-4 is markedly higher than its predecessor, GPT-3.5 (ref.15,38).Med-PaLM 2 (Google), a version of PaLM 2 fine-tuned on medical data, has recently attained state-of-the-art results, attaining close to expert human clinician level 39 .When ChatGPT responses to patient queries are compared to those provided by doctors (replying on a social network in their free time), the LLM output is preferred in terms of quality and empathy when assayed as a qualitative metric by doctor judges 17 .This has led to suggestions that AI is ready to replace doctors, but the reality is not quite so dramatic 17,[40][41][42] .Performance is far from perfect even in medical student examinations, with no reported scores approaching 100% 14,15,38,43,44 .ChatGPT has been shown to fail specialist examinations for doctors and provide inaccurate information in response to realistic patient queries regarding cardiovascular disease prevention 16,45 .Despite exhibiting an ability to interpret clinical vignettes and answer related questions, LLMs often fail to provide information to suit patients' individual circumstances [46][47][48] .These data preclude autonomous deployment for decision-making or patient communication, particularly as patients are often unable to distinguish between information provided by LLMs and human clinicians 49,50 .As conse cutive models tend to make quantitative but not qualitative gainsvulnerable to the same weaknesses, albeit at lower frequency -this is the likely status quo, at least for the foreseeable future 14,22,50 .Domain-specific LLMs may prove useful by providing novel functionality.Foresight -a model with GPT architecture fine-tuned with unstructured data corresponding to 811,336 patient electronic health records -demonstrated effectiveness in predicting and prognosticating in validation studies 51 .General risk models could provide a powerful alternative to the current myriad of tools used to stratify and triage patients.Other potential uses include counterfactual simulations and virtual clinical trials, which could accelerate clinical research by facilitating valuable risk-reward inferences that could inform researchers about which studies are most likely to provide value to patients 51 .Novel architectures, such as Hybrid Value-Aware Transformer (HVAT), may further improve performance of LLMs by enabling integration of longitudinal, multimodal clinical data 52 .</p>
<p>ChatGPT exhibits much stronger performance in tasks where specialist knowledge is not required or is provided in user prompts 5,22,32 .This illuminates avenues for implementation with more immediate promise than with clinical decision aids 53 .LLMs are capable of rapid assimilation, summarization and rephrasing of information that could reduce the administrative burden on clinicians.Discharge summaries are an instructive example -repetitive tasks involving interpretation and compression of information with little problemsolving or recollection required 54 .Emerging multimodal models will expand capabilities and compatibility with more sources of data; even doctors' handwriting may be interpreted automatically and accurately 14 .Microsoft and Google aim to integrate ChatGPT and PaLM 2, respectively, across the administrative workflow, allowing information from video calls, documents, spreadsheets, presentations and e-mails to be seamlessly and automatically integrated 55,56 .However, deployment in clinical contexts, where patient well-being is at risk, requires extensive validation 57 .Quality appraisal is essential to ensure that patient safety and administrative efficiency are not compromised, and specific governance structures are required to allocate responsibility 58 .</p>
<p>Current limitations
• Accuracy • Recency • Coherence • Transparency/ interpretability • Ethics</p>
<p>Clinical applications</p>
<p>Administrative tasks (e.g., letters, discharge summaries); decision aids (semi-autonomous); multimodal synthesis</p>
<p>Research applications</p>
<p>Critical appraisal; writing; novel (non-language) techniques</p>
<p>Educational applications</p>
<p>On-demand interactive teaching (e.g., Socratic tutor); material production</p>
<p>Opportunities for development</p>
<p>Fig. 3 | Limitations, priorities for research and development and potential use-cases of LLM applications.</p>
<p>LLMs are now at the forefront of medical AI and have great potential in clinical work, education and research.The barriers to immediate implementation in these three domains represent opportunities for further development that may be explored by LLM developers and independent research teams.Currently, LLMs are limited in medicine by their lack of accuracy, recency, coherence and transparency and by ethical concerns.LLM technology may nevertheless have a substantial impact on how medical work is done, particularly where stakes are lower, where personal data are not required and where specialist knowledge is either not required or is provided by the user.</p>
<p>Review article</p>
<p>https://doi.org/10.1038/s41591-023-02448-8</p>
<p>Educational applications</p>
<p>The strong performance of GPT-4 and Med-PaLM 2 in medical tests suggest that LLMs may be useful teaching tools for students currently attaining a lower level in such tests 38,59 .GPT-4's meta-prompt feature allows users to explicitly describe the desired role for the chatbot to take on during conversation; useful examples include a 'Socratic tutor mode', which encourages students to think for themselves by pitching questions at decreasing levels of difficulty until students are able to work out solutions to the fuller question at hand.Conversation logs could empower human teachers to monitor progress and cater teaching to directly address students' weaknesses.Khan Academy, a not-for-profit educational organization, is actively researching how to implement AI tools, such as GPT-4, in 'Khanmigo' to optimize online teaching 60 .Duolingo, a primarily free platform for learning languages, has implemented GPT-4 in roleplay and answer explanation features to improve the interactivity of online learning 61 .Similar tools could potentially augment medical education 15 .</p>
<p>However, caution is warranted, as frequent mistakes -especially in medicine -and the lack of an uncertainty indicator to accompany outputs represent a considerable problem for LLM teachers: how can students know if they are being taught accurately? 15,16,62Perpetuating falsehood and bias is a risk of LLM adoption.Despite these limitations, LLM tools may be used with expert oversight to efficiently produce material for teaching at an unprecedented scale, such as clinical vignettes, assessment questions and content summaries 63 .Multimodal LLMs could allow teachers to more quickly integrate and analyze student-produced material in diverse formats, with similar benefits to those described with clinical use-cases.</p>
<p>Research applications</p>
<p>As with clinical use-cases, the inaccuracy of LLMs precludes autonomous deployment, but deployment in an assisting role may markedly improve efficiency.Models can be instructed to summarize information succinctly, write at length to describe a set of provided results or rewrite passages to suit specified readers or audiences.Models fine-tuned with domain-specific information may exhibit superior performance, with examples derived from one LLM (BERT), including PubMedBERT and BioBERT 64,65 .This could reduce the burden of critical appraisal, research reporting and peer review, which forms a substantial component of researchers' workload 66 .Issues concerning accountability would be ameliorated by ensuring that clinicians and researchers using these tools are responsible for their output 67 .</p>
<p>LLMs may facilitate novel research, such as analysis of language at greater scale than previously possible.Demonstrative examples include ClinicalBERT, GPT-3.5 and GatorTron, which are well placed to enable researchers to efficiently analyze large quantities of clinical text data [68][69][70] .LLMs may also drive research in less obviously related domains, as text-based information encompasses more than just human language.For instance, genetic and protein structure data are usually represented in text form and are amenable to natural language processing techniques facilitated by LLMs.Models are already generating impressive results: AlphaFold deduces protein structure from amino acid sequences; ProGen generates protein sequences with predictable biological function; and TSSNote-CyaPromBERT identifies promotor regions in bacterial DNA [71][72][73] .Finally, generative AI applications used to analyze patient data may also be used to produce synthetic data; with appropriate quality assessment, this could augment clinical research by increasing the scale of the training corpora available to develop LLM and other AI tools 74 .</p>
<p>Barriers to implementation of generative AI LLMs</p>
<p>There are several issues and limitations preventing clinical deployment of ChatGPT and other similar applications at scale (Table 1).First, training datasets are not sufficient to ensure that generated information is accurate and useful.One reason for this is a lack of recency: GPT-3.5 and GPT-4 (ChatGPT's backend LLMs) were trained mostly using text generated up to September 2021 (refs.14,75).As research and innovation are continuous across fields, including medicine, a lack of more recent content may exacerbate inaccuracies.The issue is especially problematic where language changes suddenly, such as where researchers invent new terminology or change how particular words are used to describe new discoveries and methods.Issues also arise with paradigm shifts -for example, where something that was assumed to be impossible is achieved.Topical examples include development of Coronavirus Disease 2019 (COVID-19) vaccines at unprecedented speed and anti-tumor pharmaceuticals directed against previously 'undruggable' targets, such as KRAS 76,77 .Should similar events breach the training dataset threshold date, models will inevitably provide poor-quality responses to related queries.Consultation with healthcare professionals, therefore, remains essential.</p>
<p>Review article</p>
<p>https://doi.org/10.1038/s41591-023-02448-8</p>
<p>Second, training data are not verified for domain-specific accuracy, which leads to an issue of 'garbage in, garbage out' -described (more eloquently) by Charles Babbage, the father of modern computing, as long ago as 1864 (ref.78).GPT-3.5 is trained on data from books, Wikipedia and the wider internet, with no mechanisms designed to cross-check or validate the accuracy of these texts 5 .Despite the impressive size of the LLM, with 175 billion parameters, GPT-3.5 uses only 570 GB for initial training -a mere fraction of the data available on the internet, estimated as 120 zettabytes (1.2 × 10 14 GB) 5,79 .However, the relative scarcity of diverse, high-quality text data may nevertheless limit datasets, and recent estimates suggest that new text for training may run out in a matter of years 36,80 .Moreover, ChatGPT has no real-time access to the internet when responding to queries, so its knowledge base is fundamentally limited 14 .Alternative applications have been developed that can access the internet when generating responses, such as BlenderBot 3 and Sparrow 26,27 .</p>
<p>Third, LLMs are not trained to understand language as humans do.By 'learning' the statistical associations between words as they have been used by humans, GPT-3 develops an ability to successfully predict which word best completes a phrase or sentence 5 .Through intensive fine-tuning and further training, subsequent models may develop an ability to produce plausible-sounding, coherently phrased -but not necessarily accurate -responses to queries 16 .So-called 'hallucinations' have been widely reported, where inaccurate information is invented (as it is not represented in the training dataset) and espoused lucidly; an alternative term such as 'fact fabrication' is preferred to avoid inappropriate anthropomorphism 81,82 .On the other hand, LLMs may be stimulated to self-improve: chain-of-thought prompting combined with encouragement of self-consistency facilitated autonomous fine-tuning that resulted in a 5-10% improvement in reasoning by an LLM with 540 billion parameters 83,84 .However, inconsistent accuracy and a lack of uncertainty indicators necessitate caution with deployment 16 .</p>
<p>Fourth, LLM processing is a 'black box' that makes interpretability of processing and decision-making challenging 85 .Responses are not referenced or explained unless explicitly requested, and the actual representativeness of explanations is unclear.This compounds accuracy issues, as it is not obvious how models should be retrained or fine-tuned to improve performance.The problem is best illustrated by reference to another form of generative AI based on GPT-3, DALL-E 2 -an application that generates images in response to text-based prompts 86 .For example, users worried about skin cancer may use DALL-E 2 to find out how melanoma would look on their skin, but generated images are not necessarily accurate.Similar issues undoubtedly complicate ChatGPT, potentially leading to false reassurance and relayed diagnosis 16 .Explainable AI initiatives may improve interpretability, but such research in the context of natural language processing is relatively nascent, and contemporary techniques across machine learning appear insufficient to truly engender trust 87,88 .</p>
<p>Fifth, ethical concerns have arisen with the advent of generative AI models capable of producing responses indistinguishable from human-written text 49,85,89 .Using a model trained on biased data (for example, unverified content from books and the internet) risks perpetuating those biases 22 .Many other risks posed by LLM applications have been noted, but discussion here focuses on those most pertinent in clinical contexts.Research acceleration facilitated by LLM cognitive assistance could feasibly lead to dangerous declines in safety standards and ethical consideration 23,32,41,85 .Although ChatGPT is explicitly designed to reduce these risks, issues remain and have been widely reported, and adversarial prompts may be used to 'jailbreak' Chat-GPT, evading its inbuilt rules 90,91 .Despite intensive work to ameliorate these vulnerabilities, GPT-4 remains vulnerable to adversarial prompt approaches, such as 'opposite mode' and 'system message attack' 32 .Many prominent figures in big tech, industry and academia are concerned about these risks, and an open letter calling for a pause on development has attracted attention worldwide 41 .However, a lack of signatories representing leaders in LLM development suggests that innovation will continue, with developers taking responsibility for the safety of their releases 14 .</p>
<p>In addition, security and privacy concerns come hand-in-hand with adoption of internet-based platforms, particularly when run by a commercial enterprise 92 .These concerns could limit deployment opportunities if patient-identifiable data are prohibited from being input as model prompts.GPT-4 also introduces risks of person identification through assimilation of its large training data and multimodal input prompts 32 .Incorporation of personal data during model training is irreversible, conflicting with legal rights such as the General Data Protection Regulation 'right to be forgotten' 93 .Ultimately, these prohibitions and regulations are up to humans to follow, but autonomous applications raise a serious issue of accountability.</p>
<p>Scientific journals moved quickly to stop the accreditation of ChatGPT as an author, suggesting that the technology cannot provide the accountability required for authorship and should, instead, be treated like any other methodological tool assisting humans with their work [94][95][96] .Until use-cases emerge in more detail, it is difficult to envisage and design governance structures to establish accountability where AI contributes to clinical decisions.A more fundamental ethical concern lies within the issue of which tasks LLMs should be allowed to assist with or participate in.Although utilitarian arguments may be made to justify any intervention proven to improve patient outcomes, stakeholders must reach a consensus on the acceptability of AI involvement -autonomous, semi-autonomous or as an entirely subordinate tool.</p>
<p>Finally, gauging the performance of LLMs in clinical tasks represents a considerable challenge.Early quantitative studies focused on examinations, which are unvalidated measures of clinical aptitude in real-world settings 15,16,44 .Qualitative appraisal has been employed in artificial settings, such as social media arenas, for provision of advice by volunteer doctors 17 .Ultimately, clinical interventions using LLMs should be tested in randomized controlled trials evaluating the effect on mortality and morbidity, but what benchmark should be used to determine whether an intervention is suitable for such an expensive and risky trial?These open questions, and approaches to answering them, are discussed in greater depth in the next section.</p>
<p>Directions for future LLM research and development</p>
<p>The limitations outlined above provide useful indications of where subsequent research and development should focus to improve the utility of LLM applications (Fig. 3).Incorporation of domain-specific text during training can improve performance in clinical tasks 97 .Potential data sources include clinical text (for example, patient notes and medical letters) and accurate medical information (for example, guidelines and peer-reviewed literature).Existing models built or fine-tuned with clinical text include ClinicalBERT, Med-PaLM 2 and GatorTron, which have collectively outperformed various general LLMs in biomedical natural language processing tasks 39,70,98 .Up-to-date knowledge could be sourced from the internet in real time rather than relying on limited pretraining datasets; Bing AI and Google Bard already have this functionality, and ChatGPT is following suit as it begins to accept plugins 28 .However, frequent errors in medical notes, scientific literature and other internet material will continue to hamper LLM performance; clinical practice, scientific inquiry and dissemination of knowledge are not, and will never be, executed perfectly 99,100 .Dataset quality could be improved by secondary verification, but the volumes of text involved likely preclude completely manual quality assessment.Machine learning solutions -involving initial manual grading by experts, with the results used to train an automatic model to process data at larger scale -may be optimal in terms of balancing efficiency and effectiveness, illustrated by the reward model employed to optimize ChatGPT (Fig. 2) 13 .Additionally, task-specific fine-tuning guided by expert</p>
<p>Review article</p>
<p>https://doi.org/10.1038/s41591-023-02448-8validation (perhaps augmented with machine learning) may improve the accuracy and safety of outputs 58 .</p>
<p>Currently, fabricated facts and other errors inhibit confidence in LLM outputs and necessitate close oversight, particularly in high-stakes healthcare environments [14][15][16] .Before accuracy improves to match or exceed human expert performance, development of uncertainty indicators could facilitate deployment in semi-autonomous roles, provided that responsible clinicians are introduced into the loop where applications cannot provide useful information.Google Bard initially implemented safeguards that prevented the model from answering many clinical questions, but this broad-brush approach limits development and implementation of healthcare tools.</p>
<p>Where LLMs are used as tools, issues of responsibility and credit must be addressed 96,[101][102][103] .Peer-reviewed journals have taken a variety of approaches to the issue -some outright banning use, others requiring explicit description of use 40,94,[104][105][106] .Cambridge University Press has released explicit guidance summarized in four points 107 .First, use of AI must be declared and clearly explained (as with other software, tools and methodologies); second, AI does not meet the requirements for authorship; third, AI-generated text must not breach plagiarism policies; and fourth, authors are accountable for the accuracy, integrity and originality of text produced with or without AI.However, it is unclear how any regulations will be enforced: although tools are being developed to detect AI-generated language, their accuracy is currently very poor, particularly with shorter segments of text 108 .'Watermarking' protocols could facilitate high-quality text generation with detectable signatures signaling LLM involvement, but this is not currently being implemented in the most popular models 109 .Ethics problems and solutions may be use-case specific, but human oversight may be a successful general approach to mitigating risk and ensuring that accountable individuals remain responsible for clinical decisions.Although this limits potential applications to semi-autonomous AI, these could nevertheless revolutionize clinical work by automating some time-consuming cognitive labor 14 .</p>
<p>Other ethical concerns are difficult to investigate in uninterpretable black box models 87 .As a result, despite lots of demonstrations of bias in the literature, investigative research and mitigating strategies are far more limited 54,[110][111][112] .The Crowdsourced Stereotype Pairs (CrowS-Pairs) benchmark enables quantification of bias, with 50% corresponding to a 'perfect' lack of American stereotyping 113 .Worryingly, all tested LLMs exhibit bias 22,113 .However, active development has reduced the incidence of biased and dangerous output, with GPT-4 evaluated as 82% less likely than its predecessor, GPT-3.5, to respond to requests for disallowed content 14 .To work with these currently ubiquitous biases, 'data statements' may be employed to provide contextual information relating to datasets that may inform researchers and consumers about the generalizability of reported performance and conclusions 114 .On the other hand, explainable AI initiatives that address the black box issue and facilitate deeper understanding of bias and other ethical issues could have benefits beyond LLM applications, by providing new investigational approaches and insights into linguistic processing in the human brain 87 .</p>
<p>The value of engineered safeguards is only as good as their robustness in the face of adversarial attacks, as circumvention by nefarious actors may otherwise compromise efforts to mitigate risks.GPT-4 is more robust than its predecessors thanks to extensive directed training 14 .However, further work is required to tackle its remaining vulnerabilities 32,91 .Additional risk is conferred by the ability of external researchers to train their own models -perhaps without any safeguards -using data generated at scale by state-of-the-art LLMs through APIs 31 .GPT-4 keeps its internal workings confidential, to protect privacy but also to maintain a competitive advantage; API access may compromise both 14,31 .As the abilities of LLMs continue to expand, particular attention must be paid to guarding privacy, as models may be employed to identify patients from disparate information within training data and input queries 14 .Clinicians should also take care not to input identifiable data on platforms that may store and use the data for unspecified purposes.Governance structures should clearly state what is and is not permitted when developing and using these tools in medicine 115 .</p>
<p>Few experimental studies of LLM applications in medicine have been conducted, so there is a great demand for rigorous research to demonstrate and validate innovative use cases.Prospective clinical trials should be pragmatic, reflecting real-world clinical practice, and should test interventions that have a genuine chance of being implemented in terms of acceptance, effectiveness and practicality.For instance, AI assistance models (rather than autonomous models) should be evaluated relative to standard practice, as it is well established that unsupervised deployment of LLMs is unlikely to be feasible 18 .Appropriate endpoints are required to gauge success or failure, ideally reducing mortality and/or morbidity.Other innovative endpoints may include document quality (requiring validated quality assessment), work efficiency and patient or physician satisfaction.Some would contend that developing and using validated benchmarks to demonstrate genuine potential of clinical interventions would be a necessary precursor to large-scale clinical trials that may provide evidence justifying use of LLMs for clinical work.However, as non-LLM-based chatbots have been tested in randomized controlled trials before, and LLMs represent a meaningful advance in natural language processing, there may already be justification for clinical trials of LLM interventions 17,116 .Guidelines should be used where available to maximize the quality of research, and further work is required to adapt and develop frameworks suited for appraisal and conduction of studies involving natural language processing 117 .</p>
<p>In the context of clinical efficiency, studies are needed to ensure that LLM tools actually reduce workload rather than introducing an even greater administrative burden for healthcare professionals 16,118 .For example, electronic health records were hailed as a fantastic advance in digital health, but many physicians complain about resultant increases in menial data entry and administrative work 118 .Targeted studies may reduce the risk of LLMs causing similar problems.In addition, health economic analysis is required to establish that implementation of LLM applications is cost-effective rather than a wasteful 'white elephant' 119 .Researchers from different disciplines should, therefore, be encouraged to work together to improve the quality and rigor of published research 120 .</p>
<p>Conclusion</p>
<p>LLMs have revolutionized natural language processing, and state-of-the-art models, such as GPT-4 and PaLM 2, now occupy a central position at the forefront of AI innovation in medicine.Opportunities abound for this new technology across clinical, educational and research work, particularly with emerging multimodality and integration with plugin tools (Fig. 3).However, potential risks are causing considerable concern among experts and in wider society regarding safety, ethics and potential replacement of humans in certain contexts 41 .Autonomous deployment of LLM applications is not currently feasible, and clinicians will remain responsible for delivering optimal and humane care for their patients 14,16 .Validated applications may nevertheless serve as valuable tools to improve healthcare for patients and practitioners, provided ethical and technical issues are addressed.Successful validation will involve pragmatic clinical trials demonstrating real benefits with minimized bias and transparent reporting.</p>
<p>Fig. 1 |
1
Fig.1| LLMs developed in recent years.LLMs are ordered by date of publication, with the oldest models at the top.Many have been developed with parameters in the order of billions.However, size is clearly not the only measure of progress: many previous models feature more parameters than the models currently generating the greatest impact in healthcare.For instance, GPT-3 (from which GPT-3.5 was developed) features just 175 billion parameters, in comparison to multiple models featuring over 1 trillion parameters.The largest iteration of LLaMA (used in many open-source alternatives to ChatGPT) features just 65 billion parameters.Many other factors contribute to a model's utility, such as</p>
<p>Fig. 2 |
2
Fig. 2 | Fine-tuning an LLM (GPT-3.5) to develop an LLM chatbot (ChatGPT).GPT-3 -trained through word prediction tasks using a vast dataset of text sourced from the internet -was fine-tuned to develop GPT-3.5.Fine-tuning involves exposure of the model to prompt-output pairings generated by humans, allowing the model to learn how to respond appropriately to queries.To develop ChatGPT, RLHF was employed.RLHF employs a reward model trained using human grading of a limited number of GPT-3.5 outputs to a list of</p>
<p>Table 1 | Limitations of LLMs and how they may be overcome with future development
1LimitationsDescriptionMitigating strategiesRecencyGPT training datasets-Gathering training datado not include contentfrom more recent sources.created after September-Real-time internet access2021.(for example, Bing AI,All pretraining datasetsSparrow and BlenderBot 3).necessarily 'cut off' at anarbitrary date.AccuracyGPT-3 is limited to 570 GB-Validation of training data.of data.-Uncertainty indicators.Models are not trained-Fine-tuning to optimizeto 'understand'; instead,medical accuracy.they are limited to-Self-improvement throughlearning probabilisticintelligent prompts (forassociations betweenexample, chain-of-thought).words.Training data are sourcedfrom unverified andunvalidated websites andbooks.CoherenceModel outputs are based-Redeveloping modelon learned associationsarchitecture and trainingbetween words ratherstrategies to develop truethan understanding inputsemantic knowledge.queries or information-Fine-tuning to eliminateused in outputs.presentation of inaccurateFabricated facts areinformation.presented as if they weretrue.Transparency andIt is unclear how models-Requirement for outputsinterpretabilitygenerate answersto cite which parts of thefrom input queries anddataset contributed to thearchitectural data andmodel's answers.algorithms (known as-'Explainable' AI research'black box' issues).and development.It is unclear which partsof the training dataset areleveraged in generatedresponses.Ethical concernsResponses may-Fine-tuning to reduce thebe dangerous,incidence of undesirablediscriminatory oroutputs.offensive.-Establishment of-Risk of privacy andgovernance systems andsecurity breaches.overseeing authorities.-No established-Installation of a reportingaccountability forsystem for users to flagconsequences of modeldangerous responses.outputs.-Consensus-building-No consensus oninitiatives involving patientswhat roles AI shouldand practitioners.and should not play inmedicine.
AcknowledgementsD.S.W.T. is supported by the National Medical Research Council, Singapore (NMCR/HSRG/0087/2018, MOH-000655-00 and MOH-001014-00), the Duke-NUS Medical School (Duke-NUS/ RSF/2021/0018 and 05/FY2020/EX/15-A58) and the Agency for Science, Technology and Research (A20H4g2141 and H20C6a0032).These funders were not involved in the conception, execution or reporting of this review.Competing interestsD.S.W.T. holds a patent on a deep learning system for the detection of retinal diseases.The other authors declare no conflicts of interest.
A guide to deep learning in healthcare. A Esteva, Nat. Med. 252019</p>
<p>Diagnostic accuracy of deep learning in medical imaging: a systematic review and meta-analysis. R Aggarwal, NPJ Digit. Med. 4652021</p>
<p>. 10.1038/s41591-023-02448-8</p>
<p>Natural language processing. E Liddy, Encyclopedia of Library and Information Science. A Kent, H ) Lancour, Marcel Decker, 2001</p>
<p>Natural language processing: state of the art, current trends and challenges. D Khurana, A Koli, K Khatter, S Singh, Multimed. Tools Appl. 822023</p>
<p>Language models are few-shot learners. T Brown, Advances in Neural Information Processing Systems. Curran Associates202033</p>
<p>Foundation models for generalist medical artificial intelligence. M Moor, Nature. 6162023</p>
<p>Scaling laws for neural language models. J Kaplan, 10.48550/arXiv.2001.083612020Preprint at arXiv</p>
<p>Megatron-LM: training multi-billion parameter language models using model parallelism. M Shoeybi, 10.48550/arXiv.1909.080532020Preprint at arXiv</p>
<p>LaMDA: language models for dialog applications. R Thoppilan, 10.48550/arXiv.2201.082392022</p>
<p>GLM-130B: an open bilingual pre-trained model. A Zeng, 10.48550/arXiv.2210.024142022Preprint at arXiv</p>
<p>Transformer models: an introduction and catalog. X Amatriain, 10.48550/arXiv.2302.077302023Preprint at</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, 10.48550/arXiv.2203.021552022</p>
<p>10.48550/arXiv.2303.08774GPT-4 technical report. 2023OpenAIPreprint at arXiv</p>
<p>Performance of ChatGPT on USMLE: potential for AI-assisted medical education using large language models. T H Kung, PLoS Digit. Health. 2e00001982023</p>
<p>Trialling a large language model (ChatGPT) in general practice with the applied knowledge test: observational study demonstrating opportunities and limitations in primary care. A J Thirunavukarasu, JMIR Med. Educ. 9e465992023</p>
<p>Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public social media forum. J W Ayers, JAMA Intern. Med. 1832023</p>
<p>AI in health and medicine. P Rajpurkar, E Chen, O Banerjee, E J Topol, Nat. Med. 282022</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>paper/Language-Models-are-Unsupervised-Multitask-Learners. A Radford, -Radford-Wu/9405cc0d6169988371b2755e573 cc28650d14dfe. 2018Preprint at Semantic ScholarLanguage models are unsupervised multitask learners</p>
<p>Pre-trained models for natural language processing: a survey. X Qiu, Sci. China Technol. Sci. 632020</p>
<p>LLaMA: open and efficient foundation language models. H Touvron, 10.48550/arXiv.2302.139712023</p>
<p>Let's chat about ChatGPT. K Dennean, S Gantori, D K Limas, A Pu, R Gilligan, 2023</p>
<p>Why can GPT learn in-context? Language models secretly perform gradient descent as meta-optimizers. D Dai, 10.48550/arXiv.2212.105592022Preprint at arXiv</p>
<p>march_2023/Confirmed-the-new-Bing-runs-on-OpenAI's-GPT-4. 2023new Bing runs on OpenAI's GPT-4</p>
<p>Improving alignment of dialogue agents via targeted human judgements. A Glaese, 10.48550/arXiv.2209.143752022</p>
<p>BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage. K Shuster, 10.48550/arXiv.2208.031882022Preprint at arXiv</p>
<p>Language models that seek for knowledge: modular search &amp; generation for dialogue and prompt completion. K Shuster, 10.48550/arXiv.2203.132242022Preprint at arXiv</p>
<p>PaLM 2 technical report. R Anil, 10.48550/arXiv.2305.104032023</p>
<p>Alpaca: a strong, replicable instruction-following model. R Taori, 2023Preprint at</p>
<p>GPT-4 system card. 2023OpenAI</p>
<p>Quantifying the carbon emissions of machine learning. A Lacoste, A Luccioni, V Schmidt, T Dandres, 10.48550/arXiv.1910.097002019Preprint at arXiv</p>
<p>The carbon footprint of machine learning training will plateau, then shrink. D Patterson, 10.48550/arXiv.2204.051492022Preprint at arXiv</p>
<p>Energy and policy considerations for deep learning in NLP. E Strubell, A Ganesh, A Mccallum, 10.48550/arXiv.1906.022432019Preprint at arXiv</p>
<p>On the dangers of stochastic parrots: can language models be too big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, 10.1145/3442188.3445922Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency 610-623. the 2021 ACM Conference on Fairness, Accountability, and Transparency 610-623Association for Computing Machinery2021</p>
<p>ARK Investment Management LLC. Big Ideas. 2023. 2023</p>
<p>Capabilities of GPT-4 on medical challenge problems. H Nori, N King, S M Mckinney, D Carignan, E Horvitz, 10.48550/arXiv.2303.133752023Preprint at arXiv</p>
<p>Towards expert-level medical question answering with large language models. K Singhal, 10.48550/arXiv.2305.096172023Preprint at arXiv</p>
<p>Sixty seconds on… ChatGPT. M.-K Looi, BMJ. 3802052023</p>
<p>Pause giant AI experiments: an open letter. Future of Life Institute. 2023</p>
<p>Benefits, limits, and risks of GPT-4 as an AI chatbot for medicine. P Lee, S Bubeck, J Petro, N. Engl. J. Med. 3882023</p>
<p>Large language models encode clinical knowledge. K Singhal, 10.48550/arXiv.2212.131382022</p>
<p>How does ChatGPT perform on the United States Medical Licensing Examination? The implications of large language models for medical education and knowledge assessment. A Gilson, JMIR Med. Educ. 9e453122023</p>
<p>Appropriateness of cardiovascular disease prevention recommendations obtained from a popular online chat-based artificial intelligence model. A Sarraju, JAMA. 3292023</p>
<p>Does ChatGPT provide appropriate and equitable medical advice?: a vignette-based, clinical evaluation across care contexts. A J Nastasi, K R Courtright, S D Halpern, G E Weissman, 10.1101/2023.02.25.232864512023Preprint at medRxiv</p>
<p>. 10.1038/s41591-023-02448-8</p>
<p>Assessing the utility of ChatGPT throughout the entire clinical workflow. A Rao, 10.1101/2023.02.21.232858862023Preprint at medRxiv</p>
<p>The diagnostic and triage accuracy of the GPT-3 artificial intelligence model. D M Levine, 10.1101/2023.01.30.232850672023Preprint at medRxiv</p>
<p>Putting ChatGPT's medical advice to the (Turing) test. O Nov, N Singh, D M Mann, 10.1101/2023.01.23.232847352023Preprint at medRxiv</p>
<p>Large language models will not replace healthcare professionals: curbing popular fears and hype. A J Thirunavukarasu, J. R. Soc. Med. 1162023</p>
<p>Foresight-Generative Pretrained Transformer (GPT) for modelling of patient timelines using EHRs. Z Kraljevic, 10.48550/arXiv.2212.080722023Preprint at arXiv</p>
<p>Hybrid value-aware transformer architecture for joint learning from longitudinal and non-longitudinal clinical data. Y Shao, 10.1101/2023.03.09.232870462023Preprint at medRxiv</p>
<p>Leveraging GPT-4 for post hoc transformation of free-text radiology reports into structured reporting: a multilingual feasibility study. L C Adams, Radiology. 307e2307252023</p>
<p>The promise of large language models in health care. A Arora, A Arora, Lancet. 4016412023</p>
<p>Introducing Microsoft 365 Copilot-your copilot for work. The Official Microsoft Blog. J Spataro, 2023</p>
<p>Introducing PaLM 2. Z Ghahramani, 2023</p>
<p>ChatGPT: the future of discharge summaries?. S B Patel, K Lam, Lancet Digit. Health. 52023</p>
<p>Will ChatGPT transform healthcare?. Nat. Med. 292023</p>
<p>Our latest health AI research updates. 2023</p>
<p>Harnessing GPT-4 so that all students benefit. A nonprofit approach for equal access! Khan Academy Blog. S Khan, 2023</p>
<p>Introducing Duolingo Max, a learning experience powered by GPT-4. Duolingo Team, 2023</p>
<p>An explorative assessment of ChatGPT as an aid in medical education: use it with caution. Z Han, F Battaglia, A Udaiyar, A Fooks, S R Terlecky, 10.1101/2023.02.13.232858792023Preprint at medRxiv</p>
<p>ChatGPT for clinical vignette generation, revision, and evaluation. J R A Benoit, 10.1101/2023.02.04.232854782023Preprint at medRxiv</p>
<p>BioBERT: a pre-trained biomedical language representation model for biomedical text mining. J Lee, Bioinformatics. 362020</p>
<p>Domain-specific language model pretraining for biomedical natural language processing. Y Gu, ACM Trans. Comput. Health. 32022</p>
<p>Can ChatGPT-and its successors-go from cool to tool? Freedom to Tinker. M Salganik, 2023</p>
<p>Caution with AI-generated content in biomedicine. A Zhavoronkov, Nat. Med. 295322023</p>
<p>A large language model for electronic health records. X Yang, NPJ Digit. Med. 51942022</p>
<p>Large language models are few-shot clinical information extractors. M Agrawal, S Hegselmann, H Lang, Y Kim, D Sontag, 10.48550/arXiv.2205.126892022Preprint at arXiv</p>
<p>ClinicalBERT: modeling clinical notes and predicting hospital readmission. K Huang, J Altosaar, R Ranganath, 10.48550/arXiv.1904.053422020Preprint at arXiv</p>
<p>Large language models generate functional protein sequences across diverse families. A Madani, 10.1038/s41587-022-01618-2Nat. Biotechnol. 2023</p>
<p>TSSNote-CyaPromBERT: development of an integrated platform for highly accurate promoter prediction and visualization of Synechococcus sp. and Synechocystis sp. through a state-of-the-art natural language processing model BERT. D H A Mai, L T Nguyen, E Y Lee, Front. Genet. 1310675622022</p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, Nature. 5962021</p>
<p>A multifaceted benchmarking of synthetic electronic health record generation models. C Yan, Nat. Commun. 1376092022</p>
<p>Model index for researchers. Openai, </p>
<p>The lightning-fast quest for COVID vaccines-and what it means for other diseases. P Ball, Nature. 5892021</p>
<p>Anti-tumor efficacy of a potent and selective non-covalent KRASG12D inhibitor. J Hallin, Nat. Med. 282022</p>
<p>Passages from the Life of a Philosopher. C Babbage, 1864Longman, Green, Longman, Roberts, &amp; Green</p>
<p>Total data volume worldwide 2010-2025. Statista. </p>
<p>Will we run out of data? An analysis of the limits of scaling datasets in machine learning. P Villalobos, 10.48550/arXiv.2211.043252022Preprint at arXiv</p>
<p>Survey of hallucination in natural language generation. Z Ji, ACM Comput. Surv. 552023</p>
<p>Artificial hallucinations in ChatGPT: implications in scientific writing. H Alkaissi, S I Mcfarlane, Cureus. 15e351792023</p>
<p>Large language models can self-improve. J Huang, 10.48550/arXiv.2210.116102022Preprint at arXiv</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, 10.48550/arXiv.2203.111712023Preprint at arXiv</p>
<p>On the opportunities and risks of foundation models. R Bommasani, 10.48550/arXiv.2108.072582022Preprint</p>
<p>Hierarchical text-conditional image generation with CLIP latents. A Ramesh, P Dhariwal, A Nichol, C Chu, M Chen, 10.48550/arXiv.2204.061252022Preprint at arXiv</p>
<p>On the explainability of natural language processing deep models. J E Zini, M Awad, ACM Comput. Surv. 552022</p>
<p>Explainable Artificial Intelligence (XAI): concepts, taxonomies, opportunities and challenges toward responsible AI. A Barredo Arrieta, Inf. Fusion. 582020</p>
<p>Abstracts written by ChatGPT fool scientists. H Else, Nature. 6132023</p>
<p>Dan: users jailbreak AI program to get around ethical safeguards. J Taylor, The Guardian. 2023ChatGPT's alter ego</p>
<p>Ignore previous prompt: attack techniques for language models. F Perez, I Ribeiro, 10.48550/arXiv.2211.095272022Preprint at arXiv</p>
<p>An exploration on artificial intelligence application: from security, privacy and ethic perspective. X Li, T Zhang, 10.1109/ICCCBDA.2017.79519492017 IEEE 2nd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA) 416-420. Curran Associates2017</p>
<p>. 10.1038/s41591-023-02448-8</p>
<p>What is GDPR, the EU's new data protection law?. B Wolford, 2018</p>
<p>ChatGPT is fun, but not an author. H H Thorp, Science. 3793132023</p>
<p>NLP systems such as ChatGPT cannot be listed as an author because these cannot fulfill widely adopted authorship criteria. N S L Yeo-Teh, B L Tang, 10.1080/08989621.2023.2185776Account Res. 2023</p>
<p>ChatGPT listed as author on research papers: many scientists disapprove. C Stokel-Walker, Nature. 6132023</p>
<p>Do we still need clinical language models?. E Lehman, 10.48550/arXiv.2302.080912023Preprint at arXiv</p>
<p>GatorTron: a large clinical language model to unlock patient information from unstructured electronic health records. X Yang, 10.48550/arXiv.2203.035402022Preprint at arXiv</p>
<p>How accurate is the medical record? A comparison of the physician's note with a concealed audio recording in unannounced standardized patient encounters. S J Weiner, S Wang, B Kelly, G Sharma, A Schwartz, J. Am. Med. Inf. Assoc. 272020</p>
<p>Why most published research findings are false. J P A Ioannidis, PLoS Med. 2e1242005</p>
<p>Generating scholarly content with ChatGPT: ethical challenges for medical publishing. M Liebrenz, R Schleifer, A Buadze, D Bhugra, A Smith, Lancet Digit. Health. 52023</p>
<p>ChatGPT writes smart essays-should academics worry. C Stokel-Walker, Bot, 10.1038/d41586-022-04397-7Nature. 2022</p>
<p>AI-generated research paper fabrication and plagiarism in the scientific community. F R Elali, L N Rachid, Patterns. 41007062023</p>
<p>Tools such as ChatGPT threaten transparent science; here are our ground rules for their use. Nature. 6132023</p>
<p>Science journals ban listing of ChatGPT as co-author on papers. I Sample, The Guardian. 2023</p>
<p>Nonhuman 'authors' and implications for the integrity of scientific publication and medical knowledge. A Flanagin, K Bibbins-Domingo, M Berkwits, S L Christiansen, JAMA. 3292023</p>
<p>. Contributorship Cambridge Authorship, Core, </p>
<p>New AI classifier for indicating AI-written text. </p>
<p>A watermark for large language models. J Kirchenbauer, 2023</p>
<p>The Lancet Digital Health. Lancet Digit. Health. 5e1022023ChatGPT: friend or foe?</p>
<p>ChatGPT passing USMLE shines a spotlight on the flaws of medical education. A B Mbakwe, I Lourentzou, L A Celi, O J Mechanic, A Dagan, PLoS Digit. Health. 2e00002052023</p>
<p>Large language models associate Muslims with violence. A Abid, M Farooqi, J Zou, Nat. Mach. Intell. 32021</p>
<p>CrowS-Pairs: a challenge dataset for measuring social biases in masked language models. N Nangia, C Vania, R Bhalerao, S R Bowman, 10.18653/v1/2020.emnlp-main.154Proc. of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) 1953-1967. of the 2020 Conference on Empirical Methods in Natural Language essing (EMNLP) 1953-1967Association for Computational Linguistics2020</p>
<p>Data statements for natural language processing: toward mitigating system bias and enabling better science. E M Bender, B Friedman, In Transactions of the Association for Computational Linguistics. 62018</p>
<p>Ethics of large language models in medicine and medical research. H Li, Lancet Digit. Health. 52023</p>
<p>Artificial intelligence-based chatbots for promoting health behavioral changes: systematic review. A Aggarwal, C C Tam, D Wu, X Li, S Qiao, J. Med. Internet Res. 25e407892023</p>
<p>Reporting guideline for the early-stage clinical evaluation of decision support systems driven by artificial intelligence: DECIDE-AI. B Vasey, Nat. Med. 282022</p>
<p>Factors affecting physician professional satisfaction and their implications for patient care, health systems, and health policy. M W Friedberg, RAND Health Q. 312014</p>
<p>Digital health in medicine: important considerations in evaluating health economic analysis. A Kwee, Z L Teo, D S W Ting, Lancet Reg. Health West Pac. 231004762022</p>
<p>Validity of machine learning in biology and medicine increased through collaborations across fields of expertise. M Littmann, Nat. Mach. Intell. 22020</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-487 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-487</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-487</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-202577933</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1909.06674v1.pdf" target="_blank">A Step Toward Quantifying Independently Reproducible Machine Learning Research</a></p>
                <p><strong>Paper Abstract:</strong> What makes a paper independently reproducible? Debates on reproducibility center around intuition or assumptions but lack empirical results. Our field focuses on releasing code, which is important, but is not sufficient for determining reproducibility. We take the first step toward a quantifiable answer by manually attempting to implement 255 papers published from 1984 until 2017, recording features of each paper, and performing statistical analysis of the results. For each paper, we did not look at the authors code, if released, in order to prevent bias toward discrepancies between code and paper.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e487.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e487.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ambiguous_notation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ambiguous notation or language in method descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Natural language descriptions and mathematical notation in papers are sometimes unclear or ambiguous, leading implementers to misinterpret algorithms or miss necessary details when writing code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>independent reproduction workflow (manual reimplementation of published ML papers)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reproducers manually implemented algorithms described in 255 ML papers without consulting authors' released code, recording features and outcomes to study reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section (notation and prose)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>independently written implementation (reproducer's code)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous description / unclear notation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Mathematical notation or prose in the methods section is underspecified or ambiguous so that implementers cannot unambiguously determine the intended algorithmic operations, data flows, or parameter roles; this forces re-derivation or guesswork that can change behavior of the implemented code.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>algorithm specification / mathematical derivations</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual reimplementation attempts: failures or mismatches during reproduction encountered and noted by reproducers; follow-up email correspondence with authors when needed</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>qualitative failure during implementation; correlated statistically with poor Readability and non-reproduction (Readability feature significant at p = 9.68×10^-25); reproduction success counted using a 75%+ claims threshold</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can cause failed reproduction or large deviations in results; listed as a primary subjective reason for failure. Overall, 93/255 papers were not reproduced (36.5%), with ambiguous language among cited causes.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Reported qualitatively as a common subjective cause of non-reproduction in the authors' retrospective list; exact numeric prevalence not recorded.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Imprecise natural language and notation, constrained page limits, and authors omitting low-level details assumed to be 'obvious'.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Increase clarity in writing (better notation, explicit stepwise descriptions), include code-like pseudo-code or actual code, larger page limits or appendices for full derivations, and author responsiveness to queries.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Indirect evidence: papers rated as 'Excellent' readability were always reproducible; Readability strongly predictive (p = 9.68×10^-25). No direct intervention experiment reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Toward Quantifying Independently Reproducible Machine Learning Research', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e487.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e487.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>missing_steps</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Missing algorithmic steps or omitted implementation details</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Papers sometimes omit specific algorithm steps or implementation nuances in prose, preventing faithful reimplementation in code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>independent reproduction workflow (manual reimplementation of published ML papers)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reproducers attempted independent implementations and recorded whether missing steps prevented reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / algorithm description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>independently written implementation (reproducer's code)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing algorithm step or details</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Critical procedural steps (e.g., update rules, normalization, pre/post-processing steps) are omitted from the text, leaving implementers to infer or guess the missing pieces; such omissions can change algorithm dynamics and reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure, update rules, preprocessing/postprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual reimplementation where attempts stalled or produced different results; reproducers' subjective recall and direct email queries to authors to clarify missing steps</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>treated qualitatively in this study; reproduction defined quantitatively (75% of claims), so missing steps contributed to papers classified as not reproduced (93/255). No separate numeric tally for 'missing step' failures was recorded.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Often led to failed reproductions or large performance differences; authors state they rarely suspected fraud but frequently found missing or ambiguous steps preventing replication.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Listed among the principal subjective reasons for non-reproduction; exact frequency not recorded in dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Authors omitting 'minor' details assumed trivial, page limits, or implicit assumptions not stated.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Require explicit step-by-step algorithm descriptions, provide code-like pseudocode or reference implementations, and include full hyperparameter and preprocessing details in appendices or repositories.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Papers with detailed 'Code-Like' pseudo-code were more reproducible (pseudo-code feature significant p = 2.31×10^-4). No direct causal test performed.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Toward Quantifying Independently Reproducible Machine Learning Research', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e487.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e487.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>missing_gradients</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Missing gradient derivations and derivative details</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Some papers present loss functions without explicitly giving gradients or derivations required to implement gradient-based training, forcing reproducers to re-derive potentially non-trivial expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>independent reproduction workflow (manual reimplementation of published ML papers)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Implementers re-derived gradients when omitted and noted mismatches with reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / mathematical specification of loss and updates</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training code (optimizer implementations, gradient computations)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification (missing gradient/derivative details)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Losses or update equations are given but authors do not provide resulting gradients or explicit update rules; re-derivation can be non-trivial and mistakes/change choices (e.g., approximation, numerical stabilization) alter outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure / gradient computation / optimizer step</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual reimplementation difficulty and mismatch between rederived gradients' resulting behavior and paper results; sometimes clarified via author correspondence</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>qualitative identification; contributes to classification of non-reproducible papers under the study's quantitative reproduction thresholds (75% claims), but no separate numeric metric for this gap was collected.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can materially change training dynamics and final performance; cited as a common reason for failed replication attempts in the authors' subjective recall.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Reported as 'many papers' had this issue in subjective recall, but no exact count recorded.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Authors omit derivative details assuming readers can re-derive them, or to save space; implicit assumptions about numerical tricks or approximations are not documented.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Include explicit gradient expressions or supplementary derivations, provide code snippets for critical gradient computations, and describe any numerical approximations or stabilization tricks.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not directly quantified; suggested by the general correlation between more detailed algorithmic descriptions/pseudo-code and higher reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Toward Quantifying Independently Reproducible Machine Learning Research', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e487.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e487.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>missing_hyperparameters</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Missing or partially-specified hyperparameters</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Papers often omit exact hyperparameter values or selection procedures, which substantially hinders faithful reproduction of reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>independent reproduction workflow (manual reimplementation of published ML papers)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reproducers attempted to match reported experimental results and recorded whether hyperparameters were specified.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / experimental protocol</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experiment scripts / training configurations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>hyperparameter mismatch / incomplete specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Authors either do not report final hyperparameter values, only partially report them, or report only the search procedure without ranges; absent hyperparameters force implementers to tune manually, often failing to reach reported performance.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure / hyperparameters (learning rates, regularization, architecture-specific settings)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>failed or divergent reimplementations compared to reported results; authors recorded whether hyperparameters were specified and used statistical tests to assess correlation with reproduction</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Hyper-parameters Specified feature was recorded categorically (No / Partial / Yes) and tested with chi-squared (p = 8.45×10^-6) showing significant correlation; reproduction itself measured by 75%+ claims criterion and ranking rules.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Strongly positive predictor when hyperparameters are specified: papers specifying hyperparameters were more likely to be reproducible; absence/partial info correlated with non-reproduction. Quantitatively, hyperparameter-specification feature significance p = 8.45×10^-6.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Feature recorded across 255 papers; exact counts per category are in Table 17 of the paper (statistical significance reported), but the paper does not give a simple percent summary of 'missing hyperparameters'.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Authors omit final chosen values for concision, assume defaults, or only describe search procedures without enumerating chosen settings.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Report final hyperparameter values per dataset/experiment, or provide full configuration files; document the selection procedure and search ranges explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Statistical evidence suggests effectiveness: hyperparameter specification is significantly correlated with successful independent reproduction (p = 8.45×10^-6).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Toward Quantifying Independently Reproducible Machine Learning Research', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e487.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e487.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pseudocode_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pseudo-code style and detail mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different styles/levels of pseudo-code affect implementability: 'Code-Like' pseudo-code helps reproduction, while terse 'Step-Code' can harm readability and reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>independent reproduction workflow (manual reimplementation of published ML papers)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reproducers classified pseudo-code into four tiers and analyzed correlations with reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / pseudo-code blocks</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>algorithm implementation code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / insufficient pseudo-code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Pseudo-code that is high-level ('Step-Code') often omits crucial details and forces readers to cross-reference other parts of the paper repeatedly; by contrast, 'Code-Like' pseudo-code or explicit stepwise code-like descriptions enable easier reimplementation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>algorithm specification / implementation details</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual classification of pseudo-code style per paper and statistical analysis of correlation with reproduction outcome (chi-squared p = 2.31×10^-4); also observed during coding attempts</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Categorical pseudo-code feature (No / Step-Code / Yes / Code-Like) and chi-squared test for association with reproduction; supplementary cross-tabulation with Readability showed Step-Code biased toward lower Readability.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Pseudo-code detail level significantly associated with reproduction; 'Code-Like' pseudo-code linked to higher reproducibility, while 'Step-Code' associated with lower reproducibility. p = 2.31×10^-4 for pseudo-code feature overall.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Pseudo-code feature observed and categorized for all 255 papers; distribution details are in Table 12/13. Exact counts not restated in text summary.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Authors provide high-level algorithm outlines rather than detailed, unambiguous descriptions; possibly due to page limits or stylistic choices.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Provide detailed, code-like pseudo-code or real reference implementations; ensure pseudo-code includes all control flow, parameter defaults, and data-processing steps.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Empirical association: more detailed pseudo-code correlates with higher reproduction rates (statistically significant). No controlled trial reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Toward Quantifying Independently Reproducible Machine Learning Research', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e487.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e487.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>compute_env_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Compute environment and cluster configuration mismatches</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Papers that require cluster or distributed resources often omit crucial environment and configuration details, making reproduction difficult even with access to similar hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>independent reproduction workflow (manual reimplementation of published ML papers)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reproducers recorded 'Compute Needed' levels (Desktop, GPU, Server, Cluster) and assessed reproduction success by resource class.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / compute specification</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training and distributed execution code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete environment specification / different deployment variant</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers often fail to describe cluster details (interconnects, job scheduling, distributed code organization) or other environment specifics; these omissions impede reproducing experiments that require distributed hardware or complex setups.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>compute environment / deployment / distributed training</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>observed pattern in manual reproductions: papers requiring Cluster resources were never successfully reproduced by authors despite access; statistical test showed Compute Needed significant (p = 8.75×10^-5).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Compute Needed categorical feature and chi-squared test for association with reproduction (significant). Empirical observation: reproducers succeeded more often on GPU papers and failed on Cluster papers.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Zero successful reproductions observed for Cluster-required papers in this study (authors report they 'have never successfully reproduced a paper that needed such resources'), while GPU-needed papers had higher reproduction rates. This materially reduced reproducibility for cluster-dependent works.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Compute Needed was recorded for all 255 papers; cluster-required papers were a subset (counts in Table 18), but exact percent not restated in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Authors omit environmental/configuration details; distributed systems are more complex and variable; implicit assumptions about cluster setup are not portable.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Document cluster configuration, job scripts, and distributed code organization; provide containerized environments or reproducible deployment artifacts; include example job submission scripts and data-parallel code patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>No intervention tested in this study; authors hypothesize containerization and better tooling could help. Observationally, frameworks like PyTorch (for GPU) increased reproduction rates for GPU papers.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / distributed training</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Toward Quantifying Independently Reproducible Machine Learning Research', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e487.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e487.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>authors_response_effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Author correspondence and clarifying replies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct replies from paper authors to implementers' questions strongly correlate with successful independent reproduction, indicating that clarifying ambiguous descriptions helps bridge gaps to working code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>independent reproduction workflow (manual reimplementation of published ML papers, plus author contact)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reproducers contacted authors and recorded whether a reply was received and whether reproduction succeeded.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>follow-up clarification via email (natural language Q&A)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>independently written implementation informed by author responses</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing detail / ambiguous description clarified by author communication</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>When implementers encountered missing or ambiguous details in the paper, author replies often clarified specifics, enabling successful reimplementation; lack of reply left implementers stuck.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>any stage where details were ambiguous (hyperparameters, procedural steps, environment)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical: reproducers contacted authors for 50 papers and recorded replies and reproduction outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Contingency counts: of 50 contacted papers, reply rate 52%; in the 26 cases with replies, reproducers succeeded 22 times; of the 24 non-replies, reproduction succeeded once. Statistical significance p = 6.01×10^-8.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Huge effect size observed: reply correlated with ~84.6% reproduction (22/26) vs ~4.2% (1/24) when no reply. This suggests author communication can largely remediate many gaps between description and implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Authors replied to 52% of contacted papers (26/50) in the sampled subset; effect reported for that subset.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Papers leave implicit assumptions or lack detail that authors can clarify post-publication.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Encourage living documents, active author support channels, and hosting clarifications in arXiv updates or project pages; require contact information and encourage timely responses.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>High observational effectiveness: author replies strongly associated with successful reproduction (p = 6.01×10^-8); causality can't be proven but effect is large.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Toward Quantifying Independently Reproducible Machine Learning Research', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e487.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e487.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>code_availability_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between authors releasing code and independent reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Availability of authors' code did not correlate with higher rates of independent reproduction in this study, indicating that code release alone may not resolve discrepancies between paper descriptions and implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>independent reproduction workflow (dataset of 255 papers with notes on code availability)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reproducers noted whether authors released code and analyzed association with independent reproduction outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper artifact metadata (code availability statements or linked repositories)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>authors' released code (not used by reproducers in this study)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>mismatch between expected effect of code availability and actual independent reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Contrary to expectation, the mere presence of authors' released code had no significant relationship with whether an independent team could reproduce results without consulting that code; potential reasons include authors relying on code to convey details and omitting them from the paper, or variations in how usable the released code is.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>paper-to-implementation information pathway / documentation practices</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>dataset annotation and statistical test (chi-squared); feature 'Code Available' tested and found non-significant (p = 0.213)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Chi-squared test for association between 'Code Available' and reproduction; no significant association found.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Suggests that code release alone does not guarantee independent reproducibility (as defined by reproducing without consulting authors' code), and it may not substitute for clear natural language descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Code availability was recorded for all papers; exact proportion of papers with released code not restated in summary, but feature was not predictive.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Authors may omit details in the paper if code exists, released code may be incomplete or hard to run, or code may not map cleanly to the high-level description.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Release well-documented, runnable code with pinned dependencies and usage examples; nevertheless also include full methodological details in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>No direct effect demonstrated in this study; the lack of correlation suggests documentation and quality of released code matter more than mere availability.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Toward Quantifying Independently Reproducible Machine Learning Research', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e487.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e487.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>equation_density_issue</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>High equation density causing reduced readability and reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Papers with many equations per page were associated with lower readability and lower reproducibility, suggesting that dense mathematical presentation can obscure implementation-relevant details.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>independent reproduction workflow (analysis of 255 papers' features and outcomes)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reproducers counted equations per page and tested association with Readability and reproduction success.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper mathematical exposition</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>algorithm implementation guided by mathematical descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>overly dense / presentation-driven ambiguity</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>High counts of equations per page correlate with lower readability; excessive or dense mathematical presentation may make it harder to extract concrete implementation steps and hyperparameter choices needed to write code.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>algorithm specification / derivations and exposition</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>feature counting (Equations normalized per page) and statistical testing (Mann-Whitney/Kruskal-Wallis), showing Number of Equations significant (p = 0.004); readability differences significant (p = 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Normalized equations per page; Kruskal-Wallis ANOVA and Dunn post-hoc showed 'Excellent' readability papers had fewer equations per page (2.25 eq/pg) than others (p ≤ 0.002).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Negative correlation with reproducibility: papers with fewer equations per page tended to be more reproducible; this suggests presentation choices impact ease of turning description into code.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Equations counted across all 255 papers; statistical significance indicates a systematic effect though exact proportions per band are in the appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Authors favor compact mathematical derivations over explicit implementation-oriented exposition, reducing accessibility for implementers.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Balance math with clear algorithmic descriptions, include implementation notes or worked examples, and move long derivations to appendices while keeping implementation-relevant steps explicit in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Observational: better readability (fewer eq/pg) strongly associated with reproducibility; no controlled intervention tested.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Toward Quantifying Independently Reproducible Machine Learning Research', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>State of the Art: Reproducibility in Artificial Intelligence <em>(Rating: 2)</em></li>
                <li>Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets <em>(Rating: 2)</em></li>
                <li>A Practical Taxonomy of Reproducibility for Machine Learning Research <em>(Rating: 2)</em></li>
                <li>Winner's Curse? On Pace, Progress, and Empirical Rigor <em>(Rating: 1)</em></li>
                <li>Replicability is not reproducibility: nor is it good science <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-487",
    "paper_id": "paper-202577933",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "ambiguous_notation",
            "name_full": "Ambiguous notation or language in method descriptions",
            "brief_description": "Natural language descriptions and mathematical notation in papers are sometimes unclear or ambiguous, leading implementers to misinterpret algorithms or miss necessary details when writing code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "independent reproduction workflow (manual reimplementation of published ML papers)",
            "system_description": "Reproducers manually implemented algorithms described in 255 ML papers without consulting authors' released code, recording features and outcomes to study reproducibility.",
            "nl_description_type": "research paper methods section (notation and prose)",
            "code_implementation_type": "independently written implementation (reproducer's code)",
            "gap_type": "ambiguous description / unclear notation",
            "gap_description": "Mathematical notation or prose in the methods section is underspecified or ambiguous so that implementers cannot unambiguously determine the intended algorithmic operations, data flows, or parameter roles; this forces re-derivation or guesswork that can change behavior of the implemented code.",
            "gap_location": "algorithm specification / mathematical derivations",
            "detection_method": "manual reimplementation attempts: failures or mismatches during reproduction encountered and noted by reproducers; follow-up email correspondence with authors when needed",
            "measurement_method": "qualitative failure during implementation; correlated statistically with poor Readability and non-reproduction (Readability feature significant at p = 9.68×10^-25); reproduction success counted using a 75%+ claims threshold",
            "impact_on_results": "Can cause failed reproduction or large deviations in results; listed as a primary subjective reason for failure. Overall, 93/255 papers were not reproduced (36.5%), with ambiguous language among cited causes.",
            "frequency_or_prevalence": "Reported qualitatively as a common subjective cause of non-reproduction in the authors' retrospective list; exact numeric prevalence not recorded.",
            "root_cause": "Imprecise natural language and notation, constrained page limits, and authors omitting low-level details assumed to be 'obvious'.",
            "mitigation_approach": "Increase clarity in writing (better notation, explicit stepwise descriptions), include code-like pseudo-code or actual code, larger page limits or appendices for full derivations, and author responsiveness to queries.",
            "mitigation_effectiveness": "Indirect evidence: papers rated as 'Excellent' readability were always reproducible; Readability strongly predictive (p = 9.68×10^-25). No direct intervention experiment reported.",
            "domain_or_field": "machine learning",
            "reproducibility_impact": true,
            "uuid": "e487.0",
            "source_info": {
                "paper_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "missing_steps",
            "name_full": "Missing algorithmic steps or omitted implementation details",
            "brief_description": "Papers sometimes omit specific algorithm steps or implementation nuances in prose, preventing faithful reimplementation in code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "independent reproduction workflow (manual reimplementation of published ML papers)",
            "system_description": "Reproducers attempted independent implementations and recorded whether missing steps prevented reproduction.",
            "nl_description_type": "research paper methods section / algorithm description",
            "code_implementation_type": "independently written implementation (reproducer's code)",
            "gap_type": "missing algorithm step or details",
            "gap_description": "Critical procedural steps (e.g., update rules, normalization, pre/post-processing steps) are omitted from the text, leaving implementers to infer or guess the missing pieces; such omissions can change algorithm dynamics and reported results.",
            "gap_location": "training procedure, update rules, preprocessing/postprocessing",
            "detection_method": "manual reimplementation where attempts stalled or produced different results; reproducers' subjective recall and direct email queries to authors to clarify missing steps",
            "measurement_method": "treated qualitatively in this study; reproduction defined quantitatively (75% of claims), so missing steps contributed to papers classified as not reproduced (93/255). No separate numeric tally for 'missing step' failures was recorded.",
            "impact_on_results": "Often led to failed reproductions or large performance differences; authors state they rarely suspected fraud but frequently found missing or ambiguous steps preventing replication.",
            "frequency_or_prevalence": "Listed among the principal subjective reasons for non-reproduction; exact frequency not recorded in dataset.",
            "root_cause": "Authors omitting 'minor' details assumed trivial, page limits, or implicit assumptions not stated.",
            "mitigation_approach": "Require explicit step-by-step algorithm descriptions, provide code-like pseudocode or reference implementations, and include full hyperparameter and preprocessing details in appendices or repositories.",
            "mitigation_effectiveness": "Papers with detailed 'Code-Like' pseudo-code were more reproducible (pseudo-code feature significant p = 2.31×10^-4). No direct causal test performed.",
            "domain_or_field": "machine learning",
            "reproducibility_impact": true,
            "uuid": "e487.1",
            "source_info": {
                "paper_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "missing_gradients",
            "name_full": "Missing gradient derivations and derivative details",
            "brief_description": "Some papers present loss functions without explicitly giving gradients or derivations required to implement gradient-based training, forcing reproducers to re-derive potentially non-trivial expressions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "independent reproduction workflow (manual reimplementation of published ML papers)",
            "system_description": "Implementers re-derived gradients when omitted and noted mismatches with reported results.",
            "nl_description_type": "research paper methods section / mathematical specification of loss and updates",
            "code_implementation_type": "training code (optimizer implementations, gradient computations)",
            "gap_type": "incomplete specification (missing gradient/derivative details)",
            "gap_description": "Losses or update equations are given but authors do not provide resulting gradients or explicit update rules; re-derivation can be non-trivial and mistakes/change choices (e.g., approximation, numerical stabilization) alter outcomes.",
            "gap_location": "training procedure / gradient computation / optimizer step",
            "detection_method": "manual reimplementation difficulty and mismatch between rederived gradients' resulting behavior and paper results; sometimes clarified via author correspondence",
            "measurement_method": "qualitative identification; contributes to classification of non-reproducible papers under the study's quantitative reproduction thresholds (75% claims), but no separate numeric metric for this gap was collected.",
            "impact_on_results": "Can materially change training dynamics and final performance; cited as a common reason for failed replication attempts in the authors' subjective recall.",
            "frequency_or_prevalence": "Reported as 'many papers' had this issue in subjective recall, but no exact count recorded.",
            "root_cause": "Authors omit derivative details assuming readers can re-derive them, or to save space; implicit assumptions about numerical tricks or approximations are not documented.",
            "mitigation_approach": "Include explicit gradient expressions or supplementary derivations, provide code snippets for critical gradient computations, and describe any numerical approximations or stabilization tricks.",
            "mitigation_effectiveness": "Not directly quantified; suggested by the general correlation between more detailed algorithmic descriptions/pseudo-code and higher reproducibility.",
            "domain_or_field": "machine learning",
            "reproducibility_impact": true,
            "uuid": "e487.2",
            "source_info": {
                "paper_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "missing_hyperparameters",
            "name_full": "Missing or partially-specified hyperparameters",
            "brief_description": "Papers often omit exact hyperparameter values or selection procedures, which substantially hinders faithful reproduction of reported results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "independent reproduction workflow (manual reimplementation of published ML papers)",
            "system_description": "Reproducers attempted to match reported experimental results and recorded whether hyperparameters were specified.",
            "nl_description_type": "research paper methods section / experimental protocol",
            "code_implementation_type": "experiment scripts / training configurations",
            "gap_type": "hyperparameter mismatch / incomplete specification",
            "gap_description": "Authors either do not report final hyperparameter values, only partially report them, or report only the search procedure without ranges; absent hyperparameters force implementers to tune manually, often failing to reach reported performance.",
            "gap_location": "training procedure / hyperparameters (learning rates, regularization, architecture-specific settings)",
            "detection_method": "failed or divergent reimplementations compared to reported results; authors recorded whether hyperparameters were specified and used statistical tests to assess correlation with reproduction",
            "measurement_method": "Hyper-parameters Specified feature was recorded categorically (No / Partial / Yes) and tested with chi-squared (p = 8.45×10^-6) showing significant correlation; reproduction itself measured by 75%+ claims criterion and ranking rules.",
            "impact_on_results": "Strongly positive predictor when hyperparameters are specified: papers specifying hyperparameters were more likely to be reproducible; absence/partial info correlated with non-reproduction. Quantitatively, hyperparameter-specification feature significance p = 8.45×10^-6.",
            "frequency_or_prevalence": "Feature recorded across 255 papers; exact counts per category are in Table 17 of the paper (statistical significance reported), but the paper does not give a simple percent summary of 'missing hyperparameters'.",
            "root_cause": "Authors omit final chosen values for concision, assume defaults, or only describe search procedures without enumerating chosen settings.",
            "mitigation_approach": "Report final hyperparameter values per dataset/experiment, or provide full configuration files; document the selection procedure and search ranges explicitly.",
            "mitigation_effectiveness": "Statistical evidence suggests effectiveness: hyperparameter specification is significantly correlated with successful independent reproduction (p = 8.45×10^-6).",
            "domain_or_field": "machine learning",
            "reproducibility_impact": true,
            "uuid": "e487.3",
            "source_info": {
                "paper_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "pseudocode_mismatch",
            "name_full": "Pseudo-code style and detail mismatch",
            "brief_description": "Different styles/levels of pseudo-code affect implementability: 'Code-Like' pseudo-code helps reproduction, while terse 'Step-Code' can harm readability and reproduction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "independent reproduction workflow (manual reimplementation of published ML papers)",
            "system_description": "Reproducers classified pseudo-code into four tiers and analyzed correlations with reproducibility.",
            "nl_description_type": "research paper methods section / pseudo-code blocks",
            "code_implementation_type": "algorithm implementation code",
            "gap_type": "incomplete specification / insufficient pseudo-code",
            "gap_description": "Pseudo-code that is high-level ('Step-Code') often omits crucial details and forces readers to cross-reference other parts of the paper repeatedly; by contrast, 'Code-Like' pseudo-code or explicit stepwise code-like descriptions enable easier reimplementation.",
            "gap_location": "algorithm specification / implementation details",
            "detection_method": "manual classification of pseudo-code style per paper and statistical analysis of correlation with reproduction outcome (chi-squared p = 2.31×10^-4); also observed during coding attempts",
            "measurement_method": "Categorical pseudo-code feature (No / Step-Code / Yes / Code-Like) and chi-squared test for association with reproduction; supplementary cross-tabulation with Readability showed Step-Code biased toward lower Readability.",
            "impact_on_results": "Pseudo-code detail level significantly associated with reproduction; 'Code-Like' pseudo-code linked to higher reproducibility, while 'Step-Code' associated with lower reproducibility. p = 2.31×10^-4 for pseudo-code feature overall.",
            "frequency_or_prevalence": "Pseudo-code feature observed and categorized for all 255 papers; distribution details are in Table 12/13. Exact counts not restated in text summary.",
            "root_cause": "Authors provide high-level algorithm outlines rather than detailed, unambiguous descriptions; possibly due to page limits or stylistic choices.",
            "mitigation_approach": "Provide detailed, code-like pseudo-code or real reference implementations; ensure pseudo-code includes all control flow, parameter defaults, and data-processing steps.",
            "mitigation_effectiveness": "Empirical association: more detailed pseudo-code correlates with higher reproduction rates (statistically significant). No controlled trial reported.",
            "domain_or_field": "machine learning",
            "reproducibility_impact": true,
            "uuid": "e487.4",
            "source_info": {
                "paper_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "compute_env_mismatch",
            "name_full": "Compute environment and cluster configuration mismatches",
            "brief_description": "Papers that require cluster or distributed resources often omit crucial environment and configuration details, making reproduction difficult even with access to similar hardware.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "independent reproduction workflow (manual reimplementation of published ML papers)",
            "system_description": "Reproducers recorded 'Compute Needed' levels (Desktop, GPU, Server, Cluster) and assessed reproduction success by resource class.",
            "nl_description_type": "research paper methods section / compute specification",
            "code_implementation_type": "training and distributed execution code",
            "gap_type": "incomplete environment specification / different deployment variant",
            "gap_description": "Papers often fail to describe cluster details (interconnects, job scheduling, distributed code organization) or other environment specifics; these omissions impede reproducing experiments that require distributed hardware or complex setups.",
            "gap_location": "compute environment / deployment / distributed training",
            "detection_method": "observed pattern in manual reproductions: papers requiring Cluster resources were never successfully reproduced by authors despite access; statistical test showed Compute Needed significant (p = 8.75×10^-5).",
            "measurement_method": "Compute Needed categorical feature and chi-squared test for association with reproduction (significant). Empirical observation: reproducers succeeded more often on GPU papers and failed on Cluster papers.",
            "impact_on_results": "Zero successful reproductions observed for Cluster-required papers in this study (authors report they 'have never successfully reproduced a paper that needed such resources'), while GPU-needed papers had higher reproduction rates. This materially reduced reproducibility for cluster-dependent works.",
            "frequency_or_prevalence": "Compute Needed was recorded for all 255 papers; cluster-required papers were a subset (counts in Table 18), but exact percent not restated in main text.",
            "root_cause": "Authors omit environmental/configuration details; distributed systems are more complex and variable; implicit assumptions about cluster setup are not portable.",
            "mitigation_approach": "Document cluster configuration, job scripts, and distributed code organization; provide containerized environments or reproducible deployment artifacts; include example job submission scripts and data-parallel code patterns.",
            "mitigation_effectiveness": "No intervention tested in this study; authors hypothesize containerization and better tooling could help. Observationally, frameworks like PyTorch (for GPU) increased reproduction rates for GPU papers.",
            "domain_or_field": "machine learning / distributed training",
            "reproducibility_impact": true,
            "uuid": "e487.5",
            "source_info": {
                "paper_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "authors_response_effect",
            "name_full": "Author correspondence and clarifying replies",
            "brief_description": "Direct replies from paper authors to implementers' questions strongly correlate with successful independent reproduction, indicating that clarifying ambiguous descriptions helps bridge gaps to working code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "independent reproduction workflow (manual reimplementation of published ML papers, plus author contact)",
            "system_description": "Reproducers contacted authors and recorded whether a reply was received and whether reproduction succeeded.",
            "nl_description_type": "follow-up clarification via email (natural language Q&A)",
            "code_implementation_type": "independently written implementation informed by author responses",
            "gap_type": "missing detail / ambiguous description clarified by author communication",
            "gap_description": "When implementers encountered missing or ambiguous details in the paper, author replies often clarified specifics, enabling successful reimplementation; lack of reply left implementers stuck.",
            "gap_location": "any stage where details were ambiguous (hyperparameters, procedural steps, environment)",
            "detection_method": "empirical: reproducers contacted authors for 50 papers and recorded replies and reproduction outcomes",
            "measurement_method": "Contingency counts: of 50 contacted papers, reply rate 52%; in the 26 cases with replies, reproducers succeeded 22 times; of the 24 non-replies, reproduction succeeded once. Statistical significance p = 6.01×10^-8.",
            "impact_on_results": "Huge effect size observed: reply correlated with ~84.6% reproduction (22/26) vs ~4.2% (1/24) when no reply. This suggests author communication can largely remediate many gaps between description and implementation.",
            "frequency_or_prevalence": "Authors replied to 52% of contacted papers (26/50) in the sampled subset; effect reported for that subset.",
            "root_cause": "Papers leave implicit assumptions or lack detail that authors can clarify post-publication.",
            "mitigation_approach": "Encourage living documents, active author support channels, and hosting clarifications in arXiv updates or project pages; require contact information and encourage timely responses.",
            "mitigation_effectiveness": "High observational effectiveness: author replies strongly associated with successful reproduction (p = 6.01×10^-8); causality can't be proven but effect is large.",
            "domain_or_field": "machine learning",
            "reproducibility_impact": true,
            "uuid": "e487.6",
            "source_info": {
                "paper_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "code_availability_mismatch",
            "name_full": "Mismatch between authors releasing code and independent reproducibility",
            "brief_description": "Availability of authors' code did not correlate with higher rates of independent reproduction in this study, indicating that code release alone may not resolve discrepancies between paper descriptions and implementations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "independent reproduction workflow (dataset of 255 papers with notes on code availability)",
            "system_description": "Reproducers noted whether authors released code and analyzed association with independent reproduction outcomes.",
            "nl_description_type": "paper artifact metadata (code availability statements or linked repositories)",
            "code_implementation_type": "authors' released code (not used by reproducers in this study)",
            "gap_type": "mismatch between expected effect of code availability and actual independent reproducibility",
            "gap_description": "Contrary to expectation, the mere presence of authors' released code had no significant relationship with whether an independent team could reproduce results without consulting that code; potential reasons include authors relying on code to convey details and omitting them from the paper, or variations in how usable the released code is.",
            "gap_location": "paper-to-implementation information pathway / documentation practices",
            "detection_method": "dataset annotation and statistical test (chi-squared); feature 'Code Available' tested and found non-significant (p = 0.213)",
            "measurement_method": "Chi-squared test for association between 'Code Available' and reproduction; no significant association found.",
            "impact_on_results": "Suggests that code release alone does not guarantee independent reproducibility (as defined by reproducing without consulting authors' code), and it may not substitute for clear natural language descriptions.",
            "frequency_or_prevalence": "Code availability was recorded for all papers; exact proportion of papers with released code not restated in summary, but feature was not predictive.",
            "root_cause": "Authors may omit details in the paper if code exists, released code may be incomplete or hard to run, or code may not map cleanly to the high-level description.",
            "mitigation_approach": "Release well-documented, runnable code with pinned dependencies and usage examples; nevertheless also include full methodological details in the paper.",
            "mitigation_effectiveness": "No direct effect demonstrated in this study; the lack of correlation suggests documentation and quality of released code matter more than mere availability.",
            "domain_or_field": "machine learning",
            "reproducibility_impact": true,
            "uuid": "e487.7",
            "source_info": {
                "paper_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "equation_density_issue",
            "name_full": "High equation density causing reduced readability and reproducibility",
            "brief_description": "Papers with many equations per page were associated with lower readability and lower reproducibility, suggesting that dense mathematical presentation can obscure implementation-relevant details.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "independent reproduction workflow (analysis of 255 papers' features and outcomes)",
            "system_description": "Reproducers counted equations per page and tested association with Readability and reproduction success.",
            "nl_description_type": "research paper mathematical exposition",
            "code_implementation_type": "algorithm implementation guided by mathematical descriptions",
            "gap_type": "overly dense / presentation-driven ambiguity",
            "gap_description": "High counts of equations per page correlate with lower readability; excessive or dense mathematical presentation may make it harder to extract concrete implementation steps and hyperparameter choices needed to write code.",
            "gap_location": "algorithm specification / derivations and exposition",
            "detection_method": "feature counting (Equations normalized per page) and statistical testing (Mann-Whitney/Kruskal-Wallis), showing Number of Equations significant (p = 0.004); readability differences significant (p = 0.001).",
            "measurement_method": "Normalized equations per page; Kruskal-Wallis ANOVA and Dunn post-hoc showed 'Excellent' readability papers had fewer equations per page (2.25 eq/pg) than others (p ≤ 0.002).",
            "impact_on_results": "Negative correlation with reproducibility: papers with fewer equations per page tended to be more reproducible; this suggests presentation choices impact ease of turning description into code.",
            "frequency_or_prevalence": "Equations counted across all 255 papers; statistical significance indicates a systematic effect though exact proportions per band are in the appendix.",
            "root_cause": "Authors favor compact mathematical derivations over explicit implementation-oriented exposition, reducing accessibility for implementers.",
            "mitigation_approach": "Balance math with clear algorithmic descriptions, include implementation notes or worked examples, and move long derivations to appendices while keeping implementation-relevant steps explicit in main text.",
            "mitigation_effectiveness": "Observational: better readability (fewer eq/pg) strongly associated with reproducibility; no controlled intervention tested.",
            "domain_or_field": "machine learning",
            "reproducibility_impact": true,
            "uuid": "e487.8",
            "source_info": {
                "paper_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "State of the Art: Reproducibility in Artificial Intelligence",
            "rating": 2,
            "sanitized_title": "state_of_the_art_reproducibility_in_artificial_intelligence"
        },
        {
            "paper_title": "Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets",
            "rating": 2,
            "sanitized_title": "replicability_analysis_for_natural_language_processing_testing_significance_with_multiple_datasets"
        },
        {
            "paper_title": "A Practical Taxonomy of Reproducibility for Machine Learning Research",
            "rating": 2,
            "sanitized_title": "a_practical_taxonomy_of_reproducibility_for_machine_learning_research"
        },
        {
            "paper_title": "Winner's Curse? On Pace, Progress, and Empirical Rigor",
            "rating": 1,
            "sanitized_title": "winners_curse_on_pace_progress_and_empirical_rigor"
        },
        {
            "paper_title": "Replicability is not reproducibility: nor is it good science",
            "rating": 1,
            "sanitized_title": "replicability_is_not_reproducibility_nor_is_it_good_science"
        }
    ],
    "cost": 0.01657075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Step Toward Quantifying Independently Reproducible Machine Learning Research
14 Sep 2019</p>
<p>Edward Raff Booz 
University of Maryland
Baltimore County</p>
<p>Allen Hamilton 
University of Maryland
Baltimore County</p>
<p>A Step Toward Quantifying Independently Reproducible Machine Learning Research
14 Sep 2019
What makes a paper independently reproducible? Debates on reproducibility center around intuition or assumptions but lack empirical results. Our field focuses on releasing code, which is important, but is not sufficient for determining reproducibility. We take the first step toward a quantifiable answer by manually attempting to implement 255 papers published from 1984 until 2017, recording features of each paper, and performing statistical analysis of the results. For each paper, we did not look at the authors code, if released, in order to prevent bias toward discrepancies between code and paper.</p>
<p>Introduction</p>
<p>As the fields of Artificial Intelligence (AI) and Machine Learning (ML) have grown in recent years, so too have calls that we are currently in an AI/ML reproducibility crisis [1]. Conferences, such as NeurIPS, have added reproducibility as a factor in the reviewing cycle or implemented policies to encourage code sharing. Many are pursing work centered around code and data availability as one of the more direct methods of enhancing reproducibility. For example, Dror et al. [2] developed a proposal to standardize the description and release of datasets. Others have proposed taxonomies and ontologies over reproducibility based on the availability of algorithm description, code, and data [3,4]. Others have focused on building frameworks for sharing code and automation of hyper parameter selection in order to enable easier reconstruction of results [5].</p>
<p>While the ability to replicate the results of papers through open sourced code and data is valuable and should be lauded, it has been argued that releasing code is insufficient [6]. The inability to reproduce results without code availability may suggest problems with the paper. This may be due to the following: insufficient explanation of the approach, failure to describe important minute details, or a discrepancy between the code and description. We will call the act of reproducing the results of a paper without use of code from the paper's authors, independent reproducibility. We argue that for a paper to be scientifically sound and complete, it should be independently reproducible.</p>
<p>The question we wish to answer in this work is what makes a paper independently reproducible? Many have argued fiercely for different aspects of writing and publishing as critical factors of reproducability. Quantifiable study of these efforts is needed to advance the conversation. Otherwise, we as a community will not have scientific understanding that our work is addressing aspects of reproducibility. Gundersen and Kjensmo [7] defined several paper-properties of interest in regard to reproducibility. However, they defined a paper as reproducible purely as a function of the features without knowing if the selected features (e.g., method is described, data is available) actually impact a paper's reproducibility.</p>
<p>As a first step toward answering this question, we performed a study of 255 papers that we have attempted to implement independently. We developed the first empirical quantification about indepen-dent reproducibility by recording features from each paper and reproduction outcome. We will review the entire procedure and features obtained in section 2. In section 3 we will discuss which features were determined to be statistically significant, and we will discuss the implication of these results. We will discuss the deficiencies of our study in section 4, with subjective analysis in section 5, and then conclude in section 6.</p>
<p>Procedure and Features</p>
<p>For clarity, we will refer to ourselves, the author of this paper, as the reproducers, distinct from the authors of the papers we attempt to independently reproduce. To perform our analysis, we obtained features from 255 papers. Inclusion criteria included papers that proposed at least one new algorithm/method that is the subject of reproduction, and papers where the first implementation and reproduction attempts occurred between January 1st 2012 through December 31st 2017. We chose varied paper topics based on our historical interest. No papers were included from 2018 to present, as some papers take more time to reproduce than others, which could negatively skew results for papers from the past year. If the available source code for a paper under consideration was seen before having successfully reproduced the paper, we excluded the paper from this analysis because at that point we are not a fully independent party. In line with this, any paper was excluded if the paper's authors had any significant relationship with the reproducers (e.g., academic advisor, coworker, close friends, etc.) because intimate knowledge of communication style, work preferences, or the ability to have more regular communication could bias results. A paper was considered to be reproduced if the code for results were written by the reproducers, allowing the use of reasonable and standard libraries (e.g., BLAS, PyTorch, etc.), and the code reproduced the majority of claims from the paper.</p>
<p>Specifically, we regarded a paper as reproducible if the majority (75%+) of the claims in the paper could be confirmed with code we independently wrote. If a claimed improvement was measured in orders-of-magnitude, being within the same order-of-magnitude was considered sufficient (e.g., a paper claims 700x faster, but reproducers observe 300x). This same order-of-magnitude criterion comes from an observation that such claims are highly dependent upon constant factor efficiency improvements that may be had/missing from both the prior methods, and the proposed method being replicated. Presence or absence of these improvements can cause, apparently, "dramatic" impacts without fundamentally changing the nature of the contribution we are attempting to reproduce. When compared to other algorithms, we consider a paper reproduced if the considerable majority (90%+) of the new algorithm's rankings correspond to those found in the paper (e.g., the claim is that the proposed method was most accurate on 95% of tasks compared to 4 other models, we want to see our reproduction be most accurate on at least 95% · 90% = 81% of the same tasks, compared to the same models). As a last resort, we considered getting within 10% of the numbers reported in the paper (or better), or in the case of non-quantitative results (e.g., GAN sample quality), we subjectively compare our results with the paper to make a decision. We include this flexibility in specification to allow for small differences that can occur. While not common, we did encounter more than one instance where our independent reproduction achieved better results than the original paper.</p>
<p>After this selection process, we are left with 255 papers, of which 162 (63.5%) were successfully replicated and 93 were not. We note that this is significantly better than the 26% reproducibility determined by [7], who defined reproducibility as a function of the features they believed would determine reproduction. Below we will describe each of the features used. We attempt to catalog both features that are believed relevant to a paper's reproduction and features that should not be relevant, which will help us quantify if these expectations hold. We will use statistical tests to determine which of these features have a significant relationship with reproduction. An anonymized version of the data can be found at https://github.com/EdwardRaff/ Quantifying-Independently-Reproducible-ML.</p>
<p>Quantified Features</p>
<p>We have manually recorded 26 attributes from each paper, which took approximately 20 minutes per paper to complete 1 . A policy for each feature was developed to minimize as much subjectivity as possible. Below we will review each feature, and how they were recorded, in order from least to most subjective. Each feature was obtained based on the body of the main paper only, excluding any appendices (unless specified otherwise).</p>
<p>Features to consider were selected based on two factors: 1) would one reasonably believe the feature should be correlated with the ability to reproduce a paper (positive or negative) and 2) was the feature reasonably available with little additional work? This was done to capture as much useful information as possible while also avoiding limiting our study to items where a priori one might believe that a feature's relevance (or lack thereof) to be "obvious."</p>
<p>Unambiguous Features: Some features are not ambiguous in nature. A few are simple and innate properties that require no explanation. This included the Number of Authors, the existence of an appendix (or supplementary material), the number of pages (including references, excluding any appendix), the number of references, the year the paper was published, the year first attempted to implement, the venue type (Book, Journal, Conference, Workshop, Tech-Report), as well as the specific publication venue (e.g., NeurIPS, ICML). Many papers follow a progression from Tech-Report to Workshop to Conference to Journal as the paper becomes more complete. For any paper that participated in parts of this progression, we use the version from the most "complete" venue under the assumption that it would be the most reproducible version of the paper allowing us to avoid issues with double-counting papers.</p>
<p>We also include whether or not the Author Replied to questions about their paper. If any author replied to any email, it was counted as a "Yes". If no author ever replied, we marked it as "No." In all cases, every paper author was sent an email before marking it as "No." If a current email could not be found, we marked that the authors were not contacted.</p>
<p>Mild Subjectivity: We spend more time expounding on the next set of features, which had minor degrees of subjectivity. We state below the developed procedure we used to make their quantification practical and reproducible.</p>
<p>• Number of Tables: The total number of tables in the paper, regardless of the content of those tables. While tables usually contain results, they often contain a wide variety of content, and we make no distinction between them due to their frequency and variety. • Number of Graphs/Plots: The total number of plots/graphs contained in the paper which includes scatter plots, bar-charts, contour-plots, or any other kind of 2D-3D numeric data visualization. • Number of Equations: Due to differing writing styles, we do not use equation number provided by the paper, nor do we count everything that might be typed between LaTeX "$$" brackets. We manually reviewed every line of every paper to arrive as a consistent counting process 2 . Inline mathematics were only counted if the the math involved 1) two or more variables interacting (e.g., x · y) or 2) two or more "operations" (e.g, P (x|y) or O(x 2 )). If only one "operation" occurred (e.g, P (x) or x 2 ), it was not considered. Inline equations were counted only once per line of text, regardless of how many equations occurred in a line of text. Whole-line equations were always counted, regardless of the simplicity of the equation. If multiple whole lines were used because of equation length (e.g., a "+" ), it was counted as one equation. If multiple whole lines were used due to showing a mathematical step or derivation, each step counted as an additional equation. Partial deference was given to equation numbers. If every line of an equation received its own number, they were counted accordingly. If a derivation over n whole lines received only one equation number, the equation was counted ⌈n/3⌉ times. • Number of Proofs: A proof was only counted if it was done in a formal manner, beginning with the statement of a corollary or theorem, and included at least an overview of how to achieve the proof. A proof was counted if it occurred in the appendix or supplementary material. Derivations of update rules or other equations did not count as a proof unless the paper stated them as a proof. This was done as a practical matter in reducing ambiguity and the process of collecting the information. • Exact Compute Specified: If a paper indicated any of the specific compute resources used (e.g., CPU GHz speed or model number, GPU model, number of computers used), we considered it to have satisfied this requirement. • Hyper-parameters Specified: If a paper specified the final hyper-parameter values selected for each dataset or the method of selecting hyper-parameters (e.g., cross validation factor) and the value range (e.g., λ ∈ [1, 1000]), we consider it to have satisfied this requirement. Simply stating that a grid-search (or similar procedure) was used was not sufficient. If a paper introduced multiple hyper-parameters but only specified how a sub-set of the parameters where chosen, we marked it as "Partial". • Compute Needed: We defined the compute level needed to reproduce a paper's results as needing either a Desktop (i.e., ≤ $2000), a consumer GPU (e.g., an Nvida Geforce type card), a Server (used 20 cores or more, or 64 GB of RAM or more), or a Cluster. If the compute resources needed were not explicitly stated, this was subjectively based on the computational complexity of the approach and amount of experiments believed necessary to reach reproduction. We stress that this compute level was selected based on today's common compute resources, not those available at the time of the paper's publication. • Data Available: If any of the datasets used in the paper are publicly available, we note it as having satisfied this requirement. • Pseudo Code: We allow for four different options for this feature: 1) no pseudo code is given in the paper, 2) "</p>
<p>Step-Code" is given, where the paper outlines the algorithm/method as a sequence of steps, but the steps are terse and high-level or refer to other parts of the paper for details, 3) "Yes", the paper has some pseudo code which outlines the algorithm at a high level but with sufficient detail that it feels mostly complete, and 4) "Code-Like", the paper summarizes the approach in great detail that is reminiscent of reading code (or is in fact code ).</p>
<p>Subjective:</p>
<p>We have a final set of features which we recognize are of a significantly subjective nature. For all of these features, we are aware there may be significant issues, and in practice, any alternative protocol would impose its own different set of issues. We have made the choices in an attempt to minimize as many issues as possible and make the survey possible. Below is the protocol we followed to reduce ambiguity and make our procedure as reproducible as possible for future studies, which will help the reader fully understand our interpetation of the results.</p>
<p>• Number of Conceptualization Figures: Many papers include graphics or content for which the purpose is not to convey a result, but to try to convey the idea / method proposed itself. These are usually included to make it easier to understand the algorithm, and so we identify them as a separate item to count. • Uses Exemplar Toy Problem: As a binary "Yes"/"No" option, did the paper include an exemplar toy problem? These problems are not meaningful toward any application of the algorithm, but they are devised to show specific behaviors or create demonstrations that are easier to reproduce / help conceptualize the algorithm being presented. These are often 2D or 3D problems, or they are synthetically generated from some specified set of distributions.  Table, or Conceptualization Figure as defined above. For most papers, this included samples of the output produced by an algorithm or example input images for Computer Vision applications. • Rigor vs Empirical: There have been a number of calls for more scientific rigor within the ML community [8], with many arguing that an overly empirical focus may in fact slow down progress [9]. We are not aware of any agreed upon taxonomy of what makes a paper "rigorous". Based on the interpretation that rigor equates to having grounded understanding of why and how our methods work, beyond simply showing that they do so empirically, we develop the following protocol: a paper is classified as "Theory" (read, rigorous), if it has formal proofs, provides mathematical reasoning or explanation to modeling decisions, or provides mathematical reasoning or explanation to why prior methods fail on some dataset. By default, we classify all other papers as "Empirical." However, if a "Theory" paper also includes discussion of practical implementation or deployment concerns, complete discussion of hyper-parameter setting such that there is no ambiguity, ablation studies of decisions made, or experiments on production datasets, we consider the paper "Balanced" as having both theory and empirical components. • Paper Readability: We give each paper a readability score of "Low", "Ok", "Good", or "Excellent." To minimize subjectivity in these scores, we tie each to the amount of times we had to read the paper in order to reach a point where we felt we had the proposed algorithm implemented in its entirety, and the failure to replicate would be a matter of finding and removing bugs. The score of "Excellent" means that we needed to read the paper only once to produce an implementation, "Good" papers needed two or three readings, "Ok" papers needed four or five, with "Low" being six or more reads through the paper 3 . • Algorithm Difficulty: We categorize the difficulty of implementing an algorithm as either "Low", "Medium", or "High." We grounded this to lines of code for any paper successfully implemented or which made its implementation available online. For ones never successfully implemented and without code, we estimated this based on our intuition and experience on where the implementation would have landed based on reading the paper. "Low" difficulties could be completed in 500 lines of code or less, "Medium" difficulty between 500 and 1,500 lines, and "High" was &gt; 1,500 lines. In these numbers we assume using common libraries (e.g., auto-differentiation, BLAS, etc.). • Primary Topic: For each paper we tried to specify a single primary topic of the paper. Many papers cover different aspects of multiple problems, making this a challenge. We adjusted topics into higher-level categories so that each topic had at least three members, so that we could do meaningful statistics. Topics can be found in the appendix. • Looks Intimidating: The most subjective, does the paper "look intimidating" at first glance?  [11] confirmed that none of our features would have been appropriate for use with a Student's t-test and so the non-parametric testing is preferred. For all categorical features, we used a Chi-Squared test [12] with continuity correction [13]. In our analysis we will also examine relationships between some of our categorical features and other numeric features for suspected relationships. We will continue to use non-parametric tests for robustness/conservative estimates of significance, relying on the Kruskal-Walls [14] for ANOVA testing and the Dunn test [15] for post-hoc analysis. JSAP was used to compute all statistical tests [16]. In Table 1 we show the results for deciding which of our 26 features were correlated with a paper's reproducibility.</p>
<p>Results</p>
<p>Tables and graphs of all the features are too numerous to fit in the main paper, and will be found in the appendix.</p>
<p>We begin by noting that the year a paper was published or the year that we first tried to implement the paper were not correlated with successful reproduction. The concerns of a reproducibility crisis would generally imply that the issue is a recent one. However, the year a paper was published is not correlated with successful reproduction, with the oldest paper being from 1984. This would suggest that independent reproducibility has not changed over time. Depending on one's perspective, we could argue that there is not a reproducibility crisis, or that one has been ongoing for several decades. It is important the reader qualify this statistical result with the fact that the year of paper publication in our study is not evenly distributed over time, with the majority of papers occurring in between 2000 through 2017.</p>
<p>To our study's benefit, the year first attempted for reproduction was not significant. If our success was correlated with time (as one might expect in advance-with skill increasing with experience), we would worry about this skewing our results. This appears to not be an issue, removing a potential problem from our results.</p>
<p>Significant Relationships</p>
<p>There were ten variables that are significantly correlated with a paper's reproducibility. Of them, Number of Tables, Equations, Compute Needed, Pseudo-Code, and Hyper-parameters Specified are the least subjective variables which were significant.</p>
<p>Readability had the strongest empirical relationship, which on its face is not surprising. Note that by our definition, Readability corresponds to how many reads through the paper were necessary to get to a mostly complete implementation. As expected, the fewer attempts to read through a paper, the more likely it was to be reproduced. For "Excellent" papers, we were always able to reproduce results. Exact counts can be found in Table 11. Based on these results we argue the importance of clear and effective communication of implementation details, which may often be neglected. This neglect may come from forced page limits or a preference towards other, competing factors (e.g., preferring figures/results that better show the method's value, at the cost of method details). We suspect that a factor in this is page limits which we test by proxy via paper page length. A Krusal-Wallis test confirms the significance of paper length in pages (p = 0.035). A Dunn post-hoc test shows that "Low" Readability papers are the statistically significant source of this relationship, which are 3.17-5.67 pages shorter than the other Readability types. As a field that has historically focused on open-access and online availability, and with the decreasing relevance of paper conference and journal distributions, our study suggests that raising page limits on papers, and adding technical algorithmic details as an explicit review factor, could aid in increasing the reproducibility of papers.  Table 12, we see that Pseudo-Code has a complicated relationship with Reproduction. Highly Detailed "Code-Like" descriptions are more reproducible, but having "No" pseudo-code is also positively related with reproduction. Based on these results papers which can effectively describe their algorithms without pseudo-code are communicating the information in another way, but papers with "Step-Code" do an inadequate job at this task. Examining the relationship between Pseudo-Code and Readability in Table 2 supports this, where we see that using</p>
<p>Step-Code is biased toward lower readability. This also makes sense in abstract, as step-code often requires one to repeatedly reference different parts of a paper. The relationship between an Algorithm's difficulty is more direct and intuitive, Table 10 showing that reproducibility decreases with difficulty.</p>
<p>It is also interesting to note how Rigor vs Empirical is correlated with reproducibility. One may have expected papers that focus on proving their methods correct would be the most reproducible.</p>
<p>In Table 8 we can see that papers that are "Empirical" or "Balanced" both have higher than expected reproduction rates, while "Theory" oriented papers have lower than expected. These results would seem to suggest that empiricism is intrinsically valuable for reproduction on the micro scale of individual papers. This does not contradict any of the concerns about long-term behaviors and results that are side effects of overly-empirical issues discussed by Sculley et al. [9], such as new methods being inappropriately considered due to ineffectively tuned baselines and lack of ablation studies. We take this result as a further indication that rigor cannot just be math or learning bounds for their own sake, but that the practical relevance and execution of any theorems must be at the forefront in all papers 4 .</p>
<p>Unfortunately, the primary topic of a paper was found to be a significant factor for independent reproducibility. We were not able to reproduce any Bayesian or Fairness based papers. We had a higher-than expected success in implementing papers about Deep Learning and Search/Retrieval.</p>
<p>We, the reproducers, are not experts in all of the primary topic areas listed, and so we advise against extrapolation from this particular result. This leads to interesting questions regarding reproduction from inside/outside an expert peer group and when one qualifies as an expert in a general topic area. We hope to explore these questions further in future work.</p>
<p>Both Number of Tables and Hyper-parameters were positively correlated with reproducibility. The more tables included in a paper, or the more parameters specified, the more likely the paper was to be reproducible. This is not a surprising result for the Hyper-parameters case, and supports the emphasis the community has placed on this factor [5]. It is somewhat peculiar that Tables are significant, but Graphs/Plots are not as both convey primarily numeric information to the reader. We suspect that the ability for the reader to quickly understand the exact value/result from a table is the differentiating factor as it gives a target to meet and measure against. While a plot/graph may describe overall behavior, it may not readily avail itself to quickly extracting a hard number and using it as a goal.</p>
<p>The Number of Equations per page was negatively correlated with reproduction. Two theories as to why were developed based on our experience implementing the papers: 1) having a larger number of equations makes the paper more difficult to read, hence more difficult to reproduce or 2) papers with more equations correspond to more complex and difficult algorithms, naturally being more difficult to reproduce. A Kruskal-Wallis ANOVA reveals that the readability hypothesis is significant (p = 0.001) but not the difficulty hypothesis (p = 0.239). Following with a Dunn post-hoc test shows that papers which have "Excellent" readability have fewer equations per page (2.25 eq/pg) than the others, as the source of the significant (p ≤ 0.002) relationship. There are no significant differences between papers of "Low," "Ok," and "Good" readability (3.91, 3.60, 3.78 equations per page respectively), leading us to postulate that the most readable and reproducible papers make careful and judicious use of equations.</p>
<p>Our last paper-intrinsic property is Compute Needed, which could be a "Desktop", "GPU", "Server", or "Cluster". In the time that these papers were implemented, we have had access to all four compute levels to varying degrees. Looking at Table 18, we see the use of a Cluster or GPU are the ones that depart from expectations. Despite having access to cluster resources, we have never successfully reproduced a paper that needed such resources. At the same time, we have a higher reproduction rate for works that require a GPU. Our suspicion is that frameworks such as PyTorch and Tensorflow, which make use of GPUs relatively easy, have been converging toward an effective paradigm for using that kind of resource. These libraries make it easier to reproduce current papers and historical ones that lacked such advanced tools, which then inflates reproduction rate. While frameworks like Spark exist for distributed computation, they may not be sufficiently developed for Machine Learning use cases to ease replication. Another alternative hypothesis for Cluster reproduction failure is that the details of how a cluster is organized, with interconnects, job scheduling, and more sophisticated code, are increasing the reproduction barrier and lack necessary details. We do not have sufficient information to confirm or reject these hypotheses, but we encourage others to consider them as avenues for study.</p>
<p>This leaves us with the last significant result, which is not a property of the paper itself: whether the paper authors reply to questions about their paper. We reached out to the authors of 50 different papers and had a reply rate of 52%. Table 7 which shows that replying was the most individually predictive attribute studied. In the 24 cases where the author did not respond to questions, we succeeded in replication only once. For the 26 cases where they did reply, we succeeded 22 times. While this result demonstrates the importance of corresponding with readers, it gives credence to the idea of a non-stationary and "living" paper where updates may be made over time to address questions and concerns. Such is possible today with arxiv.org and distill.pub, and provides quantifiable evidence that their ability to update articles is a meaningful and powerful tool toward reproducibility (if leveraged). Other confounding hypotheses exist as well, such as receiving a reply increasing the motivation of the reproducers, or the nature of a discussion that is not constrained to a paper's limitations may also impact reproduction rates.</p>
<p>Interesting Non-Significant/Negative Results</p>
<p>While we have already discussed some non-significant results as they relate directly to significant ones above, we also want to highlight interesting non-significant results. In particular, we expected a priori that the use of Conceptualization Figures and Exemplar Problems would be significant pre-dictors, as we have found them useful in our personal experiences both to understand the algorithm, and as an initial test-bed to confirm an algorithm was working to a minimal degree. Yet neither are significant. We also find that neither have a relationship with a paper's Readability (p ≥ 0.476).</p>
<p>These results give us pause regarding our assumptions about what makes a "good" reproducible paper, and reinforce the importance of quantifying these important questions.</p>
<p>A positive indicator is that Venue (e.g., NeurIPS vs PKDD) had no significant impact, nor did Venue type (e.g., Workshop vs Journal). This result would seem to imply that the same issues and successes are occurring across most academic levels, though selection bias may play a role in this result.</p>
<p>The non-significance of including an appendix is of note given our results that the papers which are hardest to reproduce ("Low" Readability) are shorter on average. There is no significant difference between a paper's readability and the presence of an appendix (p = 0.650), which implies that appendices are not sufficient means of circumventing page limits at conference/workshop venues.</p>
<p>We found it interesting that whether or not the papers' authors released their code has no significant relationship with the paper's independent reproducibility. Before analysis, we could see hypotheticals that would cause correlations in either direction. Authors who release code might include less details in the paper under the assumption that readers will find them in the code itself. Conversely, one might imagine that authors who release code care more about reproduction and would include more of the necessary details. With more conferences encouraging code availability as a reviewer criteria, we would not necessarily expect any change in independent reproducibility from this change in isolation (impacts on cultural changes induced being a question beyond our scope).</p>
<p>Study Deficiencies</p>
<p>While we have taken the first step toward studying and quantifying factors of reproducibility, we must also acknowledge deficiencies in our study. Most apparent are a number of potential biases.</p>
<p>The papers under consideration have a selection bias based on interest and filtering from consideration any paper where we had previously looked at released source code. More importantly, all papers were attempted by just this paper's author. So while we have a large sample size of papers, we have a low sample size of implementers. It is entirely possible that those with a different background in education, training, career, and interests, would find different papers easy or difficult to reproduce.</p>
<p>Because we are the sole reproducer, all the results must be taken with consideration conditioned on our background, and the origin of this work. A majority of attempted reproductions where in pursuit of contribution to a machine learning library that we are the author of, JSAT [17]. As such, we focused initially on a number of more common and widely used algorithm. These methods had already been independently reproduced by others many times, and alternative materials (e.g., lecture notes) were available to provide guidance without consulting code written by others. Further papers where spurred by our personal interest in what we considered useful for such a library, and our own personal interests (historical interests including nearest neighbor algorithms, linear models, and kernel methods). Such well known works do not make the majority of reproduction attempts, but they make up a sizable sub-population of the methods we attempted for JSAT, and so may skew results.</p>
<p>Our study is also limited by our own historical records. The use of paper cataloging software to take notes and record information made this study possible, but it also limits our study to the recorded notes and what can be re-derived from the paper itself (e.g., number of pages).</p>
<p>Towards improving upon the number of implementers and recording information, we hope to encourage extensions to projects such as the ICLR Reproducibility Challenge. A communal effort to standardize on an initial set of paper features, keep track of time and resources spent on reproduction, and information about the reproducers (years of experience, education, and background) may allow for a richer and more thorough macro study of reproducibility in the future. A design constraint we would like to include in such a system is differential privacy so that it is not known which individual papers are having reproduction difficulties. We have intentionally avoided identifying papers to avoid any perceived "naming and shaming", as our or other attempts in isolation should not be seen as conclusive statements on any individual paper's lack of reproduction.</p>
<p>In our experience attempting to reproduce these papers, we also note a failure in the framing of the problem: that a paper is reproducible or not. Depending on the paper, differing levels of resources and even teams may be necessary for reproduction. As a point of reference, the longest effort toward reproduction we studied took 4.5 years of (non-continuous) effort to finally reproduce the results.</p>
<p>In this light, it may be better to model reproduction as a kind of survival analysis conditioned on properties of the implementer(s). A paper "survives" as the implementers attempt reproduction and "dies" once successfully reproduced (or "lives" forever if never reproduced). Viewed in this light, we may ask: what environmental factors (e.g., libraries like PyTorch, Scikit-Learn, compute resources) impact survival rates and times, and should the necessity of code release be a function of survival time? A real life example of this is playing out now, as people attempt to reproduce OpenAI's recent GPT-2 results 5 , where information and data was intentionally withheld due to security concerns.</p>
<p>An important factor not included in our analysis are the authors of a paper, which has a direct impact on writing style, topic, and other factors. Subjectively we note that there are authors whose work we regularly fail to reproduce and ones we regularly succeed in reproducing, even when both make code available. Study of how the backgrounds and styles of both authors and implementers interact and impact reproduction seems to be a valuable line of inquiry, but it is beyond our current scope and requires additional thought and consideration.</p>
<p>We also note that the most significant factors in reproducibility are the most subjective factors. While we endeavored to reduce the impact of subjectivity as much as possible with our stated protocols, this indicates that more work is warranted in developing more objective measures that are related to these subjective factors, or using communal effort to reach a distributional determination on these subjective factors, for future studies.</p>
<p>A Subjective Recall of Non-Reproduction</p>
<p>We did not record the believed reason for failure to reproduce, although this would have been valuable information. We hope that this will be noted by others in the future, but for now we recount a subjective summary of the primary reasons we felt a paper could not be reproduced. We note that part of our belief in the below list stems from our efforts to email papers' authors when attempting to independently reproduce their works, in which we are often seeking information that would elucidate the below issues:</p>
<ol>
<li>
<p>Unclear notation or language. A component of the algorithm is explained, but not in a way easily understood by the reproducers, or was ambiguously specified.</p>
</li>
<li>
<p>Missing algorithm step or details, a step was completely left out of description.</p>
</li>
<li>
<p>Many papers would specify loss functions or other equations for which the gradient needed to be taken, but not detail the resulting gradients. Depending on the functions and math involved re-deriving was non-trivial, and our results did not match.</p>
</li>
<li>
<p>Missing hyper-parameters, or similar nuance details. The reproducers believe we have an implementation accurate to what was described, but some "minor" detail was not specified and makes a big difference in results.</p>
</li>
</ol>
<p>We avoided in this paper any attempt to imply or cast doubt on the veracity of any individual paper. In our experiences through this work, we have rarely had suspicion that the results of a paper were false or the result of serious flawed implementation, and thus could never be reproduced.</p>
<p>Conclusions</p>
<p>In this work we have conducted the first empirical study of what impacts a paper's reproducibility.</p>
<p>We suspect this will lead to considerable debate about the meaning of results, and we hope to spur further quantifiable studies. Based on our results, we find that paper reproduction rates have not changed (in a statistically significant way) over the past 35 years. Papers of a more empirical nature tend to be more reproducible, as are ones that include factors relevant to implementation details -though simply including Pseudo-Code is not sufficient. Our study indicates papers with fewer equations and more tables tend to be more reproducible, and that there is a potential latent issue in reproduction when cluster computing becomes a requirement.</p>
<p>A Some Preemptive Responses to Questions</p>
<p>In preparation of this manuscript, I have sought advice and feedback from a number of colleagues. These discussions have resulted in a few common questions of related themes. As such, many do not necessarily belong in the main content of an academic paper. Here I will list and preemptively answer the common ones in hope of aiding the reader in better understanding this work, the context around it, and the potential biases that may exist in the results as a function of my personal background.</p>
<p>A.1 Why Where you Recording Information About Papers?</p>
<p>Before I started working on JSAT, or had even learned about machine learning, I had a side project implementing an arbitrary precision math library 6 . I worked on this library for four years, and implemented a number of algorithms for computing different decomposition, mathematical constants, common functions (e.g., Fibonacci numbers), complex numbers, all at arbitrary precision. As part of this I began to read and implement a number of papers for these techniques. As time went on, and I occasionally found and discovered bugs in my previous implementations, I grew frustrated in the bug fixing process. Fixing bugs for these more involved methods required me to re-understand and find previous papers, which I was not good at. As a result, when I started JSAT, I began keeping notes to myself from the onset. I again intended it to be a multi-year and long term project, and wanted to avoid repetition of previous failures.</p>
<p>A.2 Why Where you Implementing so many Papers?</p>
<p>Early in my computer science career, I had forgone most all advanced math courses -taking the bare minimum to get my degree, with the exception of a numerical analysis course that I thought would be relevant to my arbitrary precision math library mentioned above. I had instead focused on taking many more CS courses until I happened to take a machine learning class and became enamoured with the concept and the field. This left me with a situation where I wanted to get into a domain that was math heavy, but my skills had long languished. While I personally felt I have always understood an algorithm or technique best once I can implement it, I came to rely on implementing an algorithm as a crutch to my lesser mathematical skills. This has continued far longer than I would like in many ways and so have continued to attempt to implement papers I want to understand as the fastest way for me to come to a functional understanding of a paper.</p>
<p>B Statistical Test assumptions</p>
<p>In Table 3, we perform a normality test, which confirms that all of our numeric features deviate significantly from a normal distribution, making a standard Student's t-test inappropriate for hypothesis testing.</p>
<p>The Mann-Whitney test assumes that the variance of the two distributions under test are equal. We can see from Table 4 that this again holds for all of our numeric features, with the exception of the Year of the publication and the total number of pages. If we instead preformed a Welch test, which does not have the equality of variance assumption, we still arrive at the conclusion that Year (p = .554) and number of Pages (p = 0.134) do not have any significant relationship with reproducibility. The pages variable is also impacted by a few outliers (the most extreme of which has over 400 pages), which is the cause of the apparent discrepancy in variance.                </p>
<p>D Contingency Tables for Nominal Features</p>
<p>•
Number of Other Figures: This was a catch-all class for any figure that was not a Graph/Plot,</p>
<p>Figure 1 :
1Histograms of the unnormalized numeric variables considered.</p>
<p>Figure 2 :
2Histograms of the page normalized numeric variables considered.</p>
<p>Table 1 :
1Significance test of which paper properties impact reproducibility. Results significant at α ≤ 0.05 marked with" * ".Our features are either numeric or categorical. For each numeric feature (except the number of pages and number of authors), we normalized the value by the number of pages in the paper. Longer papers naturally have more space to include more equations, figures, etc., and this was done to make all papers more directly comparable. For numeric features we used the non-parametric Mann-Whitney U[10] test to determine significance. A Shapiro-Wilk test of normalityFeature 
p-value </p>
<p>Year Published 
0.964 
Year First Attempted 
0.674 
Venue Type 
0.631 
Rigor vs Empirical * 
1.55 × 10 −9 
Has Appendix 
0.330 
Looks Intimidating 
0.829 
Readability * 
9.68 × 10 −25 
Algorithm Difficulty * 
2.94 × 10 −5 
Pseudo Code * 
2.31 × 10 −4 
Primary Topic * 
7.039 × 10 −4 
Exemplar Problem 
0.720 
Compute Specified 
0.257 
Hyperparameters Specified * 8.45 × 10 −6 
Compute Needed * 
8.75 × 10 −5 
Authors Reply * 
6.01 × 10 −8 
Code Available 
0.213 
Pages 
0.364 
Publication Venue 
0.342 
Number of References 
0.740 
Number Equations * 
0.004 
Number Proofs 
0.130 
Number Tables * 
0.010 
Number Graphs/Plots 
0.139 
Number Other Figures 
0.217 
Conceptualization Figures 
0.365 
Number of Authors 
0.497 </p>
<p>Table 2 :
2Relationship between use of Pseudo Code and ReadabilityPaper Readability </p>
<p>Table 3 :
3Test of Normality (Shapiro-Wilk) of numeric features, showing that a standard t-test would 
not be appropriate 
Reproduced 
W 
p-value 
Number of References 
No 
0.920 
2.583 × 10 −5 
Yes 
0.634 
1.824 × 10 −18 
Normalized Num References 
No 
0.953 
0.002 
Yes 
0.848 
1.084 × 10 −11 
Normalized Number of Equations 
No 
0.841 
1.366 × 10 −8 
Yes 
0.816 
5.417 × 10 −13 
Normalized Number of Proofs 
No 
0.654 
1.757 × 10 −13 
Yes 
0.671 
1.454 × 10 −17 
Normalized Total Tables and Figures 
No 
0.903 
3.833 × 10 −6 
Yes 
0.722 
3.608 × 10 −16 
Normalized Number of Tables 
No 
0.710 
2.990 × 10 −12 
Yes 
0.885 
6.970 × 10 −10 
Normalized Number of Graphs/Plots 
No 
0.842 
1.539 × 10 −8 
Yes 
0.659 
7.344 × 10 −18 
Normalized Number of Other Figures 
No 
0.632 
6.193 × 10 −14 
Yes 
0.353 
8.797 × 10 −24 
Normalized Conceptualization Figures 
No 
0.572 
4.755 × 10 −15 
Yes 
0.606 
4.003 × 10 −19 
Pages 
No 
0.789 
3.090 × 10 −10 
Yes 
0.697 
7.302 × 10 −17 </p>
<p>Table 4 :
4Test of Equality of Variances (Levene's) for numeric features.F df 
p </p>
<p>Table 5 :
5χ 2 test for Venue Type (p = 0.502) counts and expectations for a paper's Readability towards ability to reproduce its results.Table 6: χ 2 test for Author's Code being made available (p = 0.184) counts and expectations for a paper's Readability towards ability to reproduce its results.Type </p>
<p>Table 7 :
7χ 2 text for whether an Author Replied to email questions (p = 6.016 × 10 −8 ) counts and expectations for a paper's Readability towards ability to reproduce its results.Authors Reply 
Reproduced 
No 
Yes 
No 
Count 23.00 
4.00 
Expected count 12.96 14.04 
Yes 
Count 
1.00 22.00 
Expected count 11.04 11.96 </p>
<p>Table 8 :
8χ 2 tests for Rigor vs Empirical (p = 1.545 × 10 −9 ) counts and expectations for a paper's Readability towards ability to reproduce its results.Table 9: χ 2 tests for a paper having an Appendix p = 0.330 counts and expectations for a paper's Readability towards ability to reproduce its results.Rigor vs Empirical 
Reproduced 
Empirical Theory Balance 
No 
Count 
14.00 
53.00 
26.00 
Expected count 
29.18 
30.64 
33.19 
Yes 
Count 
66.00 
31.00 
65.00 
Expected count 
50.82 
53.36 
57.81 </p>
<p>Has Appendix 
Reproduced 
No 
Yes 
No 
Count 
52.00 41.00 
Expected count 
56.16 36.84 
Yes 
Count 102.00 60.00 
Expected count 
97.84 64.16 </p>
<p>Table 10 :
10χ 2 tests for when a paper "Looks Intimidating" (p = 0.829) counts and expectations for a paper's Readability towards ability to reproduce its results.Looks Intimidating 
Reproduced 
No 
Yes 
No 
Count 49.00 
44.00 
Expected count 50.33 
42.67 
Yes 
Count 89.00 
73.00 
Expected count 87.67 
74.33 </p>
<p>Table 11 :
11χ 2 test (p = 9.681 × 10 −25 ) counts and expectations for a paper's Readability towards ability to reproduce its results.Paper Readability </p>
<p>Table 12 :
12χ 2 tests for an Algorithm's Difficulty (p = 2.939 × 10 −5 ) counts and expectations for a paper's Readability towards ability to reproduce its results.Algorithm Difficulty 
Reproduced 
Low Medium High 
No 
Count 21.00 
38.00 34.00 
Expected count 37.56 
32.09 23.34 
Yes 
Count 82.00 
50.00 30.00 
Expected count 65.44 
55.91 40.66 </p>
<p>Table 13 :
13χ 2 tests for whether a paper has Pseudo-Code (p = 2.308 × 10 −4 ) counts and expectations for a paper's Readability towards ability to reproduce its results.Pseudo Code </p>
<p>Table 14 :
14χ 2 tests for Data being Available (p = .558) counts and expectations for a paper's Readability towards ability to reproduce its results.Table 15: χ 2 tests for use of an Exemplar Toy Problem (p = 0.720) counts and expectations for a paper's Readability towards ability to reproduce its results.Data Available 
Reproduced 
No 
Yes 
No 
Count 17.00 
75.00 
Expected count 14.85 
77.15 
Yes 
Count 24.00 138.00 
Expected count 26.15 135.85 </p>
<p>Uses Exemplar Toy Problem 
Reproduced 
No 
Yes 
No 
Count 
65.00 
28.00 
Expected count 
66.74 
26.26 
Yes 
Count 118.00 
44.00 
Expected count 116.26 
45.74 </p>
<p>Table 16 :
16χ 2 tests for Exact Compute Used being specified (p = 0.257) counts and expectations for a paper's Readability towards ability to reproduce its results.Exact Compute Used 
Reproduced 
No 
Yes 
No 
Count 
76.00 
17.00 
Expected count 
71.85 
21.15 
Yes 
Count 121.00 
41.00 
Expected count 125.15 
36.85 </p>
<p>Table 17 :
17χ 2 tests for Hyperparamters being Specified (p = 8.450 × 10 −6 ) counts and expectations for a paper's Readability towards ability to reproduce its results.Hyperparameters Specified 
Reproduced 
No 
Yes 
Partial 
No 
Count 34.00 
54.00 
5.00 
Expected count 20.06 
70.02 
2.92 
Yes 
Count 21.00 138.00 
3.00 
Expected count 34.94 121.98 
5.08 </p>
<p>Table 18 :
18χ 2 tests for the level of Compute Needed (p = 2.788 × 10 −5 ) counts and expectations for a paper's Readability towards ability to reproduce its results.Compute Needed </p>
<p>Table 19 :
19χ 2 test for Primary Topic (p = 7.039 × 10 −4 ) counts and expectations for a paper's Readability towards ability to reproduce its results.Reproduced </p>
<p>33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.
Not done in a continuous run. Feature collection, and paper selection, and total time preparing the study data took approximately 6 months.
Not all papers have L A T E X available, and older papers are often scanned making automation difficult.
This information was obtained from our own record keeping over time and paper-organizing software
This would not apply for pure theory papers, and we remind the reader that all papers in this study proposed and evaluated some new algorithm, and thus do not fall into a pure theory category.
https://openai.com/blog/better-language-models/
e.g., see the GMP project as an example of a far more robust and similar project https://gmplib.org/
AcknowledgementsI would like to thank Jared Sylvester, Arash Rahnama, Charles Nicholas, Cynthia Matuszek, Frank Ferraro, Ian Soboroff, and Ashley Klein, who all provided valuable discussion and feedback on this work through its formation to completion.
Artificial intelligence faces reproducibility crisis. M Hutson, Science. 3596377M. Hutson, "Artificial intelligence faces reproducibility crisis," Science, vol. 359, no. 6377, pp. 725-726, 2018. [Online]. Available: https://science.sciencemag.org/content/359/6377/725</p>
<p>Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets. R Dror, G Baumer, M Bogomolov, R Reichart, Transactions of the Association for Computational Linguistics. 5R. Dror, G. Baumer, M. Bogomolov, and R. Reichart, "Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets," Transactions of the Association for Computational Linguistics, vol. 5, pp. 471-486, 12 2017. [Online]. Available: https://www.aclweb.org/anthology/Q17-1033</p>
<p>A Practical Taxonomy of Reproducibility for Machine Learning Research. R Tatman, J Vanderplas, S Dane, Reproducibility in ML Workshop, ICML'18. R. Tatman, J. Vanderplas, and S. Dane, "A Practical Taxonomy of Reproducibility for Machine Learning Research," in Reproducibility in ML Workshop, ICML'18, 2018.</p>
<p>ML-Schema : Exposing the Semantics of Machine Learning with Schemas and Ontologies. G C Publio, D Esteves, H Zafar, Reproducibility in ML Workshop, ICML'18. G. C. Publio, D. Esteves, and H. Zafar, "ML-Schema : Exposing the Semantics of Machine Learning with Schemas and Ontologies," in Reproducibility in ML Workshop, ICML'18, 2018.</p>
<p>Reproducible Research Environments with repo2docker. J Forde, T Head, C Holdgraf, Y Panda, F Perez, G Nalvarte, B Ragan-Kelley, E Sundell, Reproducibility in ML Workshop, ICML'18. J. Forde, T. Head, C. Holdgraf, Y. Panda, F. Perez, G. Nalvarte, B. Ragan-kelley, and E. Sundell, "Reproducible Research Environments with repo2docker," in Reproducibility in ML Workshop, ICML'18, 2018.</p>
<p>Replicability is not reproducibility: nor is it good science. C Drummond, Proceedings of the Evaluation Methods for Machine Learning Workshop at the 26th ICML. the Evaluation Methods for Machine Learning Workshop at the 26th ICMLMontreal, Canada; Montreal, CanadaEvaluation Methods for Machine Learning Workshop, the 26th ICMLC. Drummond, "Replicability is not reproducibility: nor is it good science," in Proceedings of the Evaluation Methods for Machine Learning Workshop at the 26th ICML, Montreal, Canada,2009, ser. Evaluation Methods for Machine Learning Workshop, the 26th ICML, June 14-18, 2009, Montreal, Canada, 2009.</p>
<p>State of the Art: Reproducibility in Artificial Intelligence. O E Gundersen, S Kjensmo, Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI-18). the 32nd AAAI Conference on Artificial Intelligence (AAAI-18)O. E. Gundersen and S. Kjensmo, "State of the Art: Reproducibility in Artificial Intelligence," Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI-18), pp. 1644-1651, 2018.</p>
<p>NIPS 2017 Test-of-time award presentation. A Rahimi, B Recht, A. Rahimi and B. Recht, "NIPS 2017 Test-of-time award presentation," 2017.</p>
<p>Winner's Curse? On Pace, Progress, and Empirical Rigor. D Sculley, J Snoek, A Rahimi, A Wiltschko, ICLR Workshop track. D. Sculley, J. Snoek, A. Rahimi, and A. Wiltschko, "Winner's Curse? On Pace, Progress, and Empirical Rigor," in ICLR Workshop track, 2018. [Online]. Available: https://openreview.net/ pdf?id=rJWF0Fywf</p>
<p>On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other. H B Mann, D R Whitney, The Annals of Mathematical Statistics. 181H. B. Mann and D. R. Whitney, "On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other," The Annals of Mathematical Statistics, vol. 18, no. 1, pp. 50-60, 3 1947. [Online]. Available: http://projecteuclid.org/euclid.aoms/1177730491</p>
<p>An analysis of variance test for normality (complete samples). S S Shapiro, M B Wilk, https:/academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/52.3-4.591Biometrika. 523-4S. S. Shapiro and M. B. Wilk, "An analysis of variance test for normality (complete samples)," Biometrika, vol. 52, no. 3-4, pp. 591-611, 12 1965. [Online]. Available: https://academic.oup. com/biomet/article-lookup/doi/10.1093/biomet/52.3-4.591</p>
<p>On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. K Pearson, https:/www.tandfonline.com/doi/full/10.1080/14786440009463897Journal of Science. 50302K. Pearson, "On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling," The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, vol. 50, no. 302, pp. 157-175, 7 1900. [Online]. Available: https://www. tandfonline.com/doi/full/10.1080/14786440009463897</p>
<p>Contingency Tables Involving Small Numbers and the χ2 Test. F Yates, Supplement to the Journal of the Royal Statistical Society. 12F. Yates, "Contingency Tables Involving Small Numbers and the χ2 Test," Supplement to the Journal of the Royal Statistical Society, vol. 1, no. 2, pp. 217-235, 1934. [Online]. Available: http://www.jstor.org/stable/2983604</p>
<p>Use of Ranks in One-Criterion Variance Analysis. W H Kruskal, W A Wallis, Journal of the American Statistical Association. 472601952W. H. Kruskal and W. A. Wallis, "Use of Ranks in One-Criterion Variance Analysis," Journal of the American Statistical Association, vol. 47, no. 260, pp. 583-621, 12 1952. [Online].</p>
<p>. https:/www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483441Available: https://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483441</p>
<p>Multiple Comparisons Among Means. O J Dunn, Journal of the American Statistical Association. 56293O. J. Dunn, "Multiple Comparisons Among Means," Journal of the American Statistical Association, vol. 56, no. 293, pp. 52-64, 1961. [Online]. Available: http://www.jstor.org/ stable/2282330</p>
<p>JASP (Version 0.9. Jasp Team, Computer softwareJASP Team, "JASP (Version 0.9)[Computer software]," 2018. [Online]. Available: https:// jasp-stats.org/</p>
<p>JSAT: Java Statistical Analysis Tool, a Library for Machine Learning. E Raff, Journal of Machine Learning Research. 1823E. Raff, "JSAT: Java Statistical Analysis Tool, a Library for Machine Learning," Journal of Machine Learning Research, vol. 18, no. 23, pp. 1-5, 2017. [Online]. Available: http://jmlr. org/papers/v18/16-131.html</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5007 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5007</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5007</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-6825ba09383bc758f9a2feaebabe35a6cd4adc4c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6825ba09383bc758f9a2feaebabe35a6cd4adc4c" target="_blank">How Language Model Hallucinations Can Snowball</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work hypothesizes that in some cases, when justifying previously generated hallucinations, LMs output false claims that they can separately recognize as incorrect, which leads to hallucination snowballing: an LM over-commits to early mistakes, leading to more mistakes that it otherwise would not make.</p>
                <p><strong>Paper Abstract:</strong> A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements. Hallucinations are often attributed to knowledge gaps in LMs, but we hypothesize that in some cases, when justifying previously generated hallucinations, LMs output false claims that they can separately recognize as incorrect. We construct three question-answering datasets where ChatGPT and GPT-4 often state an incorrect answer and offer an explanation with at least one incorrect claim. Crucially, we find that ChatGPT and GPT-4 can identify 67% and 87% of their own mistakes, respectively. We refer to this phenomenon as hallucination snowballing: an LM over-commits to early mistakes, leading to more mistakes that it otherwise would not make.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5007.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5007.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (direct prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo) under direct answer-first prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned OpenAI transformer (gpt-3.5-turbo) evaluated zero-shot with a direct prompt that elicits an immediate Yes/No answer followed by a justification (answer-first format).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary instruction-tuned transformer LLM from OpenAI (gpt-3.5-turbo); architecture: transformer, instruction-tuned; exact size not reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Answer-first justification (direct prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Prompting that encourages the model to state a final Yes/No answer token immediately and then produce an explanation; the model therefore generates a single continuation (one chain) conditioned on the committed answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Primality Testing; Senator Search; Graph Connectivity (three in-paper QA datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Three zero-shot yes/no datasets designed to be inherently sequential: (1) primality testing of large primes (correct answer Yes), (2) senator-alma mater search (correct answer No), (3) directed graph connectivity instantiated as flight-route existence (correct answer No). Each dataset has 500 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Aggregate accuracy (direct prompt): ChatGPT average accuracy 39.87% (Table 6). Per-task error rates (Table 6): Graph connectivity error 82.0% (accuracy 18.0%), Primality error 67.8% (accuracy 32.2%), Senator Search error 30.6% (accuracy 69.4%). When the model produced an incorrect justification, ChatGPT recognized its incorrect claims when re-queried in isolation in 67.37% of such cases (detection of its own mistakes; Table 7 / main text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>When prompted with chain-of-thought style 'Let's think step by step', the paper reports large accuracy improvements (text states: senator solved perfectly; primality ≤10% error; graph ≤30% error) and a large reduction in snowballed-hallucination rate (see chain-of-thought entries and Table 9: chain-of-thought average snowballed-hallucination rate across datasets 9.40%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Under the direct (answer-first) prompt ChatGPT often commits immediately to Yes/No and then generates justifications that include incorrect claims (hallucinations). Many of these incorrect claims are recognizable by the model if presented in isolation (ChatGPT detected ~67% of its incorrect claims). The answer-first behavior induces 'snowballing' where an early incorrect commitment leads to additional false assertions that the model would often mark as incorrect when checked separately.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No evidence in the paper that direct answer-first prompting outperforms chain-of-thought; direct prompting yields higher snowballing and lower overall accuracy compared to the chain-of-thought prompt in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Language Model Hallucinations Can Snowball', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5007.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5007.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (direct prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 under direct answer-first prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI GPT-4 evaluated zero-shot with a direct prompt that elicits an immediate Yes/No answer followed by a justification; shows frequent immediate commitment and subsequent justifications that can contain false claims.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large transformer-based model from OpenAI (GPT-4); instruction-tuned; exact model size/parameters not disclosed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Answer-first justification (direct prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Same answer-first prompting as for ChatGPT: the model tends to produce a single immediate Yes/No token and then a single chain of justification conditioned on that commitment.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Primality Testing; Senator Search; Graph Connectivity (same three QA datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See ChatGPT entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Aggregate accuracy (direct prompt): GPT-4 average accuracy 16.6% (Table 6). Per-task error rates (Table 6): Graph connectivity error 88.4% (accuracy 11.6%), Primality error 74.8% (accuracy 25.2%), Senator Search error 87.0% (accuracy 13.0%). When the model produced an incorrect justification, GPT-4 recognized its incorrect claims when presented in isolation in 87.03% of such cases (detection of its own mistakes; Table 7 / main text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Chain-of-thought prompting ('Let's think step-by-step') substantially reduced the rate of snowballed hallucinations (Table 9: chain-of-thought average snowballed-hallucination rate 3.87%) and the paper reports notable accuracy improvements under that prompt (see chain-of-thought entry).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 frequently over-commits to an initial answer and then produces supporting incorrect facts (hallucinations) that it often can flag as incorrect if those facts are examined in isolation. Thus GPT-4's hallucinations can be induced by earlier commitments rather than pure knowledge gaps. GPT-4 recognized a high fraction (~87%) of its own incorrect claims in separate verification queries.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although chain-of-thought improved overall metrics, the paper reports that GPT-4 still exhibited a high snowballed-hallucination rate under some settings and sometimes produced invalid reasoning chains even when arriving at the correct final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Language Model Hallucinations Can Snowball', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5007.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5007.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-thought ('Let's think step-by-step')</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot chain-of-thought prompting via appending 'Let's think step-by-step' to the prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that elicits the model to produce intermediate reasoning steps before stating a final answer, intended to avoid committing prematurely and to expose stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to ChatGPT and GPT-4 (reported for both)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned transformer models evaluated zero-shot with the appended chain-of-thought cue.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Zero-shot chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>A single explicit chain-of-thought generation where the model writes step-by-step reasoning prior to or alongside producing its final answer; the model still generates one primary chain, not an ensemble of diverse chains.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Primality Testing; Senator Search; Graph Connectivity</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same three QA tasks designed to be sequential and hard to solve in a single timestep.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported improvements: the paper states chain-of-thought allowed the model to solve Senator Search perfectly, reduce Primality Testing error to ≤10%, and reduce Graph Connectivity error to ≤30% (text summary). Measured reduction in snowballed-hallucination rate (measured as fraction of all dataset examples): ChatGPT average snowballed-hallucination rate 9.40% and GPT-4 average 3.87% under the 'Let's think step-by-step' prompt (Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared to direct answer-first prompting: direct prompt ChatGPT average snowballed-hallucination rate ~67.37% and GPT-4 ~87.03% when measured as fraction of incorrect claims recognized (see text and tables); chain-of-thought substantially lowered snowball rates and improved task accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Encouraging models to produce step-by-step reasoning before committing to an answer substantially improves accuracy on the studied sequential tasks and markedly reduces the incidence of snowballed hallucinations (i.e., downstream wrong claims induced by an earlier incorrect commitment). However, chain-of-thought does not eliminate hallucinations: models sometimes hallucinate inside the reasoning chain itself, which can still propagate and cause incorrect conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The paper reports that even with chain-of-thought the model sometimes introduces false intermediate steps (e.g., listing a nonexistent flight), which then becomes a snowballed hallucination; GPT-4 still had high snowball rates in some failure cases (paper notes GPT-4 had 94.90% snowballed hallucination rate in certain chain-of-thought error subsets — see discussion), showing chain-of-thought is not a complete fix.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Language Model Hallucinations Can Snowball', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5007.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5007.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diverse decoding (temperature, sampling, beam search discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decoding-time diversity: increased temperature, top-k/nucleus sampling, and beam search (beam search discussed but not tested)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decoding strategies that aim to produce more diverse continuations: higher temperature and stochastic sampling spread probability mass across tokens; beam search keeps multiple high-probability sequences (potentially maintaining alternative uncommitted continuations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to ChatGPT and GPT-4 in experiments (temperature/top-k/nucleus), beam search discussed but not applied</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same transformer LLMs; experiments performed with greedy decoding and also with increased temperature (t=0.6, t=0.9) and discussion of top-k/nucleus and beam search.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Diverse decoding (temperature / top-k / nucleus / beam search [discussed])</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Increasing temperature (t>0) spreads probability mass to less-likely tokens, enabling more varied continuations; top-k and nucleus sampling restrict the support for sampling but still produce stochastic diverse outputs; beam search retains multiple candidate continuations to avoid premature commitment.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Primality Testing; Senator Search; Graph Connectivity (same three tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above; tested whether stochastic/diverse decoding reduces immediate commitment and downstream snowball hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Increasing temperature to t=0.6 and t=0.9 yielded only small changes in error rates and snowballed-hallucination rates (Table 10 and Table 11). Example averages: ChatGPT average error ~60.13% at t=0.0 and ~58.53% at t=0.6/0.9 (minor improvement); GPT-4 average error ~83.40% at t=0.0 and ~81.67% at t=0.9 (small change). The paper reports that sampling or raising temperature did not meaningfully reduce snowballing; top-k/nucleus sampling is argued would not help and could worsen immediate commitments, and beam search could in principle help but was not testable via the API.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared to chain-of-thought prompting: chain-of-thought showed substantial reductions in snowballed-hallucination rates and larger accuracy gains; diverse decoding (temperature) showed minimal gains in accuracy or snowball reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Diverse decoding via higher temperature or sampling did not significantly reduce error rates or the incidence of snowballed hallucinations in these experiments. The paper argues top-k/nucleus sampling is unlikely to help and beam search could theoretically mitigate snowballing by keeping alternative continuations, but beam search was not evaluated due to API limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The experiments show only marginal improvements from increasing temperature and indicate top-k/nucleus sampling could even increase the probability of immediate (and potentially incorrect) answers; beam search remains an untested but hypothesized remedy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Language Model Hallucinations Can Snowball', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>The curious case of neural text degeneration <em>(Rating: 1)</em></li>
                <li>Why exposure bias matters: An imitation learning perspective of error accumulation in language generation <em>(Rating: 2)</em></li>
                <li>The parallelism tradeoff: Limitations of log-precision transformers <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5007",
    "paper_id": "paper-6825ba09383bc758f9a2feaebabe35a6cd4adc4c",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "ChatGPT (direct prompt)",
            "name_full": "ChatGPT (gpt-3.5-turbo) under direct answer-first prompting",
            "brief_description": "Instruction-tuned OpenAI transformer (gpt-3.5-turbo) evaluated zero-shot with a direct prompt that elicits an immediate Yes/No answer followed by a justification (answer-first format).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "Proprietary instruction-tuned transformer LLM from OpenAI (gpt-3.5-turbo); architecture: transformer, instruction-tuned; exact size not reported in paper.",
            "reasoning_method_name": "Answer-first justification (direct prompt)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Prompting that encourages the model to state a final Yes/No answer token immediately and then produce an explanation; the model therefore generates a single continuation (one chain) conditioned on the committed answer.",
            "task_name": "Primality Testing; Senator Search; Graph Connectivity (three in-paper QA datasets)",
            "task_description": "Three zero-shot yes/no datasets designed to be inherently sequential: (1) primality testing of large primes (correct answer Yes), (2) senator-alma mater search (correct answer No), (3) directed graph connectivity instantiated as flight-route existence (correct answer No). Each dataset has 500 examples.",
            "performance": "Aggregate accuracy (direct prompt): ChatGPT average accuracy 39.87% (Table 6). Per-task error rates (Table 6): Graph connectivity error 82.0% (accuracy 18.0%), Primality error 67.8% (accuracy 32.2%), Senator Search error 30.6% (accuracy 69.4%). When the model produced an incorrect justification, ChatGPT recognized its incorrect claims when re-queried in isolation in 67.37% of such cases (detection of its own mistakes; Table 7 / main text).",
            "comparison_with_other_method": true,
            "performance_other_method": "When prompted with chain-of-thought style 'Let's think step by step', the paper reports large accuracy improvements (text states: senator solved perfectly; primality ≤10% error; graph ≤30% error) and a large reduction in snowballed-hallucination rate (see chain-of-thought entries and Table 9: chain-of-thought average snowballed-hallucination rate across datasets 9.40%).",
            "key_findings": "Under the direct (answer-first) prompt ChatGPT often commits immediately to Yes/No and then generates justifications that include incorrect claims (hallucinations). Many of these incorrect claims are recognizable by the model if presented in isolation (ChatGPT detected ~67% of its incorrect claims). The answer-first behavior induces 'snowballing' where an early incorrect commitment leads to additional false assertions that the model would often mark as incorrect when checked separately.",
            "counter_examples_or_negative_results": "No evidence in the paper that direct answer-first prompting outperforms chain-of-thought; direct prompting yields higher snowballing and lower overall accuracy compared to the chain-of-thought prompt in the paper's experiments.",
            "uuid": "e5007.0",
            "source_info": {
                "paper_title": "How Language Model Hallucinations Can Snowball",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4 (direct prompt)",
            "name_full": "GPT-4 under direct answer-first prompting",
            "brief_description": "OpenAI GPT-4 evaluated zero-shot with a direct prompt that elicits an immediate Yes/No answer followed by a justification; shows frequent immediate commitment and subsequent justifications that can contain false claims.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Proprietary large transformer-based model from OpenAI (GPT-4); instruction-tuned; exact model size/parameters not disclosed in the paper.",
            "reasoning_method_name": "Answer-first justification (direct prompt)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Same answer-first prompting as for ChatGPT: the model tends to produce a single immediate Yes/No token and then a single chain of justification conditioned on that commitment.",
            "task_name": "Primality Testing; Senator Search; Graph Connectivity (same three QA datasets)",
            "task_description": "See ChatGPT entry.",
            "performance": "Aggregate accuracy (direct prompt): GPT-4 average accuracy 16.6% (Table 6). Per-task error rates (Table 6): Graph connectivity error 88.4% (accuracy 11.6%), Primality error 74.8% (accuracy 25.2%), Senator Search error 87.0% (accuracy 13.0%). When the model produced an incorrect justification, GPT-4 recognized its incorrect claims when presented in isolation in 87.03% of such cases (detection of its own mistakes; Table 7 / main text).",
            "comparison_with_other_method": true,
            "performance_other_method": "Chain-of-thought prompting ('Let's think step-by-step') substantially reduced the rate of snowballed hallucinations (Table 9: chain-of-thought average snowballed-hallucination rate 3.87%) and the paper reports notable accuracy improvements under that prompt (see chain-of-thought entry).",
            "key_findings": "GPT-4 frequently over-commits to an initial answer and then produces supporting incorrect facts (hallucinations) that it often can flag as incorrect if those facts are examined in isolation. Thus GPT-4's hallucinations can be induced by earlier commitments rather than pure knowledge gaps. GPT-4 recognized a high fraction (~87%) of its own incorrect claims in separate verification queries.",
            "counter_examples_or_negative_results": "Although chain-of-thought improved overall metrics, the paper reports that GPT-4 still exhibited a high snowballed-hallucination rate under some settings and sometimes produced invalid reasoning chains even when arriving at the correct final answer.",
            "uuid": "e5007.1",
            "source_info": {
                "paper_title": "How Language Model Hallucinations Can Snowball",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Chain-of-thought ('Let's think step-by-step')",
            "name_full": "Zero-shot chain-of-thought prompting via appending 'Let's think step-by-step' to the prompt",
            "brief_description": "A prompting method that elicits the model to produce intermediate reasoning steps before stating a final answer, intended to avoid committing prematurely and to expose stepwise reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to ChatGPT and GPT-4 (reported for both)",
            "model_description": "Instruction-tuned transformer models evaluated zero-shot with the appended chain-of-thought cue.",
            "reasoning_method_name": "Zero-shot chain-of-thought prompting",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "A single explicit chain-of-thought generation where the model writes step-by-step reasoning prior to or alongside producing its final answer; the model still generates one primary chain, not an ensemble of diverse chains.",
            "task_name": "Primality Testing; Senator Search; Graph Connectivity",
            "task_description": "Same three QA tasks designed to be sequential and hard to solve in a single timestep.",
            "performance": "Reported improvements: the paper states chain-of-thought allowed the model to solve Senator Search perfectly, reduce Primality Testing error to ≤10%, and reduce Graph Connectivity error to ≤30% (text summary). Measured reduction in snowballed-hallucination rate (measured as fraction of all dataset examples): ChatGPT average snowballed-hallucination rate 9.40% and GPT-4 average 3.87% under the 'Let's think step-by-step' prompt (Table 9).",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared to direct answer-first prompting: direct prompt ChatGPT average snowballed-hallucination rate ~67.37% and GPT-4 ~87.03% when measured as fraction of incorrect claims recognized (see text and tables); chain-of-thought substantially lowered snowball rates and improved task accuracy.",
            "key_findings": "Encouraging models to produce step-by-step reasoning before committing to an answer substantially improves accuracy on the studied sequential tasks and markedly reduces the incidence of snowballed hallucinations (i.e., downstream wrong claims induced by an earlier incorrect commitment). However, chain-of-thought does not eliminate hallucinations: models sometimes hallucinate inside the reasoning chain itself, which can still propagate and cause incorrect conclusions.",
            "counter_examples_or_negative_results": "The paper reports that even with chain-of-thought the model sometimes introduces false intermediate steps (e.g., listing a nonexistent flight), which then becomes a snowballed hallucination; GPT-4 still had high snowball rates in some failure cases (paper notes GPT-4 had 94.90% snowballed hallucination rate in certain chain-of-thought error subsets — see discussion), showing chain-of-thought is not a complete fix.",
            "uuid": "e5007.2",
            "source_info": {
                "paper_title": "How Language Model Hallucinations Can Snowball",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Diverse decoding (temperature, sampling, beam search discussion)",
            "name_full": "Decoding-time diversity: increased temperature, top-k/nucleus sampling, and beam search (beam search discussed but not tested)",
            "brief_description": "Decoding strategies that aim to produce more diverse continuations: higher temperature and stochastic sampling spread probability mass across tokens; beam search keeps multiple high-probability sequences (potentially maintaining alternative uncommitted continuations).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to ChatGPT and GPT-4 in experiments (temperature/top-k/nucleus), beam search discussed but not applied",
            "model_description": "Same transformer LLMs; experiments performed with greedy decoding and also with increased temperature (t=0.6, t=0.9) and discussion of top-k/nucleus and beam search.",
            "reasoning_method_name": "Diverse decoding (temperature / top-k / nucleus / beam search [discussed])",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Increasing temperature (t&gt;0) spreads probability mass to less-likely tokens, enabling more varied continuations; top-k and nucleus sampling restrict the support for sampling but still produce stochastic diverse outputs; beam search retains multiple candidate continuations to avoid premature commitment.",
            "task_name": "Primality Testing; Senator Search; Graph Connectivity (same three tasks)",
            "task_description": "As above; tested whether stochastic/diverse decoding reduces immediate commitment and downstream snowball hallucinations.",
            "performance": "Increasing temperature to t=0.6 and t=0.9 yielded only small changes in error rates and snowballed-hallucination rates (Table 10 and Table 11). Example averages: ChatGPT average error ~60.13% at t=0.0 and ~58.53% at t=0.6/0.9 (minor improvement); GPT-4 average error ~83.40% at t=0.0 and ~81.67% at t=0.9 (small change). The paper reports that sampling or raising temperature did not meaningfully reduce snowballing; top-k/nucleus sampling is argued would not help and could worsen immediate commitments, and beam search could in principle help but was not testable via the API.",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared to chain-of-thought prompting: chain-of-thought showed substantial reductions in snowballed-hallucination rates and larger accuracy gains; diverse decoding (temperature) showed minimal gains in accuracy or snowball reduction.",
            "key_findings": "Diverse decoding via higher temperature or sampling did not significantly reduce error rates or the incidence of snowballed hallucinations in these experiments. The paper argues top-k/nucleus sampling is unlikely to help and beam search could theoretically mitigate snowballing by keeping alternative continuations, but beam search was not evaluated due to API limitations.",
            "counter_examples_or_negative_results": "The experiments show only marginal improvements from increasing temperature and indicate top-k/nucleus sampling could even increase the probability of immediate (and potentially incorrect) answers; beam search remains an untested but hypothesized remedy.",
            "uuid": "e5007.3",
            "source_info": {
                "paper_title": "How Language Model Hallucinations Can Snowball",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "The curious case of neural text degeneration",
            "rating": 1
        },
        {
            "paper_title": "Why exposure bias matters: An imitation learning perspective of error accumulation in language generation",
            "rating": 2
        },
        {
            "paper_title": "The parallelism tradeoff: Limitations of log-precision transformers",
            "rating": 2
        }
    ],
    "cost": 0.018831749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>How Language Model Hallucinations Can Snowball</h1>
<p>Muru Zhang ${ }^{\text { }}$ Ofir Press ${ }^{\text { }}$ William Merrill Alisa Liu ${ }^{\text { }}$ Noah A. Smith ${ }^{\circledR}$<br>${ }^{\circ}$ Paul G. Allen School of Computer Science and Engineering, University of Washington<br>${ }^{\text {a }}$ New York University<br>${ }^{\text {a }}$ Allen Institute for Artificial Intelligence<br>nanami17@cs.washington.edu</p>
<h4>Abstract</h4>
<p>A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements. Hallucinations are often attributed to knowledge gaps in LMs, but we hypothesize that in some cases, when justifying previously generated hallucinations, LMs output false claims that they can separately recognize as incorrect. We construct three questionanswering datasets where ChatGPT and GPT-4 often state an incorrect answer and offer an explanation with at least one incorrect claim. Crucially, we find that ChatGPT and GPT-4 can identify $67 \%$ and $87 \%$ of their own mistakes, respectively. We refer to this phenomenon as hallucination snowballing: an LM over-commits to early mistakes, leading to more mistakes that it otherwise would not make. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Language models are increasingly being deployed to interface with humans in open-ended information-seeking and problem-solving settings. Despite their diverse capabilities and extreme fluency, a major open challenge is that LMs still hallucinate by making up facts or citing sources that do not exist (Maynez et al., 2020; Liu et al., 2023, i.a.), often while sounding extremely plausible.</p>
<p>Hallucination is commonly attributed to knowledge gaps in LMs (Zheng et al., 2023), motivating mitigation strategies through retrieval over knowledge bases (Lewis et al., 2020; Shuster et al., 2021; Peng et al., 2023) But, do LMs only hallucinate when they do not "know" a fact? We present a setting where LMs often generate hallucinations that they immediately recognize as wrong when presented in isolation. Specifically, after an LM answers a question incorrectly, it usually justifies that answer by making incorrect assertions that it separately acknowledges as incorrect (Figure 1).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: GPT-4 mistakenly claims that 9677 is not prime, followed by an incorrect explanation that $13 \times$ $745=9677$. We refer to this factorization as a snowballed hallucination, as GPT-4 appears to "know" that 13 is not a factor of 9677 when asked separately. Thus, hallucinations are not necessarily reflective of a model's lack of "knowledge," but may be induced by an earlier hallucination for consistency.</p>
<p>To study this behavior empirically, we automatically construct three question-answering (QA) datasets. These datasets span different domains: determining whether a number is prime, whether there is a U.S. senator satisfying two given constraints, and whether two cities are connected given a set of flights between cities. Empirically, we find that ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023) commit to an answer within the first token (Yes/No) over $95 \%$ of the time; these answers are often incorrect, and then followed by an incorrect explanation. Yet, when presented with the incorrect explanation alone, we find that the LM is likely able to recognize it as incorrect.</p>
<p>We refer to this phenomenon as hallucination snowballing. We hypothesize that LMs produce snowballed hallucinations for consistency with earlier hallucinations (rather than due to a "knowledge gap" in the model), as they recognize the snow-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Original Question</th>
<th style="text-align: center;">Verification Question</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Primality Testing</td>
<td style="text-align: center;">User: Is 10733 a prime number? <br> GPT-4: No... It can be factored into $3 \times 3577$.</td>
<td style="text-align: center;">User: Is 10733 divisible by 3? Answer with either Yes or No. <br> GPT-4: No</td>
</tr>
<tr>
<td style="text-align: center;">Senator Search</td>
<td style="text-align: center;">User: Was there ever a US senator that represented the state of New Hampshire and whose alma mater was the University of Pennsylvania? <br> GPT-4: Yes... His name was John P. Hale</td>
<td style="text-align: center;">User: Was John P. Hale's alma mater University of Pennsylvania? <br> GPT-4: No... [it] was Bowdoin</td>
</tr>
<tr>
<td style="text-align: center;">Graph Connectivity</td>
<td style="text-align: center;">User: Current flight information (the following flights are one-way only, and all the flights available are included below): <br> There is a flight from city F to city K <br> There is a flight from city H to city A <br> [... 10 other rules cut for space ...] <br> Question: Is there a series of flights that goes from city B to city E? <br> GPT-4: Yes... the route is as follows: ... City K to City G...</td>
<td style="text-align: center;">User: [...flight information given in the context...] <br> Based on the above flight information, is City K to City G a valid flight? <br> GPT-4: No, based on the above flight information, there is no direct flight from City K to City G.</td>
</tr>
</tbody>
</table>
<p>Table 1: Original Question: An example from each dataset and GPT-4's (incorrect) response, with the snowballed hallucination underlined. Verification Question: The corresponding query used to probe whether GPT-4 recognizes its claim is incorrect, with its refutation of the hallucination underlined.
balled hallucination is incorrect when presented in isolation (i.e., in a separate interaction session).</p>
<p>While prompting strategies that encourage the LM to reason before stating an answer improve accuracy on the task, our work points to the broader issue that conditioning on faulty context leads LMs to produce extremely simple mistakes that they wouldn't otherwise make. Indeed, when prompting with "Let's think step by step" (Kojima et al., 2023), snowballed hallucinations still occur in $95 \%$ of cases where the model fails to answer correctly. We observe that sometimes even when "Let's think step by step" does lead to the right answer, it uses invalid reasoning chains.</p>
<p>In this paper, we demonstrate the phenomenon of hallucination snowballing by leveraging recent LMs' tendency to state and justify their answers. Rather than over-committing to its previously generated context, we believe that LMs should acknowledge their initial mistake, and then revise their answer. We have indeed observed GPT-4 doing this in a limited number of cases; amplifying this behavior would be beneficial, as well as developing new methods in which LMs can backtrack.</p>
<h2>2 Why do we expect hallucination snowballing?</h2>
<p>In this section, we explain why we hypothesize that LMs are susceptible to hallucination snowballing. We predict that snowballing will occur on questions
with two key properties:</p>
<ol>
<li>Initial committal: The prompt leads the LM to first state an answer (before outputting the explanation). This applies to many yes/no questions.</li>
<li>Inherently sequential: Transformers cannot find the answer within one timestep because of their limited reasoning abilities within one timestep.
We now discuss how these properties may lead to snowballed hallucination.</li>
</ol>
<p>Initial committal. In English and many other languages, speakers often say the final Yes/No answers to questions before explaining their answer. We therefore hypothesize that LMs and especially instruction-tuned LMs (Wei et al., 2021; Sanh et al., 2021; Ouyang et al., 2022; Wang et al., 2022) will reflect this answer format where the answer comes before the explanation. Indeed, on our datasets (presented in $\S 3.1$ ), we observe that GPT-4 and ChatGPT immediately commit to an answer to the question: the first token is Yes or No $95.67 \%$ and $98.40 \%$ of the time for GPT-4 and ChatGPT respectively. In the remaining cases, the model often commits to an answer within the first few tokens of the response (e.g., "There is no record of a U.S. Senator..."). Crucially, once the LM generates Yes or No, that token remains in the context, and coherence would require commitment to that choice through the subsequent justification. Thus, the model pro-</p>
<p>duces an answer to a complex question in a single timestep, and it then continues by generating an explanation for that answer, which inevitably will be incorrect.</p>
<p>Inherently sequential. Furthermore, transformers cannot solve inherently sequential reasoning problems like primality testing or graph connectivity within a single timestep, ${ }^{2}$ as documented in recent theoretical results (Merrill and Sabharwal, 2023). ${ }^{3}$ Our graph connectivity and primality datasets are concrete instantiations of these problems. Because the transformer must use one step to answer a question that requires multiple timesteps to answer correctly, it will necessarily sometimes commit to an incorrect answer. We hypothesize that this leads the LM to hallucinate supporting incorrect facts that it otherwise would not generate.</p>
<h2>3 Experiments</h2>
<p>We design three QA datasets with the properties described in $\S 2$ to probe hallucination snowballing, and evaluate ChatGPT and GPT-4. We first check whether the LM returns the correct answer to the given question, and we show that when the model returns the wrong answer, it frequently provides an incorrect explanation for that wrong answer. We automatically extract the incorrect claim in the explanation and ask the same LM to check whether its claim is correct. See Table 1 for a representative example from each dataset.</p>
<h3>3.1 Datasets</h3>
<p>We design three QA datasets, each containing 500 yes/no questions that we expect are not answerable by transformers in one timestep. To aid evaluation, the questions are designed so that an incorrect answer would be justified with easily verifiable claims.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>For each dataset, we fix one specific label for all examples, so that if the model chooses the incorrect answer (e.g., that 9677 is not prime), it would produce a specific claim to support it (e.g., an incorrect factorization). This enables us to systematically examine model-written justifications for incorrect answers.</p>
<p>Primality testing For this dataset, we query the primality of 500 randomly chosen primes between 1,000 and 20,000; the correct answer is always Yes. When the model answers incorrectly, we expect it to justify its answer with an incorrect factorization.</p>
<p>Senator search This dataset consists of 500 questions of the form "Was there ever a US senator that represented the state of $x$ and whose alma mater was $y$ ?" where $x$ is a U.S. state and $y$ is a U.S. college. For these questions, the correct answer is always No. When the model answers incorrectly, we expect it to falsely claim that a particular senator both represented $x$ and attended $y$.</p>
<p>To create the dataset we consider all U.S. states and a manually constructed list of twelve popular U.S. colleges (see $\S \mathrm{A}$ for the full list); for each possible pair, we generate a question following the template, and manually remove pairs where the answer is Yes.</p>
<p>Graph connectivity For each of the 500 questions in this dataset, we present 12 flights among 14 cities, and ask if there is a sequence of flights from a particular city to another. The problem always corresponds to the same underlying directed graph structure (see $\S$ A.1), where flights are edges and cities are nodes. For each instance in the dataset, we randomly assign letters from the English alphabet to name the nodes. To formulate the query, we sample a source city $s$ and destination city $t$ in different subgraphs, with the additional constraint that $s$ corresponds to a source node, and $t$ a leaf node, so that 1 -step heuristics cannot be used to solve the problem.</p>
<p>We formulate the problem as a flight-finding question in natural language so that it sounds more natural: in the prompt, we list the twelve flights ("There is a flight from city F to city K; there is a flight from city $G$ to city $N, \ldots$...), followed by the question "Is there a series of flights... from s to $t$ ?". Note the correct answer is always No. When the model answers incorrectly, we expect it to justify its answer with a flight that does not exist.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Percentage of hallucination and percentage of snowballed hallucination (both calculated with respect to the entire dataset) for ChatGPT and GPT-4. The precise numbers for this plot are available in Table 6 and Table 7 in the Appendix.</p>
<h3>3.2 Inference Setup</h3>
<p><strong>Language models.</strong> We run all experiments on ChatGPT (gpt-3.5-turbo) and GPT-4 with greedy decoding.</p>
<p>Our experiments are <em>zero-shot</em> (i.e., we do not show the model any example QA pairs in the prompt). We focus on the model behavior under the direct prompt (see §A for full examples), which is the most common way users interact with LMs. See §4 for experiments with the zero-shot chain-of-thought style prompting method.</p>
<p>For each dataset, we perform a two-stage evaluation. First, we evaluate the model's accuracy (i.e., how many of the questions it answers correctly). When either models is <em>incorrect</em>, empirically it <em>always</em> generates a justification. In the second stage, we assess whether the model can identify the incorrect step in the explanation.</p>
<p>For a given question, we evaluate the model's response by examining whether the output begins with either Yes or No. In cases where the response does not fall into these categories, we manually determine the answer conveyed by the model.</p>
<h3>3.3 LM Recognition of Snowballed Hallucinations</h3>
<p>We probe whether LMs recognize their snowballed hallucinations by verifying the model's incorrect claims in the output against the model itself. Note that our recognition procedure relies on heuristics gained from manual examination of the model output, and these heuristics might not work on other models (e.g., a different model might not provide factors when supporting the claim that a number is not prime).</p>
<p><strong>Graph Connectivity</strong> For each sample where the model thinks there is a series of connecting flights (where answer starts with Yes), we manually extract the list of flights from the model's output and identify the invalid or discontinuous flights.</p>
<p>We then, in a new session, ask the model to verify whether the extracted flights are valid based on the flight information, and if consecutive flights are indeed connected. We manually assess the verification output to check if the model correctly detects the error. See Appendix Table 3 for how we prompt the model and an example of successful verification.</p>
<p><strong>Primality Testing</strong> For each sample where the model answers that the number is not prime, we extract the factors the model uses to justify it. The extraction is done by putting the output in the context and asking <em>"What are the factors proposed in the above text? List them out."</em> We use ChatGPT for extraction with one-shot demonstration (for its fast inference speed); we manually checked 30 examples and found that it can always extract the correct factors.</p>
<p>We then, in a new session, ask the model to verify each extracted factor individually. See Appendix Table 4 for an example of successful verification.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Error rate and snowballed hallucination rate (hatch pattern) for ChatGPT and GPT-4, when using the original prompt versus <em>"Let's think step by step"</em>. See Appendix Table 8 and Table 9 for the exact numbers.</p>
<p><strong>Senator Search</strong> For each sample where the model thinks there is such a senator, we extract the name of the senator the model uses to justify the existence, by putting the output in the context and asking <em>"What is the senator mentioned in the above text? Just give the name"</em>. Again, we use ChatGPT and manually observed perfect extraction on 30 examples.</p>
<p>We then, in a new session, ask the model if that senator's alma mater is the college in the question and has represented the state in the question. See Appendix Table 5 for an example of successful detection.</p>
<h3>3.4 Results</h3>
<p><strong>Question-answering accuracy</strong> Figure 2 shows that both ChatGPT and GPT-4 experience very low accuracy across the board. With the exception of ChatGPT on the <strong>Senator Search</strong> dataset, all models achieve less than 50% accuracy. (See Appendix Table 6 for a breakdown of the error rate by dataset.) We observe that GPT-4 performs worse than ChatGPT across all datasets despite popularly being considered superior to ChatGPT (OpenAI, 2023). While ChatGPT has an average accuracy of 39.87%, GPT-4 has only 16.6%.</p>
<p><strong>Hallucination detection</strong> Here, we check whether the model can identify that the incorrect claim is wrong when it is presented alone. As shown in Figure 2, ChatGPT detects 67.37% of incorrect claims in explanations (i.e., snowballed hallucinations), and GPT-4 detects 87.03%. Notice that when the model fails the verification (an example in Appendix Table 12), we do not consider it a snowballed hallucination.</p>
<p>Overall, we find that ChatGPT and GPT-4 are both extremely susceptible to hallucination snowballing, leading to extremely simple mistakes.</p>
<h2>4 Can we prevent snowball hallucinations?</h2>
<p>We hypothesize that hallucination snowballing occurs because LMs are trained to model continuations consistent with their current context (the given prompt and prior outputs). Although a fix to the fundamental problem might require more than just inference-time modification, in this section we study the effectiveness of two inference strategies in alleviating hallucination snowballing: prompting (§4.1) and decoding or training methods (§4.2).</p>
<h3>4.1 Engineering Better Prompts</h3>
<p>In this section, we examine the effectiveness of better prompts on preventing snowballed hallucination by using a different zero-shot prompt that encourages the model to generate the reasoning chain before the answer. Since the outputs generated under these prompts are less structured, we manually inspect them to determine correctness and the presence of snowballed hallucinations.</p>
<p>For each task, we append <em>"Let's think step-by-step"</em> at the end of the original question (shown in Table 1). As shown in Figure 3, the model can solve the <strong>Senator Search</strong> task perfectly, achieve ≤10% error rate on <strong>Primality Testing</strong>, and ≤30% on <strong>Graph Connectivity</strong>. Despite the large improve</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Error rate and snowballed hallucination rate (hatch pattern) from ChatGPT and GPT-4, when using different values for temperature at decoding-time. See Appendix Table 10 and Table 11 for the exact numbers.</p>
<p>ment in accuracy, we identify a potential issue: the model sometimes hallucinate while outputting the reasoning chain, which causes snowballed hallucination in future steps. For example, in the below output,</p>
<blockquote>
<p><em>[....previous steps omitted]</em></p>
<p><em>Step 3: From city E, we have three options: a flight to city N, a flight to city B, or a flight to city C.</em></p>
<p><em>Step 4: The only option that could potentially lead us to city M is the flight from city E to city C.</em></p>
<p><em>[....rest of the output omitted]</em></p>
</blockquote>
<p>ChatGPT incorrectly states that there are three options in the step 3 (there are only two), inducing the snowballed hallucination <em>"or a flight to city C"</em> (ChatGPT can verify that E → C is not a valid flight in a separate session). As shown in Figure 3, GPT-4 still has a high overall snowballed hallucination rate at 94.90% averaged across tasks, and ChatGPT also obtains a similarly high snowballed hallucination rate.</p>
<p>Finally, while our experiments have focused on simple multi-step problems that are suitable for breaking down step-by-step, we hypothesize that hallucination snowballing appears in open-ended text generation more broadly, where one mistake in the generation triggers more (Arora et al., 2022). In these cases, better prompting would neither be able to anticipate nor fix these mistakes.</p>
<h3>4.2 Algorithmic Corrections</h3>
<p><strong>Increasing the temperature</strong> During decoding, the temperature <em>t</em> controls the sharpness of the output distribution, with higher <em>t</em> spreading probability mass away from the model's most likely prediction for each next word. Our experiments in §3 used greedy decoding, which is equivalent to <em>t</em> = 0. At <em>t</em> = 0.6 and <em>t</em> = 0.9, both error rates and snowballed hallucination rate remain similarly high, in both GPT-4 and ChatGPT (Figure 4).</p>
<p><strong>Top-k and nucleus sampling</strong> Using sampling methods such as top-<em>k</em> sampling or nucleus sampling (Holtzman et al., 2020) would not help since they only <em>narrow</em> the range of tokens to be considered, and thus can only <em>increase</em> the probability that the model will immediately commit to an answer.</p>
<p><strong>Beam search</strong> The argument for hallucination snowballs in §2 relies on the fact that, once a model generates some tokens committing to an answer, they remain in the context and influence later generations. One potential way around this is <em>beam search</em>, i.e., maintaining a beam of high-probability sequences at each timestep rather than a single sequence. In principle, if some sequences in the beam after the initial token do not commit to an answer (or commit to the right answer), their continuations may eventually have higher probability than those that initially commit incorrectly and later produce incorrect reasoning as a result. If so, beam search would solve the snowball hallucination problem. Unfortunately, we cannot test the effect of beam search on hallucination snowballs because the Ope-</p>
<p>nAI API does not support beam search.
Learning strategies A more general way to further reduce snowballing might be to change aspects of the pretraining or instruction tuning phases. In particular, a greater emphasis on having the model produce a reasoning chain before generating an answer could be a good way to accommodate its computational limitations and avoid committing to wrong answers that force hallucinations.</p>
<p>In addition, we hypothesize that finetuning on data with backtracking might improve a model's performance on the tasks we present. This could be accomplished by, for example, giving a question, followed by a wrong solution, and then issuing a phrase like "Sorry, that was incorrect" before giving the correct solution. This solution is related to the "Review your previous answer and find problems with your answer." prompt from Kim et al. (2023).</p>
<h2>5 Related Work</h2>
<p>Hallucinations Hallucination in text generation is a well-studied problem (Rohrbach et al., 2018; Maynez et al., 2020; Raunak et al., 2021, i.a.) that has recently become more prominent due to ChatGPT's tendency to produce plausible-sounding falsehoods. Hallucinations are often attributed to knowledge gaps in LMs (Zheng et al., 2023), and several works have shown the promise of using retrieval over knowledge bases to mitigate them (Lewis et al., 2020; Shuster et al., 2021; Peng et al., 2023). Our work demonstrates hallucination can be induced from context, thus motivating further mitigation techniques.</p>
<p>Hallucination snowballing is likely the result of exposure bias: LMs were only exposed to gold history during training, but during inference, conditions on possibly erroneous previous predictions. Prior work linked this to compounding hallucinations in machine translation (Wang and Sennrich, 2020) and open-ended text generation (Arora et al., 2022). We go beyond demonstrating error propagation by showing that the propagated errors (which we call snowballed hallucinations) are recognized by the LM itself.</p>
<p>Our observations are related to previous findings that LMs hallucinate when given questions that contain false presuppositions (e.g., "Which linguist invented the lightbulb?"; Kim et al., 2021, 2022) or that are otherwise misleading (e.g., "Who really caused 9/11?"; Lin et al., 2022), in that faulty
context misguides the LM. However, our work differs in that our questions are not intentionally misleading, showing that this failure mode may be triggered even on innocent information-seeking queries to the LM.</p>
<p>LM (in)consistency Our work adds to a growing body of work demonstrating the extent to which LMs are inconsistent across different prompts on the same issue. For instance, allowing an LM to generate intermediate steps (Nye et al., 2021; Wei et al., 2022; Press et al., 2022) enables it to reach a different answer than it otherwise would. Other work has shown that simply prepending "Professor Smith was given the following instructions" to a prompt can improve performance, despite providing no valuable information about the problem itself (Lin et al., 2022).</p>
<h2>6 Conclusion</h2>
<p>We define the phenomenon of hallucination snowballing and demonstrate its prevalence in generations from state-of-the-art models, leading to hallucinations on simple facts that wouldn't otherwise occur. Our findings point to the risk of training language models that prioritize fluency and coherence indiscriminatively at the expense of factuality, and we encourage future work to study remedial actions at all levels of model development.</p>
<h2>Limitations</h2>
<p>We focus on hallucination snowballing in the context of question answering in English, and we do not explore it on other tasks, such as summarization or code generation.</p>
<p>In addition, we only conduct experiments on two proprietary models, namely ChatGPT and GPT-4, due to their state-of-the-art performance on many benchmarks (OpenAI, 2023). Due to the limitations of the APIs for these models, we do not have access to the probability distributions they output and do not have the ability to finetune them. This restricts our ability to explore potential mitigation strategies. Having access to the output distributions would allow us to investigate mitigating the snowballing hallucination issue using alternative sampling methods such as beam search. Having the ability to finetune the model would allow us to explore whether instruction tuning with different annotations could lead to better handling of the questions we use to instigate hallucination snowballing.</p>
<h2>Acknowledgements</h2>
<p>We thank Sofia Serrano, Yizhong Wang, Yanai Elazar, Michael Hu and Richard Yuanzhe Pang for their valuable feedback and fruitful discussions. While writing this paper, Ofir Press was a visitor at New York University's Center for Data Science, hosted by Kyunghyun Cho.</p>
<h2>References</h2>
<p>Manindra Agrawal, Neeraj Kayal, and Nitin Saxena. 2004. Primes is in p. Annals of Mathematics, 160:781-793. Godel Prize, Fulkerson Prize.</p>
<p>Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Cheung. 2022. Why exposure bias matters: An imitation learning perspective of error accumulation in language generation. In Findings of the Association for Computational Linguistics: ACL 2022, pages 700-710, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations.</p>
<p>Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. Language models can solve computer tasks.</p>
<p>Najoung Kim, Phu Mon Htut, Sam Bowman, and Jackson Petty. 2022. (qa)2: Question answering with questionable assumptions. ArXiv, abs/2212.10003.</p>
<p>Najoung Kim, Ellie Pavlick, Burcu Karagol Ayan, and Deepak Ramachandran. 2021. Which linguist invented the lightbulb? presupposition verification for question-answering. In Annual Meeting of the Association for Computational Linguistics.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large language models are zero-shot reasoners.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeintensive nlp tasks. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20, Red Hook, NY, USA. Curran Associates Inc.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214-3252, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Nelson F. Liu, Tianyi Zhang, and Percy Liang. 2023. Evaluating verifiability in generative search engines.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, Online. Association for Computational Linguistics.</p>
<p>William Merrill and Ashish Sabharwal. 2023. The parallelism tradeoff: Limitations of log-precision transformers.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show your work: Scratchpads for intermediate computation with language models.</p>
<p>OpenAI. 2022. Introducing chatgpt.
OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.</p>
<p>Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models.</p>
<p>Vikas Raunak, Arul Menezes, and Marcin JunczysDowmunt. 2021. The curious case of hallucinations in neural machine translation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1172-1183, Online. Association for Computational Linguistics.</p>
<p>Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018. Object hallucination in image captioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4035-4045, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin</p>
<p>Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2021. Multitask prompted training enables zero-shot task generalization.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3784-3803, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Chaojun Wang and Rico Sennrich. 2020. On exposure bias, hallucination and domain shift in neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3544-3552, Online. Association for Computational Linguistics.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2021. Finetuned language models are zero-shot learners.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.</p>
<p>Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023. Why does chatgpt fall short in answering questions faithfully?</p>
<h2>A Dataset Details</h2>
<h2>A. 1 Graph Connectivity</h2>
<p>In this dataset, the list of flights can be represented by a directed graph. We generated the flight information to ensure all the graphs share a specific connection pattern, with the node names randomly chosen among the 26 letters in the English alphabet. For an illustration of the underlying graph structure, see Figure 5.</p>
<h2>A. 2 Senator search</h2>
<p>The twelve colleges used in the datasets are: MIT, University of Chicago, Johns Hopkins University, California Institute of Technology, Duke University, Northwestern University, Dartmouth College, Brown University, Vanderbilt University, Rice University, University of Washington. We constructed this list by taking a list of top universities in the U.S. and excluding from it universities which also appeared on The U.S. News \&amp; World Report's list of Top 10 Colleges for Members of Congress.</p>
<h2>B Additional Results</h2>
<p>We provide the detail breakdown of the questionanswering accuracy in Table 6 and the hallucination detection accuracy in Table 7.</p>
<p>Graph Connectivity: Current flight information (the following flights are one-way only, and all the flights available are included below):
There is a flight from city F to city K
There is a flight from city H to city A
There is a flight from city B to city F
There is a flight from city N to city H
There is a flight from city B to city M
There is a flight from city N to city G
There is a flight from city M to city J
There is a flight from city G to city L
There is a flight from city H to city E
There is a flight from city G to city C
There is a flight from city M to city I
There is a flight from city F to city D</p>
<p>Question: Is there a series of flights that goes from city B to city E?</p>
<p>Table 2: Input example for the Graph Connectivity task. Figure 5 describes the underlying graph structure for this example. We use the same graph structure in all examples, and vary the source and target cities and the labels of the nodes. The cities are literally called "City A", "City B", and so on.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The underlying graph structure corresponds to example in Table 2.</p>
<p>Input: Current flight information (the following flights are one-way only, and all the flights available are included below):
There is a flight from city $G$ to city $B$
There is a flight from city $H$ to city $K$
There is a flight from city $L$ to city $M$
There is a flight from city $F$ to city $H$
There is a flight from city $G$ to city $J$
There is a flight from city $B$ to city $I$
There is a flight from city $L$ to city $A$
There is a flight from city $H$ to city $N$
There is a flight from city $B$ to city $D$
There is a flight from city J to city C
Question: Is there a series of flights that goes from city F to city I ?
Output: Yes, there is a series of flights that goes from city F to city I. The route is as follows:</p>
<ol>
<li>City F to City H</li>
<li>City H to City K</li>
<li>City K to City G</li>
<li>City G to City B</li>
<li>City B to City I</li>
</ol>
<p>Verification Question: Current flight information (the following flights are one-way only, and all the flights available are included below):
There is a flight from city $G$ to city $B$
There is a flight from city $H$ to city $K$
[... 10 other rules cut for space ...]
Based on the above flight information, is City K to City G a valid flight?
Verification Answer:
No, based on the above flight information, there is no direct flight from City K to City G.
Table 3: Graph Connectivity dataset verification example, with GPT-4.</p>
<p>Input: Is 9791 a prime number?
Output: No, 9791 is not a prime number. It can be factored into $13 \times 751$.
Verification Q1: Is 9791 divisible by 13? Answer with either Yes or No.
Verification A1: No
Verification Q2: Is 9791 divisible by 751? Answer with either Yes or No.
Verification A2: No
Table 4: Primality Testing dataset verification example, with GPT-4.</p>
<p>Input: Was there ever a US senator that represented the state of New Hampshire and whose alma mater was University of Pennsylvania?
Output: Yes, there was a U.S. Senator who represented the state of New Hampshire and whose alma mater was the University of Pennsylvania. His name is John P. Hale, who served as a Senator from 1847 to 1853 and again from 1855 to 1865. He graduated from Bowdoin College in 1827 and later attended the University of Pennsylvania Law School.
Verification Q1: Has John P. Hale represented the state of New Hampshire as US senator?
Verification A1: Yes, John P. Hale represented the state of New Hampshire as a U.S. Senator. He served in the Senate from March 4, 1847, to March 3, 1853, and again from March 4, 1855, to March 3, 1865 .</p>
<p>Verification Q2: Was John P. Hale's alma mater University of Pennsylvania?
Verification A2: No, John P. Hale's alma mater was Bowdoin College in Brunswick, Maine.
Table 5: Senator Alma Mater dataset verification example, with GPT-4.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Task</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: left;">Graph Connectivity</td>
<td style="text-align: left;">Primality Testing</td>
<td style="text-align: left;">Senator Search</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">$410 / 500(82.0 \%)$</td>
<td style="text-align: left;">$339 / 500(67.8 \%)$</td>
<td style="text-align: left;">$153 / 500(30.6 \%)$</td>
<td style="text-align: left;">$60.13 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">$442 / 500(88.4 \%)$</td>
<td style="text-align: left;">$374 / 500(74.8 \%)$</td>
<td style="text-align: left;">$435 / 500(87.0 \%)$</td>
<td style="text-align: left;">$83.40 \%$</td>
</tr>
</tbody>
</table>
<p>Table 6: Number of mistakes out of the number of samples, the percentage here is the error rate.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Task</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: left;">Graph Connectivity</td>
<td style="text-align: left;">Primality Testing</td>
<td style="text-align: left;">Senator Search</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">$396 / 410(96.6 \%)$</td>
<td style="text-align: left;">$125 / 339(36.9 \%)$</td>
<td style="text-align: left;">$98 / 153(68.6 \%)$</td>
<td style="text-align: left;">$67.37 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">$417 / 442(94.3 \%)$</td>
<td style="text-align: left;">$346 / 374(92.5 \%)$</td>
<td style="text-align: left;">$323 / 435(74.3 \%)$</td>
<td style="text-align: left;">$87.03 \%$</td>
</tr>
</tbody>
</table>
<p>Table 7: Number of snowballed hallucination out of number of hallucination generated in the original output.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Task</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: left;">Graph Connectivity</td>
<td style="text-align: left;">Primality Testing</td>
<td style="text-align: left;">Senator Search</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">$139 / 500(27.8 \%)$</td>
<td style="text-align: left;">$2 / 500(0.4 \%)$</td>
<td style="text-align: left;">$0 / 500(0.0 \%)$</td>
<td style="text-align: left;">$9.40 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">$21 / 500(4.2 \%)$</td>
<td style="text-align: left;">$37 / 500(7.4 \%)$</td>
<td style="text-align: left;">$0 / 500(0.0 \%)$</td>
<td style="text-align: left;">$3.87 \%$</td>
</tr>
</tbody>
</table>
<p>Table 8: Number of mistakes out of the number of samples, the percentage here is the error rate, using "Let's think step by step" prompt.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Task</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: left;">Graph Connectivity</td>
<td style="text-align: left;">Primality Testing</td>
<td style="text-align: left;">Senator Search</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">$123 / 139(88.5 \%)$</td>
<td style="text-align: left;">$0 / 2(0 \%)$</td>
<td style="text-align: left;">$0 / 0(\mathrm{~N} / \mathrm{A})$</td>
<td style="text-align: left;">$44.25 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">$20 / 21(95.2 \%)$</td>
<td style="text-align: left;">$35 / 37(94.6 \%)$</td>
<td style="text-align: left;">$0 / 0(\mathrm{~N} / \mathrm{A})$</td>
<td style="text-align: left;">$94.90 \%$</td>
</tr>
</tbody>
</table>
<p>Table 9: Number of snowballed hallucination out of number of hallucination generated in the original output, using "Let's think step by step" prompt.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Graph</th>
<th>Prime</th>
<th>Senator</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td>ChatGPT $(t=0.0)$</td>
<td>$410 / 500(82.0 \%)$</td>
<td>$339 / 500(67.8 \%)$</td>
<td>$153 / 500(30.6 \%)$</td>
<td>$60.13 \%$</td>
</tr>
<tr>
<td>ChatGPT $(t=0.6)$</td>
<td>$407 / 500(81.4 \%)$</td>
<td>$310 / 500(63.2 \%)$</td>
<td>$155 / 500(31.0 \%)$</td>
<td>$58.53 \%$</td>
</tr>
<tr>
<td>ChatGPT $(t=0.9)$</td>
<td>$403 / 500(80.6 \%)$</td>
<td>$312 / 500(62.4 \%)$</td>
<td>$163 / 500(32.6 \%)$</td>
<td>$58.53 \%$</td>
</tr>
<tr>
<td>GPT-4 $(t=0.0)$</td>
<td>$442 / 500(88.4 \%)$</td>
<td>$374 / 500(74.8 \%)$</td>
<td>$435 / 500(87.0 \%)$</td>
<td>$83.40 \%$</td>
</tr>
<tr>
<td>GPT-4 $(t=0.6)$</td>
<td>$438 / 500(87.6 \%)$</td>
<td>$365 / 500(75.4 \%)$</td>
<td>$423 / 500(84.6 \%)$</td>
<td>$82.53 \%$</td>
</tr>
<tr>
<td>GPT-4 $(t=0.9)$</td>
<td>$437 / 500(87.4 \%)$</td>
<td>$377 / 500(73.0 \%)$</td>
<td>$423 / 500(84.6 \%)$</td>
<td>$81.67 \%$</td>
</tr>
</tbody>
</table>
<p>Table 10: Number of mistakes out of the number of samples, the percentage here is the error rate, with different temperature setups</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Graph</th>
<th style="text-align: left;">Prime</th>
<th style="text-align: left;">Senator</th>
<th style="text-align: left;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT $(t=0.0)$</td>
<td style="text-align: left;">$396 / 410(96.6 \%)$</td>
<td style="text-align: left;">$125 / 339(36.9 \%)$</td>
<td style="text-align: left;">$98 / 153(68.6 \%)$</td>
<td style="text-align: left;">$67.37 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT $(t=0.6)$</td>
<td style="text-align: left;">$396 / 407(97.3 \%)$</td>
<td style="text-align: left;">$113 / 310(36.5 \%)$</td>
<td style="text-align: left;">$103 / 155(66.5 \%)$</td>
<td style="text-align: left;">$66.77 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT $(t=0.9)$</td>
<td style="text-align: left;">$399 / 402(99.3) \%$</td>
<td style="text-align: left;">$116 / 312(37.2 \%)$</td>
<td style="text-align: left;">$104 / 163(63.8 \%)$</td>
<td style="text-align: left;">$66.77 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 $(t=0.0)$</td>
<td style="text-align: left;">$417 / 442(94.3 \%)$</td>
<td style="text-align: left;">$346 / 374(92.5 \%)$</td>
<td style="text-align: left;">$323 / 435(74.3 \%)$</td>
<td style="text-align: left;">$87.03 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 $(t=0.6)$</td>
<td style="text-align: left;">$402 / 438(91.8 \%)$</td>
<td style="text-align: left;">$324 / 365(88.8 \%)$</td>
<td style="text-align: left;">$329 / 423(77.8 \%)$</td>
<td style="text-align: left;">$86.13 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 $(t=0.9)$</td>
<td style="text-align: left;">$398 / 437(91.1 \%)$</td>
<td style="text-align: left;">$331 / 377(87.8 \%)$</td>
<td style="text-align: left;">$320 / 423(75.7 \%)$</td>
<td style="text-align: left;">$84.87 \%$</td>
</tr>
</tbody>
</table>
<p>Table 11: Number of mistakes out of the number of samples, the percentage here is the error rate, with different temperature setups</p>
<p>Input: Was there ever a US senator that represented the state of North Carolina and whose alma mater was Dartmouth College?
Output: Yes, there was a U.S. Senator who represented the state of North Carolina and whose alma mater was Dartmouth College. His name was Willis Smith, and he served as a Senator from 1950 to 1953.</p>
<p>Verification Q1: Has Willis Smith represented the state of North Carolina as a US senator?
Verification A1: Yes, Willis Smith represented the state of North Carolina as a U.S. Senator. He served from 1950 until his death in 1953.</p>
<p>Verification Q2: Was Willis Smith's alma mater Dartmouth College?
Verification A2: Yes, Willis Smith's alma mater was Dartmouth College. He graduated from there in 1917.</p>
<p>Table 12: An example of GPT-4 failing the verification test for Senator Search task. Willis Smith was a senator from North Carolina, but he has never attended Dartmouth College.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Technically, this holds only for inputs above a certain hardness level, i.e., the size of the prime number for primality testing, or the size of the graph for graph connectivity.
${ }^{3}$ Merrill and Sabharwal (2023) show that, with a single generation step, bounded-precision transformers cannot solve any problem outside the complexity class $\mathrm{TC}^{0}$, which corresponds to a highly parallelizable subclass of both L (log-space) and P (polynomial-time). Graph connectivity is an L-complete problem, which means it cannot be in $\mathrm{TC}^{0}$ unless $\mathrm{TC}^{0}=\mathrm{L}$, i.e., all of L can be parallelized to a surprisingly high degree. Primality testing was shown to be in P (Agrawal et al., 2004) but cannot be in $\mathrm{TC}^{0}$ unless it is also in L ; i.e., any $n$ can be factored with $O(\log \log n)$ bits of overhead. In summary, unless standard complexity-theoretic conjectures are false, graph connectivity and primality testing are outside $\mathrm{TC}^{0}$ and thus are too inherentially sequential for transformers to solve in a single generation (cf. Merrill and Sabharwal, 2023).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
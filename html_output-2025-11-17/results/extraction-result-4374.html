<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4374 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4374</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4374</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-226289939</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2011.04843v3.pdf" target="_blank">Multi-document Summarization via Deep Learning Techniques: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Multi-document summarization (MDS) is an effective tool for information aggregation that generates an informative and concise summary from a cluster of topic-related documents. Our survey, the first of its kind, systematically overviews the recent deep learning based MDS models. We propose a novel taxonomy to summarize the design strategies of neural networks and conduct a comprehensive summary of the state-of-the-art. We highlight the differences between various objective functions that are rarely discussed in the existing literature. Finally, we propose several future directions pertaining to this new and exciting field.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4374.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4374.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pre-trained LMs for MDS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pre-trained Language Models applied to Multi-Document Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of approaches that reuse large pre-trained Transformer language models (e.g., BERT, GPT-2/3, T5, PEGASUS) as encoders/decoders or initialization for multi-document summarization (MDS), often fine-tuned or adapted to handle long multi-document inputs or to provide few-shot capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pre-trained Language Models for MDS (generic approach)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The survey describes applying large pre-trained Transformer-based LMs as components in MDS pipelines: replacing low-level token encoders with pre-trained LMs, fine-tuning encoder/decoder parameters for downstream MDS tasks, or using decoder-only conditional language models with plug-in adapters for few-shot settings. Hierarchical architectures are commonly used where a pre-trained LM encodes short blocks (token-level), and higher-level Transformer layers fuse block outputs to encode long multi-document clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>BERT, RoBERTa, GPT-2, GPT-3, XLNet, ALBERT, T5, PEGASUS (as mentioned in the paper); also Longformer, Reformer, Big Bird as long-sequence variants suggested for MDS</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Fine-tuning and hierarchical encoding of document blocks with pre-trained LM token encoders; also few-shot conditioning via plug-in networks; extractive pre-selection (e.g., top-k paragraphs) before LM encoding is often used</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Abstractive generation via LM decoders or decoder-only conditional LMs; hierarchical summarization where block encodings are fused by higher-level layers; graph-informed self-attention in some architectures to incorporate cross-document relations</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>varies by dataset; typical clusters range from a few documents up to 10+ documents per cluster (paper notes top-10 references for WikiSum-style tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>general news, Wikipedia article generation, opinion/review summarization, and suggested for scientific papers (e.g., Multi-XScience, SciSumm domains)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>abstractive summaries (single article/Wikipedia page), extractive selections, or hybrid summaries; can be configured to output structured summaries depending on pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE (ROUGE-1/2/L) primarily; human judgment evaluations; other semantic metrics discussed (BERTScore, MoverScore, SUPERT) — survey reports ROUGE as dominant in MDS evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>traditional summarization models, non-pretrained neural encoders/decoders, and extractive baselines (survey does not report specific numeric comparisons for aggregated pre-trained LM approach)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pre-trained LMs help overcome limited MDS labeled data by transferring general language knowledge; replacing token-level encoders with pre-trained LMs in hierarchical models improves representational quality; however, naive fine-tuning of very large models can lead to overfitting and poor generation in low-data MDS settings, motivating few-shot or adapter-style solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Quadratic memory growth of self-attention when handling long concatenated documents; overfitting when fine-tuning very large LMs on scarce MDS datasets; need to adapt architectures for long sequences (Longformer/BigBird/Reformer suggested); potential hallucination/factual inconsistency not deeply quantified in the survey but noted as concern for abstractive outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Survey reports qualitative scaling trends: larger pre-trained models and targeted pre-training (e.g., PEGASUS for summarization) tend to improve downstream MDS performance, but computational and memory costs increase; hierarchical/blockwise encoding and long-sequence Transformer variants are recommended to scale to larger document clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-document Summarization via Deep Learning Techniques: A Survey', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4374.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4374.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PEGASUS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-based pre-trained encoder-decoder model specifically tailored for abstractive summarization, using gap-sentence generation (GSG) pre-training objective where whole sentences are masked based on importance and the model is trained to generate them.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PEGASUS: Pre-training with Extracted Gapsentences for Abstractive Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PEGASUS (pre-trained summarization LM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PEGASUS is a large pre-trained Transformer encoder-decoder model that uses a gap-sentence generation objective: important sentences are removed from documents during pre-training and the model learns to generate these sentences from the remaining text, providing a pre-training task closely aligned with summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>PEGASUS (Transformer encoder-decoder; model sizes vary in original PEGASUS paper, but the survey does not list sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Pre-training-based representation learning where entire sentences are masked (gap sentences) to teach the model to identify and generate salient sentences; can be fine-tuned on MDS datasets</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Abstractive summarization via the model's decoder that generates missing/gap sentences conditioned on multi-document input (typically after some extractive selection or hierarchical encoding to respect length constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>design intended for single- and multi-document summarization; number of source documents per instance depends on dataset (survey references WikiSum / Multi-News style setups where up to ~10 references may be used)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>general summarization domains including news and Wikipedia-style generation; recommended as promising for MDS abstractive tasks</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>abstractive summaries (sentence-level generation aligning with gap-sentences pre-training)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE variants (ROUGE-1/2/L) typically used for evaluation in summarization tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>standard pre-training objectives (e.g., MLM) and baseline encoder-decoder models; survey states PEGASUS's GSG is more suitable for summarization than random or lead masking but does not give numeric results here</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Masking whole sentences based on importance (GSG) in pre-training is more effective for downstream summarization than random or simple lead-based masking, making PEGASUS particularly suitable for abstractive MDS adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Survey notes computational cost and document-length constraints when applying pre-trained encoder-decoder models to very long multi-document inputs; PEGASUS pre-training is oriented to sentence prediction which may require adaptation for cross-document fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Survey recommends combining PEGASUS with hierarchical encoding or long-document Transformers to scale to larger multi-document inputs; explicit quantitative scaling trends not provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-document Summarization via Deep Learning Techniques: A Survey', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4374.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4374.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WikiSum / Generating Wikipedia</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generating Wikipedia by Summarizing Long Sequences (WikiSum)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model and dataset approach that generates Wikipedia articles by summarizing multiple long reference documents and web search results, using a Transformer-based decoder-only sequence transduction architecture built on top of an extractive selection stage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating Wikipedia by Summarizing Long Sequences</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>WikiSum (Transformer decoder-only MDS pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>WikiSum first gathers multiple source documents (citations for a Wikipedia topic or top-N search results), applies an extractive stage to select salient text (e.g., top-k tokens/paragraphs), and trains a Transformer decoder-only sequence transduction model to generate the target Wikipedia article text, combining extractive results and the reference summary into training sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Transformer decoder-only architecture (survey describes decoder-only Transformer; does not specify a particular pretrained LLM name/size in-text)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>An extractive pre-selection stage ranks/selects top-k tokens/paragraphs (survey mentions selection of top-k tokens as input to decoder-only model); selection reduces input length before generation</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Decoder-only Transformer generates an abstractive article by conditioning on concatenated selected extractive fragments; synthesis fuses information across multiple sources into cohesive long-form text</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>survey describes using the documents cited in a Wikipedia article or top-10 Google search results (i.e., up to ~10 source documents per instance)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Wikipedia-style article generation from web documents; the approach is general and can apply to multi-document summarization across domains</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>long-form abstractive articles (Wikipedia pages)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE metrics and human evaluation for long-form article quality are typical (survey mentions using ROUGE and hierarchical evaluation practices for such tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>flat encoder-decoder models and other MDS baselines (survey notes decoder-only approach chosen because encoder-decoder struggles with very long inputs), but specific numeric baselines not provided in-survey</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using an extractive pre-selection stage followed by a decoder-only Transformer is an effective architecture choice for very long multi-source inputs like Wikipedia generation; the extractive stage reduces input length and helps the decoder focus on salient content.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Long input length remains a core challenge; naive encoder-decoder approaches struggle with very long concatenated sources; selection quality heavily affects generated content; potential factual inconsistency/hallucination risks for abstractive long-form generation acknowledged but not quantified in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Survey indicates scaling to long-source settings requires extractive filtering and/or hierarchical architectures; no quantitative scaling curves provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-document Summarization via Deep Learning Techniques: A Survey', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4374.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4374.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FewSum / Few-shot conditional LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot Transformer conditional language model with plug-in network (FewSum-style approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A few-shot MDS approach that conditions a large Transformer conditional language model with a small plug-in network to enable extractive and abstractive summarization without extensive fine-tuning on MDS datasets, reducing rapid overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Few-shot conditional Transformer + plug-in network (Brazinskas et al. style)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The approach uses a large conditional Transformer language model and attaches a lightweight plug-in network to adapt the model to multi-document summarization in few-shot settings; the plug-in acts as an adapter to avoid full fine-tuning of the large LM and mitigate rapid overfitting and poor generation caused by naive fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Transformer conditional language models (survey references a conditional LM used by Brazinskas et al.; specific family/size not enumerated in the survey)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Few-shot conditioning using the plug-in network to bias generation toward salient content; may incorporate extractive pre-selection as a preprocessing step (survey notes method is for extractive and abstractive MDS)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>The conditional LM synthesizes summaries in a few-shot manner, with the plug-in guiding extraction/abstraction and preventing overfitting; used for both extractive and abstractive outputs</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>designed to work with few-shot setups where only a small number of annotated examples are available (survey does not give absolute paper-count ranges)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>opinion/review summarization and general MDS; method aimed to be domain-agnostic but demonstrated on opinion summarization tasks in referenced work</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>extractive or abstractive summaries depending on scoring/decoding setup</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE and human evaluations commonly used in cited few-shot MDS work (survey cites ROUGE as primary metric across MDS tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>naive fine-tuning of large LMs and standard extractive/abstractive MDS models; survey notes approach was proposed to overcome overfitting and poor generation from naive fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adapter/plug-in style few-shot adaptation of large conditional LMs can reduce overfitting and stabilize generation for MDS when labeled data are scarce; few-shot strategies are a promising direction for large-parameter models in MDS.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Survey notes few-shot methods still face generation quality and factuality challenges; lacks extensive quantitative evaluation in MDS settings within the survey; model choice and adapter design influence results heavily.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Survey qualitatively reports that few-shot plug-in methods help avoid rapid overfitting when scaling model parameter counts, but provides no quantitative scaling curves.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-document Summarization via Deep Learning Techniques: A Survey', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PEGASUS: Pre-training with Extracted Gapsentences for Abstractive Summarization <em>(Rating: 2)</em></li>
                <li>Generating Wikipedia by Summarizing Long Sequences <em>(Rating: 2)</em></li>
                <li>Few-Shot Learning for Opinion Summarization <em>(Rating: 2)</em></li>
                <li>Longformer: The Long-document Transformer <em>(Rating: 2)</em></li>
                <li>Big Bird: Transformers for Longer Sequences <em>(Rating: 2)</em></li>
                <li>Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles <em>(Rating: 2)</em></li>
                <li>Scisummnet: A Large Annotated Corpus and Content-impact Models for Scientific Paper Summarization with Citation Networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4374",
    "paper_id": "paper-226289939",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "Pre-trained LMs for MDS",
            "name_full": "Pre-trained Language Models applied to Multi-Document Summarization",
            "brief_description": "A class of approaches that reuse large pre-trained Transformer language models (e.g., BERT, GPT-2/3, T5, PEGASUS) as encoders/decoders or initialization for multi-document summarization (MDS), often fine-tuned or adapted to handle long multi-document inputs or to provide few-shot capabilities.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Pre-trained Language Models for MDS (generic approach)",
            "system_description": "The survey describes applying large pre-trained Transformer-based LMs as components in MDS pipelines: replacing low-level token encoders with pre-trained LMs, fine-tuning encoder/decoder parameters for downstream MDS tasks, or using decoder-only conditional language models with plug-in adapters for few-shot settings. Hierarchical architectures are commonly used where a pre-trained LM encodes short blocks (token-level), and higher-level Transformer layers fuse block outputs to encode long multi-document clusters.",
            "llm_model_used": "BERT, RoBERTa, GPT-2, GPT-3, XLNet, ALBERT, T5, PEGASUS (as mentioned in the paper); also Longformer, Reformer, Big Bird as long-sequence variants suggested for MDS",
            "extraction_technique": "Fine-tuning and hierarchical encoding of document blocks with pre-trained LM token encoders; also few-shot conditioning via plug-in networks; extractive pre-selection (e.g., top-k paragraphs) before LM encoding is often used",
            "synthesis_technique": "Abstractive generation via LM decoders or decoder-only conditional LMs; hierarchical summarization where block encodings are fused by higher-level layers; graph-informed self-attention in some architectures to incorporate cross-document relations",
            "number_of_papers": "varies by dataset; typical clusters range from a few documents up to 10+ documents per cluster (paper notes top-10 references for WikiSum-style tasks)",
            "domain_or_topic": "general news, Wikipedia article generation, opinion/review summarization, and suggested for scientific papers (e.g., Multi-XScience, SciSumm domains)",
            "output_type": "abstractive summaries (single article/Wikipedia page), extractive selections, or hybrid summaries; can be configured to output structured summaries depending on pipeline",
            "evaluation_metrics": "ROUGE (ROUGE-1/2/L) primarily; human judgment evaluations; other semantic metrics discussed (BERTScore, MoverScore, SUPERT) — survey reports ROUGE as dominant in MDS evaluation",
            "performance_results": null,
            "comparison_baseline": "traditional summarization models, non-pretrained neural encoders/decoders, and extractive baselines (survey does not report specific numeric comparisons for aggregated pre-trained LM approach)",
            "performance_vs_baseline": null,
            "key_findings": "Pre-trained LMs help overcome limited MDS labeled data by transferring general language knowledge; replacing token-level encoders with pre-trained LMs in hierarchical models improves representational quality; however, naive fine-tuning of very large models can lead to overfitting and poor generation in low-data MDS settings, motivating few-shot or adapter-style solutions.",
            "limitations_challenges": "Quadratic memory growth of self-attention when handling long concatenated documents; overfitting when fine-tuning very large LMs on scarce MDS datasets; need to adapt architectures for long sequences (Longformer/BigBird/Reformer suggested); potential hallucination/factual inconsistency not deeply quantified in the survey but noted as concern for abstractive outputs.",
            "scaling_behavior": "Survey reports qualitative scaling trends: larger pre-trained models and targeted pre-training (e.g., PEGASUS for summarization) tend to improve downstream MDS performance, but computational and memory costs increase; hierarchical/blockwise encoding and long-sequence Transformer variants are recommended to scale to larger document clusters.",
            "uuid": "e4374.0",
            "source_info": {
                "paper_title": "Multi-document Summarization via Deep Learning Techniques: A Survey",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "PEGASUS",
            "name_full": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization",
            "brief_description": "A Transformer-based pre-trained encoder-decoder model specifically tailored for abstractive summarization, using gap-sentence generation (GSG) pre-training objective where whole sentences are masked based on importance and the model is trained to generate them.",
            "citation_title": "PEGASUS: Pre-training with Extracted Gapsentences for Abstractive Summarization",
            "mention_or_use": "mention",
            "system_name": "PEGASUS (pre-trained summarization LM)",
            "system_description": "PEGASUS is a large pre-trained Transformer encoder-decoder model that uses a gap-sentence generation objective: important sentences are removed from documents during pre-training and the model learns to generate these sentences from the remaining text, providing a pre-training task closely aligned with summarization.",
            "llm_model_used": "PEGASUS (Transformer encoder-decoder; model sizes vary in original PEGASUS paper, but the survey does not list sizes)",
            "extraction_technique": "Pre-training-based representation learning where entire sentences are masked (gap sentences) to teach the model to identify and generate salient sentences; can be fine-tuned on MDS datasets",
            "synthesis_technique": "Abstractive summarization via the model's decoder that generates missing/gap sentences conditioned on multi-document input (typically after some extractive selection or hierarchical encoding to respect length constraints)",
            "number_of_papers": "design intended for single- and multi-document summarization; number of source documents per instance depends on dataset (survey references WikiSum / Multi-News style setups where up to ~10 references may be used)",
            "domain_or_topic": "general summarization domains including news and Wikipedia-style generation; recommended as promising for MDS abstractive tasks",
            "output_type": "abstractive summaries (sentence-level generation aligning with gap-sentences pre-training)",
            "evaluation_metrics": "ROUGE variants (ROUGE-1/2/L) typically used for evaluation in summarization tasks",
            "performance_results": null,
            "comparison_baseline": "standard pre-training objectives (e.g., MLM) and baseline encoder-decoder models; survey states PEGASUS's GSG is more suitable for summarization than random or lead masking but does not give numeric results here",
            "performance_vs_baseline": null,
            "key_findings": "Masking whole sentences based on importance (GSG) in pre-training is more effective for downstream summarization than random or simple lead-based masking, making PEGASUS particularly suitable for abstractive MDS adaptation.",
            "limitations_challenges": "Survey notes computational cost and document-length constraints when applying pre-trained encoder-decoder models to very long multi-document inputs; PEGASUS pre-training is oriented to sentence prediction which may require adaptation for cross-document fusion.",
            "scaling_behavior": "Survey recommends combining PEGASUS with hierarchical encoding or long-document Transformers to scale to larger multi-document inputs; explicit quantitative scaling trends not provided in the survey.",
            "uuid": "e4374.1",
            "source_info": {
                "paper_title": "Multi-document Summarization via Deep Learning Techniques: A Survey",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "WikiSum / Generating Wikipedia",
            "name_full": "Generating Wikipedia by Summarizing Long Sequences (WikiSum)",
            "brief_description": "A model and dataset approach that generates Wikipedia articles by summarizing multiple long reference documents and web search results, using a Transformer-based decoder-only sequence transduction architecture built on top of an extractive selection stage.",
            "citation_title": "Generating Wikipedia by Summarizing Long Sequences",
            "mention_or_use": "mention",
            "system_name": "WikiSum (Transformer decoder-only MDS pipeline)",
            "system_description": "WikiSum first gathers multiple source documents (citations for a Wikipedia topic or top-N search results), applies an extractive stage to select salient text (e.g., top-k tokens/paragraphs), and trains a Transformer decoder-only sequence transduction model to generate the target Wikipedia article text, combining extractive results and the reference summary into training sentences.",
            "llm_model_used": "Transformer decoder-only architecture (survey describes decoder-only Transformer; does not specify a particular pretrained LLM name/size in-text)",
            "extraction_technique": "An extractive pre-selection stage ranks/selects top-k tokens/paragraphs (survey mentions selection of top-k tokens as input to decoder-only model); selection reduces input length before generation",
            "synthesis_technique": "Decoder-only Transformer generates an abstractive article by conditioning on concatenated selected extractive fragments; synthesis fuses information across multiple sources into cohesive long-form text",
            "number_of_papers": "survey describes using the documents cited in a Wikipedia article or top-10 Google search results (i.e., up to ~10 source documents per instance)",
            "domain_or_topic": "Wikipedia-style article generation from web documents; the approach is general and can apply to multi-document summarization across domains",
            "output_type": "long-form abstractive articles (Wikipedia pages)",
            "evaluation_metrics": "ROUGE metrics and human evaluation for long-form article quality are typical (survey mentions using ROUGE and hierarchical evaluation practices for such tasks)",
            "performance_results": null,
            "comparison_baseline": "flat encoder-decoder models and other MDS baselines (survey notes decoder-only approach chosen because encoder-decoder struggles with very long inputs), but specific numeric baselines not provided in-survey",
            "performance_vs_baseline": null,
            "key_findings": "Using an extractive pre-selection stage followed by a decoder-only Transformer is an effective architecture choice for very long multi-source inputs like Wikipedia generation; the extractive stage reduces input length and helps the decoder focus on salient content.",
            "limitations_challenges": "Long input length remains a core challenge; naive encoder-decoder approaches struggle with very long concatenated sources; selection quality heavily affects generated content; potential factual inconsistency/hallucination risks for abstractive long-form generation acknowledged but not quantified in survey.",
            "scaling_behavior": "Survey indicates scaling to long-source settings requires extractive filtering and/or hierarchical architectures; no quantitative scaling curves provided.",
            "uuid": "e4374.2",
            "source_info": {
                "paper_title": "Multi-document Summarization via Deep Learning Techniques: A Survey",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "FewSum / Few-shot conditional LM",
            "name_full": "Few-shot Transformer conditional language model with plug-in network (FewSum-style approach)",
            "brief_description": "A few-shot MDS approach that conditions a large Transformer conditional language model with a small plug-in network to enable extractive and abstractive summarization without extensive fine-tuning on MDS datasets, reducing rapid overfitting.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Few-shot conditional Transformer + plug-in network (Brazinskas et al. style)",
            "system_description": "The approach uses a large conditional Transformer language model and attaches a lightweight plug-in network to adapt the model to multi-document summarization in few-shot settings; the plug-in acts as an adapter to avoid full fine-tuning of the large LM and mitigate rapid overfitting and poor generation caused by naive fine-tuning.",
            "llm_model_used": "Transformer conditional language models (survey references a conditional LM used by Brazinskas et al.; specific family/size not enumerated in the survey)",
            "extraction_technique": "Few-shot conditioning using the plug-in network to bias generation toward salient content; may incorporate extractive pre-selection as a preprocessing step (survey notes method is for extractive and abstractive MDS)",
            "synthesis_technique": "The conditional LM synthesizes summaries in a few-shot manner, with the plug-in guiding extraction/abstraction and preventing overfitting; used for both extractive and abstractive outputs",
            "number_of_papers": "designed to work with few-shot setups where only a small number of annotated examples are available (survey does not give absolute paper-count ranges)",
            "domain_or_topic": "opinion/review summarization and general MDS; method aimed to be domain-agnostic but demonstrated on opinion summarization tasks in referenced work",
            "output_type": "extractive or abstractive summaries depending on scoring/decoding setup",
            "evaluation_metrics": "ROUGE and human evaluations commonly used in cited few-shot MDS work (survey cites ROUGE as primary metric across MDS tasks)",
            "performance_results": null,
            "comparison_baseline": "naive fine-tuning of large LMs and standard extractive/abstractive MDS models; survey notes approach was proposed to overcome overfitting and poor generation from naive fine-tuning",
            "performance_vs_baseline": null,
            "key_findings": "Adapter/plug-in style few-shot adaptation of large conditional LMs can reduce overfitting and stabilize generation for MDS when labeled data are scarce; few-shot strategies are a promising direction for large-parameter models in MDS.",
            "limitations_challenges": "Survey notes few-shot methods still face generation quality and factuality challenges; lacks extensive quantitative evaluation in MDS settings within the survey; model choice and adapter design influence results heavily.",
            "scaling_behavior": "Survey qualitatively reports that few-shot plug-in methods help avoid rapid overfitting when scaling model parameter counts, but provides no quantitative scaling curves.",
            "uuid": "e4374.3",
            "source_info": {
                "paper_title": "Multi-document Summarization via Deep Learning Techniques: A Survey",
                "publication_date_yy_mm": "2020-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PEGASUS: Pre-training with Extracted Gapsentences for Abstractive Summarization",
            "rating": 2,
            "sanitized_title": "pegasus_pretraining_with_extracted_gapsentences_for_abstractive_summarization"
        },
        {
            "paper_title": "Generating Wikipedia by Summarizing Long Sequences",
            "rating": 2,
            "sanitized_title": "generating_wikipedia_by_summarizing_long_sequences"
        },
        {
            "paper_title": "Few-Shot Learning for Opinion Summarization",
            "rating": 2,
            "sanitized_title": "fewshot_learning_for_opinion_summarization"
        },
        {
            "paper_title": "Longformer: The Long-document Transformer",
            "rating": 2,
            "sanitized_title": "longformer_the_longdocument_transformer"
        },
        {
            "paper_title": "Big Bird: Transformers for Longer Sequences",
            "rating": 2,
            "sanitized_title": "big_bird_transformers_for_longer_sequences"
        },
        {
            "paper_title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles",
            "rating": 2,
            "sanitized_title": "multixscience_a_largescale_dataset_for_extreme_multidocument_summarization_of_scientific_articles"
        },
        {
            "paper_title": "Scisummnet: A Large Annotated Corpus and Content-impact Models for Scientific Paper Summarization with Citation Networks",
            "rating": 1,
            "sanitized_title": "scisummnet_a_large_annotated_corpus_and_contentimpact_models_for_scientific_paper_summarization_with_citation_networks"
        }
    ],
    "cost": 0.01979325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Multi-document Summarization via Deep Learning Techniques: A Survey
9 Dec 2021</p>
<p>Congbo Ma congbo.ma@adelaide.edu.au 
H U Wang hu.wang@adelaide.edu.au 
Quan Z Sheng </p>
<p>The University of Adelaide WEI EMMA ZHANG
The University of Adelaide MINGYU GUO
The University of Adelaide</p>
<p>The University of Adelaide</p>
<p>Macquarie University</p>
<p>CONGBO MA
The University of Adelaide</p>
<p>Multi-document Summarization via Deep Learning Techniques: A Survey
9 Dec 2021FE45C3C8392C9D538C4389D5F0E8EB9810.1145/nnnnnnn.nnnnnnnarXiv:2011.04843v3[cs.CL]CCS Concepts:Computing methodologies → Natural language processingMachine learning algorithmsInformation extraction Multi-document summarization, Deep neural networks, Machine learning
Multi-document summarization (MDS) is an effective tool for information aggregation that generates an informative and concise summary from a cluster of topic-related documents.Our survey, the first of its kind, systematically overviews the recent deep learning based MDS models.We propose a novel taxonomy to summarize the design strategies of neural networks and conduct a comprehensive summary of the state-ofthe-art.We highlight the differences between various objective functions that are rarely discussed in the existing literature.Finally, we propose several future directions pertaining to this new and exciting field.</p>
<p>INTRODUCTION</p>
<p>In this era of rapidly advancing technology, the exponential increase of data availability makes analyzing and understanding text files a tedious, labor-intensive, and time-consuming task [63,117].The need to process this abundance of text data rapidly and efficiently calls for new, effective text summarization techniques.Text summarization is a key natural language processing (NLP) tasks that automatically converts a text, or a collection of texts within the same topic, into a concise summary that contains key semantic information which can be beneficial for many downstream applications such as creating news digests, search engine, and report generation [123].</p>
<p>Text can be summarized from one or several documents, resulting in single document summarization (SDS) and multi-document summarization (MDS).While simpler to perform, SDS may not produce comprehensive summaries because it does not make good use of related, or more recent, documents.Conversely, MDS generates more comprehensive and accurate summaries from documents written at different times, covering different perspectives, but is accordingly more complicated as it tries to resolve potentially diverse and redundant information [146].</p>
<p>In addition, excessively long input documents often lead to model degradation [71].It is challenging for models to retain the most critical contents of complex input sequences while generating a coherent, non-redundant, factual consistent and grammatically readable summary.Therefore, MDS requires models to have stronger capabilities for analyzing the input documents, identifying and merging consistent information.</p>
<p>MDS enjoys a wide range of real-world applications, including summarization of news [43], scientific publications [166], emails [22,170], product reviews [49], medical documents [1], lecture feedback [98,99], software project activities [2], and Wikipedia articles generation [93].Recently, MDS technology has also received a great amount of industry attention; an intelligent multilingual news reporter bot named Xiaomingbot [160] was developed for news generation, which can summarize multiple news sources into one article and translate it into multiple languages.Massive application requirements and rapidly growing online data have promoted the development of MDS.Existing methods using traditional algorithms are based on: term frequency-inverse document frequency (TF-IDF) [10,127], clustering [51,154], graphs [101,153] and latent semantic analysis [7,58].Most of these works still generate summaries with manually crafted features [105,153], such as sentence position features [11,40], sentence length features [40], proper noun features [152], cue-phrase features [57], biased word features, sentence-to-sentence cohesion and sentence-tocentroid cohesion.Deep learning has gained enormous attention in recent years due to its success in various domains, for instance, computer vision [78], natural language processing [35] and multimodal learning [156].Both industry and academia have embraced deep learning to solve complex tasks due to its capability of capturing highly nonlinear relations of data.Moreover, deep learning based models reduce dependence on manual feature extraction and pre-knowledge in the field of linguistics, drastically improving the ease of engineering [147].Therefore, deep learning based methods demonstrate outstanding performance in MDS tasks in most cases [20,82,90,94,97].With recently dramatic improvements in computational power and the release of increasing numbers of public datasets, neural networks with deeper layers and more complex structures have been applied in MDS [89,94], accelerating the development of text summarization with more powerful and robust models.These tasks are attracting attention in the natural language processing community; the number of research publications on deep learning based MDS has increased rapidly over the last five years.</p>
<p>The prosperity of deep learning for summarization in both academia and industry requires a comprehensive review of current publications for researchers to better understand the process and research progress.However, most of the existing summarization survey papers are based on traditional algorithms instead of deep learning based methods or target general text summarization [38,45,59,113,138].We have therefore surveyed recent publications on deep learning methods for MDS that, to the best of our knowledge, is the first comprehensive survey of this field.This survey has been designed to classify neural based MDS techniques into diverse categories thoroughly and systematically.We also conduct a detailed discussion on the categorization and progress of these approaches to establish a clearer concept standing in the shoes of readers.We hope this survey provides a panorama for researchers, practitioners and educators to quickly understand and step into the field of deep learning based MDS.The key contributions of this survey are three-fold:</p>
<p>• We propose a categorization scheme to organize current research and provide a comprehensive review for deep learning based MDS techniques, including deep learning based models, objective functions, benchmark datasets and evaluation metrics.• We review development movements and provide a systematic overview and summary of the state-of-the-art.We also summarize nine network design strategies based on our extensive studies of the current models.readers choose suitable indices to evaluate the effectiveness of a model.Section 6 summarizes standard and the variant MDS datasets.Finally, Section 7 discusses future research directions for deep learning based MDS followed by conclusions in Section 8.</p>
<p>FROM SINGLE TO MULTI-DOCUMENT SUMMARIZATION</p>
<p>Before we dive into the details of existing deep learning based techniques, we start by defining SDS and MDS, and introducing the concepts used un both methods.The aim of MDS is to generate a concise and informative summary  from a collection of documents . denotes a cluster of topic-related documents {  |  ∈ [1,  ]}, where  is the number of documents.Each document   consists of    sentences  , |  ∈ [1,    ] . , refers to the -th sentence in the -th document.The standard summary   is called the golden summary or reference summary.Currently, most golden summaries are written by experts.We keep this notation consistent throughout the article.</p>
<p>To give readers a clear understanding of the processing of deep learning based summarization tasks, we summarize and illustrate the processing framework as shown in Figure 2. The first step is preprocessing input document(s), such as segmenting sentences, tokenizing non-alphabetic characters, and removing punctuation [140].MDS models in particular need to select suitable concatenation methods to capture cross-document relations.Then, an appropriate deep learning based model is chosen to generate semantic-rich representation for downstream tasks.The next step is to fuse these various types of representation for later sentence selection or summary generation.Finally, document(s) are transformed into a concise and informative summary.Each of the highlighted steps in Figure 2 (indicated by triangles) indicates a difference between SDS and MDS.Based on this process, the research questions of MDS can be summarized as follows:</p>
<p>• How to capture the cross-document relations and in-document relations from the input documents?• Compared to SDS, how to extract or generate salient information in a larger search space containing conflict, duplication and complementary information?• How to best fuse various representation from deep learning based models and external knowledge?• How to comprehensively evaluate the performance of MDS models?</p>
<p>The following sections provide a comprehensive analysis of the similarities and differences between SDS and MDS.</p>
<p>Similarities between SDS and MDS</p>
<p>Existing SDS and MDS methods share the summarization construction types, learning strategies, evaluation indexes and objective functions.SDS and MDS both seek to compress the document(s) into a short and informative summary.Existing summarization methods can be grouped into abstractive summarization, extractive summarization and hybrid summarization (Figure 3).Extractive summarization methods select salient snippets from the source documents to creat informative summaries, and generally contain two major components: sentence ranking and sentence selection [19,109].Abstractive summarization methods aim to present the main information of input documents by automatically generating summaries that are both succinct and coherent; this cluster of methods allows models to generate new words and sentences from a corpus pool [123].Hybrid models are proposed to combine the advantages of both extractive and abstractive methods to process the input texts.Research on summarization focuses on two learning strategies.One strategy seeks to enhance the generalization performance by improving the architecture design of the end-toend models [30,43,71,94].The other leverages external knowledge or other auxiliary tasks to complement summary selection or generation [18,90].Furthermore, both SDS and MDS aim to minimize the distance between machine-generated summary and golden summary.Therefore, SDS and MDS could share some indices to evaluate the performance of summarization models such as Recall-Oriented Understudy for Gisting Evaluation (ROUGE, see Section 5), and objective functions to guide model optimization.</p>
<p>Differences between SDS and MDS</p>
<p>In the early stages of MDS, researchers directly applied SDS models to MDS [102].However, a number of aspects in MDS that are different from SDS and these differences are also the breakthrough point for exploring the MDS models.We summarize the differences in the following five aspects:</p>
<p>• More diverse input document types;</p>
<p>• Insufficient methods to capture cross-document relations;</p>
<p>• High redundancy and contradiction across input documents;</p>
<p>• Larger searching space but lack of sufficient training data;</p>
<p>• Lack of evaluation metrics specifically designed for MDS.</p>
<p>A defining different character between SDS and MDS is the number of input documents.The MDS tasks deal with multiple sources, of types that can be roughly divided into three groups:</p>
<p>• Many short sources, where each document is relatively short but the quantity of the input data is large.A typical example is product reviews summarization that aims to generate a short, informative summary from numerous individual reviews [4].• Few long sources.For example, generating a summary from a group of news articles [43], or constructing a Wikipedia style article from several web articles [93].</p>
<p>• Hybrid sources containing one or few long documents with several to many shorter documents.For example, news article(s) with several readers' comments to this news [88], or a scientific summary from a long paper with several short corresponding citations [166].As SDS only uses one input document, no additional processing is required to assess relationships between SDS inputs.By their very nature, the multiple input documents used in MDS are likely to contain more contradictory, redundant, and complementary information [126].MDS models therefore require sophisticated algorithms to identify and cope with redundancy and contradictions  across documents to ensure that the final summary is comprehensive.Detecting these relations across documents can bring benefits for MDS models.In the MDS tasks, there are two common methods to concatenate multiple input documents:</p>
<p>• Flat concatenation is a simple yet powerful concatenation method, where all input documents are spanned and processed as a flat sequence; to a certain extent, this method converts MDS to an SDS tasks.Inputting flat-concatenated documents requires models to have strong ability to process long sequences.• Hierarchical concatenation is able to preserve cross-document relations.However, many existing deep learning methods do not make full use of this hierarchical relationship [43,93,155].Taking advantage of hierarchical relations among documents instead of simply flat concatenating articles facilitates the MDS model to obtain representation with built-in hierarchical information, which in turn improves the effectiveness of the models.The input documents within a cluster describe a similar topic logically and semantically.Figure 4 illustrates two representative methods of hierarchical concatenation.Existing hierarchical concatenation methods either perform document-level condensing in a cluster separately [3] or process documents in word/sentence-level inside document cluster [5,111,155].In Figure 4(a), the extractive or abstractive summaries, or representation from the input documents are fused in the subsequent processes for final summary generation.The models using documentlevel concatenation methods are usually two-stage models.In Figure 4(b), sentences in the documents can be replaced by words.For the word or sentence-level concatenation methods, clustering algorithms and graph-based techniques are the most commonly used methods.Clustering methods could help MDS models decrease redundancy and increase the information coverage for the generated summaries [111].Sentence relation graph is able to model hierarchical relations among multi-documents as well [5,166,167].Most of the graph construction methods utilize sentences as vertexes and the edge between two sentences indicates their sentence-level relations [5].Cosine similarity graph [40], discourse graph [29,94,167], semantic graph [122] and heterogeneous graph [155] can be used for building sentence graph structures.These graph structure could all serve as external knowledge to improve the performance of MDS models.In addition to capture cross-document relation, hybrid summarization models can also be used to capture complex documents semantically, as well as to fuse disparate features that are more commonly adopted by MDS tasks.These models usually process data in two stages: extractiveabstractive and abstractive-abstractive (the right part of Figure 3).The two-stage models try to gather important information from source documents with extractive or abstractive methods at the first stage, to significantly reduce the length of documents.In the second stage, the processed texts are fed into an abstractive model to form final summaries [3,82,90,93,94].</p>
<p>Furthermore, conflict, duplication, and complementarity among multiple source documents require MDS models to have stronger abilities to handle complex information.However, applying the SDS model directly on MDS tasks is difficult to handle much higher redundancy [102].Therefore, the MDS models are required not only to generate coherent and complete summary but also more sophisticated algorithms to identify and cope with redundancy and contradictions across documents ensuring that the final summary should be complete in itself.MDS also involves larger searching spaces but has smaller-scale training data than SDS, which sets obstacles for deep learning based models to learn adequate representation [102].In addition, there are no specific evaluation metrics designed for MDS; however, existing SDS evaluation metrics can not evaluate the relationship between the generated abstract and different input documents well.</p>
<p>DEEP LEARNING BASED MULTI-DOCUMENT SUMMARIZATION METHODS</p>
<p>Deep neural network (DNN) models learn multiple levels of representation and abstraction from input data and can fit data in a variety of research fields, such as computer vision [78] and natural language process [35].Deep learning algorithms replace manual feature engineering by learning distinctive features through back-propagation to minimize a given objective function.It is well known that linear solvable problems possess many advantages, such as being easily solved and having numerous theoretically proven supports; however, many NLP tasks are highly non-linear.As theoretically proven by Hornik et al. [62], neural networks can fit any given continuous function as a universal approximator.For the MDS tasks, DNNs also perform considerably better than traditional methods to effectively process large-scale documents and distill informative summaries due to their strong fitting abilities.In this section, we first introduce our novel taxonomy that generalizes nine neural network design strategies (Section 3.1).We then present the state-of-the-art DNN based MDS models according to the main neural network architecture they adopt (Section 3.2 -3.7), before finishing with a brief introduction to MDS variant tasks (Section 3.8).</p>
<p>Architecture Design Strategies</p>
<p>Architecture design strategies play a critical role in deep learning based models, and many architectures have been applied to variants MDS tasks.Here, we have generalized the network architectures and summarize them into nine types based on how they generate or fuse semanticrich and syntactic-rich representation to improve MDS model performance (Figure 5); these different architectures can also be used as basic structures or stacked on each other to obtain more diverse design strategies.In Figure 5, deep neural models are in green boxes, and can be flexibly substituted with other backbone networks.The blue boxes indicate the neural embeddings processed by neural networks or heuristic-designed approaches, e.g., "sentence/document" or "other" representation.The explanation of each sub-figure is listed as follows:</p>
<p>• Naive Networks (Figure 5  • Ensemble Networks (Figure 5(b)).Ensemble based methods leverage multiple learning algorithms to obtain better performance than individual algorithms.To capture semantic-rich and syntactic-rich representation, Ensemble networks feed input documents to multiple paths with different network structures or operations.Later on, the representation from different networks is fused to enhance model expression capability.The majority vote or the average score can be used to determine the final output.• Auxiliary Task Networks (Figure 5(c)) employ different tasks in the summarization models, where text classification, text reconstruction or other auxiliary tasks serve as complementary representation learners to obtain advanced features.Meanwhile, auxiliary task networks also provide researchers with a solution to use appropriate data from other tasks.In this strategy, parameters sharing scheme are used for jointly optimizing different tasks.• Reconstruction Networks (Figure 5(d)) optimize models from an unsupervised learning paradigm, which allows summarization models to overcome the limitation of insufficient annotated golden summaries.The use of such a paradigm enables generated summaries to be constrained in the natural language domain in a good manner.</p>
<p>• Fusion Networks (Figure 5(e)) fuse representation generated from neural networks and handcrafted features.These hand-crafted features contain adequate prior knowledge that facilitates the optimization of summarization models.• Graph Neural Networks (Figure 5(f)).This strategy captures cross-document relations, crucial and beneficial for multi-document model training, by constructing graph structures based on the source documents, including word, sentence, or document-level information.• Encoder-Decoder Structure (Figure 5(g)).The encoder embeds source documents into the hidden representation, i.e., word, sentence and document representation.This representation, containing compressed semantic and syntactic information, is passed to the decoder which processes the latent embeddings to synthesize local and global semantic/syntactic information to produce the final summaries.• Pre-trained Language Models (Figure 5(h)) obtain contextualized text representation by predicting words or phrases based on their context using large amounts of the corpus, which can be further fine-tuned for downstream task adaption [36].The models can fine-tune with randomly initialized decoders in an end-to-end fashion since transfer learning can assist the model training process [90].• Hierarchical Networks (Figure 5(i)).Multiple documents are concatenated as inputs to feed into the first DNN based model to capture low-level representation.Another DNN based model is cascaded to generate high-level representation based on the previous ones.The hierarchical networks empower the model with the ability to capture abstract-level and semantic-level features more efficiently.</p>
<p>Recurrent Neural Networks based Models</p>
<p>Recurrent Neural Networks (RNNs) [133] excel in modeling sequential data by capturing sequential relations and syntactic/semantic information from word sequences.In RNN models, neurons are connected through hidden layers and unlike other neural network structures, the inputs of each RNN neuron come not only from the word or sentence embedding but also from the output of the previous hidden state.Despite being powerful, vanilla RNN models often encounter gradient explosion or vanishing issues, so a large number of RNN-variants have been proposed.The most prevalent ones are Long Short-Term Memory (LSTM) [61], Gated Recurrent Unit (GRU) [31] and Bi-directional Long Short-Term Memory (Bi-LSTM) [64].The DNN based Model in Figure 5 can be replaced with RNN based models to design models.RNN based models have been used in MDS tasks since 2015.Cao et al. [19] proposed an RNNbased model termed Ranking framework upon Recursive Neural Networks (R2N2), which leverages manually extracted words and sentence-level features as inputs.This model transfers the sentence ranking task into a hierarchical regression process, which measures the importance of sentences and constituents in the parsing tree.Zheng et al. [181] used a hierarchical RNN structure to utilize the subtopic information by extracting not only sentence and document embeddings, but also topic embeddings.In this SubTopic-Driven Summarization (STDS) model, the readers' comments are seen as auxiliary documents and the model employs soft clustering to incorporate comment and sentence representation for further obtaining subtopic representation.Arthur et al. [15] introduced a GRUbased encoder-decoder architecture to minimize the diversity of opinions reflecting the dominant views while generating multi-review summaries.Mao et al. [102] proposed a maximal margin relevance guided reinforcement learning framework (RL-MMR) to incorporate the advantages of neural sequence learning and statistical measures.The proposed soft attention for learning adequate representation allows more exploration of search space.</p>
<p>To leverage the advantage of hybrid summarization model, Reinald et al. [3] proposed a two-stage framework, viewing opinion summarization as an instance of multi-source transduction to distill salient information from source documents.The first stage of the model leverages a Bi-LSTM auto-encoder to learn word and document-level representation; the second stage fuses multi-source representation and generates an opinion summary with a simple LSTM decoder combined with a vanilla attention mechanism [8] and a copy mechanism [151].</p>
<p>Since paired MDS datasets are rare and hard to obtain, Li et al. [89] developed a RNN-based framework to extract salient information vectors from sentences in input documents in an unsupervised manner.Cascaded attention retains the most relevant embeddings to reconstruct the original input sentence vectors.During the reconstruction process, the proposed model leverages a sparsity constraint to penalize trivial information in the output vectors.Also, Chu et al. [30] proposed an unsupervised end-to-end abstractive summarization architecture called MeanSum.This LSTM-based model formalizes product or business reviews summarization problem into two individual closed-loops.Inspired by MeanSum, Coavoux et al. [32] used a two-layer standard LSTM to construct sentence representation for aspect-based multi-document abstractive summarization, and discovered that the clustering strategy empowers the model to reward review diversity and handle contradictory ones.</p>
<p>Convolutional Neural Networks Based Models</p>
<p>Convolutional neural networks (CNNs) [84] achieve excellent results in computer vision tasks.The convolution operation scans through the word/sentence embeddings and uses convolution kernels to extract important information from input data objects.Using a pooling operation at intervals can return simple to complex feature levels.CNNs have been proven to be effective for various NLP tasks in recent years [37,74] as they can process natural language after sentence/word vectorization.Most of the CNN based MDS models use CNNs for semantic and syntactic feature representation.As with RNN, CNN-based models can also replace DNN-based models in network design strategies (Please refer to Figure 5).</p>
<p>A simple way to use CNNs in MDS is by sliding multiple filters with different window sizes over the input documents for semantic representation.Cao et al. [20] proposed a hybrid CNN-based model PriorSum to capture latent document representation.The proposed representation learner slides over the input documents with filters of different window widths and two-layer max-overtime pooling operations [33] to fetch document-independent features that are more informative than using standard CNNs.</p>
<p>Similarly, HNet [141] uses distinct CNN filters and max-over-time-pooling to generate salient feature representation for downstream processes.Cho et al. [28] also used different filter sizes in DPPcombined model to extract low-level features.Yin et al. [168] presented an unsupervised CNN-based model termed Novel Neural Language Model (NNLM) to extract sentence representation and diminish the redundancy of sentence selection.The NNLM framework contains only one convolution layer and one max-pooling layer, and both element-wise averaging sentence representation and context words representation are used to predict the next word.For aspect-based opinion summarization, Stefanos et al. [4] leveraged a CNN based model to encode the product reviews which contain a set of segments for opinion polarity.</p>
<p>People with different background knowledge and understanding can produce different summaries of the same documents.To account for this variability, Zhang et al. [178] suggested a MV-CNN model that ensembles three individual models to incorporate multi-view learning and CNNs to improve the performance of MDS.In this work, three CNNs with dual-convolutional layers used multiple filters with different window sizes to extract distinct saliency scores of sentences.</p>
<p>To overcome the MDS bottlenecks of insufficient training data, Cao et al.Unlike RNNs that support the processing of long time-serial signals, a naive CNN layer struggles to capture long-distance relations while processing sequential data due to the limitation of the fixed-sized convolutional kernels, each of which has a specific receptive field size.Nevertheless, CNN based models can increase their receptive fields through formation of hierarchical structures to calculate sequential data in a parallel manner.Because of this highly parallelizable characteristic, training of CNN-based summarization models is more efficient than for RNN-based models.However, summarizing lengthy input articles is still a challenging task for CNN based models because they are not skilled in modeling non-local relationships.</p>
<p>Graph Neural Networks Based Models</p>
<p>CNNs have been successfully applied to many computer vision tasks to extract distinguished image features from the Euclidean space, but struggle when processing non-Euclidean data.Natural language data consist of vocabularies and phrases with strong relations which can be better represented with graphs than with sequential orders.Graph neural networks (GNNs, Figure 5 (f)) are composed of an ideal architecture for NLP since they can model strong relations between entities semantically and syntactically.Graph convolution networks (GCNs) and graph attention networks (GANs) are the most commonly adopted GNNs because of their efficiency and simplicity for integration with other neural networks.These models first build a relation graph based on input documents, where nodes can be words, sentences or documents, and edges capture the similarity among them.At the same time, input documents are fed into a DNN based model to generate embeddings at different levels.The GNNs are then built over the top to capture salient contextual information.Table 1 describes the current GNN based models used for MDS with details of nodes, edges, edge weights, and applied GNN methods.</p>
<p>Yasunage et al. [167] developed a GCN based extractive model to capture the relations between sentences.This model first builds a sentence-based graph and then feeds the pre-processed data into a GCN [75] to capture sentence-wise related features.Defined by the model, each sentence is regarded as a node and the relation between each pair of sentences is defined as an edge.Inside each document cluster, the sentence relation graph can be generated through a cosine similarity graph [40], approximate discourse graph [29], and the proposed personalized discourse graph.Both the sentence relation graph and sentence embeddings extracted by a sentence-level RNN are fed into GCN to produce the final sentence representation.With the help of a document-level GRU, the model generates cluster embeddings to fully aggregate features between sentences.</p>
<p>Similarly, Antognini et al. [5] proposed a GCN based model named SemSentSum that constructs a graph based on sentence relations.In contrast to Yasunage et al. [167], this work leverages external universal embeddings, pre-trained on the unrelated corpus, to construct a sentence semantic relation graph.Additionally, an edge removal method has been applied to deal with the sparse graph problems emphasizing high sentence similarities; if the weight of the edge is lower than a given threshold, the edge is removed.The sentence relation graph and sentence embeddings are fed into a GCN [75] to generate saliency estimation for extractive summaries.Yasunage et al. [166] also designed a GCN based model for summarizing scientific papers.The proposed ScisummNet model uses not only the abstract of source scientific papers but also the relevant text from papers that cite the original source.The total number of citations is also incorporated in the model as an authority feature.A cosine similarity graph is applied to form the sentence relation graph, and GCNs are adopted to predict the sentence salience estimation from the sentence relation graph, authority scores and sentence embeddings.</p>
<p>Existing GNN based models focused mainly on the relationships between sentences, and do not fully consider the relationships between words, sentences, and documents.To fill this gap, Wang et al. [155] proposed a heterogeneous GAN based model, called HeterDoc-SUM Graph, that is specific for extractive MDS.This heterogeneous graph structure includes word, sentence, and document nodes, where sentence nodes and document nodes are connected according to the contained word nodes.Word nodes thus act as an intermediate bridge to connect the sentence and document nodes, and are used to better establish document-document, sentence-sentence and sentence-document relations.TF-IDF values are used to weight word-sentence and word-document edges, and the node representation of these three levels are passed into the graph attention networks for model update.</p>
<p>In each iteration, bi-directional updating of both word-sentence and word-document relations are performed to better aggregate cross-level semantic knowledge.</p>
<p>Pointer-generator Networks Based Models</p>
<p>Pointer-generator (PG) networks [135] are proposed to overcome the problems of factual errors and high redundancy in the summarization tasks.This network has been inspired by Pointer Network [151], CopyNet [55], forced-attention sentence compression [104], and coverage mechanism from machine translation [148].PG networks combine sequence-to-sequence (Seq2Seq) model and pointer networks to obtain a united probability distribution allowing vocabularies to be selected from source texts or generated by machines.Additionally, the coverage mechanism prevents PG networks from consistently choosing the same phrases.</p>
<p>The Maximal Marginal Relevance (MMR) method is designed to select a set of salient sentences from source documents by considering both importance and redundancy indices [21].The redundancy score controls sentence selection to minimize overlap with the existing summary.The MMR model adds a new sentence to the objective summary based on importance and redundancy scores until the summary length reaches a certain threshold.Inspired by MMR, Alexander et al. [43] proposed an end-to-end Hierarchical MMR-Attention Pointer-generator (Hi-MAP) model to incorporate PG networks and MMR [21] for abstractive MDS.The Hi-MAP model improves PG networks by modifying attention weights (multipling MMR scores by the original attention weights) to include better important sentences in, and filter redundant information from, the summary.Similarly, the MMR approach is implemented by PG-MMR model [83] to identify salient source sentences from multi-document inputs, albeit with a different method for calculating MMR scores from Hi-MAP; instead, ROUGE-L Recall and ROUGE-L Precision [91] serve as evaluation metrics to calculate the importance and redundancy scores.To overcome the scarcity of MDS datasets, the PG-MMR model leverages a support vector regression model that is pre-trained on a SDS dataset to recognize the important contents.This support vector regression model also calculates the score of each input sentence by considering four factors: sentence length, sentence relative/absolute position, sentence-document similarities, and sentence quality obtained by a PG network.Sentences with the top- scores are fed into another PG network to generate a concise summary.</p>
<p>Transformer Based Models</p>
<p>As discussed, CNN based models are not as good at processing sequential data as RNN based models.However, RNN based models are not amenable to parallel computing, as the current states in RNN models highly depend on results from the previous steps.Additionally, RNNs struggle to process long sequences since former knowledge will fade away during the learning process.Adopting Transformer based architectures [150] is one solution to solve these problems.The Transformer is based on the self-attention mechanism, has natural advantages for parallelization, and retains relative long-range dependencies.The Transformer model has achieved promising results in MDS tasks [71,90,93,94] and can replace the DNN based Model in Figure 5.Most of the Transformer based models follow an encoder-decoder structure.Transformer based models can be divided into flat Transformer, hierarchical Transformer, and pre-train language models.Flat Transformer.Liu et al. [93] introduced Transformer to MDS tasks, aiming to generate a Wikipedia article from a given topic and set of references.The authors argue that the encoderdecoder based sequence transduction model cannot cope well with long input documents, so their model selects a series of top- tokens and feeds them into a Transformer based decoder-only sequence transduction model to generate Wikipedia articles.More specifically, the Transformer decoder-only architecture combines the result from the extractive stage and golden summary into a sentence for training.To obtain rich semantic representation from different granularity, Jin et al. [71] proposed a Transformer based multi-granularity interaction network MGSum and unified extractive and abstractive MDS.Words, sentences and documents are considered as three granular levels of semantic unit connected by a granularity hierarchical relation graph.In the same granularity, a self-attention mechanism is used to capture the semantic relationships.Sentence granularity representation is employed in the extractive summarization, and word granularity representation is adapted to generate an abstractive summary.MGSum employs a fusion gate to integrate and update the semantic representation.Additionally, a spare attention mechanism is used to ensure the summary generator focus on important information.Brazinskas et al. [16] created a precedent for few-shot learning for MDS that leverages a Transformer conditional language model and a plug-in network for both extractive and abstractive MDS to overcome rapid overfitting and poor generation problems resulting from naive fine-tuning of large parameter models.Hierarchical Transformer.To handle huge input documents, Yang et al. [94] proposed a twostage Hierarchical Transformer (HT) model with an inter-paragraph and graph-informed attention mechanism that allows the model to encode multiple input documents hierarchically instead of by simple flat-concatenation.A logistic regression model is employed to select the top- paragraphs, which are fed into a local Transformer layer to obtain contextual features.A global Transformer layer mixes the contextual information to model the dependencies of the selected paragraphs.To leverage graph structure to capture cross-document relations, Li et al. [90] proposed an end-toend Transformer based model GraphSum, based on the HT model.In the graph encoding layers, GraphSum extends the self-attention mechanism to the graph-informed self-attention mechanism, which incorporates the graph representation into the Transformer encoding process.Furthermore, the Gaussian function is applied to the graph representation matrix to control the intensity of the graph structure impact on the summarization model.The HT and GraphSum models are both based on the self-attention mechanism leading quadratic memory growth increases with the number of input sequences; to address this issue, Pasunuru et al. [122] modified the full self-attention with local and global attention mechanism [13] to scale the memory linearly.Dual encoders are proposed for encoding truncated concatenated documents and linearized graph information from full documents.Pre-trained language models (LMs).Pre-trained Transformers on large text corpora have shown great successes in downstream NLP tasks including text summarization.The pre-trained LMs can be trained on non-summarization or SDS datasets to overcome lack of MDS data [90,122,172].Most pre-trained LMs such as BERT [34] and RoBERTa [95] can work well on short sequences.In hierarchical Transformer architecture, replacing the low-level Transformer (token-level) encoding layer with pre-trained LMs helps the model break through length limitations to perceive further information [90].Inside a hierarchical Transformer architecture, the output vector of the "[CLS]" token can be used as input for high-level Transformer models.To avoid the self-attention quadraticmemory increment when dealing with document-scale sequences, a Longformer based approach [13], including local and global attention mechanisms, can be incorporated with pre-trained LMs to scale the memory linearly for MDS [122].Another solution for computational issues can be borrowed from SDS is to use a multi-layer Transformer architecture to scale the length of documents allowing pre-trained LMs to encode a small block of text and the information can be shared among the blocks between two successive layers [53].PEGASUS [172] is a pre-trained Transformer-based encoder-decoder model with gap-sentences generation (GSG) specifically designed for abstractive summarization.GSG shows that masking whole sentences based on importance, instead of through random or lead selection, works well for downstream summarization tasks.</p>
<p>Deep Hybrid Models</p>
<p>Many neural models can be integrated to formalize a more powerful and expressive model.In this section, we summarize the existing deep hybrid models that have proven to be effective for MDS.CNN + LSTM + Capsule networks.Cho et al. [28] proposed a hybrid model based on the determinantal point processes for semantically measuring sentence similarities.A convolutional layer slides over the pairwise sentences with filters of different sizes to extract low-level features.Capsule networks [134,162] are employed to identify redundant information by transforming the spatial and orientational relationships for high-level representation.The authors also used LSTM to reconstruct pairwise sentences and add reconstruction loss to the final objective function.CNN + Bi-LSTM + Multi-layer Perceptron (MLP).Abhishek et al. [141] proposed an extractive MDS framework that considers document-dependent and document-independent information.In this model, a CNN with different filters captures phrase-level representation.Full binary trees formed with these salient representation are fed to the recommended Bi-LSTM tree indexer to enable better generalization abilities.A MLP with ReLU function is employed for leaf node transformation.More specifically, the Bi-LSTM tree indexer leverages the time serial power of LSTMs and the compositionality of recursive models to capture both semantic and compositional features.PG networks + Transformer.In generating a summary, it is necessary to consider the information fusion of multiple sentences, especially sentence pairs.Logan et al. [82] found the majority of summary sentences are generated by fusing one or two source sentences; so they proposed a two-stage summarization method that considers the semantic compatibility of sentence pairs.This method joint-scores single sentence and sentence pairs to filter representative from the original documents.Sentences or sentence pairs with high scores are then compressed and rewritten to generate a summary that leverages PG network.This paper uses a Transformer based model to encode both single sentence and sentence pairs indiscriminately to obtain the deep contextual representation of words and sequences.</p>
<p>The Variants of Multi-document Summarization</p>
<p>In this section, we briefly introduce several MDS task variants to give researchers a comprehensive understanding of MDS.These tasks can be modeled as MDS problems and adopt the aforementioned deep learning techniques and neural network architectures.Query-oriented MDS calls for a summary from a set of documents that answers a query.It tries to solve realistic query-oriented scenario problems and only summarizes important information that best answers the query in a logical order [121].Specifically, query-oriented MDS combines the information retrieval and MDS techniques.The content that needs to be summarized is based on the given queries.Liu et al. [94] incorporated the query by simply prepending the query to the top-ranked document during encoding.Pasunuru [121] involved a query encoder and integrated query embedding into an MDS model, ranking the importance of documents for a given query.Dialogue summarization aims to provide a succinct synopsis from multiple textual utterances of two or more participants, which could help quickly capture relevant information without having to listen to long and convoluted dialogues [92].Dialogue summary covers several areas, including meetings [44,77,186], email threads [174], medical dialogues [39,72,142], customer service [92] and media interviews [185].Challenges in dialogue summarization can be summarized into the following seven categories: informal language use, multiple participants, multiple turns, referral and coreference, repetition and interruption, negations and rhetorical questions, role and language change [24].The flow of the dialogue would be neglected if MDS models are directly applied for dialogue summarization.Liu et al. [92] relied on human annotations to capture the logic of the dialogue.Wu et al. [158] used summary sketch to identify the interaction between speakers and their corresponding textual utterances in each turn.Chen et al. [24] proposed a multi-view sequence to sequence based encoder to extract dialogue structure and a multi-view decoder to incorporate different views to generate final summaries.</p>
<p>Stream summarization aims to summarize new documents in a continuously growing document stream, such as information from social media.Temporal summarization and real-time summarization (RTS) 9 can be seen as a form of stream document summarization.Stream summarization considers both historical dependencies and future uncertainty of the document stream.Yang et al. [161] used deep reinforcement learning to solve the relevance, redundancy, and timeliness issues in steam summarization.Tan et al. [145] transformed the real time summarization task as a sequential decision making problem and used a LSTM layer and three fully connected neural network layers to maximize the long-term rewards.</p>
<p>Discussion</p>
<p>In this section, we have reviewed the state-of-the-art works of deep learning based MDS models according to the neural networks applied.Table 2 summarizes the reviewed works by considering the type of neural networks, construction types, and concatenation methods; and provides a highlevel summary of their relative advantages and disadvantages.Transformer based models have been most commonly used in the last three years because they overcome the limitations of CNN's fixed-size receptive field and RNN's inability to parallel process.However, deep learning based MDS models face some challenges.Firstly, the complexity of deep learning based models and the data-driven deep learning systems do require more training data, with concomitant increased efforts in data labelling, and computing resources than non-deep learning based methods, which are not time efficient.Secondly, deep learning based methods lack linguistic knowledge that can serve as important roles in assisting deep learning based learners to have informative representation and better guide the summary generation.We believe that this is one possible reason that some nondeep learning based MDS methods sometimes show better performance than deep learning based methods [20,97] as non-deep learning based methods pay more attention to linguistic information.We discuss this point in Section 7. Further researches could also be based on techniques adopted in non-deep learning based MDS as reviewed in [38,45,138].</p>
<p>OBJECTIVE FUNCTIONS</p>
<p>In this section, we will take a closer look at different objective functions adopted by various MDS models.In summarization models, objective functions play an important role by guiding the model to achieve specific purposes.To the best of our knowledge, we are the first to provide a comprehensive survey on different objectives of summarization tasks.</p>
<p>Cross-Entropy Objective</p>
<p>Cross-entropy usually acts as an objective function to measure the distance between two distributions.Many existing MDS models adopt it to measure the difference between the distributions of generated summaries and the golden summaries [19,28,155,166,171,178]. Formally, cross-entropy loss is defined as:
𝐿 𝐶𝐸 = − ∑︁ 𝑖=1 y i log( ŷi ),(1)
where y i is the target score from golden summaries and machine-generated summaries, and ŷi is the predicted estimation from the deep learning based models.Different from calculations in other tasks, such as text classification, in summarization tasks, y i and ŷi have several methods to calculate.ŷi usually is calculated by Recall-Oriented Understudy for Gisting Evaluation (ROUGE) (Please refer to Section 5).For example, ROUGE-1 [5], ROUGE-2 [94] or the normalized average of ROUGE-1 and ROUGE-2 scores [167] could be adopted to compute the ground truth score between the selected sentences and golden summary.</p>
<p>Reconstructive Objective</p>
<p>Reconstructive objectives are used to train a distinctive representation learner by reconstructing the input vectors in an unsupervised learning manner.The objective function is defined as:
𝐿 𝑅𝑒𝑐 = x i − 𝜙 ′ (𝜙 (x i ; 𝜃 ); 𝜃 ′ ) * ,(2)
where x i represents the input vector;  and  ′ represent the encoder and decoder with  and  ′ as their parameters respectively, measuring function to calculate the distance between source documents and their reconstructive outputs.Chu et al. [30] used a reconstructive loss to constrain the generated text into the natural language domain, reconstructing reviews in a token-by-token manner.Moreover, this paper also proposes a variant termed reconstruction cycle loss.By using the variant, the reviews are encoded into a latent space to further generate the summary, and the summary is then decoded to the reconstructed reviews to form another reconstructive closed-loop.An unsupervised learning loss was designed by Li et al. [89] to reconstruct the condensed output vectors to the original input sentence vectors with  2 distance.This paper further constrains the condensed output vector with a  1 regularizer to ensure sparsity.Similarly, Zheng et al. [181] adopted a bi-directional GRU encoderdecoder framework to reconstruct both news sentences and comment sentences in a word sequence manner.Liu et al. [93] used reconstruction within the abstractive stage of a two-stage strategy to alleviate the problem introduced by long input documents.Both input and output sequences are concatenated to predict the next token to train the abstractive model.There are also some variants, such as leveraging the latent vectors of variational auto-encoder for reconstruction to capture better representation.Li et al. [88] introduced three individual reconstructive losses to consider both news reconstruction and comments reconstruction separately, along with a variational auto-encoder lower bound.Bravzinskas et al. [15] utilized a variational auto-encoder to generate the latent vectors of given reviews, where each review is reconstructed by the latent vectors combined with other reviews.</p>
<p>Redundancy Objective</p>
<p>Redundancy is an important objective to minimize the overlap between semantic units in a machinegenerated summary.By using this objective, models are encouraged to maximize information coverage.Formally,
𝐿 𝑅𝑒𝑑 = 𝑆𝑖𝑚(x i , x j ),(3)
where (•) is the similarity function to measure the overlap between different x i and x j , which can be phrases, sentences, topics or documents.The redundancy objective is often treated as an auxiliary objective combined with other loss functions.Li et al. [89] penalized phrase pairs with similar meanings to eliminate the redundancy.Nayeem et al. [111] used the redundancy objective to avoid generating repetitive phrases, constraining a sentence to appear only once while maximizing the scores of important phrases.Zheng et al. [181] adopted a redundancy loss function to measure overlaps between subtopics; intuitively, smaller overlaps between subtopics resulted in less redundancy in the output domain.Yin et al. [168] proposed a redundancy objective to estimate the diversity between different sentences.</p>
<p>Max Margin Objective</p>
<p>Max Margin Objectives (MMO) are also used to empower the MDS models to learn better representation.The objective function is formalized as:
𝐿 𝑀𝑎𝑟𝑔𝑖𝑛 = max 0, 𝑓 (x i ; 𝜃 ) − 𝑓 (x j ; 𝜃 ) + 𝛾 ,(4)
where x i and x j represent the input vectors,  are parameters of the model function  (•), and  is the margin threshold.The MMO aims to force function  (x i ;  ) and function  (x j ;  ) to be separated by a predefined margin .In Cao et al. [18], a MMO is designed to constrain a pair of randomly sampled sentences with different salience scores -the one with higher score should be larger than the other one more than a marginal threshold.Two max margin losses are proposed in Zhong et al. [182]: a margin-based triplet loss that encouraged the model to pull the golden summaries semantically closer to the original documents than to the machine-generated summaries; and a pair-wise margin loss based on a greater margin between paired candidates with more disparate ROUGE score rankings.</p>
<p>Multi-Task Objective</p>
<p>Supervision signals from MDS objectives may not be strong enough for representation learners, so some works seek other supervision signals from multiple tasks.A general form is as follows:
𝐿 𝑀𝑢𝑙 = 𝐿 𝑆𝑢𝑚𝑚 + 𝐿 𝑂𝑡ℎ𝑒𝑟 ,(5)
where   is the loss function of MDS tasks, and  ℎ is the loss function of an auxiliary task.Angelidis et al. [4] assumed that the aspect-relevant words not only provides a reasonable basis for model aspect reconstruction, but also a good indicator for product domain.Similarly, multi-task classification was introduced by Cao et al. [18].Two models are maintained: text classification and text summarization models.In the first model, CNN is used to classify text categories and crossentropy loss is used as the objective function.The summarization model and the text classification model share parameters and pooling operations, so are equivalent to the shared document vector representation.Coavoux et al. [32] jointly optimized the model from a language modeling objective and two other multi-task supervised classification losses, which are polarity loss and aspect loss.</p>
<p>Other Types of Objectives</p>
<p>There are many other types of objectives in addition to those mentioned above.Cao et al. [20] proposed using ROUGE-2 to calculate the sentence saliency scores and the model tries to estimate this saliency with linear regression.Yin et al. [168] suggested summing the squares of the prestige vectors calculated by the PageRank algorithm to identify sentence importance.Zhang et al. [178] proposed an objective function by ensembling individual scores from multiple CNN models; besides the cross-entropy loss, a consensus objective is adopted to minimize disagreement between each pair of classifiers.Amplay et al. [3] used two objectives in the abstract module: the first to optimize the generation probability distribution by maximizing the likelihood; and the second to constrain the model output to be close to its golden summary in the encoding space, as well as being distant from the random sampled negative summaries.Chu et al. [30] designed a similarity objective that shares the encoder and decoder weights within the auto-encoder module, while in the summarization module, the average cosine distance indicates the similarity between the generated summary and the reviews.A variant similarity objective termed early cosine objective is further proposed to compute the similarity in a latent space which is the average of the cells states and hidden states to constrain the generated summaries semantically close to reviews.</p>
<p>Discussion</p>
<p>In MDS, cross-entropy is the most commonly adopted objective function that bridges the predicted candidate summaries and the golden summaries by treating the golden summaries as strong supervision signals.However, adopting cross-entropy loss alone may not lead the model to achieve good performance since the supervisory signal for cross-entropy objective is not strong enough by itself to effectively learn good representation.Several other objectives can thus serve as complements, e.g., reconstruction objectives offer a view from the unsupervised learning perspective; the redundancy objective constrains models from generating redundant content; while max-margin objectives require a step-change improvements from previous versions.By using multiple objectives, model optimization could be conducted with the input documents themselves if the manual annotation is scarce.The models that adopt multi-task objectives explicitly define multiple auxiliary tasks to assist the main summarization task for better generalization, and provide various constraints from different angles that lead to better model optimization.</p>
<p>EVALUATION METRICS</p>
<p>Evaluation metrics are used to measure the effectiveness of a given method objectively, so welldefined evaluation metrics are crucial to MDS research.We classify the existing evaluation metrics in two categories and will discuss each category in detail: (1) ROUGE: the most commonly used evaluation metrics in the summarization community; and (2) other evaluation metrics that have not been widely used in MDS research to date.</p>
<p>ROUGE</p>
<p>Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [91] is a collection of evaluation indicators that is one of the most essential metrics for many natural language processing tasks, including machine translation and text summarization.ROUGE obtains prediction/ground-truth similarity scores through comparing automatically generated summaries with a set of corresponding humanwritten references.ROUGE has many variants to measure candidate abstracts in a variety of ways [91].The most commonly used ones are ROUGE-N and ROUGE-L.ROUGE-N (ROUGE with n-gram co-occurrence statistics ) measures a n-gram recall between reference summaries and their corresponding candidate summaries [91].Formally, ROUGE-N can be calculated as:
𝑅𝑂𝑈 𝐺𝐸-𝑁 = 𝑆𝑢𝑚 ∈ {𝑅𝑒 𝑓 } 𝑔𝑟𝑎𝑚 𝑛 ∈𝑆𝑢𝑚 𝐶𝑜𝑢𝑛𝑡 𝑚𝑎𝑡𝑐ℎ (𝑔𝑟𝑎𝑚 𝑛 ) 𝑆𝑢𝑚 ∈ {𝑅𝑒 𝑓 } 𝑔𝑟𝑎𝑚 𝑛 ∈𝑆𝑢𝑚 𝐶𝑜𝑢𝑛𝑡 (𝑔𝑟𝑎𝑚 𝑛 ) ,(6)
where   and  are reference summary and machine-generated summary,  represents the length of n-gram, and  ℎ (  ) represents the maximum number of n-grams in the reference summary and corresponding candidates.The numerator of ROUGE-N is the number of n-grams owned by both the reference and generated summary, while the denominator is the total number of n-grams occurring in the golden summary.The denominator could also be set to the number of candidate summary n-grams to measure precision; however, ROUGE-N mainly focuses on quantifying recall, so precision is not usually calculated.ROUGE-1 and ROUGE-2 are special cases of ROUGE-N that are usually chosen as best practices and represent the unigram and bigram, respectively.ROUGE-L (ROUGE with Longest Common Subsequence) adopts the longest common subsequence algorithm to count the longest matching vocabularies [91].Formally, ROUGE-L is calculated using:
𝐹 𝑙𝑐𝑠 = (1 + 𝛽 2 )𝑅 𝑙𝑐𝑠 𝑃 𝑙𝑐𝑠 𝑅 𝑙𝑐𝑠 + 𝛽 2 𝑃 𝑙𝑐𝑠 ,(7)
where
𝑅 𝑙𝑐𝑠 = 𝐿𝐶𝑆 (𝑅𝑒 𝑓 , 𝑆𝑢𝑚) 𝑚 ,(8)
and
𝑃 𝑙𝑐𝑠 = 𝐿𝐶𝑆 (𝑅𝑒 𝑓 , 𝑆𝑢𝑚) 𝑛 .(9)
where LCS(•) represents the longest common subsequence function.ROUGE-L is termed as LCSbased F-measure as it is obtained from LCS-Precision   and LCS-Recall   . is the balance factor between   and   .It can be set by the fraction of   and   ; by setting  to a big number, only   is considered.The use of ROUGE-L enables measurement of the similarity of two text sequences at sentence-level.ROUGE-L also has the advantage of automatically deciding the n-gram without extra manual input, since the calculation of LCS empowers the model to count grams adaptively.</p>
<p>Other ROUGE Based Metrics.ROUGE-W [91] is proposed to weight consecutive matches to better measure semantic similarities between two texts.ROUGE-S [91] stands for ROUGE with Skip-bigram co-occurrence statistics that allows the bigram to skip arbitrary words.An extension of ROUGE-S, ROUGE-SU [91] refers to ROUGE with Skip-bigram plus Unigram-based co-occurrence statistics and is able to be obtained from ROUGE-S by adding a begin-of-sentence token at the start of both references and candidates.ROUGE-WE [116] is proposed to further extend ROUGE by measuring the pair-wise summary distances in word embeddings space.In recent years, more ROUGE-based evaluation models have been proposed to compare golden and machine-generated summaries, not just according to their literal similarity, but also considering semantic similarity [137,175,180].In terms of the ROUGE metric for multiple golden summaries, the Jackknifing procedure (similar to K-fold validation) has been introduced [91].The  best scores are computed from sets composed of -1 reference summaries and the final ROUGE-N is the average of  scores.This procedure can also be applied to ROUGE-L, ROUGE-W and ROUGE-S.</p>
<p>Other Evaluation Metrics</p>
<p>Besides ROUGE-based [91] metrics, other evaluation metrics for MDS exist, but have received less attention than ROUGE.We hope this section will give researchers and practitioners a holistic view of alternative evaluation metrics in this field.Based on the mode of summaries matching, we divide the evaluation metrics into two groups: lexical matching metrics and semantic matching metrics.Lexical Matching Metrics.BLEU [119] is a commonly used vocabulary-based evaluation metric that provides a precision-based evaluation indicator, as opposed to ROUGE that mainly focuses on recall.Perplexity [69] is used to evaluate the quality of the language model by calculating the negative log probability of a word's appearance.A low perplexity on a test dataset is a strong indicator of a summary's high grammatical quality because it measures the probability of words appearing in sequences.Based on Pyramid [114] calculation, the abstract sentences are manually divided into several Summarization Content Units (SCUs), each representing a core concept formed from a single word or phrase/sentence.After sorting SCUs in order of importance to form the Pyramid, the quality of automatic summarization is evaluated by calculating the number and importance of SCUs included in the document [115].Intuitively, more important SCUs exist at higher levels of the pyramid.Although Pyramid shows strong correlation with human judgment, it requires professional annotations to match and evaluate SCUs in generated and golden summaries.Some recent works focus on the construction of Pyramid [47,60,120,139,163].Responsiveness [96] measures content selection and linguistic quality of summaries by directly rating scores.Additionally, the assessments are calculated without reference to model summaries.Data Statistics [54] contain three evaluation metrics: extractive fragment coverage measures the novelty of generated summaries by calculating the percentage of words in the summary that are also present in source documents; extractive fragment density measures the average length of the extractive block to which each word in the summary belongs; and compression ratio compares the word numbers in the source documents and generated summary.Semantic Matching Metrics.METEOR (Metric for Evaluation of Translation with Explicit Ordering) [9] is an improvement to BLEU.The main idea behind METEOR is that while candidate summaries can be correct with similar meanings, they are not exactly matched with references.In such a case, WordNet10 is introduced to expand the synonym set, and the word form is also taken into account.SUPERT [48] is an unsupervised MDS evaluation metric that measures the semantic similarity between the pseudo-reference summary and the machine-generated summary.SUPERT obviates the need for human annotations by not referring to golden summaries.Contextualized embeddings and soft token alignment techniques are leveraged to select salient information from the input documents to evaluate summary quality.Preferences based Metric [188] is a pairwise sentence preference-based evaluation model and it does not depend on the golden summaries.The underlying premise is to ask annotators about their pair-wise preferences rather than writing complex golden summaries, and are much easier and faster to obtain than traditional reference summary-based evaluation models.BERTScore [175] computes a similarity score for each token within the candidate sentence and the reference sentence.It measures the soft overlap of two texts' BERT embeddings.MoverScore [180] adopts a distance to evaluate the agreement between two texts in the context of BERT and ELMo word embeddings.This proposed metric has a high correlation with human judgment of text quality by adopting earth mover's distance.Importance [125] is a simple but rigorous evaluation metric from the aspect of information theory.It is a final indicator calculated from the three aspects: Redundancy, Relevance, and Informativeness.A good summary should have low Redundancy and high Relevance and high Informativeness.The cluster of Human Evaluation is used to supplement automatic evaluation on relatively small instances.</p>
<p>Annotators evaluate the quality of machine-generated summaries by rating Informativeness, Fluency, Conciseness, Readability, Relevance.Model ratings are usually computed by averaging the rating on all selected summary pairs.</p>
<p>Discussion</p>
<p>We summarize the advantages and disadvantages of above-mentioned evaluation metrics in Table 3.Although there are many evaluation metrics for MDS, the indicators of the ROUGE series are generally accepted by the summarization community.Almost all the research works utilize ROUGE for evaluation, while other evaluation indicators are just for assistance currently.Among the ROUGE family, ROUGE-1, ROUGE-2 and ROUGE-L are the most commonly used evaluation metrics.In addition, there are plenty of existing evaluation metrics in other natural language processing tasks that could be potentially adjusted for MDS tasks, such as efficiency, effectiveness and coverage from information retrieval.</p>
<p>DATASETS</p>
<p>Compared to SDS tasks, large-scale MDS datasets, which contain more general scenarios with many downstream tasks, are relatively scarce.In this section, we present our investigation on the 10 most representative datasets commonly used for MDS and its variant tasks.4, the DUC and TAC datasets provide small datasets for model evaluation that only include hundreds of news documents and human-annotated summaries.Of note, the first sentence in a news item is usually information-rich that renders bias in the news datasets, so it fails to reflect the structure of natural documents in daily lives.These two datasets are on a relatively small scale and not ideal for large-scale deep neural based MDS model training and evaluation.OPOSUM.OPOSUM [4] collects multiple reviews of six product domains from Amazon.This dataset not only contains multiple reviews and corresponding summaries but also products' domain and polarity information.The latter information could be used as the auxiliary supervision signals.</p>
<p>WikiSum.WikiSum [93] targets abstractive MDS.For a specific Wikipedia theme, the documents cited in Wikipedia articles or the top-10 Google search results (using the Wikipedia theme as query) are seen as the source documents.Golden summaries are the real Wikipedia articles.However, some of the URLs are not available and can be identical to each other in parts.To remedy these problems, Liu et al. [94] cleaned the dataset and deleted duplicated examples, so here we report statistical results from [94].</p>
<p>Multi-News.Multi-News [43] is a relatively large-scale dataset in the news domain; the articles and human-written summaries are all from the Web 13 .This dataset includes 56,216 article-summary pairs and contain trace-back links to the original documents.Moreover, the authors compared the Multi-News dataset with prior datasets in terms of coverage, density, and compression, revealing that this dataset has various arrangement styles of sequences.</p>
<p>Opinosis.The Opinosis dataset [46] contains reviews of 51 topic clusters collected from TripAdvisor 14 , Amazon 15 , and Edmunds 16 .For each topic, approximately 100 sentences on average are provided and the reviews are fetched from different sources.For each cluster, five professional written golden summaries are provided for the model training and evaluation.Rotten Tomatoes.The Rotten Tomatoes dataset [157] consists of the collected reviews of 3,731 movies from the Rotten Tomato website 17 .The reviews contain both professional critics and user comments.For each movie, a one-sentence summary is created by professional editors.Yelp.Chu et al. [30] proposed a dataset named Yelp based on the Yelp Dataset Challenge.This dataset includes multiple customer reviews with five-star ratings.The authors provided 100 manualwritten summaries for model evaluation using Amazon Mechanical Turk (AMT), within which every eight input reviews are summarized into one golden summary.Scisumm.Scisumm dataset [166] is a large, manually annotated corpus for scientific document summarization.The input documents are a scientific publication, called the reference paper, and multiple sentences from the literature that cite this reference paper.In the SciSumm dataset, the 1,000 most cited papers from the ACL Anthology Network [128] are treated as reference papers, and an average 15 citation sentences are provided after cleaning.For each cluster, one golden summary is created by five NLP-based Ph.D. students or equivalent professionals.WCEP.The Wikipedia Current Events Portal dataset (WCEP) [50] contains human-written summaries of recent news events.Similar articles are provided by searching similar articles from Common Crawl News dataset 18 to extend the inputs to obtain large-scale news articles.Overall, the WCEP dataset has good alignment with the real-world industrial use cases.</p>
<p>Multi-XScience.The source data of Multi-XScience [97] are from Arxiv and Microsoft academic graphs and this dataset is suitable for abstractive MDS.Multi-XScience contains fewer positional and extractive biases than WikiSum and Multi-News datasets, so the drawback of obtaining higher scores from a copy sentence at a certain position can be partially avoided.</p>
<p>Datasets for MDS Variants.The representative query-oriented MDS datasets are Debatepedia [112], AQUAMUSE [79], and QBSUM [179].The representative dialogue summarization datasets are DIALOGSUM [25], AMI [23], MEDIASUM [185], and QMSum [183].RTS is a track at the Text Retrieval Conference (TREC) which provides several RTS datasets 19 .Tweet Contextualization track [12] (2012-2014) is derived from the INEX 2011 Question Answering Track, that focuses on more NLP-oriented tasks and moves to MDS.</p>
<p>Discussion.  [90,155,166,167].To this end, a promising and important direction would be to design a better mechanism to introduce different graph structures [29] or linguistic knowledge [14,100], possibly into the attention mechanism in deep learning based models, to capture cross-document relations and to facilitate summarization.</p>
<p>Creating More High-quality Datasets for MDS</p>
<p>Benchmark datasets allow researchers to train, evaluate and compare the capabilities of different models on the same stage.High-quality datasets are critical to develop MDS tasks.DUC and TAC, the most common datasets used for MDS tasks, have a relatively small number of samples so are not very suitable for training DNN models.In recent years, some large datasets have been proposed, including WikiSum [93], Multi-News [43], and WCEP [50], but more efforts are still needed.Datasets with documents of rich diversity, with minimal positional and extractive biases are desperately required to promote and accelerate MDS research, as are datasets for other applications such as summarization of medical records or dialogue [108], email [149,170], code [103,131], software project activities [2], legal documents [73], and multi-modal data [85].The development of large-scale cross-task datasets will facilitate multi-task learning [159].However, the datasets of MDS combining with text classification, question answering, or other language tasks have seldom been proposed in the MDS research community, but these datasets are essential and widely employed in industrial applications.</p>
<p>Improving Evaluation Metrics for MDS</p>
<p>To our best knowledge, there are no evaluation metrics specifically designed for MDS models -SDS and MDS models share the same evaluation metrics.New MDS evaluation metrics should be able to:</p>
<p>(1) evaluating the relations between the different input documents in the generated summary; (2) measuring to what extent the redundancy in input documents is reduced; and (3) judging whether the contradictory information across documents is reasonably handled.A good evaluation indicator is able to reflect the true performance of an MDS model and guide design of improved models.However, current evaluation metrics [42] still have several obvious defects.For example, despite the effectiveness of commonly used ROUGE metrics, they struggle to accurately measure the semantic similarity between a golden and generated summary because ROUGE-based evaluation metrics only consider vocabulary-level distances; as such, even if a ROUGE score improves, it does not necessarily mean that the summary is of a higher quality and so is not ideal for model training.Recently, some works extend ROUGE along with WordNet [137] or pre-trained LMs [175] to alleviate these drawbacks.It is challenging to propose evaluation indicators that can reflect the true quality of generated summaries comprehensively and as semantically as human raters.Another frontline challenge for evaluation metrics research is unsupervised evaluation, being explored by a number of recent studies [48,143].</p>
<p>Reinforcement Learning for MDS</p>
<p>Reinforcement learning [107] is a cluster of algorithms based on dynamic programming according to the Bellman Equation to deal with sequential decision problems, where state transition dynamics of the environment are provided in advance.Several existing works [110,123,165] model the document summarization task as a sequential decision problem and adopt reinforcement learning to tackle the task.Although deep reinforcement learning for SDS has made great progress, we still face challenges to adapt existing SDS models to MDS, as the latter suffer from a large state, action space, and problems with high redundancy and contradiction [102].Additionally, current summarization methods are based on model-free reinforcement learning algorithms, in which the model is not aware of environment dynamics but continuously explores the environment through simple trial-and-error strategies, so they inevitably suffer from low sampling efficiencies.Nevertheless, the model-based approaches can leverage data more efficiently since they update models upon the prior to the environment.In this case, data-efficient reinforcement learning for MDS could potentially be explored in the future.</p>
<p>7.5 Pre-trained Language Models for MDS In many NLP tasks, the limited labeled corpora are not adequate to train semantic-rich word vectors.Using large-scale, unlabeled, task-agnostic corpora for pre-training can enhance the generalization ability of models and accelerate convergence of networks [106,124].At present, pre-trained LMs have led to successes in many deep learning based NLP tasks.Among the reviewed papers [82,90,182], multiple works adopt pre-trained LMs for MDS and achieve promising improvements.Applying pre-trained LMs such as BERT [34], GPT-2 [129], GPT-3 [17], XLNet [164], ALBERT [81],</p>
<p>or T5 [130], and fine-tuning them on a variety of downstream tasks allows the model to achieve faster convergence speed and can improve model performance.MDS requires the model to have a strong ability to process long sequences.It is promising to explore powerful LMs specifically targeting long sequence input characteristics and avoiding quadratic memory growth for selfattention mechanism, such as Longformer [13], REFORMER [76], or Big Bird [169] with pre-trained models.Also, tailor-designed pre-trained LMs for summarization have not been well-explored, e.g., using gap sentences generation is more suitable than using masked language model [172].Most MDS methods focus on combining pre-trained LMs in encoder and, as for capturing cross-document relations, applying them in decoder is also a worthwhile direction for research [122].</p>
<p>Creating Explainable Deep Learning Model for MDS</p>
<p>Deep learning models can be regarded as black boxes with high non-linearity; it is extremely challenging to understand the detailed transformation inside them.However, an explainable model can reveal how it generates candidate summaries -to distinguish whether the model has learned the distribution of generating condensed and coherent summaries from multiple documents without bias -and is thus crucial for model building.Recently, a large amount of researches into explainable models [132,173] have proposed easing the non-interpretable concern of deep neural networks, within which model attention plays an especially important role in model interpretation [136,184].While explainable methods have been intensively researched in NLP [65,80], studies into explainable MDS models are relatively scarce and would benefit from future development.</p>
<p>Adversarial Attack and Defense for MDS</p>
<p>Adversarial examples are strategically modified samples that aim to fool deep neural networks based models.An adversarial example is created via the worst-case perturbation of the input to which a robust DNN model would still assign correct labels, while a vulnerable DNN model would have high confidence in the wrong prediction.The idea of using adversarial examples to examine the robustness of a DNN model originated from research in Computer Vision [144] and was introduced in NLP by Jia et al. [70].An essential purpose for generating adversarial examples for neural networks is to utilize these adversarial examples to enhance the model's robustness [52].Therefore, research on adversarial examples not only helps identify and apply a robust model but also helps to build robust models for different tasks.Following the pioneering work proposed by Jia et al. [70], many attack methods have been proposed to address this problem in NLP applications [176] with limited research for MDS [27].It is worth filling this gap by exploring existing and developing new, adversarial attacks on the state-of-the-art DNN-based MDS models.</p>
<p>Multi-modality for MDS</p>
<p>Existing multi-modal summarization is based on non-deep learning techniques [66][67][68]86], leaving a huge opportunity to exploit deep learning techniques for this task.Multi-modal learning has led to successes in many deep learning tasks, such as Visual Language Navigation [156] and Visual Question Answering [6].Combining MDS with multi-modality has a range of applications:</p>
<p>• text + image: generating summaries with pictures and texts for documents with pictures.This kind of multi-modal summary can improve the satisfaction of users [187]; • text + video: based on the video and its subtitles, generating a concise text summary that describes the main context of video [118].Movie synopsis is one application; • text + audio: generating short summaries of audio files that people could quickly preview without actually listening to the entire audio recording [41].Deep learning is well-suited for multi-modal tasks [56], as it is able to effectively capture highly nonlinear relationships between images, text or video data.Existing MDS models target at dealing with textual data only.Involving richer modalities based on textual data requires models to embrace larger capacity to handle these multi-modal data.The big models such as UNITER [26]</p>
<p>Fig. 2 .
2
Fig. 2. The Processing Framework of Text Summarization.Each of the highlighted steps (the one with triangle mark) indicates the differences between SDS and MDS.</p>
<p>Fig. 3 .
3
Fig. 3. Summarization Construction Types for Text Summarization.</p>
<p>Fig. 4 .
4
Fig. 4. The Methods of Hierarchical Concatenation.</p>
<p>Fig. 5 .
5
Fig. 5. Network Design Strategies.</p>
<p>[18] developed a TCSum model incorporating an auxiliary text classification sub-task into MDS to introduce more supervision signals.The text classification model uses a CNN descriptor to project documents onto the distributed representation, and to classify input documents into different categories.The summarization model shares the projected sentence embedding from the classification model, and the TCSum model then chooses the corresponding category based transformation matrices according to classification results to transform the sentence embedding into the summary embedding.</p>
<p>Publication, Vol. 1 ,
1
No. 1, Article .Publication date: December 2021.</p>
<p>(a)).Multiple concatenated documents are input through DNN based models to extract features.Word-level, sentence-level or document-level representation is used to generate the downstream summary or select sentences.Naive networks represent the most naive model that lays the foundation for other strategies.
C.Ma et al.Doc DocDNN based ModelProcessing RepresentationSummaryDoc 2 Doc 1DNN based Model 1 DNN based Model 2Processing Representation 1 Representation 2 ProcessingEnsembleSummary(a) Naive Networks(b) Ensemble NetworksDoc DocDNN based Model DNN based Model Share the weights Representation Other Processing RepresentationSummary Other OutputDoc 2 Doc 1DNN based ModelProcessing Summary RepresentationDNN based ModelDocuments Reconstruction(c) Auxiliary Task Networks(d) Reconstruction NetworksRelation GraphProcessingDoc DocDNN Based ModelRepresentation Other RepresentationFusionSummaryDoc 2 Doc 1Model based DNNProcessing RepresentationGNNSummary(e) Fusion Networks(f) Graph Neural NetworksEncoderDecoderDoc DocDNN based ModelRepresentation ProcessingDNN based ModelSummaryDoc 1 Doc 2Pre-trained Language ModelsDNN based ModelProcessing RepresentationSummary(g) Encoder-Decoder Structure(h) Pre-trained Language ModelsDoc 2 Doc 1DNN based ModelLow-level Representation Processingbased Model DNNRepresentation High-level ProcessingSummary(i) Hierarchical Networks</p>
<p>Table 1 .
1
Multi-document Summarization Models based on Graph Neural Networks.
ModelsNodesEdgesEdge WeightsGNN MethodsHeterDoc-SumGraph [155]word, sentence, documentword-sentence, word-documentTF-IDFGraph Attention NetworksGraph-based Neural MDS [167]sentencesentence-sentencePersonalized Discourse GraphGraph Convolutional NetworksSemSentSum [5]sentencesentence-sentenceCosine Similarity Graph Edge Removal MethodGraph Convolutional NetworksScisummNet [166]sentencesentence-sentence Cosine Similarity GraphGraph Convolutional Networks</p>
<p>Table 2 .
2
Deep Learning based Methods."Ext", "Abs" and "Hyd" mean extractive, abstractive and hybrid respectively; "FC" and "HC" represent Flat Concatenate, Hierarchical Concatenate respectively.
ConstructionDocument-levelComparison ofMethodsWorksTypesRelationshipDL based techniquesExt Abs Hyb FCHCPros and ConsMeanSum [30]✓✓Zhang et al. [171]✓✓Pros: Can capture sequentialSTDS [181]✓✓relations and syntactic/semanticParaFuse_doc [111]✓✓information from wordRNNR2N2 [19] CondaSum [3]✓✓✓✓sequences Cons: Not easy to parallelC-Attention [89]✓✓computing; Highly dependingWang et al.[157]✓✓on results from theRL-MMR [102]✓✓previous stepsCoavoux et al.[32]✓✓MV-CNN [178]✓✓TCSum [18]✓✓Pros: Good parallel computing;CNNCNNLM [168]✓✓Cons: Not good at processingPriorSum [20]✓✓sequential dataAngelidis et al.[4]✓✓Yasunaga et al.[167] ✓✓Pros: Can capture cross-documentGNNSemSentSum [5] Scisummnet [166]✓ ✓✓ ✓and in-document relations Cons: Inefficient whenHDSG [155]✓✓dealing with large graphsPGPG-MMR [83] Hi-MAP [43]✓ ✓✓ ✓Pros: Low redundancy Cons: Hard to trainHT [94]✓✓Pros: Good performance; GoodMGSum [71]✓✓✓parallel computing; Can captureTransformerFewSum [16] GraphSum [90]✓✓ ✓✓✓cross-document and in-document relationsBart-Long [122]✓✓Cons: Time-consuming; ProblemsWikiSum [93]✓✓with position encodingCho et al.[28]✓✓Pros: Combines the advantagesDeep Hybid ModelGT-SingPairMix [82]✓✓of different DL modelsHNet [141]✓✓Cons: Computationally intensive</p>
<p>Table 3 .
3
Advantages and disadvantages of different evaluation metrics.
Evaluation MetricsAdvantagesDisadvantages• Widely used• Cannot measure textsROUGE• Intuitivesemantically• Easily computed• Exact matchingLexical MatchingBLEU• Intuitive • Easily computed • High correlations with human judgments• Cannot measure texts semantically • Cannot deal with languages lacking word boundariesMetricsPerplexity• Easily computed • Intuitive• Sensitive to certain symbols and wordsPyramid• High correlations with human judgments• Requires manually extraction of units • Bias results easily• Consider both content andResponsivenesslinguistic quality • Can be calculated without• Not widely adoptedreferenceData• Can measure the density• Cannot measure textsStatisticsand coverage of summarysemanticallyMEREOR• Consider non-exact matching • Sensitive to lengthSUPERT• Can measuring texts semantic similarity• Not widely adoptedPreferences• Does not depend on the• Require humanbased Metricgolden summariesannotationsSemantic MatchingBERTScore• Semantically measure texts to some extent • Mimic human evaluation• High computational demandsMetrics• Semantically measure texts toMoverScoresome extent • More similar to human evaluation by adopting earth• High computational demandsmover's distanceImportance• Combining redundancy, relevance and informativeness • Theoretically supported• Non-trivial for implementationHuman• Can accurately and• Require humanEvaluationsemantically measure textsannotations</p>
<p>DUC &amp; TAC.DUC 11 (Document Understanding Conference) provides official text summarization competitions each year from 2001-2007 to promote summarization research.DUC changed its name to Text Analysis Conference (TAC) 12 in 2008.Here, the DUC datasets refer to the data collected from 2001-2007; the TAC datasets refer to the dataset after 2008.Both DUC and TAC datasets are from the news domains, including various topics such as politics, natural disaster and biography.Nevertheless, as shown in Table</p>
<p>Table 4 .
4
Comparison of Different Datasets.In the table, "Ave", "Summ", "Len", "bus", "rev" and "#" represent average, summary, length, business, reviews and numbers respectively; "Docs" and "sents" mean documents and sentences respectively.
DatasetsCluster #Document #Summ #Ave Summ LenTopicDUC0130309 docs60 summ100 wordsNewsDUC0259567 docs116 summ100 wordsNewsDUC0330298 docs120 summ100 wordsNewsDUC045010 docs / cluster200 summ665 bytesNewsDUC055025-50 docs / cluster140 summ250 wordsNewsDUC065025 docs / cluster4 summ / cluster250 wordsNewsDUC074525 docs / cluster4 summ / cluster250 wordsNewsTAC 20084810 docs / cluster4 summ / cluster100 wordsNewsTAC 20094410 docs / cluster4 summ / cluster100 wordsNewsTAC 20104610 docs / cluster4 summ / cluster100 wordsNewsTAC 20114410 docs / cluster4 summ / cluster100 wordsNewsOPOSUM60600 rev1 summ / cluster100 wordsAmazon reviewsWikiSum-train / val / test 1579360 / 38144 / 382051 summ / cluster 139.4 tokens/ summ Wikipediatrain / val / test263.66 words / summMulti-News-44972 / 5622 / 56221 summ / cluster9.97 sents / summNews2-10 docs / cluster262 tokens / summOpinosis516457 rev5 summ / cluster-Site reviewsRotten Tomatoes373199.8 rev / cluster1 summ / cluster 19.6 tokens / summMovie reviewsYelp-train / val / test bus: 10695 / 1337 / 1337 rev: 1038184 / 129856 / 129840--Customer reviewsScisumm100021 -928 cites / paper 15 sents / refer1 summ / cluster151 wordsScience PaperWCEP10200235 docs / cluster1 summ / cluster32 wordsWikipediaMulti-XScience-train / val / test 30369 / 5066 / 50931 summ / cluster 116.44 words / summScience Paper</p>
<p>[90]]4compares 20 MDS datasets based on the numbers of clusters and documents; the number and the average length of summaries; and the field to which the dataset belongs.Currently, the main areas covered by the MDS datasets are news (60%), scientific papers (10%) and Wikipedia (10%).In early development of the MDS tasks, most studies were performed on the DUC and TAC datasets.However, the size of these datasets is relatively small, and thus not highly suitable for training deep neural network models.Datasets on news articles are also common, but the structure of news articles (highly compressed information in the first paragraph or first sentence of each paragraph) can cause positional and extractive biases during training.In recent years, large-scale datasets such as WikiSum and Multi-News datasets have been developed and used by researchers to meet training requirements, reflecting the rising trend of data-driven approaches.7 FUTURE RESEARCH DIRECTIONS AND OPEN ISSUESAlthough existing works have established a solid foundation for MDS it is a relatively understudied field compared with SDS and other NLP topics.Summarizing on multi-modal data, medical records, codes, project activities and MDS combining with Internet of Things[177]have still received less attention.Actually, MDS techniques are beneficial for a variety of practical applications, including generating Wikipedia articles, summarizing news, scientific papers, and product reviews, and individuals, industries have a huge demand for compressing multiple related documents into highquality summaries.This section outlines several prospective research directions and open issues that we believe are critical to resolve in order to advance the field.7.1 Capturing Cross-document Relations for MDSCurrently, many MDS models still center on simple concatenation of input documents into a flat sequence, ignoring cross-document relations.Unlike SDS, MDS input documents may contain redundant, complementary, or contradictory information[126].Discovering cross-document relations, which can assist models to extract salient information, improve the coherence and reduce redundancy of summaries[90].Research on capturing cross-document relations has begun to gain momentum in the past two years; one of the most widely studied topics is graphical models, which can easily be combined with deep learning based models such as graph neural networks and Transformer models.Several existing works indicate the efficacy of graph-based deep learning models in capturing semantic-rich and syntactic-rich representation and generating high-quality summaries</p>
<p>, VisualBERT Publication, Vol. 1, No. 1, Article .Publication date: December 2021.</p>
<p>Publication, Vol. 1, No. 1, Article . Publication date: December 2021.
http://trecrts.github.io/ Publication, Vol. 1, No. 1, Article . Publication date: December 2021.
https://wordnet.princeton.edu/ Publication, Vol. 1, No. 1, Article . Publication date: December 2021.
http://duc.nist.gov/
http://www.nist.gov/tac/
http://newser.com
https://www.tripadvisor.com/
https://www.amazon.com.au/
https://www.edmunds.com/
http://rottentomatoes.com
https://commoncrawl.org/2016/10/news-dataset-available/ Publication, Vol. 1, No. 1, Article . Publication date: December 2021.
http://trecrts.github.io/ Publication, Vol. 1, No. 1, Article . Publication date: December
.
CONCLUSIONIn this article, we have presented the first comprehensive review of the most notable works to date on deep learning based multi-document summarization (MDS).We propose a taxonomy for organizing and clustering existing publications and devise the network design strategies based on the state-of-the-art methods.We also provide an overview of the existing multi-document objective functions, evaluation metrics and datasets, and discuss some of the most pressing open problems and promising future extensions in MDS research.We hope this survey provides readers with a comprehensive understanding of the key aspects of the MDS tasks, clarifies the most notable advances, and sheds light on future studies.
deserve more attention in multi-modality MDS tasks. However, at present, there is little multimodal research work based on MDS; this is a promising, but largely under-explored, area where more studies are expected. </p>
<p>Summarization from Medical Documents: A Survey. Stergos Afantenos, Vangelis Karkaletsis, Panagiotis Stamatopoulos, Artificial Intelligence in Medicine. 332005</p>
<p>Human-Like Summaries from Heterogeneous and Time-Windowed Software Development Artefacts. Mahfouth Alghamdi, Christoph Treude, Markus Wagner, Proceedings of the 6th International Conference of Parallel Problem Solving from Nature (PPSN 2020). the 6th International Conference of Parallel Problem Solving from Nature (PPSN 2020)Leiden, The Netherlands2020</p>
<p>Informative and Controllable Opinion Summarization. Reinald Kim, Amplayo , Mirella Lapata, Proceedings of the 16th Conference of the European Chapter. the 16th Conference of the European ChapterOnline2021. 2021</p>
<p>Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They are Both Weakly Supervised. Stefanos Angelidis, Mirella Lapata, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, Belgium2018. 2018</p>
<p>Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization. Diego Antognini, Boi Faltings, Proceedings of the 2nd Workshop on New Frontiers in Summarization. the 2nd Workshop on New Frontiers in SummarizationChinaHongkong2019. 2019</p>
<p>VQA: Visual Question Answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, Devi Parikh, Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV 2015). the 2015 IEEE International Conference on Computer Vision (ICCV 2015)Santiago, Chile2015</p>
<p>Latent Dirichlet Allocation and Singular Value Decomposition based Multi-document Summarization. Rachit Arora, Balaraman Ravindran, Proceedings of the 2008 Eighth IEEE International Conference on Data Mining (ICDM 2008). the 2008 Eighth IEEE International Conference on Data Mining (ICDM 2008)Pisa, Italy2008</p>
<p>Neural Machine Translation by Jointly Learning to Align and Translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Proceedings of the 3rd International Conference on Learning Representations (ICLR 2015). the 3rd International Conference on Learning Representations (ICLR 2015)San Diego, CA, United States2015</p>
<p>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization2005. 2005</p>
<p>Multi-document Summarization Exploiting Frequent Itemsets. Elena Baralis, Luca Cagliero, Saima Jabeen, Alessandro Fiori, Proceedings of the 27th Annual ACM Symposium on Applied Computing (SAC2012). the 27th Annual ACM Symposium on Applied Computing (SAC2012)Riva, Italy2012</p>
<p>Machine-made Index for Technical Literature -An Experiment. Phyllis B Baxendale, IBM Journal of Research and Development. 21958</p>
<p>Patrice Bellot, Véronique Moriceau, Josiane Mothe, Eric Sanjuan, Xavier Tannier, INEX Tweet Contextualization task: Evaluation, results and lesson learned. Information Processing and Management. 2016. 201652</p>
<p>Longformer: The Long-document Transformer. Iz Beltagy, Matthew E Peters, Arman Cohan, arXiv:2004.051502020arXiv preprint</p>
<p>Abstractive Multi-Document Summarization via Phrase Selection and Merging. Lidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei Guo, Rebecca J Passonneau, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language ProcessingBeijing, China2015. 2015</p>
<p>Unsupervised Multi-Document Opinion Summarization as Copycat-Review Generation. Arthur Bražinskas, Mirella Lapata, Ivan Titov, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020). the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020)Online2019</p>
<p>Few-Shot Learning for Opinion Summarization. Arthur Brazinskas, Mirella Lapata, Ivan Titov, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingOnline2020. 2020</p>
<p>. Article . Publication date. 11December 2021Publication</p>
<p>Language Models Are Few-shot Learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Online2020. 2020. 2020</p>
<p>Improving Multi-document Summarization via Text Classification. Ziqiang Cao, Wenjie Li, Sujian Li, Furu Wei, Proceedings of the 31st AAAI Conference on Artificial Intelligence. the 31st AAAI Conference on Artificial IntelligenceSan Francisco, United States2017. 2017</p>
<p>Ranking with Recursive Neural Networks and its Application to Multi-document Summarization. Ziqiang Cao, Furu Wei, Li Dong, Sujian Li, Ming Zhou, Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI 2015). the 29th AAAI Conference on Artificial Intelligence (AAAI 2015)Austin, United States2015</p>
<p>Learning Summary Prior Representation for Extractive Summarization. Ziqiang Cao, Furu Wei, Sujian Li, Wenjie Li, Ming Zhou, Houfeng Wang, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing2015. 2015</p>
<p>. China Beijing, </p>
<p>The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries. Jaime G Carbonell, Jade Goldstein, Proceedings of the 21st Annual International Conference on Research and Development in Information Retrieval. the 21st Annual International Conference on Research and Development in Information RetrievalMelbourne, Australia1998. 1998</p>
<p>Summarizing Email Conversations with Clue Words. Giuseppe Carenini, Raymond T Ng, Xiaodong Zhou, Proceedings of the 16th International Conference on World Wide Web. the 16th International Conference on World Wide WebBanff, CanadaWWW2007. 2007</p>
<p>The AMI Meeting Corpus: A Pre-announcement. Jean Carletta, Simone Ashby, Sebastien Bourban, Mike Flynn, Maël Guillemot, Thomas Hain, Jaroslav Kadlec, Vasilis Karaiskos, Wessel Kraaij, Melissa Kronenthal, Guillaume Lathoud, Mike Lincoln, Agnes Lisowska, Iain Mccowan, Wilfried Post, Dennis Reidsma, Pierre Wellner, Machine Learning for Multimodal Interaction. Edinburgh, UK2005Second International Workshop (MLMI 2005)</p>
<p>Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization. Jiaao Chen, Diyi Yang, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingOnline2020. 2020</p>
<p>DialogSumm: A Real-Life Scenario Dialogue Summarization Dataset. Yulong Chen, Yang Liu, Liang Chen, Yue Zhang, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL 2021). the 59th Annual Meeting of the Association for Computational Linguistics (ACL 2021)Online2021</p>
<p>Uniter: Universal Image-text Representation Learning. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, European conference on computer vision. Springer2020</p>
<p>Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples. Minhao Cheng, Jinfeng Yi, Pin-Yu Chen, Huan Zhang, Cho-Jui Hsieh, Proceedings of the 34th AAAI Conference on Artificial Intelligence (AAAI 2020). the 34th AAAI Conference on Artificial Intelligence (AAAI 2020)New York, NY, United States2020</p>
<p>Improving the Similarity Measure of Determinantal Point Processes for Extractive Multi-Document Summarization. Sangwoo Cho, Logan Lebanoff, Hassan Foroosh, Fei Liu, Proceedings of the 57th Conference of the Association for Computational Linguistics. the 57th Conference of the Association for Computational LinguisticsFlorence, Italy2019. 2019</p>
<p>Towards Coherent Multi-document Summarization. Janara Christensen, Stephen Soderland, Oren Etzioni, Proceedings of the 2013 conference of the North American chapter of the association for computational linguistics: Human language technologies. the 2013 conference of the North American chapter of the association for computational linguistics: Human language technologies2013</p>
<p>MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization. Eric Chu, Peter J Liu, Proceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine LearningLong Beach, United States2019. 2019</p>
<p>Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, Yoshua Bengio, arXiv:1412.3555Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. 2014arXiv preprint</p>
<p>Unsupervised Aspect-Based Multi-Document Abstractive Summarization. Maximin Coavoux, Hady Elsahar, Matthias Gallé, Proceedings of the 2nd Workshop on New Frontiers in Summarization. the 2nd Workshop on New Frontiers in SummarizationHong Kong, China2019</p>
<p>Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel Kuksa, Natural Language Processing. 201112</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, United States2019. 2019</p>
<p>Fast and Robust Neural Network Joint Models for Statistical Machine Translation. Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, John Makhoul, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. the 52nd Annual Meeting of the Association for Computational LinguisticsBaltimore, United States2014. 2014</p>
<p>Unified Language Model Pre-training for Natural Language Understanding and Generation. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. 2019. 2019. 2019</p>
<p>Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts. Dos Cicero, Santos , Maira Gatti, Proceedings of the International Conference on Computational Linguistics (COLING. the International Conference on Computational Linguistics (COLINGDublin, Ireland2014. 2014</p>
<p>. Article . Publication date. 11December 2021Publication</p>
<p>Automatic Text Summarization: A Comprehensive Survey. S Wafaa, Cherif R El-Kassas, Ahmed A Salama, Hoda K Rafea, Mohamed, Expert Systems with Applications. 1651136792021. 2021</p>
<p>Generating Medical Reports from Patient-doctor Conversations Using Sequence-to-sequence Models. Seppo Enarvi, Marilisa Amoia, Miguel Del-Agua Teba, Brian Delaney, Frank Diehl, Stefan Hahn, Kristina Harris, Liam Mcgrath, Yue Pan, Joel Pinto, Proceedings of the first workshop on natural language processing for medical conversations. the first workshop on natural language processing for medical conversations2020</p>
<p>Lexrank: Graph-based Lexical Centrality as Salience in Text Summarization. Günes Erkan, Dragomir R Radev, Journal of Artificial Intelligence Research. 222004</p>
<p>Multimodal Summarization of Meeting Recordings. Berna Erol, Dar-Shyang Lee, Jonathan J Hull, Proceedings of the 2003 IEEE International Conference on Multimedia and Expo (ICME 2003). the 2003 IEEE International Conference on Multimedia and Expo (ICME 2003)Baltimore, United States2003</p>
<p>Summeval: Re-evaluating Summarization Evaluation. Wojciech Alexander R Fabbri, Bryan Kryściński, Caiming Mccann, Richard Xiong, Dragomir Socher, Radev, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model. Alexander R Fabbri, Irene Li, Tianwei She, Suyi Li, Dragomir R Radev, Proceedings of the 57th Conference of the Association for Computational Linguistics. the 57th Conference of the Association for Computational LinguisticsFlorence, Italy2019. 2019</p>
<p>Dialogue Discourse-Aware Graph Convolutional Networks for Abstractive Meeting Summarization. Xiachong Feng, Xiaocheng Feng, Bing Qin, Xinwei Geng, Ting Liu, Proceedings of the 30th International Joint Conference on Artificial Intelligence (IJCAI 2021). the 30th International Joint Conference on Artificial Intelligence (IJCAI 2021)2021</p>
<p>A Multi-document Summarization System based on Statistics and Linguistic Treatment. Rafael Ferreira, Luciano De Souza, Frederico Cabral, Rafael Dueire Freitas, Gabriel Lins, De França, Steven J Silva, Luciano Simske, Favaro, Expert Systems with Applications. 412014</p>
<p>Opinosis: A Graph Based Approach to Abstractive Summarization of Highly Redundant Opinions. Kavita Ganesan, Chengxiang Zhai, Jiawei Han, Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010). the 23rd International Conference on Computational Linguistics (COLING 2010)Beijing, China2010</p>
<p>Automated Pyramid Summarization Evaluation. Yanjun Gao, Chen Sun, Rebecca J Passonneau, Proceedings of the 23rd Conference on Computational Natural Language Learning. the 23rd Conference on Computational Natural Language LearningHong Kong, China2019. 2019</p>
<p>SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization. Yang Gao, Wei Zhao, Steffen Eger, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020). the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020)Online2020</p>
<p>Abstractive Summarization of Product Reviews Using Discourse Structure. Shima Gerani, Yashar Mehdad, Giuseppe Carenini, Raymond Ng, Bita Nejat, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. the 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, Qatar2014. 2014</p>
<p>A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal. Demian Gholipour Ghalandari, Chris Hokamp, The Nghia, John Pham, Georgiana Glover, Ifrim, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020). the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020)Online2020</p>
<p>Multi-document Summarization by Sentence Extraction. Jade Goldstein, O Vibhu, Jaime G Mittal, Mark Carbonell, Kantrowitz, Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Applied Natural Language Processing Conference. the Conference of the North American Chapter of the Association for Computational Linguistics: Applied Natural Language Processing ConferenceSeattle, United States2000. 2000</p>
<p>Explaining and Harnessing Adversarial Examples. Ian J Goodfellow, Jonathon Shlens, Christian Szegedy, Proceedings of the 3rd International Conference on Learning Representations (ICLR 2015). the 3rd International Conference on Learning Representations (ICLR 2015)San Diego, CA, United States2015</p>
<p>Globalizing BERT-based Transformer Architectures for Long Document Summarization. Quentin Grail, Julien Perez, Eric Gaussier, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume (EACL2021). the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume (EACL2021)Online2021</p>
<p>Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies. Max Grusky, Mor Naaman, Yoav Artzi, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, USA2018. 2018</p>
<p>Incorporating Copying Mechanism in Sequence-to-Sequence Learning. Jiatao Gu, Zhengdong Lu, Hang Li, O K Victor, Li, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, Germany2016. 2016</p>
<p>Deep Multimodal Representation Learning: A Survey. Wenzhong Guo, Jianwen Wang, Shiping Wang, IEEE Access. 72019. 2019</p>
<p>A Survey of Text Summarization Extractive Techniques. Vishal Gupta, Gurpreet Singh, Lehal , Journal of Emerging Technologies in Web Intelligence. 22010</p>
<p>Exploring Content Models for Multi-document Summarization. Aria Haghighi, Lucy Vanderwende, Proceedings of the 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. the 2009 Annual Conference of the North American Chapter of the Association for Computational LinguisticsBoulder, United States2009. 2009</p>
<p>Literature Review of Automatic Multiple Documents Text Summarization. Majharul Haque, Suraiya Pervin, Zerina Begum, International Journal of Innovation and Applied Studies. 32013</p>
<p>Automatic Pyramid Evaluation Exploiting Edu-based Extractive Reference Summaries. Tsutomu Hirao, Hidetaka Kamigaito, Masaaki Nagata, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Publication. the 2018 Conference on Empirical Methods in Natural Language Publication2018. December 20211Publication date</p>
<p>. C Ma, 2018Brussels, Belgium</p>
<p>Long Short-term Memory. Sepp Hochreiter, Jürgen Schmidhuber, Neural Computation. 91997</p>
<p>Multilayer feedforward networks are universal approximators. Kurt Hornik, Maxwell Stinchcombe, Halbert White, Neural networks. 21989. 1989</p>
<p>Opinion Mining from Online Hotel Reviews-A Text Summarization Approach. Ya-Han Hu, Yen-Liang Chen, Hui-Ling Chou, Information Processing &amp; Management. 532017</p>
<p>Zhiheng Huang, Wei Xu, Kai Yu, arXiv:1508.01991Bidirectional LSTM-CRF Models for Sequence Tagging. 2015arXiv preprint</p>
<p>Learning to Faithfully Rationalize by Construction. Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, Byron C Wallace, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020). the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020)Online2020</p>
<p>Text-image-video Summary Generation Using Joint Integer Linear Programming. Anubhav Jangra, Adam Jatowt, Mohammad Hasanuzzaman, Sriparna Saha, Advances in Information Retrieval. 120361902020. 2020</p>
<p>Multi-modal Summary Generation Using Multi-objective Optimization. Anubhav Jangra, Sriparna Saha, Adam Jatowt, Mohammad Hasanuzzaman, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval2020. 2020</p>
<p>Multi-Modal Supplementary-Complementary Summarization using Multi-Objective Optimization. Anubhav Jangra, Sriparna Saha, Adam Jatowt, Mohammed Hasanuzzaman, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR2021). the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR2021)Online2021</p>
<p>Perplexity -A Measure of the Difficulty of Speech Recognition Tasks. Fred Jelinek, Robert L Mercer, Lalit R Bahl, James K Baker, The Journal of the Acoustical Society of America. 621977</p>
<p>Adversarial Examples for Evaluating Reading Comprehension Systems. Robin Jia, Percy Liang, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, Denmark2017. 2017</p>
<p>Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization. Hanqi Jin, Tianming Wang, Xiaojun Wan, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020). the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020)Online2020</p>
<p>Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures. Anirudh Joshi, Namit Katariya, Xavier Amatriain, Anitha Kannan, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, (EMNLP2020). the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, (EMNLP2020)Online2020</p>
<p>Text Summarization from Legal Documents: A Survey. Ambedkar Kanapala, Sukomal Pal, Rajendra Pamula, Artificial Intelligence Review. 512019</p>
<p>Convolutional Neural Networks for Sentence Classification. Yoon Kim, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. the 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, Qatar2014. 2014</p>
<p>Semi-Supervised Classification with Graph Convolutional Networks. Thomas N Kipf, Max Welling, Proceedings of the 5th International Conference on Learning Representations. the 5th International Conference on Learning RepresentationsToulon, France2017. 2017</p>
<p>Reformer: The Efficient Transformer. Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya, Proceedings of the 8th International Conference on Learning Representations (ICLR 2020). the 8th International Conference on Learning Representations (ICLR 2020)EthiopiaAddis Ababa2020</p>
<p>How Domain Terminology Affects Meeting Summarization Performance. Jin Jia, Alexander Koay, Xiaojin Roustai, Dillon Dai, Alec Burns, Fei Kerrigan, Liu, Proceedings of the 28th International Conference on Computational Linguistics (COLING 2020). the 28th International Conference on Computational Linguistics (COLING 2020)Online2020</p>
<p>Imagenet Classification with Deep Convolutional Neural Networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Proceedings of the 26th Annual Conference on Neural Information Processing Systems. the 26th Annual Conference on Neural Information Processing SystemsLake Tahoe, United States2012. 2012</p>
<p>AQuaMuSe: Automatically Generating Datasets for Query-Based Multi-Document Summarization. Sayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, Eugene Ie, CoRR abs/2010.126942020. 2020</p>
<p>NILE : Natural Language Inference with Faithful Natural Language Explanations. Sawan Kumar, Partha P Talukdar, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020). the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020)Online2020</p>
<p>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, Proceedings of the 8th International Conference on Learning Representations (ICLR 2020). the 8th International Conference on Learning Representations (ICLR 2020)Addis Ababa, Ethiopia2020</p>
<p>Scoring Sentence Singletons and Pairs for Abstractive Summarization. Logan Lebanoff, Kaiqiang Song, Franck Dernoncourt, Soon Doo, Seokhwan Kim, Walter Kim, Fei Chang, Liu, Proceedings of the 57th Conference of the Association for Computational Linguistics. the 57th Conference of the Association for Computational LinguisticsFlorence, Italy2019. 2019</p>
<p>Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization. Logan Lebanoff, Kaiqiang Song, Fei Liu, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, Belgium2018. 2018</p>
<p>Gradient-based Learning Applied to Document Recognition. Yann Lecun, Léon Bottou, Yoshua Bengio, Patrick Haffner, Proc. IEEE. 861998</p>
<p>. Article . Publication date. 11December 2021Publication</p>
<p>Aspect-Aware Multimodal Summarization for Chinese E-Commerce Products. Haoran Li, Peng Yuan, Song Xu, Youzheng Wu, Xiaodong He, Bowen Zhou, The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI2020). New York, USA2020</p>
<p>Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video. Haoran Li, Junnan Zhu, Cong Ma, Jiajun Zhang, Chengqing Zong, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP2017). the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP2017)Copenhagen, Denmark2017</p>
<p>Visualbert: A Simple and Performant Baseline for Vision and Language. Liunian Harold, Li , Mark Yatskar, Cho-Jui Da Yin, Kai-Wei Hsieh, Chang, arXiv:1908.035572019. 2019arXiv preprint</p>
<p>Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset. Piji Li, Lidong Bing, Wai Lam, Proceedings of the Workshop on New Frontiers in Summarization. the Workshop on New Frontiers in SummarizationCopenhagen, Denmark2017. 2017</p>
<p>Cascaded Attention based Unsupervised Information Distillation for Compressive Summarization. Piji Li, Wai Lam, Lidong Bing, Weiwei Guo, Hang Li, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, Denmark2017. 2017</p>
<p>Leveraging Graph to Improve Abstractive Multi-Document Summarization. Wei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang, Junping Du, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020). the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020)Online2020</p>
<p>Rouge: A Package for Automatic Evaluation of Summaries. Chin-Yew Lin, Text Summarization Branches Out. 2004</p>
<p>Automatic Dialogue Summary Generation for Customer Service. Chunyi Liu, Peng Wang, Jiang Xu, Zang Li, Jieping Ye, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data MiningAnchorage, USA2019. 2019</p>
<p>Generating Wikipedia by Summarizing Long Sequences. J Peter, Mohammad Liu, Etienne Saleh, Ben Pot, Ryan Goodrich, Lukasz Sepassi, Noam Kaiser, Shazeer, Proceedings of the 6th International Conference on Learning Representations. the 6th International Conference on Learning RepresentationsVancouver,Canada2018. 2018</p>
<p>Hierarchical Transformers for Multi-Document Summarization. Yang Liu, Mirella Lapata, Proceedings of the 57th Conference of the Association for Computational Linguistics. the 57th Conference of the Association for Computational LinguisticsFlorence, Italy2019. 2019</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A Robustly Optimized Bert Pretraining Approach. 2019arXiv preprint</p>
<p>Automatically Assessing Machine Summary Content Without A Gold Standard. Annie Louis, Ani Nenkova, Computational Linguistics. 392013</p>
<p>Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles. Yao Lu, Yue Dong, Laurent Charlin, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingOnline2020. 2020</p>
<p>Summarizing Student Responses to Reflection Prompts. Wencan Luo, Diane Litman, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, Portugal2015. 2015</p>
<p>Automatic Summarization of Student Course Feedback. Wencan Luo, Fei Liu, Zitao Liu, Diane J Litman, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego California, United States2016. 2016</p>
<p>Congbo Ma, Wei Emma Zhang, Hu Wang, Shubham Gupta, Mingyu Guo, arXiv:2109.11199Incorporating Linguistic Knowledge for Abstractive Multi-document Summarization. 2021. 2021arXiv preprint</p>
<p>Multi-Document Summarization by Graph Search and Matching. Inderjeet Mani, Eric Bloedorn, Proceedings of the Fourteenth National Conference on Artificial Intelligence and Ninth Innovative Applications of Artificial Intelligence Conference (AAAI 1997). the Fourteenth National Conference on Artificial Intelligence and Ninth Innovative Applications of Artificial Intelligence Conference (AAAI 1997)Providence, United States1997</p>
<p>Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement Learning. Yuning Mao, Yanru Qu, Yiqing Xie, Xiang Ren, Jiawei Han, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingOnline2020. 2020</p>
<p>Automatic Documentation Generation via Source Code Summarization of Method Context. W Paul, Collin Mcburney, Mcmillan, Proceedings of the 22nd International Conference on Program Comprehension (ICPC. the 22nd International Conference on Program Comprehension (ICPCHyderabad, India2014. 2014</p>
<p>Language as a Latent Variable: Discrete Generative Models for Sentence Compression. Yishu Miao, Phil Blunsom, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, United States2016. 2016</p>
<p>A Language Independent Algorithm for Single and Multiple Document Summarization. Rada Mihalcea, Paul Tarau, Proceedings of the 2nd International Joint Conference, Companion Volume to the Proceedings of Conference including Posters/Demos and Tutorial Abstracts (IJCNLP 2005). the 2nd International Joint Conference, Companion Volume to the Conference including Posters/Demos and Tutorial Abstracts (IJCNLP 2005)Jeju Island, Republic of Korea2005</p>
<p>Distributed Representations of Words and Phrases and Their Compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Proceedings of the 27th Annual Conference on Neural Information Processing Systems. the 27th Annual Conference on Neural Information Processing SystemsLake Tahoe, United States2013. 2013</p>
<p>Asynchronous Methods for Deep Reinforcement Learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, Proceedings of the 33nd International Conference on Machine Learning (ICML2016). the 33nd International Conference on Machine Learning (ICML2016)New York City; United States2016</p>
<p>. Article . Publication date. 11December 2021Publication</p>
<p>Medical Dialogue Summarization for Automated Reporting in Healthcare. Sabine Molenaar, Lientje Maas, Verónica Burriel, Fabiano Dalpiaz, Sjaak Brinkkemper, Proceedings of the International Conference on Advanced Information Systems Engineering (CAiSE Workshops 2020). the International Conference on Advanced Information Systems Engineering (CAiSE Workshops 2020)Grenoble, France2020</p>
<p>SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents. Ramesh Nallapati, Feifei Zhai, Bowen Zhou, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. the Thirty-First AAAI Conference on Artificial IntelligenceSan Francisco, United States2017. 2017</p>
<p>Ranking Sentences for Extractive Summarization with Reinforcement Learning. Shashi Narayan, Shay B Cohen, Mirella Lapata, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018). the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018)New Orleans, United States2018</p>
<p>Abstractive Unsupervised Multi-Document Summarization using Paraphrastic Sentence Fusion. Tanvir Mir Tafseer Nayeem, Yllias Ahmed Fuad, Chali, Proceedings of the 27th International Conference on Computational Linguistics (COLING 2018). the 27th International Conference on Computational Linguistics (COLING 2018)Santa Fe, United States2018</p>
<p>Diversity driven attention model for query-based abstractive summarization. Preksha Nema, M Mitesh, Anirban Khapra, Balaraman Laha, Ravindran, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, Canada2017. 2017</p>
<p>A Survey of Text Summarization Techniques. Ani Nenkova, Kathleen R Mckeown, Mining Text Data. 2012</p>
<p>The Pyramid Method: Incorporating Human Content Selection Variation in Summarization Evaluation. Ani Nenkova, Rebecca Passonneau, Kathleen Mckeown, ACM Transactions on Speech and Language Processing. 442007</p>
<p>Evaluating Content Selection in Summarization: The Pyramid Method. Ani Nenkova, Rebecca J Passonneau, Proceedings of the Human Language Technology Conference of the North American Chapter. the Human Language Technology Conference of the North American ChapterBoston, United Statesthe Association for Computational Linguistics2004. 2004</p>
<p>Better Summarization Evaluation with Word Embeddings for ROUGE. Jun-Ping Ng, Viktoria Abrecht, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, Portugal2015. 2015</p>
<p>Ayoub Ait Lahcen, and Samir Belfkih. Ahmed Oussous, Fatima-Zahra Benjelloun, Journal of King Saud University-Computer and Information Sciences. 3042018Big Data Technologies: A Survey</p>
<p>Multimodal Abstractive Summarization for How2 Videos. Shruti Palaskar, Jindrich Libovický, Spandana Gella, Florian Metze, Proceedings of the 57th Conference of the Association for Computational Linguistics. the 57th Conference of the Association for Computational Linguistics2019. 2019</p>
<p>. Italy Florence, </p>
<p>BLEU: A Method for Automatic Evaluation of Machine Translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL2002). the 40th Annual Meeting of the Association for Computational Linguistics (ACL2002)Philadelphia, United States2002</p>
<p>Automated Pyramid Scoring of Summaries Using Distributional Semantics. Rebecca J Passonneau, Emily Chen, Weiwei Guo, Dolores Perin, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013). the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013)Sofia, Bulgaria2013</p>
<p>Data Augmentation for Abstractive Query-Focused Multi-Document Summarization. Ramakanth Pasunuru, Asli Celikyilmaz, Michel Galley, Chenyan Xiong, Yizhe Zhang, Mohit Bansal, Jianfeng Gao, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI 2021). the AAAI Conference on Artificial Intelligence (AAAI 2021)Online2021</p>
<p>Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters. Ramakanth Pasunuru, Mengwen Liu, Mohit Bansal, Sujith Ravi, Markus Dreyer, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2021). the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2021)Online2021</p>
<p>A Deep Reinforced Model for Abstractive Summarization. Romain Paulus, Caiming Xiong, Richard Socher, Proceedings of the 6th International Conference on Learning Representations. the 6th International Conference on Learning RepresentationsVancouver, Canada2018. 2018</p>
<p>Deep Contextualized Word Representations. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, United States2018. 2018</p>
<p>A Simple Theoretical Model of Importance for Summarization. Maxime Peyrard, Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL 2019). the 57th Conference of the Association for Computational Linguistics (ACL 2019)Florence, ItalyAssociation for Computational Linguistics2019</p>
<p>A Common Theory of Information Fusion from Multiple Text Sources Step One: Cross-Document Structure. R Dragomir, Radev, Proceedings of the Workshop of the 1st Annual Meeting of the Special Interest Group on Discourse and Dialogue. the Workshop of the 1st Annual Meeting of the Special Interest Group on Discourse and DialogueHong Kong, China2000. 2000</p>
<p>Centroid-based Summarization of Multiple Documents. Hongyan Dragomir R Radev, Małgorzata Jing, Daniel Styś, Tam, Information Processing &amp; Management. 402004</p>
<p>. R Dragomir, Pradeep Radev, Muthukrishnan, Amjad Vahed Qazvinian, Abu-Jbara, The ACL Anthology Network Corpus. Lang. Resour. Evaluation. 472013</p>
<p>Language Models are Unsupervised Multitask Learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI Blog. 192019</p>
<p>. Article . Publication date. 11December 2021Publication</p>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21672020</p>
<p>Improving Automated Source Code Summarization via An Eye-tracking Study of Programmers. Paige Rodeghero, Collin Mcmillan, Paul W Mcburney, Nigel Bosch, Sidney D' Mello, Proceedings of the 36th International Conference on Software Engineering. the 36th International Conference on Software EngineeringHyderabad, India2014. 2014</p>
<p>Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead. Cynthia Rudin, Nature Machine Intelligence. 12019</p>
<p>Learning Representations by Back-propagating Errors. Geoffrey E David E Rumelhart, Ronald J Hinton, Williams, Nature. 3231986</p>
<p>Dynamic Routing Between Capsules. Sara Sabour, Nicholas Frosst, Geoffrey E Hinton, Proceedings of the 2017 Annual Conference on Neural Information Processing Systems. the 2017 Annual Conference on Neural Information Processing SystemsLong Beach, United States2017. 2017</p>
<p>Get To The Point: Summarization with Pointer-Generator Networks. Abigail See, Peter J Liu, Christopher D Manning, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational Linguistics2017. 2017</p>
<p>. Canada Vancouver, </p>
<p>Is Attention Interpretable?. Sofia Serrano, Noah A Smith, Proceedings of the 57th Conference of the Association for Computational Linguistics. the 57th Conference of the Association for Computational LinguisticsFlorence, Italy2019. 2019</p>
<p>A Graph-theoretic Summary Evaluation for Rouge. Elaheh Shafieibavani, Mohammad Ebrahimi, Raymond Wong, Fang Chen, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018. 2018</p>
<p>Literature Study on Multi-document Text Summarization Techniques. Chintan Shah, Anjali Jivani, Proceedings of the International Conference on Smart Trends for Information Technology and Computer Communications. the International Conference on Smart Trends for Information Technology and Computer Communications2016</p>
<p>Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation. Ori Shapira, David Gabay, Yang Gao, Hadar Ronen, Ramakanth Pasunuru, Mohit Bansal, Yael Amsterdamer, Ido Dagan, Proceedings of the Conference of the North American Chapter. the Conference of the North American ChapterMinneapolis, United Statesthe Association for Computational Linguistics2019. 2019</p>
<p>Extractive Text Summarization Using Deep Learning. S Nikhil, Samidha Shirwandkar, Kulkarni, Proceedings of the 2018 Fourth International Conference on Computing Communication Control and Automation. the 2018 Fourth International Conference on Computing Communication Control and AutomationPune, India2018. 2018ICCUBEA</p>
<p>Unity in Diversity: Learning Distributed Heterogeneous Sentence Representation for Extractive Summarization. Abhishek Kumar Singh, Manish Gupta, Vasudeva Varma, Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI 2018). the 32nd AAAI Conference on Artificial Intelligence (AAAI 2018)New Orleans, United States2018</p>
<p>Summarizing Medical Conversations via Identifying Important Utterances. Yan Song, Yuanhe Tian, Nan Wang, Fei Xia, Proceedings of the 28th International Conference on Computational Linguistics (COLING2020). the 28th International Conference on Computational Linguistics (COLING2020)Online2020</p>
<p>The Feasibility of Embedding Based Automatic Evaluation for Single Document Summarization. Simeng Sun, Ani Nenkova, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaEMNLP-IJCNLP2019. 2019</p>
<p>Intriguing Properties of Neural Networks. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J Goodfellow, Rob Fergus, Proceedings of the 2nd International Conference on Learning Representations. the 2nd International Conference on Learning RepresentationsBanff, Canada2014. 2014</p>
<p>Neural Network based Reinforcement Learning for Real-time Pushing on Text Stream. Haihui Tan, Ziyu Lu, Wenjie Li, Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR2017). the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR2017)Tokyo, Japan2017</p>
<p>A Survey Automatic Text Summarization. Oguzhan Tas, Farzad Kiyani, PressAcademia Procedia. 52007</p>
<p>Amirsina Torfi, Rouzbeh A Shirvani, Yaser Keneshloo, Nader Tavaf, Edward A Fox, CoRR abs/2003.01200Natural Language Processing Advancements By Deep Learning: A Survey. 2020. 2020</p>
<p>Modeling Coverage for Neural Machine Translation. Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, Hang Li, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, Germany2016. 2016</p>
<p>A Publicly Available Annotated Corpus for Supervised Email Summarization. Jan Ulrich, Gabriel Murray, Giuseppe Carenini, Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence in Enhanced Messaging Workshop (AAAI 2008). the Twenty-Third AAAI Conference on Artificial Intelligence in Enhanced Messaging Workshop (AAAI 2008)Chicago, United States2008</p>
<p>Attention Is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Proceedings of the Annual Conference on Neural Information Processing Systems. the Annual Conference on Neural Information Processing SystemsLong Beach, United States2017. 2017</p>
<p>Pointer Networks. Oriol Vinyals, Meire Fortunato, Navdeep Jaitly, Proceedings of the 2015 Annual Conference on Neural Information Processing Systems. the 2015 Annual Conference on Neural Information Processing SystemsMontreal, Canada2015. 2015</p>
<p>Extractive Text Summarization: Can We Use the Same Techniques for Any Text. Tatiana Vodolazova, Elena Lloret, Rafael Muñoz, Manuel Palomar, Proceedings of the 18th International Conference on Applications of Natural Language to Information Systems. the 18th International Conference on Applications of Natural Language to Information SystemsSalford, UK2013. 2013</p>
<p>. Article . Publication date. 11December 2021Publication</p>
<p>Improved Affinity Graph based Multi-document Summarization. Xiaojun Wan, Jianwu Yang, Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics. the Human Language Technology Conference of the North American Chapter of the Association of Computational LinguisticsNew York,United States2006. 2006</p>
<p>Multi-document Summarization Using Cluster-based Link Analysis. Xiaojun Wan, Jianwu Yang, Proceedings of the 31st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2008). the 31st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2008)Singapore2008</p>
<p>Heterogeneous Graph Neural Networks for Extractive Document Summarization. Danqing Wang, Pengfei Liu, Yining Zheng, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020). the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020)Online2020Xipeng Qiu, and Xuanjing Huang</p>
<p>Soft Expert Reward Learning for Vision-and-Language Navigation. Hu Wang, Qi Wu, Chunhua Shen, Proceedings of the 16th European Conference on Computer Vision (ECCV 2020). the 16th European Conference on Computer Vision (ECCV 2020)Online2020</p>
<p>Neural Network-Based Abstract Generation for Opinions and Arguments. Lu Wang, Wang Ling, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego California, United States2016. 2016</p>
<p>Controllable Abstractive Dialogue Summarization with Sketch Supervision. Chien-Sheng Wu, Linqing Liu, Wenhao Liu, Pontus Stenetorp, Caiming Xiong, Findings of the Association for Computational Linguistics: (ACL/IJCNLP 2021). Online2021</p>
<p>MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization. Canwen Xu, Jiaxin Pei, Hongtao Wu, Yiyu Liu, Chenliang Li, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020). the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020)Online2020</p>
<p>Xiaomingbot: A Multilingual Robot News Reporter. Runxin Xu, Jun Cao, Mingxuan Wang, Jiaze Chen, Hao Zhou, Ying Zeng, Yuping Wang, Li Chen, Xiang Yin, Xijin Zhang, Songcheng Jiang, Yuxuan Wang, Lei Li, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL 2020). the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL 2020)Online2020</p>
<p>. Min Yang, Chengming Li, Fei Sun, Zhou Zhao, Ying Shen, Chenglin Wu, n.d.</p>
<p>Deep Reinforcement Learning for Real-Time Event Summarization. Be Relevant, Timely Non-Redundant, The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI2020). New York, USA</p>
<p>Investigating Capsule Networks with Dynamic Routing for Text Classification. Min Yang, Wei Zhao, Jianbo Ye, Zeyang Lei, Zhou Zhao, Soufei Zhang, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, Belgium2018. 2018</p>
<p>PEAK: Pyramid Evaluation via Automated Knowledge Extraction. Qian Yang, Rebecca J Passonneau, Gerard De, Melo , Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI 2016). the Thirtieth AAAI Conference on Artificial Intelligence (AAAI 2016)Phoenix, Arizona2016</p>
<p>Xlnet: Generalized Autoregressive Pretraining for Language Understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, Quoc V Le, Proceedings of the Annual Conference on Neural Information Processing System. the Annual Conference on Neural Information Processing SystemVancouver, Canada2019. 2019</p>
<p>Deep Reinforcement Learning for Extractive Document Summarization. Kaichun Yao, Libo Zhang, Tiejian Luo, Yanjun Wu, Neurocomputing. 2842018</p>
<p>Scisummnet: A Large Annotated Corpus and Content-impact Models for Scientific Paper Summarization with Citation Networks. Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R Fabbri, Irene Li, Dan Friedman, Dragomir R Radev, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI 2019). the AAAI Conference on Artificial Intelligence (AAAI 2019)Honolulu, United States2019</p>
<p>Graph-based Neural Multi-Document Summarization. Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush Pareek, Krishnan Srinivasan, Dragomir R Radev, Proceedings of the 21st Conference on Computational Natural Language Learning. the 21st Conference on Computational Natural Language LearningVancouver, Canada2017. 2017</p>
<p>Optimizing Sentence Modeling and Selection for Document Summarization. Wenpeng Yin, Yulong Pei, Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI 2015). the 24th International Joint Conference on Artificial Intelligence (IJCAI 2015)Buenos Aires, Argentina2015</p>
<p>Big Bird: Transformers for Longer Sequences. Manzil Zaheer, Guru Guruganesh, Avinava Kumar, Joshua Dubey, Chris Ainslie, Santiago Alberti, Philip Ontañón, Anirudh Pham, Qifan Ravula, Li Wang, Amr Yang, Ahmed, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Online2020. 2020. NeurIPS 2020</p>
<p>Single-document and Multi-document Summarization Techniques for Email Threads Using Sentence Compression. Bonnie J David M Zajic, Jimmy Dorr, Lin, Information Processing &amp; Management. 442008</p>
<p>Adapting Neural Single-document Summarization Model for Abstractive Multi-document Summarization: A Pilot Study. Jianmin Zhang, Jiwei Tan, Xiaojun Wan, Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language GenerationTilburg, Netherlands2018. 2018</p>
<p>PEGASUS: Pre-training with Extracted Gapsentences for Abstractive Summarization. Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter J Liu, Proceedings of the 37th International Conference on Machine Learning (ICML2020). the 37th International Conference on Machine Learning (ICML2020)Online2020</p>
<p>Interpretable Convolutional Neural Networks. Quanshi Zhang, Ying Nian Wu, Song-Chun Zhu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR2018). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR2018)Salt Lake City; United States2018</p>
<p>. Article . Publication date. 11December 2021Publication</p>
<p>EmailSum: Abstractive Email Thread Summarization. Shiyue Zhang, Asli Celikyilmaz, Jianfeng Gao, Mohit Bansal, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL/IJCNLP 2021). the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL/IJCNLP 2021)Online2021</p>
<p>BERTScore: Evaluating Text Generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, Proceedings of the 8th International Conference on Learning Representations (ICLR 2020). the 8th International Conference on Learning Representations (ICLR 2020)EthiopiaAddis Ababa2020</p>
<p>Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. Emma Wei, Quan Z Zhang, Ahoud Sheng, F Abdulrahmn, Chenliang Alhazmi, Li, ACM Transactions on Intelligent Systems and Technology. 11412020</p>
<p>The 10 Research Topics in the Internet of Things. Emma Wei, Quan Z Zhang, Adnan Sheng, Dai Mahmood, Munazza Hoang Tran, Salma Zaib, Abdulwahab Abdalla Hamad, Ahoud Aljubairy, F Abdulrahmn, Subhash Alhazmi, Congbo Sagar, Ma, 6th IEEE International Conference on Collaboration and Internet Computing (CIC 2020). Atlanta, USA2020</p>
<p>Multiview Convolutional Neural Networks for Multidocument Extractive Summarization. Yong Zhang, Meng Joo Er, Rui Zhao, Mahardhika Pratama, IEEE Transactions on Cybernetics. 472016</p>
<p>QBSUM: A Large-scale Query-based Document Summarization Dataset from real-world applications. Mingjun Zhao, Shengli Yan, Bang Liu, Xinwang Zhong, Qian Hao, Haolan Chen, Di Niu, Bowei Long, Weidong Guo, Comput. Speech Lang. 661011662021. 2021</p>
<p>MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, Steffen Eger, Proceedings of the Conference on Empirical Methods in Natural Language and the 9th International Joint Conference on Natural Language Processing. the Conference on Empirical Methods in Natural Language and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaEMNLP-IJCNLP2019. 2019</p>
<p>Subtopic-driven Multi-Document Summarization. Xin Zheng, Aixin Sun, Jing Li, Karthik Muthuswamy, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaEMNLP-IJCNLP2019. 2019</p>
<p>Extractive Summarization as Text Matching. Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020). the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020)Online2020Xipeng Qiu, and Xuanjing Huang</p>
<p>QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization. Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, Dragomir R Radev, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterOnline2021. 2021</p>
<p>Learning Deep Features for Discriminative Localization. Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba, Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. the IEEE conference on Computer Vision and Pattern RecognitionLas Vegas, United States2016. 2016</p>
<p>MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization. Chenguang Zhu, Yang Liu, Jie Mei, Michael Zeng, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterOnline2021. 2021</p>
<p>A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining. Chenguang Zhu, Ruochen Xu, Michael Zeng, Xuedong Huang, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings (EMNLP 2020). the 2020 Conference on Empirical Methods in Natural Language Processing: Findings (EMNLP 2020)Online2020</p>
<p>MSMO: Multimodal Summarization with Multimodal Output. Junnan Zhu, Haoran Li, Tianshang Liu, Yu Zhou, Jiajun Zhang, Chengqing Zong, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, Belgium2018. 2018</p>
<p>Estimating Summary Quality with Pairwise Preferences. Markus Zopf, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018). the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018)New Orleans, United States2018</p>            </div>
        </div>

    </div>
</body>
</html>
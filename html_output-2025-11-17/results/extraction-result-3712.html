<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3712 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3712</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3712</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-88.html">extraction-schema-88</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-fa3609e00f9f422a309c621a35394c4a38f88687</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fa3609e00f9f422a309c621a35394c4a38f88687" target="_blank">Using cognitive psychology to understand GPT-3</a></p>
                <p><strong>Paper Venue:</strong> Proceedings of the National Academy of Sciences of the United States of America</p>
                <p><strong>Paper TL;DR:</strong> The present article finds that GPT-3 can solve many of these tasks reasonably well, despite being only taught to predict future word occurrences on a vast amount of text from the Internet and books.</p>
                <p><strong>Paper Abstract:</strong> Significance Language models are trained to predict the next word for a given text. Recently, it has been shown that scaling up these models causes them to not only generate language but also to solve challenging reasoning problems. The present article lets a large language model (GPT-3) do experiments from the cognitive psychology literature. We find that GPT-3 can solve many of these tasks reasonably well, despite being only taught to predict future word occurrences on a vast amount of text from the Internet and books. We additionally utilize analysis tools from the cognitive psychology literature to demystify how GPT-3 solves different tasks and use the thereby acquired insights to make recommendations for how to improve future model iterations.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3712",
    "paper_id": "paper-fa3609e00f9f422a309c621a35394c4a38f88687",
    "extraction_schema_id": "extraction-schema-88",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0055165,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Using cognitive psychology to understand GPT-3</h1>
<p>Marcel Binz ${ }^{1, <em>}$ and Eric Schulz ${ }^{1}$<br>${ }^{1}$ MPRG Computational Principles of Intelligence, Max Planck Institute for Biological Cybernetics, 72076 TÃ¼bingen, Germany<br></em>marcel.binz@tue.mpg.de</p>
<h4>Abstract</h4>
<p>We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3's decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3's behavior is impressive: it solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multi-armed bandit task, and shows signatures of model-based reinforcement learning. Yet we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in causal reasoning task. These results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.</p>
<h2>Introduction</h2>
<p>With the advent of increasingly capable artificial agents, comes the urgency to improve our understanding of how they learn and make decisions ${ }^{1}$. Take as an example large language models ${ }^{2}$. These models' abilities are, by many standards, impressive. They can generate text that human evaluators have difficulty distinguishing from text written by other humans ${ }^{2}$, generate computer code ${ }^{3}$, or converse with humans about a range of different topics ${ }^{4}$. What is perhaps even more surprising, is that these models' abilities go beyond mere language generation: they can, for instance, also play chess at a reasonable level ${ }^{5}$ and solve university-level math problems ${ }^{6}$. These observations have prompted some to argue that this new class of foundation models, which are models trained on broad data at scale and adapted to a wide range of downstream tasks, shows some form of general intelligence ${ }^{7}$. Yet others have been more skeptical, pointing out that these models are still a far cry away from a human-level understanding of language and semantics ${ }^{8}$. But how can we evaluate whether or not these models -at least in some situationslearn and think like people? One approach towards evaluating a model's human-likeness comes from cognitive psychology. Psychologists, after all, are experienced in trying to formally understand another notoriously impenetrable algorithm: the human mind.</p>
<p>In the present article, we investigate the Generative Pre-trained Transformer 3 model (or short: GPT-3) ${ }^{2}$ on several experiments taken from the cognitive psychology literature. Our analyses cover two types of experiments: vignette-based and task-based experiments. While vignette-based experiments involve a short and predefined description of a hypothetical scenario, task-based experiments are programmatically generated on a trail-by-trial basis. The selected tasks for both of these settings cover well-known areas of cognitive psychology: decision-making, information search, deliberation, and causal reasoning. We are primarily interested in whether GPT-3 can solve these tasks appropriately as well as how its behavior compares to human subjects. Our results show that GPT-3 can solve challenging vignette-based problems. Yet, we also highlight that these vignettes or similar texts might have been part of its training set. Moreover, we find that GPT-3's behavior strongly depends on how the vignettes are presented. Thus, we also subject GPT-3 to a battery of task-based problems. The results from these task-based assessments show that GPT-3 can make human-level decisions in both description-based and experience-based decision-making experiments, yet does not learn and explore in a human-like fashion. Furthermore, even though GPT-3 shows signatures of model-based reinforcement learning, it fails altogether in a causal reasoning task. Taken together, our results improve our understanding of current large language models, suggest ways in which they can be improved, and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.</p>
<h2>GPT-3</h2>
<p>GPT-3 is an auto-regressive language model ${ }^{2}$. It utilizes the transformer architecture ${ }^{9}$-a deep learning model that heavily relies on the mechanism of self-attention- to produce human-like text. Just like recurrent neural networks, transformers are designed to process sequential data, such as natural language. However, unlike recurrent neural networks, transformers process the entire data all at once, with the attention mechanism providing context for any position in the input sequence. The model</p>
<p>Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations.</p>
<p>Q: Which option is the most probable?</p>
<ul>
<li>Option 1: Linda is a bank teller.</li>
<li>Option 2: Linda is a bank teller and is active in the feminist movement.</li>
<li>Option 3: Linda is a member of the NRA.</li>
</ul>
<p>A: Option</p>
<p>B
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Vignette-based tasks. A: Example prompt of hypothetical scenario, in this case the famous Linda problem, as submitted to GPT-3. B: Results. While in 12 out 12 standard vignettes, GPT-3 either answers correctly or makes human-like mistakes, it makes mistakes that are not human-like when given the adversarial vignettes.
itself is large, it has 175 billion parameters, and it was trained on a vast amount of text: hundreds of billions of words from the internet and books. GPT-3's architecture is similar to that of its predecessor, GPT-2 ${ }^{10}$, but contains many more trainable parameters. Thus, GPT-3 can be thought of as an experiment in massively scaling up known algorithms ${ }^{11}$. Larger models can capture more of the complexities of the data they were trained on and can transfer this knowledge to tasks that they have not been specifically trained for. Rather than being fine-tuned on a problem, these large language models can be given an instruction together with some examples of the task and identify what to do based on this alone. This is called "in-context learning" because the model picks up on patterns in its "context", for example, the string of words that the model is asked to complete. GPT-3 does incredibly well at in-context learning across a range of settings ${ }^{12}$, sometimes even performing at a level comparable to the best fine-tuned models ${ }^{2,13}$. Since GPT-3 is one of the biggest and most versatile large language models, it is a good candidate to be scrutinized using cognitive psychology.</p>
<h1>A cognitive psychology view on GPT-3</h1>
<p>We will subject GPT-3 to several tasks taken from the cognitive psychology literature. These tasks fall into four categories: 1. decision-making, 2. information search, 3. deliberation, and 4. causal reasoning. We will begin our investigations with several, classical vignette-based problems. For these vignette-based investigations, we confronted GPT-3 with text-based descriptions of hypothetical situations while collecting its responses. However, as we will point out, these vignettes have the problem that GPT-3 has likely experienced identical or similar such tasks in its training data. Moreover, we found that GPT-3's response can be tampered with just by marginally changing the vignettes and thereby creating adversarial situations. Thus, we also evaluated GPT-3's abilities in various task-based experiments. In these task-based investigations, we take canonical tasks from the literature and emulate their experimental structure as programmatically generated text to which GPT-3 responds on every experimental trial. We then use GPT-3's responses to analyze its behavior similar to how cognitive psychologists would analyze human behavior in the same tasks.</p>
<h2>Results</h2>
<p>We used the public OpenAI API to run all our simulations ${ }^{14}$. There are four GPT-3 models accessible through this API: "Ada", "Babbage", "Curie" and "Davinci" (sorted from the least to the most complex model). We focused our investigation on the most powerful of these models ("Davinci") unless otherwise noted. We furthermore set the temperature parameter to 0 , leading to deterministic answers, and kept the default values for all other parameters.</p>
<h1>Vignette-based investigations</h1>
<p>For the vignette-based investigations, we took canonical scenarios from the cognitive psychology literature, entered them as prompts into GPT-3, and recorded its answer. For each scenario, we report if GPT-3 responded correctly or not. Moreover, we classified each response as something a human could have said because it was either the correct response or a mistake commonly observed in human data. For cases where there were only two options, one correct and one that is normally chosen by human subjects, we added a third option that was neither correct nor plausibly chosen by people. The following subsections briefly summarize our main findings. We refer the reader to SI Appendix for a detailed description of the submitted prompts and GPT-3's corresponding answers.</p>
<h2>Decision-making: Heuristics and biases</h2>
<p>We began our investigations of GPT-3's decision-making by prompting the canonical "Linda problem" 15 (Linda, see Figure 1A). This problem has been known to assess the conjunction fallacy, a formal fallacy that occurs when it is assumed that specific conditions are more probable than a single general one. In the standard vignette, a hypothetical woman named Linda is described as "outspoken, bright, and politically active". Participants are then asked if it was more likely that Linda is a bank teller or that she is a bank teller and an active feminist. GPT-3, just like people, chose the second option, thereby falling for the conjunction fallacy.</p>
<p>Next, we prompted the so-called "cab problem" 16 (Cab, see SI Appendix), in which participants commonly fail to take the base rate of different colors of taxis in a city into account when judging the probability of the color of a cab that was involved in an accident. Unlike people, GPT-3 did not fall for the base-rate fallacy, i.e. to ignore the base rates of different colors, but instead provided the (approximately) correct answer.</p>
<p>Finally, we asked GPT-3 to provide an answer to the "hospital problem" 17 (Hospital, see SI Appendix), in which participants are asked which of two hospitals, a smaller or a larger one, is more likely to report more days on which more than $60 \%$ of all born children were boys. While the correct answer would be the smaller hospital (due to the larger variance of smaller samples), GPT-3, just like people, thought that the probability was about equal.</p>
<h2>Information search: Questions and hypothesis tests</h2>
<p>First, we assessed if GPT-3 can adaptively change between constraint-seeking vs. hypothesis-scanning questions. Constraintseeking questions target a feature shared by multiple objects, such as "Is the person female?", whereas hypothesis-scanning questions target a single object, such as "Is the person Linda?". Crucially, which type of question is more informative depends on past observations. Ruggeri et al. ${ }^{18}$ manipulated the particular reasons for why a fictitious character named Toma was repeatedly late to school (Toma, see SI Appendix). While for one group he was frequently late because his bicycle had broken, for the other group he was late for various reasons with half of them being that he could not find various objects. While trying to find out why Toma is late to school again, the first group should ask the hypothesis-scanning question "Was he late because his bicycle broke?", whereas the second group should ask the constraint-seeking question "Was he late because he could not find something?". GPT-3 picked the appropriate question in each scenario.</p>
<p>Secondly, we confronted GPT-3 with a scenario originally presented by Baron et al. ${ }^{19}$ in which subjects need to choose an appropriate test to discriminate between two illnesses (Test, see SI Appendix). Empirically, participants tend to choose the wrong test, likely because they overvalue questions that have a high probability of a positive result given the most likely hypothesis. GPT-3, just like people, fell for the same congruence bias.</p>
<p>Finally, we presented Wason's well-known "Card Selection Task"20 to GPT-3, explaining that the visible faces of four cards showed A, K, 4 and 7, and that the truth of the proposition "If a card shows a vowel on one face, then its opposite face shows an even number" needed to be tested (Wason, see SI Appendix). GPT-3 suggested to turn around A and 7, which is commonly accepted as the correct answer, even though most people turn around A and 4.</p>
<h2>Deliberation: The Cognitive Reflection Test</h2>
<p>We also tried to estimate GPT-3's tendency to override an incorrect fast response with answers derived by further deliberation. For this, we prompted the three items of the Cognitive Reflection Test ${ }^{21}$ (CRT1-CRT3, see SI Appendix). One example item of this task is: "If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?". While the initial response might be to say " 100 ", 100 machines would just be as fast as 5 machines and thus also take 5 minutes. For all three items of the CRT, GPT-3 responded with the intuitive but incorrect answer, as has been observed in earlier work ${ }^{22}$.</p>
<h2>Causal reasoning: Blickets, interventions, and counterfactuals</h2>
<p>We lastly assessed GPT-3's causal reasoning abilities. In a first test, we prompted GPT-3 with a version of the well-known "Blicket" experiment ${ }^{23}$ (Blicket, see SI Appendix). For this, blickets are introduced as objects that turn on a machine. Afterward, two objects are introduced. The first object turns on the machine on its own. The second machine does not turn on the machine on its own. Finally, both objects together turn on the machine. GPT-3, just like people, managed to correctly identify that the first but not the second object is a blicket.</p>
<p>In a second test, we asked GPT-3 to intervene in a scenario by removing the correct object to prevent an effect after having read about three different objects, one causing and two not causing the effect (in this case, an allergic reaction; Intervene, see SI Appendix). GPT-3 identified the correct object to be removed.</p>
<p>In the final test, we probed GPT-3's ability of mature causal reasoning ${ }^{24}$ (Mature, see SI Appendix). In this task, GPT-3 was told that there were four pills: A, B, C and D. While A and B individually could kill someone, C and D could not. GPT-3 successfully answered multiple questions about counterfactuals correctly, such as: "A man took pill B and pill C and he died. If he had not taken pill B , could he still have died?".</p>
<h1>Problems with vignette-based investigations</h1>
<p>Of the 12 vignette-based problems presented to GPT-3, it answered six correctly and all 12 in a way that could be described as human-like (Figure 1B). Does this mean that GPT-3 could pass as a human in a cognitive psychology experiment? We believe that the answer, based on the vignette-based tasks alone, has to be "No.". Since many of the prompted scenarios were taken from famous psychological experiments, there is a chance that GPT-3 has encountered these scenarios or similar ones in its training set. Moreover, in additional investigations, we found that many of the vignettes could be slightly modified, i.e., made into adversarial vignettes, such that GPT-3 would give vastly different responses. In the cab problem, for example, it is clearly stated that $15 \%$ of the cabs are blue and $85 \%$ are green. Yet asking GPT-3 about the probability that a cab involved in an accident was black, it responded with " $20 \%$ " (Black Cab, see SI Appendix). Simply changing the order of the options in Wason's card selection task from "A, K, 4, and 7" to "4, 7, A, and K" caused GPT-3 to suggest turning around "A" and "K" (Reverse Wason, see SI Appendix). Giving GPT-3 the first item of the CRT and stating that "The bat costs $\$ 1.00$ more than the bat.", it still thought that the ball was $\$ 0.10$ (Wrong CRT, see SI Appendix). Finally, when phrasing the mature causal reasoning problem as a "Blicket" problem in which machines could be turned on or off, GPT-3 answered some questions incorrectly while contradicting itself in its explanations (Immature Blicket, see SI Appendix). There have recently been other, much larger investigations using similar vignettes, whose results agree largely with our assessment ${ }^{25}$.</p>
<h2>Task-based investigations</h2>
<p>The results from the previous section indicate that GPT-3 can produce passable responses in some vignette-based tasks. It is, however, not possible to decide whether it is merely behaving like a parrot, repeating what it has seen in the training data, or whether it is reasoning successfully. We, therefore, next turned our lens of investigation to a more challenging setting and tested GPT-3 on actual, task-based experiments. In order to do so, we selected a set of four classical experiments that we believe to be representative of the cognitive psychology literature. For each of these, we programmatically generated a description that was entered as a prompt and -if there were multiple trials- updated the text with GPT-3's response and the received feedback.</p>
<h2>Decision-making: Decisions from descriptions</h2>
<p>How people make decisions from descriptions is one of the most well-studied areas of cognitive psychology, ranging from the early, seminal work of Kahneman \&amp; Tversky ${ }^{28}$ to modern, large-scale investigations ${ }^{26,27}$. In the decisions from descriptions paradigm, a decision-maker is asked to choose between one of two hypothetical gambles like the ones shown in Figure 2A. To test whether GPT-3 can reliably solve such problems, we presented the model with over 13,000 problems taken from a recent benchmark data-set ${ }^{26}$. Figure 2B shows the regret, which is defined as the difference between the expected outcome of the optimal option and that of the actually chosen option, obtained by different models in the GPT-3 family and compares their performance to human decisions. We found that only the largest of the GPT-3 models ("Davinci") was able to solve these problems above chance-level $(t(29134)=-16.85, p=&lt;.001)$, whereas the three smaller models did not (all $p&gt;0.05$ ). While the "Davinci" model did reasonably well, it did not reach human-level performance $(t(29134)=-11.50, p&lt;.001)$.</p>
<p>However, given that GPT-3 was not too far away from human performance, it is reasonable to ask whether the model also exhibited human-like, cognitive biases. In their original work on prospect theory, Kahneman \&amp; Tversky ${ }^{17}$ identified several biases of human decision-making by contrasting answers to multiple carefully selected problems pairs. We replicated the original analysis of Kahneman \&amp; Tversky using choice probabilities of GPT-3 and found that GPT-3 showed three of the six biases identified by Kahneman \&amp; Tversky. First, it displayed a framing effect, meaning that its preferences changed depending on whether a choice was presented in terms of gains or losses. GPT-3 was also subject to a certainty effect, meaning that it preferred guaranteed outcomes to risky ones even when they had slightly lower expected values. Finally, GPT-3 showed an overweighting bias and assigned higher importance to a difference between two small probabilities (e.g., $1 \%$ and $2 \%$ ) than to the same differences between two larger probabilities (e.g., $41 \%$ and $42 \%$ ). Figure 2C contains an analysis of these three biases and the three additional ones we did not find in GPT-3. For a detailed description of the conducted analysis, see SI Appendix.</p>
<h2>Information search: Directed and Random Exploration</h2>
<p>GPT-3 did well in the vignette-based information search tasks, so we were curious how it would fare in a more complex setting. The multi-armed bandit paradigm provides a suitable test-bed for this purpose. It extends the decisions from descriptions paradigm from the last section by adding two layers of complexity. First, the decision-maker is not provided with descriptions</p>
<p>Q: Which option do you prefer?</p>
<ul>
<li>Option F: 69.0 dollars with $1.0 \%$ chance, 26.0 dollars with $99.0 \%$ chance.</li>
<li>Option J: 2.0 dollars with $75.0 \%$ chance, 94.0 dollars with $25.0 \%$ chance.</li>
</ul>
<p>A: Option</p>
<p>B
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>C
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2. Decisions from descriptions. A: Example prompt of a problem provided to GPT-3. B: Mean regret averaged over all 13,000 problems taken from Peterson et al. ${ }^{26}$. Lower regret means better performance. Error bars indicate the standard error of the mean. C: Log-odds ratios of different contrasts used to test for cognitive biases. Positive values indicate that the given bias is present in humans (circle) or GPT-3 (triangle). Human data adapted from Ruggeri et al. ${ }^{27}$. For a detailed description of this analysis, see SI Appendix.
for each option anymore but has to learn their value from noisy samples, i.e. from experience ${ }^{29}$. Second, the interaction is not confined to a single choice but potentially involves repeated decisions about which option to sample. Together, these two modifications call for an important change in how a decision-maker must approach such problems. It is not enough to merely exploit currently available knowledge anymore, but also crucial to explore options that are unfamiliar and thereby gain information about their value. Previous research suggests that people solve this exploration-exploitation trade-off by applying a combination of two distinct strategies: directed and random exploration ${ }^{30}$. Whereas directed exploration encourages the decision-maker to collect samples from previously unexplored options, random exploration strategies inject some form of stochasticity into the decision process ${ }^{31,32}$.</p>
<p>Wilson's horizon task is the canonical experiment to test whether a decision-maker applies the two aforementioned forms of exploration ${ }^{30}$. It involves a series of two-armed bandit tasks, in each of which the decision-maker is provided with data from four forced-choice trials, followed by either one or six free-choice trials (referred to as the horizon). Forced-choice trials are used to control the amount of information available to the decision-maker. They either provide two observations for each option (equal information condition) or a single observation from one option and three from the other (unequal information condition). These two conditions make it possible to tease apart directed and random exploration by looking at the decision in the first free-choice trial. In the equal information condition, a choice is classified as random exploration if it corresponds to the option with the lower estimated mean. In the unequal information condition, a choice is classified as directed exploration if it corresponds to the option that was observed fewer times during the forced-choice trials. Note that short-horizon tasks do not benefit from making exploratory choices and, hence, we should expect the decision-maker to make fewer such choices in them.</p>
<p>You are going to a casino that owns two slot machines. You earn money each time you play on one of these machines.</p>
<p>You have received the following amount of dollars when playing in the past:</p>
<ul>
<li>Machine F delivered 51 dollars.</li>
<li>Machine J delivered 39 dollars.</li>
<li>Machine J delivered 40 dollars.</li>
<li>Machine J delivered 26 dollars.</li>
</ul>
<p>Your goal is to maximize the sum of received dollars within six additional rounds.</p>
<p>Q: Which machine do you choose?
A: Machine</p>
<p>C
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>B
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>D
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 3. Horizon task. A: Example prompt for one trial as submitted to GPT-3. B: Mean regret for GPT-3 and human subjects by horizon condition. Lower regret means better performance. Error bars indicate the standard error of the mean. Human data taken from Zaller et al. ${ }^{33}$. C: Probability of selecting option "J" in the equal information condition for both GPT-3 and human subjects by horizon condition. D: Probability of selecting the more informative option in the unequal information condition for both GPT-3 and human subjects by horizon condition.</p>
<p>We presented a text-based version of the horizon task as illustrated in Figure 3A to GPT-3. Figure 3B compares the model's regret to the regret of human subjects. For short-horizon tasks, GPT-3's performance was indistinguishable from human performance $(t(5566)=-0.043, p=.97)$. This result highlights that GPT-3 can not only make sensible decisions when presented with descriptions of options but is also able to integrate this information from noisy samples. The initial regret of GPT-3 in long-horizon tasks was significantly lower than the corresponding human regret $(t(5550)=-4.07, p&lt;.001)$ and was</p>
<p>only slightly above the one from short-horizon tasks. However, within each task people improved more than GPT-3 and reached a final regret that was slightly but not significantly lower than that of GPT-3 $(t(5550)=-0.75, p=.23)$. Looking at the entire experiment, GPT-3 $(M=2.72, S D=5.98)$ achieved a significantly lower regret than human subjects $(M=3.24, S D=10.26)$, $t(38878)=-5.03, p&lt;.001$.</p>
<p>To investigate how GPT-3 managed the trade-off between exploration and exploitation, we fitted a separate logistic regression model for each information condition. We used the estimated reward difference, horizon, their interaction, and a bias term as independent variables for both models. The model for the equal information condition used an indicator for selecting option J in the first free-choice trial as the dependent variable, whereas the model for the unequal condition used an indicator for selecting the more informative option (i.e., the one that has been observed fewer times during the forced-choice trials). The results of this regression analysis are summarized visually in Figure 3C and D. If GPT-3 applied random exploration, we should observe a positive effect of estimated reward difference. If its random exploration was furthermore strategic, we should find more noisy decisions in long-horizon tasks of the equal information condition (reflected in a negative interaction effect of estimated reward difference and horizon). People show both of these effects ${ }^{30}$. GPT-3 also displayed a significant effect of estimated reward difference ( $\beta=0.18 \pm 0.01, z=14.48, p&lt;.001$ ), suggesting that it used at least a rudimentary form of random exploration. However, we did not find a significant interaction effect between estimated reward difference and horizon ( $\beta=-0.02 \pm 0.02, z=-1.47, p=.14$ ), indicating that GPT-3 did not apply random exploration in a strategic way and simply ignored the information about the task horizon. If GPT-3 applied directed exploration, we should find a positive effect of horizon in the unequal information condition, indicating that more informative actions were sampled more frequently when the horizon was longer. While humans show such an effect ${ }^{30}$, we did not find it in GPT-3 ( $\beta=-0.15 \pm 0.27, z=-0.56, p=.58$ ), which indicates that the model also did not employ directed exploration.</p>
<p>Lastly, we found that GPT-3 had a tendency to repeat previously observed options. For example, in the unequal information condition (Figure 3D), GPT-3 showed a strong bias to select the option from which it had seen more samples, even when there was not reason to do so. We believe that this bias partially arose from how GPT-3 was trained: if the goal is to predict future words, and the agent has recently observed a certain phrase (in this case either "Option F" or "Option J"), it is likely that the same phrase will appear again in the near future. Interestingly, humans also show such a perseveration bias in many situations ${ }^{34}$, but, in the case of the horizon task, it seems to be overruled by other processes.</p>
<h1>Deliberation: Model-based and model-free reinforcement learning</h1>
<p>Many realistic sequential decision-making problems do not only require the decision-maker to keep track of reward probabilities, but also to learn how to navigate from state to state within an environment. Two modes of learning are plausible in such scenarios: model-free and model-based learning. Model-free learning -the more habitual mode of the two- stipulates that the decision-maker should adjust its strategy directly using the actually observed rewards. If something led to a good outcome, a model-free agent will do more of it; if it led to a bad outcome, a model-free agent will do less of it. Model-based learning -the more deliberate mode of the two- instead stipulates that the agent should explicitly learn the transition and reward probabilities of the environment and use them to update its strategy by reasoning about future outcomes.</p>
<p>These two modes of learning can be disentangled empirically in the two-step task paradigm ${ }^{35}$. The two-step task involves a series of two-stage decision problems. There are two actions available from the initial state: taking a spaceship to planet X or to planet Y. Taking a spaceship transfers the agent to a second stage. The spaceship arrives with a probability of 0.7 to the selected planet, and with a probability of 0.3 to the other planet. After arriving at one of these planets, the agent encounters two local aliens with which it can trade. Trading with an alien can lead to receiving treasures or junk. The probabilities of receiving treasures are initialized randomly from a uniform distribution with a minimum of 0.25 and a maximum value of 0.75 for each alien. While these probabilities drift slowly over time to encourage learning, the first-stage transition probabilities remain fixed throughout the entire experiment. Model-free learning predicts that the probability of the selected first-stage action should increase upon receiving treasures in the second stage, regardless of whether the decision-maker experienced a rare or a common first-stage transition. Model-based learning, on the other hand, predicts that, upon encountering a rare transition and receiving treasures, the probability of the selected first-stage action should decrease. SI Appendix contains plots of simulated behavior for the two learning strategies. People tend to solve this task using a combination of model-free and model-based learning ${ }^{35-37}$ as shown in Figure 4A.</p>
<p>We tested how GPT-3 learns in the two-step task by providing it with prompts like the one shown in Figure 4B. We ran 200 simulations in total and measured the stay probability of the first-stage action for each combination of transition (rare or common) and reward (treasures or junk). Each simulation involved 20 repetitions of the two stages. Figure 4C visualizes our results. We observed that the probability of repeating the previous first-stage action decreased after finding treasures through a rare transition $(t(1982)=-6.16, p&lt;.001)$. Meanwhile, the probability of repeating the same first-stage action increased after a rare and not rewarded action $(t(1814)=2.33, p=.01)$. These two findings suggest that GPT-3 relies on a deliberate model-based approach to solve the two-step task. Interestingly, this conclusion is at odds with our earlier simulations on the CRT, where GPT-3 consistently chose the intuitive but wrong over the more deliberate but correct answer. The contrast between</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>C
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>B</p>
<p>You will travel to foreign planets in search of treasures. When you visit a planet, you can choose an alien to trade with. The chance of getting treasures from these aliens changes over time. Your goal is to maximize the number of received treasures.</p>
<p>Your previous space travels went as follows:</p>
<ul>
<li>3 days ago, you boarded the spaceship to planet X , arrived at planet X , traded with alien D , and received treasures.</li>
<li>2 days ago, you boarded the spaceship to planet Y , arrived at planet X , traded with alien D , and received junk.</li>
<li>1 day ago, you boarded the spaceship to planet Y , arrived at planet Y , traded with alien K , and received junk.</li>
</ul>
<p>Q: Do you want to take the spaceship to planet X or planet Y ?
A: Planet X .</p>
<p>You arrive at planet X .</p>
<p>Q: Do you want to trade with alien D or F?
A: Alien</p>
<p>Figure 4. Two-step task. A: Human behavior in dependency of rewarded and unrewarded as well as common and rare transitions. Human data adapted from Daw et al. ${ }^{35}$. B: Example prompt of one trial in the canonical two-step task as submitted to GPT-3. C: GPT-3's behavior in dependency of rewarded and unrewarded as well as common and rare transitions. Error bars indicate the standard error of the mean.
those two analyses suggests that the answer to whether GPT-3 engages in deliberate reasoning might be more nuanced than initially thought.</p>
<h1>Causal reasoning: Interventions after passive observations</h1>
<p>The analysis of the two-step task indicated that GPT-3 can learn a model of the environment and use this learned model to update its strategy. In our final test, we wanted to analyze whether GPT-3 can also use such a model to make more complex inferences, such as reasoning about cause and effect. From our earlier vignette-based investigations, we have already learned that GPT-3 can solve some causal reasoning problems, although these results depended heavily on how the problems were presented.</p>
<p>Perhaps the most crucial insight of theories of causal reasoning is that there is a difference between merely observing variables and actively manipulating them. Take, for instance, the classical example of a barometer. Under normal circumstances, barometer measurements provide insights into the upcoming weather. However, if someone would manually set the scale of the barometer to a particular value, then it would become totally uninformative about the weather - a clear difference from the observational inference. Waldman \&amp; Hagmayer ${ }^{38}$ devised an experiment to highlight that people are sensitive to the difference between seeing and doing. They first presented subjects with 20 observations of a three-variable system, and then provided additional information about the causal structure of the system. In the common-cause condition, they told participants that $A$ causes both $B$ and $C(B \leftarrow A \rightarrow C)$. In the causal-chain condition, they inverted the causal direction of $A$ and $B$, such that $B$ now causes $A$, which, as before, causes $C(B \rightarrow A \rightarrow C)$. Finally, they asked their subjects to imagine 20 new observations</p>
<p>You have previously observed the following chemical substances in different wine casks:</p>
<ul>
<li>Cask 1: substance $A$ was present, substance $B$ was present, substance $C$ was present.</li>
<li>Cask 2: substance $A$ was present, substance $B$ was present, substance $C$ was present.
$[\ldots]$</li>
<li>Cask 20: substance $A$ was absent, substance $B$ was absent, substance $C$ was absent.</li>
</ul>
<p>You have the following additional information from previous research:</p>
<ul>
<li>Substance $A$ likely causes the production of substance $B$.</li>
<li>Substance $A$ likely causes the production of substance $C$.</li>
</ul>
<p>Imagine that you test 20 new casks in which you have manually added substance $B$.</p>
<p>Q: How many of these new casks will contain substance $C$ on average?
A: [insert] casks.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 5. Causal reasoning. A: Example prompt for the causal reasoning task adapted from Waldman et al. ${ }^{38}$. B: Simulation results comparing GPT-3's responses with people as well as the ideal agent in the common-cause condition. C: Simulation results comparing GPT-3's responses with people as well as the ideal agent in the causal-chain condition.
for which they either had actively intervened on the values of $B$ or for which they merely had observed a particular value of $B$. Participants had to report for how many of these 20 new observations variable $C$ would be active. Like in the barometer example, observing an active value of $B$ in the common-cause condition enabled participants to make the inference that $A$ was likely to be active as well, which, in turn, made it more likely that $C$ was also active. In contrast, activating $B$ by means of interventions did not allow for such an inference. Mathematically, the act of intervening can be formalized by Pearl's $d o()$ operator ${ }^{39}$, which sets a variable to a particular value but deletes all arrows going into that variable from the causal graph. For the causal-chain condition one therefore would expect to find no differences between intervening and observing, as there was no arrow going into $B$ that had to be deleted, and hence both inferences were identical.</p>
<p>We probed GPT-3's ability to make causal inferences in this task using a cover story about substances found in different wine casks ${ }^{40}$ (see Figure 5A). When provided with the additional information about the common-cause structure, GPT-3 made interventional inferences that matched the normative prescription of causal inference as illustrated in Figure 5B. GPT-3 furthermore predicted an increase in the number of observations with $C=1$ after observing $B=1$, which was in line with both the normative theory and human judgments. However, when observing $B=0$, GPT-3 did not reduce its prediction, which was neither the correct inference nor human-like. The causal-chain condition does not lead to a difference between observational and interventional inferences from a normative perspective. While human subjects show exactly this pattern ${ }^{38}$, GPT-3 made identical predictions compared to the common-cause condition as illustrated in Figure 5C. This observation suggests that the model was not able to incorporate the additional information about the underlying causal structure into its inference process and therefore makes it likely that the results from the common-cause condition were purely accidental. Taken together, these results suggest that GPT-3 has difficulties with causal reasoning in tasks that go beyond a vignette-based characterization.</p>
<h1>Discussion</h1>
<p>In 1904, sixteen leading academics of the Prussian Academy of Sciences signed a statement indicating that a horse, named "Clever Hans", could solve mathematical problems at a human-like level. Back then, it took another scientist, Oskar Pfungst, years of systematic investigations to prove that the horse was merely reacting to the people who were watching him ${ }^{41}$. With the advent of large-scale machine learning models, the risks of over-interpreting simple behaviors as intelligent runs rampant. The abilities of large language models, in particular the ability to solve tasks beyond language generation, are impressive at first glance. These models have, therefore, been called many things; some think they are sentient ${ }^{42}$ and that they show a form of general intelligence ${ }^{7}$. Yet others believe that they are merely stochastic parrots ${ }^{43}$ or a linguistic one-trick pony ${ }^{8}$. But how can we realistically gauge these models' abilities?</p>
<p>We have argued to approach this problem similar to how Oskar Pfungst approached his object of study: via systematic investigations and psychological experimentation. Using tools from cognitive psychology, we have subjected one particular large language model, GPT-3, to a series of investigations, probing its decision-making, information search, deliberation, and causal reasoning abilities. Our results have shown that GPT-3 can solve some vignette-based experiments similarly or better than human subjects. However, interpreting these results is difficult because many of these vignettes might have been part of its training set, and GPT-3's performance suffered greatly given only minor changes to the original vignettes. We, therefore, turned the lens of our investigations to task-based assessments of GPT-3's abilities. Therein, we found that GPT-3 made reasonable decisions for gambles provided as descriptions while also mirroring some human behavioral biases. GPT-3 also managed to solve a multi-armed bandit task well, where it performed better than human subjects; yet it only showed traces of random but not of directed exploration. In the canonical two-step decision-making task, GPT-3 showed signatures of model-based reinforcement learning. However, GPT-3 failed spectacularly in using an underlying causal structure for its inference, leading to responses that were neither correct nor human-like.</p>
<p>What do we make of GPT-3's performance in our tasks? We believe that GPT-3's performance contained both surprising and expected elements. We found it surprising that GPT-3 could solve many of the provided tasks reasonably well, that it performed well in gambles, a simple bandit task, and even showed signatures of model-based reinforcement learning. These findings could indicate that -at least in some instances-GPT-3 is not just a stochastic parrot and could pass as a valid subject for some of the experiments we have administered. Yet what was not surprising were some of GPT-3's failure cases. GPT-3 did not show any signatures of directed exploration. We believe that this is intuitive and can be explained by the differences in how humans and GPT-3 learn about the world. Whereas humans learn by connecting with other people, asking them questions, and actively engaging with their environments, large language models learn by being passively fed a lot of text and predicting what word comes next. GPT-3 also failed to learn about and use causal knowledge in a simple reasoning task. Causal reasoning is frequently seen as a pillar of intelligent behavior ${ }^{44}$ and has been difficult to master for artificial agents ${ }^{45}$. We believe it makes sense that GTP-3 struggles to reason causally because acquiring knowledge about interventions from passive streams of data is hard to impossible ${ }^{46}$. The upside of our findings is the recommendation that to create more intelligent agents researchers should not only scale up algorithms that are passively fed with data but instead let agents directly interact and engage with the world ${ }^{47}$.</p>
<p>We are not the first to probe large-scale machine learning models' abilities. Indeed, recently there has been a push towards creating large benchmarks to assess the capability of foundation models ${ }^{48-50}$. Large language models have also been studied using other methods from cognitive psychology, such as property induction ${ }^{51}$, thinking-out-loud protocols ${ }^{52}$, or learning causal over-hypotheses ${ }^{53}$, where researchers have come to similar conclusions. Methods from cognitive psychology have also previously been applied to understand other deep learning models' behavior ${ }^{54}$. Therefore, our current work can be seen as part of a larger scientific movement where methods from psychology are becoming increasingly more important to understand capable black-box algorithms' learning and decision-making processes ${ }^{55-58}$.</p>
<p>Although we consider the present work as a step towards a psychological understanding of foundation models, several shortcomings remain. First of all, as we have seen in our vignette-based assessment, GPT-3's responses often times crucially depend on how a prompt is presented. The same might hold for our task-based assessments, where it is conceivable that GPT-3's behavior could change if the generating program of the tasks was modified. Yet we have simply tried to show that -in principle- GPT-3 could solve some of these tasks and believe that our current results emphasize the differences between GPT-3 and humans well. Secondly, we have only focused on a rather small subset of cognitive tasks, where we have tried to cover informative ground about GPT-3's abilities. Futures investigations could focus on additional psychological domains such as category learning, problem-solving, or economic games, to name but a few. Finally, our current results run the risk of portraying GPT-3 as more intelligent than it actually is, simply because canonical tasks taken from the psychological literature might be too easy to solve. In that sense, showing that large language models can perform well in such tasks might tell us more about how solvable and perhaps overly simplistic some tasks are than about GPT-3 itself and point to the importance of using more complex paradigms to study both natural and artificial agents ${ }^{59,60}$.</p>
<p>To summarize, we studied GPT-3, a recent large-scale language model, using tools from cognitive psychology. We assessed GPT-3's decision-making, information search, deliberation, and causal reasoning abilities, and found that it was able to solve</p>
<p>most of the presented tasks at a decent level. Less than two years ago, the sheer fact that a general-purpose language model could give reasonable responses to our problems would have been a large surprise. From this perspective, our analysis highlights how far these models have come. Nevertheless, we also found that small perturbations to the provided prompts easily led GPT-3 astray and that it lacks important features of human cognition, such as directed exploration and causal reasoning. While it does not seems so far-fetched that even larger models could acquire more robust and sophisticated reasoning abilities, we ultimately believe that actively interacting with the world will be crucial for matching the full complexity of human cognition. Fortunately, many user already interact with GPT-3-like models, and this number is only increasing with new applications on the horizon. Future language models will likely be trained on this data, leading to a natural interaction loop between artificial and natural agents.</p>
<h1>References</h1>
<ol>
<li>Gunning, D. et al. Xaiâexplainable artificial intelligence. Sci. Robotics 4, eaay7120 (2019).</li>
<li>Brown, T. et al. Language models are few-shot learners. Adv. neural information processing systems 33, 1877-1901 (2020).</li>
<li>Chen, M. et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).</li>
<li>Lin, Z. et al. Caire: An end-to-end empathetic chatbot. In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, 13622-13623 (2020).</li>
<li>Noever, D., Ciolino, M. \&amp; Kalin, J. The chess transformer: Mastering play using generative language models. arXiv preprint arXiv:2008.04057 (2020).</li>
<li>Drori, I. et al. A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. arXiv preprint arXiv:2112.15594 (2021).</li>
<li>Chalmers, D. Gpt-3 and general intelligence. Dly. Nous, July 30 (2020).</li>
<li>Marcus, G. \&amp; Davis, E. Gpt-3, bloviator: Openai's language generator has no idea what it's talking about. Technol. Rev. (2020).</li>
<li>Vaswani, A. et al. Attention is all you need. Adv. neural information processing systems 30 (2017).</li>
<li>Radford, A., Narasimhan, K., Salimans, T. \&amp; Sutskever, I. Improving language understanding by generative pre-training. None (2018).</li>
<li>Sutton, R. The bitter lesson. Incomplete Ideas (blog) 13, 12 (2019).</li>
<li>Liu, J. et al. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804 (2021).</li>
<li>Lampinen, A. K. et al. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329 (2022).</li>
<li>OpenAI API. https://beta.openai.com/overview. Accessed: 2022-06-20.</li>
<li>Tversky, A. \&amp; Kahneman, D. Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment. Psychol. review 90, 293 (1983).</li>
<li>Tversky, A. \&amp; Kahneman, D. Causal schemas in judgments under uncertainty. Prog. social psychology 1, 49-72 (2015).</li>
<li>Kahneman, D. \&amp; Tversky, A. Subjective probability: A judgment of representativeness. Cogn. psychology 3, 430-454 (1972).</li>
<li>Ruggeri, A., Sim, Z. L. \&amp; Xu, F. "why is toma late to school again?" preschoolers identify the most informative questions. Dev. psychology 53, 1620 (2017).</li>
<li>Baron, J., Beattie, J. \&amp; Hershey, J. C. Heuristics and biases in diagnostic reasoning: Ii. congruence, information, and certainty. Organ. Behav. Hum. Decis. Process. 42, 88-110 (1988).</li>
<li>Wason, P. C. Reasoning about a rule. Q. journal experimental psychology 20, 273-281 (1968).</li>
<li>Frederick, S. Cognitive reflection and decision making. J. Econ. perspectives 19, 25-42 (2005).</li>
<li>Nye, M., Tessler, M., Tenenbaum, J. \&amp; Lake, B. M. Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning. Adv. Neural Inf. Process. Syst. 34, 25192-25204 (2021).</li>
<li>Sobel, D. M., Yoachim, C. M., Gopnik, A., Meltzoff, A. N. \&amp; Blumenthal, E. J. The blicket within: Preschoolers' inferences about insides and causes. J. Cogn. Dev. 8, 159-182 (2007).</li>
<li>
<p>Nyhout, A. \&amp; Ganea, P. A. Mature counterfactual reasoning in 4-and 5-year-olds. Cognition 183, 57-66 (2019).</p>
</li>
<li>
<p>Srivastava, A. et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, DOI: 10.48550/ARXIV.2206.04615 (2022).</p>
</li>
<li>Peterson, J. C., Bourgin, D. D., Agrawal, M., Reichman, D. \&amp; Griffiths, T. L. Using large-scale experiments and machine learning to discover theories of human decision-making. Science 372, 1209-1214 (2021).</li>
<li>Ruggeri, K. et al. Replicating patterns of prospect theory for decision under risk. Nat. human behaviour 4, 622-633 (2020).</li>
<li>Kahneman, D. Prospect theory: An analysis of decisions under risk. Econometrica 47, 278 (1979).</li>
<li>Hertwig, R., Barron, G., Weber, E. U. \&amp; Erev, I. Decisions from experience and the effect of rare events in risky choice. Psychol. science 15, 534-539 (2004).</li>
<li>Wilson, R. C., Geana, A., White, J. M., Ludvig, E. A. \&amp; Cohen, J. D. Humans use directed and random exploration to solve the explore-exploit dilemma. J. Exp. Psychol. Gen. 143, 2074 (2014).</li>
<li>Gershman, S. J. Deconstructing the human algorithms for exploration. Cognition 173, 34-42 (2018).</li>
<li>Schulz, E. \&amp; Gershman, S. J. The algorithmic architecture of exploration in the human brain. Curr. opinion neurobiology 55, 7-14 (2019).</li>
<li>Zaller, I., Zorowitz, S. \&amp; Niv, Y. Information seeking on the horizons task does not predict anxious symptomatology. Biol. Psychiatry 89, S217-S218 (2021).</li>
<li>Gershman, S. J. Origin of perseveration in the trade-off between reward and complexity. Cognition 204, 104394 (2020).</li>
<li>Daw, N. D., Gershman, S. J., Seymour, B., Dayan, P. \&amp; Dolan, R. J. Model-based influences on humans' choices and striatal prediction errors. Neuron 69, 1204-1215 (2011).</li>
<li>GlÃ¤scher, J., Daw, N., Dayan, P. \&amp; OâDoherty, J. P. States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning. Neuron 66, 585-595 (2010).</li>
<li>Kool, W., Cushman, F. A. \&amp; Gershman, S. J. Competition and cooperation between multiple reinforcement learning systems. Goal-directed decision making 153-178 (2018).</li>
<li>Waldmann, M. R. \&amp; Hagmayer, Y. Seeing versus doing: two modes of accessing causal knowledge. J. Exp. Psychol. Learn. Mem. Cogn. 31, 216 (2005).</li>
<li>Pearl, J. Causality (Cambridge university press, 2009).</li>
<li>Meder, B., Hagmayer, Y. \&amp; Waldmann, M. R. Inferring interventional predictions from observational learning data. Psychon. Bull. \&amp; Rev. 15, 75-80 (2008).</li>
<li>Pfungst, O. Das Pferd des Herrn von Osten: der kluge Hans. Ein Beitrag zur experimentellen Tier-und MenschenPsychologie (Barth, 1907).</li>
<li>Luscombe, R. Google engineer put on leave after saying ai chatbot has become sentient. The Guard. .</li>
<li>Bender, E. M., Gebru, T., McMillan-Major, A. \&amp; Shmitchell, S. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610-623 (2021).</li>
<li>SchÃ¶lkopf, B. Causality for machine learning. In Probabilistic and Causal Inference: The Works of Judea Pearl, 765-804 (2022).</li>
<li>Lakretz, Y., Desbordes, T., Hupkes, D. \&amp; Dehaene, S. Causal transformers perform below chance on recursive nested constructions, unlike humans. arXiv preprint arXiv:2110.07240 (2021).</li>
<li>Dasgupta, I. et al. Causal reasoning from meta-reinforcement learning. arXiv preprint arXiv:1901.08162 (2019).</li>
<li>Hill, F. et al. Environmental drivers of systematicity and generalization in a situated agent. In International Conference on Learning Representations (2020).</li>
<li>Bommasani, R. et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021).</li>
<li>Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. \&amp; Iwasawa, Y. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 (2022).</li>
<li>Collins, K. M., Wong, C., Feng, J., Wei, M. \&amp; Tenenbaum, J. B. Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks. arXiv preprint arXiv:2205.05718 (2022).</li>
<li>
<p>Han, S. J., Ransom, K., Perfors, A. \&amp; Kemp, C. Human-like property induction is a challenge for large language models. PsyArXiv (2022).</p>
</li>
<li>
<p>Betz, G., Richardson, K. \&amp; Voigt, C. Thinking aloud: Dynamic context generation improves zero-shot reasoning performance of gpt-2. arXiv preprint arXiv:2103.13033 (2021).</p>
</li>
<li>Kosoy, E. et al. Towards understanding how machines can learn causal overhypotheses, DOI: 10.48550/ARXIV.2206.08353 (2022).</li>
<li>Ritter, S., Barrett, D. G., Santoro, A. \&amp; Botvinick, M. M. Cognitive psychology for deep neural networks: A shape bias case study. In International conference on machine learning, 2940-2949 (PMLR, 2017).</li>
<li>Rich, A. S. \&amp; Gureckis, T. M. Lessons for artificial intelligence from the study of natural stupidity. Nat. Mach. Intell. 1, 174-180 (2019).</li>
<li>Rahwan, I. et al. Machine behaviour. Nature 568, 477-486 (2019).</li>
<li>Schulz, E. \&amp; Dayan, P. Computational psychiatry for computers. Iscience 23, 101772 (2020).</li>
<li>Schramowski, P., Turan, C., Andersen, N., Rothkopf, C. A. \&amp; Kersting, K. Large pre-trained language models contain human-like biases of what is right and wrong to do. Nat. Mach. Intell. 4, 258-268 (2022).</li>
<li>Ma, W. J. \&amp; Peters, B. A neural network walks into a lab: towards using deep nets as models for human behavior. arXiv preprint arXiv:2005.02181 (2020).</li>
<li>BrÃ¤ndle, F., Binz, M. \&amp; Schulz, E. Exploration beyond bandits (2021).</li>
</ol>
<h1>SI Appendix</h1>
<h2>Vigettes</h2>
<p>Tables 1, 2, 3, 4 and 5 contain a detailed description of submitted prompts and GPT-3's corresponding answers.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Problem</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Response</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Linda</td>
<td style="text-align: center;">Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. <br> Q: Which option is the most probable? <br> - Option 1: Linda is a bank teller. <br> - Option 2: Linda is a bank teller and is active in the feminist movement. <br> - Option 3: Linda is a member of the NRA.</td>
<td style="text-align: center;">A: Option 2.</td>
</tr>
<tr>
<td style="text-align: center;">Cab</td>
<td style="text-align: center;">A cab was involved in a hit-and-run accident at night. Two cab companies, the Green and the Blue, operate in the city. You are given the following data: $85 \%$ of the cabs in the city are Green and $15 \%$ are Blue. A witness identified the cab as Blue. The court tested the reliability of the witness under the circumstances that existed on the night of the accident and concluded that the witness correctly identified each one of the two colors $80 \%$ of the time and failed $20 \%$ of the time. <br> Q: What is the probability that the cab involved in the accident was Blue rather than Green?</td>
<td style="text-align: center;">A: The probability that the cab involved in the accident was Blue rather than Green is 0.4 .</td>
</tr>
<tr>
<td style="text-align: center;">Hospital</td>
<td style="text-align: center;">A certain town is served by two hospitals. In the larger hospital, about 45 babies are born each day, and in the smaller hospital, about 15 babies are born each day. As you know, about $50 \%$ of all babies are boys. The exact percentage of baby boys, however, varies from day to day. Sometimes it may be higher than $50 \%$, sometimes lower. For a period of 1 year, each hospital recorded the days on which more than $60 \%$ of the babies born were boys. Q: Which hospital do you think recorded more such days? <br> - Option 1: The larger hospital. <br> - Option 2: The smaller hospital. <br> - Option 3: About the same (i.e., within 5\% of each other).</td>
<td style="text-align: center;">A: Option 3.</td>
</tr>
</tbody>
</table>
<p>Table 1. Prompts and answers for vignettes testing GPT-3's decision-making.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Problem</th>
<th style="text-align: left;">Prompt</th>
<th style="text-align: left;">Response</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Toma <br> v1</td>
<td style="text-align: left;">In the last 6 days, Toma always arrived late to school. <br> On the first day, Toma was late because he could not find his jacket. <br> On the second day, Toma was late because his bicycle broke. <br> On the third day, Toma was late because his bicycle broke. <br> On the fourth day, Toma was late because he could not find his shoes. <br> On the fifth day, Toma was late because he was watching TV. <br> On the sixth day, Toma was late because he could not find his books. <br> Today, on the seventh day, Toma was again late to school.</td>
<td style="text-align: left;">A: Option 1.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Q: Which of the following question would be better if wanted to find out why Toma was again late <br> to school. <br> -Option 1: Were you late because you could not find something? <br> -Option 2: Were you late because your bicycle was broken?</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Toma <br> v2</th>
<th style="text-align: center;">In the last 8 days, Toma always arrived late to school. <br> On the first day, Toma was late because he could not find his jacket. <br> On the second day, Toma was late because his bicycle broke. <br> On the third day, Toma was late because his bicycle broke. <br> On the fourth day, Toma was late because he could not find his shoes. <br> On the fifth day, Toma was late because his bicycle broke. <br> On the sixth day, Toma was late because his bicycle broke. <br> On the seventh day, Toma was late because he was watching TV. <br> On the eighth day, Toma was late because his bicycle broke. <br> Today, on the ninth day, Marcel was again late to school. <br> Q: Which of the following question would be better if wanted to find out why Toma was again late to school. <br> -Option 1: Were you late because you could not find something? <br> -Option 2: Were you late because your bicycle was broken?</th>
<th style="text-align: center;">A: Option 2.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">A patient has a 0.8 probability of having Chamber-of-Commerce disease and a 0.2 probability of Elk's disease. (He surely has one or the other.) A tetherscopic examination yields a positive result in $90 \%$ of patients with Chamber-of-Commerce disease and in $20 \%$ of patients without it (including those with some other disease). An intraocular smear yields a positive result in $90 \%$ of patients with Elk's disease and in $10 \%$ of patients without it. <br> Q: Which test would you run on the patient? <br> - Option 1: A tetherscopic examination. <br> - Option 2: An intraocular smear. <br> - Option 3: Both are equally good.</td>
<td style="text-align: center;">A: Option 1.</td>
</tr>
<tr>
<td style="text-align: center;">Wason</td>
<td style="text-align: center;">You are shown a set of four cards placed on a table, each of which has a number on one side and a colored patch on the other side. The visible faces of the cards show A, K, 4, and 7. <br> Q: Which cards must you turn over in order to test the truth of the proposition that if a card shows a vowel on one face, then its opposite face shows an even number?</td>
<td style="text-align: center;">A: The cards that must be turned over are the A and the 7 .</td>
</tr>
</tbody>
</table>
<p>Table 2. Prompts and answers for vignettes testing GPT-3's information search abilities.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Problem</th>
<th style="text-align: left;">Prompt</th>
<th style="text-align: left;">Response</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CRT1</td>
<td style="text-align: left;">A bat and a ball cost 1.10intotal. Thebatcosts1.00 more than the ball. <br> Q: How much does the ball cost?</td>
<td style="text-align: left;">A: The ball costs $\$ 0.10$.</td>
</tr>
<tr>
<td style="text-align: left;">CRT2</td>
<td style="text-align: left;">Q: If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to <br> make 100 widgets?</td>
<td style="text-align: left;">A: 100 machines would <br> make 100 widgets in 100 <br> minutes.</td>
</tr>
<tr>
<td style="text-align: left;">CRT3</td>
<td style="text-align: left;">In a lake, there is a patch of lily pads. Every day, the patch doubles in size. <br> Q: If it takes 48 days for the patch to cover the entire lake, how long would it take for the patch to <br> cover half of the lake?</td>
<td style="text-align: left;">A: It would take 24 days <br> for the patch to cover half <br> of the lake.</td>
</tr>
</tbody>
</table>
<p>Table 3. Prompts and answers for vignettes testing GPT-3's intuitive vs. reflective answers. Items are based on the Cognitive Reflection Test (CRT).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Problem</th>
<th style="text-align: left;">Prompt</th>
<th style="text-align: left;">Response</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Blickets</th>
<th style="text-align: center;">Blickets are objects that turn on a machine. I have two objects, A and B, and a machine. My goal is to find out if any or both of these objects are a blicket. When I put object A on the machine, the machine turned on. When I put object A and object B on the machine, the machine turned on. When I put object B on the machine, the machine did not turn on. <br> Q: Do you think object A is a blicket? <br> Q: Do you think object B is a blicket? <br> Q: Why is object B not a blicket?</th>
<th style="text-align: center;">A: Yes, I think object A is a blicket. <br> A: No, I think object B is not a blicket. <br> A: I think object B is not a blicket because it did not turn on the machine when it was placed on the machine by itself.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Intervene</td>
<td style="text-align: center;">I am allergic to flowers. When my friend gave me red flowers, I did not sneeze. When my friend gave me yellow flowers, I did not sneeze. When my friend gave me blue flowers, I sneezed. <br> Q: Imagine a bouquet containing red, yellow, and blue flowers. Which flowers would need to be removed such that I would not show an allergic reaction?</td>
<td style="text-align: center;">A: The blue flowers would need to be removed.</td>
</tr>
<tr>
<td style="text-align: center;">Mature</td>
<td style="text-align: center;">Imagine there are four different pills: pill A, pill B, pill C, and pill D. Pill A can kill people. Pill B can kill people. Pill C cannot kill people. Pill D cannot kill people. <br> Q: A man took pill A and pill B and he died. If he had not taken pill A, could he still have died, yes or no? <br> Q: A man took pill B and pill C and he died. If he had not taken pill B, could he still have died, yes or no? <br> Q: A man took pill A and pill D and he died. If he had not taken pill D, could he still have died, yes or no? <br> Q: A man took pill C and pill D and he did not die. If he had not taken pill D, could he have died, yes or no?</td>
<td style="text-align: center;">A: Yes <br> A: No <br> A: Yes <br> A: No</td>
</tr>
</tbody>
</table>
<p>Table 4. Prompts and answers for vignettes testing GPT-3's causal reasoning abilities. In cases for which there are more than one Q , questions were asked in succession.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Problem</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Response</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Black <br> Cab</td>
<td style="text-align: center;">A cab was involved in a hit-and-run accident at night. Two cab companies, the Green and the Blue, operate in the city. You are given the following data: $85 \%$ of the cabs in the city are Green and $15 \%$ are Blue. A witness identified the cab as Blue. The court tested the reliability of the witness under the circumstances that existed on the night of the accident and concluded that the witness correctly identified each one of the two colors $80 \%$ of the time and failed $20 \%$ of the time. <br> Q: What is the probability that the cab involved in the accident was black?</td>
<td style="text-align: center;">A: The probability that the cab involved in the accident was black is 0.2 .</td>
</tr>
<tr>
<td style="text-align: center;">Reverse <br> Wason</td>
<td style="text-align: center;">You are shown a set of four cards placed on a table, each of which has a number on one side and a colored patch on the other side. The visible faces of the cards show $4,7, \mathrm{~A}$, and K . <br> Q: Which cards must you turn over in order to test the truth of the proposition that if a card shows a vowel on one face, then its opposite face shows an even number?</td>
<td style="text-align: center;">A: The A and the K.</td>
</tr>
<tr>
<td style="text-align: center;">Wrong <br> CRT</td>
<td style="text-align: center;">A bat and a ball cost 1.10 intotal. Thebatcosts 1.00 more than the bat. <br> Q: How much does the ball cost?</td>
<td style="text-align: center;">A: The ball costs $\$ 0.10$.</td>
</tr>
<tr>
<td style="text-align: center;">Immature <br> Blicket</td>
<td style="text-align: center;">Imagine a box that makes a sound if certain objects are placed on it. When I put a red object on it, the box made a sound. When I put a green object on it, the box made a sound. When I put a blue object on it, the box did not make a sound. When I put a yellow object on it, the box did not make a sound. <br> Q: I put a green and a yellow object on the box and it made a sound. Would it still have made a sound if I had not put the yellow object on it? <br> Q: Would the box make a sound if I only put a green object on it? <br> Q: But if there are two objects on the box, a green and a yellow one, and I remove the yellow one, then there would only be a green object on it. Why does that not turn on the machine?</td>
<td style="text-align: center;">A: No, the box would not have made a sound if you had not put the yellow object on it. <br> A: Yes, the box would make a sound if you only put a green object on it. <br> A: The box would not make a sound if there is only a green object on it.</td>
</tr>
</tbody>
</table>
<p>Table 5. Prompts and answers for vignettes deliberately designed to confuse GPT-3. In cases for which there are more than one Q , questions were asked in succession.</p>
<h1>Decisions from descriptions</h1>
<p>Following Kahneman \&amp; Tversky ${ }^{28}$, we tested whether a cognitive bias is present by contrasting answers to two decisions from description problems. The full list of these problems can be found in Table 6. Table 7 shows a list of used contrasts. For each contrast, we obtained the probability of selecting option F (setting GPT-3's temperature parameter to 1) and then computed the log-odds ratio between the choice probabilities of both questions. The order of presented options was counterbalanced. Positive log-odds ratios indicate that the given bias is present.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Problem</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">$p(\mathrm{~F})$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $33 \%$ chance at 2,500 dollars, a $66 \%$ chance at 2,400 dollars, and a $1 \%$ chance of 0 dollars. <br> - Option J: Guaranteed 2,400 dollars. <br> A: Option</td>
<td style="text-align: center;">0.20455745</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $33 \%$ chance of 2,500 dollars ( $67 \%$ chance of 0 dollars). <br> - Option J: $34 \%$ chance of 2,400 dollars ( $66 \%$ chance of 0 dollars). <br> A: Option</td>
<td style="text-align: center;">0.38613685</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $80 \%$ chance of 4,000 dollars ( $20 \%$ chance of 0 dollars). <br> - Option J: $100 \%$ guarantee of 3,000 dollars. <br> A: Option</td>
<td style="text-align: center;">0.27955511</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $20 \%$ chance of 4,000 dollars ( $80 \%$ chance of 0 dollars). <br> - Option J: $25 \%$ chance of 3,000 dollars ( $75 \%$ chance of 0 dollars). <br> A: Option</td>
<td style="text-align: center;">0.22312672</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $45 \%$ chance of 6,000 dollars ( $55 \%$ chance of 0 dollars). <br> - Option J: $90 \%$ chance of 3,000 dollars ( $10 \%$ chance of 0 dollars). <br> A: Option</td>
<td style="text-align: center;">0.05952152</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $0.1 \%$ chance of 6,000 dollars ( $99.9 \%$ chance of 0 dollars). <br> - Option J: $0.2 \%$ chance of 3,000 dollars ( $99.8 \%$ chance of 0 dollars). <br> A: Option</td>
<td style="text-align: center;">0.4925257</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $80 \%$ chance of losing 4,000 dollars ( $20 \%$ chance of losing 0 dollars). <br> - Option J: $100 \%$ guarantee of losing 3,000 dollars. <br> A: Option</td>
<td style="text-align: center;">0.69021772</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $20 \%$ chance of losing 4,000 dollars ( $80 \%$ chance of losing 0 dollars). <br> - Option J: $25 \%$ chance of losing 3,000 dollars ( $75 \%$ chance of losing 0 dollars). <br> A: Option</td>
<td style="text-align: center;">0.37691269</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $45 \%$ chance of losing 6,000 dollars ( $55 \%$ chance of losing 0 dollars). <br> - Option J: $90 \%$ chance of losing 3,000 dollars ( $10 \%$ chance of losing 0 dollars). <br> A: Option</td>
<td style="text-align: center;">0.49701181</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $0.1 \%$ chance of losing 6,000 dollars ( $99.9 \%$ chance of losing 0 dollars). <br> - Option J: $0.2 \%$ chance of losing 3,000 dollars ( $99.8 \%$ chance of losing 0 dollars). <br> A: Option</td>
<td style="text-align: center;">0.34726597</td>
</tr>
</tbody>
</table>
<p>| 11 | Imagine you are playing a game with two levels, but you have to make a choice about the second level before you know the outcome of the first. At the first level, there is a $75 \%$ chance that the game will end without you winning anything, and a $25 \%$ chance that you will advance to the second level.
Q: What would you choose in the second level? <br> - Option F: $80 \%$ chance of 4,000 dollars ( $20 \%$ chance of 0 dollars). <br> - Option J: $100 \%$ guarantee of 3,000 dollars. <br> A: Option | 0.66757223 |
| :--: | :--: | :--: |
| 12 | Imagine we gave you 1,000 dollars right now to play a game. <br> Q: Which option do you prefer? <br> - Option F: $50 \%$ chance to gain an additional 1,000 dollars ( $50 \%$ chance of gaining 0 dollars beyond what you already have). <br> - Option J: $100 \%$ guarantee of gaining an additional 500 dollars. <br> A: Option | 0.51510189 |
| 13 | Imagine we gave you 2,000 dollars right now to play a game. <br> Q: Which option do you prefer? <br> - Option F: $50 \%$ chance you will lose 1,000 dollars ( $50 \%$ chance of losing 0 dollars). <br> - Option J: $100 \%$ chance you will lose 500 dollars. <br> A: Option | 0.61653453 |
| 14 | Q: Which option do you prefer? <br> - Option F: $25 \%$ chance of 6,000 dollars ( $75 \%$ chance of 0 dollars). <br> - Option J: $25 \%$ chance of 4,000 dollars ( $25 \%$ chance of 2,000 dollars, $50 \%$ chance of 0 dollars). <br> A: Option | 0.21011495 |
| 15 | Q: Which option do you prefer? <br> - Option F: $25 \%$ chance of losing 6,000 dollars ( $75 \%$ chance of losing nothing). <br> - Option J: $25 \%$ chance of losing 4,000 dollars ( $25 \%$ chance of 2,000 dollars, $50 \%$ chance of 0 dollars). <br> A: Option | 0.20015262 |
| 16 | Q: Which option do you prefer? <br> - Option F: $0.1 \%$ chance at 5,000 dollars ( $99.9 \%$ chance of 0 dollars). <br> - Option J: $100 \%$ guarantee of 5 dollars. <br> A: Option | 0.09527163 |
| 17 | Q: Which option do you prefer? <br> - Option F: $0.1 \%$ chance of losing 5,000 dollars ( $99.9 \%$ chance of losing nothing). <br> - Option J: $100 \%$ guarantee of losing 5 dollars. <br> A: Option | 0.82562455 |</p>
<p>Table 6. Decision from description problems used for the contrast analysis from Figure 2C. The rightmost column displays GPT-3's choice probabilities.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Contrast ID</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
<th style="text-align: center;">7</th>
<th style="text-align: center;">8</th>
<th style="text-align: center;">9</th>
<th style="text-align: center;">10</th>
<th style="text-align: center;">11</th>
<th style="text-align: center;">12</th>
<th style="text-align: center;">13</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Problem 1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: left;">Problem 2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">16</td>
</tr>
</tbody>
</table>
<p>Table 7. List of contrasts used for the analysis of Figure 2C.</p>
<p>Two-step task</p>
<p>Figure 6 contains simulated behavior of a model-free and model-based reinforcement learning algorithm. For a detailed description of these algorithms, see Daw et al. ${ }^{35}$.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 6. Model simulations on the two-step task. A: Model-free reinforcement learning algorithm. B: Model-based reinforcement learning algorithm. Figure adapted from Daw et al. ${ }^{35}$.</p>            </div>
        </div>

    </div>
</body>
</html>
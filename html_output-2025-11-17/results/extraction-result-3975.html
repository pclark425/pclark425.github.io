<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3975 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3975</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3975</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-92.html">extraction-schema-92</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-9ec42d155e2014e86ab49adcf76fd40a41a867ea</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9ec42d155e2014e86ab49adcf76fd40a41a867ea" target="_blank">Evaluating large language models on a highly-specialized topic, radiation oncology physics</a></p>
                <p><strong>Paper Venue:</strong> Frontiers in Oncology</p>
                <p><strong>Paper Abstract:</strong> Purpose We present the first study to investigate Large Language Models (LLMs) in answering radiation oncology physics questions. Because popular exams like AP Physics, LSAT, and GRE have large test-taker populations and ample test preparation resources in circulation, they may not allow for accurately assessing the true potential of LLMs. This paper proposes evaluating LLMs on a highly-specialized topic, radiation oncology physics, which may be more pertinent to scientific and medical communities in addition to being a valuable benchmark of LLMs. Methods We developed an exam consisting of 100 radiation oncology physics questions based on our expertise. Four LLMs, ChatGPT (GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against medical physicists and non-experts. The performance of ChatGPT (GPT-4) was further explored by being asked to explain first, then answer. The deductive reasoning capability of ChatGPT (GPT-4) was evaluated using a novel approach (substituting the correct answer with “None of the above choices is the correct answer.”). A majority vote analysis was used to approximate how well each group could score when working together. Results ChatGPT GPT-4 outperformed all other LLMs and medical physicists, on average, with improved accuracy when prompted to explain before answering. ChatGPT (GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices across a number of trials, whether correct or incorrect, a characteristic that was not observed in the human test groups or Bard (LaMDA). In evaluating deductive reasoning ability, ChatGPT (GPT-4) demonstrated surprising accuracy, suggesting the potential presence of an emergent ability. Finally, although ChatGPT (GPT-4) performed well overall, its intrinsic properties did not allow for further improvement when scoring based on a majority vote across trials. In contrast, a team of medical physicists were able to greatly outperform ChatGPT (GPT-4) using a majority vote. Conclusion This study suggests a great potential for LLMs to work alongside radiation oncology experts as highly knowledgeable assistants.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3975.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3975.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RO-100 Exam</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>100-question Radiation Oncology Physics Multiple-Choice Examination</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bespoke 100-item multiple-choice exam created by the authors covering seven radiation oncology physics subtopics (basic physics, radiation measurements, treatment planning, imaging, brachytherapy, advanced planning/special procedures, safety/QA) used as the primary benchmark to evaluate LLM performance and compare to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy on exam items (overall and by category), consistency across repeated trials, ability to perform math-based questions, deductive reasoning when correct choice is removed, change in accuracy when prompted to explain first.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Design a new MCQ exam to avoid training-data contamination; run each LLM across 5 independent trials (new threads/resets) with controlled initialization and instruction prompts; compare LLMs to two human groups (medical physicists and non-experts) taking the same exam; apply ablations: 'explain-first-then-answer' prompting and a deductive-reasoning transformation (replace correct option with 'None of the above') ; aggregate answers using majority-vote analyses; compute statistical measures (mean score, standard deviation across trials, average Pearson correlation between trials) and compare to expected random-guess distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>A novel 100-question radiation oncology physics MCQ exam (17 numeric/maths items) created by an experienced medical physicist for this study; questions aligned to ABR study guide categories and listed in the paper's appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Mean percent-correct (overall and by category), standard deviation of total scores across trials, average Pearson correlation between trials, per-question frequency of being correct across trials (distribution vs Poisson random-guess expectation), majority-vote score improvements, specific score changes due to explain-first prompting and deductive test (e.g., GPT-4 improved overall by ~5% with explain-first; GPT-4 scored ~67% correct per trial baseline; medical physicist majority vote improved by 23%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>High — humans created the exam, served as comparative test-takers (four board-certified medical physicists + residents/fellows; six non-experts with advanced degrees outside radiation oncology), and provided ground-truth/benchmark for comparison; human raters also informed RLHF discussion in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Exam is not fully representative of nuanced daily clinical tasks; single custom exam may not generalize; small human-group sample sizes; some LLMs (BLOOMZ) only tested once; possible but minimized risk of training-data contamination (authors intentionally created new questions); comparing repeated LLM trials to human retesting is not strictly symmetric; high consistency/confident-but-wrong behavior in LLMs could indicate memorization/overfitting rather than robust understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4 attained the best performance among evaluated LLMs and outperformed the average medical physicist on this exam; explain-first prompting increased GPT-4's overall score by ~5% with notable gains in brachytherapy and math subcategories; GPT-4 showed high trial-to-trial consistency (low SD, high correlation), and surprisingly nontrivial performance (55% when explain-first) on the deductive 'None of the above' transformation; majority-vote aggregation improved human expert group performance substantially (23%) but yielded minimal gains for GPT-3.5/GPT-4 (~1%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating large language models on a highly-specialized topic, radiation oncology physics', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3975.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3975.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Explain-first</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explain-first then answer (chain-of-thought prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy instructing the LLM to produce an explanation or intermediate reasoning before emitting the final answer, used here to develop multi-step reasoning and potentially improve accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Improvement in answer accuracy (overall and by category), especially on reasoning- or math-intensive items, and impact on deductive/rejection-style problems.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Modify LLM instruction to: 'first give an explanation for the answer followed by the correct answer (letter choice),' then compare performance to baseline runs where only the answer was requested; analyze per-category changes and aggregate score deltas.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to the same 100-question radiation oncology physics exam as the baseline condition.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Overall score improvement (reported ~+5% for GPT-4), category-specific gains (noted largest in brachytherapy and math-based questions), improvement on deductive-reasoning test (increase to ~55% overall when explain-first was combined with the 'None of the above' transformation).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>No additional human raters required for this ablation; authors executed the modified prompts and compared automated outputs to ground-truth answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Explain-first may expose chain-of-thought which can be undesirable in some deployment settings; gains may depend heavily on model scale and architecture (emergent property) and thus may not transfer to smaller models; potential for the LLM to 'hallucinate' plausible but incorrect reasoning while still producing a correct-seeming answer.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Prompting GPT-4 to explain before answering increased its accuracy by approximately 5% overall, with the largest improvements in brachytherapy and math categories; the method also improved GPT-4's performance on the deductive 'None of the above' task, suggesting chain-of-thought prompting aided its deductive capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating large language models on a highly-specialized topic, radiation oncology physics', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3975.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3975.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deductive-test</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deductive reasoning test via 'None of the above' substitution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel evaluation transformation where the true correct option in each MCQ is replaced with 'None of the above choices is the correct answer' to force the model to deduce the correctness by rejecting the distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Ability to perform deductive reasoning by eliminating contextually plausible but incorrect choices; sensitivity to training-context patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>For each MCQ, remove the known correct choice and replace it with 'None of the above...'; run the modified exam through the LLM with baseline and explain-first prompting; humans were not tested under this transformation because they'd detect the pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to the same 100-question radiation oncology physics exam with the described transformation.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Overall percent-correct on transformed exam (GPT-4 scored ~55% when explain-first was used; baseline transformed score lower), comparisons to original exam scores.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Intentionally excluded humans from this specific transformed test because the pattern would be obvious to humans and bias their responses; authors ran LLM evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Unrealistic for human testers (pattern-detection) and may not reflect typical real-world prompts; transformation deliberately reduces contextual cues for the true correct answer (removes context overlap), so it tests a particular form of deductive skill that may not generalize; interpretation of high scores could indicate emergent abilities or possible artifacts of model training not fully understood.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4 demonstrated surprisingly strong performance on the deductive-transformed exam (notably when using explain-first prompting), suggesting an emergent deductive capability in larger LLMs; authors note this result is perplexing and merits further study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating large language models on a highly-specialized topic, radiation oncology physics', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3975.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3975.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Majority-vote</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Majority-vote aggregation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble-style evaluation where, for each question, the most common answer across multiple trials (for LLMs) or across group members (for humans) is selected as the group's final answer, used to estimate collaborative performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Potential gain in accuracy from aggregating multiple independent responses, reflecting how groups (human or model ensembles) might improve reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>For each question take the modal answer from 5 LLM trials or from the human group; break ties randomly; calculate resulting group score and compare to individual mean scores.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to the study's 100-question exam using multiple LLM trials and human group responses.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Percent-correct after majority aggregation and percent improvement over individual mean: GPT-3.5/GPT-4 improved ~1%, Bard ~4%, non-experts ~3%, medical physicists ~23% improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Direct — human group answers were aggregated; authors used human group majority-vote as a simulated team performance metric.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLMs' high per-trial consistency (confident but possibly consistently wrong) limits ensemble benefit; human group heterogeneity yields larger ensemble gains; tie-breaking randomness can affect small-sample aggregated score.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Majority-vote showed negligible benefit for GPT-3.5/GPT-4 (about +1%) but large benefit for the medical physicist group (+23%), indicating human diversity leads to better ensemble performance than aggregating multiple runs of a consistent LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating large language models on a highly-specialized topic, radiation oncology physics', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3975.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3975.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Consistency metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Trial-to-trial consistency and confidence analysis (std dev, Pearson correlation, per-question correct-frequency)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Statistical measures used to quantify how stable an LLM's answers were across independent runs and to measure confidence-like behavior via repeated correctness rates on individual items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Low standard deviation across trials and high average Pearson correlation indicate high consistency; the distribution of the number of times a question was answered correctly across trials indicates 'confidence' vs randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Compute standard deviation of total scores across trials; compute the average of the upper triangle of the Pearson correlation matrix between trial answer-vectors; count for each question how many of the five trials produced a correct answer and compare the empirical distribution to a Poisson-based model for random guessing.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to responses on the 100-question exam across repeated LLM trials and human retakes.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Reported low standard deviation and high average correlation for GPT-3.5/GPT-4 relative to human groups; specific reported values include GPT-4 averaging ~67% correct per trial and ~14% incorrect per trial (characterizing confident correctness and confident errors), GPT-3.5 having modes around 35% correct or 28% incorrect per trial; LLMs deviate strongly from Poisson random-guess expectation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Humans provided the comparative repeated-answer data (humans were tested only once each, but group distributions compared); authors computed and interpreted the statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Human repeated-testability differs from automated multiple-trial LLM testing (humans not retested in same manner); high LLM consistency may reflect memorization or overfit rather than robust reasoning; interpreting 'confidence' from repeated correctness counts is indirect.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LLMs (especially GPT-4) were markedly more consistent across trials than human test-takers, showing both concentrated correct responses and concentrated incorrect responses (i.e., confident wrongs), which reduced ensemble gains from majority-vote aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating large language models on a highly-specialized topic, radiation oncology physics', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3975.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3975.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contamination avoidance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training-data contamination avoidance via new question creation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The practice of creating novel test items not present in public corpora to avoid evaluating models on questions present in their training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Minimize the probability that evaluated items were included in model training sets so that reported performance reflects generalization rather than memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Design and use an original examination (100 new MCQs) written by domain experts rather than relying on public exams known to be widely represented in training corpora; explicitly state the intention to avoid contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Original 100-question exam used as the primary benchmark in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>No direct metric for contamination; approach described as best-practice to reduce contamination risk; authors reference the need to ensure test questions are left out of training data.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Domain experts (experienced medical physicist and collaborators) authored the exam to ensure novelty and domain validity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Creating new items reduces contamination risk but cannot guarantee absence of similar exemplars in the training corpus; small custom benchmarks may limit comparability to other studies and to broader capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Authors argue that using newly created, highly-specialized exam items yields a fairer test of LLM generalization in an obscure domain (radiation oncology physics) compared to widely circulated standardized exams.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating large language models on a highly-specialized topic, radiation oncology physics', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3975.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3975.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Limitations-summary</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Study limitations and evaluation challenges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Collective statements in the discussion that qualify the evaluation methods, noting risks of superficial assessment, representativeness, overfitting, and generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Assess whether the benchmark and evaluation methods actually probe deep domain competence, clinical applicability, and robustness rather than superficial knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Critical analysis and interpretation of results; comparison of exam-based performance to real-world clinical complexity and team-based human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>The single customized 100-question exam and its applicability to represent everyday clinical tasks were discussed as a limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Qualitative limitations (e.g., exam not representing clinical nuance) and inference that high scores could indicate superficiality; quantitative limitations include small sample sizes and limited trials for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human experts contributed to authorship, exam design, testing, and interpretation of limitations; authors recommend cautious interpretation by the radiation oncology community.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Exam may misrepresent clinical competency; LLMs' high scores might reflect superficial knowledge or memorization; small, specialized test set may not generalize; uneven trial counts across LLMs; inability to fairly simulate repeated human answering; potential unknown overlaps with training data despite new-question creation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Authors conclude the exam demonstrates strong LLM potential as assistants but caution against equating exam performance with real-world clinical competence; they also propose that LLMs could be used as a test for superficiality and suggest shifting certification to probe knowledge not known by LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating large language models on a highly-specialized topic, radiation oncology physics', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gpt-4 technical report <em>(Rating: 2)</em></li>
                <li>Sparks of artificial general intelligence: early experiments with gpt-4 <em>(Rating: 2)</em></li>
                <li>Emergent abilities of large language models <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>Can language models learn from explanations in context? <em>(Rating: 2)</em></li>
                <li>Capabilities of gpt-4 on medical challenge problems <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3975",
    "paper_id": "paper-9ec42d155e2014e86ab49adcf76fd40a41a867ea",
    "extraction_schema_id": "extraction-schema-92",
    "extracted_data": [
        {
            "name_short": "RO-100 Exam",
            "name_full": "100-question Radiation Oncology Physics Multiple-Choice Examination",
            "brief_description": "A bespoke 100-item multiple-choice exam created by the authors covering seven radiation oncology physics subtopics (basic physics, radiation measurements, treatment planning, imaging, brachytherapy, advanced planning/special procedures, safety/QA) used as the primary benchmark to evaluate LLM performance and compare to humans.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Accuracy on exam items (overall and by category), consistency across repeated trials, ability to perform math-based questions, deductive reasoning when correct choice is removed, change in accuracy when prompted to explain first.",
            "evaluation_methods": "Design a new MCQ exam to avoid training-data contamination; run each LLM across 5 independent trials (new threads/resets) with controlled initialization and instruction prompts; compare LLMs to two human groups (medical physicists and non-experts) taking the same exam; apply ablations: 'explain-first-then-answer' prompting and a deductive-reasoning transformation (replace correct option with 'None of the above') ; aggregate answers using majority-vote analyses; compute statistical measures (mean score, standard deviation across trials, average Pearson correlation between trials) and compare to expected random-guess distributions.",
            "benchmark_or_dataset": "A novel 100-question radiation oncology physics MCQ exam (17 numeric/maths items) created by an experienced medical physicist for this study; questions aligned to ABR study guide categories and listed in the paper's appendix.",
            "metrics_reported": "Mean percent-correct (overall and by category), standard deviation of total scores across trials, average Pearson correlation between trials, per-question frequency of being correct across trials (distribution vs Poisson random-guess expectation), majority-vote score improvements, specific score changes due to explain-first prompting and deductive test (e.g., GPT-4 improved overall by ~5% with explain-first; GPT-4 scored ~67% correct per trial baseline; medical physicist majority vote improved by 23%).",
            "human_involvement": "High — humans created the exam, served as comparative test-takers (four board-certified medical physicists + residents/fellows; six non-experts with advanced degrees outside radiation oncology), and provided ground-truth/benchmark for comparison; human raters also informed RLHF discussion in related work.",
            "limitations_or_challenges": "Exam is not fully representative of nuanced daily clinical tasks; single custom exam may not generalize; small human-group sample sizes; some LLMs (BLOOMZ) only tested once; possible but minimized risk of training-data contamination (authors intentionally created new questions); comparing repeated LLM trials to human retesting is not strictly symmetric; high consistency/confident-but-wrong behavior in LLMs could indicate memorization/overfitting rather than robust understanding.",
            "llm_theory_example": null,
            "evaluation_results": "GPT-4 attained the best performance among evaluated LLMs and outperformed the average medical physicist on this exam; explain-first prompting increased GPT-4's overall score by ~5% with notable gains in brachytherapy and math subcategories; GPT-4 showed high trial-to-trial consistency (low SD, high correlation), and surprisingly nontrivial performance (55% when explain-first) on the deductive 'None of the above' transformation; majority-vote aggregation improved human expert group performance substantially (23%) but yielded minimal gains for GPT-3.5/GPT-4 (~1%).",
            "uuid": "e3975.0",
            "source_info": {
                "paper_title": "Evaluating large language models on a highly-specialized topic, radiation oncology physics",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Explain-first",
            "name_full": "Explain-first then answer (chain-of-thought prompting)",
            "brief_description": "A prompting strategy instructing the LLM to produce an explanation or intermediate reasoning before emitting the final answer, used here to develop multi-step reasoning and potentially improve accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Improvement in answer accuracy (overall and by category), especially on reasoning- or math-intensive items, and impact on deductive/rejection-style problems.",
            "evaluation_methods": "Modify LLM instruction to: 'first give an explanation for the answer followed by the correct answer (letter choice),' then compare performance to baseline runs where only the answer was requested; analyze per-category changes and aggregate score deltas.",
            "benchmark_or_dataset": "Applied to the same 100-question radiation oncology physics exam as the baseline condition.",
            "metrics_reported": "Overall score improvement (reported ~+5% for GPT-4), category-specific gains (noted largest in brachytherapy and math-based questions), improvement on deductive-reasoning test (increase to ~55% overall when explain-first was combined with the 'None of the above' transformation).",
            "human_involvement": "No additional human raters required for this ablation; authors executed the modified prompts and compared automated outputs to ground-truth answers.",
            "limitations_or_challenges": "Explain-first may expose chain-of-thought which can be undesirable in some deployment settings; gains may depend heavily on model scale and architecture (emergent property) and thus may not transfer to smaller models; potential for the LLM to 'hallucinate' plausible but incorrect reasoning while still producing a correct-seeming answer.",
            "llm_theory_example": null,
            "evaluation_results": "Prompting GPT-4 to explain before answering increased its accuracy by approximately 5% overall, with the largest improvements in brachytherapy and math categories; the method also improved GPT-4's performance on the deductive 'None of the above' task, suggesting chain-of-thought prompting aided its deductive capability.",
            "uuid": "e3975.1",
            "source_info": {
                "paper_title": "Evaluating large language models on a highly-specialized topic, radiation oncology physics",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Deductive-test",
            "name_full": "Deductive reasoning test via 'None of the above' substitution",
            "brief_description": "A novel evaluation transformation where the true correct option in each MCQ is replaced with 'None of the above choices is the correct answer' to force the model to deduce the correctness by rejecting the distractors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Ability to perform deductive reasoning by eliminating contextually plausible but incorrect choices; sensitivity to training-context patterns.",
            "evaluation_methods": "For each MCQ, remove the known correct choice and replace it with 'None of the above...'; run the modified exam through the LLM with baseline and explain-first prompting; humans were not tested under this transformation because they'd detect the pattern.",
            "benchmark_or_dataset": "Applied to the same 100-question radiation oncology physics exam with the described transformation.",
            "metrics_reported": "Overall percent-correct on transformed exam (GPT-4 scored ~55% when explain-first was used; baseline transformed score lower), comparisons to original exam scores.",
            "human_involvement": "Intentionally excluded humans from this specific transformed test because the pattern would be obvious to humans and bias their responses; authors ran LLM evaluations.",
            "limitations_or_challenges": "Unrealistic for human testers (pattern-detection) and may not reflect typical real-world prompts; transformation deliberately reduces contextual cues for the true correct answer (removes context overlap), so it tests a particular form of deductive skill that may not generalize; interpretation of high scores could indicate emergent abilities or possible artifacts of model training not fully understood.",
            "llm_theory_example": null,
            "evaluation_results": "GPT-4 demonstrated surprisingly strong performance on the deductive-transformed exam (notably when using explain-first prompting), suggesting an emergent deductive capability in larger LLMs; authors note this result is perplexing and merits further study.",
            "uuid": "e3975.2",
            "source_info": {
                "paper_title": "Evaluating large language models on a highly-specialized topic, radiation oncology physics",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Majority-vote",
            "name_full": "Majority-vote aggregation analysis",
            "brief_description": "An ensemble-style evaluation where, for each question, the most common answer across multiple trials (for LLMs) or across group members (for humans) is selected as the group's final answer, used to estimate collaborative performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Potential gain in accuracy from aggregating multiple independent responses, reflecting how groups (human or model ensembles) might improve reliability.",
            "evaluation_methods": "For each question take the modal answer from 5 LLM trials or from the human group; break ties randomly; calculate resulting group score and compare to individual mean scores.",
            "benchmark_or_dataset": "Applied to the study's 100-question exam using multiple LLM trials and human group responses.",
            "metrics_reported": "Percent-correct after majority aggregation and percent improvement over individual mean: GPT-3.5/GPT-4 improved ~1%, Bard ~4%, non-experts ~3%, medical physicists ~23% improvement.",
            "human_involvement": "Direct — human group answers were aggregated; authors used human group majority-vote as a simulated team performance metric.",
            "limitations_or_challenges": "LLMs' high per-trial consistency (confident but possibly consistently wrong) limits ensemble benefit; human group heterogeneity yields larger ensemble gains; tie-breaking randomness can affect small-sample aggregated score.",
            "llm_theory_example": null,
            "evaluation_results": "Majority-vote showed negligible benefit for GPT-3.5/GPT-4 (about +1%) but large benefit for the medical physicist group (+23%), indicating human diversity leads to better ensemble performance than aggregating multiple runs of a consistent LLM.",
            "uuid": "e3975.3",
            "source_info": {
                "paper_title": "Evaluating large language models on a highly-specialized topic, radiation oncology physics",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Consistency metrics",
            "name_full": "Trial-to-trial consistency and confidence analysis (std dev, Pearson correlation, per-question correct-frequency)",
            "brief_description": "Statistical measures used to quantify how stable an LLM's answers were across independent runs and to measure confidence-like behavior via repeated correctness rates on individual items.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Low standard deviation across trials and high average Pearson correlation indicate high consistency; the distribution of the number of times a question was answered correctly across trials indicates 'confidence' vs randomness.",
            "evaluation_methods": "Compute standard deviation of total scores across trials; compute the average of the upper triangle of the Pearson correlation matrix between trial answer-vectors; count for each question how many of the five trials produced a correct answer and compare the empirical distribution to a Poisson-based model for random guessing.",
            "benchmark_or_dataset": "Applied to responses on the 100-question exam across repeated LLM trials and human retakes.",
            "metrics_reported": "Reported low standard deviation and high average correlation for GPT-3.5/GPT-4 relative to human groups; specific reported values include GPT-4 averaging ~67% correct per trial and ~14% incorrect per trial (characterizing confident correctness and confident errors), GPT-3.5 having modes around 35% correct or 28% incorrect per trial; LLMs deviate strongly from Poisson random-guess expectation.",
            "human_involvement": "Humans provided the comparative repeated-answer data (humans were tested only once each, but group distributions compared); authors computed and interpreted the statistics.",
            "limitations_or_challenges": "Human repeated-testability differs from automated multiple-trial LLM testing (humans not retested in same manner); high LLM consistency may reflect memorization or overfit rather than robust reasoning; interpreting 'confidence' from repeated correctness counts is indirect.",
            "llm_theory_example": null,
            "evaluation_results": "LLMs (especially GPT-4) were markedly more consistent across trials than human test-takers, showing both concentrated correct responses and concentrated incorrect responses (i.e., confident wrongs), which reduced ensemble gains from majority-vote aggregation.",
            "uuid": "e3975.4",
            "source_info": {
                "paper_title": "Evaluating large language models on a highly-specialized topic, radiation oncology physics",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Contamination avoidance",
            "name_full": "Training-data contamination avoidance via new question creation",
            "brief_description": "The practice of creating novel test items not present in public corpora to avoid evaluating models on questions present in their training data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Minimize the probability that evaluated items were included in model training sets so that reported performance reflects generalization rather than memorization.",
            "evaluation_methods": "Design and use an original examination (100 new MCQs) written by domain experts rather than relying on public exams known to be widely represented in training corpora; explicitly state the intention to avoid contamination.",
            "benchmark_or_dataset": "Original 100-question exam used as the primary benchmark in this work.",
            "metrics_reported": "No direct metric for contamination; approach described as best-practice to reduce contamination risk; authors reference the need to ensure test questions are left out of training data.",
            "human_involvement": "Domain experts (experienced medical physicist and collaborators) authored the exam to ensure novelty and domain validity.",
            "limitations_or_challenges": "Creating new items reduces contamination risk but cannot guarantee absence of similar exemplars in the training corpus; small custom benchmarks may limit comparability to other studies and to broader capabilities.",
            "llm_theory_example": null,
            "evaluation_results": "Authors argue that using newly created, highly-specialized exam items yields a fairer test of LLM generalization in an obscure domain (radiation oncology physics) compared to widely circulated standardized exams.",
            "uuid": "e3975.5",
            "source_info": {
                "paper_title": "Evaluating large language models on a highly-specialized topic, radiation oncology physics",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Limitations-summary",
            "name_full": "Study limitations and evaluation challenges",
            "brief_description": "Collective statements in the discussion that qualify the evaluation methods, noting risks of superficial assessment, representativeness, overfitting, and generalizability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Assess whether the benchmark and evaluation methods actually probe deep domain competence, clinical applicability, and robustness rather than superficial knowledge.",
            "evaluation_methods": "Critical analysis and interpretation of results; comparison of exam-based performance to real-world clinical complexity and team-based human performance.",
            "benchmark_or_dataset": "The single customized 100-question exam and its applicability to represent everyday clinical tasks were discussed as a limitation.",
            "metrics_reported": "Qualitative limitations (e.g., exam not representing clinical nuance) and inference that high scores could indicate superficiality; quantitative limitations include small sample sizes and limited trials for some models.",
            "human_involvement": "Human experts contributed to authorship, exam design, testing, and interpretation of limitations; authors recommend cautious interpretation by the radiation oncology community.",
            "limitations_or_challenges": "Exam may misrepresent clinical competency; LLMs' high scores might reflect superficial knowledge or memorization; small, specialized test set may not generalize; uneven trial counts across LLMs; inability to fairly simulate repeated human answering; potential unknown overlaps with training data despite new-question creation.",
            "llm_theory_example": null,
            "evaluation_results": "Authors conclude the exam demonstrates strong LLM potential as assistants but caution against equating exam performance with real-world clinical competence; they also propose that LLMs could be used as a test for superficiality and suggest shifting certification to probe knowledge not known by LLMs.",
            "uuid": "e3975.6",
            "source_info": {
                "paper_title": "Evaluating large language models on a highly-specialized topic, radiation oncology physics",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 2
        },
        {
            "paper_title": "Sparks of artificial general intelligence: early experiments with gpt-4",
            "rating": 2
        },
        {
            "paper_title": "Emergent abilities of large language models",
            "rating": 2
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Can language models learn from explanations in context?",
            "rating": 2
        },
        {
            "paper_title": "Capabilities of gpt-4 on medical challenge problems",
            "rating": 2
        }
    ],
    "cost": 0.0144125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h2>OPEN ACCESS</h2>
<p>EDITED BY
Ke Nie,
Rutgers, The State University of New Jersey, United States</p>
<p>REVIEWED BY
Jaya Lakshmi Thangaraj,
University of California, San Diego,
United States
Yang Zhang,
University of California, Irvine, United States
*CORRESPONDENCE
Jiajian Shen
目shen.jiajian@mayo.edu
Wei Liu
目liu.wei@mayo.edu
RECEIVED 08 May 2023
ACCEPTED 12 June 2023
PUBLISHED 17 July 2023</p>
<h2>CITATION</h2>
<p>Holmes J, Liu Z, Zhang L, Ding Y,
Sio TT, McGee LA, Ashman JB, Li X, Liu T,
Shen J and Liu W (2023) Evaluating large language models on a highly-specialized topic, radiation oncology physics.
Front. Oncol. 13:1219326.
doi: 10.3389/fonc.2023.1219326
COPYRIGHT
(c) 2023 Holmes, Liu, Zhang, Ding, Sio, McGee, Ashman, Li, Liu, Shen and Liu. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</p>
<h2>Evaluating large language models on a highly-specialized topic, radiation oncology physics</h2>
<p>Jason Holmes ${ }^{1}$, Zhengliang Liu ${ }^{2}$, Lian Zhang ${ }^{1}$, Yuzhen Ding ${ }^{1}$, Terence T. Sio ${ }^{1}$, Lisa A. McGee ${ }^{1}$, Jonathan B. Ashman ${ }^{1}$, Xiang $\mathrm{Li}^{3}$, Tianming Liu ${ }^{2}$, Jiajian Shen ${ }^{1 <em>}$ and Wei Liu ${ }^{1 </em>}$<br>${ }^{1}$ Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, United States, ${ }^{2}$ School of Computing, The University of Georgia, Athens, GA, United States, ${ }^{3}$ Department of Radiology, Massachusetts General Hospital, Boston, MA, United States</p>
<p>Purpose: We present the first study to investigate Large Language Models (LLMs) in answering radiation oncology physics questions. Because popular exams like AP Physics, LSAT, and GRE have large test-taker populations and ample test preparation resources in circulation, they may not allow for accurately assessing the true potential of LLMs. This paper proposes evaluating LLMs on a highly-specialized topic, radiation oncology physics, which may be more pertinent to scientific and medical communities in addition to being a valuable benchmark of LLMs.</p>
<p>Methods: We developed an exam consisting of 100 radiation oncology physics questions based on our expertise. Four LLMs, ChatGPT (GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against medical physicists and non-experts. The performance of ChatGPT (GPT-4) was further explored by being asked to explain first, then answer. The deductive reasoning capability of ChatGPT (GPT-4) was evaluated using a novel approach (substituting the correct answer with "None of the above choices is the correct answer."). A majority vote analysis was used to approximate how well each group could score when working together.</p>
<p>Results: ChatGPT GPT-4 outperformed all other LLMs and medical physicists, on average, with improved accuracy when prompted to explain before answering. ChatGPT (GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices across a number of trials, whether correct or incorrect, a characteristic that was not observed in the human test groups or Bard (LaMDA). In evaluating deductive reasoning ability, ChatGPT (GPT-4) demonstrated surprising accuracy, suggesting the potential presence of an emergent ability. Finally, although ChatGPT (GPT-4) performed well overall, its intrinsic properties did not allow for further improvement when scoring based on a majority vote across trials. In contrast, a team of medical physicists were able to greatly outperform ChatGPT (GPT-4) using a majority vote.</p>
<p>Conclusion: This study suggests a great potential for LLMs to work alongside radiation oncology experts as highly knowledgeable assistants.</p>
<h2>REYWORDS</h2>
<p>large language model, natural language processing, medical physics, artificial intelligence, ChatGPT</p>
<h1>1 Introduction</h1>
<p>The advent of large language models (LLM) has completely transformed natural language processing (NLP) (1). The traditional paradigm of NLP follows the typical pipeline of creating customized solutions for downstream applications through supervised training. For example, a pre-trained BERT (2) model must be modified with additional network layers and then fine-tuned on labeled training data to perform tasks such as sequence classification or question answering. In some situations, it might also be beneficial or necessary to further pre-train such models on domain specific data to attain acceptable performance (3, 4). For example, AgriBERT (5) was pre-trained on agriculture-related text data, to properly address NLP tasks in the food and agriculture domain. However, the expansive size and exceptional few-shot learning capabilities enable LLMs to solve NLP problems through incontext learning, which reduces or even eliminates the need for annotated training samples (6, 7). During in-context learning, LLMs generalize from a few examples (or no examples at all) based on prompts, which typically are descriptive user inputs that characterize desired responses from LLMs (6, 8). For example, "summarize the following text" is a straightforward prompt that asks the language model to produce a summary for the input text. In general, LLMs provides a novel and simplified workflow for NLP that could potentially do away with supervised fine-tuning and its associated intricacies such as hyper-parameter tuning and model architecture modification. Furthermore, in-context learning significantly reduces the need for expensive and time-consuming human annotation (6, 9). It is especially desirable in medicine and science due to the limited data available in these domains (4, $10-12)$.</p>
<p>In recent months, the world has witnessed the rise of ChatGPT ${ }^{1}$, which has enjoyed significant global popularity given its unprecedented language capabilities and accessibility to the general public through a chatbot interface. ChatGPT is based on the powerful GPT-3 model (6), one of the first large language models in history. The 175 -billion-parameters GPT-3 was trained on a large data collection that encapsulated diverse Internet data (including the Common Crawl ${ }^{2}$ and Wikipedia ${ }^{3}$ ). It demonstrates exceptional performance in a variety of NLP tasks spanning from text summarization to named entity recognition (NER) through its text generation objective (indeed, many NLP tasks can be translated to some forms of text generation). ChatGPT inherits these capabilities from GPT-3, along with the massive knowledge on diverse topics stored in the parameter space. More importantly, ChatGPT was trained through Reinforcement Learning from Human Feedback (RLHF), a reinforcement learning process that incorporates human preferences and human ranked values through user feedback. This process tunes the model to generate outputs that are most appealing and relevant to human users. The capabilities of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ChatGPT empowers diverse practical applications ranging from essay writing to code generation (13).</p>
<p>One of the most powerful LLM to date is GPT-4 ${ }^{4}$, a successor to GPT-3. While OpenAI has not revealed much about its technical details yet, GPT-4 has demonstrated superior performance over the GPT-3.5-based ChatGPT in various scenarios ( $9,12,14,15)$. In fact, as of March 2023, GPT-4 is powering Microsoft's search engine, Bing (16), which demonstrates the potential of LLM-based search. In addition, unlike its predecessors, GPT-4 is a multi-modal model that accepts image inputs, which undoubtedly leads to more interesting applications.</p>
<p>GPT-4 has been shown to perform exceptionally well on various academic and professional benchmarks (14). For example, GPT-4 passes the USMLE exam with a $&gt;20 \%$ margin (17). In fact, GPT-4 scores at over the 90th percentile on the SAT, the Uniform Bar Exam and the verbal section of the GRE (see Figure 4 in the "GPT-4 Technical Report" (14)), where almost all of them included a multiple-choice component. Indeed, multiple-choice examinations are common for evaluating LLMs (14, 18, 19). Most multiple-choice exams that have been used to evaluate LLMs are based on topics that are among the most well represented in academics. For example, in 2022, the AP physics exam had 144,526 test-takers (20), the LSAT had 128,893 test-takers (21), the GRE had approximately 342,000 test-takers (22). As a result of the large numbers of test-takers taking these exams as well as the importance placed on scores in determining university admittance, there exists an exceeding amount of resources (including text data accessible on the internet). Regardless of the particular LLM under evaluation, the ease of access and overall ubiquity of these tests and relevant test preparation materials effectively preclude a high performance when evaluating LLMs on these tests. It is therefore important to also study LLMs on more obscure and specialized topics where the size of the training data is likely much smaller. In 2022, there were only 162 medical school graduates, who applied for radiation oncology residency programs (23). Radiation oncology physics therefore represents a topic that is relatively unknown to the general population and may therefore be a more fair test in evaluating LLMs as compared to highly represented knowledge-bases. Obscure topics may represent the greatest educational opportunity and also the greatest risk for the general population in the context of LLMs, as the responses may be more relied upon while being less accurate and with mistakes being less likely to be noticed.</p>
<p>An important factor in evaluating the accuracy of LLMs is to ensure that the test questions are left out of the training data (24), i.e. not contaminated. The best way to ensure this is to create new questions for testing. In this study, a multiple-choice examination has been created for this purpose. Four transformer-based LLMs have been chosen for evaluation: ChatGPT (GPT-3.5) (6), ChatGPT (GPT-4) (14), Bard (LaMDA) (25), and BLOOMZ (26). These results are compared to radiation oncology experts as well as non-experts. Additionally, ChatGPT (GPT-4) is further explored on how to improve its answers and on its deductive reasoning capabilities. Experimental results indicate that GPT-4 attains the</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>best performance among LLMs and outperforms professional medical physicists on average, especially when prompted to explain its reasoning before answering the question. We also conduct extensive ablation studies and analyses to comprehensively measure and explain the results.</p>
<h2>2 Related work</h2>
<h3>2.1 Large language models</h3>
<p>Transformer-based pre-trained language models (PLMs), such as BERT (2) and GPT (27), have revolutionized natural language processing. Surpassing previous methods (e.g. RNN-based models) in numerous tasks, they have promoted interest in and accessibility of language models (28). Generally, PLMs can be categorized into three types: autoregressive models (like GPT), masked language models (such as BERT), and encoder-decoder models (e.g. BART (29) and T5 (30)). More recently, there is a rise of very large language models, including GPT-3 (6), Bloom (31), PaLM (32), and OPT (33). Rooted in the transformer architecture, these models draw inspiration from the likes of BERT and GPT but are developed at much larger scales.</p>
<p>The objective of large language models is to accurately learn contextual and domain-specific latent feature representations from input text (28). For example, the vector representation of “discharge” might vary considerably between medical and general domains. Smaller language programs often require continual pretraining and supervised fine-tuning on downstream tasks to achieve acceptable performance (3, 4). However, very large language models could potentially eliminate the need for fine-tuning while maintaining competitive results (6).</p>
<p>Besides the progress in model architecture, scale and training strategies, large language models can be further aligned with human preferences through reinforcement learning from human feedback (RLHF) (34). This approach has been implemented in various LLMs, such as Sparrow (35), InstructGPT (36), and ChatGPT. InstructGPT was based on GPT-3 and was trained through a process during which user preferences were prioritized through human-generated ranking feedback. As a successor to InstructGPT, ChatGPT also employs RLHF, focusing on adhering to prompts and generating comprehensive responses. OpenAI also implemented guardrails to prevent the generation of biased and undesirable outputs (31). ChatGPT has become a highly successful AI chatbot, capitalizing on GPT-3.5's capabilities to facilitate human-like interactions.</p>
<p>RLHF incorporates human feedback into the generation and selection of optimal results by training a reward model based on human annotators' rankings of generated outcomes (37). This reward model then rewards outputs that best correspond to human preferences and values. We believe these groundbreaking innovations make ChatGPT the perfect candidate for this study.</p>
<p>The recent development of GPT-4 has significantly advanced the state-of-the-art of language models. GPT-4 demonstrates enhanced reasoning abilities, creativity, image comprehension, context understanding, and multi-modal abilities, leading to more sophisticated and diverse responses. The success of large GPT models spurs exploration into specialized variants for specific fields, such as dedicated large language models for medical and healthcare applications, which could potentially revolutionize these domains.</p>
<h3>2.2 Language models and examination</h3>
<p>Large language models have exceptional natural language comprehension abilities. In addition, they are trained on massive data that supplies substantial knowledge. These characteristics make large language models ideal candidates for academic and professional benchmarks.</p>
<p>OpenAI recently released the first study in the literature that evaluates large language models on academic and professional exams designed for educated humans (14). The results indicate that GPT-4 performs extremely well on a wide variety of subjects ranging from the Uniform Bar Exam to GRE. In addition, a study from Microsoft indicates that GPT-4 can pass USMLE, the professional exam for medical residents, by a large margin (17).</p>
<p>This study is the first evaluation of large language models in the realms of radiation oncology and medical physics, and we believe it can inspire future research in evaluating LLMs on highly-specialized branches of medicine.</p>
<h3>2.3 Prompt engineering</h3>
<p>Collecting and labeling data for training or fine-tuning NLP models can be resource-intensive and costly, especially in the medical domain (4, 9, 12). Recent studies suggest that by employing prompts, large-scale pre-trained language models (PLMs) can be adapted to downstream tasks without the need for fine-tuning (6, 8).</p>
<p>A prompt consists of a set of instructions that customizes or refines the LLM's response. Prompts extend beyond merely describing the task or specifying output formats. Indeed, they can be engineered to create novel interactions. For example, it is possible to prompt ChatGPT to emulate a cybersecurity breach with simulated terminal commands (38). In addition, prompts can also be used to generate additional prompts through a self-adaptation process (38).</p>
<p>The emergence of prompt engineering signifies the start of a new era in natural language processing (8). There is no doubt that carefully crafted prompts have much potential for diverse and sophisticated applications. However, determining the ideal prompt poses a unique challenge in the age of large language models. Currently, prompts can be designed manually or generated automatically (8, 39). Although automatically produced prompts may outperform manual prompts in certain tasks (8), they often suffer from poor human-readability and explainability (8, 40). Consequently, manual prompt generation may be favored in domains where interpretability is crucial, such as clinical practices and research. In this study, we design a suite of prompts and chain-of-thought prompts based on our experience in radiation</p>
<p>oncology and medical physics and evaluate their impact on large language models.</p>
<h2>3 Methods</h2>
<p>A 100 -question multiple-choice examination on radiation oncology physics was created for this study by an experienced medical physicist. This exam includes questions on the following topics: basic physics ( 12 questions), radiation measurements ( 10 questions), treatment planning ( 20 questions), imaging modalities and applications in radiotherapy ( 17 questions), brachytherapy ( 13 questions), advanced treatment planning and special procedures (16 questions), and safety, quality assurance (QA), and radiation protection (12 questions). The seven exam categories and the associated number of questions for each category follows the official study guide of American Board of Radiology (41). Of the 100 questions, 17 require numeric calculation (math-based). The exam questions are listed in the Appendix, Section A.</p>
<h3>3.1 Comparison between LLM scores and human scores</h3>
<p>The 100 -question multiple-choice test on radiation oncology physics was inputted to each LLM in 5 separate trials (Trial 1 - Trial 5), except BLOOMZ, which was only tested in one trial. Each trial, beginning on a new thread or after reset, began with an initialization prompt notifying the LLM that it was about to be tested. Next, the LLM was prompted with instructions and 20 questions in batches until the exam was complete. In each trial, the instructions indicated to the LLM that it should only return the correct answer with no justification. The instructions were included in each batch since it was observed that the LLMs were less likely to follow the instructions otherwise. In cases where the LLM could not accept 20 questions at a time, batches of 10 questions were used instead (Bard). In cases where not all the answers were returned by the LLM, the next batch would include the non-answered question(s) as well as the entire next batch. These occurrences were rare. In each test trial, the global prompt and instructions prompt were phrased differently in order to account for response-noise due to promptnoise. The initialization prompts and instructions prompts are given in Table 1.</p>
<p>LLM test scores and their distributions were compared between each other as well as with scores from two human groups, medical physicists and non-experts. The medical physicists group included four experienced board-certified medical physicists, three medical physics residents, and two medical physics research fellows. The non-expert group included six individuals with advanced degrees in either electrical engineering, computer engineering, or computer science, but with no known prior experience or education on radiation oncology physics. Each human test-taker was allowed 3 hours to take the exam, closed book, also permitting the use of a basic calculator. In comparing the LLM scores and human scores, the mean scores, consistency in scores, and confidence in answers were evaluated.</p>
<p>To quantify accuracy, the average score was calculated for each LLM by averaging the scores from each trial. For the human test groups, individual scores were averaged over the whole group.</p>
<p>To quantify the overall consistency of scoring success, the standard deviation and average correlation between trials, defined as the average of the upper values of the Pearson correlation matrix between trials, were calculated. The average correlation indicates how consistent the correct scores were between trials where 1 is interpreted as the distribution being identical, 0 is equivalent to the distribution being purely random, and -1 is interpreted as the distribution being perfectly anti-correlated.</p>
<p>In order to quantify the degree of confidence in the answers given by the LLMs and human groups, the number of correct answers for each question were counted across all trials. For example, if each LLM answered the same question correctly 5 times, then the percentage of questions where all 5 answers were correct was incremented by $1 \%$ (since there are 100 questions). Additionally, the test results were compared to the expected distribution that would occur if the test-taker were guessing at random. The expected number of correct answers in 5 trials, when randomly guessing, is approximately $0.25 \times 5=1.25$ on average (98/</p>
<p>TABLE 1 The LLM prompts used in each trial.</p>
<table>
<thead>
<tr>
<th>Trial</th>
<th>Initialization prompt</th>
<th>Instructions prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td>Trial 1</td>
<td>I am a radiation therapy researcher. My research group would like to study the answers given by ChatGPT on the topic of radiation oncology physics. I will now proceed to ask questions about radiation oncology physics.</td>
<td>Instructions: For each multiple choice question, provide the correct answer without any justification.</td>
</tr>
<tr>
<td>Trial 2</td>
<td>I want to evaluate your knowledge on radiation oncology physics by asking some multiple choice questions.</td>
<td>Please give only the question label and the letter for the correct answer.</td>
</tr>
<tr>
<td>Trial 3</td>
<td>Please answer the following practice questions as if you were a resident preparing for board certification exams.</td>
<td>Only give the correct answer in your response. Do not explain your answers.</td>
</tr>
<tr>
<td>Trial 4</td>
<td>We want to test your understanding of radiation oncology physics. For this reason, we have created some questions to ask you.</td>
<td>In your response, only report the question label and the corresponding answer.</td>
</tr>
<tr>
<td>Trial 5</td>
<td>I will ask you some multiple-choice questions.</td>
<td>Instructions: Only respond with the correct letter choice.</td>
</tr>
</tbody>
</table>
<p>100 questions have 4 choices, 2/100 have 5 choices). Using this value, the number of correct answer occurrences for each question can be estimated following the resultant Poisson distribution.</p>
<p>Finally, ChatGPT (GPT-3.5 and GPT-4) and Bard scores were compared to human scores where the scores were calculated based on majority vote.</p>
<h3>3.2 Improving ChatGPT (GPT-4) accuracy - explain first, then answer</h3>
<p>Due to the nature of transformer-based LLMs predicting the next word based on the prior context, it has been shown that the accuracy of responses can be improved if a sufficiently large LLM is prompted to develop the answer in a step-wise manner (24, 42, 43). ChatGPT (GPT-4) was evaluated using this strategy to see if its score could be improved by prompting it to explain first, then answer. The initialization prompt was the same as in Trial 1, however the instructions prompt for Trial 1 was changed to the following: “Instructions: For each multiple choice question, first give an explanation for the answer followed by the correct answer (letter choice).” These test results were then compared with the original non-justified test results.</p>
<h3>3.3 Testing ChatGPT (GPT-4) on its deductive reasoning ability</h3>
<p>In a multiple-choice question, an LLM will be most successful when the question and answer are often used in the same context. However, what happens if the correct answer has no shared context with the question, such as when the answer is “None of the above”? In this case, the LLM must deduce the correct answer by rejecting all the other answers, all of which likely share context with the question. This scenario would seem to be especially challenging for an LLM. To study the deductive reasoning ability of ChatGPT (GPT-4), each question of the 100-question multiple-choice exam was modified. Each correct answer was removed and replaced with “None of the above choices is the correct answer.” Such a context-reduction transformation cannot be used on a human since a human would notice the pattern. Because of this, there are likely to be no examples of this sort of transformation to be found for tests that were designed for humans and were subsequently used in the training data for LLMs. It is assumed, then, that an LLM would not notice this pattern. The modified exam was given to ChatGPT (GPT-4) using the Trial 1 prompts and was subsequently tested for improving accuracy by explaining first, then answering as described in Section 3.2.</p>
<h2>4 Results</h2>
<h3>4.1 Comparison between LLM scores and human scores</h3>
<p>The raw marks and mean test scores are shown in Figures 1 and 2A respectively, where the LLM mean test scores represent the mean of 5 trials (except for BLOOMZ - 1 trial) and the mean test scores for humans represent the mean of their respective groups (see Section 3.1). Each LLM was able to outperform the non-expert human group overall while only ChatGPT (GPT-4) outperformed the medical physicist group. For math-based questions, the medical physicists outperformed ChatGPT (GPT-4).</p>
<p>As can be observed in the raw marks shown in Figure 1, each LLM and human group showed variability between trials, not only in terms of uncertainty in the overall score, but also in terms of the number of times each question was answered correctly. The standard deviation and average correlation between trials are reported in Figures 2B, C. The LLMs were much more consistent in their scores and answers as compared to the human groups, showing both a low standard deviation in scoring and a high average correlation between trials.</p>
<p>From the results shown in 3, Bard slightly outperformed the non-expert group, however both groups performed similarly to a random guesser. ChatGPT (GPT-3.5 and GPT-4) and the medical physicists showed no similarity to random guessing. ChatGPT (GPT-3.5) was either confident, getting 35% of answers correct in each trial, or confused, getting 28% of answers incorrect. ChatGPT (GPT-4) was even more confident, getting 67% of questions correct in each trial, however it also showed a propensity for confusion, getting 14% of questions incorrect in each trial. As a group, the medical physicists were neither extremely confident, nor confused, however tending towards agreement in selecting the correct answers.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p><strong>FIGURE 1</strong> Raw marks for each test where the rows are separate tests and the columns are the test questions. Dark shaded squares represent correct answers.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>FIGURE 2 Overall performance and uncertainty in test results. (A) Mean test scores for each LLM by category. (B) Standard deviation in total scores. (C) Average correlation between trials.</p>
<p>Although ChatGPT (GPT-3.5 and GPT-4) scored well overall, their scoring distributions, shown in Figure 3, suggested that if the LLMs could work together, there would be very little improvement in scoring, since they tended to be either confident or confused with low variability. Bard (LaMDA) and the non-expert groups would also likely show little improvement in working together as their answers tended towards random success. However, because medical physicists tended towards agreement on correct answers, it would be expected that their score would improve considerably when working together. To test for this, the answers for each group were combined using a "majority vote". For each question, the most common answer choice was chosen as the group answer. In the case of a tie, one answer among the most common answer choices was chosen randomly. Figure 4 shows the scoring results when utilizing a majority vote. As shown, ChatGPT (GPT-3.5 and GPT-4) improved very slightly, 1%. Bard (LaMDA) and the non-expert group improved by 4% and 3% respectively. However, the medical physicist group improved greatly, by 23%.</p>
<h3>4.2 Improving ChatGPT (GPT-4) accuracy - explain first, then answer</h3>
<p>Figure 5 shows the results for having prompted ChatGPT (GPT-4) to explain first, then answer, therefore allowing the answer to develop. ChatGPT's (GPT-4) overall score improved by 5%, exceeding each prior trial. The greatest improvement was in the brachytherapy and math-based questions categories. These results are in agreement with prior studies that found this capability to be an emergent characteristic for sufficiently large LLMs (43). Sample</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p><strong>FIGURE 3</strong> Confidence in answers. The number of correct answer occurrences per-question for each LLM and human group. The dashed red curve indicates the expected distribution if the answers were randomly selected based on the Poisson distribution.</p>
<p>Responses from ChatGPT (GPT-4) are given in the Appendix, Section A.</p>
<h3>4.3 Testing ChatGPT (GPT-4) on its deductive reasoning ability</h3>
<p>Figure 6 shows the results for the deductive reasoning test where the correct answer was replaced by “None of the above choices is the</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>FIGURE 4 Scores by category, tabulated by majority vote among trials for LLMs and within the group for humans.</p>
<h2>5 Discussion</h2>
<p>More than 1 million new cancer cases are diagnosed and more than 600,000 people die from cancer in the US every year. Radiotherapy (RT) is a standard treatment option used for nearly 50% of cancer patients (44–47). Physics plays an important role in radiation oncology due to the complexity and sophistication of physics and engineering adopted in modern radiation therapy. Therefore, it is essential for the radiation oncology professionals to understand radiation oncology physics well to ensure the safety and accuracy of the radiation treatment of cancer patients. The aim of this study was to evaluate LLMs on a highly-specialized topic, radiation oncology physics, based on a 100-question multiple choice exam that was specifically designed for this study. The exam can be found in the Appendix, Section A. The scoring results from the non-expert group suggest that the general population knows very little about radiation oncology physics as their scores were similar to random guessing. Bard (LaMDA) slightly outperformed the non-experts while BLOOMZ and ChatGPT (GPT-3.5 and GPT-4) greatly outperformed the non-experts. Amazingly, GPT-4 was able to outperform the average medical physicist in nearly all subcategories and improved its answer accuracy when prompted to explain its reasoning before answering (Figures 2A, 5). As a general principle for improving accuracy, users should consider prompting ChatGPT to explain first, then answer. ChatGPT (GPT-4) showed a surprising ability to deductively reason in answering all 100 questions where each correct answer was modified to be “None of the above choices is the correct answer.”, particularly when it was prompted to explain first, then answer, scoring 55% overall. This result is somewhat perplexing and could potentially be an emergent property. Emergent properties are known to occur as the number of parameters is increased in LLMs (43). This novel method may be a useful method in determining whether deductive reasoning improves with the number of parameters going forward.</p>
<p>While ChatGPT (GPT-4) outperformed medical physicists overall, this study has also provided evidence that individual LLMs cannot compete with a small number of medical physicists working together (Figure 4). The likely reason is that humans vary significantly in capabilities and knowledge from individual to individual, even when their professional backgrounds are similar. Additionally, while an answer in a multiple-choice question will either be correct or incorrect, the scoring count distributions shown in Figure 3 indicated that the medical physicists were far less likely to be confused, which, when aggregated over the whole group of medical physicists, allowed them to select the correct answer at a much higher rate in a majority vote. When ChatGPT (GPT-3.5 and GPT-4) was wrong, it was confidently wrong (confused). Similarly, when it was correct, it was confidently correct. Our results indicated that humans with expertise on a highly-specialized topic knew when to guess, how to guess intelligently, and were less likely to be wrong in their reasoning, even when the correct answer was not chosen. This comparison may not be completely fair as it is possible that if the exact same human could be tested repeatedly in the same manner as ChatGPT (GPT-3.5 and GPT-4), they might also repeat answers and show a degree of confusion individually. That point is arguably irrelevant, however, as there are many experienced medical physicists and only few LLMs as capable as GPT-4. The high degree of consistency in correct and incorrect answers for ChatGPT (GPT-3.5 and GPT-4) may be a sign of over-fitting (or memorization) in regards to radiation oncology physics knowledge. Regardless, being that radiation oncology physics is a highly-specialized topic, the performance of ChatGPT (GPT-4) was extraordinary and will likely continue to improve in the near-future. Practically speaking, this study suggests a great potential for radiation oncology experts to work alongside ChatGPT (GPT-4), using it as a highly knowledgeable assistant.</p>
<p>A weakness in evaluating LLMs using exams such as the one presented in this study is that this exam is not representative of the detailed and nuanced daily clinical work being performed by medical physicists and radiation oncology specialists. The relative</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>FIGURE 5 The improvement for Trial 1 as due to using the explain first, then answer method.</p>
<p>performance between LLMs and medical physicists on radiation oncology physics exams reported in this study may therefore misrepresent the degree of equivalency between LLMs and individual medical physicists. Furthermore, GPT-4's high performance on this certification-like exam, covering a highly specialized topic, suggests a degree of superficiality in the knowledge being assessed. Otherwise, we would have to entertain the possibility of GPT-4 being competent enough to fulfill the role of a medical physicist, which seems highly improbable. The radiation oncology community, and possibly the wider medical community, may therefore need to reevaluate certification procedures, as the necessity for humans to invest significant effort in acquiring such superficial knowledge will diminish as LLMs continue to advance. With this in mind, LLMs could potentially be used as a test for superficiality. Perhaps a greater focus on knowledge not known by the LLM should be more greatly emphasized.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>FIGURE 6 The scores for Trial 1 after replacing the correct answer with "None of the above choices is the correct answer.", a method for testing for deductive reasoning, and subsequent improvement as due to using the explain first, then answer method.</p>
<h3>Applying large language models in radiation oncology</h3>
<p>This study is a continuation of a line of research that applies state-of-the-art NLP methods to radiation oncology. For example, Rezayi et al. (11) trained BioBERT on a large corpus of radiation oncology literature and a curated and anonymized text dataset from a hospital to build ClinicalRadioBERT, a specialized language model for radiation oncology. Liao et al. (48) proposed a framework of directing the attention of transformer-based language models to more important input tokens that significantly affect classification decisions. This method is particularly important for few-shot learning with few annotated samples, which is a common challenge in radiation oncology where it is difficult to collect and curate large amounts of multi-institution patient data that match certain requirements due to the concern of patient privacy. On a related note, ChatGPT has demonstrated superior performance as an effective text data augmentation approach over state-of-the-art text data augmentation methods in terms of testing accuracy and distribution of the augmented samples (9), which can also be used to address the few-shot learning challenge.</p>
<p>In addition, LLMs can be employed for innovative applications such as data de-identification. For example, GPT-4 outperforms ChatGPT and other language model competitors in de-identifying clinical notes with a 99% accuracy (12). This is of extreme importance to radiation oncology and all medicine specialities in general, since it is often cumbersome to anonymize data for cross-institution clinical collaboration and academic research. Some other applications of language models include building domain-specific knowledge graphs for oncology (49) without manual annotation from clinicians or other domain experts.</p>
<h3>Multi-modal models in radiation oncology</h3>
<p>Multi-modal models are the future of language model (1) and are important in medical diagnosis (50). Some early LLM studies with multi-modal data include ChatCAD (51), a framework to integrate images and texts for computer-aided diagnosis. It supports various diagnosis networks such as those for lesion segmentation and report generation. In this framework, ChatGPT can be used to enhance the outputs of these networks.</p>
<p>GPT-4 supports multi-modal inputs such as images, which further unlocks the potential of large language models in radiation oncology. It is necessary to investigate future models and applications that integrate text, images, dosimetric data, and other modalities into the diagnosis and treatment pipelines. We believe such multi-modal models display inherent affinity to the human brain (1) and future LLM models for medicine can receive inspirations from advances in both neuroscience and NLP.</p>
<h3>Data availability statement</h3>
<p>The original contributions presented in the study are included in the article/Supplementary Material. Further inquiries can be directed to the corresponding authors.</p>
<h3>Author contributions</h3>
<p>JH, JS, and WL contributed to conception and design of the study and edited the manuscript. JS designed the 100-question exam, subsequently edited by JH and WL. ZL, XL, and TL advised on LLM concepts and contributed to the manuscript. LZ and YD advised on the concept and experimental design. JH guided the writing process, wrote initial draft, performed all data analysis. TS, LM, and JA advised on clinical concerns and approved the manuscript. All authors contributed to the article and approved the submitted version.</p>
<h3>Funding</h3>
<p>This research was supported by the National Cancer Institute (NCI) Career Developmental Award K25CA168984, President's Discovery Translational Program of Mayo Clinic, the Fred C. and Katherine B. Andersen Foundation Translational Cancer Research Award, Arizona Biomedical Research Commission Investigator Award, the Lawrence W. and Marilyn W. Matteson Fund for Cancer Research, and the Kemper Marley Foundation.</p>
<h3>Acknowledgments</h3>
<p>We would like to thank the many individuals (Yuzhen Ding, Yiran Cui, Yonghui Fan, Yikang Li, Riti Paul, Nupur Thakur, Sachin Chhabra, Chenbin Liu, Yang Li, Man Zhao, Yuchao Hu, Shumei Jia, Lian Zhang, Yao Xu, Hongying Feng, and Yunze Yang) that volunteered to take the radiation oncology physics exam.</p>
<h3>Conflict of interest</h3>
<p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
<h3>Publisher's note</h3>
<p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
<h3>Supplementary material</h3>
<p>The Supplementary Material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/fonc.2023.1219326/full#supplementary-material</p>
<h2></h2>
<h1>References</h1>
<ol>
<li>Zhao L, Zhang L, Wu Z, Chen Y, Dai H, Yu X, et al. When brain-inspired ai meets agi. arXiv preprint (2023), 2303.15935.</li>
<li>Devlin J, Chang M-W, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint (2018), arXiv:1810.04805.</li>
<li>Gu Y, Tinn R, Cheng H, Lucas M, Usuyama N, Liu X, et al. Domain-specific language model pretraining for biomedical natural language processing. ACM Trans Comput Healthc (HEALTH) (2021) 3(1):1-23.</li>
<li>Liu Z, He M, Jiang Z, Wu Z, Dai H , Zhang L, et al. Survey on natural language processing in medical image analysis. Zhong nan da xue xue bao. Yi xue ban=J Cent South University Med Sci (2022) 47(8):981-93.</li>
<li>Rezayi S, Liu Z, Wu Z, Dhakal C, Ge B, Zhen C, et al. Agribert: knowledge-infused agricultural language models for matching food and nutrition. IJCAI (2022), 5150-6. doi: 10.24963/ijcai.2022/715</li>
<li>Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, et al. Language models are few-shot learners. (2020), arxiv id: 2005.14165.</li>
<li>Dong Q, Li L, Dai D, Zheng C, Wu Z, Chang B, et al. A survey for in-context learning. (2022), arXiv:2106.07404v2.</li>
<li>Liu P, Yuan W, Fu J, Jiang Z, Hayashi H, Neubig G. Pre-train, prompt, and predict: a systematic survey of prompting methods in natural language processing. ACM Comput Surveys (2023) 55(9):1-35. doi: 10.1145/3560815</li>
<li>Dai H, Liu Z, Liao W, Huang X, Wu Z, Zhao L, et al. Chataug: leveraging chatgpt for text data augmentation. arXiv preprint (2023), 2302.13007.</li>
<li>Kagawa R, Shirasuna M, Ikeda A, Sanuki M, Honda H, Nosato H. One-second boosting: a simple and cost-effective intervention for data annotation in machine learning. (2022), arxiv id: 2112.07475.</li>
<li>Rezayi S, Dai H, Liu Z, Wu Z, Hebbar A, Burns AH, et al. Clinicalradiobert: knowledge-infused few shot learning for clinical notes named entity recognition. In: Machine learning in medical imaging: 13th international workshop, MLMI 2022, held in conjunction with MICCAI 2022, Singapore, September 18, 2022, proceedings. (Singapore: Springer (2022). p. 269-78.</li>
<li>Liu Z, Yu X, Zhang L, Wu Z, Cao C, Dai H, et al. Deid-gpt: zero-shot medical text de-identification by gpt-4. arXiv preprint (2023), arXiv:2306.11289.</li>
<li>Qin C, Zhang A, Zhang Z, Chen J, Yasunaga M, Yang D. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint (2023), arXiv:2306.11289.</li>
<li>OpenAI. Gpt-4 technical report. (2023), arxiv id: 2303.08774.</li>
<li>Kosibas A. Gpt-4 vs. gpt-3.5: a concise showdown. (2023). doi: 10.36227/ techrxiv.22312330.v2</li>
<li>The Verge. The bing ai bot has been secretly running gpt-4. (2023). Available at: https://www.theverge.com/2023/3/14/23639928/one/rooth-bing-chatbot-ai-gpt-4-fim</li>
<li>Nori H, King N, McKinney SM, Carignao D, Horvitz E. Capabilities of gpt-4 on medical challenge problems. arXiv preprint (2023), 2303.13375.</li>
<li>Lampinen AK, Dasgupta I, Chan SCY, Matthewson K, Tessler MH, Creswell A, et al. Can language models learn from explanations in context? arXiv preprint (2022), arXiv:2109.08668.</li>
<li>Savelka J, Agarwal A, Bogart C, Sakr M. Large Language models (gpt) struggle to answer multiple-choice questions about code. arXiv preprint (2023), arXiv:2206.05715v1. doi: 10.5220/0011996900003470</li>
<li>College Board. Student score distributions (2022) (Accessed 3/31/2023).</li>
<li>Law School Admission Council. Test registrants and test takers (2023) (Accessed 3/31/2023).</li>
<li>Achievable. Key gre statistics from the 2022 ets gre snapshot report (2023) (Accessed 3/31/2023).</li>
<li>National Resident Matching Program. Charting outcomes in the match: senior students of u.s. and medical schools (2023) (Accessed 3/31/2023).</li>
<li>Bubeck S, Chandrasekaran V, Eldan R, Gehrke J, Horvitz E, Kamar E, et al. Sparks of artificial general intelligence: early experiments with gpt-4. (2023), Arxiv ID: 2303.12712.</li>
<li>Thoppilan R, De Freitas D, Hall J, Shazeer N, Kulshreshtha A, Cheng H-T, et al. Lamda: language models for dialog applications. (2022), arXiv:2102.08602.</li>
<li>Muannighoff N, Wang T, Sutawika L, Roberts A, Biderman S, Le Scao T, et al. Crosslingual generalization through multitask fine tuning. (2022), arXiv:2106.11539.</li>
<li>Radford A, Narasimhan K, Salimans T, Sutskever I. Improving language understanding by generative pre-training. (2018), 1801.06146.</li>
<li>Kalyan KS, Rajasekharan A, Sangeetha S. Ammus: a survey of transformer-based pretrained models in natural language processing. (2021), 2003.08271. doi: 10.1016/ j.jbi.2021.103982</li>
<li>Lewis M, Liu Y, Goyal N, Ghazvininejad M, Mohamed A, Levy O, et al. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint (2019), arXiv:2106.12424v1. doi: 10.18653/v1/2020.acl-main.703</li>
<li>Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J Mach Learn Res (2020) 21(1):5485-551.</li>
<li>Le Scao T, Fao A, Akiki C, Pavlick E, Ilić S, Hesslow D, et al. Bloom: a 176bparameter open-access multilingual language model. (2022), 2211.05100.</li>
<li>Chowdhery A, Narang S, Devlin J, Bosma M, Mishra G, Roberts A, et al. Palm: scaling language modeling with pathways. arXiv preprint (2022), arXiv:2205.03578.</li>
<li>Zhang S, Roller S, Goyal N, Arteraz M, Chen M, Chen S, et al. Opt: open pretrained transformer language models. arXiv preprint (2022), arXiv ID: arX</li>
<li>Ziegler DM, Stiennon N, Wu J, Brown TB, Radford A, Amodei D, et al. Fine-tuning language models from human preferences. arXiv preprint (2019), arXiv:1909.08593.</li>
<li>Glaese A, McAleese N, Trębacz M, Aslanides J, Firoiu V, Ewalds T, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint (2022), 2209.14375.</li>
<li>Ouyang L, Wu J, Jiang X, Almeida D, Wainwright C, Mishkin P, et al. Training language models to follow instructions with human feedback. Adv Neural Inf Process Syst (2022) 35:27730-44.</li>
<li>Bai Y, Jones A, Ndousse K, Askell A, Chen A, DasSarma N, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint (2022), arXiv:2204.05862.</li>
<li>White J, Fu Q, Hays S, Sandborn M, Olea C, Gilbert H, et al. A prompt pattern catalog to enhance prompt engineering with chatgpt. (2023), arXiv:2306.02245v1.</li>
<li>Gao T, Fisch A, Chen D. Making pre-trained language models better few-shot learners. arXiv preprint (2020), 2012.15723. doi: 10.18653/v1/2021.acl-long. 295</li>
<li>Taylor N, Zhang Y, Joyce D, Nevado-Holgado A, Kornillitzin A. Clinical prompt learning with frozen language models. arXiv preprint (2022), arXiv:2206.09308.</li>
<li>The American Board of Radiology. Medical physics radiation oncology (2023) (Accessed 2023-04-18).</li>
<li>Shinn N, Labash B, Gopinath A. Reflexion: an autonomous agent with dynamic memory and self-reflection. (2023), arXiv:2303.11366.</li>
<li>Wei J, Tay Y, Bommasani R, Raffel C, Zoph B, Borgeaud S, et al. emergent abilities of large language models. (2022), arXiv:2201.04451.</li>
<li>Liu W, Frank SJ, Li X, Li Y, Zhu RX, Mohan R. Ptv-based impt optimization incorporating planning risk volumes vs robust optimization. Med Phys (2013) 40 (2):021709. doi: 10.1118/1.4774363</li>
<li>Deng W, Younkin JE, Souris K, Huang S, Augustine K, Fatyga M, et al. Integrating an open source monte carlo code "mcsquare" for clinical use in intensity-modulated proton therapy. Med Phys (2020) 47(6):2558-74. doi: 10.1002/ mp .14125</li>
<li>Shan J, An Y, Bues M, Schild SE, Liu W. Robust optimization in impt using quadratic objective functions to account for the minimum mu constraint. Med Phys (2018) 45(1):460-9. doi: 10.1002/mp. 12677</li>
<li>Schild SE, Rule WG, Ashman JB, Vora SA, Keole S, Anand A, et al. Proton beam therapy for locally advanced lung cancer: a review. World J Clin Oncol (2014) 5(4):568. doi: $10.5306 /$ wjco.v5.i4.568</li>
<li>Liao W, Liu Z, Dai H, Wu Z, Zhang Y, Huang X, et al. Mask-guided bert for few shot text classification. arXiv preprint (2023), arXiv:2106.01204.</li>
<li>Cai H, Liao W, Liu Z, Huang X, Zhang Y, Ding S, et al. Coarse-to-fine knowledge graph domain adaptation based on distantly-supervised iterative training. (2022), arXiv:2206.04206v1.</li>
<li>Cai X, Liu S, Han J, Yang L, Liu Z, Liu T. Chestxraybert: a pretrained language model for chest radiology report summarization. IEEE Transactions on Multimedia (2023) 25:845-55. doi: 10.1109/TMM.2021.3132724</li>
<li>Wang S, Zhao Z, Ouyang X, Wang Q, Shen D. Chatcad: interactive computeraided diagnosis on medical image using large language models. (2023), arXiv:2205.13452v1.</li>
<li>OpenAI. Introducing chatgpt (2023) (Accessed 3/31/2023).</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>1 https://openai.com/blog/chatgpt
2 http://commoncrawl.org/
3 https://www.wikipedia.org/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>4 https://openai.com/research/gpt-4&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
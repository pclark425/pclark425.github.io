<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4250 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4250</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4250</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-275471664</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.07267v1.pdf" target="_blank">Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics</a></p>
                <p><strong>Paper Abstract:</strong> Scientific team dynamics are critical in determining the nature and impact of research outputs. However, existing methods for classifying author roles based on self-reports and clustering lack comprehensive contextual analysis of contributions. Thus, we present a transformative approach to classifying author roles in scientific teams using advanced large language models (LLMs), which offers a more refined analysis compared to traditional clustering methods. Specifically, we seek to complement and enhance these traditional methods by utilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting, we categorize author roles and demonstrate that GPT-4 outperforms other models across multiple categories, surpassing traditional approaches such as XGBoost and BERT. Our methodology also includes building a predictive deep learning model using 10 features. By training this model on a dataset derived from the OpenAlex database, which provides detailed metadata on academic publications -- such as author-publication history, author affiliation, research topics, and citation counts -- we achieve an F1 score of 0.76, demonstrating robust classification of author roles.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4250.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4250.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (role classification)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (used for author role classification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was used in this study with few-shot prompting to read authors' self-reported contribution statements and assign each author a single role (Leadership, Direct Support, Indirect Support), producing high-quality labels that were then used to train a scalable predictive model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Few-shot prompt-based role classification (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The authors constructed structured few-shot prompts that defined a three-level role hierarchy (Leadership > Direct Support > Indirect Support) and provided targeted examples; temperature was set to 0.01 for deterministic outputs. GPT-4 was fed the 'Author Contribution' field (text) for each paper and instructed to assign a single role per author, resolving multi-label entries by selecting the highest-ranking role present. Zero-shot prompting was explored but found unstable; few-shot examples were used for improved disambiguation. GPT-4-generated labels were later used as supervision to train a dense neural network for scalable application.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Initial evaluation: 1,000 papers (250 per journal) producing ~5,000 author rows; additional inference on 2,000 papers (500 per journal) for modeling; further role inferences for ~10,000 authors leading to ~15,000 labeled author datapoints</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Science of science / bibliometrics (multidisciplinary scholarly publications from PNAS, Nature, Science, PLoS One)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Empirical generalizations and patterns about author roles and team-structure indicators (role categories; predictors of leadership vs. support; distributional patterns such as L-Ratio)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>1) Authors with high 'Probability of Leading' (frequent first authorship) or high 'Probability of Leading Correspondence' (frequent corresponding authorship) robustly indicate Leadership roles. 2) High 'Contribution to References' and 'Contribution to Topics' correlate with intellectual/leadership roles rather than support roles. (These are presented as interpretable empirical patterns rather than formal mechanistic laws.)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Comparison to manual human-labeled roles (human-annotated benchmark from the self-reported contribution data) using standard classification metrics (precision, recall, F1); ablation of zero-shot vs few-shot prompting behavior discussed qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPT-4: Leadership F1 = 0.993 (precision 0.995, recall 0.991); Direct Support F1 = 0.950; Indirect Support F1 = 0.947; macro-average F1 = 0.963, precision = 0.955, recall = 0.972. Authors also report GPT-4 'approaching an F1 of 0.97' in summary.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against other LLMs (Llama3 70B, Llama2 70B, Mistral 7x8B) and traditional models (BERT, RoBERTa, XGBoost). GPT-4 outperformed those LLMs and achieved higher F1 than the encoder models in few-shot/no fine-tuning setting (BERT avg F1 ≈ 0.905; RoBERTa avg F1 ≈ 0.879). For the downstream scalable classifier (DNN trained on GPT-4 labels) XGBoost (on engineered features) achieved F1 ≈ 0.78 while the DNN achieved F1 ≈ 0.76.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Few-shot GPT-4 reliably extracts structured role labels from self-reported contribution text and captures nuanced contextual cues that rule-based or smaller models miss; few-shot prompting outperformes zero-shot; GPT-4 labels enable training a scalable DNN. SHAP analysis on the downstream model shows the same high-level patterns (first/corresponding authorship and contribution-to-topics/references are strongest predictors of leadership).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LLM inference (GPT-4) is computationally and financially expensive for very large corpora, limiting direct scaling; zero-shot prompting produced unstable/ambiguous outputs; models are proprietary (accessibility concerns); potential mapping noise when linking papers to metadata (OpenAlex) and limited coverage to journals sampled; the outputs are empirical patterns/labels rather than mechanistic/causal laws, and the study does not claim formal causal discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4250.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4250.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs (science-of-science tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models applied to science-of-science tasks (identifying trends, extracting info, mapping evolution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites and discusses general uses of LLMs for processing scholarly literature to identify research trends, categorize articles, extract key information, and map the evolution of scientific fields, framing these as pattern-extraction applications in the science-of-science domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT family; Llama; Mistral (general reference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Reported in background: LLMs can be used to analyze large bodies of scholarly text to (i) identify emerging research trends, (ii) categorize scholarly articles, (iii) extract key facts and metadata, and (iv) map field evolution; techniques include zero-/few-shot prompting, summarization, and information extraction applied to paper text and metadata. No single named pipeline is given in the background discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Science of science / bibliometrics (general, cross-disciplinary scholarly literature)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>High-level empirical patterns and trends in scholarly communication (topic trends, co-evolution patterns, classification rules), i.e. empirical generalizations and patterns rather than formal theories.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Not detailed in this paper (background claim): examples would include extracting topic emergence timelines, identifying influential authors/topics, or summarizing typical contribution-role associations—no concrete extracted 'laws' are provided in the manuscript's background text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The manuscript positions LLMs as promising tools for extracting contextualized, pattern-rich summaries from scholarly text and for addressing limitations of term-based clustering; the authors motivate use of LLMs for tasks where context and nuance are critical.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Discussion-level claims: domain coverage and factuality concerns, risk of unstable zero-shot outputs, computational cost for large-scale use, and need for grounding/metadata to avoid hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4250.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4250.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT / LLM evaluation (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluations of ChatGPT/LLMs for assessing research quality and performing literature tasks (referenced studies)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites prior studies (e.g., Thelwall et al., Mengjia Wu et al.) that used ChatGPT / language models to evaluate research quality and classify research aims, as part of the broader literature on LLMs processing scholarly texts to produce high-level classifications and summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (and related LLMs in referenced studies)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Referenced work reportedly applied ChatGPT/LLMs with varied prompting/settings to judge article quality, classify research aims, or generate literature reviews; exact pipelines are cited but not described in detail in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Scholarly publishing / science-of-science evaluations (multidisciplinary)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Classification outcomes and empirical generalizations about research aims/quality (e.g., whether research is societally vs. scientifically oriented), and summarization patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Not specified in detail in this manuscript; referenced works evaluate model ability to classify article aims or assess research quality rather than extract explicit theoretical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Referenced studies used task-specific evaluations (e.g., comparing model judgments against human judgments, dataset ground truth), but exact methods are in the cited literature rather than this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited evidence supports that LLMs can be effective at literature-level classification and annotation tasks, with performance depending on prompt configuration and task framing.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>The paper notes referenced studies and prior work point to variability with settings and inputs, and that model effectiveness varies by discipline and task framing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating research quality with large language models: an analysis of chatgpt's effectiveness with different settings and inputs <em>(Rating: 2)</em></li>
                <li>Scientific progress or societal progress? a language modelbased classification of the aims of the research in scientific publications <em>(Rating: 2)</em></li>
                <li>Chatgpt outperforms crowd workers for text-annotation tasks <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4250",
    "paper_id": "paper-275471664",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "GPT-4 (role classification)",
            "name_full": "Generative Pre-trained Transformer 4 (used for author role classification)",
            "brief_description": "GPT-4 was used in this study with few-shot prompting to read authors' self-reported contribution statements and assign each author a single role (Leadership, Direct Support, Indirect Support), producing high-quality labels that were then used to train a scalable predictive model.",
            "citation_title": "TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "method_name": "Few-shot prompt-based role classification (GPT-4)",
            "method_description": "The authors constructed structured few-shot prompts that defined a three-level role hierarchy (Leadership &gt; Direct Support &gt; Indirect Support) and provided targeted examples; temperature was set to 0.01 for deterministic outputs. GPT-4 was fed the 'Author Contribution' field (text) for each paper and instructed to assign a single role per author, resolving multi-label entries by selecting the highest-ranking role present. Zero-shot prompting was explored but found unstable; few-shot examples were used for improved disambiguation. GPT-4-generated labels were later used as supervision to train a dense neural network for scalable application.",
            "number_of_papers": "Initial evaluation: 1,000 papers (250 per journal) producing ~5,000 author rows; additional inference on 2,000 papers (500 per journal) for modeling; further role inferences for ~10,000 authors leading to ~15,000 labeled author datapoints",
            "domain_or_field": "Science of science / bibliometrics (multidisciplinary scholarly publications from PNAS, Nature, Science, PLoS One)",
            "type_of_laws_extracted": "Empirical generalizations and patterns about author roles and team-structure indicators (role categories; predictors of leadership vs. support; distributional patterns such as L-Ratio)",
            "example_laws_extracted": "1) Authors with high 'Probability of Leading' (frequent first authorship) or high 'Probability of Leading Correspondence' (frequent corresponding authorship) robustly indicate Leadership roles. 2) High 'Contribution to References' and 'Contribution to Topics' correlate with intellectual/leadership roles rather than support roles. (These are presented as interpretable empirical patterns rather than formal mechanistic laws.)",
            "evaluation_method": "Comparison to manual human-labeled roles (human-annotated benchmark from the self-reported contribution data) using standard classification metrics (precision, recall, F1); ablation of zero-shot vs few-shot prompting behavior discussed qualitatively.",
            "performance_metrics": "GPT-4: Leadership F1 = 0.993 (precision 0.995, recall 0.991); Direct Support F1 = 0.950; Indirect Support F1 = 0.947; macro-average F1 = 0.963, precision = 0.955, recall = 0.972. Authors also report GPT-4 'approaching an F1 of 0.97' in summary.",
            "comparison_baseline": "Compared against other LLMs (Llama3 70B, Llama2 70B, Mistral 7x8B) and traditional models (BERT, RoBERTa, XGBoost). GPT-4 outperformed those LLMs and achieved higher F1 than the encoder models in few-shot/no fine-tuning setting (BERT avg F1 ≈ 0.905; RoBERTa avg F1 ≈ 0.879). For the downstream scalable classifier (DNN trained on GPT-4 labels) XGBoost (on engineered features) achieved F1 ≈ 0.78 while the DNN achieved F1 ≈ 0.76.",
            "key_findings": "Few-shot GPT-4 reliably extracts structured role labels from self-reported contribution text and captures nuanced contextual cues that rule-based or smaller models miss; few-shot prompting outperformes zero-shot; GPT-4 labels enable training a scalable DNN. SHAP analysis on the downstream model shows the same high-level patterns (first/corresponding authorship and contribution-to-topics/references are strongest predictors of leadership).",
            "challenges_limitations": "LLM inference (GPT-4) is computationally and financially expensive for very large corpora, limiting direct scaling; zero-shot prompting produced unstable/ambiguous outputs; models are proprietary (accessibility concerns); potential mapping noise when linking papers to metadata (OpenAlex) and limited coverage to journals sampled; the outputs are empirical patterns/labels rather than mechanistic/causal laws, and the study does not claim formal causal discovery.",
            "uuid": "e4250.0",
            "source_info": {
                "paper_title": "Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "LLMs (science-of-science tasks)",
            "name_full": "Large Language Models applied to science-of-science tasks (identifying trends, extracting info, mapping evolution)",
            "brief_description": "The paper cites and discusses general uses of LLMs for processing scholarly literature to identify research trends, categorize articles, extract key information, and map the evolution of scientific fields, framing these as pattern-extraction applications in the science-of-science domain.",
            "citation_title": "TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS",
            "mention_or_use": "mention",
            "model_name": "GPT family; Llama; Mistral (general reference)",
            "model_size": null,
            "method_name": null,
            "method_description": "Reported in background: LLMs can be used to analyze large bodies of scholarly text to (i) identify emerging research trends, (ii) categorize scholarly articles, (iii) extract key facts and metadata, and (iv) map field evolution; techniques include zero-/few-shot prompting, summarization, and information extraction applied to paper text and metadata. No single named pipeline is given in the background discussion.",
            "number_of_papers": null,
            "domain_or_field": "Science of science / bibliometrics (general, cross-disciplinary scholarly literature)",
            "type_of_laws_extracted": "High-level empirical patterns and trends in scholarly communication (topic trends, co-evolution patterns, classification rules), i.e. empirical generalizations and patterns rather than formal theories.",
            "example_laws_extracted": "Not detailed in this paper (background claim): examples would include extracting topic emergence timelines, identifying influential authors/topics, or summarizing typical contribution-role associations—no concrete extracted 'laws' are provided in the manuscript's background text.",
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "The manuscript positions LLMs as promising tools for extracting contextualized, pattern-rich summaries from scholarly text and for addressing limitations of term-based clustering; the authors motivate use of LLMs for tasks where context and nuance are critical.",
            "challenges_limitations": "Discussion-level claims: domain coverage and factuality concerns, risk of unstable zero-shot outputs, computational cost for large-scale use, and need for grounding/metadata to avoid hallucination.",
            "uuid": "e4250.1",
            "source_info": {
                "paper_title": "Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "ChatGPT / LLM evaluation (related work)",
            "name_full": "Evaluations of ChatGPT/LLMs for assessing research quality and performing literature tasks (referenced studies)",
            "brief_description": "The paper cites prior studies (e.g., Thelwall et al., Mengjia Wu et al.) that used ChatGPT / language models to evaluate research quality and classify research aims, as part of the broader literature on LLMs processing scholarly texts to produce high-level classifications and summaries.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChatGPT (and related LLMs in referenced studies)",
            "model_size": null,
            "method_name": null,
            "method_description": "Referenced work reportedly applied ChatGPT/LLMs with varied prompting/settings to judge article quality, classify research aims, or generate literature reviews; exact pipelines are cited but not described in detail in this manuscript.",
            "number_of_papers": null,
            "domain_or_field": "Scholarly publishing / science-of-science evaluations (multidisciplinary)",
            "type_of_laws_extracted": "Classification outcomes and empirical generalizations about research aims/quality (e.g., whether research is societally vs. scientifically oriented), and summarization patterns.",
            "example_laws_extracted": "Not specified in detail in this manuscript; referenced works evaluate model ability to classify article aims or assess research quality rather than extract explicit theoretical laws.",
            "evaluation_method": "Referenced studies used task-specific evaluations (e.g., comparing model judgments against human judgments, dataset ground truth), but exact methods are in the cited literature rather than this paper.",
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Cited evidence supports that LLMs can be effective at literature-level classification and annotation tasks, with performance depending on prompt configuration and task framing.",
            "challenges_limitations": "The paper notes referenced studies and prior work point to variability with settings and inputs, and that model effectiveness varies by discipline and task framing.",
            "uuid": "e4250.2",
            "source_info": {
                "paper_title": "Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating research quality with large language models: an analysis of chatgpt's effectiveness with different settings and inputs",
            "rating": 2,
            "sanitized_title": "evaluating_research_quality_with_large_language_models_an_analysis_of_chatgpts_effectiveness_with_different_settings_and_inputs"
        },
        {
            "paper_title": "Scientific progress or societal progress? a language modelbased classification of the aims of the research in scientific publications",
            "rating": 2,
            "sanitized_title": "scientific_progress_or_societal_progress_a_language_modelbased_classification_of_the_aims_of_the_research_in_scientific_publications"
        },
        {
            "paper_title": "Chatgpt outperforms crowd workers for text-annotation tasks",
            "rating": 2,
            "sanitized_title": "chatgpt_outperforms_crowd_workers_for_textannotation_tasks"
        }
    ],
    "cost": 0.013051249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS
February 26, 2025</p>
<p>Wonduk Seo 
Department of Information Management
Peking University
100871BeijingChina</p>
<p>Yi Bu buyi@pku.edu.cn 
Department of Information Management
Peking University
100871BeijingChina</p>
<p>Peking University Chongqing Research Institute of Big Data
401332ChongqingChina</p>
<p>TRANSFORMING ROLE CLASSIFICATION IN SCIENTIFIC TEAMS USING LLMS AND ADVANCED PREDICTIVE ANALYTICS
February 26, 2025737D4CE6123ED3CB75898DBA982F905BAuthor Role ClassificationLarge Language Models (LLMs)Predictive AnalyticsInterpretability AnalysisFew-shot PromptingFeature Engineering
Scientific team dynamics are critical in determining the nature and impact of research outputs.However, existing methods for classifying author roles based on self-reports and clustering lack comprehensive contextual analysis of contributions.Thus, we present a transformative approach to classifying author roles in scientific teams using advanced large language models (LLMs), which offers a more refined analysis compared to traditional clustering methods.Specifically, we seek to complement and enhance these traditional methods by utilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B, for role classification.Utilizing few-shot prompting, we categorize author roles and demonstrate that GPT-4 outperforms other models across multiple categories, surpassing traditional approaches such as XGBoost and BERT.Our methodology also includes building a predictive deep learning model using 10 features.By training this model on a dataset derived from the OpenAlex database, which provides detailed metadata on academic publications-such as author-publication history, author affiliation, research topics, and citation counts-we achieve an F1 score of 0.76, demonstrating robust classification of author roles.</p>
<p>Introduction</p>
<p>In the dynamic evolution of scientific research, the structure of research teams plays a significant role in shaping the nature and impact of their outputs [1].In such settings, accurately classifying the roles of team members is critical to understanding the mechanisms that drive scientific innovation and productivity.Effective role classification not only helps to recognize individual contributions, but also improves the management and optimization of research teams.Traditionally, methods have focused on categorizing authors into leadership, direct support, and indirect support.Leadership roles typically involve tasks such as designing and directing research, direct support encompasses activities like data collection and analysis, and indirect support includes providing feedback and editing manuscripts.The motivation behind focusing on these three categories is to understand the hierarchical structure and dynamics within scientific teams.Leadership roles significantly influence the direction and impact of a project, and identifying these roles allows to analyze patterns such as how leadership distribution affects team performance and the overall success of collaborative research efforts.Those are categorized based on clustering using terms derived from self-reported data, which refers to the information provided by the authors themselves about their contributions to a paper [2].</p>
<p>While this approach has yielded invaluable insights, including metrics such as the L-ratio, the proportion of leadership roles within teams, they are limited in their ability to capture the full context and nuances of authors' contributions.</p>
<p>arXiv:2501.07267v3 [cs.DL] 25 Feb 2025</p>
<p>Existing methods often lack the depth required to understand the specific context and impact of individual contributions.Furthermore, clustering used in previous research can be limited by their reliance on static data, failing to adapt to the evolving nature of scientific collaboration.For instance, an author who contributed to research design, data analysis, and manuscript writing might be grouped under one general category without distinguishing the importance of each task.This results in a static and incomplete representation of an author's role and impact on the research project.In addition, term-based clustering does not account for the complexity and multifaceted nature of authors' contributions, as it does not differentiate between the importance or context of multiple contributions made simultaneously by an author.This oversimplification can obscure the depth of individual effort and intellectual contribution, leading to a limited and sometimes misleading understanding of team dynamics and research contributions.One of the main challenges in classifying author roles lies in the complexity and nuance of contribution statements.Authors often describe their contributions using sophisticated and context-specific language, making it difficult for traditional models to accurately interpret and categorize these roles.This complexity requires a deep understanding of context and semantics that goes beyond simple keyword matching or basic natural language processing.</p>
<p>Given the significant advances in large language models (LLMs), they have been widely adopted for various science of science tasks such as identifying research trends, categorizing scholarly articles, extracting key information, and mapping the evolution of scientific fields [3,4].Our research explores a different setting: the use of LLMs such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B for role classification in scientific research [5,6,7].These advanced models can analyze patterns from the context of papers and authors, providing a more complete and dynamic understanding of team structures and collaboration dynamics, compared to traditional methods that often fail to capture the depth and context of individual contributions because they rely on static data and predefined categories.Furthermore, LLMs can perform few-shot learning, effectively adapting to new tasks with minimal examples, which is particularly advantageous when labeled data is scarce.They can handle the disambiguation of roles by understanding the interplay of different contributions within the same context, differentiating between leadership and support roles even when an author is involved in multiple tasks.By leveraging LLMs, we aim to overcome this limitation and provide deeper insights into the intricate roles within scientific teams.</p>
<p>We present a comprehensive framework that utilizes such LLMs to classify roles within scientific teams.Our methodology includes a few-shot prompt to accurately categorize author roles.Furthermore, after the LLM inference step, we aim at feature engineering: the framework incorporates ten extracted features reflecting each author's contributions and characteristics, such as contribution to references, contribution to topics, probability of leading, probability of managing correspondence, career age, citation count, unique topics, total publications, citation impact per year, and institutional diversity, by using a dataset derived from the OpenAlex database, which provides extensive metadata on academic publications, including citation counts, author affiliations, research topics, and publication history [8].We further train a deep learning model using 10 extracted features to classify author roles with high accuracy.Moreover, we conducted SHAP (SHapley Additive exPlanations) [9] analysis to investigate the importance of these features in the prediction tasks, providing valuable insights into their contributions.This approach not only complements, but also improves upon traditional methods, providing a more detailed and accurate classification of author roles.By leveraging the advanced capabilities of LLMs, our research aims to transform the landscape of scientific team role classification and provide a more nuanced understanding of team structures and collaboration dynamics.</p>
<p>Related Works</p>
<p>Dynamics of Scientific Teams</p>
<p>The structure and dynamics of scientific teams have been studied extensively, revealing a growing dominance and importance of team science in knowledge production.Research shows that teams tend to produce more highly cited research than individuals, a trend that has increased over time [1].Teams now produce more high-impact research, a distinction once dominated by individual authors [10].Katz and Martin [11] emphasized that collaboration among scientists leads to greater pooling of resources, sharing of knowledge, and combining of expertise, which in turn improves the quality and efficiency of scientific research.The interdisciplinary nature of many scientific teams allows for innovative approaches and solutions to complex problems [12].</p>
<p>Xu et al. [2] introduced the concept of the L-Ratio (Leadership Ratio), which measures the proportion of leadership roles within teams in their study of team structures, and demonstrated that flat team structures, which encourage equal contributions and a less hierarchical organization, are critical for fostering scientific innovation and improving team performance and research outcomes.Their findings are consistent with previous research suggesting that less hierarchical teams foster a more open exchange of ideas and promote creative problem solving [13,14].Furthermore, Wu et al. [15] found that smaller teams tend to produce more disruptive research compared to larger teams, which generally advance existing scientific knowledge.This indicates the critical role of team size in influencing research outcomes.</p>
<p>In addition, Cummings and Kiesler [16] suggested that team diversity, both in terms of disciplinary background and geographic location, is a significant predictor of team innovation and productivity.This diversity brings a variety of perspectives and skills to the table, leading to richer and more innovative outcomes.Moreover, Anicich et al. [17] demonstrated that hierarchical cultural values predict success and mortality in high-stakes teams.However, they also noted the challenges associated with coordinating diverse teams, highlighting the need for effective communication and management strategies to realize the full potential of team diversity.Haeussler and Sauermann provided insights into how the division of labor within teams is influenced by factors such as team size and interdisciplinarity, emphasizing its impact on collaborative knowledge production [18].</p>
<p>Traditional Methods of Role Classification</p>
<p>Traditionally, in the field of information science, clustering methods have been used to classify academic articles into research topics based on citation relationships and keyword analysis.Waltman and Eck [19] developed a methodology for constructing a publication-level classification system that uses citation networks and keyword co-occurrence to identify and group related research topics.This approach allows for a more objective and data-driven classification of research outputs, providing insights into the structure and evolution of scientific fields.Similar clustering techniques have been used in several other domains to categorize entities based on relational data.For example, bibliometric analysis and network clustering have been used to identify influential authors and research trends within specific disciplines [20].These methods often employ algorithms such as community detection and modularity optimization to uncover hidden patterns in complex datasets [21].By using these techniques, researchers can gain a deeper understanding of the collaborative networks and intellectual landscapes that underpin scientific progress.Additionally, Glänzel and Schubert [22] discussed the use of bibliometric methods to map the structure and dynamics of research fields.</p>
<p>However, an increasing focus has emerged on classifying roles within academic research teams, recognizing the importance of understanding team dynamics and individual contributions.Recent studies have proposed frameworks for identifying different types of research teams and their roles.For instance, a study introduced algorithms for team identification that categorize members into project-based, individual-based, and representative groups based on their contributions and collaboration patterns [23].Another work emphasized the need for a unified system to classify essential team roles beyond traditional task and social categories [24].In addition to these quantitative techniques, qualitative approaches have also been used to classify roles within academic research.Conger [25] argued that qualitative research is essential for understanding complex phenomena such as leadership within research teams.These qualitative methods complement quantitative approaches, providing a more complete picture of collaborative processes and individual contributions.Holbrook [26] highlighted the importance of qualitative evaluations in understanding the context and impact of research contributions.Moreover, different traditions of author ranking significantly shape the perception and classification of contributions.In many fields, the first author is typically seen as the primary contributor, while in others the last author, often the senior researcher or principal investigator, carries more prestige [27].These different conventions have implications for the assignment and recognition of roles.Additionally, systems such as the Contributor Roles Taxonomy have been developed to provide a transparent and detailed description of individual contributions [28].It specifies roles such as conceptualization, methodology, and software development, allowing for accurate attribution of credit.</p>
<p>Recently, metrics such as the L-Ratio have been used to measure the weight of leadership roles within research teams, providing a quantitative approach to assess the influence and contributions of different team members [2].This metric has shown that flat and egalitarian teams tend to produce more novel ideas compared to tall, hierarchical teams, highlighting the importance of team structure in fostering innovation.These findings emphasize the role of team dynamics and structure in driving creative and innovative research outcomes.</p>
<p>Large Language Models (LLMs) and Application in Science of Science</p>
<p>The advent of large language models (LLMs) such as GPT-4, Llama, and Mistral models has brought significant advances in text analysis and the production of human-like text, making them valuable tools for role classification in scientific research [29].Recent studies have demonstrated the effectiveness of LLMs in several domains, including scientific text analysis, classification tasks, and information extraction [3].The development of these models can be traced back to the introduction of transformers, a groundbreaking architecture introduced in the paper Attention is All You Need [30].Transformers use self-attention mechanisms to process input data in parallel, allowing models to capture long-range dependencies and contextual relationships more effectively than traditional recurrent neural networks (RNN) [31].In addition to the transformer architecture, several encoder-decoder models have emerged that exploit the strengths of transformers for different applications.In particular, BERT (Bidirectional Encoder Representations from Transformers), RoBERTa [32], and T5 (Text-to-Text Transformer) have been influential.BERT focuses on pre-training a deep bidirectional transformer by predicting masked words in a sentence, allowing it to capture complex context [33], while T5 frames all natural language processing (NLP) tasks as text-to-text problems, enabling the model to learn a variety of tasks through a consistent training framework [34].Decoder-only models, such as the GPT series, have also had a profound impact on the field, using a transformer decoder architecture to generate text by predicting the next word in a sequence, given all previous words [35].</p>
<p>While BERT and its variations have been widely applied in the science of science, the use of LLMs in this field is still emerging.Recent studies have begun to explore the potential of LLMs in evaluating research quality and classifying research aims.For instance, Thelwall et al. investigated the effectiveness of ChatGPT in assessing research quality under various conditions and examined the capability of ChatGPT to identify journal article quality across different academic disciplines [36,37,38].In the context of scientometrics, LLMs are used for various tasks such as identifying research trends, categorizing scholarly articles, extracting key information, and mapping the evolution of scientific fields.These capabilities enable researchers to automate the analysis of large volumes of scientific literature, facilitating more efficient and comprehensive studies of research impact and scholarly networks [3].For instance, LLMs can assist in generating literature reviews by summarizing key points from numerous papers, thus saving researchers significant time and effort [39].Additionally, they can help identify emerging research topics and influential authors by analyzing citation patterns and keyword co-occurrences [4].</p>
<p>Another important application of LLMs in scientometrics is their role in improving the transparency and reproducibility of research.By providing automated tools for literature review and data analysis, LLMs can help ensure that research methods and results are consistently documented and easily accessible.This can facilitate peer review and replication studies, which are critical to validating scientific findings [40].In addition, the use of LLMs to automate the synthesis of research findings can lead to more comprehensive meta-analyses, which are essential for drawing generalized conclusions from multiple studies [41].These advances contribute to a more robust and reliable scientific knowledge base.Furthermore, LLMs are increasingly being used to democratize access to scientific knowledge.By providing sophisticated text analysis and summarization tools, LLMs enable researchers from under-resourced institutions or regions to more effectively engage with the latest scientific literature.This can help bridge the gap between well-funded research institutions and those with fewer resources, promoting a more equitable distribution of scientific knowledge and opportunities for collaboration [42].</p>
<p>Dataset</p>
<p>We first utilize self-reported data collected from papers published in prominent journals such as PNAS, Nature, Science, and PLoS One from 2003 to 2020, which was opened by Xu et al [2].These four journals were selected due to their high impact and influence in the scientific community, ensuring high quality and significant research data.In addition, recent efforts to make scientific research fairer and more transparent have led these leading journals to require detailed reporting of each author's specific contributions.This requirement helps ensure that everyone involved in the research receives proper credit, making these journals ideal for our analysis.Specifically, PLoS One was included as it mandates comprehensive author contribution statements similar to the other journals, providing rich and structured data for analysis.Additionally, its broad scope across various scientific disciplines enables a more diverse representation of research team structures.To ensure consistency and reduce selection bias, we followed the methodology of Xu et al. [2] and stratified our sampling across these four journals.This selection provides a representative dataset that supports future work to understand and improve the dynamics of scientific collaboration.</p>
<p>The dataset focuses primarily on the "Author Contribution" field, which contains detailed information about each author's role and contributions to the paper.This field provides insight into various aspects of authors' participation in the research process, such as their involvement in experimental design, data analysis, manuscript writing, and other key tasks.By analyzing this detailed self-reported data, we were able to accurately classify authors' roles into the categories of Leadership, Direct Support, and Indirect Support.For instance, Leadership roles involve tasks such as designing and directing the research, Direct Support includes helping with data collection and analysis, and Indirect Support includes activities such as providing feedback and editing the manuscript.</p>
<p>In addition, for feature engineering purposes, we use the OpenAlex database, which is a rich resource that provides extensive metadata on academic publications, including citation counts, author affiliations, research topics, and publication history [8].This database allowed us to expand our dataset by incorporating more comprehensive academic profiles and publication records into our analysis, while ensuring that the data was clean and consistent.This included standardizing author names, resolving ambiguities in author identities, and ensuring that all relevant metadata from OpenAlex were correctly linked to the self-reported data.</p>
<p>In more detail, our dataset consists of two main parts for evaluation and modeling.In the first step, for the LLM evaluation, we used a dataset containing papers from 5,000 distinct authors, covering papers published between 2003 and 2020.To ensure a representative sample and maintain quality for validation purposes, we selected 250 papers from each of four prominent journals-PNAS, Nature, Science, and PLoS One.This selection process focused on papers with team sizes ranging from 2 to 8 authors, as these typically offer a clear division of labor and contribution roles.Through data cleaning and matching processes, we successfully aligned approximately 97% of our selected papers to their corresponding records in OpenAlex.The small proportion of papers that could not be mapped were due to inconsistencies in metadata, such as variations in author names or missing identifiers.In the second step, for feature engineering and modeling, we expanded our dataset to 2,000 papers from the same time period by using GPT-4, with 500 papers from each of the four journals.The average team size in both parts is consistently five authors per paper.</p>
<p>Overview of LLM-Based Role Classification Tasks</p>
<p>As shown in Figure 1, to ensure a representative sample and maintain quality for validation purposes, we selected 250 entries from each journal.This selection process focused on papers with between 2 and 8 authors, as these typically offer a clear division of labor and contribution roles.Using GPT-4, we assigned specific contributions to each author, categorizing them as Leadership, Direct Support, or Indirect Support based on predefined criteria.This step involved analyzing the "Author Contribution" field of each paper, which details each author's participation in various research activities such as experimental design, data analysis, manuscript writing, and other key tasks.Using the advanced natural language understanding capabilities of GPT-4, we accurately assigned contributions and categorized roles.</p>
<p>At the end of this assignment process, we compiled a dataset of 5,000 rows.We ensured this by first constraining the number of authors in each paper to be between 2 and 8 and randomly sampling 250 papers from each of the 4 journals, resulting in a total of 1,000 selected papers.Given that the average number of authors per paper was 5, we expanded the dataset to achieve a total of 5,000 rows: R = J × E × Ā where:</p>
<p>• J = 4 (number of journals);</p>
<p>• E = 250 (number of entries selected from each journal); • Ā = 5 (average number of authors per paper).</p>
<p>GPT-4 proved to be effective in accurately assigning papers and contributions, ensuring a balanced and comprehensive dataset for analysis.</p>
<p>Prompt Engineering for Role Classification</p>
<p>Once the contributions were assigned, we evaluated the intrinsic capabilities of the non-fine-tuned LLMs through few-shot prompt engineering.Our focus was on evaluating the performance of these models in accurately classifying roles based on predefined criteria.Each role category -Leadership, Direct Support, and Indirect Support -was evaluated and our expectations for each classification were outlined.</p>
<p>To facilitate this, we created specific prompts for the models, guiding them to categorize and classify research roles based on the activities associated with each role, as illustrated in Figure 2. We also explored zero-shot prompting, where the model relies solely on its pre-trained knowledge without additional examples.However, we observed that the model's predictions in zero-shot mode were less stable, often producing inconsistent or ambiguous role assignments.Without explicit demonstrations, LLMs sometimes misclassified contributions, particularly when role descriptions were subtle or overlapping.In contrast, our few-shot approach, which provided targeted examples, significantly improved classification accuracy by reducing ambiguity and ensuring better role differentiation.</p>
<p>To construct effective prompts, we defined a structured set of role categories and associated key responsibilities, ensuring clarity in classification.These role definitions provided the necessary context for LLMs to accurately assign contributions, as detailed below:</p>
<ol>
<li>Leadership: Responsibilities include designing, conceptualizing, directing, supervising, coordinating, interpreting, conducting, and writing the research.2. Direct Support: Tasks include helping, assisting, preparing, collecting, and analyzing.3. Indirect Support: Tasks include participating, providing, contributing, commenting, editing, and discussing.Given a dataset detailing the tasks of authors in a research project, the models were instructed to analyze each entry and assign roles accordingly.If an entry contained multiple categories, the role was selected from the highest category present.For example, if an author's activities included both "designing" (a Leadership task) and "providing" (an Indirect Support task), the author was classified under "Leadership" because it ranks higher in the role hierarchy.This structured hierarchy ensured that only one role was assigned to each author.</li>
</ol>
<p>LLM Model Configuration and Performance Comparison</p>
<p>Initially, we manually labeled corresponding roles for each author, which provided a benchmark for subsequent automated classification by following the criteria outlined in our prompts, assigning roles based on specific activities.We carefully examined each contribution statement, considering multiple facets such as the nature of the tasks performed, the level of responsibility, and the overall impact on the research.To automate this process, we used LLMs for inference, selecting four models: GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B.For each model, we standardized the prompting parameters to ensure consistency across evaluations.Specifically, we set temperature to 0.01 to minimize randomness in responses and provide more deterministic results.This setup allowed us to directly compare each model in accurately replicating the human-assigned roles.We performed a detailed performance comparison between GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B on three key metrics: F1 score, precision, and recall.</p>
<p>Our results demonstrated the superior ability of GPT-4 to accurately classify roles.As shown in Table 1, GPT-4 consistently led in precision and recall across all role categories, making it the most reliable of the models evaluated.Specifically, in the Leadership classification, GPT-4 achieved a precision of 0.995 and a recall of 0.991, significantly outperforming Llama3 70B, which had a precision of 0.996 but a lower recall of 0.851.For Llama3 70B, while excelling in precision for leadership, it did not perform as well in recall compared to GPT-4.Llama2 70B and Mistral 7x8B have shown moderate to poor performance in all metrics.The bar plot in Figure 3 shows notable differences in the label distributions of the four models-GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B-on the role classification tasks.GPT-4 predominantly assigned authors to the Leadership role more often than the other models, indicating its strong tendency to prioritize leadership activities.</p>
<p>Conversely, Mistral 7x8B and Llama2 70B showed a higher tendency to classify authors in the Direct Support role, suggesting that these models emphasize direct support activities more than GPT-4 and Llama3 70B.For the Indirect Support role, the distribution was relatively balanced across the models, with no single model showing a significantly higher number of classifications.</p>
<p>Given these results, GPT-4 was selected for further role classification of the remaining data due to its higher classification accuracy.We then used GPT-4 to perform additional inference on the extended dataset to ensure comprehensive and accurate classification of author roles within scientific teams.</p>
<p>Comparison of LLMs with Traditional Models</p>
<p>To compare the performance of LLMs with traditional machine learning models for the task of role classification in scientific teams, we used XGBoost, a well-known and robust machine learning algorithm, as a representative of traditional models due to its strengths in classification tasks [43].</p>
<p>For our comparison, XGBoost was used with a boosting learning rate of 0.2.The number of estimators was set to 1000 with a maximum tree depth of 8 for base learners.We split our dataset into training and validation sets using a  We also explored the performance of encoder-based transformer models, specifically BERT and RoBERTa, for this classification task.In our experiments, we fine-tuned both BERT and RoBERTa base models on our dataset using a learning rate of 2 × 10 −5 .Both models were trained for 5 epochs, using a stratified split based on the target variable, with a ratio of 0.2 for the validation set.BERT achieved an average F1 score of 0.9048, and RoBERTa achieved an average F1 score of 0.8794.Although these models also achieved promising results, it is important to emphasize several key advantages of LLMs over traditional machine learning models.First, LLMs, such as GPT-4, can perform classification tasks without requiring any training data.This few-shot capability allows LLMs to generalize from a large body of knowledge and apply it to new, unseen tasks.Second, unlike models that rely heavily on manually labeled training datasets, LLMs can infer roles and classifications directly from text.This significantly reduces the time and effort required for data preparation.Third, LLMs have advanced natural language understanding capabilities that allow them to capture complex contexts and nuances in author contributions.This results in more accurate and contextually relevant classifications.</p>
<p>Despite the lack of fine-tuning, GPT-4 showed an impressive overall score, approaching an F1 score of 0.97.This result shows the potential of LLMs for role classification tasks.One key reason LLMs outperform those models is their ability to handle complex and nuanced contribution statements that may contain ambiguous language or non-standard phrasing.For example, phrases such as "contributed extensively to the work presented in this paper" or "was involved in discussions that developed the understanding of the physical processes" present challenges for rule-based approaches, which rely on specific keywords.LLMs excel at interpreting such statements by capturing the context and inferring the appropriate role classification.This deep contextual understanding allows LLMs to surpass models such as XGBoost and BERT, particularly when authors mention multiple roles without emphasizing a primary one.While machine learning models such as XGBoost and BERT have their strengths, especially in structured data classification, the inherent advantages of LLMs make them a powerful tool for understanding and categorizing scientific roles.</p>
<p>Therefore, our comparison shows that while models such as XGBoost and BERT perform well, LLMs offer superior performance and flexibility, especially in scenarios where labeled data is scarce or when the task requires deep contextual understanding.The use of LLMs for role classification in scientific teams thus represents a promising direction for future research and applications.</p>
<p>Scalable Predictive Modeling for Author Role Classification</p>
<p>To enable large-scale and efficient analysis of author roles in scientific collaborations, we address the computational challenges associated with directly applying GPT-4 for large-scale predictions.While GPT-4 demonstrates exceptional performance in classifying author roles, its inference is computationally expensive and impractical for analyzing millions of authors.To overcome this challenge, we utilize labels generated from GPT-4 to train a dense neural network, which enables efficient classification of new data with minimal computational overhead.</p>
<p>Dataset Construction and Feature Engineering</p>
<p>To build our predictive model, we first constructed a robust dataset derived from the OpenAlex database, which provides extensive bibliometric and metadata on academic publications.We then moved on to building a predictive role classification model using feature engineering.</p>
<p>As shown in Table 3, we focused on 10 predictive features that capture the complex dynamics of academic authorship and contributions.Of these, eight features are derived from Xu et al [2].These features are:</p>
<ol>
<li>Contribution to References assesses the extent to which an author contributes references in their work.8. Total Publications reflects the total number of papers an author has published.
Contribution (References) = Overlap (References) Total (References)</li>
</ol>
<p>Total Publications = (Publications)</p>
<p>These eight features provide a well-rounded assessment of an author's contributions in the context of collaborative research teams.To further improve the model performance, we introduce two additional features to capture aspects of sustained research impact and collaboration diversity not fully represented by the original features:</p>
<ol>
<li>Citation Impact per Year is calculated by taking the total number of citations an author has received across all publications and dividing it by the number of years the author has been active, providing a measure of the average citation impact per year.</li>
</ol>
<p>Citation Impact per Year = Citation (All Publications) Years (Active)</p>
<p>(2) Institutional Diversity.With the inclusion of these features, the model's F1 score improved to approximately 0.76.This demonstrates the positive impact of these additional features on the accuracy of our role classification model.The model was trained for 20 epochs, and its performance was optimized through hyperparameter tuning.By ensuring a balanced and representative sample, we have laid a solid foundation for accurate and reliable author role classification.</p>
<p>We also experimented with the same dataset using XGBoost, configured with a boosting learning rate of 0.2.The number of estimators was set to 1000, and the maximum tree depth was set to 8 for base learners.The XGBoost model achieved a slightly better F1 score of 0.78 compared to the dense neural network.However, considering our goal to develop a model suitable for larger datasets, we chose the dense neural network due to its simplicity and scalability.</p>
<p>Furthermore, in the study by Xu et al., they achieved an F1 score of approximately 0.79 by utilizing a larger dataset and more complex modeling techniques.In contrast, our study achieved an F1 score of 0.76 using a smaller dataset and a relatively simple dense neural network model.This demonstrates that our approach is both robust and efficient, achieving comparable performance with less computational complexity and resource investment.</p>
<p>Interpretability Analysis Using SHAP</p>
<p>To further validate feature importance in our classification task, we apply SHAP (SHapley Additive exPlanations) [9,44] to investigate how each predictive feature contributes to the model's decision-making process.Specifically, we utilize Gradient SHAP, which efficiently estimates feature attributions for deep learning models using gradient-based sampling.the SHAP value for a feature i is calculated as:
ϕ i = S⊆F i / ∈S |S|!(M − |S| − 1)! M ! [f (S ∪ {i}) − f (S)]
where:</p>
<p>• F is the set of all features,</p>
<p>• S is a subset of features excluding i,</p>
<p>• f (S) is the model's prediction using only the features in S,</p>
<p>• ϕ i is the SHAP value for feature i.As shown in Figure 5, Probability of Leading Correspondence and Probability of Leading emerge as the most influential features in distinguishing leadership roles.These features likely capture the responsibility and visibility of an author within a research project, suggesting that consistent first or corresponding authorship is a strong indicator of leadership roles.Their high SHAP values further confirm their direct impact on classification predictions, reinforcing their centrality in the model's decision-making process.</p>
<p>Subsequently, Contribution to References and Contribution to Topics also hold substantial importance, showcasing their role in capturing intellectual contributions.A higher contribution to references indicates an author's influence on shaping discussions within a paper, while topic contributions suggest an author's role in guiding the thematic direction of research.The positive SHAP impact of these features suggests that intellectual involvement plays a critical role in distinguishing different author roles.</p>
<p>Additionally, the newly proposed features in this paper, including: Citation Impact per Year and Institutional Diversity, provide meaningful contributions.Citation Impact per Year accounts for the sustained influence of an author's research over time, making it a valuable indicator of long-term academic impact.Its moderate importance in the SHAP analysis suggests that while citations are significant, they are not the sole determinant of leadership roles.Institutional Diversity, on the other hand, captures the extent of an author's research collaborations across different institutions.A higher value for this feature suggests exposure to diverse academic environments, which may contribute to broader collaborative roles rather than direct leadership.</p>
<p>Conclusions</p>
<p>In this paper, we have presented a transformative approach to classifying author roles within scientific teams using advanced LLMs such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B.By integrating few-shot prompting and extensive feature engineering, we categorized author roles into Leadership, Direct Support, and Indirect Support.Our results showed that GPT-4 achieved the highest accuracy across multiple categories, outperforming other models in role classification tasks.By employing these advanced LLMs, we have improved the understanding and analysis of team roles in scientific research, providing a more detailed and accurate classification than traditional models.</p>
<p>We developed a predictive deep learning model incorporating ten features that capture the complex dynamics of academic authorship and contributions.These features included contribution to references, contribution to topics, probability of leading, probability of managing correspondence, career age, number of citations, unique topics, total publications, citation impact per year, and institutional diversity.The model, trained on a dataset derived from the OpenAlex database and self-reported contribution data, achieved an F1 score of 0.76, indicating robust performance in classifying author roles with a Dense Neural Network (DNN).</p>
<p>In addition, this methodology allows us to further calculate the L-Ratio, which quantifies the proportion of leadership roles within research teams.In future work, by integrating the L-Ratio with other variables, we can explore the relationship between leadership dynamics and various aspects of academic performance.For instance, using the L-Ratio to analyze the long-term performance of junior team members can provide insights into their career trajectories and contributions over time.This analysis aims to highlight the importance of leadership dynamics in scientific collaboration, providing valuable information on how junior researchers' roles and impacts evolve within academic teams.</p>
<p>Overall, our approach not only improves the accuracy of author role classification, but also provides deeper insights into the complex dynamics of scientific teamwork.This work represents a significant step forward in understanding contributions and collaborations within scientific teams, thereby improving our ability to analyze and promote effective scientific research environments.</p>
<p>7 Discussion: Technical and Practical Implications</p>
<p>Our study introduces a technical framework that leverages few-shot prompting with LLMs and a scalable neural network to generate high-quality labels without extensive training data.The use of SHAP for interpretability highlights the key factors-such as the probability of leading-that define leadership roles.Practically, our approach supports a deeper understanding of scientific collaboration.Moreover, the scalable predictive model paves the way for automated monitoring of author contributions across large datasets.</p>
<p>Despite these promising results, our study has limitations that suggest directions for future research.While advanced LLMs such as GPT-4 demonstrate superior performance in author role classification, their proprietary nature and computational requirements may limit accessibility for some researchers.Additionally, our reliance on data derived from the OpenAlex database may not capture the full diversity of author contributions across all scientific discipline and publication venues.To address these limitations, future work could involve exploring zero-shot inference with</p>
<p>Figure 1 :
1
Figure 1: Illustration of the workflow involving data sampling, preprocessing, contribution assigning, and classification task for the LLM-based role classification.</p>
<p>Figure 2 :
2
Figure 2: Illustration of the workflow involving prompt generation, LLM inference, and classification results.</p>
<p>Figure 3 :
3
Figure 3: Label distribution comparison across models.The figure illustrates the distribution of assigned roles-Leadership, Direct Support, and Indirect Support-by four models: GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B.</p>
<p>Figure 4 :
4
Figure 4: Line plot for F1-score by class for each model.</p>
<p>Figure 5 :
5
Figure 5: SHAP summary plot showing feature importance and the directional impact of each feature on the model's predictions.</p>
<p>Table 1 :
1
Comparison of Model Performance on Role Classification Tasks.
IndexModel NameRoleF1-Score Precision RecallLeadership0.9930.9950.9911GPT-4-1106Direct Support Indirect Support0.950 0.9470.945 0.9250.953 0.970Macro Avg.0.9630.9550.972Leadership0.9180.9960.8512Llama3 70BDirect Support Indirect Support0.569 0.8410.414 0.7480.911 0.961Macro Avg.0.7760.7190.907Leadership0.9080.9490.8703Llama2 70BDirect Support Indirect Support0.352 0.5140.314 0.3820.400 0.789Macro Avg.0.5910.5480.686Leadership0.9510.9690.9334Mistral 7x8BDirect Support0.5800.5380.629Indirect Support0.7830.6770.926Macro Avg.0.7710.7280.830</p>
<p>Table 2 :
2
Performance of BERT and RoBERTa Models on Role Classificaiton.
ModelsLearning RateEpoch Avg. F1-Score10.792720.8560BERT-base2 × 10 −530.892540.904850.886910.844420.8570RoBERTa-base2 × 10 −530.819140.863750.8794</p>
<p>Table 3 :
3
Categorization of the 10 Extracted Features.Contribution to Topics assesses the direction and influence of an author's previous work on the current research topic.Probability of Leading indicates the likelihood that an author has often been the first author on previous papers.Probability of Leading Correspondence shows the probability that an author has been the corresponding author on previous papers.Career Age measures the total number of years an author has been active in research.
IndexCategoryFeature Name1 2Contribution MetricsContribution to References Contribution to Topics3 4Leadership MetricsProbability of Leading Probability of Leading Correspondence5Career Duration MetricsCareer Age6 7Citation MetricsCitation Count Citation Impact per Year8Research Diversity MetricsUnique Topics9Publication MetricsTotal Publications10Collaboration Diversity MetricsInstitutional Diversity2. Contribution (Topics) =Overlap (Topics) Total (Topics)3. Probability (Leading) =Num (Times First Author) Total (Papers)4. Probability (Leading Correspondence) =Num (Times Corresponding Author) Total (Papers)5. Career Age = Year (Last Publication) − Year (First Publication)6. Citation Count refers to the total number of citations an author has received for all of his or her previouspublications.Citation Count =Citations (All Publications)7. Unique Topics is operationalized by the number of unique research topics an author has covered in his or herpublication history.UniqueTopics = Count (Unique Keywords)
LLM-Based Role Classification: Methods and EvaluationIn this stage, we focus on classifying and categorizing roles within scientific teams using self-reported data. Our methodology involves the use of open-source instructional LLMs and the GPT-4 model, augmented with few-shot prompting. Through this process, we aim to achieve a detailed categorization of scientific roles, thereby improving our understanding of team dynamics in scientific research environments. This classification serves as a crucial foundation for subsequent predictive modeling. While LLMs provide high-quality classifications, their computational demands make them impractical for large-scale applications. To address this, the LLM-generated labels will be leveraged to train a predictive model capable of efficiently scaling to extensive datasets. Section
details this predictive modeling process, which extends our role classification framework to broader scientific collaborations.
2. Institutional Diversity measures the number of unique institutions an author has been affiliated with or collaborated with over their career.A higher diversity of institutions suggests a broader network and potentially more diverse research experiences and influences.Institutional Diversity = Count (Unique Institutions)These 10 features were selected based on their relevance and ability to comprehensively capture the various dimensions of an author's contributions in collaborative research settings.The combination of these features enables the model to effectively classify authors' roles, ensuring a balanced and comprehensive assessment of their contributions.Data Splitting and NormalizationTo ensure a robust evaluation of the prediction models, we expanded our dataset by further inferring the roles of 10,000 distinct authors using the classification framework developed in the first phase, resulting in a total dataset of about 15,000 data points.Originally, the dataset included three classes: Leadership, Direct Support, and Indirect Support.For the purpose of this binary classification task, we merged the Direct Support and Indirect Support roles into a single "Support" category, while maintaining "Leadership" as a distinct category.This expanded dataset included detailed role classifications based on each author's contributions.The dataset was then divided into training and test sets using stratified sampling to maintain the distribution of the target variable.A ratio of 0.8 to 0.2 was used for the split, stratifying by two types of roles classified as leadership and non-leadership types.Additionally, Normalizing the dataset was a critical step in preparing it for predictive modeling.We applied a min-max scaling to each feature to ensure that they contributed equally to the model, without any single feature disproportionately influencing the results due to its size.The formula used for min-max normalization is given by:Predictive Model Training and PerformanceFor the role classification task, we constructed a Dense Neural Network (DNN) model.The architecture of the DNN was designed to effectively process the 10 predictive features and accurately classify author roles.We chose this model due to its simplicity and adequacy for our task.Dense neural networks are straightforward to implement and require less computational power compared to more complex architectures, making them suitable for large-scale applications.The model can process large datasets quickly, which is essential for applying the classification to extensive corpora of scientific publications.where:• x represents the input vector of the predictive features.• W 1 , W 2 , W 3 are the weight matrices for the hidden layers and output layer.• σ 1 , σ 2 , σ 3 are the ReLU (Rectified Linear Unit) activation functions for the hidden layers and a Sigmoid activation function for the output layer.• b 1 , b 2 , b 3 are the bias vectors for the hidden layers and the output layer.Initially, the model was trained using only the first eight features.This version of the model achieved an F1 score of about 0.74.To improve model performance, we introduced two additional features: (1) Citation Impact per Year and LLMs to obtain more natural and pattern-rich results.Furthermore, analyzing the patterns and insights derived from zero-shot inference could enhance our understanding of author roles and contributions, particularly in handling complex or ambiguous cases that challenge traditional rule-based approaches.Data AvailabilityThe self-reported data analyzed in this paper is available at https://github.com/fenglixu/Self-report-Contribution-Data.All data used in this study are open and available at OpenAlex https://openalex.org/.Competing interestsThe authors do not declare any competing interests.AcknowledgmentsYi Bu acknowledges the support from the National Natural Science Foundation of China (#72474009, #72104007, and #72174016) and from the 2024 Cultural Research Project of Ningbo under Grant WH24-2-4.The authors are grateful to Yifan Tian and Zonghao Yuan who had fruitful discussions with the authors.
The increasing dominance of teams in production of knowledge. Stefan Wuchty, Benjamin F Jones, Brian Uzzi, Science. 31658272007</p>
<p>Flat teams drive scientific innovation. Fengli Xu, Lingfei Wu, James Evans, Proceedings of the National Academy of Sciences. 11923e22009271192022</p>
<p>Ai for social science and social science of ai: A survey. Ruoxi Xu, Yingfei Sun, Mengjie Ren, Shiguang Guo, Ruotong Pan, Hongyu Lin, Le Sun, Xianpei Han, Information Processing &amp; Management. 6131036652024</p>
<p>Predicting research trends with semantic and neural networks with an application in quantum physics. Mario Krenn, Anton Zeilinger, Proceedings of the National Academy of Sciences. 11742020</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.040882024Mixtral of experts. arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Openalex: A fully-open index of scholarly works, authors, venues, institutions, and concepts. Jason Priem, Heather Piwowar, Richard Orr, arXiv:2205.018332022arXiv preprint</p>
<p>Scott Lundberg, arXiv:1705.07874A unified approach to interpreting model predictions. 2017arXiv preprint</p>
<p>Atypical combinations and scientific impact. Brian Uzzi, Satyam Mukherjee, Michael Stringer, Ben Jones, Science. 34261572013</p>
<p>What is research collaboration? Research policy. Sylvan Katz, Ben R Martin, 199726</p>
<p>Studying research collaboration using co-authorships. Göran Melin, Olle Persson, Scientometrics. 361996</p>
<p>Beyond team types and taxonomies: A dimensional scaling conceptualization for team description. Bianca John R Hollenbeck, Maartje E Beersma, Schouten, 2012Academy of Management Review37</p>
<p>Evidence for a collective intelligence factor in the performance of human groups. Anita Williams Woolley, Christopher F Chabris, Alex Pentland, Nada Hashmi, Thomas W Malone, science. 33060042010</p>
<p>Large teams develop and small teams disrupt science and technology. Lingfei Wu, Dashun Wang, James A Evans, Nature. 56677442019</p>
<p>Collaborative research across disciplinary and organizational boundaries. Jonathon N Cummings, Sara Kiesler, Social studies of science. 3552005</p>
<p>Hierarchical cultural values predict success and mortality in high-stakes teams. Eric M Anicich, Roderick I Swaab, Adam D Galinsky, Proceedings of the National Academy of Sciences. 11252015</p>
<p>Division of labor in collaborative knowledge production: The role of team size and interdisciplinarity. Carolin Haeussler, Henry Sauermann, Research Policy. 4961039872020</p>
<p>A new methodology for constructing a publication-level classification system of science. Ludo Waltman, Nees Jan, Van Eck, Journal of the American Society for Information Science and Technology. 63122012</p>
<p>Co-citation analysis, bibliographic coupling, and direct citation: Which citation approach represents the research front most accurately. Kevin W Boyack, Richard Klavans, Journal of the American Society for information Science and Technology. 61122010</p>
<p>Fast unfolding of communities in large networks. Jean-Loup Vincent D Blondel, Renaud Guillaume, Etienne Lambiotte, Lefebvre, Journal of statistical mechanics: theory and experiment. 10P100082008. 2008</p>
<p>Analysing scientific networks through co-authorship. Wolfgang Glänzel, András Schubert, Handbook of quantitative science and technology research: The use of publication and patent statistics in studies of S&amp;T systems. Springer2004</p>
<p>A method for identifying different types of university research teams. Zhe Cheng, Yihuan Zou, Yueyang Zheng, Humanities and Social Sciences Communications. 1112024</p>
<p>Team roles and hierarchic system in group discussion. Group Decision and Negotiation. Manabu Fujimoto, 201625</p>
<p>Qualitative research as the cornerstone methodology for understanding leadership. Jay A Conger, The Leadership Quarterly. 911998</p>
<p>Peer review, interdisciplinarity, and serendipity. Britt Holbrook, 2017</p>
<p>Collaborative research in the social sciences: Multiple authorship and publication credit. James W Endersby, Social Science Quarterly. 1996</p>
<p>Beyond authorship: Attribution, contribution, collaboration, and credit. Amy Brand, Liz Allen, Micah Altman, Marjorie Hlava, Jo Scott, 2015Learned Publishing28</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Attention is all you need. Vaswani, Advances in Neural Information Processing Systems. 2017</p>
<p>Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm) network. Alex Sherstinsky, Physica D: Nonlinear Phenomena. 4041323062020</p>
<p>Yinhan Liu, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. 2019364arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of naacL-HLT. Jacob Devlin, Ming-Wei Chang, Kenton , Lee Kristina, Toutanova , naacL-HLTMinneapolis, Minnesota20191</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of machine learning research. 211402020</p>
<p>Improving language understanding by generative pre-training. Alec Radford, 2018</p>
<p>Evaluating research quality with large language models: an analysis of chatgpt's effectiveness with different settings and inputs. Thelwall, Journal of Data and Information Science. 2024</p>
<p>Mike Thelwall, Abdallah Yaghi, arXiv:2409.16695which fields can chatgpt detect journal article quality? an evaluation of ref2021 results. 2024arXiv preprint</p>
<p>Scientific progress or societal progress? a language modelbased classification of the aims of the research in scientific publications. A Language Modelbased Classification of the Aims of the Research in Scientific Publications. Mengjia Wu, Gunnar Sivertsen, Lin Zhang, Fan Qi, Yi Zhang, April 22, 20242024</p>
<p>Openai chatgpt generated literature review: Digital twin in healthcare. Ömer Aydın, Enis Karaarslan ; Aydın, Ö Karaarslan, E , Emerging Computer Technologies. Ö. Aydın22022. 2022OpenAI ChatGPT Generated Literature Review: Digital Twin in Healthcare</p>
<p>Chatgpt outperforms crowd workers for text-annotation tasks. Fabrizio Gilardi, Meysam Alizadeh, Maël Kubli, Proceedings of the National Academy of Sciences. 12030e23050161202023</p>
<p>Can chatgpt write a good boolean query for systematic review literature search?. Shuai Wang, Harrisen Scells, Bevan Koopman, Guido Zuccon, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval2023</p>
<p>Marketing with chatgpt: Navigating the ethical terrain of gpt-based chatbot technology. Pablo Rivas, Liang Zhao, AI. 422023</p>
<p>A comparative analysis of gradient boosting algorithms. Candice Bentéjac, Anna Csörgő, Gonzalo Martínez-Muñoz, Artificial Intelligence Review. 541937-1967, 2021</p>
<p>Scott M Lundberg, Su-In Gabriel G Erion, Lee, arXiv:1802.03888Consistent individualized feature attribution for tree ensembles. 2018arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>